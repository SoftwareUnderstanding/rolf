{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "| Dataset | F1 | Precision | Recall | Accuracy |\n| ------ | ------ | ------ |  ------ |  ----- |\n| **ICAB-EvalITA07** | **86.240** | 85.939 | 86.544 | 98.534 | \n| **WikiNER-ITA** | **90.483** | 90.328 | 90.638 | 98.661 | \n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "| Dataset | F1 | Precision | Recall | Accuracy |\n| ------ | ------ | ------ |  ------ |  ------ |\n| **ICAB-EvalITA07** | **87.565**  | 86.596  | 88.556  | 98.690 | \n| **WikiNER-ITA** | **92.531**  | 92.509 | 92.553 | 99.136 | \n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Special thanks to I-CAB (Italian Content Annotation Bank) and [EvalITA](http://www.evalita.it/) authors to provide the datasets as part of Master Thesis Research project with [School of Engineering, University of Bologna](https://www.unibo.it/en/university/campuses-and-structures/schools/school-of-engineering).\n\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1810.04805",
      "https://arxiv.org/abs/1907.11692",
      "https://arxiv.org/abs/1609.06204"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "All of the original datasets are publicly available or were released with the owners' grant. The datasets are all released under a CC0 or CCBY license.\n\n* UD Italian-ISDT Dataset [Github](https://github.com/UniversalDependencies/UD_Italian-ISDT)\n* UD Italian-ParTUT Dataset [Github](https://github.com/UniversalDependencies/UD_Italian-ParTUT)\n* WIKINER [Page](https://figshare.com/articles/Learning_multilingual_named_entity_recognition_from_Wikipedia/5462500) , [Paper](https://www.sciencedirect.com/science/article/pii/S0004370212000276?via%3Dihub)\n* I-CAB (Italian Content Annotation Bank), EvalITA [Page](http://www.evalita.it/)\n```\n@inproceedings {magnini2006annotazione,\n\ttitle = {Annotazione di contenuti concettuali in un corpus italiano: I - CAB},\n\tauthor = {Magnini,Bernardo and Cappelli,Amedeo and Pianta,Emanuele and Speranza,Manuela and Bartalesi Lenzi,V and Sprugnoli,Rachele and Romano,Lorenza and Girardi,Christian and Negri,Matteo},\n\tbooktitle = {Proc.of SILFI 2006},\n\tyear = {2006}\n}\n@inproceedings {magnini2006cab,\n\ttitle = {I - CAB: the Italian Content Annotation Bank.},\n\tauthor = {Magnini,Bernardo and Pianta,Emanuele and Girardi,Christian and Negri,Matteo and Romano,Lorenza and Speranza,Manuela and Lenzi,Valentina Bartalesi and Sprugnoli,Rachele},\n\tbooktitle = {LREC},\n\tpages = {963--968},\n\tyear = {2006},\n\torganization = {Citeseer}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "* BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding [Paper](https://arxiv.org/abs/1810.04805), [Github](https://github.com/google-research/bert)\n* CamemBERT: a Tasty French Language Model [Paper](https://www.researchgate.net/publication/337183733_CamemBERT_a_Tasty_French_Language_Model), [Page](https://camembert-model.fr/)\n* GilBERTo: An Italian pretrained language model based on RoBERTa [Github](https://github.com/idb-ita/GilBERTo)\n* RoBERTa: A Robustly Optimized BERT Pretraining Approach [Paper](https://arxiv.org/abs/1907.11692), [Github](https://github.com/pytorch/fairseq/tree/master/fairseq/models)\n* Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing [Paper](https://www.aclweb.org/anthology/D18-2012/), [Github](https://github.com/google/sentencepiece)\n* Asynchronous Pipeline for Processing Huge Corpora on Medium to Low Resource Infrastructures [Paper](https://hal.inria.fr/hal-02148693), [Page]()\n* Italy goes to Stanford: a collection of CoreNLP modules for Italian (TINT) [Paper](https://arxiv.org/abs/1609.06204), [Github](https://github.com/dhfbk/tint), [Page](https://dh.fbk.eu/technologies/tint-italian-nlp-tool) \n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.884169924062702
      ],
      "excerpt": "Paolo Magnani: paul.magnani95 at gmail dot com, paulthemagno<br> \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/musixmatchresearch/umberto",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-01-10T09:55:31Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-23T22:34:10Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "UmBERTo inherits from RoBERTa base model architecture which improves the initial BERT by identifying key hyperparameters for better results.\nUmberto extends Roberta and uses two innovative approaches: ***SentencePiece*** and ***Whole Word Masking***.\nSentencePiece Model (**SPM**) is a language-independent subword tokenizer and detokenizer designed for Neural-based text processing and creates sub-word units specifically to the size of the chosen vocabulary and the language of the corpus. \nWhole Word Masking (**WWM**) applies mask to an entire word, if at least one of all tokens created by SentencePiece Tokenizer was originally chosen as mask. So only entire word are masked, not subwords.\n\nTwo models are released:\n  - **umberto-wikipedia-uncased-v1**, an uncased model trained on a relative small corpus (~7GB) extracted from \n  [Wikipedia-ITA](https://linguatools.org/tools/corpora/wikipedia-monolingual-corpora/).\n  - **umberto-commoncrawl-cased-v1**, a cased model trained on Commoncrawl ITA exploiting [OSCAR](https://traces1.inria.fr/oscar/) (Open Super-large Crawled ALMAnaCH coRpus) Italian large corpus ( ~69GB)\n\nBoth models have 12-layer, 768-hidden, 12-heads, 110M parameters (BASE).\n\n\n| Model | WWM | CASED | TOKENIZER | VOCAB SIZE  | TRAIN STEPS | FAIRSEQ  | TRANSFORMERS |\n| ------ | ------ | ------ | ------ | ------ |------ | ------ | --- |\n| `umberto-wikipedia-uncased-v1` | YES  | NO | SPM | 32K | 100k | [Link](http://bit.ly/2s7JmXh)| [Link](http://bit.ly/35wbSj6) |\n| `umberto-commoncrawl-cased-v1` | YES | YES | SPM | 32K | 125k | [Link](http://bit.ly/2TakHfJ)| [Link](http://bit.ly/35zO7GH) |\n\nWe trained both the models on 8 Nvidia V100 GPUs (p2.8xlarge P2 EC2 instance) during 4 days on [AWS Sagemaker](https://aws.amazon.com/it/sagemaker/).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.976066773935776,
        0.8882526052964252
      ],
      "excerpt": "UmBERTo is a Roberta-based Language Model trained on large Italian Corpora. \nThis implementation is based on Facebook Research AI code (https://github.com/pytorch/fairseq) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "    model=\"Musixmatch/umberto-commoncrawl-cased-v1\", \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9706917108436942,
        0.9548896876278453,
        0.957477279860426
      ],
      "excerpt": "We obtained state-of-the-art results for POS tagging, confirming that cased models trained with WWM perform better than uncased ones. \nOur model Umberto-Wikipedia-Uncased trained with WWM on a smaller dataset and uncased, produces important results comparable to the cased results. \nThese results refers to umberto-wikipedia-uncased model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.957477279860426
      ],
      "excerpt": "These results refers to umberto-commoncrawl-cased model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "UmBERTo: an Italian Language Model trained with Whole Word Masking.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/musixmatchresearch/umberto/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Mon, 27 Dec 2021 15:54:43 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/musixmatchresearch/umberto/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "musixmatchresearch/umberto",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.999746712887969,
        0.9635979784348493,
        0.9893272198983933,
        0.9906248903846466,
        0.999746712887969,
        0.9583854375834157,
        0.9879863063452118,
        0.9906248903846466,
        0.999746712887969
      ],
      "excerpt": "pip install transformers \nTo install transformers from original repo (TESTED): \ngit clone https://github.com/huggingface/transformers.git \ncd transformers \npip install . \nTo use a version of fairseq with UmBERTo support, build from source doing these steps: \ngit clone https://github.com/musixmatchresearch/fairseq \ncd fairseq \npip install . \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8370395646757436
      ],
      "excerpt": "result = fill_mask(\"Umberto Eco \u00e8 <mask> un grande scrittore\") \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "from transformers import AutoTokenizer, AutoModel \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8009529246210325
      ],
      "excerpt": "umberto.eval()  #: disable dropout (or leave in train mode to finetune) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.814044620754941,
        0.8289669050403863
      ],
      "excerpt": "result = umberto.fill_mask(masked_line, topk=20) \n: Output: \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/musixmatchresearch/umberto/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 Musixmatch Research\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "UmBERTo: an Italian Language Model trained with Whole Word Masking",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "umberto",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "musixmatchresearch",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/musixmatchresearch/umberto/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\ntorch >= 1.3.1\nsentencepiece\ntransformers\nfairseq\n```\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 76,
      "date": "Mon, 27 Dec 2021 15:54:43 GMT"
    },
    "technique": "GitHub API"
  }
}