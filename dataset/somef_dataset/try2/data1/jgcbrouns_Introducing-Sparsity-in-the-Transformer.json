{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1706.03762"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8391686582976066
      ],
      "excerpt": "\"Attention is All You Need\" (Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin, arxiv, 2017) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jgcbrouns/Introducing-Sparsity-in-the-Transformer",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-12-11T14:44:15Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-09-16T22:17:01Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.984301934811915
      ],
      "excerpt": "A proof of concept implementation of evolutionary sparsity in the Transformer model architecture. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9121493360867373
      ],
      "excerpt": "*Original architecture with a rewritten trainingsloop and using custom transfer-function in order to validate the obtained results * \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8397205170115146
      ],
      "excerpt": "sets the dataset to be used for the trainings-task \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8355502781128447
      ],
      "excerpt": "- 'testdata': Use a very small subset of the original trainings-task \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jgcbrouns/Introducing-Sparsity-in-the-Transformer/releases",
    "technique": "GitHub API"
  },
  "faq": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- The test sys argument gives me error: UnicodeEncodeError: 'ascii' codec can't encode character '\\xe4' in position 6: ordinal not in range(128).   \n**Solution:** *run in terminal:* ```export LC_CTYPE=C.UTF-8```\n  \n  \n\n",
      "technique": "Header extraction"
    }
  ],
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Sat, 25 Dec 2021 15:52:58 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jgcbrouns/Introducing-Sparsity-in-the-Transformer/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "jgcbrouns/Introducing-Sparsity-in-the-Transformer",
    "technique": "GitHub API"
  },
  "invocation": [
    {
      "confidence": [
        0.8579496641137323,
        0.9391828765704823
      ],
      "excerpt": "Sparse variant architecture, trained on the original data (29.000 samples in training set, 1024 samples in test set) \npython3 en2de_main.py sparse origdata \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991
      ],
      "excerpt": "python3 en2de_main.py originalWithTransfer origdata \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jgcbrouns/Introducing-Sparsity-in-the-Transformer/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Introducing Sparsity in the Transformer model (Keras Implementation)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Introducing-Sparsity-in-the-Transformer",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "jgcbrouns",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jgcbrouns/Introducing-Sparsity-in-the-Transformer/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Sat, 25 Dec 2021 15:52:58 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- **Transformer implementation in Keras by LSdefine:**  \n[The Transformer model in Attention is all you need\uff1aa Keras implementation.](https://github.com/Lsdefine/attention-is-all-you-need-keras)\n- **Attention is all you need - A Pytorch implementation**  \n[Jadore801120/attention-is-all-you-need-pytorch](https://github.com/jadore801120/attention-is-all-you-need-pytorch).\n- **Sparsity SET-procedure based on the proof-of-concept code of:**   \n[Dr. D.C. Mocanu - TU/e](https://github.com/dcmocanu/sparse-evolutionary-artificial-neural-networks/blob/master/SET-MLP-Keras-Weights-Mask/fixprob_mlp_keras_cifar10.py)\n\n",
      "technique": "Header extraction"
    }
  ]
}