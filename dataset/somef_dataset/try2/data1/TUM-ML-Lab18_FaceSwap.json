{
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/TUM-ML-Lab18/FaceSwap",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-08-31T07:57:35Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-06-09T07:38:16Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9010859273296813
      ],
      "excerpt": "a lot of NumPy arrays with extracted features. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9401907086059381,
        0.9625353946490773
      ],
      "excerpt": "be modeled using our neural nets. The second part of that pipeline fits generated images back into the original scene. \nThis module is built using classical computer vision methods. For the extraction of facial landmarks we used the great \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9854997951757091,
        0.8276566121071486
      ],
      "excerpt": "position of faces in images, but also as input features for our networks. \nDeepFakes uses deep convolutional auto encoders to swap the face of two people, preserving the facial expression. It is publicly available via GitHub and caught our attention through an excellent article on hackernoon.com (https://hackernoon.com/exploring-deepfakes-20c9947c22d9). The architecture can be depicted as follows: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.996119377528515
      ],
      "excerpt": "The idea behind our \"Latent Model\" is to use already extracted features as input for a simple decoder network. The network is trained end to end with an image as target and features extracted from it as input. If the features have a higher dimensionality we reduced it by flattening it. Several features were just concatenated to one input vector. As features we experimented with several representations of landmarks (i.e. combine all landmarks of the nose to one), low resolution image, histogram and a deep face encoding. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9725755980725345,
        0.9818962600132287
      ],
      "excerpt": "For the decoder we just reused the decoder from the Deepfakes architecture. We achieved best results by combining all landmarks and a 8x8x3 (RGB) representation of the input image to a feature vector. The loss is just defined by the L1 distance of the input image and the generated one. \nThe \"LatentGAN\" is an architecture which combines our latent model with a discriminator, forming an architecture similar to a generative adversarial network. The motivation behind this architecture was the fact that our latent model already produced quite decent results. However, we wanted to make them look even more realistic and came up with the idea of integrating a discriminator into the model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9918035134927412
      ],
      "excerpt": "Deep convolutional GANs were proposed in 'Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks' from Radford et. al (https://arxiv.org/pdf/1511.06434.pdf). They combine the classic GAN approach with deep convolutional neural networks. By the means of CNNs as high capable function approximators, the performance of GANs should be increased. In this model, we have also introduced condition on facial key points to preserve the facial expression of an input image. However, this conditioning can be unstable to some extend due to the high dimensionality of our conditioning vector. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Emotion-preserving face swapping algorithms using deep generative models",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/TUM-ML-Lab18/FaceSwap/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Sun, 26 Dec 2021 07:24:51 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/TUM-ML-Lab18/FaceSwap/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "TUM-ML-Lab18/FaceSwap",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/TUM-ML-Lab18/FaceSwap/master/misc/ssh_init.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8067724896156271
      ],
      "excerpt": "In the next step you have to define the path to your dataset. This can be done in Configuration/config_general: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9387875948612477
      ],
      "excerpt": "Preprocessing finished! You can now start to train your models! \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8639986685036579,
        0.8639986685036579
      ],
      "excerpt": "    |-- image1.jpg \n    |-- image2.jpg \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639986685036579
      ],
      "excerpt": "    `-- image9999.jpg \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8001115475554887
      ],
      "excerpt": "Now you need to run the script preprocess.py and wait until your data is processed: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8927430287682983
      ],
      "excerpt": "(face) max@MaxLaptop:~$ python preprocess.py  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8080818649989495
      ],
      "excerpt": "All images processed. Storing results in NumPy arrays...                                                                                                                                                                                                                 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/TUM-ML-Lab18/FaceSwap/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "General",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "FaceSwap",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "TUM-ML-Lab18",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/TUM-ML-Lab18/FaceSwap/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 7,
      "date": "Sun, 26 Dec 2021 07:24:51 GMT"
    },
    "technique": "GitHub API"
  }
}