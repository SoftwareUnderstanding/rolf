{
  "citation": [
    {
      "confidence": [
        0.9083996137433826
      ],
      "excerpt": "BERT-Base, Uncased(https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip): 12-layer, 768-hidden, 12-heads, 110M parameters \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/SkullFang/BERT_NLP_Classification",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-01-30T08:35:47Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-08T10:33:55Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Migration learning is already hot in the CV space. However, there has been no development in the NLP field. According to the theory, migration learning can be used in the NLP field. In October last year, Google sent a migration study paper in the NLP field: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Although this is not the earliest paper in the field of NLP migration learning, the first papers are ELMo, fasiai's ULMFit and OpenAI's Transformer. Google created BERT based on predecessors, and BERT is much better than other models. Attention and transformer are mainly used in BERT. Detailed details can be seen in the original text.\nHttps://arxiv.org/abs/1810.04805\nThis project is fine-tuned on this basis, mainly modifying the run_classify.py file. Refer to this blog. https://mp.weixin.qq.com/s/XmeDjHSFI0UsQmKeOgwnyA\nAnd the code has been modified to better suit our project needs.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8247119797510858,
        0.9210360634103224,
        0.9785900836526855,
        0.9796943582606806
      ],
      "excerpt": "The log will show the model performance and generate output_models folde. \nFinding the checkpoint file with the largest tail number is the model after migration learning. \nThe performance is shown on our verification data set. \nThis effect is better than the UIMFIT model. Here I also implemented the migration learning algorithm of UIMFIT on the same dataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Use transfer learning for Text_classification with BERT.",
      "technique": "GitHub API"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "some detail\nbert github\nhttps://github.com/google-research/bert\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/SkullFang/BERT_NLP_Classification/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 5,
      "date": "Tue, 28 Dec 2021 02:22:42 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/SkullFang/BERT_NLP_Classification/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "SkullFang/BERT_NLP_Classification",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/SkullFang/BERT_NLP_Classification/master/PREDICT.ipynb",
      "https://raw.githubusercontent.com/SkullFang/BERT_NLP_Classification/master/GET_DATA.ipynb",
      "https://raw.githubusercontent.com/SkullFang/BERT_NLP_Classification/master/DEAL_WITH_TEST.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/SkullFang/BERT_NLP_Classification/master/build_model.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Download the data on our server. The pymysql package is used. Do a simple stop word processing. And the data is divided into training data set train.tsv, val.tsv, test.tsv, and stored in the specified folder.\n```\nmkdir DATA_DIR\n```\n\nrun GET_DATA.ipynb\n\nFor legal reasons, I am temporarily unable to disclose the data set. So I hidden the account password in the code. You can prepare the data set in the format I gave.\n\n The directory structure is as follows:\n>DATA_DIR/\n\n>>train.tsv\n\n>>test.tsv\n\n>>val.tsv\n\ntrain.tsv  sample as\uff1a\n\n![Alt text](https://github.com/SkullFang/BERT_NLP_Classification/blob/master/image/train.png)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9935459261120722
      ],
      "excerpt": "pip3 install -r requirements.txt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9122314369824116,
        0.9023697225149864
      ],
      "excerpt": "modify build_model.sh \nvim build_model.sh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9465718491881494
      ],
      "excerpt": "bash build_model.sh \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8339446831753
      ],
      "excerpt": "unzip model to BERT_BASE_DIR folder \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/SkullFang/BERT_NLP_Classification/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "BERT_NLP_Classification",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "BERT_NLP_Classification",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "SkullFang",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/SkullFang/BERT_NLP_Classification/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 19,
      "date": "Tue, 28 Dec 2021 02:22:42 GMT"
    },
    "technique": "GitHub API"
  }
}