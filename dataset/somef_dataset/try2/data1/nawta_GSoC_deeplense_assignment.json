{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1909.07346\n \n [2] https://github.com/Jammy2211/PyAutoAstro/blob/master/autoastro/profiles/mass_profiles/mass_sheets.py\n   \n [3] Introduction to Gravitational Lensing Lecture scripts, pp37,39,41\n http://www.ita.uni-heidelberg.de/~massimo/sub/Lectures/gl_all.pdf\n \n ------\n \n \n## TASK 2\n\nQ. Using a deep learning algorithm of your choice, learn the representation of dark matter in the strong gravitational lensing images provided using PyTorch.\nThe goal is to use this learned representation for anomaly detection, hence you should pick the most appropriate algorithm and discuss your strategy.   \n   \nA. Considering the purpose is anomaly detection, I choose Autoencoder as the most appropriate method, as discussed in [1].  \nI think Conditional VAE (CVAE",
      "https://arxiv.org/abs/1406.5298\n\n[6]Latent representations of transient candidates from an astronomical image difference pipeline using Variational Autoencoders  \nhttps://pdfs.semanticscholar.org/57ce/a9520008706b8cb190d94513e695068210cd.pdf\n\n[7]Convolutional Sparse Autoencoders for Image Classification (SCAE",
      "https://arxiv.org/abs/1909.07346\n  \n  "
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "  \n----  \n  \n[1]Deep Learning the Morphology of Dark Matter Substructure, pp8  \nhttps://arxiv.org/pdf/1909.07346.pdf\n\n[2]Deep Clustering with Convolutional Autoencoders  \nhttps://link.springer.com/chapter/10.1007/978-3-319-70096-0_39\n\n[3]Pytorch\u306b\u3088\u308bAutoEncoder Family\u306e\u5b9f\u88c5(Implementation of various Autoencoders using PyTorch)  \nhttps://elix-tech.github.io/ja/2016/07/17/autoencoder.html\n\n[4]\u69d8\u3005\u306a\u30aa\u30fc\u30c8\u30a8\u30f3\u30b3\u30fc\u30c0\u306b\u3088\u308b\u7570\u5e38\u691c\u77e5(Anomaly Detection with lots of Autoencoders)  \nhttps://sinyblog.com/deaplearning/auto_encoder_001/\n\n[5]Semi-Supervised Learning with Deep Generative Models (CVAE)  \nhttps://arxiv.org/abs/1406.5298\n\n[6]Latent representations of transient candidates from an astronomical image difference pipeline using Variational Autoencoders  \nhttps://pdfs.semanticscholar.org/57ce/a9520008706b8cb190d94513e695068210cd.pdf\n\n[7]Convolutional Sparse Autoencoders for Image Classification (SCAE)  \nhttps://ieeexplore.ieee.org/document/7962256\n\n[8]Auto-Encoding Variational Bayes(VAE)  \nhttps://arxiv.org/pdf/1312.6114.pdf\n\n[9]Improving Variational Autoencoder with Deep Feature Consistent and Generative Adversarial Training (GAN)  \nhttps://arxiv.org/pdf/1906.01984.pdf\n\n[10]Anomaly Manufacturing Product Detection using Unregularized Anomaly Score on Deep Generative Models  \nhttps://confit.atlas.jp/guide/event-img/jsai2018/2A1-03/public/pdf?type=in  \nhttps://qiita.com/shinmura0/items/811d01384e20bfd1e035\n\n[11] Deep Learning the Morphology of Dark Matter Substructure https://arxiv.org/abs/1909.07346\n  \n  \n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "   \n [1] Deep Learning the Morphology of Dark Matter Substructure\n https://arxiv.org/abs/1909.07346\n \n [2] https://github.com/Jammy2211/PyAutoAstro/blob/master/autoastro/profiles/mass_profiles/mass_sheets.py\n   \n [3] Introduction to Gravitational Lensing Lecture scripts, pp37,39,41\n http://www.ita.uni-heidelberg.de/~massimo/sub/Lectures/gl_all.pdf\n \n ------\n \n \n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/nawta/GSoC_deeplense_assignment",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-03-19T16:55:33Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-07-01T19:36:08Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8961164670941676,
        0.999658291902287
      ],
      "excerpt": "Q.Modify and/or use the already existing functionality of PyAutoLens to simulate strong lensing from superfluid dark matter. Specifically, you will need to simulate the effects of lensing from a linear mass density - imagine this being a string of mass on galactic scales. \nA.First of all, For simplicity, I am going to think the dark matter discussed in this task as axion. This is because axion is a representative of the dark matter with superfluidity, and is in the paper of DEEPLENSE project[1] . \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8328096487294623
      ],
      "excerpt": "According to pp.3[1], vortex solution is characterized by a density profile that can be parameterized below \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9316223715772194,
        0.8272491199076885,
        0.9540350381132262,
        0.9912269100009625
      ],
      "excerpt": "there're two ways in the aspect of implementation. \nFirst, I implement the dark matter halo as a spherical isothermal, like the related work[1] does. I use the parameter according to the paper. \nSecond, I implement it using mass-sheet class, which is at PyAutoAstro project[2]. \nIn this case, according to [3], I calculate kappa(The magnitude of the convergence of the mass-sheet) in the equation below.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.970039431810699
      ],
      "excerpt": "Q. Using a deep learning algorithm of your choice, learn the representation of dark matter in the strong gravitational lensing images provided using PyTorch. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8441934590222794
      ],
      "excerpt": "A. Considering the purpose is anomaly detection, I choose Autoencoder as the most appropriate method, as discussed in [1]. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8613941684981881,
        0.9931171441483438,
        0.9794861375006628,
        0.833835195917102,
        0.9895976349317581,
        0.9973152101170906,
        0.9059545605324374,
        0.9764582012621423
      ],
      "excerpt": "Deep neural network architecture is known to function relatively better in tasks about images than MLP(multilayer perception), and in the early stage Convolutional AutoEncoder(hereafter CAE)[2] and Variational AutoEncoder(VAE)[8] were the most competent method. \nOf cource, it is said that CNN architecture in general performs well, speaking of image processing. \nCAE incorpolates CNN's algorithm into itself, and according to [3],[4] (written in Japnanese), it clearly shows that CAE overwhelms other MLP algorithms.  \nVAE incorpolates probability distribution into latent variable z, and it also performs as well as CAE does. \nThese days, lots of methods derived from these, and I noticed CVAE is the best, which add supervising features to VAE. \nit is compared with lots of other autoencoder methods on purpose of analysis on astronomical images[6], and prove that the latent variables obtained with CVAE preserve more information and are more discriminative towards real astronomical transients.(SCAE[7] did well as well, but slightly CVAE has an advantage of it)   \nThis paper was published in April 2018, and it is the most recent paper applying autoencoder to processing astronomical images.(According to my survey on Google Scholor) \nIn addition to that, CVAE is implementable on laptop, whereas other methods sometimes need a bulk of computer properties(such as [9].)   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8611674981340225
      ],
      "excerpt": "I couldn't do operational check because of my laptop's lack in spec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9328126392997268,
        0.8273191022740018
      ],
      "excerpt": "About optimizer, I use Adam because this is most popular optimizers' father and since this is not that complicated, the convergence speed is fast.   \nin inplementation, I refered to this site:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "preliminary assignment of deep learning's application for Astronomy, in GSoC 2020",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/nawta/GSoC_deeplense_assignment/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sun, 26 Dec 2021 00:42:16 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/nawta/GSoC_deeplense_assignment/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "nawta/GSoC_deeplense_assignment",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/nawta/GSoC_deeplense_assignment/master/Task%202.ipynb",
      "https://raw.githubusercontent.com/nawta/GSoC_deeplense_assignment/master/Task%201.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8007727959479307
      ],
      "excerpt": "(x: the distance from spectator to source plane, or lensed galaxy) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8819061505481144
      ],
      "excerpt": "Directory structures are below. you should put lenses directory at the same place as this notebook.  \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8712898244322753,
        0.8712898244322753,
        0.8737892364852982,
        0.8737892364852982
      ],
      "excerpt": "- lenses/train/sub/(3001 images) \n- lenses/train/no_sub/(3001 images) \n- lenses/test/sub/(2000 images) \n- lenses/test/no_sub/(2000 images) \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/nawta/GSoC_deeplense_assignment/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "GSoC_deeplense_assignment",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "GSoC_deeplense_assignment",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "nawta",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/nawta/GSoC_deeplense_assignment/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Sun, 26 Dec 2021 00:42:16 GMT"
    },
    "technique": "GitHub API"
  }
}