{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1707.06347",
      "https://arxiv.org/abs/1802.09477",
      "https://arxiv.org/abs/2010.01652",
      "https://arxiv.org/abs/1802.09477",
      "https://arxiv.org/abs/1801.01290"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8573139947629018
      ],
      "excerpt": "Temporal Difference Methods and Q-learning \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Rafael1s/Deep-Reinforcement-Learning-Algorithms",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-04-07T17:22:57Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-23T16:35:44Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.908540504174483,
        0.9690461596632891,
        0.8069890455664129
      ],
      "excerpt": "The projects are deployed in the matrix form: [env x model], where env is the environment  \nto be solved, and model is the model/algorithm which solves this environment. In some cases,   \nthe same environment is resolved by several algorithms. All projects are presented as  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9567588029116127
      ],
      "excerpt": "HopperBulletEnv,  LunarLander,  LunarLanderContinuous,  Markov Decision 6x6,  Minitaur, Minitaur with Duck,     \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9455880521623741
      ],
      "excerpt": "Four environments (Navigation,  Crawler, Reacher,  Tennis) are solved in the framework of the  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9581781930945077,
        0.9756995537140584,
        0.8697340118931205,
        0.8109066249621223,
        0.9494684742082062
      ],
      "excerpt": "In Monte Carlo (MC), we play episodes of the game until we reach the end, we grab the rewards    \ncollected on the way and move backward to the start of the episode. We repeat this method  \na sufficient number of times and we average  the value of each state.    \nTemporal Difference Methods and Q-learning \nReinforcement Learning in Continuous Space (Deep Q-Network) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8760315677888482
      ],
      "excerpt": "The Universal Approximation Theorem (UAT) states that feed-forward neural networks containing a    \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9165415932901354,
        0.8504053526248315
      ],
      "excerpt": "Random-restart hill-climbing is a surprisingly effective algorithm in many cases.  Simulated annealing is a good   \nprobabilistic technique because it does not accidentally think a local extrema is a global extrema. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9646120885267858,
        0.8532867061900392,
        0.95752846932856
      ],
      "excerpt": "The key difference from A2C is the Asynchronous part. A3C consists of multiple independent agents(networks) with  \ntheir own weights, who interact with a different copy of the environment in parallel. Thus, they can explore   \na bigger part of the state-action space in much less time.   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.961093431597431,
        0.9554064189119046,
        0.9554588500393743,
        0.8395987583210103,
        0.957787542191343,
        0.8811526777768702
      ],
      "excerpt": "Model-based reinforcement learning uses the model in a sophisticated way, often based  \non deterministic or stochastic optimal control theory to optimize the policy based  \non the model. FORK only uses the system network as a blackbox  to forecast future states,  \nand does not use it as a mathematical model for optimizing control actions.    \nWith this key distinction, any model-free Actor-Critic algorithm with FORK  remains \nto be model-free. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9629962971611608,
        0.808099016697036
      ],
      "excerpt": "Udacity Project 1: Navigation, DQN, ReplayBuffer    \nUdacity Project 2: Continuous Control-Reacher, DDPG, environment Reacher (Double-Jointed-Arm)     \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9528137161429222,
        0.9528137161429222,
        0.8131545514897772,
        0.9090176073552221
      ],
      "excerpt": "Cartpole with Deep Q-Learning    \nCartpole with Doouble Deep Q-Learning \non Policy-Gradient Methods, see 1, 2, 3. \non REINFORCE, see 1, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9090176073552221,
        0.9090176073552221,
        0.9338604744350135,
        0.9090176073552221,
        0.9090176073552221,
        0.9090176073552221,
        0.9377353135022518,
        0.9468929126638493,
        0.9429095683116127
      ],
      "excerpt": "on PPO,  see 1, 2, 3, 4, 5.         \non DDPG, see 1, 2.         \non Actor-Critic Methods, and A3C, see 1, 2, 3, 4.           \non TD3, see 1, 2, 3     \non SAC, see 1, 2, 3, 4, 5      \non A2C,  see 1, 2, 3, 4, 5 \nHow does the Bellman equation work in Deep Reinforcement Learning?   \nA pair of interrelated neural networks in Deep Q-Network     \nThree aspects of Deep Reinforcement Learning: noise, overestimation and exploration       \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8252701129634485
      ],
      "excerpt": "Artificial snake on the way \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8354789014027051,
        0.9567588029116127
      ],
      "excerpt": "Four stages of Minitaur training \nChessboard chase with four Pybullet actors \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "32 projects in the framework of Deep Reinforcement Learning algorithms: Q-learning, DQN, PPO, DDPG, TD3, SAC, A2C and others. Each project is provided with a detailed training log.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 93,
      "date": "Sat, 25 Dec 2021 07:21:01 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Rafael1s/Deep-Reinforcement-Learning-Algorithms/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Rafael1s/Deep-Reinforcement-Learning-Algorithms",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/HalfCheetahBulletEnv-TD3/HalfCheetahBulletEnv_TD3_005std.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/HalfCheetahBulletEnv-TD3/WatchAgent_HalfCheetah.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/BipedalWalkerHardcore-TD3-FORK/BipedalWalkerHardcore_TD3-FORK_8681epis.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/BipedalWalkerHardcore-TD3-FORK/BipedalWalkerHardcore_TD3-FORK_6783epis.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/Cartpole-Double-Deep-Q-Learning/CartPole-v0_DoubleDQN_Pytorch_612episodes.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/Cartpole-Double-Deep-Q-Learning/CartPole-v1_DoubleDQN_Pytorch_1030episodes.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/Cartpole-Double-Deep-Q-Learning/WatchAgent-DoubleDQN.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/Cartpole-Double-Deep-Q-Learning/CartPole-v0_DoubleDQN_Pytorch_239episodes.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/Walker2DBulletEnv-v0_SAC/Walker2DBulletEnv-SAC_lr0.0003_epis6934.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/Walker2DBulletEnv-v0_SAC/Watch_SAC_Walker2D_with_Video.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/Project-2_Continuous-Control-Crawler-PPO/Crawler_PPO_676epis_500score.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/Project-2_Continuous-Control-Crawler-PPO/Crawler_PPO_532epis_800score.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/Project-2_Continuous-Control-Crawler-PPO/WatchAgent.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/Project-2_Continuous-Control-Crawler-PPO/Crawler_PPO_678epis_1200score.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/Project-2_Continuous-Control-Crawler-PPO/Crawler_PPO_550epis_650score.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/MountainCar-Q-Learning/MountainCar_QLearning_283600epis.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/CartPole-Policy-Gradient-Reinforce/CartPole_REINFORCE.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/HopperBulletEnv-v0-SAC/HopperBulletEnv-SAC_lr0.0001_epis7662.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/HopperBulletEnv-v0-SAC/HopperBulletEnv-SAC_lr0.0003_epis3814.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/HopperBulletEnv_v0-TD3/HopperBulletEnv_0.02std_5438epis.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/HopperBulletEnv_v0-TD3/HopperBulletEnv_003std_3240epis_U.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/Project-2_Continuous-Control-Reacher-DDPG/REPORT.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/Project-2_Continuous-Control-Reacher-DDPG/WatchAgent.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/Project-2_Continuous-Control-Reacher-DDPG/Continuous_Control.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/Cartpole-Deep-Q-Learning/CartPole-v0_DQN_Pytorch.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/Cartpole-Deep-Q-Learning/WatchAgent-DQN.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/Cartpole-Deep-Q-Learning/CartPole-v1_DQN_Pytorch-II.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/Minitaur-Soft-Actor-Critic/Minitaur-SAC_lr0.0001-b128-1745ep.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/MountainCarContinuous-TD3/MountainCar_TD3_1156epis.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/Markov-Decision-Process_6x6/markov_dec_proc_6x6.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/LunarLanderContinuous-v2-DDPG/WatchAgent-DDPG.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/LunarLanderContinuous-v2-DDPG/LunarLanderContinuous-v2-DDPG_746epis.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/LunarLanderContinuous-v2-DDPG/LunarLanderContinuous-v2-DDPG_2560epis.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/MountainCar-DQN/WatchAgent-MountainCar-DQN.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/MountainCar-DQN/MountainCar_1835epis_DQN.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/LunarLander-v2-DQN/LunarLander-v2-DQN.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/LunarLander-v2-DQN/WatchAgent-DQN.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/BipedalWalker-Soft-Actor-Critic/BipedalWalker-SAC_lr00008_408epis.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/BipedalWalker-Soft-Actor-Critic/BipedalWalker-SAC_lr0001_756epis.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/BipedalWalker-Soft-Actor-Critic/WatchAgent.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/BipedalWalker-Soft-Actor-Critic/BipedalWalker-SAC_lr0005_540epis.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/MinitaurDuck-Soft-Actor-Critic/MinitaurDuck_SAC_h420_lr0.00003_b128.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/MountainCarContinuous_PPO/WatchAgent_MountainCarCont_score90.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/MountainCarContinuous_PPO/MountainCarContinuous_PPO_VecEnv-16proc_21epis.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/BipedalWalker-TwinDelayed-DDPG%20%28TD3%29/WatchAgent.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/BipedalWalker-TwinDelayed-DDPG%20%28TD3%29/BipedalWalker_1795ep_300-5sc_9h44m.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/BipedalWalker-PPO-VectorizedEnv/BipedalWalker_PPO_VecEnv_450epis.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/Project-1_Navigation-DQN/REPORT.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/Project-1_Navigation-DQN/Navigation.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/Walker2DBulletEnv-v0_TD3/WatchAgent_Walker2D.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/Walker2DBulletEnv-v0_TD3/Walker2DBulletEnv_std0.02.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/Project-3_Collaboration_Competition-Tennis-Maddpg/REPORT.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/Project-3_Collaboration_Competition-Tennis-Maddpg/WatchAgent.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/Project-3_Collaboration_Competition-Tennis-Maddpg/Tennis.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/Pong-Policy-Gradient-PPO/pong-PPO_800epis.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/Pong-Policy-Gradient-PPO/WatchAgent.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/BipedalWalker-A2C-VectorizedEnv/BipedalWalker_A2C_VecEnv_RMSprop.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/BipedalWalker-A2C-VectorizedEnv/BipedalWalker_A2C_VecEnv_Adam.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/CarRacing-From-Pixels-PPO/WatchAgent.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/CarRacing-From-Pixels-PPO/CarRacing_PPO-2760epis_902score.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/Pong-Policy-Gradient-REINFORCE/WatchAgent.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/Pong-Policy-Gradient-REINFORCE/pong-REINFORCE_2300ep.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/CartPole-Policy-Based-Hill-Climbing/CartPole-v1_Hill_Climbing.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/CartPole-Policy-Based-Hill-Climbing/CartPole-v0_Hill_Climbing.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/CartPole-Policy-Based-Hill-Climbing/CartPole_Hill_Climbing.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/Ant-PyBulletEnv-Soft-Actor-Critic/AntPyBulletEnv-SAC_lr0001-sc2500.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/Snake-Pygame-DQN/WatchAgent.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/Snake-Pygame-DQN/WatchAgent-with-Video.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/Snake-Pygame-DQN/Snake-DQN_lr0.00001.ipynb",
      "https://raw.githubusercontent.com/Rafael1s/Deep-Reinforcement-Learning-Udacity/master/Snake-Pygame-DQN/Snake-DQN_lr0.0001.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8288330512633938
      ],
      "excerpt": "BipedalWalker, PPO, Vectorized Environment \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8288330512633938
      ],
      "excerpt": "BipedalWalker, A2C, Vectorized Environment \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8288330512633938
      ],
      "excerpt": "MountainCarContinuous, PPO, Vectorized Environment    \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8288330512633938
      ],
      "excerpt": " BipedalWalker, PPO, Vectorized Environment  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8288330512633938
      ],
      "excerpt": "* BipedalWalker, A2C, Vectorized Environment \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8271029241609154
      ],
      "excerpt": "a jupyter notebook containing training log.   \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Rafael1s/Deep-Reinforcement-Learning-Algorithms/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Deep Reinforcement Learning Algorithms",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Deep-Reinforcement-Learning-Algorithms",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Rafael1s",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Rafael1s/Deep-Reinforcement-Learning-Algorithms/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 298,
      "date": "Sat, 25 Dec 2021 07:21:01 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "deep-rl-algorithms",
      "github-udacity",
      "dqn-ppo-ddpg",
      "dqn",
      "td3",
      "cartpole",
      "bipedalwalker",
      "deep-reinforcement-learning",
      "sac",
      "carracing",
      "hopperbulletenv",
      "lunarlander",
      "ddpg",
      "ppo",
      "a2c",
      "antbulletenv",
      "soft-actor-critic",
      "halfcheetahbulletenv",
      "walker2dbulletenv"
    ],
    "technique": "GitHub API"
  }
}