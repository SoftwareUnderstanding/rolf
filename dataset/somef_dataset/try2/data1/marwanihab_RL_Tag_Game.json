{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1703.04908"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Environments in this repo:\n<pre>\n@article{lowe2017multi,\n  title={Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments},\n  author={Lowe, Ryan and Wu, Yi and Tamar, Aviv and Harb, Jean and Abbeel, Pieter and Mordatch, Igor},\n  journal={Neural Information Processing Systems (NIPS)},\n  year={2017}\n}\n</pre>\n\nOriginal particle world environment:\n<pre>\n@article{mordatch2017emergence,\n  title={Emergence of Grounded Compositional Language in Multi-Agent Populations},\n  author={Mordatch, Igor and Abbeel, Pieter},\n  journal={arXiv preprint arXiv:1703.04908},\n  year={2017}\n}\n</pre>\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "In order to make this I have used a simple multi-agent particle world with a continuous observation and discrete action space, along with some basic simulated physics and the MADDPG \nalgorithm mentioned in the paper [Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments](https://arxiv.org/pdf/1706.02275.pdf), \nI forked  [Multi-Agent Particle Environment](https://github.com/openai/multiagent-particle-envs) and used it.\n\n#Modifications:\nI have tweaked some changes in the original environments regarding the agents numbers and speed and accelerations also, \nI have added a done callback to terminate the episode once the escapper is tagged also when escapper got out of bounds but this will be commented \nin the code so episode will terminate only if the agent got tagged.\n\nAll the code is implemented using pytorch and I have tried new architecture for the neural networks that \nalso gave a good results.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{mordatch2017emergence,\n  title={Emergence of Grounded Compositional Language in Multi-Agent Populations},\n  author={Mordatch, Igor and Abbeel, Pieter},\n  journal={arXiv preprint arXiv:1703.04908},\n  year={2017}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{lowe2017multi,\n  title={Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments},\n  author={Lowe, Ryan and Wu, Yi and Tamar, Aviv and Harb, Jean and Abbeel, Pieter and Mordatch, Igor},\n  journal={Neural Information Processing Systems (NIPS)},\n  year={2017}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/marwanihab/RL_TAG_GAME",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-04-02T22:00:56Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-04-03T02:49:30Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9537654081989773
      ],
      "excerpt": "A game of tag with 2 autonomous agents in a 2D environment. The game generation begins with a \"tagger\" agent whose goal is to tag the other agent, and a second \"escaper\" agent whose goal is to escape this tagger. The generation ends after either the tagger agent touches the escaper (in which case the tagger wins), or after a fixed number of timesteps elapsed (in which case the escaper wins).  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8985991911119664
      ],
      "excerpt": "./multiagent/core.py: contains classes for various objects (Entities, Landmarks, Agents, etc.) that are used throughout the code. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9186136692352519,
        0.9008513089457102
      ],
      "excerpt": "./multiagent/policy.py: contains code for interactive policy based on keyboard input. \n./multiagent/scenario.py: contains base scenario object that is extended for all scenarios. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8528618469733519
      ],
      "excerpt": "    1) make_world(): creates all of the entities that inhabit the world (landmarks, agents, etc.), assigns their capabilities (whether they can communicate, or move, or both). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8025689979366682
      ],
      "excerpt": "    2) reset_world(): resets the world by assigning properties (position, color, etc.) to all entities in the world \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9604236543201415
      ],
      "excerpt": "    4) observation(): defines the observation space of a given agent \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.911029958506795,
        0.8583931897814655
      ],
      "excerpt": "agent.py: contains the code for the MADDPG algorithm and the agent functions. \nactor_critic_model.py: contains the code for the critic and the actor neural network architecture. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Reinforcemenet Learning game of tag with 2 autonomous agents in a 2D environment. The game generation begins with a \"tagger\" agent whose goal is to tag the other agent, and a second \"escaper\" agent whose goal is to escape this tagger. The generation ends after either the tagger agent touches the escaper (in which case the tagger wins), or after a fixed number of timesteps elapsed (in which case the escaper wins).  The tagger and escaper do not ever swap roles; each agent retains his goal across generations.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/marwanihab/RL_Tag_Game/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Mon, 27 Dec 2021 15:57:06 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/marwanihab/RL_TAG_GAME/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "marwanihab/RL_TAG_GAME",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/marwanihab/RL_Tag_Game/master/finalTaining.ipynb"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.8000106800794896
      ],
      "excerpt": "replay_buffer.py: contains code for the buffer needed in training. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/marwanihab/RL_TAG_GAME/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "RL TAG GAME",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "RL_TAG_GAME",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "marwanihab",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/marwanihab/RL_TAG_GAME/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Mon, 27 Dec 2021 15:57:06 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "python3",
      "reinforcement-learning",
      "reinforcement-learning-algorithms",
      "machine-learning",
      "pytorch",
      "pytorch-implementation",
      "deep-learning",
      "openai-gym"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- git clone \n\n- To install, `cd` into the root directory and type `pip install -e`.\n\n- To interactively view moving to landmark scenario (see others in ./scenarios/):\n`bin/interactive.py --scenario simple.py`\n\n- Known dependencies: Python (3.5.4), OpenAI gym (0.10.5), numpy (1.14.5), pytorch (1.4.0)\n\n- To start the training cd to the main directory:\n   `python main.py`\n   \n- To display the existing models weights and see how they perform, just provide the display flag and if you want you can choose the number of the tagger agents using `--num_adversaries=3` for example :\n    `python main.py --display --num_adversaries=3`\n- By default the number of taggers is 1 and its policy is DDPG because as mentioned in the paper, \ntraining the \"taggers\" with DDPG and the \"escaper\" with MADDPG provided better results.\n- Check `train.py` to see more options for the flags and their corresponding descriptions.\n\n",
      "technique": "Header extraction"
    }
  ]
}