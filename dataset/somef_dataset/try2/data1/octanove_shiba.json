{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2103.06874",
      "https://arxiv.org/abs/1907.11692",
      "https://arxiv.org/abs/2104.07705"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "<a id=\"1\">[1]</a> Jonathan H. Clark and Dan Garrette and Iulia Turc and John Wieting (2021). [CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation](https://arxiv.org/abs/2103.06874). CoRR, abs/2103.06874.\n\n<a id=\"2\">[2]</a> Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov (2019). [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692). CoRR, abs/1907.11692.\n\n<a id=\"3\">[3]</a>\nPeter Izsak and Moshe Berchansky and Omer Levy (2021). [How to Train BERT with an Academic Budget](https://arxiv.org/abs/2104.07705). CoRR, abs/2104.07705.\n\n\n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "There is no paper associated with SHIBA, but the repository can be cited like this:\n\n```bibtex\n@misc{shiba,\n  author = {Joshua Tanner and Masato Hagiwara},\n  title = {SHIBA: Japanese CANINE model},\n  year = {2021},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/octanove/shiba}},\n}\n```\n\nPlease also cite the original CANINE paper:\n```bibtex\n@misc{clark2021canine,\n      title={CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation}, \n      author={Jonathan H. Clark and Dan Garrette and Iulia Turc and John Wieting},\n      year={2021},\n      eprint={2103.06874},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{clark2021canine,\n      title={CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation}, \n      author={Jonathan H. Clark and Dan Garrette and Iulia Turc and John Wieting},\n      year={2021},\n      eprint={2103.06874},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{shiba,\n  author = {Joshua Tanner and Masato Hagiwara},\n  title = {SHIBA: Japanese CANINE model},\n  year = {2021},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/octanove/shiba}},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9902322248106731
      ],
      "excerpt": "If you would like to reproduce our training, please see TRAINING.md. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/octanove/shiba",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-04-16T05:19:43Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-03T19:10:05Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9519047325418695
      ],
      "excerpt": "SHIBA is an approximate reimplementation of CANINE[1] in raw Pytorch, pretrained on the Japanese wikipedia corpus using random span masking. If you are unfamiliar with CANINE, you can think of it as a very efficient (approximately 4x as efficient) character-level BERT model. Of course, the name SHIBA comes from the identically named Japanese canine.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9855927782001588
      ],
      "excerpt": "The biggest advantages SHIBA provides are in terms of utility, because (like CANINE) it: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.902007711617461,
        0.9228850158015938,
        0.9316369735206166,
        0.8169815468012596
      ],
      "excerpt": "2. Efficiently handles a lot of characters. Compared to caracter-level BERT models, you can get 4x as many character embeddings in one pass (2048) for a comparable amount of compute. \nThat said, we compared SHIBA against baselines on two downstream tasks, and it also performs pretty well. \nThe first task was classification on the Livedoor News Corpus, using as much of the article as we text as we could fit into the model in one pass.  \n| Model | Accuracy  | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "| Model | F1  | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9248724069805224,
        0.9606279285360928,
        0.9853531563378296,
        0.8444262519645226
      ],
      "excerpt": "It turns out it's pretty difficult to beat MeCab on UD word segmentation, but we expect SHIBA to be useful for segmenting messy text where tools like MeCab which use dictionaries struggle. \nA techinical blog post about SHIBA will be available shortly, but below are some of the important details. \nThe architecture of the SHIBA model is very similar to that of CANINE. However, there are a few differences of note. \nSHIBA uses windowed local attention, not the blockwise local attention used by CANINE.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9711544894703728,
        0.8966720243933309,
        0.9287341235806307
      ],
      "excerpt": "There are very minor differences in how the downsampling of charater embeddings is done. The most important is that SHIBA will not truncate the last character of max-length sequences like CANINE will. \nThe model code can be found here, and the tokenizer code can be found here. The model code was written to be relatively easy to understand and to change, so if you are curious about how the model works, the fastest way be reading and changing the code yourself. \nWe trained on the Japanese Wikipedia corpus, using mostly identical preprocessing to the Tohoku University Japanese Bert model. Training example creation was done similarly to RoBERTa[2], packing as many sentences as could fit into each training example. Our masking strategy was random span masking, whereby we perform dynamic random masking on contiguous spans of characters. Where [M] is a unicode codepoint representing a mask character, an example masking transformation might look like the below. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8533264785674893
      ],
      "excerpt": "Random replacements are selected from a pretrained BPE vocabulary, such that the replacement for a span of length two would be a two-character BPE token. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9940532593660887,
        0.9865588966461061,
        0.9628768146824288
      ],
      "excerpt": "Our hyperparameters (including the masking type) were chosen based their performance on much smaller subsets of the data, and on the hyperparameters used in training similar transformer encoders: specifically those of RoBERTa[2] and Academic Budget BERT[3]. \nThis repository also includes the code used for actually training the SHIBA model (which is not in the shiba-model package). This code has significantly more dependencies than just the model and is not as polished, but if you are considering training a SHIBA/CANINE model it may be of interest to you. In particular, there are implementations of BPE masking and random span masking in the masking.py file. \nThe default model is the encoder model that performs best on downstream tasks, but we provide a few other checkpoints.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Pytorch implementation and pre-trained Japanese model for CANINE, the efficient character-level transformer.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/octanove/shiba/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 6,
      "date": "Tue, 28 Dec 2021 11:23:11 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/octanove/shiba/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "octanove/shiba",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/octanove/shiba/main/training/data/preprocess.sh",
      "https://raw.githubusercontent.com/octanove/shiba/main/training/data/install_mecab.sh",
      "https://raw.githubusercontent.com/octanove/shiba/main/training/data/livedoor_news/get_data.sh"
    ],
    "technique": "File Exploration"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/octanove/shiba/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'Apache License\\n==============\\n\\n_Version 2.0, January 2004_  \\n_&lt;http://www.apache.org/licenses/&gt;\\n\\n### Terms and Conditions for use, reproduction, and distribution\\n\\n#### 1. Definitions\\n\\n\\xe2\\x80\\x9cLicense\\xe2\\x80\\x9d shall mean the terms and conditions for use, reproduction, and\\ndistribution as defined by Sections 1 through 9 of this document.\\n\\n\\xe2\\x80\\x9cLicensor\\xe2\\x80\\x9d shall mean the copyright owner or entity authorized by the copyright\\nowner that is granting the License.\\n\\n\\xe2\\x80\\x9cLegal Entity\\xe2\\x80\\x9d shall mean the union of the acting entity and all other entities\\nthat control, are controlled by, or are under common control with that entity.\\nFor the purposes of this definition, \\xe2\\x80\\x9ccontrol\\xe2\\x80\\x9d means (i) the power, direct or\\nindirect, to cause the direction or management of such entity, whether by\\ncontract or otherwise, or (ii) ownership of fifty percent (50%) or more of the\\noutstanding shares, or (iii) beneficial ownership of such entity.\\n\\n\\xe2\\x80\\x9cYou\\xe2\\x80\\x9d (or \\xe2\\x80\\x9cYour\\xe2\\x80\\x9d) shall mean an individual or Legal Entity exercising\\npermissions granted by this License.\\n\\n\\xe2\\x80\\x9cSource\\xe2\\x80\\x9d form shall mean the preferred form for making modifications, including\\nbut not limited to software source code, documentation source, and configuration\\nfiles.\\n\\n\\xe2\\x80\\x9cObject\\xe2\\x80\\x9d form shall mean any form resulting from mechanical transformation or\\ntranslation of a Source form, including but not limited to compiled object code,\\ngenerated documentation, and conversions to other media types.\\n\\n\\xe2\\x80\\x9cWork\\xe2\\x80\\x9d shall mean the work of authorship, whether in Source or Object form, made\\navailable under the License, as indicated by a copyright notice that is included\\nin or attached to the work (an example is provided in the Appendix below).\\n\\n\\xe2\\x80\\x9cDerivative Works\\xe2\\x80\\x9d shall mean any work, whether in Source or Object form, that\\nis based on (or derived from) the Work and for which the editorial revisions,\\nannotations, elaborations, or other modifications represent, as a whole, an\\noriginal work of authorship. For the purposes of this License, Derivative Works\\nshall not include works that remain separable from, or merely link (or bind by\\nname) to the interfaces of, the Work and Derivative Works thereof.\\n\\n\\xe2\\x80\\x9cContribution\\xe2\\x80\\x9d shall mean any work of authorship, including the original version\\nof the Work and any modifications or additions to that Work or Derivative Works\\nthereof, that is intentionally submitted to Licensor for inclusion in the Work\\nby the copyright owner or by an individual or Legal Entity authorized to submit\\non behalf of the copyright owner. For the purposes of this definition,\\n\\xe2\\x80\\x9csubmitted\\xe2\\x80\\x9d means any form of electronic, verbal, or written communication sent\\nto the Licensor or its representatives, including but not limited to\\ncommunication on electronic mailing lists, source code control systems, and\\nissue tracking systems that are managed by, or on behalf of, the Licensor for\\nthe purpose of discussing and improving the Work, but excluding communication\\nthat is conspicuously marked or otherwise designated in writing by the copyright\\nowner as \\xe2\\x80\\x9cNot a Contribution.\\xe2\\x80\\x9d\\n\\n\\xe2\\x80\\x9cContributor\\xe2\\x80\\x9d shall mean Licensor and any individual or Legal Entity on behalf\\nof whom a Contribution has been received by Licensor and subsequently\\nincorporated within the Work.\\n\\n#### 2. Grant of Copyright License\\n\\nSubject to the terms and conditions of this License, each Contributor hereby\\ngrants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free,\\nirrevocable copyright license to reproduce, prepare Derivative Works of,\\npublicly display, publicly perform, sublicense, and distribute the Work and such\\nDerivative Works in Source or Object form.\\n\\n#### 3. Grant of Patent License\\n\\nSubject to the terms and conditions of this License, each Contributor hereby\\ngrants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free,\\nirrevocable (except as stated in this section) patent license to make, have\\nmade, use, offer to sell, sell, import, and otherwise transfer the Work, where\\nsuch license applies only to those patent claims licensable by such Contributor\\nthat are necessarily infringed by their Contribution(s) alone or by combination\\nof their Contribution(s) with the Work to which such Contribution(s) was\\nsubmitted. If You institute patent litigation against any entity (including a\\ncross-claim or counterclaim in a lawsuit) alleging that the Work or a\\nContribution incorporated within the Work constitutes direct or contributory\\npatent infringement, then any patent licenses granted to You under this License\\nfor that Work shall terminate as of the date such litigation is filed.\\n\\n#### 4. Redistribution\\n\\nYou may reproduce and distribute copies of the Work or Derivative Works thereof\\nin any medium, with or without modifications, and in Source or Object form,\\nprovided that You meet the following conditions:\\n\\n (a) You must give any other recipients of the Work or Derivative Works a copy of\\nthis License; and\\n (b) You must cause any modified files to carry prominent notices stating that You\\nchanged the files; and\\n (c) You must retain, in the Source form of any Derivative Works that You distribute,\\nall copyright, patent, trademark, and attribution notices from the Source form\\nof the Work, excluding those notices that do not pertain to any part of the\\nDerivative Works; and\\n (d) If the Work includes a \\xe2\\x80\\x9cNOTICE\\xe2\\x80\\x9d text file as part of its distribution, then any\\nDerivative Works that You distribute must include a readable copy of the\\nattribution notices contained within such NOTICE file, excluding those notices\\nthat do not pertain to any part of the Derivative Works, in at least one of the\\nfollowing places: within a NOTICE text file distributed as part of the\\nDerivative Works; within the Source form or documentation, if provided along\\nwith the Derivative Works; or, within a display generated by the Derivative\\nWorks, if and wherever such third-party notices normally appear. The contents of\\nthe NOTICE file are for informational purposes only and do not modify the\\nLicense. You may add Your own attribution notices within Derivative Works that\\nYou distribute, alongside or as an addendum to the NOTICE text from the Work,\\nprovided that such additional attribution notices cannot be construed as\\nmodifying the License.\\n\\nYou may add Your own copyright statement to Your modifications and may provide\\nadditional or different license terms and conditions for use, reproduction, or\\ndistribution of Your modifications, or for any such Derivative Works as a whole,\\nprovided Your use, reproduction, and distribution of the Work otherwise complies\\nwith the conditions stated in this License.\\n\\n#### 5. Submission of Contributions\\n\\nUnless You explicitly state otherwise, any Contribution intentionally submitted\\nfor inclusion in the Work by You to the Licensor shall be under the terms and\\nconditions of this License, without any additional terms or conditions.\\nNotwithstanding the above, nothing herein shall supersede or modify the terms of\\nany separate license agreement you may have executed with Licensor regarding\\nsuch Contributions.\\n\\n#### 6. Trademarks\\n\\nThis License does not grant permission to use the trade names, trademarks,\\nservice marks, or product names of the Licensor, except as required for\\nreasonable and customary use in describing the origin of the Work and\\nreproducing the content of the NOTICE file.\\n\\n#### 7. Disclaimer of Warranty\\n\\nUnless required by applicable law or agreed to in writing, Licensor provides the\\nWork (and each Contributor provides its Contributions) on an \\xe2\\x80\\x9cAS IS\\xe2\\x80\\x9d BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied,\\nincluding, without limitation, any warranties or conditions of TITLE,\\nNON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are\\nsolely responsible for determining the appropriateness of using or\\nredistributing the Work and assume any risks associated with Your exercise of\\npermissions under this License.\\n\\n#### 8. Limitation of Liability\\n\\nIn no event and under no legal theory, whether in tort (including negligence),\\ncontract, or otherwise, unless required by applicable law (such as deliberate\\nand grossly negligent acts) or agreed to in writing, shall any Contributor be\\nliable to You for damages, including any direct, indirect, special, incidental,\\nor consequential damages of any character arising as a result of this License or\\nout of the use or inability to use the Work (including but not limited to\\ndamages for loss of goodwill, work stoppage, computer failure or malfunction, or\\nany and all other commercial damages or losses), even if such Contributor has\\nbeen advised of the possibility of such damages.\\n\\n#### 9. Accepting Warranty or Additional Liability\\n\\nWhile redistributing the Work or Derivative Works thereof, You may choose to\\noffer, and charge a fee for, acceptance of support, warranty, indemnity, or\\nother liability obligations and/or rights consistent with this License. However,\\nin accepting such obligations, You may act only on Your own behalf and on Your\\nsole responsibility, not on behalf of any other Contributor, and only if You\\nagree to indemnify, defend, and hold each Contributor harmless for any liability\\nincurred by, or claims asserted against, such Contributor by reason of your\\naccepting any such warranty or additional liability.\\n\\n_END OF TERMS AND CONDITIONS\\n\\n### APPENDIX: How to apply the Apache License to your work\\n\\nTo apply the Apache License to your work, attach the following boilerplate\\nnotice, with the fields enclosed by brackets [] replaced with your own\\nidentifying information. (Don\\'t include the brackets!) The text should be\\nenclosed in the appropriate comment syntax for the file format. We also\\nrecommend that a file or class name and description of purpose be included on\\nthe same \\xe2\\x80\\x9cprinted page\\xe2\\x80\\x9d as the copyright notice for easier identification within\\nthird-party archives.\\n\\n    Copyright 2021 Octanove Labs\\n    \\n    Licensed under the Apache License, Version 2.0 (the \"License\");\\n    you may not use this file except in compliance with the License.\\n    You may obtain a copy of the License at\\n    \\n      http://www.apache.org/licenses/LICENSE-2.0\\n    \\n    Unless required by applicable law or agreed to in writing, software\\n    distributed under the License is distributed on an \"AS IS\" BASIS,\\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n    See the License for the specific language governing permissions and\\n    limitations under the License.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "What is SHIBA?",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "shiba",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "octanove",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/octanove/shiba/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 60,
      "date": "Tue, 28 Dec 2021 11:23:11 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "natural-language-processing",
      "deep-learning",
      "neural-network"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you just want to use the SHIBA model, you can install it like this:\n> pip install shiba-model\n\n\nFor an example of how to load and use the pretrained model, see below. `get_pretrained_state_dict()` will automatically download the pretrained model for you, but if you'd like to do it yourself the model can be downloaded from [here](https://storage.googleapis.com/shiba.octanove.com/published_checkpoints/shiba_check45k.pt).\n\n```python\nfrom shiba import Shiba, CodepointTokenizer, get_pretrained_state_dict\nshiba_model = Shiba()\nshiba_model.load_state_dict(get_pretrained_state_dict())\nshiba_model.eval() #: disable dropout\ntokenizer = CodepointTokenizer()\n\ninputs = tokenizer.encode_batch(['\u81ea\u7136\u8a00\u8a9e\u51e6\u7406', '\u67f4\u30c9\u30ea\u30eb'])\noutputs = shiba_model(**inputs)\n```\n\nSHIBA can then be fine-tuned for classification or character-level tasks just like any other transformer encoder. Adding task-specific layers should be relatively easy, but premade models for classification and sequence labeling are also included. These are `ShibaForClassification` and `ShibaForSequenceLabeling`, respectively. \n\n```python\nfrom shiba import ShibaForClassification\ncls_model = ShibaForClassification(vocab_size=3)\ncls_model.load_encoder_checkpoint()\n```\n\n`load_encoder_checkpoint()` loads just the pretrained encoder checkpoint, but running `cls_model.shiba_model.load_state_dict(get_pretrained_state_dict())` will give you the same result. \n\nIf your task is simple and you just need an efficient character-level model, you can also just train a SHIBA model from scratch.\n\n",
      "technique": "Header extraction"
    }
  ]
}