{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1901.03353",
      "https://arxiv.org/abs/1904.01355"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please consider citing this project in your publications if it helps your research. The following is a BibTeX reference. The BibTeX entry requires the `url` LaTeX package.\n```\n@misc{massa2018mrcnn,\nauthor = {Massa, Francisco and Girshick, Ross},\ntitle = {{maskrcnn-benchmark: Fast, modular reference implementation of Instance Segmentation and Object Detection algorithms in PyTorch}},\nyear = {2018},\nhowpublished = {\\url{https://github.com/facebookresearch/maskrcnn-benchmark}},\nnote = {Accessed: [Insert date here]}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{massa2018mrcnn,\nauthor = {Massa, Francisco and Girshick, Ross},\ntitle = {{maskrcnn-benchmark: Fast, modular reference implementation of Instance Segmentation and Object Detection algorithms in PyTorch}},\nyear = {2018},\nhowpublished = {\\url{https://github.com/facebookresearch/maskrcnn-benchmark}},\nnote = {Accessed: [Insert date here]}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9200020374065359
      ],
      "excerpt": "Link of presentation video: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8149778486646536
      ],
      "excerpt": "You shall only consider the main branch. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "no_pred : 10 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.842790493796475
      ],
      "excerpt": "A sample output video \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8090016440670298
      ],
      "excerpt": "class MyDataset(object): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8140608737051792
      ],
      "excerpt": "        #: as you would do normally \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9783595426586629
      ],
      "excerpt": "    boxes = [[0, 0, 10, 10], [10, 20, 50, 50]] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8550101043698384
      ],
      "excerpt": "    labels = torch.tensor([10, 20]) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9878217113125949,
        0.9218387569487573
      ],
      "excerpt": "  Cheng-Yang Fu, Mykhailo Shvets, and Alexander C. Berg. \n  Tech report, arXiv,1901.03353. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeOfConduct": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://raw.githubusercontent.com/PeterTKovacs/zold137/main/CODE_OF_CONDUCT.md",
    "technique": "File Exploration"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/PeterTKovacs/zold137",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contributing to Mask-RCNN Benchmark\nWe want to make contributing to this project as easy and transparent as\npossible.\nOur Development Process\nMinor changes and improvements will be released on an ongoing basis. Larger changes (e.g., changesets implementing a new paper) will be released on a more periodic basis.\nPull Requests\nWe actively welcome your pull requests.\n\nFork the repo and create your branch from master.\nIf you've added code that should be tested, add tests.\nIf you've changed APIs, update the documentation.\nEnsure the test suite passes.\nMake sure your code lints.\nIf you haven't already, complete the Contributor License Agreement (\"CLA\").\n\nContributor License Agreement (\"CLA\")\nIn order to accept your pull request, we need you to submit a CLA. You only need\nto do this once to work on any of Facebook's open source projects.\nComplete your CLA here: https://code.facebook.com/cla\nIssues\nWe use GitHub issues to track public bugs. Please ensure your description is\nclear and has sufficient instructions to be able to reproduce the issue.\nFacebook has a bounty program for the safe\ndisclosure of security bugs. In those cases, please go through the process\noutlined on that page and do not file a public issue.\nCoding Style\n\n4 spaces for indentation rather than tabs\n80 character line length\nPEP8 formatting following Black\n\nLicense\nBy contributing to Mask-RCNN Benchmark, you agree that your contributions will be licensed\nunder the LICENSE file in the root directory of this source tree.",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-10-05T19:27:57Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-01-07T10:33:53Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9841881305912181
      ],
      "excerpt": "Project of team 'zold137' in course BMWVITMAV45. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8028952661337226,
        0.9700098383268188,
        0.9538465767153912
      ],
      "excerpt": "Note: unfortunately, Soma and \u00c1d\u00e1m left the team before submission. They contributed equally in milestone 1. In milestone 2, their most important contribution was the script fetching images from Youtube. (That had to be seriously corrected in some sense much later by me - see summarizing report). After milestone 2, they abandoned me.  \nI believe that they did not give much meaningful contribution to the project at all. \nThis readme is the most important source of information about what pieces of code and when to run. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9201883178890129,
        0.8425018398134815,
        0.9192660788845417,
        0.851605355036927,
        0.8940159756713122,
        0.997624215345634
      ],
      "excerpt": "As a rule of thumb, scripts you may want to run (besides data manipulation) are in drone_demo. There is the drone_demo/summary.pdf report too, in PDF and the TeX source is attached too. (I am super sorry to realise at 23:54 13.12.2020 that the name of the author is the dummy one given by the NeurIPS2020 template, that I don't have time to correct) \nThe below introduction gives a solid idea of how to install the environment we used and how to train and infer with my code snippets. Using callbacks will be introduced too. \nOur project aims to detect various objects in drone-captured images. \nWe would like to solve this problem as a supervised task, so acquired two datasets containing pictures taken by drones. ('VisDrone' and 'MultiDrone'.) \nBoth sets come with annotations, what means coordinates for bounding boxes and the ground-truth classes of annotated objects. \nOur work is largely based on the following repository: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9975455393160232
      ],
      "excerpt": "this is in fact a fork of  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9805307091684974,
        0.9801231936017821,
        0.925116872163146
      ],
      "excerpt": "Unfortunately, we had to implement many changes in the orignal code and it has become the easiest way to do so to upload the code instead of forking. (We have found the appropriate arrangement in a Docker-container and from that position, this was the easiest way to upload code.) \nNevertheless, we are grateful for the creators and maintainers of both repositories. \nLoading our custom data is implemented in datasets/data_gen.py. For the installation procedure, only one video ('giro1') is processed. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9422271139438478,
        0.8307335920524824,
        0.8468446851832166,
        0.9367965705433986
      ],
      "excerpt": "however, due to the dataset management of the current implementation, you won't be able to merge the data of the videos, but this can be done easily when doing training: just add the desired datasets into the config file's (eg. drone_demo/d_e2e_faster_rcnn_X_101_32x8d_FPN_1x_visdrone.yaml appropriate row \nvalidation set has to be given in the drone_demo/custom_dict.txt, see later the details \nVery importantly: you will have to clean data: remove frames with unidentified ('-') class on them (they will cause error) and removing images that are not aerial images. For these purposes, helper scripts are included datasets/giro_data/images/rmdash.py and datasets/filter.py respectively, but before using them parse through the codeor even rewrite it. Also, you won't be able to get away with removing ground images without watching the videos! (Behold that they are 25fps, urls in datasets/data_gen.py. \nThis whole procedure is performed by the drone_demo/custom_train.py. The main idea is to pass arguments in two ways: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9310811696157707
      ],
      "excerpt": "Here, the config file is basically the DNA of your model, containing all relevant info to build it \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.899730705102326
      ],
      "excerpt": "custom_dict contains additional information on the model building and training \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8895222461458746
      ],
      "excerpt": "to_unfreeze : roi, rpn \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.902556214886328
      ],
      "excerpt": "to_unfreeze : named patrameters of the model containing these keywords will be unfrozen \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.930284489982906,
        0.8018004244518369
      ],
      "excerpt": "val_frequency: the number of batches after which validation is performed \nbest_name name under which to save best accuracy model, into drone_demo \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9150544558493271,
        0.8931920040289364
      ],
      "excerpt": "Your first training:  obviously, when just starting, you won't have anything to reload, just delete the respective row. I is also advised to train first with roiheads.box.predictor_ unfrozen only then these weights are 'factory new', in the part of the model that is newly added. \nIt is safe to set LR in the config to be BASE_LR: 4e-05 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8407369375283031
      ],
      "excerpt": "When you specify the testset(s) too, testing is automatically done for the metrics introduced in the report. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9810592055637906
      ],
      "excerpt": "What you definitely will have to do: before every training, delete 'last_checkpoint' directory that is produced by the checkpointer. For some inexplainable reason, loading weights is in default setting done by the checkpointer and this is the way to circumvent the problems it causes. (For more details, refer to drone_demo/custom_train.py)  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8025040340314196,
        0.963435293014375,
        0.806614676152854
      ],
      "excerpt": "To see what type of output you shall see while training, refer to drone_demo/log.txt \nThe aim is to do inference on single images and then save the results as JPG (see report).  \nThe script drone_demo/draw_inference.py is is ran as follows: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8943377298833224,
        0.8745046694592769
      ],
      "excerpt": "reload: what model to use to inference \nout_path: save outcome pics with boxes here (ground truth with red) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9672535404730784,
        0.8649482797138763,
        0.8793110443412158,
        0.9713647200773524
      ],
      "excerpt": "annfile : file with annotations of the above pictures \nno_pred: how many predictions to try to make (at most, maybe give error in there are not sufficiently enough. The exact way the pics are chosen from the in directory is specified in the code \nThis code basically draws ground-truth boxes and predictions above a given confidency threshold. I am not sure if loading by cv2 instead of the usual PIL does not cause some problems that I didn't have time to investigate. \nThus, we mean to do transfer-learning on that model, because we only have access to limited GPU resorces. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9777246171253098
      ],
      "excerpt": "2. RPN - Region Proposal Network for the domains of interest on the picture \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9967744832546018,
        0.9511164717641171,
        0.9897091534823275,
        0.9252799949876707
      ],
      "excerpt": "In our (our predecessors') code, the model is generated from a config file with the machinery of the maskrcnn-benchmark repository. The model weigths for the particular model we want to make transfer-learn are loaded. Details previously mentioned. \nObviously, the original model did not use the same classes as we did. The reason why the sample training works is because the class IDs of our data form a subset of the original (at least I guess so). \nThis means that we will have to rebuild the last stages and transfer-learn them. This is not implemented yet, due to lack of time. (Setting up the enviromnet was a nightmare, with many modifications in the original Dockerfile. Moreover, the codebase is not too-well docmented so I had to explore the whole stuff. Finally the resoultion of this annotation business was a real pain-in-the-neck too.) \nThis directory contains our code and documentation for data acquisition and basic processing. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9571487502941054,
        0.8063084186458337,
        0.9846445813396593
      ],
      "excerpt": "description_of_datasets is our writeup about the data with a self-explaining name. \nIn addition to this, we provide basic visualization in Jupyter Notebooks 'Saving_frames_and_drawing_boxes.ipynb' and 'Data_inspection.ipynb' for datasets MultiDrone and VisDrone respectively. \nThe main purpose of these notebooks is to introduce the datasets and give some impression about them. As both sets consist of numerous pictures, they are not uploadaded to GitHub.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8418338058717837
      ],
      "excerpt": "This directory is not part of the 1<sup>st</sup> milestone. We started to experiment with renowned CNNs if they correctly classify objects cropped from the ground-truth bounding boxes.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9759954991374692
      ],
      "excerpt": "This is a fork of maskrcnn-benchmark repository that is focused on detecting objects from drone view images. The model checkpoint is trained on the VisDrone dataset.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9409538965081561
      ],
      "excerpt": "This project aims at providing the necessary building blocks for easily \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9860194369638717,
        0.9111405623254083
      ],
      "excerpt": "PyTorch 1.0: RPN, Faster R-CNN and Mask R-CNN implementations that matches or exceeds Detectron accuracies \nVery fast: up to 2x faster than Detectron and 30% faster than mmdetection during training. See MODEL_ZOO.md for more details. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9410651605279441,
        0.9769563761413725
      ],
      "excerpt": "CPU support for inference: runs on CPU in inference time. See our webcam demo for an example \nProvides pre-trained models for almost all reference Mask R-CNN and Faster R-CNN configurations with 1x schedule. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8679584484468544,
        0.8008429079547844
      ],
      "excerpt": "For that, all you need to do is to modify maskrcnn_benchmark/config/paths_catalog.py to \npoint to the location where your dataset is stored. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9290588165016882
      ],
      "excerpt": "Most of the configuration files that we provide assume that we are running on 8 GPUs. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9068075750396347,
        0.9515987478979276,
        0.9150702841864603
      ],
      "excerpt": "This should work out of the box and is very similar to what we should do for multi-GPU training. \nBut the drawback is that it will use much more GPU memory. The reason is that we set in the \nconfiguration files a global batch size that is divided over the number of GPUs. So if we only \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.912799824094703
      ],
      "excerpt": "to out-of-memory errors. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9846648517808161
      ],
      "excerpt": "Here is an example for Mask R-CNN R-50 FPN with the 1x schedule: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9356291442486512
      ],
      "excerpt": "We also changed the batch size during testing, but that is generally not necessary because testing \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9828127235304283
      ],
      "excerpt": "For more information on some of the main abstractions in our implementation, see ABSTRACTIONS.md. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8774325842748769
      ],
      "excerpt": "    #: add the labels to the boxlist \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8593993976119897
      ],
      "excerpt": "    #: return the image, the boxlist and the idx in your dataset \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9165259544213658,
        0.9385084359224708
      ],
      "excerpt": "    #: we want to split the batches according to the aspect ratio \n    #: of the image, as it can be more efficient than loading the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9602131827359933,
        0.816129664952698,
        0.9004816733994234
      ],
      "excerpt": "For a full example of how the COCODataset is implemented, check maskrcnn_benchmark/data/datasets/coco.py. \nOnce you have created your dataset, it needs to be added in a couple of places: \n- maskrcnn_benchmark/data/datasets/__init__.py: add it to __all__ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8715535020839494,
        0.9613777704908333
      ],
      "excerpt": "free to open a new issue. \nRetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "zold137, object detection project",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/PeterTKovacs/zold137/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 28 Dec 2021 11:31:31 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/PeterTKovacs/zold137/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "PeterTKovacs/zold137",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/PeterTKovacs/zold137/main/docker/Dockerfile",
      "https://raw.githubusercontent.com/PeterTKovacs/zold137/main/docker/docker-jupyter/Dockerfile",
      "https://raw.githubusercontent.com/PeterTKovacs/zold137/main/mileStone1/drone_test/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Check [INSTALL.md](INSTALL.md) for installation instructions.\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```bash\ncd demo\nwget https://download.pytorch.org/models/maskrcnn/e2e_faster_rcnn_X_101_32x8d_FPN_1x.pth\npython demo.py\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "1. pip install torch==1.1.0 torchvision==0.3.0 -f https://download.pytorch.org/whl/torch_stable.html\n2. pip install ninja yacs cython matplotlib tqdm opencv-contrib-python\n3. Follow these scripts. We will download cocoapi here as well\n\n```bash\n#:#: in ./Drone_FasterRCNN\n#: install pycocotools\ngit clone https://github.com/cocodataset/cocoapi.git\ncd cocoapi/PythonAPI\npython setup.py build_ext install\n\ncd ../..\n#:#: back in ./Drone_FasterRCNN\n#: install PyTorch Detection\n\n#: the following will install the lib with\n#: symbolic links, so that you can modify\n#: the files if you want and won't need to\n#: re-build it\npython setup.py build develop\n\n\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "We think that using Docker containers is the only sensible solution.\nBecause of the transfer-learning and our 'custom' dataset, matters are even more complicated, there are caveats that need extra attention.\n\n1. get the [docker/Dockerfile](Dockerfile), download it to your local device\n2. get the weigths for the model: https://drive.google.com/file/d/1SCJf2JJmyCbxpDuy4njFaDw7xPqurpaQ/view?usp=sharing into the _same_ directory which contains the Dockerfile (during building the Dockerimage, the daemon will look for it in that context!)\n3. get the link for the custom dataset annotations: as we work on custom data, we implemented the fetching of appropriate images in the Dockerfile. _However_, due to lincense issues, we must not publicly release the annotations. To circumvent this, the following scheme is implemented\n    + first, PM me (kptzeg@gmail.com) to request access to the annotations\n    + I will send the link to a private GithubGist, which contains the annot files\n    + __what you have to do__: edit the Dockerfile such that you paste this link into the appropriate place before building, so the annotations will be cloned into the container\n4. after the new Dockerfile and the weigths (~800 MB) are assembled in your directory, build the image with eg.: (from the parentdir) __sudo nvidia-docker build -t zold137 docker/__\n5. building is likely to take ~ 1 hour. After that, you have the image, which is roughly 12 GB\n    + during the building, a sample dataset from the custom data is downloaded too\n    \n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9804384564894714
      ],
      "excerpt": "To be able to run anything, you have to install the environment via the Dockerfile, as elaborated later. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9906248903846466
      ],
      "excerpt": "cd drone_demo \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9241342904531881,
        0.957524036534409,
        0.8385218634777329
      ],
      "excerpt": "For the following examples to work, you need to first install maskrcnn_benchmark. \nYou will also need to download the COCO dataset. \nWe recommend to symlink the path to the coco dataset to datasets/ as follows \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8453586014693425,
        0.8957597120131504,
        0.9134125685853338
      ],
      "excerpt": ": symlink the coco dataset \ncd ~/github/maskrcnn-benchmark \nmkdir -p datasets/coco \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9253569476729215
      ],
      "excerpt": ": or use COCO 2017 version \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8569965685513768
      ],
      "excerpt": "You can also configure your own paths to the datasets. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8684410238522189
      ],
      "excerpt": "1. Run the following without modifications \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.806186973744316
      ],
      "excerpt": "But the drawback is that it will use much more GPU memory. The reason is that we set in the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8106025703118147
      ],
      "excerpt": "have a single GPU, this means that the batch size for that GPU will be 8x larger, which might lead \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9233695797448548
      ],
      "excerpt": "python custom_train.py --config-file d_e2e_faster_rcnn_X_101_32x8d_FPN_1x_visdrone.yaml --weights visdrone_model_0360000.pth --custom-dict custom_dict.txt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8253382327559856
      ],
      "excerpt": "by custom_dict.txt drone_demo/custom_dict.txt, sample file: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8263992053991287
      ],
      "excerpt": "final_name: name of final model analogously \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9227631896757174
      ],
      "excerpt": "python draw_inference.py --config-file d_e2e_faster_rcnn_X_101_32x8d_FPN_1x_visdrone.yaml --weights visdrone_model_0360000.pth --custom-dict draw_dict.txt --threshold 0.5 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8754148801078214
      ],
      "excerpt": "in_path : /zold137/datasets/giro_data/images/giro1/test \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9333574343066754
      ],
      "excerpt": "python demo.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8798195837401794,
        0.8801854956928516
      ],
      "excerpt": "from maskrcnn_benchmark.config import cfg \nfrom predictor import COCODemo \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8023167441917616
      ],
      "excerpt": ": update the config options with the config file \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8988821909411175
      ],
      "excerpt": "python /path_to_maskrcnn_benchmark/tools/train_net.py --config-file \"/path/to/config/file.yaml\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.811147856781077
      ],
      "excerpt": "python tools/train_net.py --config-file \"configs/e2e_mask_rcnn_R_50_FPN_1x.yaml\" SOLVER.IMS_PER_BATCH 2 SOLVER.BASE_LR 0.0025 SOLVER.MAX_ITER 720000 SOLVER.STEPS \"(480000, 640000)\" TEST.IMS_PER_BATCH 1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8638326688924323
      ],
      "excerpt": "python -m torch.distributed.launch --nproc_per_node=$NGPUS /path_to_maskrcnn_benchmark/tools/train_net.py --config-file \"path/to/config/file.yaml\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8066881629701139
      ],
      "excerpt": "For a full example of how the COCODataset is implemented, check maskrcnn_benchmark/data/datasets/coco.py. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8870758199028167
      ],
      "excerpt": "- maskrcnn_benchmark/data/datasets/__init__.py: add it to __all__ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8155539106212164
      ],
      "excerpt": "Create a script tools/trim_detectron_model.py like here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8061238353465737
      ],
      "excerpt": "Then you can simply point the converted model path in the config file by changing MODEL.WEIGHT. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/PeterTKovacs/zold137/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "TeX",
      "Cuda",
      "C++",
      "Dockerfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2018 Facebook\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Cyclist detection in aerial images",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "zold137",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "PeterTKovacs",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/PeterTKovacs/zold137/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 28 Dec 2021 11:31:31 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "In this section we introduce the basic functionality of the codebase. The nub of our project in a nutshell: we build our model on top of the second repository. This is beneficial, because the second author has released the weigths of a model of his, trained on the VisDrone dataset. \n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "We provide a simple webcam demo that illustrates how you can use `maskrcnn_benchmark` for inference:\n```bash\ncd demo\n#: by default, it runs on the GPU\n#: for best results, use min-image-size 800\npython webcam.py --min-image-size 800\n#: can also run it on the CPU\npython webcam.py --min-image-size 300 MODEL.DEVICE cpu\n#: or change the model that you want to use\npython webcam.py --config-file ../configs/caffe2/e2e_mask_rcnn_R_101_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu\n#: in order to see the probability heatmaps, pass --show-mask-heatmaps\npython webcam.py --min-image-size 300 --show-mask-heatmaps MODEL.DEVICE cpu\n#: for the keypoint demo\npython webcam.py --config-file ../configs/caffe2/e2e_keypoint_rcnn_R_50_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu\n```\n\nA notebook with the demo can be found in [demo/Mask_R-CNN_demo.ipynb](demo/Mask_R-CNN_demo.ipynb).\n\n",
      "technique": "Header extraction"
    }
  ]
}