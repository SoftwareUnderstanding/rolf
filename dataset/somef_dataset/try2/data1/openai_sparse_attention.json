{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1904.10509"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find this helpful in your work, you can consider citing the following:\n\n```\n@article{child2019sparsetransformer,\n  title={Generating Long Sequences with Sparse Transformers},\n  author={Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},\n  journal={URL https://openai.com/blog/sparse-transformers},\n  year={2019}\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{child2019sparsetransformer,\n  title={Generating Long Sequences with Sparse Transformers},\n  author={Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},\n  journal={URL https://openai.com/blog/sparse-transformers},\n  year={2019}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/openai/sparse_attention",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-04-12T13:06:26Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-20T17:26:11Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9649823890776694,
        0.9611551189820323,
        0.977200331834652,
        0.8075666231839261
      ],
      "excerpt": "This repository contains the sparse attention primitives used in Sparse Transformers (see blog and paper). Specifically, it includes the following: \n1) A faster implementation of normal attention (the upper triangle is not computed, and many operations are fused). \n2) An implementation of \"strided\" and \"fixed\" attention, as in the Sparse Transformers paper. \n3) A simple recompute decorator, which can be adapted for usage with attention. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Examples of using sparse attention, as in \"Generating Long Sequences with Sparse Transformers\"",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/openai/sparse_attention/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 149,
      "date": "Thu, 23 Dec 2021 10:55:28 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/openai/sparse_attention/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "openai/sparse_attention",
    "technique": "GitHub API"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/openai/sparse_attention/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Sparse Attention",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "sparse_attention",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "openai",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/openai/sparse_attention/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "For fp32 and blocksize `32`, any NVIDIA GPU past Kepler can be used (i.e. compute capability beyond 3.5).\n\nFor fp16 and blocksize `8, 16, 32, 64`, a GPU with Tensor Cores (e.g. the V100 GPU, compute capability >= 7.0) is required.\n\nThe primary dependency is the OpenAI [blocksparse](https://github.com/openai/blocksparse/) package.\n\nWith CUDA 10 and tensorflow-gpu, you can install blocksparse with `pip install blocksparse`.\n\nFor other setups, you must install blocksparse from source, and directions can be found in the [root of the repository](https://github.com/openai/blocksparse/).\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1188,
      "date": "Thu, 23 Dec 2021 10:55:28 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Run the following on a non-V100 GPU:\n```\npython attention.py\n```\n\nOn a V100 GPU:\n```\npython attention.py fp16\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "An example can be found at the bottom of `attention.py`.\n\n```python\n\nfull_attn_tf = attention_impl(q, k, v, heads=4, attn_mode=\"all\", recompute=True)\nfull_attn_bs = blocksparse_attention_impl(q, k, v, heads=4, attn_mode=\"all\", recompute=True)\n\n#: first step of strided attention\nlocal_attn_bs = blocksparse_attention_impl(q, k, v, heads=4, attn_mode=\"local\", local_attn_ctx=32, recompute=True)\nlocal_attn_tf = attention_impl(q, k, v, heads=4, attn_mode=\"local\", local_attn_ctx=32, recompute=True)\n\n#: second step of strided attention\nstrided_attn_bs = blocksparse_attention_impl(q, k, v, heads=4, attn_mode=\"strided\", local_attn_ctx=32, recompute=True)\nstrided_attn_tf = attention_impl(q, k, v, heads=4, attn_mode=\"strided\", local_attn_ctx=32, recompute=True)\n\n#: #: the 'fixed' attention pattern\nfixed = blocksparse_attention_impl(q, k, v, heads=4, attn_mode=\"fixed\", local_attn_ctx=128, num_verts=4, vertsize=1, recompute=True)\n\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}