{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1910.02054",
      "https://arxiv.org/abs/2010.13369",
      "https://arxiv.org/abs/2101.06840",
      "https://arxiv.org/abs/2102.02888",
      "https://arxiv.org/abs/2104.07857",
      "https://arxiv.org/abs/2104.06069",
      "https://arxiv.org/abs/2108.06084",
      "https://arxiv.org/abs/1910.02054](https://arxiv.org/abs/1910.02054) and [In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC '20)](https://dl.acm.org/doi/10.5555/3433701.3433727).\n2. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. (2020) DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters. [In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD '20, Tutorial)](https://dl.acm.org/doi/10.1145/3394486.3406703).\n3. Minjia Zhang, Yuxiong He. (2020) Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping. [https://arxiv.org/abs/2010.13369](https://arxiv.org/abs/2010.13369) and [NeurIPS 2020](https://proceedings.neurips.cc/paper/2020/hash/a1140a3d0df1c81e24ae954d935e8926-Abstract.html).\n4. Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, Yuxiong He. (2021) ZeRO-Offload: Democratizing Billion-Scale Model Training. [https://arxiv.org/abs/2101.06840](https://arxiv.org/abs/2101.06840).\n5. Hanlin Tang, Shaoduo Gan, Ammar Ahmad Awan, Samyam Rajbhandari, Conglong Li, Xiangru Lian, Ji Liu, Ce Zhang, Yuxiong He. (2021) 1-bit Adam: Communication Efficient Large-Scale Training with Adam's Convergence Speed. [https://arxiv.org/abs/2102.02888](https://arxiv.org/abs/2102.02888) and [ICML 2021](http://proceedings.mlr.press/v139/tang21a.html).\n6. Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, Yuxiong He. (2021) ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning. [https://arxiv.org/abs/2104.07857](https://arxiv.org/abs/2104.07857).\n7. Conglong Li, Ammar Ahmad Awan, Hanlin Tang, Samyam Rajbhandari, Yuxiong He. (2021) 1-bit LAMB: Communication Efficient Large-Scale Large-Batch Training with LAMB's Convergence Speed. [https://arxiv.org/abs/2104.06069](https://arxiv.org/abs/2104.06069).\n8. Conglong Li, Minjia Zhang, Yuxiong He. (2021) Curriculum Learning: A Regularization Method for Efficient and Stable Billion-Scale GPT Model Pre-Training. [https://arxiv.org/abs/2108.06084](https://arxiv.org/abs/2108.06084).\n\n# Videos\n1. DeepSpeed KDD 2020 Tutorial\n    1. [Overview](https://www.youtube.com/watch?v=CaseqC45DNc&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=29)\n    2. [ZeRO + large model training](https://www.youtube.com/watch?v=y4_bCiAsIAk&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=28)\n    3. [17B T-NLG demo](https://www.youtube.com/watch?v=9V-ZbP92drg&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=27)\n    4. [Fastest BERT training + RScan tuning](https://www.youtube.com/watch?v=o1K-ZG9F6u0&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=26)\n    5. DeepSpeed hands on deep dive: [part 1](https://www.youtube.com/watch?v=_NOk-mBwDYg&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=92), [part 2](https://www.youtube.com/watch?v=sG6_c4VXLww&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=94), [part 3](https://www.youtube.com/watch?v=k9yPkBTayos&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=93)\n    6. [FAQ](https://www.youtube.com/watch?v=nsHu6vEgPew&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=24)\n2. Microsoft Research Webinar\n    * Registration is free and all videos are available on-demand.\n    * [ZeRO & Fastest BERT: Increasing the scale and speed of deep learning training in DeepSpeed](https://note.microsoft.com/MSR-Webinar-DeepSpeed-Registration-On-Demand.html).\n3. [DeepSpeed on AzureML](https://youtu.be/yBVXR8G8Bg8)\n4. Community Tutorials\n    * [DeepSpeed: All the tricks to scale to gigantic models](https://www.youtube.com/watch?v=pDGI668pNg0)\n    * [Turing-NLG, DeepSpeed and the ZeRO optimizer](https://www.youtube.com/watch?v=tC01FRB0M7w)",
      "https://arxiv.org/abs/2010.13369](https://arxiv.org/abs/2010.13369) and [NeurIPS 2020](https://proceedings.neurips.cc/paper/2020/hash/a1140a3d0df1c81e24ae954d935e8926-Abstract.html).\n4. Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, Yuxiong He. (2021) ZeRO-Offload: Democratizing Billion-Scale Model Training. [https://arxiv.org/abs/2101.06840](https://arxiv.org/abs/2101.06840).\n5. Hanlin Tang, Shaoduo Gan, Ammar Ahmad Awan, Samyam Rajbhandari, Conglong Li, Xiangru Lian, Ji Liu, Ce Zhang, Yuxiong He. (2021) 1-bit Adam: Communication Efficient Large-Scale Training with Adam's Convergence Speed. [https://arxiv.org/abs/2102.02888](https://arxiv.org/abs/2102.02888) and [ICML 2021](http://proceedings.mlr.press/v139/tang21a.html).\n6. Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, Yuxiong He. (2021) ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning. [https://arxiv.org/abs/2104.07857](https://arxiv.org/abs/2104.07857).\n7. Conglong Li, Ammar Ahmad Awan, Hanlin Tang, Samyam Rajbhandari, Yuxiong He. (2021) 1-bit LAMB: Communication Efficient Large-Scale Large-Batch Training with LAMB's Convergence Speed. [https://arxiv.org/abs/2104.06069](https://arxiv.org/abs/2104.06069).\n8. Conglong Li, Minjia Zhang, Yuxiong He. (2021) Curriculum Learning: A Regularization Method for Efficient and Stable Billion-Scale GPT Model Pre-Training. [https://arxiv.org/abs/2108.06084](https://arxiv.org/abs/2108.06084).\n\n# Videos\n1. DeepSpeed KDD 2020 Tutorial\n    1. [Overview](https://www.youtube.com/watch?v=CaseqC45DNc&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=29)\n    2. [ZeRO + large model training](https://www.youtube.com/watch?v=y4_bCiAsIAk&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=28)\n    3. [17B T-NLG demo](https://www.youtube.com/watch?v=9V-ZbP92drg&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=27)\n    4. [Fastest BERT training + RScan tuning](https://www.youtube.com/watch?v=o1K-ZG9F6u0&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=26)\n    5. DeepSpeed hands on deep dive: [part 1](https://www.youtube.com/watch?v=_NOk-mBwDYg&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=92), [part 2](https://www.youtube.com/watch?v=sG6_c4VXLww&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=94), [part 3](https://www.youtube.com/watch?v=k9yPkBTayos&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=93)\n    6. [FAQ](https://www.youtube.com/watch?v=nsHu6vEgPew&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=24)\n2. Microsoft Research Webinar\n    * Registration is free and all videos are available on-demand.\n    * [ZeRO & Fastest BERT: Increasing the scale and speed of deep learning training in DeepSpeed](https://note.microsoft.com/MSR-Webinar-DeepSpeed-Registration-On-Demand.html).\n3. [DeepSpeed on AzureML](https://youtu.be/yBVXR8G8Bg8)\n4. Community Tutorials\n    * [DeepSpeed: All the tricks to scale to gigantic models](https://www.youtube.com/watch?v=pDGI668pNg0)\n    * [Turing-NLG, DeepSpeed and the ZeRO optimizer](https://www.youtube.com/watch?v=tC01FRB0M7w)",
      "https://arxiv.org/abs/2101.06840](https://arxiv.org/abs/2101.06840).\n5. Hanlin Tang, Shaoduo Gan, Ammar Ahmad Awan, Samyam Rajbhandari, Conglong Li, Xiangru Lian, Ji Liu, Ce Zhang, Yuxiong He. (2021) 1-bit Adam: Communication Efficient Large-Scale Training with Adam's Convergence Speed. [https://arxiv.org/abs/2102.02888](https://arxiv.org/abs/2102.02888) and [ICML 2021](http://proceedings.mlr.press/v139/tang21a.html).\n6. Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, Yuxiong He. (2021) ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning. [https://arxiv.org/abs/2104.07857](https://arxiv.org/abs/2104.07857).\n7. Conglong Li, Ammar Ahmad Awan, Hanlin Tang, Samyam Rajbhandari, Yuxiong He. (2021) 1-bit LAMB: Communication Efficient Large-Scale Large-Batch Training with LAMB's Convergence Speed. [https://arxiv.org/abs/2104.06069](https://arxiv.org/abs/2104.06069).\n8. Conglong Li, Minjia Zhang, Yuxiong He. (2021) Curriculum Learning: A Regularization Method for Efficient and Stable Billion-Scale GPT Model Pre-Training. [https://arxiv.org/abs/2108.06084](https://arxiv.org/abs/2108.06084).\n\n# Videos\n1. DeepSpeed KDD 2020 Tutorial\n    1. [Overview](https://www.youtube.com/watch?v=CaseqC45DNc&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=29)\n    2. [ZeRO + large model training](https://www.youtube.com/watch?v=y4_bCiAsIAk&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=28)\n    3. [17B T-NLG demo](https://www.youtube.com/watch?v=9V-ZbP92drg&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=27)\n    4. [Fastest BERT training + RScan tuning](https://www.youtube.com/watch?v=o1K-ZG9F6u0&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=26)\n    5. DeepSpeed hands on deep dive: [part 1](https://www.youtube.com/watch?v=_NOk-mBwDYg&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=92), [part 2](https://www.youtube.com/watch?v=sG6_c4VXLww&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=94), [part 3](https://www.youtube.com/watch?v=k9yPkBTayos&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=93)\n    6. [FAQ](https://www.youtube.com/watch?v=nsHu6vEgPew&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=24)\n2. Microsoft Research Webinar\n    * Registration is free and all videos are available on-demand.\n    * [ZeRO & Fastest BERT: Increasing the scale and speed of deep learning training in DeepSpeed](https://note.microsoft.com/MSR-Webinar-DeepSpeed-Registration-On-Demand.html).\n3. [DeepSpeed on AzureML](https://youtu.be/yBVXR8G8Bg8)\n4. Community Tutorials\n    * [DeepSpeed: All the tricks to scale to gigantic models](https://www.youtube.com/watch?v=pDGI668pNg0)\n    * [Turing-NLG, DeepSpeed and the ZeRO optimizer](https://www.youtube.com/watch?v=tC01FRB0M7w)",
      "https://arxiv.org/abs/2102.02888](https://arxiv.org/abs/2102.02888) and [ICML 2021](http://proceedings.mlr.press/v139/tang21a.html).\n6. Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, Yuxiong He. (2021) ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning. [https://arxiv.org/abs/2104.07857](https://arxiv.org/abs/2104.07857).\n7. Conglong Li, Ammar Ahmad Awan, Hanlin Tang, Samyam Rajbhandari, Yuxiong He. (2021) 1-bit LAMB: Communication Efficient Large-Scale Large-Batch Training with LAMB's Convergence Speed. [https://arxiv.org/abs/2104.06069](https://arxiv.org/abs/2104.06069).\n8. Conglong Li, Minjia Zhang, Yuxiong He. (2021) Curriculum Learning: A Regularization Method for Efficient and Stable Billion-Scale GPT Model Pre-Training. [https://arxiv.org/abs/2108.06084](https://arxiv.org/abs/2108.06084).\n\n# Videos\n1. DeepSpeed KDD 2020 Tutorial\n    1. [Overview](https://www.youtube.com/watch?v=CaseqC45DNc&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=29)\n    2. [ZeRO + large model training](https://www.youtube.com/watch?v=y4_bCiAsIAk&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=28)\n    3. [17B T-NLG demo](https://www.youtube.com/watch?v=9V-ZbP92drg&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=27)\n    4. [Fastest BERT training + RScan tuning](https://www.youtube.com/watch?v=o1K-ZG9F6u0&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=26)\n    5. DeepSpeed hands on deep dive: [part 1](https://www.youtube.com/watch?v=_NOk-mBwDYg&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=92), [part 2](https://www.youtube.com/watch?v=sG6_c4VXLww&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=94), [part 3](https://www.youtube.com/watch?v=k9yPkBTayos&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=93)\n    6. [FAQ](https://www.youtube.com/watch?v=nsHu6vEgPew&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=24)\n2. Microsoft Research Webinar\n    * Registration is free and all videos are available on-demand.\n    * [ZeRO & Fastest BERT: Increasing the scale and speed of deep learning training in DeepSpeed](https://note.microsoft.com/MSR-Webinar-DeepSpeed-Registration-On-Demand.html).\n3. [DeepSpeed on AzureML](https://youtu.be/yBVXR8G8Bg8)\n4. Community Tutorials\n    * [DeepSpeed: All the tricks to scale to gigantic models](https://www.youtube.com/watch?v=pDGI668pNg0)\n    * [Turing-NLG, DeepSpeed and the ZeRO optimizer](https://www.youtube.com/watch?v=tC01FRB0M7w)",
      "https://arxiv.org/abs/2104.07857](https://arxiv.org/abs/2104.07857).\n7. Conglong Li, Ammar Ahmad Awan, Hanlin Tang, Samyam Rajbhandari, Yuxiong He. (2021) 1-bit LAMB: Communication Efficient Large-Scale Large-Batch Training with LAMB's Convergence Speed. [https://arxiv.org/abs/2104.06069](https://arxiv.org/abs/2104.06069).\n8. Conglong Li, Minjia Zhang, Yuxiong He. (2021) Curriculum Learning: A Regularization Method for Efficient and Stable Billion-Scale GPT Model Pre-Training. [https://arxiv.org/abs/2108.06084](https://arxiv.org/abs/2108.06084).\n\n# Videos\n1. DeepSpeed KDD 2020 Tutorial\n    1. [Overview](https://www.youtube.com/watch?v=CaseqC45DNc&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=29)\n    2. [ZeRO + large model training](https://www.youtube.com/watch?v=y4_bCiAsIAk&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=28)\n    3. [17B T-NLG demo](https://www.youtube.com/watch?v=9V-ZbP92drg&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=27)\n    4. [Fastest BERT training + RScan tuning](https://www.youtube.com/watch?v=o1K-ZG9F6u0&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=26)\n    5. DeepSpeed hands on deep dive: [part 1](https://www.youtube.com/watch?v=_NOk-mBwDYg&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=92), [part 2](https://www.youtube.com/watch?v=sG6_c4VXLww&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=94), [part 3](https://www.youtube.com/watch?v=k9yPkBTayos&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=93)\n    6. [FAQ](https://www.youtube.com/watch?v=nsHu6vEgPew&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=24)\n2. Microsoft Research Webinar\n    * Registration is free and all videos are available on-demand.\n    * [ZeRO & Fastest BERT: Increasing the scale and speed of deep learning training in DeepSpeed](https://note.microsoft.com/MSR-Webinar-DeepSpeed-Registration-On-Demand.html).\n3. [DeepSpeed on AzureML](https://youtu.be/yBVXR8G8Bg8)\n4. Community Tutorials\n    * [DeepSpeed: All the tricks to scale to gigantic models](https://www.youtube.com/watch?v=pDGI668pNg0)\n    * [Turing-NLG, DeepSpeed and the ZeRO optimizer](https://www.youtube.com/watch?v=tC01FRB0M7w)",
      "https://arxiv.org/abs/2104.06069](https://arxiv.org/abs/2104.06069).\n8. Conglong Li, Minjia Zhang, Yuxiong He. (2021) Curriculum Learning: A Regularization Method for Efficient and Stable Billion-Scale GPT Model Pre-Training. [https://arxiv.org/abs/2108.06084](https://arxiv.org/abs/2108.06084).\n\n# Videos\n1. DeepSpeed KDD 2020 Tutorial\n    1. [Overview](https://www.youtube.com/watch?v=CaseqC45DNc&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=29)\n    2. [ZeRO + large model training](https://www.youtube.com/watch?v=y4_bCiAsIAk&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=28)\n    3. [17B T-NLG demo](https://www.youtube.com/watch?v=9V-ZbP92drg&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=27)\n    4. [Fastest BERT training + RScan tuning](https://www.youtube.com/watch?v=o1K-ZG9F6u0&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=26)\n    5. DeepSpeed hands on deep dive: [part 1](https://www.youtube.com/watch?v=_NOk-mBwDYg&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=92), [part 2](https://www.youtube.com/watch?v=sG6_c4VXLww&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=94), [part 3](https://www.youtube.com/watch?v=k9yPkBTayos&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=93)\n    6. [FAQ](https://www.youtube.com/watch?v=nsHu6vEgPew&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=24)\n2. Microsoft Research Webinar\n    * Registration is free and all videos are available on-demand.\n    * [ZeRO & Fastest BERT: Increasing the scale and speed of deep learning training in DeepSpeed](https://note.microsoft.com/MSR-Webinar-DeepSpeed-Registration-On-Demand.html).\n3. [DeepSpeed on AzureML](https://youtu.be/yBVXR8G8Bg8)\n4. Community Tutorials\n    * [DeepSpeed: All the tricks to scale to gigantic models](https://www.youtube.com/watch?v=pDGI668pNg0)\n    * [Turing-NLG, DeepSpeed and the ZeRO optimizer](https://www.youtube.com/watch?v=tC01FRB0M7w)",
      "https://arxiv.org/abs/2108.06084](https://arxiv.org/abs/2108.06084).\n\n# Videos\n1. DeepSpeed KDD 2020 Tutorial\n    1. [Overview](https://www.youtube.com/watch?v=CaseqC45DNc&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=29)\n    2. [ZeRO + large model training](https://www.youtube.com/watch?v=y4_bCiAsIAk&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=28)\n    3. [17B T-NLG demo](https://www.youtube.com/watch?v=9V-ZbP92drg&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=27)\n    4. [Fastest BERT training + RScan tuning](https://www.youtube.com/watch?v=o1K-ZG9F6u0&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=26)\n    5. DeepSpeed hands on deep dive: [part 1](https://www.youtube.com/watch?v=_NOk-mBwDYg&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=92), [part 2](https://www.youtube.com/watch?v=sG6_c4VXLww&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=94), [part 3](https://www.youtube.com/watch?v=k9yPkBTayos&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=93)\n    6. [FAQ](https://www.youtube.com/watch?v=nsHu6vEgPew&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=24)\n2. Microsoft Research Webinar\n    * Registration is free and all videos are available on-demand.\n    * [ZeRO & Fastest BERT: Increasing the scale and speed of deep learning training in DeepSpeed](https://note.microsoft.com/MSR-Webinar-DeepSpeed-Registration-On-Demand.html).\n3. [DeepSpeed on AzureML](https://youtu.be/yBVXR8G8Bg8)\n4. Community Tutorials\n    * [DeepSpeed: All the tricks to scale to gigantic models](https://www.youtube.com/watch?v=pDGI668pNg0)\n    * [Turing-NLG, DeepSpeed and the ZeRO optimizer](https://www.youtube.com/watch?v=tC01FRB0M7w)"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8423591230008168
      ],
      "excerpt": "<p align=\"center\"><i><b>Minimal Code Change</b></i></p> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9222383658450612
      ],
      "excerpt": "| Article                                                                                        | Description                                  | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8687179211148074
      ],
      "excerpt": "| CIFAR-10 Tutorial                               |  Getting started with CIFAR-10 and DeepSpeed | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8706498213570278
      ],
      "excerpt": "DeepSpeed welcomes your contributions! Please see our \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999622207033674
      ],
      "excerpt": "Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He. (2019) ZeRO: memory optimizations toward training trillion parameter models. arXiv:1910.02054 and In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC '20). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9787417321952059,
        0.9998209888974385,
        0.9999861329683857,
        0.8437937917088121,
        0.9883350273619793,
        0.999892937406814
      ],
      "excerpt": "Minjia Zhang, Yuxiong He. (2020) Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping. arXiv:2010.13369 and NeurIPS 2020. \nJie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, Yuxiong He. (2021) ZeRO-Offload: Democratizing Billion-Scale Model Training. arXiv:2101.06840. \nHanlin Tang, Shaoduo Gan, Ammar Ahmad Awan, Samyam Rajbhandari, Conglong Li, Xiangru Lian, Ji Liu, Ce Zhang, Yuxiong He. (2021) 1-bit Adam: Communication Efficient Large-Scale Training with Adam's Convergence Speed. arXiv:2102.02888 and ICML 2021. \nSamyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, Yuxiong He. (2021) ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning. arXiv:2104.07857. \nConglong Li, Ammar Ahmad Awan, Hanlin Tang, Samyam Rajbhandari, Yuxiong He. (2021) 1-bit LAMB: Communication Efficient Large-Scale Large-Batch Training with LAMB's Convergence Speed. arXiv:2104.06069. \nConglong Li, Minjia Zhang, Yuxiong He. (2021) Curriculum Learning: A Regularization Method for Efficient and Stable Billion-Scale GPT Model Pre-Training. arXiv:2108.06084. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9398184441320769
      ],
      "excerpt": "DeepSpeed hands on deep dive: part 1, part 2, part 3 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8793854886769767
      ],
      "excerpt": "Microsoft Research Webinar \n",
      "technique": "Supervised classification"
    }
  ],
  "codeOfConduct": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://raw.githubusercontent.com/microsoft/DeepSpeed/master/CODE_OF_CONDUCT.md",
    "technique": "File Exploration"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/microsoft/DeepSpeed",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "title: \"Contributing\"\npermalink: /contributing/\n\nDeepSpeed welcomes your contributions!\nPrerequisites\nDeepSpeed uses pre-commit to ensure that formatting is\nconsistent across DeepSpeed. First, ensure that pre-commit is installed from either\ninstalling DeepSpeed or pip install pre-commit. Next, the pre-commit hooks must be\ninstalled once before commits can be made:\nbash\npre-commit install\nAfterwards, our suite of formatting tests run automatically before each git commit. You\ncan also run these manually:\nbash\npre-commit run --all-files\nIf a formatting test fails, it will fix the modified code in place and abort\nthe git commit. After looking over the changes, you can git add &lt;modified files&gt;\nand then repeat the previous git commit command.\nTesting\nDeepSpeed tracks two types of tests: unit tests and more costly model convergence tests.\nThe model convergence tests train\nDeepSpeedExamples and measure\nend-to-end convergence and related metrics. Unit tests are found in tests/unit/ and\nthe model convergence tests are found in tests/model/.\nUnit Tests\nPyTest is used to execute tests. PyTest can be\ninstalled from PyPI via pip install pytest. Simply invoke pytest --forked to run the\nunit tests:\nbash\npytest --forked tests/unit/\nYou can also provide the -v flag to pytest to see additional information about the\ntests. Note that pytest-forked and the\n--forked flag are required to test CUDA functionality in distributed tests.\nModel Tests\nModel tests require four GPUs and training data downloaded for\nDeepSpeedExamples.\nTo execute model tests, first install DeepSpeed. The\nDeepSpeedExamples repository is cloned\nas part of this process. Next, execute the model test driver:\nbash\ncd tests/model/\npytest run_sanity_check.py\nNote that the --forked flag is not necessary for the model tests.\nContributor License Agreement\nThis project welcomes contributions and suggestions. Most contributions require you to\nagree to a Contributor License Agreement (CLA) declaring that you have the right to, and\nactually do, grant us the rights to use your contribution. For details, visit\nhttps://cla.opensource.microsoft.com.\nWhen you submit a pull request, a CLA bot will automatically determine whether you need\nto provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply\nfollow the instructions provided by the bot. You will only need to do this once across\nall repos using our CLA.\nCode of Conduct\nThis project has adopted the Microsoft Open Source Code of\nConduct. For more information see the\nCode of Conduct FAQ or contact\nopencode@microsoft.com with any additional questions or\ncomments.",
    "technique": "File Exploration"
  },
  "contributor": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This project welcomes contributions and suggestions. Most contributions require you to\nagree to a Contributor License Agreement (CLA) declaring that you have the right to, and\nactually do, grant us the rights to use your contribution. For details, visit\nhttps://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need\nto provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply\nfollow the instructions provided by the bot. You will only need to do this once across\nall repos using our CLA.\n\n",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-01-23T18:35:18Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-28T02:35:23Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8910048993550043,
        0.8081243542457909,
        0.8666738124108497,
        0.9439042744544683
      ],
      "excerpt": "[2021/12/09] DeepSpeed-MoE for NLG: Reducing the training cost of language models by 5 times \n[2021/08/18] DeepSpeed powers 8x larger MoE model training with high performance \nMixture of Experts (MoE) tutorial. \nMixture of Experts (MoE) for NLG tutorial. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8066250053257493,
        0.8410245845996474,
        0.9752695862830884
      ],
      "excerpt": "Read more on how to train large models with DeepSpeed \n[2021/08/16] Curriculum learning: a regularization method for stable and 3.3x faster GPT-2 pre-training with 8x/4x larger batch size/learning rate \nDeepSpeed is a deep learning optimization \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.903139731873487,
        0.9676810415610069,
        0.9696642844325037,
        0.9615350802899358,
        0.9542093353503249
      ],
      "excerpt": "DeepSpeed delivers extreme-scale model training for everyone, from data scientists training on massive supercomputers to those training on low-end clusters or even on a single GPU: \n* Extreme scale: Using current generation of GPU clusters with hundreds of devices,  3D parallelism of DeepSpeed can efficiently train deep learning models with trillions of parameters. \n* Extremely memory efficient: With just a single GPU, ZeRO-Offload of DeepSpeed can train models with over 10B parameters, 10x bigger than the state of arts, democratizing multi-billion-parameter model training such that many deep learning scientists can explore bigger and better models. \n* Extremely long sequence length: Sparse attention of DeepSpeed powers an order-of-magnitude longer input sequence and obtains up to 6x faster execution comparing with dense transformers. \n* Extremely communication efficient: 3D parallelism improves communication efficiency allows users to train multi-billion-parameter models 2\u20137x faster on clusters with limited network bandwidth.  1-bit Adam/1-bit LAMB reduce communication volume by up to 5x while achieving similar convergence efficiency to Adam/LAMB, allowing for scaling to different types of GPU clusters and networks. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9482299612188899
      ],
      "excerpt": "a language model (LM) with over 17B parameters called \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9235134564707205,
        0.9634038914040632
      ],
      "excerpt": "establishing a new SOTA in the LM category. \nDeepSpeed is an important part of Microsoft\u2019s new \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9044251593404974
      ],
      "excerpt": "For further documentation, tutorials, and technical deep-dives please see deepspeed.ai! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8457457155819266
      ],
      "excerpt": "| Features                   |  Feature list and overview                  | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9394449182630016
      ],
      "excerpt": "| Contributing           |  Instructions for contributing              | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9333769817186591,
        0.9175423955855755
      ],
      "excerpt": "Training advanced deep learning models is challenging. Beyond model design, \nmodel scientists also need to set up the state-of-the-art training techniques \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8982750200382874,
        0.9949901874433842,
        0.8699759846391634
      ],
      "excerpt": "performance and convergence rate. Large model sizes are even more challenging: \na large model easily runs out of memory with pure data parallelism and it is \ndifficult to use model parallelism. DeepSpeed addresses these challenges to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.878151521971628
      ],
      "excerpt": "Below we provide a brief feature list, see our detailed feature \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877,
        0.9103754479321569
      ],
      "excerpt": "Model Parallelism \nSupport for Custom Model Parallelism \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9196241412598118,
        0.8490037945672047
      ],
      "excerpt": "3D Parallelism \nThe Zero Redundancy Optimizer (ZeRO) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.948090587005854,
        0.8104426079870416,
        0.8666738124108497,
        0.9214975586515987
      ],
      "excerpt": "Complementary to many other DeepSpeed features \nPerformance Analysis and Debugging \nMixture of Experts (MoE) \nAll DeepSpeed documentation can be found on our website: deepspeed.ai \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8930901044020226
      ],
      "excerpt": "| DeepSpeed Features                                       |  DeepSpeed features                          | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8272945003240391,
        0.8807518949129635,
        0.9234539619149897
      ],
      "excerpt": "| 1Cycle Tutorial                               |  SOTA learning schedule in DeepSpeed         | \nDeepSpeed welcomes your contributions! Please see our \ncontributing guide for more details on formatting, testing, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8399748571878369,
        0.9177762152211659,
        0.8373310029600464
      ],
      "excerpt": "This project has adopted the Microsoft Open Source Code of \nConduct. For more information see the \nCode of Conduct FAQ or contact \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9351750086128282
      ],
      "excerpt": "Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. (2020) DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD '20, Tutorial). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.897298524178459
      ],
      "excerpt": "DeepSpeed hands on deep dive: part 1, part 2, part 3 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9643363136338831,
        0.9264528297859279,
        0.8372396012058242
      ],
      "excerpt": "Registration is free and all videos are available on-demand. \nZeRO & Fastest BERT: Increasing the scale and speed of deep learning training in DeepSpeed. \nDeepSpeed on AzureML \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "DeepSpeed is a deep learning optimization library that makes distributed training easy, efficient, and effective.",
      "technique": "GitHub API"
    }
  ],
  "documentation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "https://deepspeed.readthedocs.io/",
      "technique": "Regular expression"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/microsoft/DeepSpeed/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 689,
      "date": "Tue, 28 Dec 2021 12:54:19 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/microsoft/DeepSpeed/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "microsoft/DeepSpeed",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/microsoft/DeepSpeed/master/docker/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/microsoft/DeepSpeed/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/microsoft/DeepSpeed/master/install.sh",
      "https://raw.githubusercontent.com/microsoft/DeepSpeed/master/csrc/aio/py_test/run_write_sweep.sh",
      "https://raw.githubusercontent.com/microsoft/DeepSpeed/master/csrc/aio/py_test/run_read_sweep.sh",
      "https://raw.githubusercontent.com/microsoft/DeepSpeed/master/release/release.sh",
      "https://raw.githubusercontent.com/microsoft/DeepSpeed/master/azure/create_vms.sh",
      "https://raw.githubusercontent.com/microsoft/DeepSpeed/master/azure/attach.sh",
      "https://raw.githubusercontent.com/microsoft/DeepSpeed/master/azure/azure_ssh.sh",
      "https://raw.githubusercontent.com/microsoft/DeepSpeed/master/azure/build_docker_image.sh",
      "https://raw.githubusercontent.com/microsoft/DeepSpeed/master/azure/setup_vms.sh",
      "https://raw.githubusercontent.com/microsoft/DeepSpeed/master/azure/setup_docker.sh",
      "https://raw.githubusercontent.com/microsoft/DeepSpeed/master/azure/start_container.sh",
      "https://raw.githubusercontent.com/microsoft/DeepSpeed/master/azure/shutdown_vms.sh",
      "https://raw.githubusercontent.com/microsoft/DeepSpeed/master/tests/model/BingBertSquad/run_BingBertSquad_sanity.sh",
      "https://raw.githubusercontent.com/microsoft/DeepSpeed/master/tests/model/BingBertSquad/run_BingBertSquad.sh",
      "https://raw.githubusercontent.com/microsoft/DeepSpeed/master/tests/model/BingBertSquad/run_tests.sh",
      "https://raw.githubusercontent.com/microsoft/DeepSpeed/master/tests/model/Megatron_GPT2/ds_gpt2_test.sh",
      "https://raw.githubusercontent.com/microsoft/DeepSpeed/master/docs/code-docs/build-api-docs.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The quickest way to get started with DeepSpeed is via pip, this will install\nthe latest release of DeepSpeed which is not tied to specific PyTorch or CUDA\nversions. DeepSpeed includes several C++/CUDA extensions that we commonly refer\nto as our 'ops'.  By default, all of these extensions/ops will be built\njust-in-time (JIT) using [torch's JIT C++ extension loader that relies on\nninja](https://pytorch.org/docs/stable/cpp_extension.html) to build and\ndynamically link them at runtime.\n\n**Note:** [PyTorch](https://pytorch.org/) must be installed _before_ installing\nDeepSpeed.\n\n```bash\npip install deepspeed\n```\n\nAfter installation, you can validate your install and see which extensions/ops\nyour machine is compatible with via the DeepSpeed environment report.\n\n```bash\nds_report\n```\n\nIf you would like to pre-install any of the DeepSpeed extensions/ops (instead\nof JIT compiling) or install pre-compiled ops via PyPI please see our [advanced\ninstallation instructions](https://www.deepspeed.ai/tutorials/advanced-install/).\n\nOn Windows you can build wheel with following steps, currently only inference mode is supported.\n1. Install pytorch, such as pytorch 1.8 + cuda 11.1\n2. Install visual cpp build tools, such as VS2019 C++ x64/x86 build tools\n3. Launch cmd console with Administrator privilege for creating required symlink folders\n4. Run `python setup.py bdist_wheel` to build wheel in `dist` folder\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9786124855018795
      ],
      "excerpt": "| Install                |  Installation details                       | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.858204411280099
      ],
      "excerpt": "| Contributing           |  Instructions for contributing              | \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8405210537621255
      ],
      "excerpt": "Sparse attention \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8174540907975313
      ],
      "excerpt": "Training Optimizers \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8174540907975313
      ],
      "excerpt": "Training Agnostic Checkpointing \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8334721411866115
      ],
      "excerpt": "| BERT Pre-training Tutorial             |  Pre-train BERT with DeepSpeed               | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.810094876824159
      ],
      "excerpt": "17B T-NLG demo \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/microsoft/DeepSpeed/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Cuda",
      "C++",
      "Shell",
      "C",
      "Dockerfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'    MIT License\\n\\n    Copyright (c) Microsoft Corporation.\\n\\n    Permission is hereby granted, free of charge, to any person obtaining a copy\\n    of this software and associated documentation files (the \"Software\"), to deal\\n    in the Software without restriction, including without limitation the rights\\n    to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\n    copies of the Software, and to permit persons to whom the Software is\\n    furnished to do so, subject to the following conditions:\\n\\n    The above copyright notice and this permission notice shall be included in all\\n    copies or substantial portions of the Software.\\n\\n    THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\n    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\n    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\n    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\n    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\n    OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\n    SOFTWARE\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Latest News",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "DeepSpeed",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "microsoft",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/microsoft/DeepSpeed/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "jeffra",
        "body": "",
        "dateCreated": "2021-11-30T18:02:12Z",
        "datePublished": "2021-12-01T01:16:49Z",
        "html_url": "https://github.com/microsoft/DeepSpeed/releases/tag/v0.5.8",
        "name": "v0.5.8: Patch release",
        "tag_name": "v0.5.8",
        "tarball_url": "https://api.github.com/repos/microsoft/DeepSpeed/tarball/v0.5.8",
        "url": "https://api.github.com/repos/microsoft/DeepSpeed/releases/54371192",
        "zipball_url": "https://api.github.com/repos/microsoft/DeepSpeed/zipball/v0.5.8"
      },
      {
        "authorType": "User",
        "author_name": "jeffra",
        "body": "",
        "dateCreated": "2021-11-19T22:45:02Z",
        "datePublished": "2021-12-01T01:16:33Z",
        "html_url": "https://github.com/microsoft/DeepSpeed/releases/tag/v0.5.7",
        "name": "v0.5.7: Patch release",
        "tag_name": "v0.5.7",
        "tarball_url": "https://api.github.com/repos/microsoft/DeepSpeed/tarball/v0.5.7",
        "url": "https://api.github.com/repos/microsoft/DeepSpeed/releases/54371181",
        "zipball_url": "https://api.github.com/repos/microsoft/DeepSpeed/zipball/v0.5.7"
      },
      {
        "authorType": "User",
        "author_name": "jeffra",
        "body": "",
        "dateCreated": "2021-11-11T16:57:17Z",
        "datePublished": "2021-11-11T17:14:38Z",
        "html_url": "https://github.com/microsoft/DeepSpeed/releases/tag/v0.5.6",
        "name": "v0.5.6: Patch release",
        "tag_name": "v0.5.6",
        "tarball_url": "https://api.github.com/repos/microsoft/DeepSpeed/tarball/v0.5.6",
        "url": "https://api.github.com/repos/microsoft/DeepSpeed/releases/53186277",
        "zipball_url": "https://api.github.com/repos/microsoft/DeepSpeed/zipball/v0.5.6"
      },
      {
        "authorType": "User",
        "author_name": "jeffra",
        "body": "",
        "dateCreated": "2021-11-02T21:41:10Z",
        "datePublished": "2021-11-05T16:11:14Z",
        "html_url": "https://github.com/microsoft/DeepSpeed/releases/tag/v0.5.5",
        "name": "v0.5.5: Patch release",
        "tag_name": "v0.5.5",
        "tarball_url": "https://api.github.com/repos/microsoft/DeepSpeed/tarball/v0.5.5",
        "url": "https://api.github.com/repos/microsoft/DeepSpeed/releases/52798055",
        "zipball_url": "https://api.github.com/repos/microsoft/DeepSpeed/zipball/v0.5.5"
      },
      {
        "authorType": "User",
        "author_name": "jeffra",
        "body": "",
        "dateCreated": "2021-10-06T16:08:18Z",
        "datePublished": "2021-10-06T16:23:03Z",
        "html_url": "https://github.com/microsoft/DeepSpeed/releases/tag/v0.5.4",
        "name": "v0.5.4: Patch release",
        "tag_name": "v0.5.4",
        "tarball_url": "https://api.github.com/repos/microsoft/DeepSpeed/tarball/v0.5.4",
        "url": "https://api.github.com/repos/microsoft/DeepSpeed/releases/50905183",
        "zipball_url": "https://api.github.com/repos/microsoft/DeepSpeed/zipball/v0.5.4"
      },
      {
        "authorType": "User",
        "author_name": "jeffra",
        "body": "",
        "dateCreated": "2021-09-16T16:43:41Z",
        "datePublished": "2021-09-18T05:07:12Z",
        "html_url": "https://github.com/microsoft/DeepSpeed/releases/tag/v0.5.3",
        "name": "v0.5.3: Patch release",
        "tag_name": "v0.5.3",
        "tarball_url": "https://api.github.com/repos/microsoft/DeepSpeed/tarball/v0.5.3",
        "url": "https://api.github.com/repos/microsoft/DeepSpeed/releases/49787106",
        "zipball_url": "https://api.github.com/repos/microsoft/DeepSpeed/zipball/v0.5.3"
      },
      {
        "authorType": "User",
        "author_name": "jeffra",
        "body": "",
        "dateCreated": "2021-09-13T16:37:32Z",
        "datePublished": "2021-09-14T22:50:48Z",
        "html_url": "https://github.com/microsoft/DeepSpeed/releases/tag/v0.5.2",
        "name": "v0.5.2: Patch release",
        "tag_name": "v0.5.2",
        "tarball_url": "https://api.github.com/repos/microsoft/DeepSpeed/tarball/v0.5.2",
        "url": "https://api.github.com/repos/microsoft/DeepSpeed/releases/49563485",
        "zipball_url": "https://api.github.com/repos/microsoft/DeepSpeed/zipball/v0.5.2"
      },
      {
        "authorType": "User",
        "author_name": "jeffra",
        "body": "",
        "dateCreated": "2021-08-26T18:05:43Z",
        "datePublished": "2021-08-26T22:24:00Z",
        "html_url": "https://github.com/microsoft/DeepSpeed/releases/tag/v0.5.1",
        "name": "v0.5.1: Patch release",
        "tag_name": "v0.5.1",
        "tarball_url": "https://api.github.com/repos/microsoft/DeepSpeed/tarball/v0.5.1",
        "url": "https://api.github.com/repos/microsoft/DeepSpeed/releases/48534557",
        "zipball_url": "https://api.github.com/repos/microsoft/DeepSpeed/zipball/v0.5.1"
      },
      {
        "authorType": "User",
        "author_name": "jeffra",
        "body": "* Mixture of Experts (MoE) support\r\n* Curriculum learning",
        "dateCreated": "2021-08-17T05:18:45Z",
        "datePublished": "2021-08-17T05:29:19Z",
        "html_url": "https://github.com/microsoft/DeepSpeed/releases/tag/v0.5.0",
        "name": "DeepSpeed v0.5.0",
        "tag_name": "v0.5.0",
        "tarball_url": "https://api.github.com/repos/microsoft/DeepSpeed/tarball/v0.5.0",
        "url": "https://api.github.com/repos/microsoft/DeepSpeed/releases/47950310",
        "zipball_url": "https://api.github.com/repos/microsoft/DeepSpeed/zipball/v0.5.0"
      },
      {
        "authorType": "User",
        "author_name": "jeffra",
        "body": "",
        "dateCreated": "2021-08-06T21:32:05Z",
        "datePublished": "2021-08-10T17:45:52Z",
        "html_url": "https://github.com/microsoft/DeepSpeed/releases/tag/v0.4.5",
        "name": "v0.4.5: Patch release",
        "tag_name": "v0.4.5",
        "tarball_url": "https://api.github.com/repos/microsoft/DeepSpeed/tarball/v0.4.5",
        "url": "https://api.github.com/repos/microsoft/DeepSpeed/releases/47620006",
        "zipball_url": "https://api.github.com/repos/microsoft/DeepSpeed/zipball/v0.4.5"
      },
      {
        "authorType": "User",
        "author_name": "jeffra",
        "body": "",
        "dateCreated": "2021-07-30T04:06:34Z",
        "datePublished": "2021-07-30T19:50:11Z",
        "html_url": "https://github.com/microsoft/DeepSpeed/releases/tag/v0.4.4",
        "name": "v0.4.4: Patch release",
        "tag_name": "v0.4.4",
        "tarball_url": "https://api.github.com/repos/microsoft/DeepSpeed/tarball/v0.4.4",
        "url": "https://api.github.com/repos/microsoft/DeepSpeed/releases/47070166",
        "zipball_url": "https://api.github.com/repos/microsoft/DeepSpeed/zipball/v0.4.4"
      },
      {
        "authorType": "User",
        "author_name": "jeffra",
        "body": "",
        "dateCreated": "2021-07-12T21:57:52Z",
        "datePublished": "2021-07-13T16:14:10Z",
        "html_url": "https://github.com/microsoft/DeepSpeed/releases/tag/v0.4.3",
        "name": "v0.4.3: Patch release",
        "tag_name": "v0.4.3",
        "tarball_url": "https://api.github.com/repos/microsoft/DeepSpeed/tarball/v0.4.3",
        "url": "https://api.github.com/repos/microsoft/DeepSpeed/releases/46134340",
        "zipball_url": "https://api.github.com/repos/microsoft/DeepSpeed/zipball/v0.4.3"
      },
      {
        "authorType": "User",
        "author_name": "jeffra",
        "body": "",
        "dateCreated": "2021-06-28T19:42:39Z",
        "datePublished": "2021-07-01T17:00:55Z",
        "html_url": "https://github.com/microsoft/DeepSpeed/releases/tag/v0.4.2",
        "name": "v0.4.2: Patch release",
        "tag_name": "v0.4.2",
        "tarball_url": "https://api.github.com/repos/microsoft/DeepSpeed/tarball/v0.4.2",
        "url": "https://api.github.com/repos/microsoft/DeepSpeed/releases/45575293",
        "zipball_url": "https://api.github.com/repos/microsoft/DeepSpeed/zipball/v0.4.2"
      },
      {
        "authorType": "User",
        "author_name": "jeffra",
        "body": "",
        "dateCreated": "2021-06-21T17:41:27Z",
        "datePublished": "2021-06-23T17:19:47Z",
        "html_url": "https://github.com/microsoft/DeepSpeed/releases/tag/v0.4.1",
        "name": "v0.4.1: Patch release",
        "tag_name": "v0.4.1",
        "tarball_url": "https://api.github.com/repos/microsoft/DeepSpeed/tarball/v0.4.1",
        "url": "https://api.github.com/repos/microsoft/DeepSpeed/releases/45115540",
        "zipball_url": "https://api.github.com/repos/microsoft/DeepSpeed/zipball/v0.4.1"
      },
      {
        "authorType": "User",
        "author_name": "jeffra",
        "body": "# DeepSpeed v0.4.0\r\n\r\n* [Press release] [DeepSpeed: Accelerating large-scale model inference and training via system optimizations and compression](https://www.microsoft.com/en-us/research/blog/deepspeed-accelerating-large-scale-model-inference-and-training-via-system-optimizations-and-compression/)\r\n* New inference API [inference setup](https://deepspeed.readthedocs.io/en/latest/inference-init.html)\r\n* [DeepSpeed Inference: Multi-GPU inference with customized inference kerenls and quantization support](https://www.deepspeed.ai/news/2021/03/15/inference-kernel-optimization.html)\r\n  * [Getting Started with DeepSpeed for Inferencing Transformer based Models](https://www.deepspeed.ai/tutorials/inference-tutorial/)\r\n* [Mixture-of-Quantization: A novel quantization approach for reducing model size with minimal accuracy impact](https://www.deepspeed.ai/news/2020/05/27/MoQ.html)\r\n  * [MoQ tutorial](https://www.deepspeed.ai/tutorials/MoQ-tutorial/) for more details.\r\n",
        "dateCreated": "2021-06-08T18:47:08Z",
        "datePublished": "2021-06-08T19:26:49Z",
        "html_url": "https://github.com/microsoft/DeepSpeed/releases/tag/v0.4.0",
        "name": "DeepSpeed v0.4.0",
        "tag_name": "v0.4.0",
        "tarball_url": "https://api.github.com/repos/microsoft/DeepSpeed/tarball/v0.4.0",
        "url": "https://api.github.com/repos/microsoft/DeepSpeed/releases/44302465",
        "zipball_url": "https://api.github.com/repos/microsoft/DeepSpeed/zipball/v0.4.0"
      },
      {
        "authorType": "User",
        "author_name": "jeffra",
        "body": "# v0.3.16 Release notes\r\n\r\n* Full precision (fp32) support for ZeRO Stage2 and Stage3 (#1004) \r\n* 1-bit LAMB optimizer (#970) \r\n* Refactor param_dict to config (#1008)\r\n* [ZeRO Infinity] Allow Init to take a dict for the deepspeed config (#983)\r\n* Bug fix for zero-1 w.r.t. reduce-scatter (#907)\r\n* Use amp autocast in ZeRO3 linear (#990) \r\n* Add find_unused_parameters option to DeepSpeedEngine (#945)\r\n* Relax dataset type check in deepspeed io (#1012)\r\n\r\nSpecial thanks to our contributors: @stas00, @SeanNaren, @sdtblck, @wbuchwalter, @ghosthamlet, @zhujiangang, \r\n",
        "dateCreated": "2021-04-30T05:32:41Z",
        "datePublished": "2021-04-30T19:40:48Z",
        "html_url": "https://github.com/microsoft/DeepSpeed/releases/tag/v0.3.16",
        "name": "DeepSpeed v0.3.16",
        "tag_name": "v0.3.16",
        "tarball_url": "https://api.github.com/repos/microsoft/DeepSpeed/tarball/v0.3.16",
        "url": "https://api.github.com/repos/microsoft/DeepSpeed/releases/42289198",
        "zipball_url": "https://api.github.com/repos/microsoft/DeepSpeed/zipball/v0.3.16"
      },
      {
        "authorType": "User",
        "author_name": "jeffra",
        "body": "# v0.3.15 Release notes\r\n\r\n* [ZeRO-Infinity](https://www.microsoft.com/en-us/research/blog/zero-infinity-and-deepspeed-unlocking-unprecedented-model-scale-for-deep-learning-training/) release allowing nvme offload and more!\r\n* Deprecated `cpu_offload` in config JSON, see [JSON docs](https://www.deepspeed.ai/docs/config-json/#zero-optimizations-for-fp16-training) for more details.\r\n* Automatic external parameter registration, more details in the [ZeRO 3 docs](https://deepspeed.readthedocs.io/en/latest/zero3.html#registering-external-parameters).\r\n\r\n* Several bug fixes for ZeRO stage 3\r\n\r\n",
        "dateCreated": "2021-04-19T15:24:10Z",
        "datePublished": "2021-04-19T16:31:10Z",
        "html_url": "https://github.com/microsoft/DeepSpeed/releases/tag/v0.3.15",
        "name": "DeepSpeed v0.3.15",
        "tag_name": "v0.3.15",
        "tarball_url": "https://api.github.com/repos/microsoft/DeepSpeed/tarball/v0.3.15",
        "url": "https://api.github.com/repos/microsoft/DeepSpeed/releases/41657234",
        "zipball_url": "https://api.github.com/repos/microsoft/DeepSpeed/zipball/v0.3.15"
      },
      {
        "authorType": "User",
        "author_name": "jeffra",
        "body": "Notes to come",
        "dateCreated": "2021-04-08T16:44:16Z",
        "datePublished": "2021-04-08T16:45:27Z",
        "html_url": "https://github.com/microsoft/DeepSpeed/releases/tag/v0.3.14",
        "name": "DeepSpeed v0.3.14",
        "tag_name": "v0.3.14",
        "tarball_url": "https://api.github.com/repos/microsoft/DeepSpeed/tarball/v0.3.14",
        "url": "https://api.github.com/repos/microsoft/DeepSpeed/releases/41136404",
        "zipball_url": "https://api.github.com/repos/microsoft/DeepSpeed/zipball/v0.3.14"
      },
      {
        "authorType": "User",
        "author_name": "jeffra",
        "body": "# v0.3.13 Release notes\r\nCombined release notes since Jan 12th v0.3.10 release\r\n\r\n* ZeRO 3 Offload (#834) \r\n* more detailed notes to come",
        "dateCreated": "2021-03-16T22:22:55Z",
        "datePublished": "2021-03-16T22:28:45Z",
        "html_url": "https://github.com/microsoft/DeepSpeed/releases/tag/v0.3.13",
        "name": "DeepSpeed v0.3.13",
        "tag_name": "v0.3.13",
        "tarball_url": "https://api.github.com/repos/microsoft/DeepSpeed/tarball/v0.3.13",
        "url": "https://api.github.com/repos/microsoft/DeepSpeed/releases/39909878",
        "zipball_url": "https://api.github.com/repos/microsoft/DeepSpeed/zipball/v0.3.13"
      },
      {
        "authorType": "User",
        "author_name": "jeffra",
        "body": "# v0.3.10 Release notes\r\nCombined release notes since November 12th v0.3.1 release\r\n\r\n* Various updates to torch.distributed initialization\r\n  * New `deepspeed.init_distributed` API, #608, #645, #644\r\n  * Improved AzureML support for patching torch.distributed backend, #542 \r\n  * Simplify dist init and only init if needed #553\r\n* Transformer kernel updates\r\n  * Support for different hidden dimensions #559\r\n  * Support arbitrary sequence-length #587\r\n* Elastic training support (#602) \r\n  * NOTE: More details to come on this feature, currently still in initial piloting of this feature.\r\n* Module replacement support #586\r\n  * NOTE: Will be used more and documented in the short-term to help automatically inject/replace deepspeed ops into client models.\r\n* #528 removes dependencies psutil and cpufeature\r\n* Various ZeRO 1 and 2 bug fixes and updates: #531, #532, #545, #548 \r\n* #543 backwards compatible checkpoints with older deepspeed v0.2 version\r\n* Add static_loss_scale support to unfused optimizer #546\r\n* Bug fix for norm calculation in absence of model parallel group #551\r\n* Switch CI from azure pipelines to github actions\r\n* Deprecate client ability to disable gradient reduction #552\r\n* Bug fix for tracking optimizer step in cpu-adam when loading checkpoint #564\r\n* Improved support for Ampere architecture #572, #570, #577, #578, #591, #642\r\n* Fix potential random layout inconsistency issues in sparse attention modules #534\r\n* Supported customizing kwargs for lr_scheduler #584\r\n* Support deepspeed.initialize with dict configuration instead of arg #632\r\n* Allow DeepSpeed models to be initialized with optimizer=None #469\r\n\r\n\r\n# Special thanks to our contributors in this release\r\n@stas00, @gcooper-isi, @g-karthik, @sxjscience, @brettkoonce, @carefree0910, @Justin1904, @harrydrippin\r\n\r\n",
        "dateCreated": "2021-01-08T21:00:44Z",
        "datePublished": "2021-01-12T18:17:34Z",
        "html_url": "https://github.com/microsoft/DeepSpeed/releases/tag/v0.3.10",
        "name": "DeepSpeed v0.3.10",
        "tag_name": "v0.3.10",
        "tarball_url": "https://api.github.com/repos/microsoft/DeepSpeed/tarball/v0.3.10",
        "url": "https://api.github.com/repos/microsoft/DeepSpeed/releases/36294690",
        "zipball_url": "https://api.github.com/repos/microsoft/DeepSpeed/zipball/v0.3.10"
      },
      {
        "authorType": "User",
        "author_name": "jeffra",
        "body": "## Updates \r\n* Efficient and robust compressed training through [progressive layer dropping](https://www.deepspeed.ai/news/2020/10/28/progressive-layer-dropping-news.html)\r\n* JIT compilation of C++/CUDA extensions\r\n* Python-only install support, ~10x faster install time\r\n* PyPI hosted installation via `pip install deepspeed`\r\n* Removed apex dependency\r\n* Bug fixes for ZeRO-offload and CPU-Adam\r\n* Transformer support for dynamic sequence length (#424)\r\n* Linear warmup+decay lr schedule (#414) \r\n",
        "dateCreated": "2020-11-12T19:51:38Z",
        "datePublished": "2020-11-12T19:55:22Z",
        "html_url": "https://github.com/microsoft/DeepSpeed/releases/tag/v0.3.1",
        "name": "DeepSpeed v0.3.1",
        "tag_name": "v0.3.1",
        "tarball_url": "https://api.github.com/repos/microsoft/DeepSpeed/tarball/v0.3.1",
        "url": "https://api.github.com/repos/microsoft/DeepSpeed/releases/33872274",
        "zipball_url": "https://api.github.com/repos/microsoft/DeepSpeed/zipball/v0.3.1"
      },
      {
        "authorType": "User",
        "author_name": "jeffra",
        "body": "## New features\r\n* [DeepSpeed: Extreme-scale model training for everyone](linklink)\r\n  * [Powering 10x longer sequences and 6x faster execution through DeepSpeed Sparse Attention](https://www.deepspeed.ai/news/2020/09/08/sparse-attention-news.html)\r\n  * [Training a trillion parameters with pipeline parallelism](https://www.deepspeed.ai/news/2020/09/08/pipeline-parallelism.html)\r\n  * [Up to 5x less communication and 3.4x faster training through 1-bit Adam](https://www.deepspeed.ai/news/2020/09/08/onebit-adam-news.html)\r\n  * [10x bigger model training on a single GPU with ZeRO-Offload](https://www.deepspeed.ai/news/2020/09/08/ZeRO-Offload.html)\r\n\r\n## Software improvements\r\n* Refactor codebase to make cleaner distinction between ops/runtime/zero/etc.\r\n* Conditional Op builds \r\n  * Not all users should have to spend time building transformer kernels if they don't want to use them.\r\n  * To ensure DeepSpeed is portable in multiple environments some features require unique dependencies that not everyone will be able to or want to install.\r\n* DeepSpeed launcher supports different backends in additional to pdsh such as Open MPI and MVAPICH.",
        "dateCreated": "2020-09-10T19:32:05Z",
        "datePublished": "2020-09-10T19:44:20Z",
        "html_url": "https://github.com/microsoft/DeepSpeed/releases/tag/v0.3.0",
        "name": "DeepSpeed v0.3.0",
        "tag_name": "v0.3.0",
        "tarball_url": "https://api.github.com/repos/microsoft/DeepSpeed/tarball/v0.3.0",
        "url": "https://api.github.com/repos/microsoft/DeepSpeed/releases/31082014",
        "zipball_url": "https://api.github.com/repos/microsoft/DeepSpeed/zipball/v0.3.0"
      },
      {
        "authorType": "User",
        "author_name": "jeffra",
        "body": "# DeepSpeed 0.2.0 Release Notes\r\n\r\n## Features\r\n* ZeRO-1 with reduce scatter\r\n* ZeRO-2\r\n* Transformer kernels\r\n* Various bug fixes and usability improvements",
        "dateCreated": "2020-06-12T04:24:44Z",
        "datePublished": "2020-06-16T06:32:36Z",
        "html_url": "https://github.com/microsoft/DeepSpeed/releases/tag/v0.2.0",
        "name": "DeepSpeed v0.2.0",
        "tag_name": "v0.2.0",
        "tarball_url": "https://api.github.com/repos/microsoft/DeepSpeed/tarball/v0.2.0",
        "url": "https://api.github.com/repos/microsoft/DeepSpeed/releases/27582733",
        "zipball_url": "https://api.github.com/repos/microsoft/DeepSpeed/zipball/v0.2.0"
      },
      {
        "authorType": "User",
        "author_name": "jeffra",
        "body": "# DeepSpeed 0.1.0 Release Notes\r\n\r\n## Features\r\n* Distributed Training with Mixed Precision\r\n  * 16-bit mixed precision\r\n  * Single-GPU/Multi-GPU/Multi-Node\r\n* Model Parallelism\r\n  * Support for Custom Model Parallelism  \r\n  * Integration with Megatron-LM\r\n* Memory and Bandwidth Optimizations\r\n  * Zero Redundancy Optimizer (ZeRO) stage 1 with all-reduce\r\n  * Constant Buffer Optimization (CBO)\r\n  * Smart Gradient Accumulation\r\n* Training Features\r\n  * Simplified training API\r\n  * Gradient Clipping\r\n  * Automatic loss scaling with mixed precision\r\n* Training Optimizers\r\n  * Fused Adam optimizer and arbitrary torch.optim.Optimizer\r\n  * Memory bandwidth optimized FP16 Optimizer\r\n  * Large Batch Training with LAMB Optimizer\r\n  * Memory efficient Training with ZeRO Optimizer\r\n* Training Agnostic Checkpointing\r\n* Advanced Parameter Search\r\n  * Learning Rate Range Test\r\n  * 1Cycle Learning Rate Schedule\r\n* Simplified Data Loader\r\n* Performance Analysis and Debugging",
        "dateCreated": "2020-05-18T16:33:15Z",
        "datePublished": "2020-05-19T06:41:13Z",
        "html_url": "https://github.com/microsoft/DeepSpeed/releases/tag/v0.1.0",
        "name": "DeepSpeed v0.1.0",
        "tag_name": "v0.1.0",
        "tarball_url": "https://api.github.com/repos/microsoft/DeepSpeed/tarball/v0.1.0",
        "url": "https://api.github.com/repos/microsoft/DeepSpeed/releases/26637619",
        "zipball_url": "https://api.github.com/repos/microsoft/DeepSpeed/zipball/v0.1.0"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 6057,
      "date": "Tue, 28 Dec 2021 12:54:19 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "deep-learning",
      "pytorch",
      "gpu",
      "machine-learning",
      "billion-parameters",
      "data-parallelism",
      "model-parallelism"
    ],
    "technique": "GitHub API"
  }
}