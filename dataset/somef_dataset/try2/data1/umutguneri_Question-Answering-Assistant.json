{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1412.1632},\n  Lei Yu, Karl Moritz Hermann, Phil Blunsom, Stephen Pulman\n  (Submitted on 4 Dec 2014",
      "https://arxiv.org/abs/1503.08895},\n  Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus\n  (Submitted on 31 Mar 2015 (v1",
      "https://arxiv.org/abs/1410.3916},\n  Jason Weston, Sumit Chopra, Antoine Bordes Submitted on 15 Oct 2014.\n\n5-Neural Machine Translation by Jointly Learning to Align and Translate\n  {https://arxiv.org/abs/1409.0473},\n  Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio\nSubmitted on 1 Sep 2014 , last revised 19 May 2016\n\n6-A Sample of the Penn Treebank Corpus\n  {https://www.kaggle.com/nltkdata/penn-tree-bank},\n  University of Pennsylvania 2017.\n  \n7-Embedding made from the text8 Wikipedia dump.\n  {https://data.world/jaredfern/text-8-w-2-v}\n  \n8-SQuAD: 100,000+ Questions for Machine Comprehension of Text\n  {https://rajpurkar.github.io/SQuAD-explorer/},\n  Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang\n(Submitted on 16 Jun 2016, last revised 11 Oct 2016 ",
      "https://arxiv.org/abs/1409.0473},\n  Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio\nSubmitted on 1 Sep 2014 , last revised 19 May 2016\n\n6-A Sample of the Penn Treebank Corpus\n  {https://www.kaggle.com/nltkdata/penn-tree-bank},\n  University of Pennsylvania 2017.\n  \n7-Embedding made from the text8 Wikipedia dump.\n  {https://data.world/jaredfern/text-8-w-2-v}\n  \n8-SQuAD: 100,000+ Questions for Machine Comprehension of Text\n  {https://rajpurkar.github.io/SQuAD-explorer/},\n  Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang\n(Submitted on 16 Jun 2016, last revised 11 Oct 2016 "
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1-Deep Learning for Answer Sentence Selection\n  {https://arxiv.org/abs/1412.1632},\n  Lei Yu, Karl Moritz Hermann, Phil Blunsom, Stephen Pulman\n  (Submitted on 4 Dec 2014).\n  \n2-End-To-End Memory Networks\n  {https://arxiv.org/abs/1503.08895},\n  Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus\n  (Submitted on 31 Mar 2015 (v1), last revised 24 Nov 2015)\n  \n3-Text REtrieval Conference (TREC) Question Answering Collections\n  {https://trec.nist.gov/data/qa.html},\n  Voorhees, E. Tice, D. \"Building a Question Answering Test Collection\", Proceedings of\nSIGIR-2000, July, 2000, pp. 200-207\n\n4-Memory Networks\n  {https://arxiv.org/abs/1410.3916},\n  Jason Weston, Sumit Chopra, Antoine Bordes Submitted on 15 Oct 2014.\n\n5-Neural Machine Translation by Jointly Learning to Align and Translate\n  {https://arxiv.org/abs/1409.0473},\n  Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio\nSubmitted on 1 Sep 2014 , last revised 19 May 2016\n\n6-A Sample of the Penn Treebank Corpus\n  {https://www.kaggle.com/nltkdata/penn-tree-bank},\n  University of Pennsylvania 2017.\n  \n7-Embedding made from the text8 Wikipedia dump.\n  {https://data.world/jaredfern/text-8-w-2-v}\n  \n8-SQuAD: 100,000+ Questions for Machine Comprehension of Text\n  {https://rajpurkar.github.io/SQuAD-explorer/},\n  Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang\n(Submitted on 16 Jun 2016, last revised 11 Oct 2016 )\n\n9-The Dialog State Tracking Challenge (DSTC) is an on-going series of research community challenge tasks.\n  {https://www.microsoft.com/en-us/research/event/dialog-state-tracking-challenge/},\n  \n10-The Dialog State Tracking Challenge\n  {https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/dstc2013.pdf}, \n  Jason D. Williams, Antoine Raux, Deepak Ramachadran, and Alan Black. Proceedings of the SIGDIAL 2013 Conference, Metz, France, August 2013.\n  \n11-The Second Dialog State Tracking Challenge\n  {https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/summaryWriteup.pdf}, \n  Matthew Henderson, Blaise Thomson, and Jason D. Williams. Proceedings of SIGDIAL 2014 Conference, Philadelphia, USA, June 2014.\n  \n12-The Third Dialog State Tracking Challenge\n  {https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/writeup.pdf}, \n  Matthew Henderson, Blaise Thomson, and Jason D. Williams. Proceedings IEEE Spoken Language Technology Workshop (SLT), South Lake Tahoe, USA, December 2014.\n  \n13-2015 International Workshop Series on Spoken Dialogue Systems Technology\n  {http://wikicfp.com/cfp/servlet/event.showcfp=eventid=38683=copyownerid=66338}, \n  \n14-The Fourth Dialog State Tracking Challenge\n  {https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/dstc4final-1.pdf}, \n  Seokhwan Kim, Luis F. D'Haro, Rafael E Banchs, Matthew Henderson, and Jason D. Williams. Proceedings IEEE Spoken Language Technology Workshop (SLT), South Lake Tahoe, USA, December 2014.\n  \n15-2016 IEEE Workshop on Spoken Language Technology\n  {https://www2.securecms.com/SLT2016//Default.asp}, \n  13\u201316 December 2016 \u2022 San Diego, California\n  \n16-The Fifth Dialog State Tracking Challenge. Seokhwan Kim, Luis F. D'Haro, Rafael E Banchs, Matthew Henderson, Jason D. Williams, and Koichiro Yoshino. Proceedings IEEE Spoken Language Technology Workshop (SLT), San Diego, USA, December 2016.\n  \n17-  The Dialogue Breakdown Detection Challenge\n  {https://pdfs.semanticscholar.org/c1c1/34f2a411d70687142cb63da14e9d55ffbfb6.pdf}, \n  Ryuichiro Higashinaka, Kotaro Funakoshi, Yuka Kobayashi, Michimasa Inaba, NTT Media Intelligence Laboratories\n, Honda Research Institute Japan Co., Ltd., Toshiba Corporation, Hiroshima City University\n\n\n18-2017 Conference on Neural Information Processing Systems\n  {https://nips.cc/}, \n  Long Beach Convention and Entertainment Center, Long Beach, California, USA\n  \n19-Restaurant information train and development set \n  {http://camdial.org/~mh521/dstc/}, \n  Paul Crook - Microsoft Research, Maxine Eskenazi - Carnegie Mellon University, Milica Gasic - University of Cambridge (3rd February 2014)\n  \n20-Ziraat Bank frequently asked questions\n  {https://www.ziraatbank.com.tr/tr/sss}\n  \n21- Tensorflow - An open source machine learning framework for everyone\n  {https://www.tensorflow.org/}\n  \n22- Vector Representations of Words \n  {https://www.tensorflow.org/tutorials/word2vec}\n  \n23-Optimization: Stochastic Gradient Descent\n  {http://ufldl.stanford.edu/tutorial/}\n  \n24- Understanding LSTM Networks\n  {http://colah.github.io/posts/2015-08-Understanding-LSTMs/}  \n  \n25- Cosine Similarity\n  {http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/}\n  \n26- Levensthein Distance\n  {http://stackabuse.com/levenshtein-distance-and-text-similarity-in-python/}\n  \n27-Qgram Similarity\n  {http://cs.uef.fi/~zhao/Link/Similarity-strings.html}  \n\n\n\n\n\n\n\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/umutguneri/Question-Answering-Assistant",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-10-18T14:30:08Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-10-18T15:22:07Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9956465446162843
      ],
      "excerpt": "The goal of this project is to implement a text based question answering system which \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9542091176930155,
        0.8197779089443321,
        0.98520216479881,
        0.9924300664012704
      ],
      "excerpt": "correct answer to the user.   In case of the answer could not be found,  this system \ninforms the user and finds soon a suitable answer. If the chosen answer is satisfactory \nfor the user, the new question-answer binary is added to the database. After a certain \nperiod of time, the data is retrained to increase the success of system. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9012512701817598,
        0.8516952950166352,
        0.962097327737114
      ],
      "excerpt": "using neural network and second approach is using string similarity methods( Cosine \nsimilarity, Levenhstein distance, Qgram similarity) as a solution of the problem. \nNeural networks models depend on size of data significantly, therefore string similarity \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9958395494398284
      ],
      "excerpt": "On the other hand, in case of the size of data increases, the accuracy of the neural \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8254466375846096
      ],
      "excerpt": "Report of Project: https://github.com/umutguneri/Question-Answering-Assistant/blob/master/project_report.pdf \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9948329168799329,
        0.9371003903322688
      ],
      "excerpt": "On this project, it is aimed to response with appropriate answers to the questions asked \nby the user.  It is expected that the system will only answer the questions in specific \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9966474696551649,
        0.9873287953533503,
        0.9809107813458882
      ],
      "excerpt": "Objective of the Thesis \nThe project is a question answering system that improves learning with the logic that \nis set up at the back. It is aimed to create more accurate and meaningful answers over \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9932097639144775,
        0.9495605872139435,
        0.9380577714653601,
        0.9874909745458798,
        0.9404373183788212
      ],
      "excerpt": "data and other needs for the project, \n2.  Collection of question and answer text data from specific sectors, \n3.  Cleaning of collected data, \n4.  Training  of  different  approaches  of  gathered  data  deep  learning  framework, \napplication of optimization, regulation methods, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9883549169929069,
        0.8955877153648355,
        0.8285252786569139
      ],
      "excerpt": "On this project my purpose is to give correct answers to the questions asked by users, \nalthough the level of achievement can not be reached in the first stage, it is aimed to \nreaching the acceptable levels of this level step by step. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8471021805935617,
        0.9684915575131307
      ],
      "excerpt": "nltk (natural language toolkit for tokenized the sentences) \ntqdm (for the nice progression bars) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8281392253120085,
        0.9728170047085597,
        0.9179466132199997,
        0.977727591510122,
        0.869631001950506,
        0.9829463188730883
      ],
      "excerpt": "Workflow Schema for question answering system is shown in Figure: \nThe dataset which is required for implementation of my project, has been obtained \nfrom section of frequently asked questions in Ziraat Bank website \n. On this section of  website  there  were  approximately  400  pair  of  question-answer  \nabout  different subjects in banking sector. \nThis  size  of  dataset  is  not  enough  for  implementation  my  deep  learning  model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9291548158368804,
        0.8633856711384789,
        0.9812361505397464
      ],
      "excerpt": "can be enlarged for the deep learning model \nSome of sample questions and answers are shown in Figure: \nThe models in this work are implemented with Tensorflow, and all experiments \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8073709619849031,
        0.9420797773692283,
        0.8536156321312,
        0.8984251043178405,
        0.866451703488436
      ],
      "excerpt": "epoch and best hyper-parameter settings for testing. \nThe word embedding is trained by Word2vec, and the word vector size is 300. \nWord embeddings are also parameters and are optimized as well during the training. \nStochastic Gradient Descent(SGD) is the optimization strategy.  I tried different \nmargin values, such as 0.05, 0.1 and 0.2, and finally fixed the margin as 0.2. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9832374460238021,
        0.8199024995545773,
        0.8771491200570781,
        0.8192622447084501,
        0.8923858908958955,
        0.9544165123403201,
        0.9644100322184471
      ],
      "excerpt": "Each word vector, which is obtained of the questions in the data set, is used as input. \nThe output layer has the indexes of the sentences placed in order. During training of \nthe model after every epoch, the sentence index in the output layer is changed to 1 as \n14 a binary value. Total number of questions in dataset is currently 488, which are used \nfor training the model. \nAfter 100 epochs the training phase of model is ended.  Leaning rate is set to 0.001 \nand dropout rate is set to 0.5.  This approach is very similar to the text classification \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8484020730088279,
        0.9860739992120692,
        0.8671957458275185,
        0.9809170925641789,
        0.9567694656578464,
        0.8617897439831902,
        0.856583443864577,
        0.894540820883971
      ],
      "excerpt": "Multi-layer perceptron model for question answering is shown in Figure: \nThe  size  of  dataset  is  not  enough  for  implementation  my  deep  learning  model. \nTherefore, in some cases the web application can not reply with correct answer. \nThe solution for this problem is enlarging dataset for the deep learning model using \ndesktop application.  The mentality of desktop application is not complicated.  The \nprogram finds closest question to obtained question using some similarity methods \nsuch as cosine similarity, levenshtein distance and Qqram similarity. If the user is  \nsatisfied with answer then he has to click Ok button so the new question and answer  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8382559548453034,
        0.8118512451344517,
        0.9137261723554904
      ],
      "excerpt": "If the user is not satisfied with answer,  the user can choose one of 5 most similar \nquestions. In this case, a new answer will be shown on text area. If the user is still not \nsatisfied with answer, then he has to click on Not Ok button, the question and answer \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9684915575131307,
        0.9073106391048146
      ],
      "excerpt": "for the question. \nUsing this desktop application, enlarging dataset for the deep learning model can be \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9420414482207314,
        0.9845857016458837
      ],
      "excerpt": "UML diagram of desktop application is shown in Figure: \nThe main page of Web application is shown in Figure.  For now it is only for a \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9059390693544191,
        0.8511750783824699
      ],
      "excerpt": "User can ask a question using web application. It is shown in Figure: \nThe answer and question will be shown on text area. It is shown in Figure: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8232581588223681,
        0.8100042256707934
      ],
      "excerpt": "The main page of desktop application is shown in Figure .  Users can write their \nown question in text label as an input.  After a couple of milliseconds the system will \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639670692646687,
        0.9593805407719731
      ],
      "excerpt": "question  and  similarity  rate  are  shown  on  screen.   Due  the  question  is  already  in \ndatabase, similarity rate is 1.0 and correct answer is seen in text area.  It is shown \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8147641638565019,
        0.968953842643767,
        0.9686411944273936,
        0.8624075437912109
      ],
      "excerpt": "0.9 and same answer will be seen on text area. It is shown in Figure: \nAt the bottom of text area an important question is asked to user.  It means \"Are you \nsatisfied with answer?\".  Also, Ok and Not ok button take part in right side of this \nquestion. It is shown in Figure: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9377212934307217
      ],
      "excerpt": "message \"The question is added in database\" is shown in Figure: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8422117168647716,
        0.8543616425509994
      ],
      "excerpt": "database.   The admin will enter new answers after a while for the question.   The \nmessage \"For your question will be found better answer in soon\" is shown in Figure: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8584621739324342,
        0.8907698379110658
      ],
      "excerpt": "application. It is shown in Figure: \nIn this application, the user can choose the method which is used to find most similar \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.911724708839874,
        0.8382559548453034
      ],
      "excerpt": "and qram. It is shown in Figure: \nIf the user is not satisfied with question, the user can choose one of 5 most similar \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.819915534531895,
        0.9324252099745702,
        0.9585417920500433,
        0.8232581588223681,
        0.8100042256707934
      ],
      "excerpt": "section is shown in Figure: \nBefore the main page of mobile application is shown, the splash screen is shown during \na couple of seconds on android application. It is shown in Figure: \nThe main page of mobile application is shown in Figure.  Users can write their \nown question in text label as an input.  After a couple of milliseconds the system will \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8365289952115507,
        0.978637565987063,
        0.9686411944273936,
        0.8624075437912109
      ],
      "excerpt": "shown on screen. It is shown in Figure: \nAt the above of text area an important question is asked to user.  It means \"Are you \nsatisfied with answer?\".  Also, Ok and Not ok button take part in right side of this \nquestion. It is shown in Figure: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9377212934307217
      ],
      "excerpt": "message \"The question is added in database\" is shown in Figure: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8422117168647716,
        0.8543616425509994,
        0.8382559548453034
      ],
      "excerpt": "database.   The admin will enter new answers after a while for the question.   The \nmessage \"For your question will be found better answer in soon\" is shown in Figure: \nIf the user is not satisfied with question, the user can choose one of 5 most similar \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.819915534531895,
        0.9129734021403579
      ],
      "excerpt": "section is shown in Figure: \nAn  evaluation  model  was  used  to  measure  user  satisfaction  with  the  question \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8461796212321409,
        0.8599774146075823,
        0.9750818312144827
      ],
      "excerpt": "Participants were requested to ask similar or related questions to these questions using \nweb application and desktop application.  After each response of the application, the \nusers evaluated the system with a score of 1-5.   This evaluating was based on the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9254632022913959
      ],
      "excerpt": "The result of the rating is shown in Table, which have done by the participants: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8581319521677299
      ],
      "excerpt": "Accordingly, the overall success of the system was estimated to be approximately **92%. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8977811503007829,
        0.8457287577220043,
        0.9723084138099672,
        0.8167815586847328,
        0.9383123861753735,
        0.8258155416616381,
        0.8389307345409965,
        0.9679056724961992,
        0.873817447834306,
        0.9845843539428177,
        0.9896294465990565
      ],
      "excerpt": "First, when the application created on the mobile platform is examined, the opening \ntime of the program takes 1-2 ms. No stuck or waiting period occurs during usage. \nSimilarly, when we look at the desktop and web application, the opening time of the \nprogram lasts in ms. There is no hanging or waiting during usage. \nIn this project, a text based question answering assistant was implemented which can \nbe used for different sectors. The system was designed that finds the closest question \namong the previously asked questions by user and also the system returns correct \nanswer to the user. In case of the answer could not be found, this system informs the \nuser and finds soon a suitable answer. If the chosen answer is satisfactory for the user, \nthe new question-answer binary is added to the database.  After a certain period of \ntime, the data is retrained to increase the success of system. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9407767738619656
      ],
      "excerpt": "neural network model (LSTM) and second method is to use string similarity methods( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9178615741733793,
        0.9834800931685531
      ],
      "excerpt": "needs more data sets. \nWhen the experimental results of the project were reviewed, it was found that the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.973277389492187,
        0.9643580122782862,
        0.9590299617233967,
        0.8397829851087562,
        0.8820227074609119
      ],
      "excerpt": "Quantity, Manner, Relation) it was scored over 4 out of 5. Overall score is 4.6 points, \nwhich is about 92%. \nOn this project my purpose was to give correct answers to the questions asked by \nusers, although the level of achievement could not be reached in the first stage, it was \nreached the acceptable levels of this level step by step.  There are not many studies \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Domain Spesific Question Answering Assistant",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/umutguneri/Question-Answering-Assistant/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 28 Dec 2021 02:13:22 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/umutguneri/Question-Answering-Assistant/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "umutguneri/Question-Answering-Assistant",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.9272102707687195,
        0.8947724525056635,
        0.8837680365796365
      ],
      "excerpt": "System Requirements \nThe web interface requires the following dependencies: \npython 3.5 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9783681404788063
      ],
      "excerpt": "CUDA (for using GPU) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9220451905096577
      ],
      "excerpt": "following 4 criteria: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8817198529568041
      ],
      "excerpt": "Are you satisfied with answers? (Quantity) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8344035977640186
      ],
      "excerpt": "Question Answering Assistant has been tested on two different platforms (Windows \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8097774996416431
      ],
      "excerpt": "as an Hidden Layer, 1 of them as an output Layer). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8015363192700379
      ],
      "excerpt": "for training the model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8649053773892037
      ],
      "excerpt": "print the answer to the screen: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8649053773892037
      ],
      "excerpt": "print the answer to the screen: \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/umutguneri/Question-Answering-Assistant/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Java"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Domain Spesific Question Answering Assistant",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Question-Answering-Assistant",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "umutguneri",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/umutguneri/Question-Answering-Assistant/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 28 Dec 2021 02:13:22 GMT"
    },
    "technique": "GitHub API"
  }
}