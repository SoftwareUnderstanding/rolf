{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1906.08237"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/https-seyhan/BugAI",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-03-10T13:30:06Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-25T08:01:20Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9470641141826635,
        0.9354325439469069
      ],
      "excerpt": "Deep Learning Models (Long Short Term Memory (LSTM), Recurrent Neural Networks (RNN), Convolutional Neural Networks (CNN) for AI based Bug prediction. \nIn addition, other Machine Learning models such as SVM, oneClassClassifier, isolationForest are also employed. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9961311177713795,
        0.9679031823120354
      ],
      "excerpt": "The anomaly score of each sample is called Local Outlier Factor. It measures the local deviation of density of a given sample with respect to its neighbors. It is local in that the anomaly score depends on how isolated the object is with respect to the surrounding neighborhood. More precisely, locality is given by k-nearest neighbors, whose distance is used to estimate the local density. By comparing the local density of a sample to the local densities of its neighbors, one can identify samples that have a substantially lower density than their neighbors. These are considered outliers. (https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html) \nA random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the max_samples parameter if bootstrap=True (default), otherwise the whole dataset is used to build each tree. (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8595808776448169
      ],
      "excerpt": "Return the anomaly score of each sample using the IsolationForest algorithm \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8884432348533124,
        0.9514880711682708,
        0.8880699507423508,
        0.9957529030486397,
        0.9910878901475272
      ],
      "excerpt": "Since recursive partitioning can be represented by a tree structure, the number of splittings required to isolate a sample is equivalent to the path length from the root node to the terminating node. \nThis path length, averaged over a forest of such random trees, is a measure of normality and our decision function. \nRandom partitioning produces noticeably shorter paths for anomalies. Hence, when a forest of random trees collectively produce shorter path lengths for particular samples, they are highly likely to be anomalies. (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html) \nA dummy classifier is a type of classifier which does not generate any insight about the data and classifies the given data using only simple rules. The classifier\u2019s behaviour is completely independent of the training data as the trends in the training data are completely ignored and instead uses one of the strategies to predict the class label. \nIt is used only as a simple baseline for the other classifiers i.e. any other classifier is expected to perform better on the given dataset. It is especially useful for datasets where are sure of a classImbalance. It is based on the philosophy that any analytic approach for a classification problem should be better than a random guessing approach. (https://www.geeksforgeeks.org/ml-dummy-classifiers-using-sklearn/) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.979331616217918,
        0.9797138141843956,
        0.9847634340459537
      ],
      "excerpt": "One common way of performing outlier detection is to assume that the regular data come from a known distribution (e.g. data are Gaussian distributed). From this assumption, we generally try to define the \u201cshape\u201d of the data, and can define outlying observations as observations which stand far enough from the fit shape. \nThe scikit-learn provides an object covariance.EllipticEnvelope that fits a robust covariance estimate to the data, and thus fits an ellipse to the central data points, ignoring points outside the central mode. \nFor instance, assuming that the inlier data are Gaussian distributed, it will estimate the inlier location and covariance in a robust way (i.e. without being influenced by outliers). The Mahalanobis distances obtained from this estimate is used to derive a measure of outlyingness. This strategy is illustrated below. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9520740996286093,
        0.9641969542906381,
        0.9211091487853439,
        0.9990580718082324
      ],
      "excerpt": "Long Short Term Memory networks \u2013 usually just called \u201cLSTMs\u201d \u2013 are a special kind of RNN, capable of learning long-term dependencies. They were introduced by Hochreiter & Schmidhuber (1997), and were refined and popularized by many people in following work.1 They work tremendously well on a large variety of problems, and are now widely used. \nLSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn! \nAll recurrent neural networks have the form of a chain of repeating modules of neural network. In standard RNNs, this repeating module will have a very simple structure, such as a single tanh layer. (https://colah.github.io/posts/2015-08-Understanding-LSTMs/) \nWith the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Deep Learning Models (Long Short Term Memory (LSTM), Recurrent Neural Networks (RNN), Convolutional Neural Networks (CNN) for AI based Bug prediction",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/https-seyhan/BugAI/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sun, 26 Dec 2021 05:06:15 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/https-seyhan/BugAI/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "https-seyhan/BugAI",
    "technique": "GitHub API"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/https-seyhan/BugAI/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "HTML"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "BugAI",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "BugAI",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "https-seyhan",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/https-seyhan/BugAI/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 5,
      "date": "Sun, 26 Dec 2021 05:06:15 GMT"
    },
    "technique": "GitHub API"
  }
}