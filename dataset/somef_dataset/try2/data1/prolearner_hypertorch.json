{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1909.04630",
      "https://arxiv.org/abs/1909.01377",
      "https://arxiv.org/abs/2006.16218"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you use this code, please cite [our paper](https://arxiv.org/abs/2006.16218)\n```\n@inproceedings{grazzi2020iteration,\n  title={On the Iteration Complexity of Hypergradient Computation},\n  author={Grazzi, Riccardo and Franceschi, Luca and Pontil, Massimiliano and Salzo, Saverio},\n  journal={Thirty-seventh International Conference on Machine Learning (ICML)},\n  year={2020}\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{grazzi2020iteration,\n  title={On the Iteration Complexity of Hypergradient Computation},\n  author={Grazzi, Riccardo and Franceschi, Luca and Pontil, Massimiliano and Salzo, Saverio},\n  journal={Thirty-seventh International Conference on Machine Learning (ICML)},\n  year={2020}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/prolearner/hypertorch",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-04-23T15:56:19Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-08T20:51:59Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9012671888734881,
        0.9225553068779601,
        0.9688033715739299,
        0.9885955192573002,
        0.9835481333572843
      ],
      "excerpt": "is called the outer objective (e.g. the validation loss). \n*  is called the fixed point map (e.g. a gradient descent step or the state update function in a recurrent model) \n* finding the solution of the fixed point equation  is referred to as the inner problem. This can be solved by repeatedly applying the fixed point map or using a different inner algorithm. \nSee this notebbook, where we show how to compute the hypergradient to optimize the regularization parameters of a simple logistic regression model. \nexamples/iMAML.py shows an implementation of the method described in the paper Meta-learning with implicit gradients. The code uses higher to get stateless version of torch nn.Module-s and torchmeta for meta-dataset loading and minibatching. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8475690694680337,
        0.8986026640164422
      ],
      "excerpt": "- a list of tensors representing the inner variables (models' weights); \n- another list of tensors for the outer variables (hyperparameters/meta-learner paramters); \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9290463897220291
      ],
      "excerpt": "the inner problem. This allows to optimize the inner solver parameters such as the learning rate and momentum. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.966646718346744,
        0.9806380608697852
      ],
      "excerpt": "- reverse_unroll: computes the approximate hypergradient by unrolling the entire computational graph of the update dynamics for solving the inner problem. The method is essentially a wrapper for standard backpropagation. IMPORTANT NOTE: the weights must be non-leaf tensors obtained through the application of \"PyThorch differentiable\" update dynamics (do not use built-in optimizers!). NOTE N2.: this method is memory hungry! \n- reverse: computes the hypergradient as above but uses less memory. It uses the trajectory information and recomputes all other necessary intermediate variables in the backward pass. It requires the list of past weights and the list of callable update mappings applied during the inner optimization. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8264020469441534,
        0.9698354755828192,
        0.9774382279020079
      ],
      "excerpt": " * Using an approximate solution to the inner problem instead of the true one. \n * Computing an approximate solution to the linear system (I-J)x_star = b, where J and  b are respectively the transpose of the jacobian of the fixed point map and the gradient of the outer objective both w.r.t the inner variable and computed on the approximate solution to the inner problem. \nSince computing and storing J is usually unfeasible, these methods exploit torch.autograd to compute the Jacobian-vector product Jx efficiently. Additionally, they do not require storing the trajectory of the inner solver, thus providing a potentially large memory advantage over iterative differentiation. These methods are not suited to optimize the parameters of the inner solver like the learning rate. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/prolearner/hypertorch/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 10,
      "date": "Sun, 26 Dec 2021 18:37:07 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/prolearner/hypertorch/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "prolearner/hypertorch",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/prolearner/hypertorch/master/examples/logistic_regression.ipynb",
      "https://raw.githubusercontent.com/prolearner/hypertorch/master/examples/Equilibrium%20models%20%28RNN-style%20model%20on%20MNIST%29.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Requires python 3 and PyTorch version >= 1.4.\n\n```\ngit clone git@github.com:prolearner/hypergrad.git\ncd hypergrad\npip install .\n```\n`python setup.py install` would also work.\n\n",
      "technique": "Header extraction"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/prolearner/hypertorch/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 Riccardo Grazzi\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "HyperTorch",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "hypertorch",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "prolearner",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/prolearner/hypertorch/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 65,
      "date": "Sun, 26 Dec 2021 18:37:07 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Hypergadients are useful to perform\n- gradient-based hyperparamter optimization\n- meta-learning\n- training models that use an internal state (some types of RNNs and GNNs, Deep Equilibrium Networks, ...) \n\n",
      "technique": "Header extraction"
    }
  ]
}