{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This is an open source project (formerly named **Listen, Attend and Spell - PyTorch Implementation**) for end-to-end ASR by [Tzu-Wei Sung](https://github.com/WindQAQ) and me.\nImplementation was mostly done with Pytorch, the well known deep learning toolkit.\n\nThe end-to-end ASR was based on Listen, Attend and Spell<sup>[1](#Reference)</sup>. Multiple techniques proposed recently were also implemented, serving as additional plug-ins for better performance. For the list of techniques implemented, please refer to the [highlights](#Highlights), [configuration](config/) and [references](#Reference).\n\nFeel free to use/modify them, any bug report or improvement suggestion will be appreciated. If you find this project helpful for your research, please do consider to cite [our paper](#Citation), thanks!\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "To test a model, run the following command\n```\npython3 main.py --config <path of config file> --test --njobs <int>\n```\n***Please notice that the decoding is performed without batch processing, use more workers to speedup at the cost of using more RAM.***\nBy default, recognition result will be stored at `result/<name>/` as two csv files with auto-naming according to the decoding config file. `output.csv` will store the best hypothesis provided by ASR and `beam.csv` will recored the top hypotheses during beam search. The result file may be evaluated with `eval.py`. For example, test the example ASR trained on LibriSpeech and check performance with\n```\npython3 main.py --config config/libri/decode_example.yaml --test --njobs 8\n#: Check WER/CER\npython3 eval.py --file result/asr_example_sd0_dev_output.csv\n```\n\nMost of the options work similar to training phase except the followings:\n\n| Options | Description |\n|---------|-------------|\n| test    | *Must be enabled*|\n| config  | Path to the decoding config file.|\n| outdir  | Path to store decode result.|\n| njobs   | Number of threads used for decoding, very important in terms of efficiency. Large value equals fast decoding yet RAM/GPU RAM expensive.    |\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "- Parts of the implementation refer to [ESPnet](https://github.com/espnet/espnet), a great end-to-end speech processing toolkit by Watanabe *et al*.\n- Special thanks to [William Chan](http://williamchan.ca/), the first author of LAS, for answering my questions during implementation.\n- Thanks [xiaoming](https://github.com/lezasantaizi), [Odie Ko](https://github.com/odie2630463), [b-etienne](https://github.com/b-etienne), [Jinserk Baik](https://github.com/jinserk) and [Zhong-Yi Li](https://github.com/Chung-I) for identifying several issues in our implementation. \n\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1508.01211v2",
      "https://arxiv.org/abs/1506.07503",
      "https://arxiv.org/abs/1609.06773",
      "https://arxiv.org/abs/1706.02737"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\n@inproceedings{liu2019adversarial,\n  title={Adversarial Training of End-to-end Speech Recognition Using a Criticizing Language Model},\n  author={Liu, Alexander and Lee, Hung-yi and Lee, Lin-shan},\n  booktitle={Acoustics, Speech and Signal Processing (ICASSP)},\n  year={2019},\n  organization={IEEE}\n}\n\n@misc{alex2019sequencetosequence,\n    title={Sequence-to-sequence Automatic Speech Recognition with Word Embedding Regularization and Fused Decoding},\n    author={Alexander H. Liu and Tzu-Wei Sung and Shun-Po Chuang and Hung-yi Lee and Lin-shan Lee},\n    year={2019},\n    eprint={1910.12740},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "1. [Listen, Attend and Spell](https://arxiv.org/abs/1508.01211v2), W Chan *et al.*\n2. [Neural Machine Translation of Rare Words with Subword Units](http://www.aclweb.org/anthology/P16-1162), R Sennrich *et al.*\n3. [Attention-Based Models for Speech Recognition](https://arxiv.org/abs/1506.07503), J Chorowski *et al*.\n4. [Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks](https://www.cs.toronto.edu/~graves/icml_2006.pdf), A Graves *et al*.\n5. [Joint CTC-Attention based End-to-End Speech Recognition using Multi-task Learning](https://arxiv.org/abs/1609.06773), S Kim *et al.* \n6.  [Advances in Joint CTC-Attention based End-to-End Speech Recognition with a Deep CNN Encoder and RNN-LM](https://arxiv.org/abs/1706.02737), T Hori *et al.* \n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{alex2019sequencetosequence,\n    title={Sequence-to-sequence Automatic Speech Recognition with Word Embedding Regularization and Fused Decoding},\n    author={Alexander H. Liu and Tzu-Wei Sung and Shun-Po Chuang and Hung-yi Lee and Lin-shan Lee},\n    year={2019},\n    eprint={1910.12740},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{liu2019adversarial,\n  title={Adversarial Training of End-to-end Speech Recognition Using a Criticizing Language Model},\n  author={Liu, Alexander and Lee, Hung-yi and Lee, Lin-shan},\n  booktitle={Acoustics, Speech and Signal Processing (ICASSP)},\n  year={2019},\n  organization={IEEE}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8107048990510205
      ],
      "excerpt": "Training End-to-end ASR  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9324223832382442
      ],
      "excerpt": "Speech Recognition with End-to-end ASR (i.e. Decoding) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mnm-rnd/elsa-voice-asr",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-07-06T12:18:40Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-01T00:03:00Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8203804197873813,
        0.9968029537584643
      ],
      "excerpt": "On-the-fly feature extraction using torchaudio as backend \nCharacter/subword<sup>2</sup>/word encoding of text \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9276126263383156,
        0.8982189075231126
      ],
      "excerpt": "Seq2seq ASR with different types of encoder/attention<sup>3</sup> \nCTC-based ASR<sup>4</sup>, which can also be hybrid<sup>5</sup> with the former \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.851111409930682
      ],
      "excerpt": "Speech Recognition with End-to-end ASR (i.e. Decoding) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9274242196685012
      ],
      "excerpt": "RNN language model training and joint decoding for ASR<sup>6</sup> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9048167925340842
      ],
      "excerpt": "For more details, please refer to python3 util/generate_vocab_file.py -h. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8039392602573314
      ],
      "excerpt": "Pure CTC training / CTC beam decode bug (out-of-candidate) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Implementation of the Traditional ASR for the Elsa Health ",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mnm-rnd/elsa-voice-asr/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Thu, 23 Dec 2021 12:19:35 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/mnm-rnd/elsa-voice-asr/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "mnm-rnd/elsa-voice-asr",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "All the parameters related to training/decoding will be stored in a yaml file. Hyperparameter tuning and experiments can be managed easily this way. See [documentation and examples](config/) for the exact format. **Note that the example configs provided were not fine-tuned**, you may want to write your own config for best performance.\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8700404180804112
      ],
      "excerpt": "You may checkout some example log files with TensorBoard by downloading them from coming soon \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.862891317416015
      ],
      "excerpt": "|cudnn-ctc| Use CuDNN as the backend of PyTorch CTC. Unstable, see this issue, not sure if solved in latest Pytorch with cudnn version > 7.6| \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9479397847420237
      ],
      "excerpt": "  <img src=\"tests/sample_data/demo.png\" width=\"570\" height=\"300\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8280524513565667
      ],
      "excerpt": "The subword model is trained with sentencepiece. As for character/word model, you have to generate the vocabulary file containing the vocabulary line by line. You may also use util/generate_vocab_file.py so that you only have to prepare a text file, which contains all texts you want to use for generating the vocabulary file or subword model. Please update data.text.* field in the config file if you want to change the mode or vocabulary file. For subword model, use the one ended with .model as vocab_file. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.922956028178308,
        0.8554138574456002
      ],
      "excerpt": "python3 util/generate_vocab_file.py --input_file TEXT_FILE \\ \n                                    --output_file OUTPUT_FILE \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9285065137266004
      ],
      "excerpt": "python3 main.py -h \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8884863022464559
      ],
      "excerpt": "python3 main.py --config config/libri/asr_example.yaml \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8884863022464559
      ],
      "excerpt": "python3 main.py --config config/libri/lm_example.yaml --lm \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8514726589823334
      ],
      "excerpt": "All settings will be parsed from the config file automatically to start training, the log file can be accessed through TensorBoard. Please notice that the error rate reported on the TensorBoard is biased (see issue #10), you should run the testing phase in order to get the true performance of model.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8545825642922474,
        0.8043247475852672
      ],
      "excerpt": "| logdir  | Path to store training logs (log files for tensorboard), default log/.| \n| ckpdir  | The directory to store model, default ckpt/.| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8056309389682001
      ],
      "excerpt": "| lm      | Switch to rnnlm training mode. | \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/mnm-rnd/elsa-voice-asr/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2017 XenderLiu\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "End-to-end Automatic Speech Recognition Systems - PyTorch Implementation",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "elsa-voice-asr",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "mnm-rnd",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mnm-rnd/elsa-voice-asr/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Python 3\n- Computing power (high-end GPU) and memory space (both RAM/GPU's RAM) is **extremely important** if you'd like to train your own model.\n- Required packages and their use are listed [requirements.txt](requirements.txt).\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Thu, 23 Dec 2021 12:19:35 GMT"
    },
    "technique": "GitHub API"
  }
}