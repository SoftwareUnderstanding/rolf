{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2005.01770",
      "https://arxiv.org/abs/1512.03385\" >https://arxiv.org/abs/1512.03385</a> <br><br>\n \n Paradrop Traffic Camera\n=======================\n\nThis demo chute uses an OpenCV cascade classifier to detect and count\nvehicles in images from a camera mounted on a street pole.\n\n![Screenshot](screenshot.png",
      "https://arxiv.org/abs/1512.03385</a> <br><br>\n \n Paradrop Traffic Camera\n=======================\n\nThis demo chute uses an OpenCV cascade classifier to detect and count\nvehicles in images from a camera mounted on a street pole.\n\n![Screenshot](screenshot.png"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9938206540624737
      ],
      "excerpt": "Image Processing and Computer Vision \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9554441738822752
      ],
      "excerpt": "Rules Violation Video Representation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8456806903995955
      ],
      "excerpt": "   dst(x, y) = maxVal if scr(x, y) &gt; thresh else 0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8740892610239802
      ],
      "excerpt": " You can download the dataset via the link below.<br><br> <a href=\"https://github.com/OlafenwaMoses/Traffic-Net/releases/tag/1.0\" >https://github.com/OlafenwaMoses/Traffic-Net/releases/tag/1.0</a>  <br><br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9913433916740021
      ],
      "excerpt": "  <b><a href=\"https://github.com/OlafenwaMoses/Traffic-Net/releases/download/1.0/trafficnet_resnet_model_ex-055_acc-0.913750.h5\" >https://github.com/OlafenwaMoses/Traffic-Net/releases/download/1.0/trafficnet_resnet_model_ex-055_acc-0.913750.h5</a></b><br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9713679853829249
      ],
      "excerpt": "<b>>>> Video & Prediction Results</b> <br><br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9996921519245323,
        0.9999582833911902
      ],
      "excerpt": "Kaiming H. et al, Deep Residual Learning for Image Recognition <br> \n <a href=\"https://arxiv.org/abs/1512.03385\" >https://arxiv.org/abs/1512.03385</a> <br><br> \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rahulchaurasiya1/cautious-waddle",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-06-18T18:41:54Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-12-09T17:49:32Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The increasing number of cars in cities can cause high volume of traffic, and implies that traffic violations become more critical nowadays in Bangladesh and also around the world. This causes severe destruction of property and more accidents that may endanger the lives of the people. To solve the alarming problem and prevent such unfathomable consequences, traffic violation detection systems are needed. For which the system enforces proper traffic regulations at all times, and apprehend those who does not comply. A traffic violation detection system must be realized in real-time as the authorities track the roads all the time. Hence, traffic enforcers will not only be at ease in implementing safe roads accurately, but also efficiently; as the traffic detection system detects violations faster than humans. This system can detect most common three types of traffic violation in real-time which are signal violation, parking violation and wrong direction violation. A user friendly graphical interface is associated with the system to make it simple for the user to operate the system, monitor traffic and take action against the violations of traffic rules.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9931984531704222
      ],
      "excerpt": "This repository contains the working implementation of the paper link. A simple algorithm for traffic density estimation using image processing and machine learning. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.882763341834874,
        0.8029961407550836
      ],
      "excerpt": "2) save_HOG_LBP.py : saves the HOG(Histogram of Oriented Gradients) and LBP(Local Binary Pattern) features into pickle file \n3) classifier.py : trains the SVM classifier and saves the model into pickle \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8164996843936199,
        0.8576171041696997
      ],
      "excerpt": "Initial Release (Everything is not completed) \n:star: Please star this project. It helps a lot. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8473224818617674
      ],
      "excerpt": "This is a software for practice of developing a system from completely scratch. Understanding this will help a lot in system development and basic structure of a system along with computer vision, GUI with python library PyQt and basic opencv. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8146672464981137,
        0.979819100225587
      ],
      "excerpt": "This project is made for the third year second semester System Development(CSE-3200) course. \nThe goal of the project is to automate the traffic rules violation detection system and make it ease for the traffic police department to monitor the traffic and take action against the violated vehicle owner in a fast and efficient way. Detecting and tracking the vehicle and their activities accurately is the main priority of the system. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9264628598347279,
        0.8967356437549718,
        0.908925214220865,
        0.9929926434522777
      ],
      "excerpt": "First the CCTV camera footage from the road side is sent to the system. Vehicles are detected from the footage. Tracking the activity of vehicles system determines if their is any any violation or not. Different types of violations have different algorithms to determine the violation. A system flowchart 1 shows how the system works. \nThe Graphical User Interface (GUI) makes the system interactive for user to use. User can monitor the traffic footage and get the alert of violation with the captured vehicle image. User can take further action using the GUI. \nGrayscaling and blurring  \n   As the part of preprocessing the input frame got from the CCTV footage, the image is grayscaled and blurred with Gaussian Blur method. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9456928329889872
      ],
      "excerpt": "   Background subtraction method is used to subtract the current frame from the reference frame to get the desired object\u2019s area. equation (1) shows the method. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.811266144932007,
        0.9642496990657746
      ],
      "excerpt": " Binary Threshold  \n   Binarization method is used to remove all the holes and noises from the frame and get the desired object area accurately. equation (2) shows how the binary threshold works. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9139830922044209,
        0.9839440343046428
      ],
      "excerpt": "   After getting the thresholded image, it is dilated to fill the holes and the contour is found from the image. drawing rectangle box over the contours desired moving objects are taken. \nFrom the preprocessed image moving objects are extracted. A vehicle classification model is used to classify those moving objects into three class - Car, Motobike and Non-vehicle. The classifier model is built with mobilenet v1 neural network architecture. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9394972429934794
      ],
      "excerpt": "Transfer learning approach is used to training the model with our dataset.The dataset consists of 500  images per class. The training parameters are mentioned in table (2). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8467046706318527,
        0.9277793655813001,
        0.9920853931610472
      ],
      "excerpt": "Parking violation: if a vehicle stands still in no parking zone for a predefined time, it is detected as a parking violation. \nDirection violation: when a vehicle comes from a wrong direction,it is detected by tracking the vehicle. The direction of the vehicle is determined using its current position and previous few positions. \nWe have used SQLite database with python to manage the whole data of our application. Here, in the relational database we have used BCNF of 5 tables. The tables are: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8753637559349008,
        0.9569682731099897,
        0.8373792556908093,
        0.9487613534465109,
        0.9682586427928893,
        0.9144861340576604,
        0.9645383294876306
      ],
      "excerpt": " Here are the descriptions of each tables: \nThis table will hold the recorded cars by the camera. A car entity is a car with a unique identifier(id), color(color), license-number of the car(license), where the car is first sighted (first_sighted), an image of the license number (license_image), an image of the car(car_image), number of rules broken so far(num_rules_broken) and the owner of the car (owner). \nThis table holds all the rules, their description(name) and fine for breaking that rule (fine). \nCamera table holds a unique identifier for the camera(id), location description(location), the longitude(coordinate_x) and the latitude(coordinate_y) of the location of the camera, where the camera will feed its data video(feed) and in which group the camera is in(group). \nThis table simply holds the unique group names of the camera groups(name). Violations: This table takes all the ids of other tables as foreign key and creates a semantic record like this: A car with this id has broken that rule at this time, which is captured by this camera. \nOpenCV computer vision library is used in  Python for image processing purpose. For implementing the vehicle classifier with ,  Tensorflow machine learning framework is used. \nThe user interface has all the options needed for the administration and other debugging purpose so that, we do not need to edit code for any management. For example, if we need to add some sample cars or camera in the database, we can do it with the menu item (see fig-3). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9645698602798694
      ],
      "excerpt": "Primarily, for the start of the project usage, the administrator needs to add a camera with the menu item. In the way, the administrator can add the location of the camera, the feed file for the camera. Here the feed file is installed by the camera module over the internet. We have used Linux file sharing pattern for getting the video from the camera, where the camera will feed the given file to the server, and the server will take the feed file to process and detect violation. Also the X and Y coordinate(fig-3) of the camera location can be saved by the admin. This is done for future use, when we will try to use a map for locating the cameras with ease. Also the admin need to specify some rules with a JSON file for the camera. For example, the camera is used for cross road on red line violation, or is used for wrong place parking detection etc. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9152902494904458
      ],
      "excerpt": "The user has many other objects to insert into the database. The admin can add the following entities in the graphical user interface: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9835404835465774,
        0.8854569563630228,
        0.8025996112063846
      ],
      "excerpt": "The GUI is made mainly for this purpose that, there will always be a supervisor for a group of cameras. He can see the list of rule violations and can see details of the cars that violated the rules (fig-8). If he clicks on the detail button, a new window will appear where the user will be able to file the report or send/print ticket for the car owner. \nFigure 8: details of rule violation \nAlso the admin/user can delete the records if he gets a false positive. But there will never a record deleted. The database has a marker of which file have been archived. If we want to retrieve a record from the deleted once, then the admin needs to go to the archive window. There he can restore any record he wants. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9263927164824819
      ],
      "excerpt": "There are currently 3 rules we are concerned with. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9502123439628698
      ],
      "excerpt": "For Signal Violation, We have used a straight line in the picture. When the traffic light is red and a car is crossing the straight line, a picture of that car is registered in the database along with some environmental values. The user can see in the live preview which car are being detected real time and tested if they are crossing the line. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9955133390118602
      ],
      "excerpt": "For Parking violation, we have prefigured a rectangle, which is the restricted area for car parking. If there is a vehicle in the rectangle for more than a predefined time, then a image with other environmental values is being registered to the database. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8702099987257041
      ],
      "excerpt": "For direction violation detection, some lines are drawn to divide into regions. Then when a car moves from one region to another, its direction is measured. If the direction is wrong, then it is registered as previous. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8921020481886704
      ],
      "excerpt": "Traffic-Net is a dataset containing images of dense traffic, sparse traffic, accidents and burning vehicles. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9210826861024758
      ],
      "excerpt": " to detect traffic conditions and provide real-time monitoring, analytics and alerts. This is part of <a href=\"https://deepquestai.com\" >DeepQuest AI</a>'s to train machine learning systems to  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8943865837179198
      ],
      "excerpt": "This is the first release of the Traffic-Net dataset. It contains 4,400 images that span cover 4 classes. The classes \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8577302739126875
      ],
      "excerpt": "<b> Dense Traffic </b> <br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8757294353951907
      ],
      "excerpt": " The <b>Traffic-Net</b> dataset is provided for download in the <b>release</b> section of this repository. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9807400116439803,
        0.8121770900651457
      ],
      "excerpt": "  and perform prediction using a pretrained model (also using <b>ResNet50</b>) provided in the release section of this repository. \n  The python codebase is contained in the <b><a href=\"traffic_net.py\" >traffic_net.py</a></b> file and the model class labels for prediction is also provided the  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.93249757058849
      ],
      "excerpt": "   This pre-trained model was trained for 60 epochs only, but it achieved over 91% accuracy on 800 test images. You can see the prediction results on new images that were not part of the dataset in the Prediction Results section below. More experiments will enhance the accuracy of the model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9532947431854848
      ],
      "excerpt": "Click below to watch the video demonstration of the trained model at work. <br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8084373883145721
      ],
      "excerpt": "This demo chute uses an OpenCV cascade classifier to detect and count \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.955834126844447
      ],
      "excerpt": "The default configuration of this chute requires the Paradrop node to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "traffic management ",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rahulchaurasiya1/cautious-waddle/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 21 Dec 2021 17:46:13 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/rahulchaurasiya1/cautious-waddle/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "rahulchaurasiya1/cautious-waddle",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Log into your ThingSpeak account.\n2. Create a channel.\n3. Get the API write key for the channel.\n4. Paste the value in `sample.py`\n5. Add the `analysis.m` file to the channel.\n6. Once the script is running on the Pi (directions below) the data can be seen on the ThingSpeak dashboard.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9329778602568604
      ],
      "excerpt": " You can download the dataset via the link below.<br><br> <a href=\"https://github.com/OlafenwaMoses/Traffic-Net/releases/tag/1.0\" >https://github.com/OlafenwaMoses/Traffic-Net/releases/tag/1.0</a>  <br><br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8548526000907585
      ],
      "excerpt": "  <b><a href=\"https://github.com/OlafenwaMoses/Traffic-Net/releases/download/1.0/trafficnet_resnet_model_ex-055_acc-0.913750.h5\" >https://github.com/OlafenwaMoses/Traffic-Net/releases/download/1.0/trafficnet_resnet_model_ex-055_acc-0.913750.h5</a></b><br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9717731475190136,
        0.9904925932785111,
        0.9979681403856073,
        0.9970302192151689
      ],
      "excerpt": "Running the experiment or prediction requires that you have Tensorflow, and Keras, OpenCV and ImageAI installed. You can install this dependencies via the commands below. \n<br><span><b>- Tensorflow 1.4.0 (and later versions)  </b>      <a href=\"https://www.tensorflow.org/install/install_windows\" style=\"text-decoration: none;\" > Install</a></span> or install via pip <pre> pip3 install --upgrade tensorflow </pre>  \n<span><b>- OpenCV  </b>        <a href=\"https://pypi.python.org/pypi/opencv-python\" style=\"text-decoration: none;\" >Install</a></span> or install via pip <pre> pip3 install opencv-python </pre>  \n<span><b>- Keras 2.x  </b>     <a href=\"https://keras.io/#installation\" style=\"text-decoration: none;\" >Install</a></span> or install via pip <pre> pip3 install keras </pre>  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.913974420820379
      ],
      "excerpt": "   <span>      <pre>pip3 install imageai </pre></span> <br><br> <br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8829542152450996,
        0.9548743732358185
      ],
      "excerpt": "it is likely we installed paradrop-imserve for you. Otherwise, you will \nneed to install paradrop-imserve or change the IMAGE_SOURCE_URL \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8695026985929956,
        0.9507229266754984,
        0.999833231880651
      ],
      "excerpt": "To install paradrop-imserve, connect to the node using SSH and run \nthe following command. \nsnap install paradrop-imserve \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9298169606769424
      ],
      "excerpt": "<img src=\"images/overall-usage.png\" alt=\"Overal Usage\" align=\"right\" width=\"250\" /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8507865069687717
      ],
      "excerpt": "   dst(I) = saturate(|scr1(I) \u2212 scr2(I)|) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9295932733615537
      ],
      "excerpt": "<img src=\"images/traffic_net.jpg\" /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8405210537621255
      ],
      "excerpt": "<b> Sparse Traffic </b> <br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9295932733615537
      ],
      "excerpt": "  <img src=\"images/1.jpg\" /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9295932733615537
      ],
      "excerpt": "<img src=\"images/2.jpg\" /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9295932733615537
      ],
      "excerpt": "<img src=\"images/3.jpg\" /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9295932733615537
      ],
      "excerpt": "<img src=\"images/4.jpg\" /> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/rahulchaurasiya1/cautious-waddle/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "JavaScript",
      "HTML"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\r\\n\\r\\nCopyright (c) 2019 MOSES OLAFENWA\\r\\n\\r\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\r\\nof this software and associated documentation files (the \"Software\"), to deal\\r\\nin the Software without restriction, including without limitation the rights\\r\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\r\\ncopies of the Software, and to permit persons to whom the Software is\\r\\nfurnished to do so, subject to the following conditions:\\r\\n\\r\\nThe above copyright notice and this permission notice shall be included in all\\r\\ncopies or substantial portions of the Software.\\r\\n\\r\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\r\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\r\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\r\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\r\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\r\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\r\\nSOFTWARE.\\r\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Cautious Waddle \n\n# Smart-Traffic-Junction\n\nThis repository contains the working implementation of the paper [link](https://arxiv.org/abs/2005.01770). A simple algorithm for traffic density estimation using image processing and machine learning.\n\n## Dataset\n\nThe dataset was created using <a href=\"http://www.eecs.qmul.ac.uk/~sgg/QMUL_Junction_Datasets/Junction2/Junction2.html\">QMUL junction 2</a> video. We manually sorted the rois of the <a href=\"dataset.zip\">dataset</a>.\n\n## Files\n1) save_rois.py : saves ROIs (small blocks of image) from the QMUL junction 2 video <br>\n2) save_HOG_LBP.py : saves the HOG(Histogram of Oriented Gradients) and LBP(Local Binary Pattern) features into pickle file\n3) classifier.py : trains the SVM classifier and saves the model into pickle\n4) predictor.py : reads the video and predicts the output on the image\n\n## System Architecture\n![System Architecture of smart traffic junction](https://github.com/DevashishPrasad/Smart-Traffic-Junction/blob/master/SystemArch.png)\n\n\n# Traffic Rules Violation Detection with Computer Vision\n\n> Initial Release (Everything is not completed)\n\n:star: Please star this project. It helps a lot.\n\n![Overal use case](images/main.gif)\n\n\n![Dark theme Screen shot](images/main_black.png)\n\n## TL;DR\n\n<img src=\"images/overall-usage.png\" alt=\"Overal Usage\" align=\"right\" width=\"250\" />\nThis is a software for practice of developing a system from completely scratch. Understanding this will help a lot in system development and basic structure of a system along with computer vision, GUI with python library PyQt and basic opencv.\n\nGo [here](#quick-starting-the-project) if you don't have time.\n\n## Table of content\n\n- [TL;DR](#TL;DR)\n- [Motivation](#motivation)\n- [Introduction](#introduction)\n- [Objective](#objective)\n- [Quick Starting the project](#quick-starting-the-project)\n- [System Overview](#system-overview)\n- [Methodology](#methodology)\n  - [Image Processing](#image-processing)\n  - [Vehicle Classification](#vehicle-classification)\n  - [Violation Detection](#violation-detection)\n  - [Database Structure](#database-structure)\n- [Implementation](#implementation)\n  - [Image Processing and Computer Vision](#image-processing-and-computer-vision)\n  - [Graphical User Interface](#graphical-user-interface-gui)\n  - [Rules Violation Video Representation](#rules-violation-video-representation-in-ui)\n- [Contributing](#contributing)\n- [Links and References](#links-and-references)\n- [Author](#author)\n- [Licensing](#licensing)\n\n## Motivation\n\nThis project is made for the third year second semester System Development(CSE-3200) course.\n\n## Introduction\n\nThe increasing number of cars in cities can cause high volume of traffic, and implies that traffic violations become more critical nowadays in Bangladesh and also around the world. This causes severe destruction of property and more accidents that may endanger the lives of the people. To solve the alarming problem and prevent such unfathomable consequences, traffic violation detection systems are needed. For which the system enforces proper traffic regulations at all times, and apprehend those who does not comply. A traffic violation detection system must be realized in real-time as the authorities track the roads all the time. Hence, traffic enforcers will not only be at ease in implementing safe roads accurately, but also efficiently; as the traffic detection system detects violations faster than humans. This system can detect most common three types of traffic violation in real-time which are signal violation, parking violation and wrong direction violation. A user friendly graphical interface is associated with the system to make it simple for the user to operate the system, monitor traffic and take action against the violations of traffic rules.\n\n## Objective\n\nThe goal of the project is to automate the traffic rules violation detection system and make it ease for the traffic police department to monitor the traffic and take action against the violated vehicle owner in a fast and efficient way. Detecting and tracking the vehicle and their activities accurately is the main priority of the system.\n\n## Quick starting the project\n\n1. `git clone https://github.com/rahatzamancse/EyeTask.git`\n2. Install required python dependencies from `requirements.txt` into your python virtual environment. (`pip install -r requirements.txt`)\n3. `python main.py`\n\n## System Overview\n\n![System Overview](images/system.png)\n\nThe System consists of two main components -\n\n* Vehicle detection model and\n* A graphical user interface (GUI)\n\nFirst the CCTV camera footage from the road side is sent to the system. Vehicles are detected from the footage. Tracking the activity of vehicles system determines if their is any any violation or not. Different types of violations have different algorithms to determine the violation. A system flowchart 1 shows how the system works.\nThe Graphical User Interface (GUI) makes the system interactive for user to use. User can monitor the traffic footage and get the alert of violation with the captured vehicle image. User can take further action using the GUI.\n\n## Methodology\n\n### Image Processing\n\n1. ** Grayscaling and blurring **\n   As the part of preprocessing the input frame got from the CCTV footage, the image is grayscaled and blurred with Gaussian Blur method.\n\n2. ** Background Subtraction **\n   Background subtraction method is used to subtract the current frame from the reference frame to get the desired object\u2019s area. equation (1) shows the method.\n   `dst(I) = saturate(|scr1(I) \u2212 scr2(I)|)`\n\n3. ** Binary Threshold **\n   Binarization method is used to remove all the holes and noises from the frame and get the desired object area accurately. equation (2) shows how the binary threshold works.\n   `dst(x, y) = maxVal if scr(x, y) > thresh else 0`\n\n4. ** Dilation and find the contour **\n   After getting the thresholded image, it is dilated to fill the holes and the contour is found from the image. drawing rectangle box over the contours desired moving objects are taken.\n\n### Vehicle Classification\n\nFrom the preprocessed image moving objects are extracted. A vehicle classification model is used to classify those moving objects into three class - Car, Motobike and Non-vehicle. The classifier model is built with mobilenet v1 neural network architecture.\n\n![Mobilenet Architecture](images/mobilenetv1.png)\n\nFig: MobileNet Body Architecture.\n\n![Parameter Trainning](images/trainning.png)\n\nFig-2: Trainning hyperparameters.\n\nTransfer learning approach is used to training the model with our dataset.The dataset consists of 500  images per class. The training parameters are mentioned in table (2).\n\n### Violation detection\n\nAfter detecting the vehicles three violation cases arises-\n\n* Signal violation: if a vehicle crosses a predefined line on the road while there is red signal, it is detected as a signal violation.\n* Parking violation: if a vehicle stands still in no parking zone for a predefined time, it is detected as a parking violation.\n* Direction violation: when a vehicle comes from a wrong direction,it is detected by tracking the vehicle. The direction of the vehicle is determined using its current position and previous few positions.\n\n### Database Structure\n\nWe have used SQLite database with python to manage the whole data of our application. Here, in the relational database we have used BCNF of 5 tables. The tables are:\n\n1. Cars\n2. Rules\n3. Cameras\n4. Violations\n5. Groups\n\n![Database Scheme](images/schema.png)\n\n** Here are the descriptions of each tables: **\n\n##### Cars:\n\nThis table will hold the recorded cars by the camera. A car entity is a car with a unique identifier(id), color(color), license-number of the car(license), where the car is first sighted (first_sighted), an image of the license number (license_image), an image of the car(car_image), number of rules broken so far(num_rules_broken) and the owner of the car (owner).\n\n##### Rules:\n\nThis table holds all the rules, their description(name) and fine for breaking that rule (fine).\n\n##### Camera:\n\nCamera table holds a unique identifier for the camera(id), location description(location), the longitude(coordinate_x) and the latitude(coordinate_y) of the location of the camera, where the camera will feed its data video(feed) and in which group the camera is in(group).\n\n##### Camera_group:\n\nThis table simply holds the unique group names of the camera groups(name). Violations: This table takes all the ids of other tables as foreign key and creates a semantic record like this: A car with this id has broken that rule at this time, which is captured by this camera.\n\n## Implementation\n\n### Image Processing and Computer Vision\n\nOpenCV computer vision library is used in  Python for image processing purpose. For implementing the vehicle classifier with ,  Tensorflow machine learning framework is used.\n\n### Graphical User Interface (GUI)\n\nThe user interface has all the options needed for the administration and other debugging purpose so that, we do not need to edit code for any management. For example, if we need to add some sample cars or camera in the database, we can do it with the menu item (see fig-3).\n\n![Figure 2](images/fig2.png)\n\nFigure 2: Overall user interface view\n\nPrimarily, for the start of the project usage, the administrator needs to add a camera with the menu item. In the way, the administrator can add the location of the camera, the feed file for the camera. Here the feed file is installed by the camera module over the internet. We have used Linux file sharing pattern for getting the video from the camera, where the camera will feed the given file to the server, and the server will take the feed file to process and detect violation. Also the X and Y coordinate(fig-3) of the camera location can be saved by the admin. This is done for future use, when we will try to use a map for locating the cameras with ease. Also the admin need to specify some rules with a JSON file for the camera. For example, the camera is used for cross road on red line violation, or is used for wrong place parking detection etc.\n\n![Figure 3](images/fig3.png)\n\nFigure 3: Interface for adding camera entity\n\nActually, this is all mainly needed for starting up the system. After adding the camera, the software will automatically start detecting violations of traffic rules. After this, opening the camera by selecting it with the drop down menu, will fill the detection rules violations(fig-4).\n\n![Figure 4](images/fig4.png)\n\nFigure 4: List view of violation records\n\nThe user has many other objects to insert into the database. The admin can add the following entities in the graphical user interface:\n\n1. Camera (fig-3)\n2. Car (fig-5)\n3. Rule (fig-5)\n4. Violation (fig-5)\n\n![Figure 5](images/fig5.png)\n\nFigure 5: Adding items interface\n\nThe GUI is made mainly for this purpose that, there will always be a supervisor for a group of cameras. He can see the list of rule violations and can see details of the cars that violated the rules (fig-8). If he clicks on the detail button, a new window will appear where the user will be able to file the report or send/print ticket for the car owner.\n\n![Figure 8](images/detail.png)\n\nFigure 8: details of rule violation\n\nAlso the admin/user can delete the records if he gets a false positive. But there will never a record deleted. The database has a marker of which file have been archived. If we want to retrieve a record from the deleted once, then the admin needs to go to the archive window. There he can restore any record he wants.\nThe user can also search for a vehicle, with its license number, its color, or date of a rule violation. The license number has text prediction so the user will be sure while typing a license number that it exists.\n\n![Figure 9](images/search.png)\n\nFigure  9: Searching a car or rule violation\n\n### Rules violation video representation in UI\n\nThere are currently 3 rules we are concerned with.\n\n1. Signal Violation\n2. Parking Violation\n3. Direction Violation.\n\nFor Signal Violation, We have used a straight line in the picture. When the traffic light is red and a car is crossing the straight line, a picture of that car is registered in the database along with some environmental values. The user can see in the live preview which car are being detected real time and tested if they are crossing the line.\n\n![Figure 11](images/signal.png)\n\nFigure  11: Signal violation camera representation\n\nFor Parking violation, we have prefigured a rectangle, which is the restricted area for car parking. If there is a vehicle in the rectangle for more than a predefined time, then a image with other environmental values is being registered to the database.\n\n![Figure 12](images/parking.png)\n\nFigure  12: Parking violation camera representation\n\nFor direction violation detection, some lines are drawn to divide into regions. Then when a car moves from one region to another, its direction is measured. If the direction is wrong, then it is registered as previous.\n\n![Figure 13](images/direction.png)\n\nFigure  13: Direction violation camera representation\n\nLibraries used for graphical user interface:\n\n1. PyQt5\n2. QDarkStyle\n3. PyQtTimer\n\n# Traffic-Net\nTraffic-Net is a dataset containing images of dense traffic, sparse traffic, accidents and burning vehicles.\n<br><br>\n<img src=\"images/traffic_net.jpg\" />\n<hr>\n<b>Traffic-Net</b> is a dataset of traffic images, collected in order to ensure that machine learning systems can be trained\n to detect traffic conditions and provide real-time monitoring, analytics and alerts. This is part of <a href=\"https://deepquestai.com\" >DeepQuest AI</a>'s to train machine learning systems to \n  perceive, understand and act accordingly in solving problems in any environment they are deployed. <br><br>\n\n  This is the first release of the Traffic-Net dataset. It contains 4,400 images that span cover 4 classes. The classes\n  included in this release are: <br><br>\n\n  - <b> Accident </b> <br>\n  - <b> Dense Traffic </b> <br>\n  - <b> Fire </b> <br>\n  - <b> Sparse Traffic </b> <br>\n\n  There are <b>1,100 images</b> for each category, with <b>900 images for trainings </b> and <b>200 images for testing</b> . We are working on adding more\n   categories in the future and will continue to improve the dataset.\n  <br><br> <br> <br>\n\n  <b>>>> DOWNLOAD, TRAINING AND PREDICTION: </b> <br><br>\n The <b>Traffic-Net</b> dataset is provided for download in the <b>release</b> section of this repository.\n You can download the dataset via the link below.<br><br> <a href=\"https://github.com/OlafenwaMoses/Traffic-Net/releases/tag/1.0\" >https://github.com/OlafenwaMoses/Traffic-Net/releases/tag/1.0</a>  <br><br>\n\n We have also provided a python codebase to download the images, train <b>ResNet50</b> on the images\n  and perform prediction using a pretrained model (also using <b>ResNet50</b>) provided in the release section of this repository.\n  The python codebase is contained in the <b><a href=\"traffic_net.py\" >traffic_net.py</a></b> file and the model class labels for prediction is also provided the \n  <b><a href=\"model_class.json\" >model_class.json</a></b>. The pretrained <b>ResNet50</b> model is available for download via the link below. <br><br> \n  <b><a href=\"https://github.com/OlafenwaMoses/Traffic-Net/releases/download/1.0/trafficnet_resnet_model_ex-055_acc-0.913750.h5\" >https://github.com/OlafenwaMoses/Traffic-Net/releases/download/1.0/trafficnet_resnet_model_ex-055_acc-0.913750.h5</a></b><br>\n  <br>\n   This pre-trained model was trained for **60 epochs** only, but it achieved over **91%** accuracy on 800 test images. You can see the prediction results on new images that were not part of the dataset in the **Prediction Results** section below. More experiments will enhance the accuracy of the model.\n<br>\nRunning the experiment or prediction requires that you have **Tensorflow**, and **Keras**, **OpenCV** and **ImageAI** installed. You can install this dependencies via the commands below.\n\n<br><span><b>- Tensorflow 1.4.0 (and later versions)  </b>      <a href=\"https://www.tensorflow.org/install/install_windows\" style=\"text-decoration: none;\" > Install</a></span> or install via pip <pre> pip3 install --upgrade tensorflow </pre> \n       \n  <span><b>- OpenCV  </b>        <a href=\"https://pypi.python.org/pypi/opencv-python\" style=\"text-decoration: none;\" >Install</a></span> or install via pip <pre> pip3 install opencv-python </pre> \n       \n   <span><b>- Keras 2.x  </b>     <a href=\"https://keras.io/#installation\" style=\"text-decoration: none;\" >Install</a></span> or install via pip <pre> pip3 install keras </pre> \n  \n   <span><b>- ImageAI 2.0.3  </b>  \n   <span>      <pre>pip3 install imageai </pre></span> <br><br> <br>\n\n\n\n<b>>>> Video & Prediction Results</b> <br><br>\nClick below to watch the video demonstration of the trained model at work. <br>\n<a href=\"https://www.youtube.com/watch?v=PupK_qd3bP0\" ><img src=\"images/video_image.jpg\" /></a>\n<br><br><br><br>\n  <img src=\"images/1.jpg\" />\n<pre>\nSparse_Traffic  :  99.98759031295776\nAccident  :  0.006892996316310018\nDense_Traffic  :  0.0031178133212961257\nFire  :  0.0023975149815669283\n</pre>\n\n<hr>\n<br>\n<img src=\"images/2.jpg\" />\n<pre>\nDense_Traffic  :  100.0\nAccident  :  9.411973422857045e-07\nFire  :  2.656607822615342e-07\nSparse_Traffic  :  4.631924704900925e-09\n</pre>\n\n<hr>\n<br>\n\n<img src=\"images/3.jpg\" />\n<pre>\nAccident  :  99.94832277297974\nSparse_Traffic  :  0.04670554480981082\nFire  :  0.004610423275153153\nDense_Traffic  :  0.00035401615150476573\n</pre>\n\n<hr>\n<br>\n\n<img src=\"images/4.jpg\" />\n<pre>\nFire  :  100.0\nAccident  :  1.9869084979303675e-22\nDense_Traffic  :  3.262699368229192e-23\nSparse_Traffic  :  6.003136426033551e-28\n</pre>\n\n\n<br>\n\n<h3><b><u>References</u></b></h3>\n\n \n 1. Kaiming H. et al, Deep Residual Learning for Image Recognition <br>\n <a href=\"https://arxiv.org/abs/1512.03385\" >https://arxiv.org/abs/1512.03385</a> <br><br>\n \n Paradrop Traffic Camera",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "cautious-waddle",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "rahulchaurasiya1",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rahulchaurasiya1/cautious-waddle/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Raspberry Pi Model 3\n- Web Camera\n- GSM Module\n- Thingspeak Account\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "- Python 3.4+\n- OpenCV 3.2.0 compiled with Python3 support\n- RaspberryPi GPIO Libraries for Python\n- node.js (>= 6.0)\n- MySQL\n- Raspbian Jessie (or equivalent)\n\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To run the script,\n\n1. `ssh` onto a Raspberry Pi and paste the contents of the `raspi-scripts` folder onto the Raspberry Pi.\n2. Run `python3 sample.py`\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Tue, 21 Dec 2021 17:46:13 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. `git clone https://github.com/rahatzamancse/EyeTask.git`\n2. Install required python dependencies from `requirements.txt` into your python virtual environment. (`pip install -r requirements.txt`)\n3. `python main.py`\n\n",
      "technique": "Header extraction"
    }
  ]
}