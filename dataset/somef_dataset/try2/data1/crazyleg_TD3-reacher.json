{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Replay Buffer module was taken from Baseline package from Open AI [https://github.com/openai/baselines]\n2. TD3 is based on Medium explanaition [https://towardsdatascience.com/td3-learning-to-run-with-ai-40dfc512f93]\n",
      "technique": "Header extraction"
    }
  ],
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[image1]: train_e5.gif \"Untrained Agent\"\n[image2]: train_e15.gif \"In process of training\"\n[image3]: trained.gif \"Trained Agent\"\n[reward]: reward.png \"Plot of reward\"\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8436019038761112
      ],
      "excerpt": "You can do sigmoid and rescale if you prefer. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/crazyleg/TD3-reacher",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-04-19T13:07:23Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-04-19T21:00:29Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": " - Demo.ipynb - allows you to check enviroment and see working agent example\n - Solver.ipynb - reproduces the training procedure\n - agent.py - TD3 agent implementation\n - networks.py - actor and critic Pytorch definitions\n - replay_byffer.py - Replay Buffer implementation from OpenAI Baselines \n - actor.pth - Saved weights for Actor network from TD3\n - critic.pth - Saved weights from Critic networks from TD3\n \n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8048473592586429,
        0.9126103971222692
      ],
      "excerpt": "Repo with Implemenation of TD3 algo for Reacher 20 env. from Udacity course. First reaches reward 30 in 21 episode, gets a [0,100] mean of 30 in 100.   \nProposed enviroment has 20 Reachers simultaneously. As this is a sync. process, I choosed a TD3 algo that collects expirience 20x faster with 20 hands, treating them as sync. process. I see no sence in using async. approach here, as enviroment isn't truly async. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9567588029116127
      ],
      "excerpt": "FC: (33) -> (400) -> (300) -> (4) with ReLu actiovations. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9567588029116127
      ],
      "excerpt": "FC: (33 + 4) -> (400) -> (300) ->(1) with ReLu activations. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9840305607417282
      ],
      "excerpt": "TD3 is quite stable. I choosed a random stack of hyperparams and they worked for 20 and for 1 Reacher agent. Some tuning maeks sence \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Implementation of TD3 reinforcement learning algorithm to solve Reacher problem from Unity-ML",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/crazyleg/TD3-reacher/releases",
    "technique": "GitHub API"
  },
  "faq": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Reacher20 is a Udacity version of Reacher enviroment, containing 20 simultaneous Reacher agents.\nEach agent get a state vector of 33 floats, describing it's joints positions and speed as well as ball's position and speed.\nSo in this case state is a (20, 33) vector.\n\nAt every time step you have to perform an action for every agent, where action is a torque applied to the joint. Action is defined by floats in range (-1,1) and every agent needs 4 actions. So for 20 agent action space is (20,4).\n\nAgent get rewarded if its end is localized in a moving spherical space. Game is considerend solved when mean reward from 20 agents in the last 100 episodes is >= + 30.\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Wed, 22 Dec 2021 23:27:23 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/crazyleg/TD3-reacher/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "crazyleg/TD3-reacher",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/crazyleg/TD3-reacher/master/Demo.ipynb",
      "https://raw.githubusercontent.com/crazyleg/TD3-reacher/master/Solver.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8258951968758675
      ],
      "excerpt": "Imporant to note, last operation in Tanh activation, that scales action output to required env. spec (-1,1). \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/crazyleg/TD3-reacher/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 crazyleg\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "(Image References)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "TD3-reacher",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "crazyleg",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/crazyleg/TD3-reacher/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Wed, 22 Dec 2021 23:27:23 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Download the environment from one of the links below.\n\n    - **_Version 2: Twenty (20) Agents_**\n        - Linux: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher_Linux.zip)\n        - Mac OSX: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher.app.zip)\n        - Windows (32-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher_Windows_x86.zip)\n        - Windows (64-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher_Windows_x86_64.zip)\n    \n    (_For Windows users_) Check out [this link](https://support.microsoft.com/en-us/help/827218/how-to-determine-whether-a-computer-is-running-a-32-bit-version-or-64) if you need help with determining if your computer is running a 32-bit version or 64-bit version of the Windows operating system.\n\n    (_For AWS_) If you'd like to train the agent on AWS (and have not [enabled a virtual screen](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-on-Amazon-Web-Service.md)), then please use [this link](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/one_agent/Reacher_Linux_NoVis.zip) (version 1) or [this link](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher_Linux_NoVis.zip) (version 2) to obtain the \"headless\" version of the environment.  You will **not** be able to watch the agent without enabling a virtual screen, but you will be able to train the agent.  (_To watch the agent, you should follow the instructions to [enable a virtual screen](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-on-Amazon-Web-Service.md), and then download the environment for the **Linux** operating system above._)\n\n2. Install Python requirements\n\n2.1. Use my env\nAttn! This can be quite heavy with some packages not needed for this project. \n```bash\npip install -r requirements.txt\n```\n\n2.2 Use your own\nIt's likely that you will need only standard package of numpy, pytorch to make this work.\n\n\n",
      "technique": "Header extraction"
    }
  ]
}