{
  "citation": [
    {
      "confidence": [
        0.9734645542547574
      ],
      "excerpt": "  - Allow training style transfer networks following Cartoon-GAN paper \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9626356225854676,
        0.9449813035477502,
        0.9944484218006108
      ],
      "excerpt": "  - https://github.com/CompVis/adaptive-style-transfer \n  - https://compvis.github.io/adaptive-style-transfer/ \n  - https://arxiv.org/pdf/1807.10201.pdf \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/sundogai/style-transfer",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-05-24T05:35:37Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-10-01T12:07:30Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9286484564893562,
        0.9442507826472414
      ],
      "excerpt": "This is an implementation of adaptive style transfer in tensorflow 2.   \nIt demonstrates how to: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9423219444984027,
        0.8278062941266466,
        0.8845448220521611
      ],
      "excerpt": "   The code is writen but I'm still looking for good parameters :) \n  - Demonstrate how to use style transfer on Android. I wrote models with the same kind of optimization presented in mobilenet v2 paper. The models train well, can be use \n  for inference on a computer and can be exported in tfLite format but I still have some bug in the android app.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9477164949813303
      ],
      "excerpt": "tensorRT. To compile and build a docker image with a more recent version of tensorflow 2 please see the readme inside trt_docker subdir \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8017313560845856
      ],
      "excerpt": "The research team that releases the adaptive style transfer paper also releases style image on their owncloud \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9966474696551649
      ],
      "excerpt": "part of the cartoon GAN. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8633351233620119,
        0.8594543401074821
      ],
      "excerpt": "  - The model architecture corresponding to the checkpoint (std or mobilenet) \nFor instance to export a model in TFLite format: \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/sundogai/style-transfer/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 28 Dec 2021 14:32:47 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/sundogai/style-transfer/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "sundogai/style-transfer",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/sundogai/style-transfer/master/Dockerfile",
      "https://raw.githubusercontent.com/sundogai/style-transfer/master/trt_docker/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The easiest way to install this style transfer is by using docker. You need to install docker, docker-compose and nvidia-docker then run: `docker-compose build style-transfer` \n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.825148925474566
      ],
      "excerpt": "And you need to specified if your using tflite.   \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8612142158263317
      ],
      "excerpt": "  - Write the training loop of a GAN using tf.function and tf.GradientTape . \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8068512009076331
      ],
      "excerpt": "python3 app.py --action training \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8068512009076331
      ],
      "excerpt": "python3 app.py --action training \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8042981677884342,
        0.8033681690668286
      ],
      "excerpt": "  - The path of checkpoint to load (can be a file or a folder. In this case it will load the more recent one) \n  - The path to write the exported model \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/sundogai/style-transfer/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Dockerfile",
      "Shell",
      "Makefile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Style Transfer",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "style-transfer",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "sundogai",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/sundogai/style-transfer/blob/master/Readme.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Tue, 28 Dec 2021 14:32:47 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you want to use docker for the training, a solution is to run the docker-compose `style-transfer` app. For training you need to specified 3 environments variables : \n- PICTURE_DATASET: the path of the pictures\n- ART_DATASET : the path of the art images\n- EXPERIMENT : the folder where the program writes logs and checkpoints\n\nfor instance run :\n\n```bash\nexport PICTURE_DATASET=....\nexport ART_DATASET=....\nexport EXPERIMENT=....\ndocker-compose run style-transfer\n```\n\nAnd you will get bash prompt in a valid tensorflow 2 environment\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "As soon as you are in an environment running tensorflow 2, try :\n\n```bash\npython3 app.py --action training\n```\n\nTo train the model with default parameters. You can see the most important parameters with `python3 app.py --help` and all the parameters with \n`python3 app.py --helpfull`\n\n  - If you choose to not use docker, then you probably need to change the default paths where are stored the dataset and the tfrecords. The parameters to do this are : \n  `--picture_dataset_path`, `--picture_tfrecord_path`, `--style_dataset_path`, `--style_tfrecord_path` and `--training_dir`\n  - `--style_tfrecord_prefix` is also an option you should specified if you want to create several tfrecord files for several artists in the same folder.\n  - If you want to train a model with the same optimizations than the mobilenet-v2 then add the flag : `--mobilenet`\n \nFor instance, if you want to train a picasso style transfer with the same training parameters than the original paper, you can first run 200000 iterations with a learning\nrate of 0.0001 then 100000 iterations with a learning rate of 0.00002. Inside docker it will be:\n\n```bash\npython3 app.py --action training \\\n               --training_dir /opt/experiment/picasso \\\n               --style_tfrecord_prefix picasso\npython3 app.py --action training \\\n               --training_dir /opt/experiment/picasso_2 \\\n               --style_tfrecord_prefix picasso \\\n               --n_iterations 100000 \\\n               --lr 0.00002 \\\n               --pretrained_ckpt /opt/experiment/picasso/checkpoints/\n```\n\n\n",
      "technique": "Header extraction"
    }
  ]
}