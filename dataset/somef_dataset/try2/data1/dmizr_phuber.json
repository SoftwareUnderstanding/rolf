{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1505.07634v1",
      "https://arxiv.org/abs/1805.07836v4",
      "https://arxiv.org/abs/1505.07634v1",
      "https://arxiv.org/abs/1805.07836",
      "https://arxiv.org/abs/1512.03385"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find any piece of our code or report useful, please cite:\n```BibTeX\n@inproceedings{mizrahi2021re,\ntitle={[Re] Can gradient clipping mitigate label noise?},\nauthor={David Mizrahi and O{\\u{g}}uz Kaan Y{\\\"u}ksel and Aiday Marlen Kyzy},\nbooktitle={ML Reproducibility Challenge 2020},\nyear={2021},\nurl={https://openreview.net/forum?id=TM_SgwWJA23}\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "- Menon et al., [\"Can gradient clipping mitigate label noise?\"](https://openreview.net/pdf?id=rklB76EKPr), ICLR 2020\n- van Rooyen et al., [\"Learning with Symmetric Label Noise: The Importance of Being Unhinged\"](https://arxiv.org/abs/1505.07634v1), NeurIPS 2015\n- Zhang & Sabuncu, [\"Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels\"](https://arxiv.org/abs/1805.07836), NeurIPS 2018\n- LeCun et al., [\"Gradient-based learning applied to document recognition\"](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf), IEEE 1998\n- He et al., [\"Deep Residual Learning for Image Recognition\"](https://arxiv.org/abs/1512.03385), CVPR 2016\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{mizrahi2021re,\ntitle={[Re] Can gradient clipping mitigate label noise?},\nauthor={David Mizrahi and O{\\u{g}}uz Kaan Y{\\\"u}ksel and Aiday Marlen Kyzy},\nbooktitle={ML Reproducibility Challenge 2020},\nyear={2021},\nurl={https://openreview.net/forum?id=TM_SgwWJA23}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9979505984728331,
        0.9945713194286558
      ],
      "excerpt": "- Unhinged loss (van Rooyen et al., NeurIPS 2015) \n- Generalized Cross Entropy loss (Zhang & Sabuncu, NeurIPS 2018) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266,
        0.9030859728368266
      ],
      "excerpt": "| PHuber-CE \u03c4=10  | 99.0\u00b10.0  | 98.8\u00b10.1  | 98.5\u00b10.1  | 97.6\u00b10.0  | \n| PHuber-GCE \u03c4=10 | 98.9\u00b10.0  | 98.7\u00b10.0  | 98.4\u00b10.0  | 98.0\u00b10.0  | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "| PHuber-GCE \u03c4=10 | 95.4\u00b10.1  | 92.2\u00b10.2  | 81.5\u00b10.2  | 54.3\u00b10.5  | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8550101043698384,
        0.9030859728368266
      ],
      "excerpt": "| PHuber-CE \u03c4=10  | 60.6\u00b11.1  | 54.8\u00b11.2  | 43.1\u00b11.1  | 24.3\u00b10.8  | \n| PHuber-GCE \u03c4=10 | 72.7\u00b10.1  | 68.4\u00b10.1  | 60.2\u00b10.2  | 42.2\u00b10.4  | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dmizr/phuber",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-11-12T14:22:10Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-13T21:30:56Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9685286893623891,
        0.8809772243399198
      ],
      "excerpt": "This is a non-official PyTorch implementation of the ICLR 2020 paper \"Can gradient clipping mitigate label noise?\" by Menon et al. This paper studies the robustness of gradient clipping to symmetric label noise, and proposes partially Huberised (PHuber) versions of standard losses, which perform well in the presence of label noise. \nFor the experiments, the following losses are also implemented: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9939789081768436
      ],
      "excerpt": "This repository reproduces all the experiments of the original paper, as part of our participation in the ML Reproducibility Challenge 2020. Our report can be found on OpenReview and in the ReScience C journal. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.853134726109167
      ],
      "excerpt": "Project structure \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9093617831273219
      ],
      "excerpt": "- the model: lenet, resnet50 (e.g. model=resnet50) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9093617831273219
      ],
      "excerpt": "- the model: lenet, resnet50 (e.g. model=resnet50) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8064902637465791
      ],
      "excerpt": "To find out more about the configuration options for evaluation, use the --help flag. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8762123544136277,
        0.8372396012058242
      ],
      "excerpt": "For each configuration, the models obtained during the first trial are available on Google Drive: \n- Pretrained LeNet on MNIST \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9042910388912804
      ],
      "excerpt": "This repo also reproduces the experiments from the paper based on synthetic datasets. These experiments use simple linear models, which are implemented using NumPy and SciPy. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8899425057119099,
        0.8951640074365272
      ],
      "excerpt": "The codebase is separated into 3 parts: \nThis directory contains all the code related to the deep learning experiments on MNIST, CIFAR-10 and CIFAR-100, using PyTorch.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9228303627644162
      ],
      "excerpt": "This directory contains all the code related to experiments on synthetic data with linear models, using NumPy and SciPy. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "[Re] Can gradient clipping mitigate label noise? (ML Reproducibility Challenge 2020)",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dmizr/phuber/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 6,
      "date": "Tue, 21 Dec 2021 06:52:14 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/dmizr/phuber/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "dmizr/phuber",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/dmizr/phuber/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8315133723010852
      ],
      "excerpt": "For the experiments, the following losses are also implemented: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8245539886860519
      ],
      "excerpt": "Pretrained models \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9297031587409132
      ],
      "excerpt": "python3 train.py --help \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8433201970047614
      ],
      "excerpt": "- the label corruption probability \u03c1 of the training set (e.g. dataset.train.corrupt_prob=0.2) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8526799105321331,
        0.930166260340623,
        0.8139534354805396
      ],
      "excerpt": "For example, to evaluate a LeNet model trained on MNIST saved as models/lenet.pt, run: \npython3 eval.py dataset=mnist model=lenet checkpoint=models/lenet.pt \nBy default, trained models are only evaluated on the test set. This can be modified by overriding the dataset.train.use, dataset.val.use and dataset.test.use arguments. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8537648785978966
      ],
      "excerpt": "| CE              | 75.4\u00b10.3  | 62.2\u00b10.4  | 45.8\u00b10.9  | 26.7\u00b10.1  | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8083127137970348,
        0.8194296457558082
      ],
      "excerpt": "- Pretrained ResNet-50 on CIFAR-10 \n- Pretrained ResNet-50 on CIFAR-100 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991
      ],
      "excerpt": "python3 synthetic_1.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991
      ],
      "excerpt": "python3 synthetic_2.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8359231692140767
      ],
      "excerpt": "- Noisy MNIST, CIFAR-10 and CIFAR-100 dataset classes (with symmetric label noise), in phuber/dataset.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8209795021065854
      ],
      "excerpt": "- Config files for Hydra settings (e.g. output folder and logger) are contained in conf/hydra. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/dmizr/phuber/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "TeX"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 David Mizrahi, O\\xc4\\x9fuz Kaan Y\\xc3\\xbcksel and Aiday Marlen Kyzy\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "[Re] Can gradient clipping mitigate label noise?",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "phuber",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "dmizr",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dmizr/phuber/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "dmizr",
        "body": "Fully reproduces all experiments described in \"Can gradient clipping mitigate label noise?\" by Menon et al. (ICLR 2020)",
        "dateCreated": "2021-01-28T16:23:38Z",
        "datePublished": "2021-01-28T16:46:22Z",
        "html_url": "https://github.com/dmizr/phuber/releases/tag/v1.0",
        "name": "First release of paper reproduction",
        "tag_name": "v1.0",
        "tarball_url": "https://api.github.com/repos/dmizr/phuber/tarball/v1.0",
        "url": "https://api.github.com/repos/dmizr/phuber/releases/37040354",
        "zipball_url": "https://api.github.com/repos/dmizr/phuber/zipball/v1.0"
      }
    ],
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This project requires Python >= 3.8. Dependencies can be installed with:\n```\npip install -r requirements.txt\n```\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "By default, run metrics are logged to [TensorBoard](https://www.tensorflow.org/tensorboard). In addition, the saved models, training parameters and training log can be found in the run's directory, in `outputs/`.\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 11,
      "date": "Tue, 21 Dec 2021 06:52:14 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "label-noise",
      "robust-learning",
      "gradient-clipping",
      "pytorch"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Training LeNet on MNIST using cross-entropy loss and no label corruption:\n```\npython3 train.py dataset=mnist model=lenet loss=ce dataset.train.corrupt_prob=0.0\n```\n\nTraining a ResNet-50 on CIFAR-10 using the partially Huberised cross-entropy loss (PHuber-CE) with \u03c4=2, and label corruption probability \u03c1 of 0.2:\n\n```\npython3 train.py dataset=cifar10 model=resnet50 loss=phuber_ce loss.tau=2 dataset.train.corrupt_prob=0.2\n```\n\nTraining a ResNet-50 on CIFAR-100 using the Generalized Cross Entropy loss (GCE) and label corruption probability \u03c1 of 0.6, with [mixed precision](https://pytorch.org/blog/accelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision/):\n\n```\npython3 train.py dataset=cifar100 model=resnet50 loss=gce dataset.train.corrupt_prob=0.6 mixed_precision=true\n```\n\n Training LeNet on MNIST using cross-entropy loss, and varying label corruption probability \u03c1 (0.0, 0.2, 0.4 and 0.6). This uses [Hydra's multi-run flag](https://hydra.cc/docs/tutorials/basic/running_your_app/multi-run) for parameter sweeps:\n\n```\npython3 train.py --multirun dataset=mnist model=lenet loss=ce dataset.train.corrupt_prob=0.0,0.2,0.4,0.6\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}