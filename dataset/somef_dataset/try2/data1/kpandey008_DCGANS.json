{
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kpandey008/dcgan",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-10-08T13:40:22Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-08-23T06:46:50Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9595591802227704
      ],
      "excerpt": "Implementation of the popular paper UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9903394607094089
      ],
      "excerpt": "Generative Adversarial Network or GAN (Refer to the paper Generative Adversarial Networks by Goodfellow et.al.) is a recently introduced Generative modeling framework that has two main components- a Discriminator and a Generator- both of which are primarily modeled using Neural Networks. The concept of GANS is simple yet effective - The Generator takes the noise as input and generates some data samples while the Discriminator tries to classify those samples from the real samples. This causes a two player game between the Generator and the Discriminator where the Generator tries to generate samples that are indistinguishable from the real data samples while the Discriminator becomes better at distinguishing between the samples generated by the Generator. The process continues till none of the Discriminator or the Generator can get any better, thus reaching an equilibrium (also called as the Nash Equilibrium). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8112303760342109,
        0.9664088982290275
      ],
      "excerpt": "In practice GAN's are very hard to train mainly because of the following reasons :  \n1) The Nash Equilibrium is very hard to reach using Gradient Descent based methods used for training \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9644700131403787,
        0.9942335150930312,
        0.907116628342217,
        0.8641132557248008,
        0.8242564591307004
      ],
      "excerpt": "3) GAN training is unstable and requires a lot of model exploration and careful selection of hyperparameters. \nDCGANS try to solve some of the above problems by using ConvNets in the Generator and Discriminator instead of simple MLP's. The authors in the paper also propose some architectual contraints on the ConvNets that helps stabilize GAN training. Some of these constraints are: \n1) Using only convolutional layers in the Generator and the Discriminator by increasing the stride, therfore discouraging the use of MaxPooling and Dense layers in the ConvNet architecture \n2) Using BatchNormalization in both the Generator and the Discriminator during training. However, during training I observed that using BatchNorm in all the layers causes the training to oscillate and destabilize. \n3) The generator uses tanh activation in the final layer while the discriminator uses leaky-relu activation in all the layers. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9782232429469615
      ],
      "excerpt": "Following is the visualization for the samples generated by the Network Generator for a fixed noise vector input from the starting of the training over 5000 steps of training \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9424986185981026
      ],
      "excerpt": "Some samples generated by the Generator after the end of training are as follows: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.943625312929781,
        0.909855503556383,
        0.9450915641654913,
        0.8001517406019976
      ],
      "excerpt": "1) DCGAN performs better than the Vanilla GAN hands down !!. This may be due to the superior architecture of ConvNets over simple MLP's. The training for DCGAN's is much stable than for Vanilla GAN's \n2) The architectural constraints as stated in the paper do help in stablizing the training of DCGAN's. However careful hyperparameter selection is still needed. \n3) Using more number of filters in the convolutional layer--basically using a large CNN--with increased strides in the higher layers gives better perfomance than using MaxPooling layers in the Convolutional layers \n4) Using Dropout in the Discriminator helps in training (though this is not mentioned in the architecture proposed in the training) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9701042090429159
      ],
      "excerpt": "Some good resources for knowing more about GAN's are : \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8022355647706172
      ],
      "excerpt": "2) The paper Improved Techniques for Training GANS by Tim Salimans et.al is an excellent resource for knowing more about GAN training. The paper can be found at https://arxiv.org/pdf/1511.06434.pdf \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Implementation of Deep Convolutional Generative Adversarial Networks in Pytorch and Tensorflow",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kpandey008/DCGANS/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Mon, 27 Dec 2021 11:29:58 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/kpandey008/dcgan/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "kpandey008/dcgan",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/kpandey008/DCGANS/master/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/kpandey008/DCGANS/master/notebook/DCGAN_eager_execution.ipynb",
      "https://raw.githubusercontent.com/kpandey008/DCGANS/master/notebook/DCGAN.ipynb"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.8836704212480256
      ],
      "excerpt": "<img src='assets/gan_schema.png' /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8836704212480256
      ],
      "excerpt": "<img src='assets/DCGAN.png' /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9187782370495189
      ],
      "excerpt": "<img src='assets/output.png' /> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/kpandey008/dcgan/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2018 Kushagra Pandey\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Deep convolutional Generative Adversarial Networks (DCGANS)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "dcgan",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "kpandey008",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kpandey008/dcgan/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The following python packages must be installed for running the code\n\n- Python 2.7 or Python 3.3+\n- Tensorflow 0.12.1\n- Numpy\n- Matplotlib\n- ImageIO\n- Scikit-learn\n\nI prefer to use Google Collaboratory for training such systems due to heavy computational requirements. Here is the link to an excellent Medium Post for setting up Google Colab with Drive to manage your ML Projects: https://medium.com/deep-learning-turkey/google-colab-free-gpu-tutorial-e113627b9f5d\n\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You can find the code for the DCGAN in the jupyter notebook ```notebook/DCGAN.ipynb```. Running it should be fairly simple as the code is well-commented and explained fairly well\nOn running the code you should expect to see the following directory structure in your current working directory\n\n|--model_data_dcgan<br/>\n&nbsp; &nbsp; &nbsp;|--experiment_<id></br>\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;|--tmp/</br>\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;|--checkpoints/</br>\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;|--gifs/</br>\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;|--config.yaml</br>\n\nThe ```tmp/``` folder contains the image generated by the **Generator** at every 100th training step given a fixed noise vector (refer to the code for this part)\n\nThe ```checkpoints/``` folder basically checkpoints your training (:P) so that you can resume if in case the training ends abruptly.\n\nThe gifs folder combines the images in ```tmp/``` folder to create a visualization over time representing the generator image generation over the duration of training\n\nThe file ```config.yaml``` stores the configuration of the Generator and the Discriminator neural networks for that particular experiment\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 4,
      "date": "Mon, 27 Dec 2021 11:29:58 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "discriminator",
      "gan",
      "dcgan",
      "nash-equilibrium",
      "tensorflow",
      "neural-network",
      "pytorch"
    ],
    "technique": "GitHub API"
  }
}