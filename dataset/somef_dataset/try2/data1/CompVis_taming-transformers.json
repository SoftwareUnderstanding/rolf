{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2012.09841",
      "https://arxiv.org/abs/2012.09841v3 and the corresponding changelog.\n- Added a pretrained, [1.4B transformer model](https://k00.fr/s511rwcv",
      "https://arxiv.org/abs/1606.00915",
      "https://arxiv.org/abs/1612.03716"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{esser2020taming,\n      title={Taming Transformers for High-Resolution Image Synthesis}, \n      author={Patrick Esser and Robin Rombach and Bj\u00f6rn Ommer},\n      year={2020},\n      eprint={2012.09841},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9749159803457006
      ],
      "excerpt": "arXiv | BibTeX | Project Page \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9473221799140844
      ],
      "excerpt": "Our paper received an update: See https://arxiv.org/abs/2012.09841v3 and the corresponding changelog. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8326585122547221
      ],
      "excerpt": "| VQGAN ImageNet (f=16), 1024 |  10.54 | 7.94 | vqgan_imagenet_f16_1024 | reconstructions | Reconstruction-FIDs. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9626356225854676
      ],
      "excerpt": "| DALL-E dVAE (f=8), 8192, GumbelQuantization | 33.88 | 32.01 | https://github.com/openai/DALL-E | reconstructions | Reconstruction-FIDs. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8087636543862011
      ],
      "excerpt": "the Stuff+thing PNG-style annotations on COCO 2017 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/CompVis/taming-transformers",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-12-17T14:47:06Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-27T12:27:52Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9830835516408652
      ],
      "excerpt": "tl;dr We combine the efficiancy of convolutional approaches with the expressivity of transformers by introducing a convolutional VQGAN, which learns a codebook of context-rich visual parts, whose composition is modeled with an autoregressive transformer. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9155925243230261,
        0.9776525646390588,
        0.87860131787585
      ],
      "excerpt": "Thanks to rom1504 it is now easy to train a VQGAN on your own datasets. \nIncluded a bugfix for the quantizer. For backward compatibility it is \n  disabled by default (which corresponds to always training with beta=1.0). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8460988342441319,
        0.8226926250444243
      ],
      "excerpt": "Added a pretrained, 1.4B transformer model trained for class-conditional ImageNet synthesis, which obtains state-of-the-art FID scores among autoregressive approaches and outperforms BigGAN. \nAdded pretrained, unconditional models on FFHQ and CelebA-HQ. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8982946461968487,
        0.9792051461808381,
        0.9504981496987063,
        0.8572625372946193
      ],
      "excerpt": "Added a checkpoint of a VQGAN trained with f8 compression and Gumbel-Quantization.  \n  See also our updated reconstruction notebook.  \nWe added a colab notebook which compares two VQGANs and OpenAI's DALL-E. See also this section. \nWe now include an overview of pretrained models in Tab.1. We added models for COCO and ADE20k. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8406838795382204,
        0.9815526435872467
      ],
      "excerpt": "You can now jump right into sampling with our Colab quickstart notebook. \nThe following table provides an overview of all models that are currently available.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9803288122113792
      ],
      "excerpt": "For reference, we also include a link to the recently released autoencoder of the DALL-E model.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9671279468217098
      ],
      "excerpt": "for a comparison and discussion of reconstruction capabilities. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9621879301977615,
        0.973300195877111
      ],
      "excerpt": "model does not require any data preparation. To produce 50 samples for each of \nthe 1000 classes of ImageNet, with k=600 for top-k sampling, p=0.92 for nucleus \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8868455092811953
      ],
      "excerpt": "To restrict the model to certain classes, provide them via the --classes argument, separated by  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9127115394263291
      ],
      "excerpt": "We recommended to experiment with the autoregressive decoding parameters (top-k, top-p and temperature) for best results. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9307419714666566
      ],
      "excerpt": "To produce 50000 samples, with k=250 for top-k sampling, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.978027802044922
      ],
      "excerpt": "for FFHQ and   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8179280117722122,
        0.8202735525626309
      ],
      "excerpt": "to sample from the CelebA-HQ model. \nFor both models it can be advantageous to vary the top-k/top-p parameters for sampling. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9284728657001595
      ],
      "excerpt": "place it into logs. Follow the data preparation steps for \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8695916028078265
      ],
      "excerpt": "place it into logs. To run the demo on a couple of example depth maps \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8425499707806566
      ],
      "excerpt": "place it into logs. To run the demo on a couple of example segmentation maps \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8425499707806566
      ],
      "excerpt": "place it into logs. To run the demo on a couple of example segmentation maps \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9218640513736983
      ],
      "excerpt": "Those are the steps to follow to make this work: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9072458968619493
      ],
      "excerpt": "is used. However, since ImageNet is quite large, this requires a lot of disk \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8705884132662066,
        0.9740843578833683,
        0.8952874943564886
      ],
      "excerpt": "up by putting the data into \n${XDG_CACHE}/autoencoders/data/ILSVRC2012_{split}/data/ (which defaults to \n~/.cache/autoencoders/data/ILSVRC2012_{split}/data/), where {split} is one \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8329515101753393,
        0.8979411005071259
      ],
      "excerpt": "ILSVRC2012_img_train.tar/ILSVRC2012_img_val.tar (or symlinks to them) into \n${XDG_CACHE}/autoencoders/data/ILSVRC2012_train/ / \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8052779484014104
      ],
      "excerpt": "described above and containing a png file for each of ImageNet's JPEG \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9209427716098185
      ],
      "excerpt": "Hub. When we prepared the data, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9031472812595341
      ],
      "excerpt": "provides a v2.1 version. We haven't tested our models with depth maps obtained \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8117950220321785,
        0.9013107979299635,
        0.8843473169175269
      ],
      "excerpt": "Create a symlink data/ffhq pointing to the images1024x1024 folder obtained \nfrom the FFHQ repository. \nUnfortunately, we are not allowed to distribute the images we collected for the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9369406314230744
      ],
      "excerpt": "web to get started. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8863412691759037
      ],
      "excerpt": "(see data/flickr_tags.txt for a full list of tags used to find images) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9286826498465224
      ],
      "excerpt": "(see data/subreddits.txt for all subreddits that were used). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9582485392219222
      ],
      "excerpt": "Create a symlink data/ade20k_root containing the contents of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9195799618028209
      ],
      "excerpt": "model.params.first_stage_config.params.ckpt_path in \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9027272091313558
      ],
      "excerpt": "2020-11-09T13-33-36_faceshq_vqgan and place into logs, which \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9910516134918242
      ],
      "excerpt": "key model.params.first_stage_config.params.ckpt_path of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8174464112410705
      ],
      "excerpt": "Train a VQGAN on Depth Maps of ImageNet with \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9910516134918242
      ],
      "excerpt": "key model.params.cond_stage_config.params.ckpt_path of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.94998940049761,
        0.9781427205971015,
        0.9763957592523542
      ],
      "excerpt": "The reconstruction and compression capabilities of different fist stage models can be analyzed in this colab notebook.  \nIn particular, the notebook compares two VQGANs with a downsampling factor of f=16 for each and codebook dimensionality of 1024 and 16384,  \na VQGAN with f=8 and 8192 codebook entries and the discrete autoencoder of OpenAI's DALL-E (which has f=8 and 8192  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9568391063029965
      ],
      "excerpt": "The notebook of Rivers Have Wings. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Taming Transformers for High-Resolution Image Synthesis",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/CompVis/taming-transformers/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 320,
      "date": "Mon, 27 Dec 2021 17:14:51 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/CompVis/taming-transformers/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "CompVis/taming-transformers",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/CompVis/taming-transformers/master/scripts/reconstruction_usage.ipynb",
      "https://raw.githubusercontent.com/CompVis/taming-transformers/master/scripts/taming-transformers.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8633422378978682
      ],
      "excerpt": "| COCO-Stuff (f=16) | -- | 20.4  | coco_transformer | coco_samples.zip [5k] | evaluated on val split (5k images) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.831808035298714
      ],
      "excerpt": "Download the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9992227274298034
      ],
      "excerpt": "1. install the repo with conda env create -f environment.yaml, conda activate taming and pip install -e . \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8228411885347281
      ],
      "excerpt": "of train/validation. It should have the following structure: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356242392055834
      ],
      "excerpt": "v2.0 version, but now it \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8464836967636765
      ],
      "excerpt": "trained on COCO-Stuff. We used a PyTorch \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8302813757538782
      ],
      "excerpt": "annotations from COCO-Stuff, which \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9701417088239531
      ],
      "excerpt": "or download a pretrained one from 2020-09-23T17-56-33_imagenet_vqgan \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9534028303873898
      ],
      "excerpt": "or download a pretrained one from 2020-11-03T15-34-24_imagenetdepth_vqgan \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8435927496629332
      ],
      "excerpt": "To train the transformer, run \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8191874730857469
      ],
      "excerpt": "Take a look at ak9250's notebook if you want to run the streamlit demos on Colab. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8685814927324258
      ],
      "excerpt": "  D-RIN demo without preparing the dataset first. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8184824295951135
      ],
      "excerpt": "| Dataset  | FID vs train | FID vs val | Link |  Samples (256x256) | Comments \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8101948094601531
      ],
      "excerpt": "| ImageNet (cIN) (f=16) | 15.98/15.78/6.59/5.88/5.20 | -- | cin_transformer | cin_samples | different decoding hyperparameters | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8312678418234966
      ],
      "excerpt": "folder and place it into logs. Then, run \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8141388017421604
      ],
      "excerpt": "python scripts/sample_fast.py -r logs/2021-04-03T19-39-50_cin_transformer/ -n 50 -k 600 -t 1.0 -p 0.92 --batch_size 25 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8644598330482254
      ],
      "excerpt": "commas. For example, to sample 50 ostriches, border collies and whiskey jugs, run \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.806488090647331
      ],
      "excerpt": "to sample from the CelebA-HQ model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8467079033331523
      ],
      "excerpt": "place it into logs. To run the demo on a couple of example depth maps \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8840588295218136,
        0.8270162571618699
      ],
      "excerpt": "streamlit run scripts/sample_conditional.py -- -r logs/2020-11-20T12-54-32_drin_transformer/ --ignore_base_data data=\"{target: main.DataModuleFromConfig, params: {batch_size: 1, validation: {target: taming.data.imagenet.DRINExamples}}}\" \nTo run the demo on the complete validation set, first follow the data preparation steps for \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8050106625576697
      ],
      "excerpt": "place it into logs. To run the demo on a couple of example segmentation maps \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9247695411643931
      ],
      "excerpt": "streamlit run scripts/sample_conditional.py -- -r logs/2021-01-20T16-04-20_coco_transformer/ --ignore_base_data data=\"{target: main.DataModuleFromConfig, params: {batch_size: 1, validation: {target: taming.data.coco.Examples}}}\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8050106625576697
      ],
      "excerpt": "place it into logs. To run the demo on a couple of example segmentation maps \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9177332539308991
      ],
      "excerpt": "streamlit run scripts/sample_conditional.py -- -r logs/2020-11-20T21-45-44_ade20k_transformer/ --ignore_base_data data=\"{target: main.DataModuleFromConfig, params: {batch_size: 1, validation: {target: taming.data.ade20k.Examples}}}\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8957416776024568,
        0.8981427406397806
      ],
      "excerpt": "1. put your .jpg files in a folder your_folder \n2. create 2 text files a xx_train.txt and xx_test.txt that point to the files in your training and test set respectively (for example find $(pwd)/your_folder -name \"*.jpg\" &gt; train.txt) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9318074611004528
      ],
      "excerpt": "4. run python main.py --base configs/custom_vqgan.yaml -t True --gpus 0,1 to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8799891065511883
      ],
      "excerpt": "${XDG_CACHE}/autoencoders/data/ILSVRC2012_{split}/data/ nor a file \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8558855233462266
      ],
      "excerpt": "if you want to force running the dataset preparation again. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8471173564408397
      ],
      "excerpt": "data/imagenet_depth pointing to a folder with two subfolders train and \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.883105712735412
      ],
      "excerpt": "files. The png encodes float32 depth values obtained from MiDaS as RGBA \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.818145776485475
      ],
      "excerpt": "Create a symlink data/celebahq pointing to a folder containing the .npy \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8190681076309274
      ],
      "excerpt": "Create a symlink data/ffhq pointing to the images1024x1024 folder obtained \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8185858922313475
      ],
      "excerpt": "training images and 10764 validation images. We then obtained segmentation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8278121020599903,
        0.9200801605307152
      ],
      "excerpt": "Train a VQGAN with \npython main.py --base configs/faceshq_vqgan.yaml -t True --gpus 0, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8271141842945583
      ],
      "excerpt": "model.params.first_stage_config.params.ckpt_path in \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8285083494413175,
        0.9200801605307152
      ],
      "excerpt": "corresponds to the preconfigured checkpoint path), then run \npython main.py --base configs/faceshq_transformer.yaml -t True --gpus 0, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9200801605307152,
        0.8389182150322243
      ],
      "excerpt": "python main.py --base configs/imagenet_vqgan.yaml -t True --gpus 0, \nor download a pretrained one from 2020-09-23T17-56-33_imagenet_vqgan \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8093390710990894
      ],
      "excerpt": "key model.params.first_stage_config.params.ckpt_path of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9200801605307152
      ],
      "excerpt": "python main.py --base configs/imagenetdepth_vqgan.yaml -t True --gpus 0, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8093390710990894
      ],
      "excerpt": "key model.params.cond_stage_config.params.ckpt_path of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8666879188691472,
        0.9200801605307152
      ],
      "excerpt": "To train the transformer, run \npython main.py --base configs/drin_transformer.yaml -t True --gpus 0, \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/CompVis/taming-transformers/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Taming Transformers for High-Resolution Image Synthesis",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "taming-transformers",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "CompVis",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/CompVis/taming-transformers/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "A suitable [conda](https://conda.io/) environment named `taming` can be created\nand activated with:\n\n```\nconda env create -f environment.yaml\nconda activate taming\n```\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The commands below will start a streamlit demo which supports sampling at\ndifferent resolutions and image completions. To run a non-interactive version\nof the sampling process, replace `streamlit run scripts/sample_conditional.py --`\nby `python scripts/make_samples.py --outdir <path_to_write_samples_to>` and\nkeep the remaining command line arguments. \n\nTo sample from unconditional or class-conditional models, \nrun `python scripts/sample_fast.py -r <path/to/config_and_checkpoint>`.\nWe describe below how to use this script to sample from the ImageNet, FFHQ, and CelebA-HQ models, \nrespectively.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2242,
      "date": "Mon, 27 Dec 2021 17:14:51 GMT"
    },
    "technique": "GitHub API"
  }
}