{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2112.11081",
      "https://arxiv.org/abs/2101.03697",
      "https://arxiv.org/abs/2103.13425",
      "https://arxiv.org/abs/1803.03635.].\n\nThe codes are based on PyTorch 1.1.\n\nThe experiments reported in the paper were performed using Tensorflow. However, the backbone of the codes was refactored from the official Tensorflow benchmark (https://github.com/tensorflow/benchmarks/tree/master/scripts/tf_cnn_benchmarks), which was designed in the pursuit of extreme speed, not readability. So I decided to re-implement it in PyTorch to save time for both readers and me.\n\nCitation:\n\n    @inproceedings{DBLP:conf/nips/DingDZGHL19,\n    author    = {Xiaohan Ding and\n               Guiguang Ding and\n               Xiangxin Zhou and\n               Yuchen Guo and\n               Jungong Han and\n               Ji Liu"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{DBLP:conf/nips/DingDZGHL19,\nauthor    = {Xiaohan Ding and\n           Guiguang Ding and\n           Xiangxin Zhou and\n           Yuchen Guo and\n           Jungong Han and\n           Ji Liu},\neditor    = {Hanna M. Wallach and\n           Hugo Larochelle and\n           Alina Beygelzimer and\n           Florence d'Alch{\\'{e}}{-}Buc and\n           Emily B. Fox and\n           Roman Garnett},\ntitle     = {Global Sparse Momentum {SGD} for Pruning Very Deep Neural Networks},\nbooktitle = {Advances in Neural Information Processing Systems 32: Annual Conference\n           on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14\n           December 2019, Vancouver, BC, Canada},\npages     = {6379--6391},\nyear      = {2019},\nurl       = {http://papers.nips.cc/paper/8867-global-sparse-momentum-sgd-for-pruning-very-deep-neural-networks},\ntimestamp = {Fri, 06 Mar 2020 17:00:41 +0100},\nbiburl    = {https://dblp.org/rec/conf/nips/DingDZGHL19.bib},\nbibsource = {dblp computer science bibliography, https://dblp.org}\n\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8494551047105751
      ],
      "excerpt": "This repository contains the codes for the following NeurIPS-2019 paper  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9157315831269563
      ],
      "excerpt": "2. Find the winning tickets of LeNet-300-100 by 60X pruning together with LeNet-5 by 125X and 300X, and compare the final results with simple magnitude-based pruning [Frankle, J., & Carbin, M. (2018). The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635.]. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9890131201201999
      ],
      "excerpt": "author    = {Xiaohan Ding and \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8490817347094297,
        0.8944178096468923
      ],
      "excerpt": "           Jungong Han and \n           Ji Liu}, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9997989482033018,
        0.9984745519418284,
        0.9222383658450612,
        0.8944178096468923,
        0.9960965048981569
      ],
      "excerpt": "booktitle = {Advances in Neural Information Processing Systems 32: Annual Conference \n           on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 \n           December 2019, Vancouver, BC, Canada}, \npages     = {6379--6391}, \nyear      = {2019}, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9702105236373249,
        0.9949187583612273
      ],
      "excerpt": "biburl    = {https://dblp.org/rec/conf/nips/DingDZGHL19.bib}, \nbibsource = {dblp computer science bibliography, https://dblp.org} \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/DingXiaoH/GSM-SGD",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "dxh17@mails.tsinghua.edu.cn\n\nGoogle Scholar Profile: https://scholar.google.com/citations?user=CIjw0KoAAAAJ&hl=en\n\nMy open-sourced papers and repos: \n\nThe **Structural Re-parameterization Universe**:\n\n1. RepMLP (preprint, 2021) **MLP-style building block and Architecture**\\\n[RepMLPNet: Hierarchical Vision MLP with Re-parameterized Locality](https://arxiv.org/abs/2112.11081)\\\n[code](https://github.com/DingXiaoH/RepMLP).\n\n2. RepVGG (CVPR 2021) **A super simple and powerful VGG-style ConvNet architecture**. Up to **84.16%** ImageNet top-1 accuracy!\\\n[RepVGG: Making VGG-style ConvNets Great Again](https://arxiv.org/abs/2101.03697)\\\n[code](https://github.com/DingXiaoH/RepVGG).\n\n3. ResRep (ICCV 2021) **State-of-the-art** channel pruning (Res50, 55\\% FLOPs reduction, 76.15\\% acc)\\\n[ResRep: Lossless CNN Pruning via Decoupling Remembering and Forgetting](https://openaccess.thecvf.com/content/ICCV2021/papers/Ding_ResRep_Lossless_CNN_Pruning_via_Decoupling_Remembering_and_Forgetting_ICCV_2021_paper.pdf)\\\n[code](https://github.com/DingXiaoH/ResRep).\n\n4. ACB (ICCV 2019) is a CNN component without any inference-time costs. The first work of our Structural Re-parameterization Universe.\\\n[ACNet: Strengthening the Kernel Skeletons for Powerful CNN via Asymmetric Convolution Blocks](http://openaccess.thecvf.com/content_ICCV_2019/papers/Ding_ACNet_Strengthening_the_Kernel_Skeletons_for_Powerful_CNN_via_Asymmetric_ICCV_2019_paper.pdf).\\\n[code](https://github.com/DingXiaoH/ACNet). \n\n5. DBB (CVPR 2021) is a CNN component with higher performance than ACB and still no inference-time costs. Sometimes I call it ACNet v2 because \"DBB\" is 2 bits larger than \"ACB\" in ASCII (lol).\\\n[Diverse Branch Block: Building a Convolution as an Inception-like Unit](https://arxiv.org/abs/2103.13425)\\\n[code](https://github.com/DingXiaoH/DiverseBranchBlock).\n\n6. COMING SOON\n\n7. COMING SOON\n\n**Model compression and acceleration**:\n\n1. (CVPR 2019) Channel pruning: [Centripetal SGD for Pruning Very Deep Convolutional Networks with Complicated Structure](http://openaccess.thecvf.com/content_CVPR_2019/html/Ding_Centripetal_SGD_for_Pruning_Very_Deep_Convolutional_Networks_With_Complicated_CVPR_2019_paper.html)\\\n[code](https://github.com/DingXiaoH/Centripetal-SGD)\n\n2. (ICML 2019) Channel pruning: [Approximated Oracle Filter Pruning for Destructive CNN Width Optimization](http://proceedings.mlr.press/v97/ding19a.html)\\\n[code](https://github.com/DingXiaoH/AOFP)\n\n3. (NeurIPS 2019) Unstructured pruning: [Global Sparse Momentum SGD for Pruning Very Deep Neural Networks](http://papers.nips.cc/paper/8867-global-sparse-momentum-sgd-for-pruning-very-deep-neural-networks.pdf)\\\n[code](https://github.com/DingXiaoH/GSM-SGD)\n",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-09-24T03:16:57Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-24T09:34:17Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9170463364330964,
        0.8435597805917645
      ],
      "excerpt": "This repository contains the codes for the following NeurIPS-2019 paper  \nGlobal Sparse Momentum SGD for Pruning Very Deep Neural Networks. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9739006790128153
      ],
      "excerpt": "1. Prune a ResNet-56, get a global compression ratio of 10X (90% of the parameters are zeros). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9817280649905451,
        0.9826143822559965
      ],
      "excerpt": "The codes are based on PyTorch 1.1. \nThe experiments reported in the paper were performed using Tensorflow. However, the backbone of the codes was refactored from the official Tensorflow benchmark (https://github.com/tensorflow/benchmarks/tree/master/scripts/tf_cnn_benchmarks), which was designed in the pursuit of extreme speed, not readability. So I decided to re-implement it in PyTorch to save time for both readers and me. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.908925214220865
      ],
      "excerpt": "           Guiguang Ding and \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.908925214220865,
        0.908925214220865
      ],
      "excerpt": "           Yuchen Guo and \n           Jungong Han and \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.908925214220865,
        0.908925214220865,
        0.908925214220865,
        0.908925214220865,
        0.908925214220865
      ],
      "excerpt": "editor    = {Hanna M. Wallach and \n           Hugo Larochelle and \n           Alina Beygelzimer and \n           Florence d'Alch{\\'{e}}{-}Buc and \n           Emily B. Fox and \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8651612752193288
      ],
      "excerpt": "url       = {http://papers.nips.cc/paper/8867-global-sparse-momentum-sgd-for-pruning-very-deep-neural-networks}, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Global Sparse Momentum SGD for pruning very deep neural networks",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/DingXiaoH/GSM-SGD/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 5,
      "date": "Sun, 26 Dec 2021 02:15:23 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/DingXiaoH/GSM-SGD/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "DingXiaoH/GSM-SGD",
    "technique": "GitHub API"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/DingXiaoH/GSM-SGD/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 DingXiaoH\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Global Sparse Momentum SGD",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "GSM-SGD",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "DingXiaoH",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/DingXiaoH/GSM-SGD/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 35,
      "date": "Sun, 26 Dec 2021 02:15:23 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "  \nThis repo holds the example codes for the experiments of finding the winning lottery tickets  by GSM.\n\n1. Install PyTorch 1.1. Clone this repo and enter the directory. Modify PYTHONPATH or you will get an ImportError.\n```\nexport PYTHONPATH='WHERE_YOU_CLONED_THIS_REPO'\n```\n\n2. Modify 'CIFAR10_PATH' and 'MNIST_PATH' in dataset.py to the directory of your CIFAR-10 and MNIST datasets. If the datasets are not found, they will be automatically downloaded. \n\n3. Train a ResNet-56 and prune it by 10X via Global Sparse Momentum. The model will be tested every two epochs. Check the average accuracy in the last ten evaluations. Check the sparsity of the pruned model.\n```\npython gsm/gsm_rc56.py\npython show_log.py\npython display_hdf5.py gsm_exps/rc56_gsm/finish_pruned.hdf5\n```\n\n4. Initialize and train a LeNet-300-100, find the 1/60 winning tickets by GSM and magnitude-based pruning, respectively, and train the winning tickets. Check the final accuracy.\n```\npython gsm/gsm_lottery_ticket_lenet300.py 60\npython show_log.py | grep retrain\n```\n\n5. Initialize and train a LeNet-5, find the 1/125 and 1/300 winning tickets by GSM and magnitude-based pruning, respectively, and train the winning tickets. Check the final accuracy.\n```\npython gsm/gsm_lottery_ticket_lenet5.py 125\npython gsm/gsm_lottery_ticket_lenet5.py 300\npython show_log.py | grep retrain\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}