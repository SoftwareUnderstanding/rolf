{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1511.05952\n    tau_initial (float"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9061036823472083
      ],
      "excerpt": "A single agent following a single target \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9325182562707431
      ],
      "excerpt": "    beta_rate (float): Rate (0 to 1) for increasing beta to 1 as per Schauel et al.         https://arxiv.org/abs/1511.05952 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/snhwang/p2-continuous-control-SNH",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-11-20T07:27:21Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-11-21T03:42:17Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9225986093322734,
        0.9690206742683163,
        0.9643266705213595,
        0.990372718226042
      ],
      "excerpt": "This project was one of the requirements for completing the Deep Reinforcement Learning Nanodegree (DRLND) course at Udacity.com. \nThis project uitlized the Reacher environment. In this environment, an agent, represented by a double-jointed arm, moves its \"hand\" to target locations. The figure below shows a clip of 10 agents following their targets (green balls) with their hands (small blue balls). The agent must control its joints to maintain its hand at the target. A reward of +0.1 is provided for each step that the agent's hand is in the target location. Thus, the goal of each agent is to maintain its position at the target location for as many time steps as possible. \n\u200b                   Taken from the Udacity project introduction page \nThe observation space consists of 33 variables corresponding to position, rotation, velocity, and angular velocities of the arm. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9939383889752514,
        0.9182966377809797
      ],
      "excerpt": "A score of >30 averaged  over 100 episodes (and averaged over all of the agents for the multi-agent task) is required to successfully complete the project. \nAlthough we were instructed to choose one of the above. I focused on the second option of 20 agents but the same code can be used for both. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8063210365442327,
        0.9345366729744546,
        0.9570066249716638
      ],
      "excerpt": "To train the the agent(s) with the provided parameters, just \"run all\" under the Cell drop down menu of the Jupyter notebook.  \nThe parameters of the learning agent can be changed in Section 4 of the notebook. The parameters for running the simulation and training the agent can be modified in Section 5. The parameters are described below. During training, multiple checkpoints are saved for running the trained agent later. One of the parameters for training is a prefix string for the checkpoints. This must be provided. I initially included a default name but I kept forgetting to change it and would overwrite my previous files, so I made it a required parameter to specify. For example, if checkpoint_name is specified as 'v2,' the following checkpoint will be generated: \nv2_actor_first.pth and v2_critic_first.pth: The first time the agent(s) achieve an agent-average score of >30, averaged over all of the agents for the multi-agent version.  For a single agent, the agent-average score is the same as the individual score of the agent. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9111514400905234
      ],
      "excerpt": "v2_actor_best_agent_average.pth and v2_critic_best_agent_average.pth: The actor and critic network weights for the model that achieves the best agent average. This is only saved.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9481248926155464
      ],
      "excerpt": "The name of the checkpoint can be changed in Section 4 of the notebook. It is currently set up to run the agent through 100 episodes end provide scores and the final average score. The final parameter is the number of episodes to run and can also be changed: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9087772838050556
      ],
      "excerpt": "Continuous_Control_SNH.ipynb: Jupyter notebook to train the agent(s) and to save the trained the neural network weights as checkpoints. This notebook is set up for version 2 with multiple agents. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.955413554779079
      ],
      "excerpt": "These parameters and the implementation are discussed more in the file Report.md. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9177674984672025,
        0.939452146173442
      ],
      "excerpt": "update_every: The number of time steps between each updating of the neural networks  \nnum_updates: The number of times to update the networks at every update_every interval \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9277521956928245
      ],
      "excerpt": "network (string): The name of the neural networks that are used for learning. There are     only 2 choices, one with only 2 fully connected layers and RELU activations and one     with 3 fully connected layers with SELU activations. Their names are \"RELU\" and     \"SELU,\" respectively. Default is \"RELU.\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8966822276720685
      ],
      "excerpt": "    epsilon_initial (float): Initial value of epsilon for epsilon-greedy selection of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8730153461966612
      ],
      "excerpt": "        Higher is faster decay. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8726031645600245
      ],
      "excerpt": "    beta_initial (float): For prioritized replay. Corrects bias induced by weighted \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.891455462061632
      ],
      "excerpt": "        unless  prioritized experience replay is used. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8481044516665399
      ],
      "excerpt": "        targets instead of soft updating. \n",
      "technique": "Supervised classification"
    }
  ],
  "documentation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "http://ipython.readthedocs.io/",
      "technique": "Regular expression"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Download the environment from one of the links below.  You need only select the environment that matches your operating system:\n\n- **_Version 1: One (1) Agent_**\n  - Linux: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/one_agent/Reacher_Linux.zip)\n  - Mac OSX: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/one_agent/Reacher.app.zip)\n  - Windows (32-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/one_agent/Reacher_Windows_x86.zip)\n  - Windows (64-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/one_agent/Reacher_Windows_x86_64.zip)\n- **_Version 2: Twenty (20) Agents_**\n  - Linux: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher_Linux.zip)\n  - Mac OSX: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher.app.zip)\n  - Windows (32-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher_Windows_x86.zip)\n  - Windows (64-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher_Windows_x86_64.zip)\n\nUnzip (or decompress) the file which provides a folder.  Copy folder into the folder p2_continuous-control_SNH. If you want to run both, you must change the names of the decompressed folders since they have the same name. For example, I named the folders the following for versions 1 and 2, respectively:\n\n1. `Reacher_Windows_x86_64_v1`\n2. `Reacher_Windows_x86_64_v2`\n\nThe Jupyter notebook for running the code is called `Continuous_Control_SNH.ipynb`. The folder name indicated in Section1 of the notebook for starting the environment must match one of these.\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/snhwang/p2-continuous-control-SNH/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sat, 25 Dec 2021 15:53:48 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/snhwang/p2-continuous-control-SNH/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "snhwang/p2-continuous-control-SNH",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/snhwang/p2-continuous-control-SNH/master/Continuous_Control_SNH-pretrained.ipynb",
      "https://raw.githubusercontent.com/snhwang/p2-continuous-control-SNH/master/Continuous_Control_SNH.ipynb",
      "https://raw.githubusercontent.com/snhwang/p2-continuous-control-SNH/master/Continuous_Control_SNH-SELU.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Create an [IPython kernel](http://ipython.readthedocs.io/en/stable/install/kernel_install.html) for the `drlnd` environment:\n\n```\npython -m ipykernel install --user --name drlnd --display-name \"drlnd\"\n```\n\nThese steps only need to be performed once. \n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "The instructions at https://github.com/udacity/deep-reinforcement-learning indicate that you should enter the following on the command line to clone the the repository and install the dependencies:\n\n```\ngit clone https://github.com/udacity/deep-reinforcement-learning.git\ncd deep-reinforcement-learning/python\npip install .\n```\n\nHowever, for Windows 10, this did not work for me. The pip command fails when it tries to install torch 0.4.0. This version may no longer be available. I edited the dependencies shown in the requirements.txt file in the directory and changed the line for torch from\n\n `torch==0.4.0` to `torch==0.4.1`. \n\nThe pip command worked after the change. Otherwise you can install the required packages in the requirements folder manually. Sometimes these software packages change and you may need to refer to the specific instructions for an individual package. For example, https://pytorch.org/get-started/locally/ may be helpful for installing PyTorch. \n\nIf you clone the DRLND repository, the original files from the project can be found in the folder deep-reinforcement-learning\\p1_navigation\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "- **Linux** or **Mac**:\n\n  In a terminal window, perform the following commands:\n\n```\nconda create --name drlnd python=3.6\nsource activate drlnd\n```\n\n- **Windows**:\n\n  Make sure you are using the anaconda command line rather than the usual windows cmd.exe. \n\n```\nconda create --name drlnd python=3.6 \nactivate drlnd\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "  The installation of the software is accomplished with the package manager, conda. Installing Anaconda (https://www.anaconda.com/) will include conda as well as facilitate the installation of other data science software packages. The Jupyter Notebook App is also required for running this project and is installed automatically with Anaconda.\n\n  The dependencies for this project can be installed by following the instructions at https://github.com/udacity/deep-reinforcement-learning#dependencies.  Required components include but are are not limited to Python 3.6 (I specifically used 3.6.6), and PyTorch v0.4, and a version of the Unity ML-Agents toolkit. Note that ML-Agents are only supported on Microsoft Windows 10. I only used Windows 10, so cannot vouch for the accuracy of the instructions for other operating systems.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8442660805970531
      ],
      "excerpt": "We were given 2 choices with instructions to solve one: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8446017826602495
      ],
      "excerpt": "Although we were instructed to choose one of the above. I focused on the second option of 20 agents but the same code can be used for both. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9890348583315558,
        0.869220493838264,
        0.9554169436001461,
        0.9172361032193227,
        0.8712279230539444,
        0.9116099320754565,
        0.8820234121644812
      ],
      "excerpt": "In a terminal window, specifically an Anaconda terminal window for Microsoft Windows, activate the conda environment if not already done: \nLinux or Mac: \nsource activate drlnd \nMake sure you are using the anaconda command line rather than the usual windows cmd.exe.  \nactivate drlnd \nChange directory to the p1_navigate_SNH folder. Run Jupyter Notebook: \njupyter notebook \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9167203705163228
      ],
      "excerpt": "(taken from the Udacity instructions) \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8416647351223449
      ],
      "excerpt": "Change directory to the p1_navigate_SNH folder. Run Jupyter Notebook: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8603482154678495
      ],
      "excerpt": "v2_actor.pth and v2_critic.pth: The first time the agent(s) achieve a 100-episode-average score of >30, meaning the agent-average  score averaged over 100 episodes. Keep in mind, that the agent was changing during training during those 100 episodes. After training, a run of 100 episodes without any training is performed to see how well it performs.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8110457067478052
      ],
      "excerpt": "buffer_size: Buffer size for experience replay. Default 2e6. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8102192919040947
      ],
      "excerpt": "    tau_initial (float): Initial value for tau, the weighting factor for soft updating \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/snhwang/p2-continuous-control-SNH/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "HTML",
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Project 2: Continuous Control",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "p2-continuous-control-SNH",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "snhwang",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/snhwang/p2-continuous-control-SNH/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The instructions at https://github.com/udacity/deep-reinforcement-learning indicate that you should enter the following on the command line to clone the the repository and install the dependencies:\n\n```\ngit clone https://github.com/udacity/deep-reinforcement-learning.git\ncd deep-reinforcement-learning/python\npip install .\n```\n\nHowever, for Windows 10, this did not work for me. The pip command fails when it tries to install torch 0.4.0. This version may no longer be available. I edited the dependencies shown in the requirements.txt file in the directory and changed the line for torch from\n\n `torch==0.4.0` to `torch==0.4.1`. \n\nThe pip command worked after the change. Otherwise you can install the required packages in the requirements folder manually. Sometimes these software packages change and you may need to refer to the specific instructions for an individual package. For example, https://pytorch.org/get-started/locally/ may be helpful for installing PyTorch. \n\nIf you clone the DRLND repository, the original files from the project can be found in the folder deep-reinforcement-learning\\p1_navigation\n\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Create an [IPython kernel](http://ipython.readthedocs.io/en/stable/install/kernel_install.html) for the `drlnd` environment:\n\n```\npython -m ipykernel install --user --name drlnd --display-name \"drlnd\"\n```\n\nThese steps only need to be performed once. \n\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sat, 25 Dec 2021 15:53:48 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "  The installation of the software is accomplished with the package manager, conda. Installing Anaconda (https://www.anaconda.com/) will include conda as well as facilitate the installation of other data science software packages. The Jupyter Notebook App is also required for running this project and is installed automatically with Anaconda.\n\n  The dependencies for this project can be installed by following the instructions at https://github.com/udacity/deep-reinforcement-learning#dependencies.  Required components include but are are not limited to Python 3.6 (I specifically used 3.6.6), and PyTorch v0.4, and a version of the Unity ML-Agents toolkit. Note that ML-Agents are only supported on Microsoft Windows 10. I only used Windows 10, so cannot vouch for the accuracy of the instructions for other operating systems.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Create an [IPython kernel](http://ipython.readthedocs.io/en/stable/install/kernel_install.html) for the `drlnd` environment:\n\n```\npython -m ipykernel install --user --name drlnd --display-name \"drlnd\"\n```\n\nThese steps only need to be performed once. \n\n\n\n",
      "technique": "Header extraction"
    }
  ]
}