{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1706.07561",
      "https://arxiv.org/abs/1410.8516",
      "https://arxiv.org/abs/1706.07561",
      "https://arxiv.org/abs/1706.07561"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you use this code for your research, please cite our [paper](https://arxiv.org/abs/1706.07561):\n\n```\n@article{song2017nice,\n  title={A-NICE-MC: Adversarial Training for MCMC},\n  author={Song, Jiaming and Zhao, Shengjia and Ermon, Stefano},\n  journal={arXiv preprint arXiv:1706.07561},\n  year={2017}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{song2017nice,\n  title={A-NICE-MC: Adversarial Training for MCMC},\n  author={Song, Jiaming and Zhao, Shengjia and Ermon, Stefano},\n  journal={arXiv preprint arXiv:1706.07561},\n  year={2017}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8689723966931296,
        0.9788127900661171
      ],
      "excerpt": "Tensorflow implementation for the paper A-NICE-MC: Adversarial Training for MCMC, NIPS 2017. \nby Jiaming Song, Shengjia Zhao and Stefano Ermon, Stanford Artificial Intelligence Laboratory \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ermongroup/a-nice-mc",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[tsong@cs.stanford.edu](mailto:tsong@cs.stanford.edu)\n\nThis method is very new and experimental, so there might be cases where this fails (or because of poor parameter choices). We welcome all kinds of suggestions - including but not limited to \n\n- improving the method (MMD loss for `v`? other bootstrap techniques?) \n- additional experiments in other domains (some other applications that this method would shine?)\n- and how to improve the current code to make experiments more scalable (`save` and `load` feature?)\n\nIf something does not work as you would expect - please let me know. It helps everyone to know the strengths as well as weaknesses of the method.\n",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-06-11T17:09:42Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-10T22:52:41Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8091248184784988
      ],
      "excerpt": "Tensorflow implementation for the paper A-NICE-MC: Adversarial Training for MCMC, NIPS 2017. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8715902424890998
      ],
      "excerpt": "A-NICE-MC is a framework that trains a parametric Markov Chain Monte Carlo proposal. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9468124921920789,
        0.8726386347885726,
        0.8039335658053541,
        0.8241110040266205
      ],
      "excerpt": "This repository provides code to replicate the experiments, as well as providing grounds for further research. \nA-NICE-MC stands for Adversarial Non-linear Independent Component Estimation Monte Carlo, in that: \n- The framework utilizes a parametric proposal for Markov Chain Monte Carlo (MC). \n- The proposal is represented through Non-linear Independent Component Estimation (NICE). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9702657458326043
      ],
      "excerpt": "The running time depends on the machine, so only the ratio between running times of A-NICE-MC and HMC is particularly meaningful. Sanity check: during one update HMC computes the entire dataset for 40 + 1 times (HMC steps + MH step), while A-NICE-MC computes the entire dataset for only 1 time (only for MH step); so A-NICE-MC at this stage should not be 40x faster, but it seems reasonable that it is 10x faster. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8551982631921534,
        0.9425103677760058,
        0.9206769198577848
      ],
      "excerpt": "- A Metropolis-Hastings acceptance step (MH step), which accepts or rejects x_ according to p(x) and p(x_). \nIt might be tempting to use any generative model as the proposal; however, training is difficult because the kernel is non-differentiable, and score-based gradient estimator \nare not effective when initially the rejection rate is high. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8620670072853699,
        0.8000166873598527,
        0.9023843904921887
      ],
      "excerpt": "We can therefore use a NICE network x_, v_ = f(x, v) as our proposal, where v is the auxiliary variable we sample independently from x at every step. \nHence, we can treat f(x, v) as some \"implicit generative model\", which can be used to construct p(x_|x). \nWe use the following proposal to ensure p(x_, v_|x, v) = p(x, v|x_, v_) for all (x, v) and (x_, v_) pairs, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9394449182630016,
        0.9394449182630016
      ],
      "excerpt": "- For p = 0.5, x_, v_ = f(x, v) \n- For p = 0.5, x_, v_ = f^{-1}(x, v) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.917963263468574,
        0.8198560861041913,
        0.9773987804449629
      ],
      "excerpt": "Due to the invertibility of the NICE network, if the forward operation tranforms a point in the (x, v) manifold to another point in the (x, v) manifold, then the backward operation will do the same. Meanwhile, the forward operation will encourage the points to move toward p(x, v) and the MH step tends to reject backward operations, thereby removing random-walk behavior. \nIdeally we would like to reduce autocorrelation between the samples from the chain.  \nThis can be done by simply providing a pair of correlated data to the discriminator as generated data, so that the generator has the incentive to generate samples that are less correlated. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Code for \"A-NICE-MC: Adversarial Training for MCMC\"",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ermongroup/a-nice-mc/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 29,
      "date": "Wed, 29 Dec 2021 21:48:29 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ermongroup/a-nice-mc/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "ermongroup/a-nice-mc",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/ermongroup/a-nice-mc/master/figs/hmc_performance.ipynb",
      "https://raw.githubusercontent.com/ermongroup/a-nice-mc/master/figs/animation.ipynb"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.9563277988753164,
        0.9563277988753164
      ],
      "excerpt": "python examples/nice_ring2d.py \npython examples/nice_lord_of_rings.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9563277988753164,
        0.9563277988753164
      ],
      "excerpt": "python examples/nice_mog2.py \npython examples/nice_mog6.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9563277988753164
      ],
      "excerpt": "python examples/nice_australian.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9563277988753164
      ],
      "excerpt": "python examples/nice_german.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9563277988753164
      ],
      "excerpt": "python examples/nice_heart.py \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ermongroup/a-nice-mc/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2017 \\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "A-NICE-MC: Adversarial Training for MCMC",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "a-nice-mc",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "ermongroup",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ermongroup/a-nice-mc/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "jiamings",
        "body": "Code for reproducing the experiments for the NIPS submission.\r\n\r\nCurrently missing some features:\r\n- More flexible bootstrap setup\r\n- Save / load model\r\n- More comprehensive logging\r\n\r\nFuture Plans\r\n- Experiments on larger posterior distributions",
        "dateCreated": "2017-07-29T02:33:16Z",
        "datePublished": "2017-07-29T03:01:19Z",
        "html_url": "https://github.com/ermongroup/a-nice-mc/releases/tag/v0.1.0",
        "name": "Experiments for NIPS submission",
        "tag_name": "v0.1.0",
        "tarball_url": "https://api.github.com/repos/ermongroup/a-nice-mc/tarball/v0.1.0",
        "url": "https://api.github.com/repos/ermongroup/a-nice-mc/releases/7214885",
        "zipball_url": "https://api.github.com/repos/ermongroup/a-nice-mc/zipball/v0.1.0"
      }
    ],
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The code depends on tensorflow >= 1.0, numpy, scipy, matplotlib, and pandas.\nIt has been tested on both Python 2 and Python 3.\n\nThe Effective Sample Size metric for evaluating MCMC algorithms will appear on screen, and is stored in `logs/[experiment_name]/ess.csv`.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 119,
      "date": "Wed, 29 Dec 2021 21:48:29 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "tensorflow",
      "generative-models",
      "tensorflow-experiments",
      "markov-chain",
      "markov-chain-monte-carlo",
      "generative-adversarial-network",
      "bayesian-inference",
      "bayesian-machine-learning",
      "neural-networks"
    ],
    "technique": "GitHub API"
  }
}