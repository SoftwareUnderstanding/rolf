{
  "citation": [
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "    if ignore_grad: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9947666518110279
      ],
      "excerpt": "The BART paper -- https://arxiv.org/pdf/1910.13461.pdf \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/priyamtejaswin/multistep-retrieve-summarize",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-08-20T09:23:28Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-08-25T16:41:32Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8237340789991203
      ],
      "excerpt": "Here, the task is a FairseqTask (or can be one of the specific Translation, Classification, LM tasks). The train_step function uses native PyTorch to run forward, backward passes. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "    model.set_num_updates(update_num) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9180851872446082
      ],
      "excerpt": "Sample loss is returned at the end, to the self.task.train_step \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665730016990196
      ],
      "excerpt": "Class for Trainer that implements the train_step for a list of samples in an epoch -- https://github.com/pytorch/fairseq/blob/master/fairseq/trainer.py#L39 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8520924683982005,
        0.927901022630757
      ],
      "excerpt": "The bart.base model reigistration, along with all model params and arguments -- https://github.com/pytorch/fairseq/blob/master/fairseq/models/bart/model.py#L297 \nIssue for mismatch in BASE and LARGE vocab sizes; fix is to change the truncate the weights and save -- https://github.com/pytorch/fairseq/issues/2242 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8906878683467856
      ],
      "excerpt": "Issue discussing the confusion on MAX_TOKENS in Bart for Summarization (the README is broken) -- https://github.com/pytorch/fairseq/issues/1685 \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/priyamtejaswin/multistep-retrieve-summarize/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 29 Dec 2021 15:16:37 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/priyamtejaswin/multistep-retrieve-summarize/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "priyamtejaswin/multistep-retrieve-summarize",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8361036613298458
      ],
      "excerpt": "Once invoked, calls def cli_main in https://github.com/pytorch/fairseq/blob/master/fairseq_cli/train.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8869658483842845
      ],
      "excerpt": "The main while loop involed by fairseq-train -- https://github.com/pytorch/fairseq/blob/master/fairseq_cli/train.py#L117 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8095710235104745,
        0.915015396324703,
        0.9014157084258344,
        0.8234685689140104
      ],
      "excerpt": "Issue for fine-tuning with limited resources; lots of useful tips -- https://github.com/pytorch/fairseq/issues/1413 \nIssue for fine-tuning with different vocab sizes -- https://github.com/pytorch/fairseq/issues/2120 \nIssue discussing the confusion on MAX_TOKENS in Bart for Summarization (the README is broken) -- https://github.com/pytorch/fairseq/issues/1685 \nIssue regarding training time, resources for BARTBase  -- https://github.com/pytorch/fairseq/issues/1651 \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8932702598131124
      ],
      "excerpt": "fairseq-train is a py file; run which fairseq-train to get location. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8421785863173823
      ],
      "excerpt": "This checks for some args, and then calls distributed_main which in-turn calls the def main function in fairseq_cli/train.py -- that is the main training routine. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8088443071661184,
        0.9042358314711066
      ],
      "excerpt": "Trainer sets up the devices, params, etc, required for parallel training and returns a Trainer object to the main function in train.py \ntrain.py will now setup some stuff to start the training -- loading from the last checkpoint, epochs, meters, etc. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8386195858690308
      ],
      "excerpt": "    model.train()  #: This changes the model from `eval` mode to `train` mode!!! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.907516349586204
      ],
      "excerpt": "The train function in train.py finishes all samples in the epoch. After completing the epoch, it logs some stats, resets some meters and returns the epoch losses with should_stop to the while training loop in def main in the same file. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8175216899106563
      ],
      "excerpt": "def main ends with logging a done training message. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/priyamtejaswin/multistep-retrieve-summarize/issues{/number}",
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "multistep-retrieve-summarize",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "multistep-retrieve-summarize",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "priyamtejaswin",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/priyamtejaswin/multistep-retrieve-summarize/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 29 Dec 2021 15:16:37 GMT"
    },
    "technique": "GitHub API"
  }
}