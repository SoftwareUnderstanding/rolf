{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We would like to thank the following people who have helped and guided us through the course of my final year project.\n\nFirst of all, we want to express our deepest gratitude to our final year project supervisor Assistant Professor Feng Jiashi for his patient listening, supportive mentoring and inspiring guidance. Prof. Feng has kindly arranged weekly meetings for us to update the project progress, during which he always provided timely feedback and pointed out new research directions. We cannot finish this project without his support.\n\nWe want to thank Dr. Liew Jun Hao, who has given us a lot of suggestions as well as encouragement whenever we had doubts or difficulties in our project. We also want to thank our final year project examiner Associate Professor Robby Tan, who has listened to our CA1 presentation and given advice on how we could improve.\n\nIt was our great honour to join the big family of Learning & Vision lab led by Prof. Feng, where all the lab-mates are extremely helpful, and they have given me lots of support.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "The authors are grateful to\nNvidia, Huawei Noah's Ark Lab, ByteDance, Adobe who generously donated GPU computing in the past a few years.\n\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1912.04488",
      "https://arxiv.org/abs/2012.02310",
      "https://arxiv.org/abs/1911.07451",
      "https://arxiv.org/abs/1904.01355",
      "https://arxiv.org/abs/2001.00309",
      "https://arxiv.org/abs/2003.11712",
      "https://arxiv.org/abs/2002.10200",
      "https://arxiv.org/abs/2003.05664",
      "https://arxiv.org/abs/1911.07451"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you use this toolbox in your research or wish to refer to the baseline results published here, please use the following BibTeX entries:\n\n```BibTeX\n@misc{tian2019adelaidet,\n  author =       {Tian, Zhi and Chen, Hao and Wang, Xinlong and Liu, Yuliang and Shen, Chunhua},\n  title =        {{AdelaiDet}: A Toolbox for Instance-level Recognition Tasks},\n  howpublished = {\\url{https://git.io/adelaidet}},\n  year =         {2019}\n}\n\n@inproceedings{tian2019fcos,\n  title     =  {{FCOS}: Fully Convolutional One-Stage Object Detection},\n  author    =  {Tian, Zhi and Shen, Chunhua and Chen, Hao and He, Tong},\n  booktitle =  {Proc. Int. Conf. Computer Vision (ICCV)},\n  year      =  {2019}\n}\n\n@inproceedings{chen2020blendmask,\n  title     =  {{BlendMask}: Top-Down Meets Bottom-Up for Instance Segmentation},\n  author    =  {Chen, Hao and Sun, Kunyang and Tian, Zhi and Shen, Chunhua and Huang, Yongming and Yan, Youliang},\n  booktitle =  {Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)},\n  year      =  {2020}\n}\n\n@inproceedings{zhang2020MEInst,\n  title     =  {Mask Encoding for Single Shot Instance Segmentation},\n  author    =  {Zhang, Rufeng and Tian, Zhi and Shen, Chunhua and You, Mingyu and Yan, Youliang},\n  booktitle =  {Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)},\n  year      =  {2020}\n}\n\n@inproceedings{liu2020abcnet,\n  title     =  {{ABCNet}: Real-time Scene Text Spotting with Adaptive {B}ezier-Curve Network},\n  author    =  {Liu, Yuliang and Chen, Hao and Shen, Chunhua and He, Tong and Jin, Lianwen and Wang, Liangwei},\n  booktitle =  {Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)},\n  year      =  {2020}\n}\n\n@inproceedings{wang2020solo,\n  title     =  {{SOLO}: Segmenting Objects by Locations},\n  author    =  {Wang, Xinlong and Kong, Tao and Shen, Chunhua and Jiang, Yuning and Li, Lei},\n  booktitle =  {Proc. Eur. Conf. Computer Vision (ECCV)},\n  year      =  {2020}\n}\n\n@inproceedings{wang2020solov2,\n  title   =  {{SOLOv2}: Dynamic and Fast Instance Segmentation},\n  author  =  {Wang, Xinlong and Zhang, Rufeng and Kong, Tao and Li, Lei and Shen, Chunhua},\n  booktitle =  {Proc. Advances in Neural Information Processing Systems (NeurIPS)},\n  year    =  {2020}\n}\n\n@article{tian2019directpose,\n  title   =  {{DirectPose}: Direct End-to-End Multi-Person Pose Estimation},\n  author  =  {Tian, Zhi and Chen, Hao and Shen, Chunhua},\n  journal =  {arXiv preprint arXiv:1911.07451},\n  year    =  {2019}\n}\n\n@inproceedings{tian2020conditional,\n  title     =  {Conditional Convolutions for Instance Segmentation},\n  author    =  {Tian, Zhi and Shen, Chunhua and Chen, Hao},\n  booktitle =  {Proc. Eur. Conf. Computer Vision (ECCV)},\n  year      =  {2020}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{tian2020conditional,\n  title     =  {Conditional Convolutions for Instance Segmentation},\n  author    =  {Tian, Zhi and Shen, Chunhua and Chen, Hao},\n  booktitle =  {Proc. Eur. Conf. Computer Vision (ECCV)},\n  year      =  {2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{tian2019directpose,\n  title   =  {{DirectPose}: Direct End-to-End Multi-Person Pose Estimation},\n  author  =  {Tian, Zhi and Chen, Hao and Shen, Chunhua},\n  journal =  {arXiv preprint arXiv:1911.07451},\n  year    =  {2019}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{wang2020solov2,\n  title   =  {{SOLOv2}: Dynamic and Fast Instance Segmentation},\n  author  =  {Wang, Xinlong and Zhang, Rufeng and Kong, Tao and Li, Lei and Shen, Chunhua},\n  booktitle =  {Proc. Advances in Neural Information Processing Systems (NeurIPS)},\n  year    =  {2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{wang2020solo,\n  title     =  {{SOLO}: Segmenting Objects by Locations},\n  author    =  {Wang, Xinlong and Kong, Tao and Shen, Chunhua and Jiang, Yuning and Li, Lei},\n  booktitle =  {Proc. Eur. Conf. Computer Vision (ECCV)},\n  year      =  {2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{liu2020abcnet,\n  title     =  {{ABCNet}: Real-time Scene Text Spotting with Adaptive {B}ezier-Curve Network},\n  author    =  {Liu, Yuliang and Chen, Hao and Shen, Chunhua and He, Tong and Jin, Lianwen and Wang, Liangwei},\n  booktitle =  {Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)},\n  year      =  {2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{zhang2020MEInst,\n  title     =  {Mask Encoding for Single Shot Instance Segmentation},\n  author    =  {Zhang, Rufeng and Tian, Zhi and Shen, Chunhua and You, Mingyu and Yan, Youliang},\n  booktitle =  {Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)},\n  year      =  {2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{chen2020blendmask,\n  title     =  {{BlendMask}: Top-Down Meets Bottom-Up for Instance Segmentation},\n  author    =  {Chen, Hao and Sun, Kunyang and Tian, Zhi and Shen, Chunhua and Huang, Yongming and Yan, Youliang},\n  booktitle =  {Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)},\n  year      =  {2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{tian2019fcos,\n  title     =  {{FCOS}: Fully Convolutional One-Stage Object Detection},\n  author    =  {Tian, Zhi and Shen, Chunhua and Chen, Hao and He, Tong},\n  booktitle =  {Proc. Int. Conf. Computer Vision (ICCV)},\n  year      =  {2019}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{tian2019adelaidet,\n  author =       {Tian, Zhi and Chen, Hao and Wang, Xinlong and Liu, Yuliang and Shen, Chunhua},\n  title =        {{AdelaiDet}: A Toolbox for Instance-level Recognition Tasks},\n  howpublished = {\\url{https://git.io/adelaidet}},\n  year =         {2019}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.982605551168708
      ],
      "excerpt": "This is a final year project done by four final year student in NUS: Ai Zhengwei, Guo Haoren, Huang Lixing and Zhao Zhijie. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8550101043698384
      ],
      "excerpt": "| FCOS R50 RT | 75.465 | 89.754| 10 | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/zhaozhijie1997/Unifed-Lane-and-Traffic-Sign-detection",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-04-05T08:59:37Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-04-24T05:58:00Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8787722992398376,
        0.9785978854836377,
        0.9857840858694549,
        0.8840542899277465,
        0.8522765395362475,
        0.8209693186133599,
        0.9698579495550311,
        0.9454505416700897,
        0.931676870585698,
        0.8979602771240175
      ],
      "excerpt": "The project began in August 2020, start of the Semester one of Year 4. \nThis project aims for developing a high performance and light-weight model with lane detection and traffic sign detection features. \nThe combined model was built on AdelaiDet  which is on top of Detectron2 and modified Ultra-fast architecture to be fitted into FCOS model. \nThe main contribution of this project can be summarized as follows: \nA combined model that can perform traffic sign detection and lane detection at the same time with good accuracy and efficiency. \nWe show how to train a unified model on two disjoint datasets (i.e. lane detection dataset without traffic sign annotations and vice versa). \nCoordConv which incorporates positional information to optimize the based model. \nApply FCOS model to traffic sign detection branch to improve the efficiency of traffic sign detection but keep similar average precision. \nUltra-Fast model to lane detection branch and apply Cycle Gan to enhance the performance for night image detection. \nCycleGAN to synthesize realistic night-time images to augment the existing dataset, encouraging the model to perform better even on nighttime. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8057551791408164
      ],
      "excerpt": "|            |Original Ultra-Fast Model|New Combined Model|New Combined Model (no padding loss)| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.90387120413718,
        0.902078113666546
      ],
      "excerpt": "AdelaiDet is an open source toolbox for multiple instance-level recognition tasks on top of Detectron2. \nAll instance-level recognition works from our group are open-sourced here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9434768973015376
      ],
      "excerpt": "For more models and information, please refer to ABCNet README.md. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8793793675764062
      ],
      "excerpt": "We set OMP_NUM_THREADS=1 by default, which achieves the best speed on our machines, please change it as needed. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/zhaozhijie1997/Unifed-Lane-and-Traffic-Sign-detection/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Tue, 28 Dec 2021 12:57:39 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/zhaozhijie1997/Unifed-Lane-and-Traffic-Sign-detection/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "zhaozhijie1997/Unifed-Lane-and-Traffic-Sign-detection",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/zhaozhijie1997/Unifed-Lane-and-Traffic-Sign-detection/master/docker/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/zhaozhijie1997/Unifed-Lane-and-Traffic-Sign-detection/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/zhaozhijie1997/Unifed-Lane-and-Traffic-Sign-detection/master/onnx/pytorch-onnx-caffe-ncnn.sh",
      "https://raw.githubusercontent.com/zhaozhijie1997/Unifed-Lane-and-Traffic-Sign-detection/master/onnx/pytorch-onnx-caffe-ncnn-rt.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "First install Detectron2 following the official guide: [INSTALL.md](https://github.com/facebookresearch/detectron2/blob/master/INSTALL.md).\n\n*Please use Detectron2 with commit id [9eb4831](https://github.com/facebookresearch/detectron2/commit/9eb4831f742ae6a13b8edb61d07b619392fb6543) if you have any issues related to Detectron2.*\n\nThen build AdelaiDet with:\n\n```\ngit clone https://github.com/aim-uofa/AdelaiDet.git\ncd AdelaiDet\npython setup.py build develop\n```\n\nIf you are using docker, a pre-built image can be pulled with:\n\n```\ndocker pull tianzhi0549/adet:latest\n```\n\nSome projects may require special setup, please follow their own `README.md` in [configs](configs).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9717106327039013
      ],
      "excerpt": "SOLO (mmdet version) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8431264577996327
      ],
      "excerpt": "| Name                                                 | inf. time | e2e-hmean | det-hmean |                           download                           | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8826220462631758
      ],
      "excerpt": "setup the corresponding datasets following \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8838377520179606
      ],
      "excerpt": "Note that: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8017778607929922
      ],
      "excerpt": "| Model      |    AP    |   AP50   |   FPS  |                      \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8157439033119126
      ],
      "excerpt": "| FCOS_R_50_1x          |  16 FPS   |  38.7  | model | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8017778607929922,
        0.8017778607929922,
        0.8017778607929922
      ],
      "excerpt": "| FCOS_MS_X_101_32x8d_2x |  6.6 FPS  |  43.9  | model | \n| FCOS_MS_X_101_32x8d_dcnv2_2x |  4.6 FPS  |  46.6  | model | \n| FCOS_RT_MS_DLA_34_4x_shtw |  52 FPS   |  39.1  | model | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8612558838572962
      ],
      "excerpt": "| Name                                                 | inf. time | e2e-hmean | det-hmean |                           download                           | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9008116137719951
      ],
      "excerpt": "Pick a model and its config file, for example, fcos_R_50_1x.yaml. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8421174927206765,
        0.9057698139330033
      ],
      "excerpt": "Run the demo with \npython demo/demo.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.881670876222929
      ],
      "excerpt": "    --input input1.jpg input2.jpg \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9366641523006497
      ],
      "excerpt": "To train a model with \"train_net.py\", first \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8117007451991057,
        0.810477497050042
      ],
      "excerpt": "then run: \nOMP_NUM_THREADS=1 python tools/train_net.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8272690102910759,
        0.810477497050042
      ],
      "excerpt": "To evaluate the model after training, run: \nOMP_NUM_THREADS=1 python tools/train_net.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8183947818984721
      ],
      "excerpt": "The configs are made for 8-GPU training. To train on another number of GPUs, change the --num-gpus. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/zhaozhijie1997/Unifed-Lane-and-Traffic-Sign-detection/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Cuda",
      "C++",
      "Shell",
      "Dockerfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other",
      "url": "https://raw.githubusercontent.com/zhaozhijie1997/Unifed-Lane-and-Traffic-Sign-detection/master/LICENSE"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'AdelaiDet for non-commercial purposes\\n\\nCopyright (c) 2019 the authors\\nAll rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met:\\n\\n Redistributions of source code must retain the above copyright notice, this\\n  list of conditions and the following disclaimer.\\n\\n Redistributions in binary form must reproduce the above copyright notice,\\n  this list of conditions and the following disclaimer in the documentation\\n  and/or other materials provided with the distribution.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "NUS AY20/21 EE4002D Final Year Project: Real-time Detection of Interest Regions in Multiple Weather and Lighting Conditions",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Unifed-Lane-and-Traffic-Sign-detection",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "zhaozhijie1997",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/zhaozhijie1997/Unifed-Lane-and-Traffic-Sign-detection/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Tue, 28 Dec 2021 12:57:39 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The following are the results we got from the combined model:\n\n![image-20210415131726604](/demo/demo1.png)\n\n![image-20210415131742355](/demo/demo2.png)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "* [Demo Video 1](https://www.youtube.com/watch?v=JZg8QlqiTq4)\n* [Demo Video 2](https://www.youtube.com/watch?v=TsPYx1J9Bn8)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "| Model      |                             Name                             | inf. time | box AP | mask AP |                           download                           |\n| ---------- | :----------------------------------------------------------: | :-------: | :----: | :-----: | :----------------------------------------------------------: |\n| Mask R-CNN | [R_101_3x](https://github.com/facebookresearch/detectron2/blob/master/configs/COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml) |  10 FPS   |  42.9  |  38.6   |                                                              |\n| BlendMask  |         [R_101_3x](configs/BlendMask/R_101_3x.yaml)          |  11 FPS   |  44.8  |  39.5   | [model](https://cloudstor.aarnet.edu.au/plus/s/e4fXrliAcMtyEBy/download) |\n| BlendMask  |   [R_101_dcni3_5x](configs/BlendMask/R_101_dcni3_5x.yaml)    |  10 FPS   |  46.8  |  41.1   | [model](https://cloudstor.aarnet.edu.au/plus/s/vbnKnQtaGlw8TKv/download) |\n\nFor more models and information, please refer to BlendMask [README.md](configs/BlendMask/README.md).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "| Name                                                         | inf. time | box AP | mask AP |                           download                           |\n| ------------------------------------------------------------ | :-------: | :----: | :-----: | :----------------------------------------------------------: |\n| [MEInst_R_50_3x](https://github.com/aim-uofa/AdelaiDet/configs/MEInst-InstanceSegmentation/MEInst_R_50_3x.yaml) |  12 FPS   |  43.6  |  34.5   | [model](https://cloudstor.aarnet.edu.au/plus/s/1ID0DeuI9JsFQoG/download) |\n\nFor more models and information, please refer to MEInst [README.md](configs/MEInst-InstanceSegmentation/README.md).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "| Name                                                         | inf. time | box AP | mask AP |                           download                           |\n| ------------------------------------------------------------ | :-------: | :----: | :-----: | :----------------------------------------------------------: |\n| [CondInst_MS_R_50_1x](configs/CondInst/MS_R_50_1x.yaml)      |  14 FPS   |  39.7  |  35.7   | [model](https://cloudstor.aarnet.edu.au/plus/s/Trx1r4tLJja7sLT/download) |\n| [CondInst_MS_R_50_BiFPN_3x_sem](configs/CondInst/MS_R_50_BiFPN_3x_sem.yaml) |  13 FPS   |  44.7  |  39.4   | [model](https://cloudstor.aarnet.edu.au/plus/s/9cAHjZtdaAGnb2Q/download) |\n| [CondInst_MS_R_101_3x](configs/CondInst/MS_R_101_3x.yaml)    |  11 FPS   |  43.3  |  38.6   | [model](https://cloudstor.aarnet.edu.au/plus/s/vWLiYm8OnrTSUD2/download) |\n| [CondInst_MS_R_101_BiFPN_3x_sem](configs/CondInst/MS_R_101_BiFPN_3x_sem.yaml) |  10 FPS   |  45.7  |  40.2   | [model](https://cloudstor.aarnet.edu.au/plus/s/2p1ashxl54Su8vv/download) |\n\nFor more models and information, please refer to CondInst [README.md](configs/CondInst/README.md).\n\nNote that:\n\n- Inference time for all projects is measured on a NVIDIA 1080Ti with batch size 1.\n- APs are evaluated on COCO2017 val split unless specified.\n\n\n",
      "technique": "Header extraction"
    }
  ]
}