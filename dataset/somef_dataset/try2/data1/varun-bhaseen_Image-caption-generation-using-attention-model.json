{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1607.06450, 2016.\n\n[4] Hedi Ben-younes, Remi Cadene, Matthieu Cord, and Nicolas Thome. Mutan: Multimodal tucker fusion for visual question answering. In ICCV, 2017.\n\n[5] Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks. In NeurIPS, 2015.\n\n[6] Long Chen, Hanwang Zhang, Jun Xiao, Liqiang Nie, Jian Shao, Wei Liu, and Tat-Seng Chua. Sca-cnn: Spatial and channel-wise attention in convolutional networks for image captioning. In CVPR, 2017.\n\n[7] Yangyu Chen, Shuhui Wang, Weigang Zhang, and Qingming Huang. Less is more: Picking informative frames for video captioning. In ECCV, 2018.\n\n[8] Marco Pedersoli and Thomas Lucas and Cordelia Schmid and Jakob Verbeek: Areas of Attention for Image Captioning, arXiv, 2016.\n\n[9] Sen He, Wentong Liao, Hamed R. Tavakoli, Michael Yang, Bodo Rosenhahn, and Nicolas Pugeault: Image Captioning through Image Transformer, arXiv, 2020.\n\n\n## Links we referred to:\n\n* Local Attention : https://arxiv.org/pdf/1502.03044.pdf\n* Global Attention : https://arxiv.org/pdf/1508.04025.pdf\n* https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/4150/attention-models-in-deep-learning/8/module-8-neural-networks-computer-vision-and-deep-learning\n* Tensorflow Blog: https://www.tensorflow.org/tutorials/text/image_captioning\n* https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n* https://towardsdatascience.com/intuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f\n* Neural Machine Translation(Research Paper):https://arxiv.org/pdf/1409.0473.pdf\n\n"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[1] Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. Spice: Semantic propositional image caption evaluation. In ECCV, 2016.\n\n[2] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering. In CVPR, 2018.\n\n[3] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.\n\n[4] Hedi Ben-younes, Remi Cadene, Matthieu Cord, and Nicolas Thome. Mutan: Multimodal tucker fusion for visual question answering. In ICCV, 2017.\n\n[5] Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks. In NeurIPS, 2015.\n\n[6] Long Chen, Hanwang Zhang, Jun Xiao, Liqiang Nie, Jian Shao, Wei Liu, and Tat-Seng Chua. Sca-cnn: Spatial and channel-wise attention in convolutional networks for image captioning. In CVPR, 2017.\n\n[7] Yangyu Chen, Shuhui Wang, Weigang Zhang, and Qingming Huang. Less is more: Picking informative frames for video captioning. In ECCV, 2018.\n\n[8] Marco Pedersoli and Thomas Lucas and Cordelia Schmid and Jakob Verbeek: Areas of Attention for Image Captioning, arXiv, 2016.\n\n[9] Sen He, Wentong Liao, Hamed R. Tavakoli, Michael Yang, Bodo Rosenhahn, and Nicolas Pugeault: Image Captioning through Image Transformer, arXiv, 2020.\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8944178096468923
      ],
      "excerpt": "* Video \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9742177718389462,
        0.9826628511165726
      ],
      "excerpt": "Local Attention : https://arxiv.org/pdf/1502.03044.pdf \nGlobal Attention : https://arxiv.org/pdf/1508.04025.pdf \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/varun-bhaseen/Image-caption-generation-using-attention-model",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-09-18T04:04:45Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-09-18T05:38:24Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "![alt text](https://github.com/saeedabi1/deep_learning_project/blob/master/pictures/Screen%20Shot%202020-05-23%20at%205.08.25%20PM.png?raw=true)\n\n\n\n![alt text](https://github.com/saeedabi1/deep_learning_project/blob/master/pictures/Screen%20Shot%202020-05-23%20at%205.08.39%20PM.png?raw=true)\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Attention mechanisms are broadly used in present image captioning encoder / decoder frameworks, where at each step a weighted average is generated on encoded vectors to direct the process of caption decoding. \nHowever, the decoder has no knowledge of whether or how well the vector being attended and the attention question being given are related, which may result in the decoder providing erroneous results. \nImage captioning, that is to say generating natural automatic descriptions of language images are useful for visually impaired images and for the quest of natural language related pictures. \nIt is significantly more demanding than traditional vision tasks recognition of objects and classification of images for two guidelines. \nFirst, well formed structured output space natural language sentences are considerably more challenging than just a set of class labels to predict. \nSecondly, this dynamic output space enables a more thin understanding of the visual scenario, and therefore also a more informative one visual scene analysis to do well on this task.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9400637371141104,
        0.9748282023421272,
        0.9863007213643508,
        0.9930875241171491,
        0.9315304985539931
      ],
      "excerpt": "Deep Learning CMPE 258 - Term Project - Professor Vijay Eranti - Spring 2020 \nDuring generating the next word of the caption, this word is usually describing only a part of the image.  \nCapturing the essence of the entire input image is unable \nUsing the full representation of the image \u201ch\u201d to help in the process of generating each word cannot produce different words for different parts of the image.  \nUsing Attention mechanism to solve these problems, by focusing on certain important features that may be present in the image. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9957682741884074,
        0.987767961125644,
        0.9662688415354322
      ],
      "excerpt": "The problem with this method is that, when the model attempts to generate the caption next word, that word usually only describes a part of the image.  \nIt is not capable of capturing the essence of the whole input image.  \nUsing the entire representation of image to condition the generation of each word can not generate different words effectively for different parts of the image.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8368981416123239
      ],
      "excerpt": "To address this problem, there are many open source datasets available, such as  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9131307583082239,
        0.973053179246838
      ],
      "excerpt": "Training a model with a large number of images on a system that is not a very high end PC / Laptop may not be feasible either. \nThis dataset contains 8000 images with 5 captions each (as we already saw in the Introduction section that an image can have multiple captions, all of which are relevant at the same time). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9806955608536236
      ],
      "excerpt": "* The code below generates an instance of the VGG16 model utilizing the Keras API.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9644389455980547,
        0.9979612800455862,
        0.8884829626912978,
        0.974800675161481,
        0.8794435019425029,
        0.864500043438574
      ],
      "excerpt": "The VGG16 model was pre-trained for classification of photos on the ImageNet data-set.  \nThe VGG16 model contains a convolutionary part and a full-connected (or dense) part that is used to classify the image. \nhe whole VGG16 model is downloaded which is about 528 MB if include_top=True. \nOnly the convolutional part of the VGG16 model is downloaded which is just 57 MB if include_top=False \nUsing fully connected layers in this pre-trained model, therefore downloading the full model is needed. \nThe ENCODER output, hidden state(initialised to 0) and the DECODER input(which is the <start> token) are passed to the DECODER. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9592948198917318,
        0.8508229423184355,
        0.9603245357663206
      ],
      "excerpt": "The DECODER hidden state is then passed back into the model and the predictions are used to calculate the loss. \nWhile training, we use the Teacher Forcing technique, to decide the next input of the DECODER. \nTeacher Forcing is the technique where the target word is passed as the next input to the DECODER.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9793271105809958,
        0.9449162475149702,
        0.8946279693589196,
        0.8714287846548584
      ],
      "excerpt": "Final step is to calculate the Gradient and apply it to the optimizer and backpropagate. \nIt is similar to training step, just that we do not update the gradients, and provide the predicted output as decoder input to next RNN cell at next time steps. \nTest step is required to find out whether the model built is overfitting or not. \nThe evaluate function is similar to the training loop \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8367990545494696,
        0.811858381654858,
        0.9640481365816864,
        0.8137265826446605
      ],
      "excerpt": "The input to Decoder at each time step is its previous predictions, along with the hidden state and the ENCODER output. \nFew key points to remember while making predictions. \nStop predicting when the model predicts the end token. \nStore the attention weights for every time step. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9797472538558628,
        0.9598678702826865
      ],
      "excerpt": "Selecting that word which is most likely according to the model for the given input.  \nIt\u2019s also called as Greedy Search, as we greedily select the word with maximum probability. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8197961624351802,
        0.8874271848932603
      ],
      "excerpt": "Feed them again in the model \nSort them using the probabilities returned by the model.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.818730641288944,
        0.8169143563999578
      ],
      "excerpt": "Taking the one with the highest probability \nGoing through it till we encounter <end> or reach the maximum caption length. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9838736458379086,
        0.9635478021036623
      ],
      "excerpt": "The BLEU is simply taking the fraction of n-grams in the predicted sentence that appears in the ground-truth. \nBLEU is a well-acknowledged metric to measure the similarity of one hypothesis sentence to multiple reference sentences.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9229664639785994,
        0.8917656927657855,
        0.9760868720475748,
        0.9463357335380875,
        0.9346192067605783
      ],
      "excerpt": "The metric close to 1 means that the two are very similar. \nThis is an example where the distribution of the train and test sets will be very different and no Machine Learning model will deliver good performance in the world in such cases.  \nOverall, we have to admit that our simple first-cut model does a good job of producing captions for pictures, without any stringent hyper-parameter tuning. \nWe have to recognize that the photos used for research have to be semantically linked to those used in model training.  \nFor eg, if we train our model on the photos of cats, dogs, etc., we don't have to check it on airplane photographs, waterfalls, etc.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8355485376741364
      ],
      "excerpt": "The results only improved after we did the following: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8536429351032906
      ],
      "excerpt": "For tuning Method 1\u2019s  Model architecture: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9069235145752237
      ],
      "excerpt": "* Notebook with: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.991286515234003,
        0.9682101082387602
      ],
      "excerpt": "A CNN processes the input image in its basic form to transform it into a vector representation which is used as the initial input for an RNN. Given the previous word, the RNN sequentially predicts the next word in the caption without limiting the temporal dependence to a fixed order, as in n-gram-based approaches.  \nThe representation of the CNN image can be entered in different ways in the RNN.While some authors [6, 7] Use this only to calculate the initial RNN status, while others enter it in each RNN iteration [8, 9]. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "CMPE 258 Deep Learning Project",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/varun-bhaseen/Image-caption-generation-using-attention-model/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Fri, 24 Dec 2021 03:44:07 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/varun-bhaseen/Image-caption-generation-using-attention-model/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "varun-bhaseen/Image-caption-generation-using-attention-model",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/varun-bhaseen/Image-caption-generation-using-attention-model/master/High%20memory%20Ubuntu%20Machine/Project/Dockerfile",
      "https://raw.githubusercontent.com/varun-bhaseen/Image-caption-generation-using-attention-model/master/High%20memory%20Ubuntu%20Machine/Image_caption_2/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/varun-bhaseen/Image-caption-generation-using-attention-model/master/Method1/Colab_Photo_Caption_Generation_using_Global_and_Local_Attention.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9295050805894965
      ],
      "excerpt": "Taking the one with the highest probability \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8174540907975313
      ],
      "excerpt": "  * Training \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/varun-bhaseen/Image-caption-generation-using-attention-model/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python",
      "HTML",
      "CSS",
      "Dockerfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Photo Caption Generation using Global and Local Attention",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Image-caption-generation-using-attention-model",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "varun-bhaseen",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/varun-bhaseen/Image-caption-generation-using-attention-model/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Fri, 24 Dec 2021 03:44:07 GMT"
    },
    "technique": "GitHub API"
  }
}