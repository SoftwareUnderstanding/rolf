{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1609.08144\n1. https://arxiv.org/pdf/1506.07285.pdf\n\n\n\n# Author Contributions <br/>\n### Project <br/>\nPreprocessing; Daniele, Simge <br/>\nBaseline LSTM Network: Daniele<br/> \nBeam Search; Simge <br/>\nAttention Network; Daniele <br/>\n\n### Report <br/>\nAbstract; Simge <br/>\nIntroduction; Simge <br/>\nRelated Work; Simge, Daniele <br/>\nProject Description; Simge, Daniele <br/>\nSummary; Daniele <br/>\nConsclusion; Daniele <br/>"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. https://www.researchgate.net/profile/Xiaodong_He2/publication/306093902_Deep_Reinforcement_Learning_with_a_Natural_Language_Action_Space/links/57c4656b08aee465796c1fa3.pdf\n1. http://www.eecs.qmul.ac.uk/~josh/documents/2017/Chourdakis%20Reiss%20-%20CC-NLG.pdf\n1. http://papers.nips.cc/paper/6233-hierarchical-deep-reinforcement-learning-integrating-temporal-abstraction-and-intrinsic-motivation.pdf\n1. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.696.7314&rep=rep1&type=pdf\n1. https://arxiv.org/pdf/1506.07285.pdf\n1. http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\n1. https://arxiv.org/abs/1609.08144\n1. https://arxiv.org/pdf/1506.07285.pdf\n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8444342525991423
      ],
      "excerpt": "From wikipedia (https://en.wikipedia.org/wiki/Interactive_fiction): \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/macco3k/deepstories",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-04-08T12:17:28Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-07-05T09:56:17Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9410344745066036,
        0.8491508108653278,
        0.8018795877429536,
        0.9839565850576987,
        0.9741433986158008,
        0.9364350596525336,
        0.9470738851247412,
        0.9897533180091269,
        0.9628499403014472
      ],
      "excerpt": "Interactive fiction, often abbreviated IF, is software simulating environments in which players use text commands to control characters and influence the environment. Works in this form can be understood as literary narratives, either in the form of Interactive narratives or Interactive narrations. These works can also be understood as a form of video game, either in the form of an adventure game or role-playing game. \nInteractive Fiction provides a challenging environment for machine learning, and specifically nlp.  \nFor one, as the name suggests, we are in the realm of narrative. This implies a story, with a main plot and a number of subplots, characters, etc. \nIn addition, the narrative is interactive, meaning the flow of the text is dynamic and context sensitive: it depends on the user interaction with the environment, \nand more importantly with the history of the world the user interacts with. Given the same scene, the system's reaction \nis dependent on the user action, e.g. going right or left, picking this or that object, etc. Different scenes also depend on each other in a constrained fashion, \nas actions will change the state of the world, affecting the progress of the story as it is being told. \nOf course, this narrative-based nature also poses limits to the possibilities offered to the user. There is a story to be told, and \nthe choice of actions and scenes, as large as it may be, is still confined to what the developer allowed for in the first place. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.802470251306749
      ],
      "excerpt": "the need to let the user interact freely with the environment, in a build-your-own-adventure style.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.864140690664409,
        0.9727125189665784,
        0.8866678497635764,
        0.9995928844495077,
        0.9957455197782789,
        0.939715685762161,
        0.8286360994339489,
        0.989226750713797,
        0.9897999785541203,
        0.8889688518295146,
        0.9155676956244589,
        0.9795695926209887
      ],
      "excerpt": "Can we train an ANN to generate an interactive fiction based on a number of available playthroughs? \nOf course, this requires specifying a number of elements. Luckily, some previous work already exists on the subject (a review of a number of approaches is also available in [5], \nalthough more centered around the authorial side of things). \nThe main attempts almost always include a deep neural architecture (usually based on LSTMs) to account for the understanding of scenes (see e.g. [2]), coupled with a reinforcement learning module. However, these efforts mainly focus on training an agent to play the game, as opposed to actually create it. In this regard, [3] provides some inspiration, adopting a strategy reminiscent of GANs (generative adversarial networks). Alas, while the reinforcement learning addition is interesting -- in that it provides a way of encoding some exploratory behaviour which could be used to further improve the creativity of the narrative -- we do not currently see any easy way to embed it in our architecture, especially because of the lack of scores or rewards in the dataset we are going to use (see the next paragraph for more detail about it). Also, the approach in [3] relies on simple recombination of parts of text, thus lacking a truly generative flavour. \nAs a dataset, we will use a set of transcripts collected by ClubFloyd. These are playthroughs of a number of IF games played over the course of more than 20 years. The recovered set containes 241 transcripts. Although not all of the text is usable, as it contains meta-commands and a lot of chat between players, it still provides a wealth of data which would be very hard to collect otherwise (e.g. by actually writing an artificial player). \nIn partial contrast with the approaches presented in the introduction, we would like to framework the problem as a sequence-to-sequence task (see [6], [7]). Indeed, each game can be though of as comprising a series of &lt;scene_before, action, scene_after&gt; triplets. The network would then learn how to \"translate\" the input sequence &lt;scene_before&gt; + &lt;action&gt; into the output sequence &lt;scene_after&gt;. In addition, one peculiar aspect that we wish to investigate is the application of a hierarchical approach. This multi-scale architecture should be capable of working on multiple temporal scales (e.g. learning dependencies among scenes). For the same reason, the use of an episodic memory appears reasonable (see [9]). \nQuite naturally, a number of challenges arise. Some are listed below. \nThe pre-processing phase is going to be crucial to have as clean data as possible. Despite some commonalities among the playthroughs, there are many exceptions to deal with (e.g. about and version commands, load/save commands, etc.). The better we manage to filter out such \"noise\", the easier for the network to actually learn from game-text proper. \nWhile the use of word embeddings to obtain vector representation for words is common, usual approaches to sequence prediction use a one-hot encoding for the ouput, framing the task a a multi-class classification problem. Unfortunately, this limits the size of the vocabulary usable by the network. Instead of trying to implement efficient approximation to softmax ourselves, we could restate the prediction as a regression problem, in which the network learn to predict the embeddings themselves. This will require the definition of a custom loss function including direction and magnitude. \nHow do we design the hierarchical architecture? Do we want a single, deep+tall network or a set of loosely interacting \"controllers\" (see also [4])? \nWhat is the expected training time (and power) to generate meaningful text? \nHow do we evaluate the results? As there is no standard metric for this task, we will have to devise one ourself. Apart from subjective human evaluations, we could think of a way to measure the coherence of the generated story. E.g. whether generated scenes actually reference the previous object, or if particular commands (e.g. look) behave as one would expect from an actual IF game. In this sense, it could be useful to define a template of \"sensible\" replies, assigning a score to the network's prediction. Note how we could also define a simple error measure computing the difference between predicted and actual embeddings for each test triple, though we don't deem this very indicative of whether the task is being solved or not, as there may be many equivalent formulations for the next scene, all perfectly compatible with the same input. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/macco3k/deepstories/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 23 Dec 2021 01:54:03 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/macco3k/deepstories/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "macco3k/deepstories",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/macco3k/deepstories/master/seq2seq-categorical.ipynb",
      "https://raw.githubusercontent.com/macco3k/deepstories/master/process_transcripts.ipynb",
      "https://raw.githubusercontent.com/macco3k/deepstories/master/seq2seq.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8775119539371954
      ],
      "excerpt": "the need to let the user interact freely with the environment, in a build-your-own-adventure style.  \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8119280802658764
      ],
      "excerpt": "For one, as the name suggests, we are in the realm of narrative. This implies a story, with a main plot and a number of subplots, characters, etc. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/macco3k/deepstories/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Scope and Goals",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "deepstories",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "macco3k",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/macco3k/deepstories/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 23 Dec 2021 01:54:03 GMT"
    },
    "technique": "GitHub API"
  },
  "support": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Abstract; Simge <br/>\nIntroduction; Simge <br/>\nRelated Work; Simge, Daniele <br/>\nProject Description; Simge, Daniele <br/>\nSummary; Daniele <br/>\nConsclusion; Daniele <br/>\n",
      "technique": "Header extraction"
    }
  ]
}