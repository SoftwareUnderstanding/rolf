{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2111.04060",
      "https://arxiv.org/abs/2105.08050",
      "https://arxiv.org/abs/2105.03404",
      "https://arxiv.org/abs/2108.01072",
      "https://arxiv.org/abs/2109.04454",
      "https://arxiv.org/abs/2108.04384",
      "https://arxiv.org/abs/2109.05422",
      "https://arxiv.org/abs/2108.13341",
      "https://arxiv.org/abs/2107.00645",
      "https://arxiv.org/abs/2107.10224",
      "https://arxiv.org/abs/2107.08391",
      "https://arxiv.org/abs/2111.04060",
      "https://arxiv.org/abs/2105.08050",
      "https://arxiv.org/abs/2105.03404",
      "https://arxiv.org/abs/2108.01072",
      "https://arxiv.org/abs/2109.04454",
      "https://arxiv.org/abs/2108.04384",
      "https://arxiv.org/abs/2109.05422",
      "https://arxiv.org/abs/2108.13341",
      "https://arxiv.org/abs/2107.00645",
      "https://arxiv.org/abs/2107.10224",
      "https://arxiv.org/abs/2107.08391"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```bibtex\n@misc{tolstikhin2021mlpmixer,\n    title   = {MLP-Mixer: An all-MLP Architecture for Vision},\n    author  = {Ilya Tolstikhin and Neil Houlsby and Alexander Kolesnikov and Lucas Beyer and Xiaohua Zhai and Thomas Unterthiner and Jessica Yung and Daniel Keysers and Jakob Uszkoreit and Mario Lucic and Alexey Dosovitskiy},\n    year    = {2021},\n    eprint  = {2105.01601},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n```bibtex\n@misc{hou2021vision,\n    title   = {Vision Permutator: A Permutable MLP-Like Architecture for Visual Recognition},\n    author  = {Qibin Hou and Zihang Jiang and Li Yuan and Ming-Ming Cheng and Shuicheng Yan and Jiashi Feng},\n    year    = {2021},\n    eprint  = {2106.12368},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n```bibtex\n@article{liu2021pay,\n  title={Pay Attention to MLPs},\n  author={Liu, Hanxiao and Dai, Zihang and So, David R and Le, Quoc V},\n  journal={arXiv preprint arXiv:2105.08050},\n  year={2021}\n}\n```\n\n```bibtex\n@article{touvron2021resmlp,\n  title={Resmlp: Feedforward networks for image classification with data-efficient training},\n  author={Touvron, Hugo and Bojanowski, Piotr and Caron, Mathilde and Cord, Matthieu and El-Nouby, Alaaeldin and Grave, Edouard and Joulin, Armand and Synnaeve, Gabriel and Verbeek, Jakob and J{\\'e}gou, Herv{\\'e}},\n  journal={arXiv preprint arXiv:2105.03404},\n  year={2021}\n}\n```\n\n```bibtex\n@article{yu2021s,\n  title={S $\\^{} 2$-MLPv2: Improved Spatial-Shift MLP Architecture for Vision},\n  author={Yu, Tan and Li, Xu and Cai, Yunfeng and Sun, Mingming and Li, Ping},\n  journal={arXiv preprint arXiv:2108.01072},\n  year={2021}\n}\n```\n\n```bibtex\n@article{li2021convmlp,\n  title={ConvMLP: Hierarchical Convolutional MLPs for Vision},\n  author={Li, Jiachen and Hassani, Ali and Walton, Steven and Shi, Humphrey},\n  journal={arXiv preprint arXiv:2109.04454},\n  year={2021}\n}\n```\n\n```bibtex\n@article{tatsunami2021raftmlp,\n  title={RaftMLP: Do MLP-based Models Dream of Winning Over Computer Vision?},\n  author={Tatsunami, Yuki and Taki, Masato},\n  journal={arXiv preprint arXiv:2108.04384},\n  year={2021}\n}\n```\n\n```bibtex\n@article{tang2021sparse,\n  title={Sparse MLP for Image Recognition: Is Self-Attention Really Necessary?},\n  author={Tang, Chuanxin and Zhao, Yucheng and Wang, Guangting and Luo, Chong and Xie, Wenxuan and Zeng, Wenjun},\n  journal={arXiv preprint arXiv:2109.05422},\n  year={2021}\n}\n```\n\n```bibtex\n@article{guo2021hire,\n  title={Hire-MLP: Vision MLP via Hierarchical Rearrangement},\n  author={Guo, Jianyuan and Tang, Yehui and Han, Kai and Chen, Xinghao and Wu, Han and Xu, Chao and Xu, Chang and Wang, Yunhe},\n  journal={arXiv preprint arXiv:2108.13341},\n  year={2021}\n}\n```\n\n```bibtex\n@article{rao2021global,\n  title={Global filter networks for image classification},\n  author={Rao, Yongming and Zhao, Wenliang and Zhu, Zheng and Lu, Jiwen and Zhou, Jie},\n  journal={arXiv preprint arXiv:2107.00645},\n  year={2021}\n}\n```\n\n```bibtex\n@article{chen2021cyclemlp,\n  title={Cyclemlp: A mlp-like architecture for dense prediction},\n  author={Chen, Shoufa and Xie, Enze and Ge, Chongjian and Liang, Ding and Luo, Ping},\n  journal={arXiv preprint arXiv:2107.10224},\n  year={2021}\n}\n```\n\n```bibtex\n@article{lian2021mlp,\n  title={As-mlp: An axial shifted mlp architecture for vision},\n  author={Lian, Dongze and Yu, Zehao and Sun, Xing and Gao, Shenghua},\n  journal={arXiv preprint arXiv:2107.08391},\n  year={2021}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{lian2021mlp,\n  title={As-mlp: An axial shifted mlp architecture for vision},\n  author={Lian, Dongze and Yu, Zehao and Sun, Xing and Gao, Shenghua},\n  journal={arXiv preprint arXiv:2107.08391},\n  year={2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{chen2021cyclemlp,\n  title={Cyclemlp: A mlp-like architecture for dense prediction},\n  author={Chen, Shoufa and Xie, Enze and Ge, Chongjian and Liang, Ding and Luo, Ping},\n  journal={arXiv preprint arXiv:2107.10224},\n  year={2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{rao2021global,\n  title={Global filter networks for image classification},\n  author={Rao, Yongming and Zhao, Wenliang and Zhu, Zheng and Lu, Jiwen and Zhou, Jie},\n  journal={arXiv preprint arXiv:2107.00645},\n  year={2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{guo2021hire,\n  title={Hire-MLP: Vision MLP via Hierarchical Rearrangement},\n  author={Guo, Jianyuan and Tang, Yehui and Han, Kai and Chen, Xinghao and Wu, Han and Xu, Chao and Xu, Chang and Wang, Yunhe},\n  journal={arXiv preprint arXiv:2108.13341},\n  year={2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{tang2021sparse,\n  title={Sparse MLP for Image Recognition: Is Self-Attention Really Necessary?},\n  author={Tang, Chuanxin and Zhao, Yucheng and Wang, Guangting and Luo, Chong and Xie, Wenxuan and Zeng, Wenjun},\n  journal={arXiv preprint arXiv:2109.05422},\n  year={2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{tatsunami2021raftmlp,\n  title={RaftMLP: Do MLP-based Models Dream of Winning Over Computer Vision?},\n  author={Tatsunami, Yuki and Taki, Masato},\n  journal={arXiv preprint arXiv:2108.04384},\n  year={2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{li2021convmlp,\n  title={ConvMLP: Hierarchical Convolutional MLPs for Vision},\n  author={Li, Jiachen and Hassani, Ali and Walton, Steven and Shi, Humphrey},\n  journal={arXiv preprint arXiv:2109.04454},\n  year={2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{yu2021s,\n  title={S $\\^{} 2$-MLPv2: Improved Spatial-Shift MLP Architecture for Vision},\n  author={Yu, Tan and Li, Xu and Cai, Yunfeng and Sun, Mingming and Li, Ping},\n  journal={arXiv preprint arXiv:2108.01072},\n  year={2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{touvron2021resmlp,\n  title={Resmlp: Feedforward networks for image classification with data-efficient training},\n  author={Touvron, Hugo and Bojanowski, Piotr and Caron, Mathilde and Cord, Matthieu and El-Nouby, Alaaeldin and Grave, Edouard and Joulin, Armand and Synnaeve, Gabriel and Verbeek, Jakob and J{\\'e}gou, Herv{\\'e}},\n  journal={arXiv preprint arXiv:2105.03404},\n  year={2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{liu2021pay,\n  title={Pay Attention to MLPs},\n  author={Liu, Hanxiao and Dai, Zihang and So, David R and Le, Quoc V},\n  journal={arXiv preprint arXiv:2105.08050},\n  year={2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{hou2021vision,\n    title   = {Vision Permutator: A Permutable MLP-Like Architecture for Visual Recognition},\n    author  = {Qibin Hou and Zihang Jiang and Li Yuan and Ming-Ming Cheng and Shuicheng Yan and Jiashi Feng},\n    year    = {2021},\n    eprint  = {2106.12368},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{tolstikhin2021mlpmixer,\n    title   = {MLP-Mixer: An all-MLP Architecture for Vision},\n    author  = {Ilya Tolstikhin and Neil Houlsby and Alexander Kolesnikov and Lucas Beyer and Xiaohua Zhai and Thomas Unterthiner and Jessica Yung and Daniel Keysers and Jakob Uszkoreit and Mario Lucic and Alexey Dosovitskiy},\n    year    = {2021},\n    eprint  = {2105.01601},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{liu2021we,\n  title={Are we ready for a new paradigm shift? A Survey on Visual Deep MLP},\n  author={Liu, Ruiyang and Li, Yinghui and Liang, Dun and Tao, Linmi and Hu, Shimin and Zheng, Hai-Tao},\n  journal={arXiv preprint arXiv:2111.04060},\n  year={2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9871167629624297
      ],
      "excerpt": "Jittor and Pytorch implementaion of VISION PERMUTATOR: A PERMUTABLE MLP-LIKE ARCHITECTURE FOR VISUAL RECOGNITION. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "n,c,h,w = 2,10,4,5 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/liuruiyang98/Jittor-MLP",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-06T09:25:34Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-28T05:49:44Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9359924881150727,
        0.9140449499562083
      ],
      "excerpt": "Unofficial Implementation of MLP-Mixer, gMLP, resMLP, Vision Permutator, S2MLP, S2MLPv2, RaftMLP, HireMLP, ConvMLP, SparseMLP, ConvMixer, AS-MLP in Jittor and PyTorch. GFNet and CycleMLP in PyTorch. \nAre we ready for a new paradigm shift? A Survey on Visual Deep MLP (paper). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.903640326170189
      ],
      "excerpt": "    the bounds. The method used for generating the random values works \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8369682752146166
      ],
      "excerpt": "        std: the standard deviation of the normal distribution \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9307142052068472
      ],
      "excerpt": "    warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8122214852342996
      ],
      "excerpt": "#: Values are generated by using a truncated uniform distribution and \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8651389794112232
      ],
      "excerpt": "#: Clamp to ensure it's in the proper range \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9868922303956348
      ],
      "excerpt": "Rearrange, Reduce in einops for Jittor is support ! Easier to convert Transformer-based and MLP-based models from PyTorch to Jittor! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8857774262186158
      ],
      "excerpt": "Jittor and Pytorch implementaion of MLP-Mixer: An all-MLP Architecture for Vision. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9841144253239179,
        0.9765843757487938,
        0.8984957190311911,
        0.8676056714051851
      ],
      "excerpt": "Jittor and Pytorch implementaion of gMLP \nJittor and Pytorch implementaion of ResMLP: Feedforward networks for image classification with data-efficient training. \nJittor and Pytorch implementaion of S2-MLPv2: Improved Spatial-Shift MLP Architecture for Vision. \nJittor and Pytorch implementaion of ConvMixer: Patches Are All You Need?. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8982339398558167,
        0.8940049035761832
      ],
      "excerpt": "Jittor and Pytorch implementaion of RaftMLP: Do MLP-based Models Dream of Winning Over Computer Vision?. \nJittor and Pytorch implementaion of Sparse MLP for Image Recognition: Is Self-Attention Really Necessary?. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9241683299250001,
        0.8289425216624127,
        0.9349119676729342
      ],
      "excerpt": "Pytorch implementaion of Global filter networks for image classification. \nThere is no fft operation in Jittor! \nPytorch implementaion of Cyclemlp: A mlp-like architecture for dense prediction. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8948534887582283,
        0.8464500391004307
      ],
      "excerpt": "But There is no deform_conv in Jittor!  -- I find in JDet, but there is still some problems to be solved. \nJittor and Pytorch implementaion of As-mlp: An axial shifted mlp architecture for vision. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Unofficial Implementation of MLP-Mixer, gMLP, resMLP, Vision Permutator, S2MLP, S2MLPv2, RaftMLP, HireMLP, ConvMLP, AS-MLP, SparseMLP, ConvMixer in Jittor and PyTorch! Now, Rearrange and Reduce in einops.layers.jittor are support!! trunc_normal_ is supported for Jittor!!",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/liuruiyang98/Jittor-MLP/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 4,
      "date": "Wed, 29 Dec 2021 02:47:45 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/liuruiyang98/Jittor-MLP/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "liuruiyang98/Jittor-MLP",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/liuruiyang98/Jittor-MLP/main/create_model.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8838320662268858
      ],
      "excerpt": "AS-MLP Pytorch, shift is implemented by cupy, and can only run on GPU. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9133368656218674,
        0.9133368656218674,
        0.9012248701992861,
        0.8177905267106058
      ],
      "excerpt": "import math \nimport warnings \nimport jittor as jt \ndef trunc_normal_(var, mean=0., std=1., a=-2., b=2.): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8048891599652979
      ],
      "excerpt": "    r\"\"\"Fills the input jt.jittor_core.Var with values drawn from a truncated \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8038986659320541,
        0.8838148168639296
      ],
      "excerpt": "        b: the maximum cutoff value \n    Examples: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8458187370750686,
        0.8177905267106058
      ],
      "excerpt": "    return no_grad_trunc_normal(var, mean, std, a, b) \ndef no_grad_trunc_normal(var, mean, std, a, b): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8488411244225242
      ],
      "excerpt": "#: var.uniform(2 * l - 1, 2 * u - 1) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8731478326013286
      ],
      "excerpt": "var.multiply(std * math.sqrt(2.)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8488411244225242,
        0.8278367292485345
      ],
      "excerpt": "var.clamp(min_v=a, max_v=b) \nreturn var \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9013492953466189
      ],
      "excerpt": "from .einops_my.layers.jittor import Rearrange, Reduce (shown in ./models_jittor/raft_mlp.py, ./models_jittor/sparse_mlp.py) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8796325398080971
      ],
      "excerpt": "from torchvision.ops.deform_conv import deform_conv2d as deform_conv2d_tv \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9012248701992861
      ],
      "excerpt": "import jittor as jt \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/liuruiyang98/Jittor-MLP/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2021 liuruiyang98\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Jittor-MLP",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Jittor-MLP",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "liuruiyang98",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/liuruiyang98/Jittor-MLP/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 45,
      "date": "Wed, 29 Dec 2021 02:47:45 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "deep-learning",
      "pytorch",
      "jittor"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\nimport jittor as jt\nfrom models_jittor import gMLPForImageClassification as gMLP_jt\nfrom models_jittor import ResMLPForImageClassification as ResMLP_jt\nfrom models_jittor import MLPMixerForImageClassification as MLPMixer_jt\nfrom models_jittor import ViP as ViP_jt\nfrom models_jittor import S2MLPv2 as S2MLPv2_jt\nfrom models_jittor import S2MLPv1_deep as S2MLPv1_deep_jt \nfrom models_jittor import ConvMixer as ConvMixer_jt\nfrom models_jittor import convmlp_s as ConvMLP_s_jt \nfrom models_jittor import convmlp_l as ConvMLP_l_jt \nfrom models_jittor import convmlp_m as ConvMLP_m_jt \nfrom models_jittor import RaftMLP as RaftMLP_jt\nfrom models_jittor import SparseMLP as SparseMLP_jt\nfrom models_jittor import HireMLP as HireMLP_jt\nfrom models_jittor import AS_MLP as AS_MLP_jt\n\nmodel_jt = MLPMixer_jt(\n    image_size=(224,112),\n    patch_size=16,\n    in_channels=3,\n    num_classes=1000,\n    d_model=256,\n    depth=12,\n)\n\nimages = jt.randn(8, 3, 224, 224)\nwith jt.no_grad():\n    output = model_jt(images)\nprint(output.shape) #: \uff088\uff0c 1000\uff09\n\n#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:\n\nimport torch\nfrom models_pytorch import gMLPForImageClassification as gMLP_pt\nfrom models_pytorch import ResMLPForImageClassification as ResMLP_pt\nfrom models_pytorch import MLPMixerForImageClassification as MLPMixer_pt\nfrom models_pytorch import ViP as ViP_pt\nfrom models_pytorch import S2MLPv2 as S2MLPv2_pt \nfrom models_pytorch import S2MLPv1_deep as S2MLPv1_deep_pt\nfrom models_pytorch import ConvMixer as ConvMixer_pt \nfrom models_pytorch import convmlp_s as ConvMLP_s_pt \nfrom models_pytorch import convmlp_l as ConvMLP_l_pt \nfrom models_pytorch import convmlp_m as ConvMLP_m_pt \nfrom models_pytorch import RaftMLP as RaftMLP_pt\nfrom models_pytorch import SparseMLP as SparseMLP_pt\nfrom models_pytorch import HireMLP as HireMLP_pt\nfrom models_pytorch import GFNet as GFNet_pt\nfrom models_pytorch import CycleMLP_B2 as CycleMLP_B2_pt\nfrom models_pytorch import AS_MLP as AS_MLP_pt\n\nmodel_pt = ViP_pt(\n    image_size=224,\n    patch_size=16,\n    in_channels=3,\n    num_classes=1000,\n    d_model=256,\n    depth=30,\n    segments = 16,\n    weighted = True\n)\n\nimages = torch.randn(8, 3, 224, 224)\n\nwith torch.no_grad():\n    output = model_pt(images)\nprint(output.shape) #: \uff088\uff0c 1000\uff09\n\n\n#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#: Non-square images and patch sizes #:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:\n\nmodel_jt = ViP_jt(\n    image_size=(224, 112),\n    patch_size=(16, 8),\n    in_channels=3,\n    num_classes=1000,\n    d_model=256,\n    depth=30,\n    segments = 16,\n    weighted = True\n)\nimages = jt.randn(8, 3, 224, 112)\nwith jt.no_grad():\n    output = model_jt(images)\nprint(output.shape) #: \uff088\uff0c 1000\uff09\n\n#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#: 2 Stages S2MLPv2 #:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:\nmodel_pt = S2MLPv2_pt(\n    in_channels = 3,\n    image_size = (224,224),\n    patch_size = [(7,7), (2,2)],\n    d_model = [192, 384],\n    depth = [4, 14],\n    num_classes = 1000, \n    expansion_factor = [3, 3]\n)\n\n#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#: ConvMLP With Pretrain Params #:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:\nmodel_jt = ConvMLP_s_jt(pretrained = True, num_classes = 1000)\n\n\n#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#: RaftMLP #:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:\nmodel_jt = RaftMLP_jt(\n        layers = [\n            {\"depth\": 12,\n            \"dim\": 768,\n            \"patch_size\": 16,\n            \"raft_size\": 4}\n        ],\n        gap = True\n    )\n\n#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#: SparseMLP #:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:\nmodel_pt = SparseMLP_pt(\n        image_size=224,\n        patch_size=4,\n        in_channels=3,\n        num_classes=1000,\n        d_model=96,\n        depth=[2,10,24,2],\n        expansion_factor = 2,\n        patcher_norm= True\n    )\n\n#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#: HireMLP #:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:\n\nmodel_pt = HireMLP_pt(\n        patch_size=4,\n        in_channels=3,\n        num_classes=1000,\n        d_model=[64, 128, 320, 512],\n        h = [4,3,3,2],\n        w = [4,3,3,2],\n        cross_region_step = [2,2,1,1],\n        cross_region_interval = 2,\n        depth=[4,6,24,3],\n        expansion_factor = 2,\n        patcher_norm = True,\n    \tpadding_type = 'circular',\n    )\n\n#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#: GFNet #:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:\nmodel_pt = GFNet_pt()\n\n\n#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#: CycleMLP #:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:\nmodel_pt = CycleMLP_B2_pt()\n\n#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#: AS-MLP #:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:#:\nmodel_pt = AS_MLP_pt()\n\n```\n\n\n\n",
      "technique": "Header extraction"
    }
  ]
}