{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1409.1556"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/modestyachts/ImageNetV2",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-02-12T20:08:54Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-27T13:16:00Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The dataset creation process has several stages outlined below.\nWe describe the process here at a high level.\nIf you have questions about any individual steps, please contact Rebecca Roelofs (roelofs@cs.berkeley.edu) and Ludwig Schmidt (ludwig@berkeley.edu).\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8153049689888217,
        0.9767349523997773
      ],
      "excerpt": "The ImageNetV2 dataset contains new test data for the ImageNet benchmark. \nThis repository provides associated code for assembling and working with ImageNetV2. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8959180960501233,
        0.9492813466787414,
        0.8419949928685369,
        0.877537892471215,
        0.8521211175214175
      ],
      "excerpt": "This makes the new test data independent of existing models and guarantees that the accuracy scores are not affected by adaptive overfitting. \nWe designed the data collection process for ImageNetV2 so that the resulting distribution is as similar as possible to the original ImageNet dataset. \nOur paper \"Do ImageNet Classifiers Generalize to ImageNet?\" describes ImageNetV2 and associated experiments in detail. \nIn addition to the three test sets, we also release our pool of candidate images from which the test sets were assembled. \nEach image comes with rich metadata such as the corresponding Flickr search queries or the annotations from MTurk workers. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9094250895776169,
        0.9797318607165807,
        0.9719110205588997,
        0.9058334917712381,
        0.8144804019199345
      ],
      "excerpt": "Before explaining how the code in this repository was used to assemble ImageNetV2, we first describe how to load our new test sets. \nSimilar to the original ImageNet dataset, we used Amazon Mechanical Turk (MTurk) to filter our pool of candidates. \nThe main unit of work on MTurk is a HIT (Human Intelligence Tasks), which in our case consists of 48 images with a target class. \nThe format of our HITs was derived from the original ImageNet HITs. \nTo submit a HIT, we performed the following steps. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9702132161272282
      ],
      "excerpt": "  1. Encrypt all image URLs.  This is necessary so that MTurk workers cannot identify whether an image is from the original validation set or our candidate pool by the source URL.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8881829731066017
      ],
      "excerpt": "  2. Run the image consistency check.  This checks that all of the new candidate images have been stored to S3 and have encrypted URLs.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9571844641484732
      ],
      "excerpt": "  3. Generate hit candidates. This outputs a list of candidates to data/hit_candidates \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8322025351951318
      ],
      "excerpt": "  4. Submit live HITs to MTurk.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9541708141662641,
        0.874896621308686
      ],
      "excerpt": "  6. Type in the word LIVE to confirm submitting the HITs to MTurk (this costs money). \nThe HIT metadata created by make_hits_live.sh is stored in data/mturk/hit_data_live/. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.846097891453077,
        0.9315258250238682,
        0.8493374230593825
      ],
      "excerpt": "Additionally, we occasionally used the Jupyter notebook inspect_hit.ipynb to visually examine the HITs we created. \nThe code for this notebook is stored in inspect_hit_notebook_code.py. \nNext, we removed near-duplicates from our candidate pool. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.90189198291577
      ],
      "excerpt": "To find near-duplicates, we computed the 30 nearest neighbors for each candidate image in three different metrics: l2 distance on raw pixels, l2 distance on features extracted from a pre-trained VGG model (fc7), and SSIM (structural similarity). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9370648710099297,
        0.9512824090834903
      ],
      "excerpt": "Finally, we manually reviewed the nearest neighbor pairs using the notebook review_near_duplicates.ipynb. The file review_near_duplicates_notebook_code.py contains the code for this notebook. The review output is saved in data/metadata/nearest_neighbor_reviews_v2.json. \nAll near duplicates that we found are saved in data/metadata/near_duplicates.json. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8416792684739745
      ],
      "excerpt": "For quality control, we added a final reviewing step to our dataset creation pipeline. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9599212461972287
      ],
      "excerpt": "review_server.py is the review server used for additional cleaning of the candidate pool.  The review server starts a web UI that allows one to browse all candidate images for a particular class.  In addition, a user can easily flag images that are problematic or near duplicates. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.848718250410101
      ],
      "excerpt": "There is a script in data for starting the static file server ./start_file_server.sh. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.991321658678121,
        0.8967321800364866,
        0.8223746383563201
      ],
      "excerpt": "Our code base contains a set of data classes for working with various aspects of ImageNetV2. \nimagenet.py: This file contains the ImageNetData class that provides metadata about ImageNet (a list of classes, etc.) and functionality for loading images in the original ImageNet dataset. The scripts generate_imagenet_metadata_pickle.py are used to assemble generate_class_info_file.py some of the metadata in the ImageNetData class. \ncandidate_data.py contains the CandidateData class that provides easy access to all candidate images in ImageNetV2 (both image data and metadata). The metadata file used in this class comes from generate_candidate_metadata_pickle.py. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9088163352013866,
        0.9651857595082162
      ],
      "excerpt": "mturk_data.py provides the MTurkData class for accessing the results from our MTurk HITs. The data used by this class is assembled via generate_mturk_data_pickle. \nnear_duplicate_data.py loads and processes the information about near-duplicates in ImageNetV2. Some of the metadata is prepared with generate_review_thresholds_pickle.py. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9059338330073452
      ],
      "excerpt": "prediction_data.py provides functionality for loading the predictions of various classification models on our three test sets. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9098163211646526
      ],
      "excerpt": "Finally, we describe our evaluation pipeline for the PyTorch models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8262506791443491
      ],
      "excerpt": "where $DATASET is one of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A new test set for ImageNet",
      "technique": "GitHub API"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "In the first stage, we collected candidate images from the Flickr image hosting service.\nThis requires a [Flickr API key](https://www.flickr.com/services/api/misc.api_keys.html).\n\nWe ran the following command to search Flickr for images for a fixed list of wnids:\n\n```\npython flickr_search.py \"../data/flickr_api_keys.json\" \\\n                        --wnids \"{wnid_list.json}\" \\\n                        --max_images 200 \\\n                        --max_date_taken \"2013-07-11\"\\\n                        --max_date_uploaded \"2013-07-11\"\\\n                        --min_date_taken \"2012-07-11\"\\\n                        --min_date_uploaded \"2012-07-11\" \n```\nWe refer to the paper for more details on which Flickr search parameters we used to complete our candidate pool.\n\nThe script outputs search result metadata, including the Flickr URLs returned for each query. \nThis search result metadata is written to `/data/search_results/`. \n\nWe then stored the images to an Amazon S3 bucket using \n```\npython download_images_from_flickr.py ../data/search_results/{search_result.json} --batch --parallel\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/modestyachts/ImageNetV2/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 14,
      "date": "Wed, 29 Dec 2021 10:33:01 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/modestyachts/ImageNetV2/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "modestyachts/ImageNetV2",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/modestyachts/ImageNetV2/master/notebooks/image_loader_example.ipynb",
      "https://raw.githubusercontent.com/modestyachts/ImageNetV2/master/notebooks/imagenet_data_class_example.ipynb",
      "https://raw.githubusercontent.com/modestyachts/ImageNetV2/master/notebooks/review_near_duplicates.ipynb",
      "https://raw.githubusercontent.com/modestyachts/ImageNetV2/master/notebooks/near_duplicate_data_class_example.ipynb",
      "https://raw.githubusercontent.com/modestyachts/ImageNetV2/master/notebooks/inspect_hit.ipynb",
      "https://raw.githubusercontent.com/modestyachts/ImageNetV2/master/notebooks/final_dataset_inspection.ipynb",
      "https://raw.githubusercontent.com/modestyachts/ImageNetV2/master/notebooks/candidate_data_class_example.ipynb",
      "https://raw.githubusercontent.com/modestyachts/ImageNetV2/master/notebooks/flickr_api_explore.ipynb",
      "https://raw.githubusercontent.com/modestyachts/ImageNetV2/master/notebooks/mturk_data_class_example.ipynb",
      "https://raw.githubusercontent.com/modestyachts/ImageNetV2/master/notebooks/prediction_data_class_example.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/modestyachts/ImageNetV2/master/data/start_file_server.sh",
      "https://raw.githubusercontent.com/modestyachts/ImageNetV2/master/code/make_hits_sandbox_small.sh",
      "https://raw.githubusercontent.com/modestyachts/ImageNetV2/master/code/make_hits.sh",
      "https://raw.githubusercontent.com/modestyachts/ImageNetV2/master/code/test_near_duplicate_checker.sh",
      "https://raw.githubusercontent.com/modestyachts/ImageNetV2/master/code/make_hits_sandbox.sh",
      "https://raw.githubusercontent.com/modestyachts/ImageNetV2/master/code/run_flickr_search.sh",
      "https://raw.githubusercontent.com/modestyachts/ImageNetV2/master/code/sample_dataset_type_c.sh",
      "https://raw.githubusercontent.com/modestyachts/ImageNetV2/master/code/sample_dataset_type_b.sh",
      "https://raw.githubusercontent.com/modestyachts/ImageNetV2/master/code/make_hits_special_live.sh",
      "https://raw.githubusercontent.com/modestyachts/ImageNetV2/master/code/make_hits_live.sh",
      "https://raw.githubusercontent.com/modestyachts/ImageNetV2/master/code/sample_dataset_type_a.sh",
      "https://raw.githubusercontent.com/modestyachts/ImageNetV2/master/code/make_hits_special_sandbox.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You can download the test sets from the following url: http://imagenetv2public.s3-website-us-west-2.amazonaws.com/. There is a link for each individual dataset and the ImageNet datasets must be decompressed before use. \n\nTo load the dataset, you can use the [`ImageFolder`](https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder) class in [PyTorch](https://pytorch.org/) on the extracted folder. \n\n\nFor instance, the following code loads the `MatchedFrequency` dataset:\n\n```python\nfrom torchvision import datasets\ndatasets.ImageFolder(root='imagenetv2-matched-frequency')\n```\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "There are currently three test sets in ImageNetV2:\n\n- `Threshold0.7` was built by sampling ten images for each class among the candidates with selection frequency at least 0.7. \n\n- `MatchedFrequency` was sampled to match the MTurk selection frequency distribution of the original ImageNet validation set for each class. \n\n- `TopImages` contains the ten images with highest selection frequency in our candidate pool for each class.\n\nIn our code, we adopt the following naming convention:\nEach test set is identified with a string of the form\n\n`imagenetv2-<test-set-letter>-<revision-number>`\n\nfor instance, `imagenetv2-b-31`. The `Threshold0.7`, `MatchedFrequency`, and `TopImages` have test set letters `a`, `b`, and `c`, respectively.\nThe current revision numbers for the test sets are `imagenetv2-a-44`, `imagenetv2-b-33`, `imagenetv2-c-12`.\nWe refer to our paper for a detailed description of these test sets and the review process underlying the different test set revisions.\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9176594311760612
      ],
      "excerpt": "To submit a HIT, we performed the following steps. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.839070335887581
      ],
      "excerpt": "We use a separate bash script to sample each version of the dataset, i.e sample_dataset_type_{a}.sh. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9493089097131068
      ],
      "excerpt": "    python encrypt_copy_objects.py imagenet2candidates_mturk --strip_string \".jpg\" --pywren \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "    python image_consistency_check.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "    python generate_hit_candidates.py  --num_wnids 1000 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8718946100139504
      ],
      "excerpt": "python3 mturk.py show_hit_progress --live --hit_file ../data/mturk/hit_data_live/{hit.json} \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8009561912132407
      ],
      "excerpt": "The fc7 metric requires that each image is featurized using the same pre-trained VGG model. The scripts featurize.py, feaurize_test.py and featurize_candidates.py were used to perform the fc7 featurization.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991,
        0.9336801098518991,
        0.9336801098518991
      ],
      "excerpt": "* run_near_duplicate_checker_dssim.py \n* run_near_duplicate_checker_l2.py \n* run_near_duplicate_checker_fc7.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.851760671168511,
        0.8114871952912821
      ],
      "excerpt": "The script test_near_duplicate_checker.sh was used to run the unit tests for the near duplicate checker contained in test_near_duplicate_checker.py.  \nFinally, we manually reviewed the nearest neighbor pairs using the notebook review_near_duplicates.ipynb. The file review_near_duplicates_notebook_code.py contains the code for this notebook. The review output is saved in data/metadata/nearest_neighbor_reviews_v2.json. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8857687166454734
      ],
      "excerpt": "Each script calls sample_dataset.py and initialize_dataset_review.py with the correct arguments. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991
      ],
      "excerpt": "python3 review_server.py --use_local_images. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991,
        0.9336801098518991
      ],
      "excerpt": "* download_all_candidate_images_to_cache.py \n* download_dataset_images.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8529690793042745,
        0.902419513627913
      ],
      "excerpt": "The main file is eval.py, which can be invoked as follows: \npython eval.py --dataset $DATASET --models $MODELS \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/modestyachts/ImageNetV2/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "JavaScript",
      "Python",
      "HTML",
      "CSS",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, Vaishaal Shankar\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "ImageNetV2",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "ImageNetV2",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "modestyachts",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/modestyachts/ImageNetV2/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 149,
      "date": "Wed, 29 Dec 2021 10:33:01 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "dataset",
      "imagenet",
      "generalization",
      "robustness",
      "overfitting"
    ],
    "technique": "GitHub API"
  }
}