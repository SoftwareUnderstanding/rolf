{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This work was inspired by a number of prior works for learning joint embeddings of text and video, but in particular the *Mixture-of-Embedding-Experts* method proposed by Antoine Miech, Ivan Laptev and Josef Sivic ([paper](https://arxiv.org/abs/1804.02516), [code](https://github.com/antoine77340/Mixture-of-Embedding-Experts)). We would also like to thank Zak Stone and Susie Lim for their help with using Cloud TPUs.  The code structure uses the [pytorch-template](https://github.com/victoresque/pytorch-template) by @victoresque.",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1907.13487",
      "https://arxiv.org/abs/1804.02516",
      "https://arxiv.org/abs/1804.02516",
      "https://arxiv.org/abs/1804.02516"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[1] If you find this code useful or use the extracted features, please consider citing:\n\n```\n@inproceedings{croitoru2021teachtext,\n  title={Teachtext: Crossmodal generalized distillation for text-video retrieval},\n  author={Croitoru, I. and Bogolin, S. and Leordeanu, M. and Jin, H. and Zisserman, A. and Albanie, S. and Liu, Y.},\n  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},\n  pages={11583--11593},\n  year={2021}\n}\n\n@inproceedings{Liu2019a,\n  author    = {Liu, Y. and Albanie, S. and Nagrani, A. and Zisserman, A.},\n  booktitle = {arXiv preprint arxiv:1907.13487},\n  title     = {Use What You Have: Video retrieval using representations from collaborative experts},\n  date      = {2019},\n}\n```\n\n[2] If you make use of the MSRVTT or LSMDC features provided by Miech et al. (details are given in their respective READMEs [here](misc/datasets/msrvtt/README.md) and [here](misc/datasets/lsmdc/README.md)), please cite:\n\n```\n@article{miech2018learning,\n  title={Learning a text-video embedding from incomplete and heterogeneous data},\n  author={Miech, Antoine and Laptev, Ivan and Sivic, Josef},\n  journal={arXiv preprint arXiv:1804.02516},\n  year={2018}\n}\n```\n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{miech2018learning,\n  title={Learning a text-video embedding from incomplete and heterogeneous data},\n  author={Miech, Antoine and Laptev, Ivan and Sivic, Josef},\n  journal={arXiv preprint arXiv:1804.02516},\n  year={2018}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{Liu2019a,\n  author    = {Liu, Y. and Albanie, S. and Nagrani, A. and Zisserman, A.},\n  booktitle = {arXiv preprint arxiv:1907.13487},\n  title     = {Use What You Have: Video retrieval using representations from collaborative experts},\n  date      = {2019},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{croitoru2021teachtext,\n  title={Teachtext: Crossmodal generalized distillation for text-video retrieval},\n  author={Croitoru, I. and Bogolin, S. and Leordeanu, M. and Jin, H. and Zisserman, A. and Albanie, S. and Liu, Y.},\n  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},\n  pages={11583--11593},\n  year={2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8501823123991106
      ],
      "excerpt": "| TeachText - CE+    | Full  | t2v  | <sub><sup>14.6<sub>(0.0)</sub></sup></sub> | <sub><sup>37.9<sub>(0.1)</sub></sup></sub> | <sub><sup>50.9<sub>(0.2)</sub></sup></sub> | <sub><sup>78.9<sub>(0.0)</sub></sup></sub> | <sub><sup>10.0<sub>(0.0)</sub></sup></sub> | <sub><sup>63.1<sub>(0.2)</sub></sup></sub> | <sub><sup>30.4<sub>(0.0)</sub></sup></sub> | config_TT, model_TT, log_TT | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8501823123991106,
        0.8252756551763226
      ],
      "excerpt": "| CE+    | Full  | t2v  | <sub><sup>14.4<sub>(0.1)</sub></sup></sub> | <sub><sup>37.4<sub>(0.2)</sub></sup></sub> | <sub><sup>50.2<sub>(0.1)</sub></sup></sub> | <sub><sup>77.9<sub>(0.1)</sub></sup></sub> | <sub><sup>10.0<sub>(0.0)</sub></sup></sub> | <sub><sup>70.8<sub>(0.1)</sub></sup></sub> | <sub><sup>30.0<sub>(0.1)</sub></sup></sub> | config_TT, model_TT, log_TT | \n| TeachText - CE+    | Full  | t2v  | <sub><sup>14.9<sub>(0.1)</sub></sup></sub> | <sub><sup>38.3<sub>(0.1)</sub></sup></sub> | <sub><sup>51.5<sub>(0.1)</sub></sup></sub> | <sub><sup>79.2<sub>(0.1)</sub></sup></sub> | <sub><sup>10.0<sub>(0.0)</sub></sup></sub> | <sub><sup>62.5<sub>(0.5)</sub></sup></sub> | <sub><sup>30.9<sub>(0.1)</sub></sup></sub> | config_TT, model_TT, log_TT | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8374695145293155
      ],
      "excerpt": "| CE    | Full  | t2v  | <sub><sup>12.4<sub>(0.7)</sub></sup></sub> | <sub><sup>28.5<sub>(0.8)</sub></sup></sub> | <sub><sup>37.9<sub>(0.6)</sub></sup></sub> | <sub><sup>64.5<sub>(0.8)</sub></sup></sub> | <sub><sup>21.7<sub>(0.6)</sub></sup></sub> | <sub><sup>88.0<sub>(4.8)</sub></sup></sub> | <sub><sup>23.7<sub>(0.3)</sub></sup></sub> | config_TT, model_TT, log_TT | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8999676257149684
      ],
      "excerpt": "TeachText results on Activity-Net Benchmark \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/albanie/collaborative-experts",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-07-17T07:36:18Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-15T21:23:26Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8318612236025149,
        0.8105755666870592
      ],
      "excerpt": "Please note that the numbers are higher than in the original CE due to compression artefacts correction \nDenoising results on MSRVTT \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8105755666870592
      ],
      "excerpt": "Denoising results on MSVD \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8068776296708832
      ],
      "excerpt": "| CE    | Full  | t2v  | <sub><sup>17.1<sub>(0.9)</sub></sup></sub> | <sub><sup>41.9<sub>(0.2)</sub></sup></sub> | <sub><sup>56.0<sub>(0.5)</sub></sup></sub> | <sub><sup>83.4<sub>(0.9)</sub></sup></sub> | <sub><sup>8.0<sub>(0.0)</sub></sup></sub> | <sub><sup>42.8<sub>(2.8)</sub></sup></sub> | <sub><sup>34.2<sub>(0.4)</sub></sup></sub> | config_TT, model_TT, log_TT | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.805539735411982
      ],
      "excerpt": "| CE    | Full  | t2v  | <sub><sup>19.9<sub>(0.4)</sub></sup></sub> | <sub><sup>50.1<sub>(0.8)</sub></sup></sub> | <sub><sup>66.1<sub>(0.6)</sub></sup></sub> | <sub><sup>92.2<sub>(0.7)</sub></sup></sub> | <sub><sup>5.3<sub>(0.6)</sub></sup></sub> | <sub><sup>21.3<sub>(1.1)</sub></sup></sub> | <sub><sup>40.4<sub>(0.3)</sub></sup></sub> | config_TT, model_TT, log_TT | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9394449182630016
      ],
      "excerpt": "For MSRVTT: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9394449182630016
      ],
      "excerpt": "For MSVD: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9394449182630016
      ],
      "excerpt": "For DiDeMo: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9394449182630016
      ],
      "excerpt": "For ActivityNet: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9402871869157836,
        0.9696265891812408
      ],
      "excerpt": "1. The use of information from a wide range of modalities, including those that are typically always available in video (such as RGB) as well as more \"specific\" clues which may only occasionally be present (such as overlaid text). \n2. A module that aims to combine these modalities into a fixed size representation that in a manner that is robust to noise. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9796933692505561
      ],
      "excerpt": "Important: A note on the updated results: A previous version of the codebase (and paper) reported results on the retrieval benchmarks that included a signficant software bug leading to an overestimate of performance.  We are extremely grateful to Valentin Gabeur who discovered this bug (it has been corrected in the current codebase). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9907748138974466,
        0.9499280894037635
      ],
      "excerpt": "We provide pretrained models for each dataset to reproduce the results reported in the paper [1] (references follow at the end of this README).  Each model is accompanied by training and evaluation logs.  Performance is evalauted for retrieval in both directions (joint-embeddings can be used for either of these two tasks): \n* t2v denotes that a text query is used to retrieve videos \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9627112014616036
      ],
      "excerpt": "In the results reported below, the same model is used for both the t2v and v2t evaluations.  Each metric is reported as the mean and standard deviation (in parentheses) across three training runs. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9982641669405891
      ],
      "excerpt": "Models marked with * use the features made available with the MoEE model of [2] (without OCR, speech and scene features), unstarred models on the 1k-B and Full splits make use of OCR, speech and scene features, as well slightly stronger text encodings (GPT, rather than word2vec - see [1] for details). The MoEE model is implemented as a sanity check that our codebase approximately reproduces [2] (the MoEE paper). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8037777684057982
      ],
      "excerpt": "See the MSVD README for descriptions of the train/test splits. Note that the videos in the MSVD dataset do not have soundtracks. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9796717959931225,
        0.9172357919062787
      ],
      "excerpt": "See the LSMDC README for descriptions of the train/test splits. Please note that to obtain the features and descriptions for this dataset, you must obtain permission from MPII to use the data (this is process is described here.  Once you have done so, please request that a member of the LSMDC team contacts us to confirm approval (via albanie at robots dot ox dot ac dot uk) - we can then provide you with a link to the features. \nFor each dataset, the Collaborative Experts model makes use of a collection of pretrained \"expert\" feature extractors (see [1] for more precise descriptions). Some experts have been obtained from other sources (described where applicable), rather than extracted by us.  To reproduce the experiments listed above, the experts for each dataset have been bundled into compressed tar files.  These can be downloaded and unpacked with a utility script (recommended -- see example usage below), which will store them in the locations expected by the training code. Each set of experts has a brief README, which also provides a link from which they can be downloaded directly. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8367786968481602
      ],
      "excerpt": "For example, to reproduce the MSVD results described above, run the following sequence of commands: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8080512374528344
      ],
      "excerpt": ": fetch the pretrained experts for MSVD \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8782618521924553
      ],
      "excerpt": ": Evaluate the model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8080512374528344
      ],
      "excerpt": ": fetch the pretrained experts for LSMDC \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9659505411495183
      ],
      "excerpt": "Tensorboard lacks video support via HTML5 tags (at the time of writing) so after each evaluation of a retrieval model, a simple HTML file is generated to allow the predicted rankings of different videos to be visualised: an example screenshot is given below (this tool is inspired by the visualiser in the pix2pix codebase). To view the visualisation, navigate to the web directory (this is generated for each experiment, and will be printed in the log during training) and run python3 -m http.server 9999, then navigate to localhost:9999 in your web browser.  You should see something like the following: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Video embeddings for retrieval with natural language queries",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/albanie/collaborative-experts/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 32,
      "date": "Thu, 23 Dec 2021 23:31:19 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/albanie/collaborative-experts/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "albanie/collaborative-experts",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8787972870487167
      ],
      "excerpt": "2. A config.json file.  You can define your own, or use one of the provided configs in the configs directory. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8134005941221073
      ],
      "excerpt": "where &lt;gpu-id&gt; is the index of the GPU to train on.  This option can be ommitted to run the training on the CPU. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8130965286941574
      ],
      "excerpt": "| CE    | Full  | t2v  | <sub><sup>21.5<sub>(0.6)</sub></sup></sub> | <sub><sup>52.3<sub>(0.9)</sub></sup></sub> | <sub><sup>67.5<sub>(0.8)</sub></sup></sub> | <sub><sup>90.7<sub>(0.0)</sub></sup></sub> | <sub><sup>5.0<sub>(0.0)</sub></sup></sub> | <sub><sup>20.4<sub>(0.0)</sub></sup></sub> | <sub><sup>42.3<sub>(0.6)</sub></sup></sub> | config_TT, model_TT, log_TT | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8819483260989657
      ],
      "excerpt": "| TeachText - CE    | Full  | t2v  | <sub><sup>22.7<sub>(0.8)</sub></sup></sub> | <sub><sup>56.2<sub>(0.1)</sub></sup></sub> | <sub><sup>71.6<sub>(0.8)</sub></sup></sub> | <sub><sup>95.3<sub>(0.1)</sub></sup></sub> | <sub><sup>4.0<sub>(0.0)</sub></sup></sub> | <sub><sup>15.8<sub>(0.1)</sub></sup></sub> | <sub><sup>45.0<sub>(0.6)</sub></sup></sub> | config_TT, model_TT, log_TT | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8890818307099057
      ],
      "excerpt": "<img width=\"300\" alt=\"logo\" src=\"figs/logo-centre.png\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8232922493406938
      ],
      "excerpt": "| CE    | Full  | t2v  | <sub><sup>10.0<sub>(0.1)</sub></sup></sub> | <sub><sup>29.0<sub>(0.3)</sub></sup></sub> | <sub><sup>41.2<sub>(0.2)</sub></sup></sub> | <sub><sup>71.4<sub>(0.1)</sub></sup></sub> | <sub><sup>16.0<sub>(0.0)</sub></sup></sub> | <sub><sup>86.8<sub>(0.3)</sub></sup></sub> | config, model, log | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8468077465536752
      ],
      "excerpt": "| CE    | 1k-A  | v2t  | <sub><sup>20.6<sub>(0.6)</sub></sup></sub> | <sub><sup>50.3<sub>(0.5)</sub></sup></sub> | <sub><sup>64.0<sub>(0.2)</sub></sup></sub> | <sub><sup>89.9<sub>(0.3)</sub></sup></sub> | <sub><sup>5.3<sub>(0.6)</sub></sup></sub> | <sub><sup>25.1<sub>(0.8)</sub></sup></sub> | config, model, log | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8148507868925206
      ],
      "excerpt": "| CE | t2v  | <sub><sup>19.8<sub>(0.3)</sub></sup></sub> | <sub><sup>49.0<sub>(0.3)</sub></sup></sub> | <sub><sup>63.8<sub>(0.1)</sub></sup></sub> | <sub><sup>89.0<sub>(0.2)</sub></sup></sub> | <sub><sup>6.0<sub>(0.0)</sub></sup></sub> | <sub><sup>23.1<sub>(0.3)</sub></sup></sub> | config, model, log | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8174644846423301
      ],
      "excerpt": "| CE | v2t  | <sub><sup>17.7<sub>(0.6)</sub></sup></sub> | <sub><sup>46.6<sub>(0.7)</sub></sup></sub> | <sub><sup>62.8<sub>(0.4)</sub></sup></sub> | <sub><sup>90.9<sub>(0.2)</sub></sup></sub> | <sub><sup>6.0<sub>(0.0)</sub></sup></sub> | <sub><sup>24.4<sub>(0.5)</sub></sup></sub> | config, model, log | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8248864505463187
      ],
      "excerpt": "Evaluting a pretrained model for a given dataset requires: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8758533051329421,
        0.8817821993604646
      ],
      "excerpt": "2. A config.json file. \n3. A trained_model.pth file. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9152750849795331
      ],
      "excerpt": "python3 misc/sync_experts.py --dataset MSVD \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.920388338817455
      ],
      "excerpt": "export MODEL=data/models/msvd-train-full-ce/5bb8dda1/seed-0/2020-01-30_12-29-56/trained_model.pth \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9428034824391166
      ],
      "excerpt": "python3 test.py --config configs/msvd/train-full-ce.json --resume ${MODEL} --device 0 --eval_from_training_config \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9152750849795331,
        0.8589658548129034,
        0.9210926358526699
      ],
      "excerpt": "python3 misc/sync_experts.py --dataset LSMDC \n: Train the model \npython3 train.py --config configs/lsmdc/train-full-ce.json --device 0 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/albanie/collaborative-experts/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Apache License 2.0",
      "url": "https://api.github.com/licenses/apache-2.0"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'                                 Apache License\\n                           Version 2.0, January 2004\\n                        http://www.apache.org/licenses/\\n\\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\\n\\n   1. Definitions.\\n\\n      \"License\" shall mean the terms and conditions for use, reproduction,\\n      and distribution as defined by Sections 1 through 9 of this document.\\n\\n      \"Licensor\" shall mean the copyright owner or entity authorized by\\n      the copyright owner that is granting the License.\\n\\n      \"Legal Entity\" shall mean the union of the acting entity and all\\n      other entities that control, are controlled by, or are under common\\n      control with that entity. For the purposes of this definition,\\n      \"control\" means (i) the power, direct or indirect, to cause the\\n      direction or management of such entity, whether by contract or\\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\\n      outstanding shares, or (iii) beneficial ownership of such entity.\\n\\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\\n      exercising permissions granted by this License.\\n\\n      \"Source\" form shall mean the preferred form for making modifications,\\n      including but not limited to software source code, documentation\\n      source, and configuration files.\\n\\n      \"Object\" form shall mean any form resulting from mechanical\\n      transformation or translation of a Source form, including but\\n      not limited to compiled object code, generated documentation,\\n      and conversions to other media types.\\n\\n      \"Work\" shall mean the work of authorship, whether in Source or\\n      Object form, made available under the License, as indicated by a\\n      copyright notice that is included in or attached to the work\\n      (an example is provided in the Appendix below).\\n\\n      \"Derivative Works\" shall mean any work, whether in Source or Object\\n      form, that is based on (or derived from) the Work and for which the\\n      editorial revisions, annotations, elaborations, or other modifications\\n      represent, as a whole, an original work of authorship. For the purposes\\n      of this License, Derivative Works shall not include works that remain\\n      separable from, or merely link (or bind by name) to the interfaces of,\\n      the Work and Derivative Works thereof.\\n\\n      \"Contribution\" shall mean any work of authorship, including\\n      the original version of the Work and any modifications or additions\\n      to that Work or Derivative Works thereof, that is intentionally\\n      submitted to Licensor for inclusion in the Work by the copyright owner\\n      or by an individual or Legal Entity authorized to submit on behalf of\\n      the copyright owner. For the purposes of this definition, \"submitted\"\\n      means any form of electronic, verbal, or written communication sent\\n      to the Licensor or its representatives, including but not limited to\\n      communication on electronic mailing lists, source code control systems,\\n      and issue tracking systems that are managed by, or on behalf of, the\\n      Licensor for the purpose of discussing and improving the Work, but\\n      excluding communication that is conspicuously marked or otherwise\\n      designated in writing by the copyright owner as \"Not a Contribution.\"\\n\\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\\n      on behalf of whom a Contribution has been received by Licensor and\\n      subsequently incorporated within the Work.\\n\\n   2. Grant of Copyright License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      copyright license to reproduce, prepare Derivative Works of,\\n      publicly display, publicly perform, sublicense, and distribute the\\n      Work and such Derivative Works in Source or Object form.\\n\\n   3. Grant of Patent License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      (except as stated in this section) patent license to make, have made,\\n      use, offer to sell, sell, import, and otherwise transfer the Work,\\n      where such license applies only to those patent claims licensable\\n      by such Contributor that are necessarily infringed by their\\n      Contribution(s) alone or by combination of their Contribution(s)\\n      with the Work to which such Contribution(s) was submitted. If You\\n      institute patent litigation against any entity (including a\\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\\n      or a Contribution incorporated within the Work constitutes direct\\n      or contributory patent infringement, then any patent licenses\\n      granted to You under this License for that Work shall terminate\\n      as of the date such litigation is filed.\\n\\n   4. Redistribution. You may reproduce and distribute copies of the\\n      Work or Derivative Works thereof in any medium, with or without\\n      modifications, and in Source or Object form, provided that You\\n      meet the following conditions:\\n\\n      (a) You must give any other recipients of the Work or\\n          Derivative Works a copy of this License; and\\n\\n      (b) You must cause any modified files to carry prominent notices\\n          stating that You changed the files; and\\n\\n      (c) You must retain, in the Source form of any Derivative Works\\n          that You distribute, all copyright, patent, trademark, and\\n          attribution notices from the Source form of the Work,\\n          excluding those notices that do not pertain to any part of\\n          the Derivative Works; and\\n\\n      (d) If the Work includes a \"NOTICE\" text file as part of its\\n          distribution, then any Derivative Works that You distribute must\\n          include a readable copy of the attribution notices contained\\n          within such NOTICE file, excluding those notices that do not\\n          pertain to any part of the Derivative Works, in at least one\\n          of the following places: within a NOTICE text file distributed\\n          as part of the Derivative Works; within the Source form or\\n          documentation, if provided along with the Derivative Works; or,\\n          within a display generated by the Derivative Works, if and\\n          wherever such third-party notices normally appear. The contents\\n          of the NOTICE file are for informational purposes only and\\n          do not modify the License. You may add Your own attribution\\n          notices within Derivative Works that You distribute, alongside\\n          or as an addendum to the NOTICE text from the Work, provided\\n          that such additional attribution notices cannot be construed\\n          as modifying the License.\\n\\n      You may add Your own copyright statement to Your modifications and\\n      may provide additional or different license terms and conditions\\n      for use, reproduction, or distribution of Your modifications, or\\n      for any such Derivative Works as a whole, provided Your use,\\n      reproduction, and distribution of the Work otherwise complies with\\n      the conditions stated in this License.\\n\\n   5. Submission of Contributions. Unless You explicitly state otherwise,\\n      any Contribution intentionally submitted for inclusion in the Work\\n      by You to the Licensor shall be under the terms and conditions of\\n      this License, without any additional terms or conditions.\\n      Notwithstanding the above, nothing herein shall supersede or modify\\n      the terms of any separate license agreement you may have executed\\n      with Licensor regarding such Contributions.\\n\\n   6. Trademarks. This License does not grant permission to use the trade\\n      names, trademarks, service marks, or product names of the Licensor,\\n      except as required for reasonable and customary use in describing the\\n      origin of the Work and reproducing the content of the NOTICE file.\\n\\n   7. Disclaimer of Warranty. Unless required by applicable law or\\n      agreed to in writing, Licensor provides the Work (and each\\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\\n      implied, including, without limitation, any warranties or conditions\\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\\n      PARTICULAR PURPOSE. You are solely responsible for determining the\\n      appropriateness of using or redistributing the Work and assume any\\n      risks associated with Your exercise of permissions under this License.\\n\\n   8. Limitation of Liability. In no event and under no legal theory,\\n      whether in tort (including negligence), contract, or otherwise,\\n      unless required by applicable law (such as deliberate and grossly\\n      negligent acts) or agreed to in writing, shall any Contributor be\\n      liable to You for damages, including any direct, indirect, special,\\n      incidental, or consequential damages of any character arising as a\\n      result of this License or out of the use or inability to use the\\n      Work (including but not limited to damages for loss of goodwill,\\n      work stoppage, computer failure or malfunction, or any and all\\n      other commercial damages or losses), even if such Contributor\\n      has been advised of the possibility of such damages.\\n\\n   9. Accepting Warranty or Additional Liability. While redistributing\\n      the Work or Derivative Works thereof, You may choose to offer,\\n      and charge a fee for, acceptance of support, warranty, indemnity,\\n      or other liability obligations and/or rights consistent with this\\n      License. However, in accepting such obligations, You may act only\\n      on Your own behalf and on Your sole responsibility, not on behalf\\n      of any other Contributor, and only if You agree to indemnify,\\n      defend, and hold each Contributor harmless for any liability\\n      incurred by, or claims asserted against, such Contributor by reason\\n      of your accepting any such warranty or additional liability.\\n\\n   END OF TERMS AND CONDITIONS\\n\\n   Copyright 2020 Samuel Albanie, Yang Liu and Arsha Nagrani\\n\\n   Licensed under the Apache License, Version 2.0 (the \"License\");\\n   you may not use this file except in compliance with the License.\\n   You may obtain a copy of the License at\\n\\n       http://www.apache.org/licenses/LICENSE-2.0\\n\\n   Unless required by applicable law or agreed to in writing, software\\n   distributed under the License is distributed on an \"AS IS\" BASIS,\\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n   See the License for the specific language governing permissions and\\n   limitations under the License.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "## TeachText",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "collaborative-experts",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "albanie",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/albanie/collaborative-experts/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Dependencies can be installed via `pip install -r requirements/pip-requirements.txt`.\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 205,
      "date": "Thu, 23 Dec 2021 23:31:19 GMT"
    },
    "technique": "GitHub API"
  },
  "support": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We conduct several ablation studies to investigate the importance of different components in the Collaborative Experts design.  Each ablation is conducted on the MSRVTT dataset. \n\n**CE Design**: First, we investigate the importance of the parts used by the CE model.\n\n| Model | Task | R@1 | R@5 | R@10 | MdR | Params | Links |\n| ---   | :--: | :-: | :-: | :--: | :-: | :----: | :---: |\n| Concat | t2v  | <sub><sup>0.0<sub>(0.0)</sub></sup></sub> | <sub><sup>0.0<sub>(0.0)</sub></sup></sub> | <sub><sup>0.0<sub>(0.0)</sub></sup></sub> | <sub><sup>1495.5<sub>(0.0)</sub></sup></sub> | 369.72k | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-concat-ablation/b6fb2189/seed-0/2020-01-07_15-19-41/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-concat-ablation/b6fb2189/seed-0/2020-01-07_15-19-41/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-concat-ablation/b6fb2189/seed-0/2020-01-07_15-19-41/summary-seed-0_seed-1_seed-2.json) |\n| CE - MW,P,CG | t2v  | <sub><sup>8.5<sub>(0.1)</sub></sup></sub> | <sub><sup>25.9<sub>(0.3)</sub></sup></sub> | <sub><sup>37.6<sub>(0.2)</sub></sup></sub> | <sub><sup>19.0<sub>(0.0)</sub></sup></sub> | 246.22M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-moee-minus-moee-weights/d8ef2d99/seed-0/2020-01-07_15-20-52/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-moee-minus-moee-weights/d8ef2d99/seed-0/2020-01-07_15-20-52/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-moee-minus-moee-weights/d8ef2d99/seed-0/2020-01-07_15-20-52/summary-seed-0_seed-1_seed-2.json) |\n| CE - P,CG | t2v  | <sub><sup>9.6<sub>(0.1)</sub></sup></sub> | <sub><sup>28.0<sub>(0.2)</sub></sup></sub> | <sub><sup>39.7<sub>(0.2)</sub></sup></sub> | <sub><sup>17.7<sub>(0.6)</sub></sup></sub> | 400.41M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-moee/5738fb92/seed-0/2020-01-07_15-27-15/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-moee/5738fb92/seed-0/2020-01-07_15-27-15/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-moee/5738fb92/seed-0/2020-01-07_15-27-15/summary-seed-0_seed-1_seed-2.json) |\n| CE - CG  | t2v  | <sub><sup>9.7<sub>(0.1)</sub></sup></sub> | <sub><sup>28.1<sub>(0.2)</sub></sup></sub> | <sub><sup>40.2<sub>(0.1)</sub></sup></sub> | <sub><sup>17.0<sub>(0.0)</sub></sup></sub> | 181.07M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-ablation-dims/351a360e/seed-0/2020-01-07_15-17-10/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-ablation-dims/351a360e/seed-0/2020-01-07_15-17-10/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-ablation-dims/351a360e/seed-0/2020-01-07_15-17-10/summary-seed-0_seed-1_seed-2.json) |\n| CE    | t2v  | <sub><sup>10.0<sub>(0.1)</sub></sup></sub> | <sub><sup>29.0<sub>(0.3)</sub></sup></sub> | <sub><sup>41.2<sub>(0.2)</sub></sup></sub> | <sub><sup>16.0<sub>(0.0)</sub></sup></sub> | 183.45M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce/074e6b24/seed-0/2020-01-07_15-24-30/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce/074e6b24/seed-0/2020-01-07_15-24-30/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce/074e6b24/seed-0/2020-01-07_15-24-30/summary-seed-0_seed-1_seed-2.json) |\n| Concat | v2t  | <sub><sup>0.0<sub>(0.0)</sub></sup></sub> | <sub><sup>0.0<sub>(0.0)</sub></sup></sub> | <sub><sup>0.0<sub>(0.0)</sub></sup></sub> | <sub><sup>29897.5<sub>(0.0)</sub></sup></sub> | 369.72k | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-concat-ablation/b6fb2189/seed-0/2020-01-07_15-19-41/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-concat-ablation/b6fb2189/seed-0/2020-01-07_15-19-41/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-concat-ablation/b6fb2189/seed-0/2020-01-07_15-19-41/summary-seed-0_seed-1_seed-2.json) |\n| CE - MW,P,CG | v2t  | <sub><sup>13.7<sub>(0.4)</sub></sup></sub> | <sub><sup>38.8<sub>(1.2)</sub></sup></sub> | <sub><sup>53.1<sub>(1.1)</sub></sup></sub> | <sub><sup>9.2<sub>(0.8)</sub></sup></sub> | 246.22M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-moee-minus-moee-weights/d8ef2d99/seed-0/2020-01-07_15-20-52/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-moee-minus-moee-weights/d8ef2d99/seed-0/2020-01-07_15-20-52/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-moee-minus-moee-weights/d8ef2d99/seed-0/2020-01-07_15-20-52/summary-seed-0_seed-1_seed-2.json) |\n| CE - P,CG | v2t  | <sub><sup>14.1<sub>(0.2)</sub></sup></sub> | <sub><sup>39.5<sub>(1.0)</sub></sup></sub> | <sub><sup>53.2<sub>(0.3)</sub></sup></sub> | <sub><sup>9.0<sub>(0.0)</sub></sup></sub> | 400.41M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-moee/5738fb92/seed-0/2020-01-07_15-27-15/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-moee/5738fb92/seed-0/2020-01-07_15-27-15/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-moee/5738fb92/seed-0/2020-01-07_15-27-15/summary-seed-0_seed-1_seed-2.json) |\n| CE - CG  | v2t  | <sub><sup>15.1<sub>(0.3)</sub></sup></sub> | <sub><sup>40.3<sub>(0.5)</sub></sup></sub> | <sub><sup>54.3<sub>(0.7)</sub></sup></sub> | <sub><sup>8.8<sub>(0.3)</sub></sup></sub> | 181.07M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-ablation-dims/351a360e/seed-0/2020-01-07_15-17-10/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-ablation-dims/351a360e/seed-0/2020-01-07_15-17-10/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-ablation-dims/351a360e/seed-0/2020-01-07_15-17-10/summary-seed-0_seed-1_seed-2.json) |\n| CE    | v2t  | <sub><sup>15.6<sub>(0.3)</sub></sup></sub> | <sub><sup>40.9<sub>(1.4)</sub></sup></sub> | <sub><sup>55.2<sub>(1.0)</sub></sup></sub> | <sub><sup>8.3<sub>(0.6)</sub></sup></sub> | 183.45M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce/074e6b24/seed-0/2020-01-07_15-24-30/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce/074e6b24/seed-0/2020-01-07_15-24-30/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce/074e6b24/seed-0/2020-01-07_15-24-30/summary-seed-0_seed-1_seed-2.json) |\n\nEach row adds an additional component to the model.  The names refer to the following model designs:\n* **Concat**: A barebones concatenation model.  After aggregating each expert across time (which still requires some parameters for the variable-length VLAD layers), the experts are concatenated and compared directly against the aggregated text embeddings.  Note: this model uses a slightly greater number of VLAD clusters than the others to allow the concatentated embedding to match the dimensionality of the text.\n* **CE - MW,P,CG** - The CE model without MoE weights, projecting to a common dimension or Collaborative Gating.\n* **CE - P,CG** - The CE model without projecting to a common dimension or Collaborative Gating (note that this is equivalent to the MoEE model proposed in [2]).\n* **CE - CG** - The CE model without Collaborative Gating (CG).\n* **CE** - The full CE model.\n\nNote that in the table above some metrics have been removed to allow the number of parameters to be displayed---these additional metrics can be found in the linked logs.\n\n**Importance of Different Experts**: The next ablation investigates the value of each of the different experts towards the final embedding.  Since not all experts are available in every video, we pair each expert with scene features, to give an approximation of their usefulness.\n\n| Experts | Task | R@1 | R@5 | R@10 | MdR | Params | Links |\n| -----   | :--: | :-: | :-: | :--: | :-: | :----: | :---: |\n| Scene    | t2v  | <sub><sup>4.0<sub>(0.1)</sub></sup></sub> | <sub><sup>14.1<sub>(0.1)</sub></sup></sub> | <sub><sup>22.4<sub>(0.3)</sub></sup></sub> | <sub><sup>50.0<sub>(1.0)</sub></sup></sub> | 19.46M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene/812545a9/seed-0/2020-01-07_14-51-54/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene/812545a9/seed-0/2020-01-07_14-51-54/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-only-scene/812545a9/seed-0/2020-01-07_14-51-54/summary-seed-0_seed-1_seed-2.json) |\n| Scene + Inst. | t2v  | <sub><sup>7.2<sub>(0.1)</sub></sup></sub> | <sub><sup>22.3<sub>(0.3)</sub></sup></sub> | <sub><sup>33.0<sub>(0.2)</sub></sup></sub> | <sub><sup>25.3<sub>(0.6)</sub></sup></sub> | 41.12M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-inst/4bce7bbc/seed-0/2020-01-07_15-06-34/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-inst/4bce7bbc/seed-0/2020-01-07_15-06-34/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-only-scene-inst/4bce7bbc/seed-0/2020-01-07_15-06-34/summary-seed-0_seed-1_seed-2.json) |\n| Scene + r2p1d | t2v  | <sub><sup>6.8<sub>(0.1)</sub></sup></sub> | <sub><sup>21.7<sub>(0.1)</sub></sup></sub> | <sub><sup>32.4<sub>(0.1)</sub></sup></sub> | <sub><sup>25.7<sub>(0.6)</sub></sup></sub> | 39.95M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-r2p1d/2e357adc/seed-0/2020-01-07_15-09-23/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-r2p1d/2e357adc/seed-0/2020-01-07_15-09-23/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-only-scene-r2p1d/2e357adc/seed-0/2020-01-07_15-09-23/summary-seed-0_seed-1_seed-2.json) |\n| Scene + RGB | t2v  | <sub><sup>5.0<sub>(0.2)</sub></sup></sub> | <sub><sup>16.6<sub>(0.7)</sub></sup></sub> | <sub><sup>25.5<sub>(1.0)</sub></sup></sub> | <sub><sup>40.7<sub>(2.1)</sub></sup></sub> | 41.12M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-rgb/8c7aa94e/seed-0/2020-01-07_15-03-11/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-rgb/8c7aa94e/seed-0/2020-01-07_15-03-11/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-only-scene-rgb/8c7aa94e/seed-0/2020-01-07_15-03-11/summary-seed-0_seed-1_seed-2.json) |\n| Scene + Flow | t2v  | <sub><sup>5.3<sub>(0.3)</sub></sup></sub> | <sub><sup>17.6<sub>(0.8)</sub></sup></sub> | <sub><sup>27.1<sub>(0.9)</sub></sup></sub> | <sub><sup>36.0<sub>(1.7)</sub></sup></sub> | 40.34M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-flow/468c68d5/seed-0/2020-01-07_15-05-17/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-flow/468c68d5/seed-0/2020-01-07_15-05-17/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-only-scene-flow/468c68d5/seed-0/2020-01-07_15-05-17/summary-seed-0_seed-1_seed-2.json) |\n| Scene + Audio | t2v  | <sub><sup>5.6<sub>(0.0)</sub></sup></sub> | <sub><sup>18.7<sub>(0.1)</sub></sup></sub> | <sub><sup>28.2<sub>(0.1)</sub></sup></sub> | <sub><sup>33.7<sub>(0.6)</sub></sup></sub> | 40.34M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-audio/4aa51252/seed-0/2020-01-07_15-05-50/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-audio/4aa51252/seed-0/2020-01-07_15-05-50/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-only-scene-audio/4aa51252/seed-0/2020-01-07_15-05-50/summary-seed-0_seed-1_seed-2.json) |\n| Scene + OCR | t2v  | <sub><sup>4.1<sub>(0.1)</sub></sup></sub> | <sub><sup>14.1<sub>(0.1)</sub></sup></sub> | <sub><sup>22.2<sub>(0.2)</sub></sup></sub> | <sub><sup>50.3<sub>(1.2)</sub></sup></sub> | 49.49M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-ocr/f31d8760/seed-0/2020-01-24_08-05-47/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-ocr/f31d8760/seed-0/2020-01-24_08-05-47/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-only-scene-ocr/f31d8760/seed-0/2020-01-24_08-05-47/summary-seed-0_seed-1_seed-2.json) |\n| Scene + Speech | t2v  | <sub><sup>4.6<sub>(0.1)</sub></sup></sub> | <sub><sup>15.5<sub>(0.2)</sub></sup></sub> | <sub><sup>24.4<sub>(0.2)</sub></sup></sub> | <sub><sup>44.7<sub>(1.2)</sub></sup></sub> | 43.94M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-speech/be8b25b5/seed-0/2020-01-07_14-30-08/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-speech/be8b25b5/seed-0/2020-01-07_14-30-08/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-only-scene-speech/be8b25b5/seed-0/2020-01-07_14-30-08/summary-seed-0_seed-1_seed-2.json) |\n| Scene + Face | t2v  | <sub><sup>4.1<sub>(0.1)</sub></sup></sub> | <sub><sup>14.2<sub>(0.3)</sub></sup></sub> | <sub><sup>22.4<sub>(0.4)</sub></sup></sub> | <sub><sup>49.7<sub>(0.6)</sub></sup></sub> | 39.95M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-face/5e28c6f2/seed-0/2020-01-07_14-52-47/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-face/5e28c6f2/seed-0/2020-01-07_14-52-47/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-only-scene-face/5e28c6f2/seed-0/2020-01-07_14-52-47/summary-seed-0_seed-1_seed-2.json) |\n| Scene    | v2t  | <sub><sup>5.6<sub>(0.6)</sub></sup></sub> | <sub><sup>18.2<sub>(0.6)</sub></sup></sub> | <sub><sup>27.7<sub>(0.3)</sub></sup></sub> | <sub><sup>39.0<sub>(0.0)</sub></sup></sub> | 19.46M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene/812545a9/seed-0/2020-01-07_14-51-54/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene/812545a9/seed-0/2020-01-07_14-51-54/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-only-scene/812545a9/seed-0/2020-01-07_14-51-54/summary-seed-0_seed-1_seed-2.json) |\n| Scene + Inst. | v2t  | <sub><sup>10.1<sub>(0.3)</sub></sup></sub> | <sub><sup>29.7<sub>(0.5)</sub></sup></sub> | <sub><sup>41.9<sub>(0.7)</sub></sup></sub> | <sub><sup>15.2<sub>(0.9)</sub></sup></sub> | 41.12M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-inst/4bce7bbc/seed-0/2020-01-07_15-06-34/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-inst/4bce7bbc/seed-0/2020-01-07_15-06-34/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-only-scene-inst/4bce7bbc/seed-0/2020-01-07_15-06-34/summary-seed-0_seed-1_seed-2.json) |\n| Scene + r2p1d | v2t  | <sub><sup>9.4<sub>(0.3)</sub></sup></sub> | <sub><sup>27.8<sub>(0.6)</sub></sup></sub> | <sub><sup>40.1<sub>(1.1)</sub></sup></sub> | <sub><sup>17.2<sub>(1.1)</sub></sup></sub> | 39.95M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-r2p1d/2e357adc/seed-0/2020-01-07_15-09-23/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-r2p1d/2e357adc/seed-0/2020-01-07_15-09-23/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-only-scene-r2p1d/2e357adc/seed-0/2020-01-07_15-09-23/summary-seed-0_seed-1_seed-2.json) |\n| Scene + RGB | v2t  | <sub><sup>6.9<sub>(0.5)</sub></sup></sub> | <sub><sup>21.2<sub>(0.9)</sub></sup></sub> | <sub><sup>31.1<sub>(1.9)</sub></sup></sub> | <sub><sup>28.7<sub>(3.8)</sub></sup></sub> | 41.12M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-rgb/8c7aa94e/seed-0/2020-01-07_15-03-11/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-rgb/8c7aa94e/seed-0/2020-01-07_15-03-11/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-only-scene-rgb/8c7aa94e/seed-0/2020-01-07_15-03-11/summary-seed-0_seed-1_seed-2.json) |\n| Scene + Flow | v2t  | <sub><sup>7.3<sub>(0.6)</sub></sup></sub> | <sub><sup>22.3<sub>(1.4)</sub></sup></sub> | <sub><sup>33.4<sub>(1.7)</sub></sup></sub> | <sub><sup>25.2<sub>(2.0)</sub></sup></sub> | 40.34M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-flow/468c68d5/seed-0/2020-01-07_15-05-17/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-flow/468c68d5/seed-0/2020-01-07_15-05-17/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-only-scene-flow/468c68d5/seed-0/2020-01-07_15-05-17/summary-seed-0_seed-1_seed-2.json) |\n| Scene + Audio | v2t  | <sub><sup>8.2<sub>(0.4)</sub></sup></sub> | <sub><sup>24.8<sub>(0.4)</sub></sup></sub> | <sub><sup>36.0<sub>(0.1)</sub></sup></sub> | <sub><sup>21.7<sub>(0.6)</sub></sup></sub> | 40.34M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-audio/4aa51252/seed-0/2020-01-07_15-05-50/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-audio/4aa51252/seed-0/2020-01-07_15-05-50/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-only-scene-audio/4aa51252/seed-0/2020-01-07_15-05-50/summary-seed-0_seed-1_seed-2.json) |\n| Scene + OCR | v2t  | <sub><sup>5.4<sub>(0.5)</sub></sup></sub> | <sub><sup>18.6<sub>(1.2)</sub></sup></sub> | <sub><sup>26.6<sub>(1.2)</sub></sup></sub> | <sub><sup>40.0<sub>(1.0)</sub></sup></sub> | 49.49M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-ocr/f31d8760/seed-0/2020-01-24_08-05-47/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-ocr/f31d8760/seed-0/2020-01-24_08-05-47/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-only-scene-ocr/f31d8760/seed-0/2020-01-24_08-05-47/summary-seed-0_seed-1_seed-2.json) |\n| Scene + Speech | v2t  | <sub><sup>6.0<sub>(0.2)</sub></sup></sub> | <sub><sup>20.4<sub>(0.5)</sub></sup></sub> | <sub><sup>30.3<sub>(1.0)</sub></sup></sub> | <sub><sup>33.0<sub>(2.0)</sub></sup></sub> | 43.94M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-speech/be8b25b5/seed-0/2020-01-07_14-30-08/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-speech/be8b25b5/seed-0/2020-01-07_14-30-08/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-only-scene-speech/be8b25b5/seed-0/2020-01-07_14-30-08/summary-seed-0_seed-1_seed-2.json) |\n| Scene + Face | v2t  | <sub><sup>5.6<sub>(1.0)</sub></sup></sub> | <sub><sup>17.9<sub>(0.7)</sub></sup></sub> | <sub><sup>26.7<sub>(0.8)</sub></sup></sub> | <sub><sup>39.1<sub>(2.6)</sub></sup></sub> | 39.95M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-face/5e28c6f2/seed-0/2020-01-07_14-52-47/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-face/5e28c6f2/seed-0/2020-01-07_14-52-47/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-only-scene-face/5e28c6f2/seed-0/2020-01-07_14-52-47/summary-seed-0_seed-1_seed-2.json) |\n\nWe can also study their cumulative effect:\n\n| Experts | Task | R@1 | R@5 | R@10 | MdR | Params | Links |\n| -----   | :--: | :-: | :-: | :--: | :-: | :----: | :---: |\n| Scene    | t2v  | <sub><sup>4.0<sub>(0.1)</sub></sup></sub> | <sub><sup>14.1<sub>(0.1)</sub></sup></sub> | <sub><sup>22.4<sub>(0.3)</sub></sup></sub> | <sub><sup>50.0<sub>(1.0)</sub></sup></sub> | 19.46M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene/812545a9/seed-0/2020-01-07_14-51-54/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene/812545a9/seed-0/2020-01-07_14-51-54/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-only-scene/812545a9/seed-0/2020-01-07_14-51-54/summary-seed-0_seed-1_seed-2.json) |\n| Prev. + Speech    | t2v  | <sub><sup>4.6<sub>(0.1)</sub></sup></sub> | <sub><sup>15.5<sub>(0.2)</sub></sup></sub> | <sub><sup>24.4<sub>(0.2)</sub></sup></sub> | <sub><sup>44.7<sub>(1.2)</sub></sup></sub> | 43.94M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-speech/be8b25b5/seed-0/2020-01-07_14-30-08/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-speech/be8b25b5/seed-0/2020-01-07_14-30-08/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-only-scene-speech/be8b25b5/seed-0/2020-01-07_14-30-08/summary-seed-0_seed-1_seed-2.json) |\n| Prev. + Audio    | t2v  | <sub><sup>5.8<sub>(0.1)</sub></sup></sub> | <sub><sup>19.0<sub>(0.3)</sub></sup></sub> | <sub><sup>28.8<sub>(0.2)</sub></sup></sub> | <sub><sup>32.3<sub>(0.6)</sub></sup></sub> | 62.45M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-speech-audio/9e758bc6/seed-0/2020-01-07_14-30-08/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-speech-audio/9e758bc6/seed-0/2020-01-07_14-30-08/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-only-scene-speech-audio/9e758bc6/seed-0/2020-01-07_14-30-08/summary-seed-0_seed-1_seed-2.json) |\n| Prev. + Flow    | t2v  | <sub><sup>6.7<sub>(0.2)</sub></sup></sub> | <sub><sup>21.8<sub>(0.4)</sub></sup></sub> | <sub><sup>32.5<sub>(0.5)</sub></sup></sub> | <sub><sup>25.3<sub>(0.6)</sub></sup></sub> | 80.96M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-speech-audio-flow/b7e54cca/seed-0/2020-01-07_14-28-36/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-speech-audio-flow/b7e54cca/seed-0/2020-01-07_14-28-36/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-only-scene-speech-audio-flow/b7e54cca/seed-0/2020-01-07_14-28-36/summary-seed-0_seed-1_seed-2.json) |\n| Prev. + RGB    | t2v  | <sub><sup>7.5<sub>(0.1)</sub></sup></sub> | <sub><sup>23.4<sub>(0.0)</sub></sup></sub> | <sub><sup>34.1<sub>(0.2)</sub></sup></sub> | <sub><sup>23.7<sub>(0.6)</sub></sup></sub> | 100.26M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-speech-audio-flow-rgb/7ace4a8c/seed-0/2020-01-07_14-32-06/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-speech-audio-flow-rgb/7ace4a8c/seed-0/2020-01-07_14-32-06/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-only-scene-speech-audio-flow-rgb/7ace4a8c/seed-0/2020-01-07_14-32-06/summary-seed-0_seed-1_seed-2.json) |\n| Prev. + Inst    | t2v  | <sub><sup>9.5<sub>(0.2)</sub></sup></sub> | <sub><sup>27.7<sub>(0.1)</sub></sup></sub> | <sub><sup>39.4<sub>(0.1)</sub></sup></sub> | <sub><sup>18.0<sub>(0.0)</sub></sup></sub> | 119.56M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-speech-audio-flow-rgb-inst/f580d35f/seed-0/2020-01-07_14-29-58/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-speech-audio-flow-rgb-inst/f580d35f/seed-0/2020-01-07_14-29-58/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-only-scene-speech-audio-flow-rgb-inst/f580d35f/seed-0/2020-01-07_14-29-58/summary-seed-0_seed-1_seed-2.json) |\n| Prev. + R2P1D    | t2v  | <sub><sup>9.9<sub>(0.1)</sub></sup></sub> | <sub><sup>28.6<sub>(0.3)</sub></sup></sub> | <sub><sup>40.7<sub>(0.1)</sub></sup></sub> | <sub><sup>17.0<sub>(0.0)</sub></sup></sub> | 137.67M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-speech-audio-flow-rgb-inst-r2p1d/95fb9733/seed-0/2020-01-07_14-28-36/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-speech-audio-flow-rgb-inst-r2p1d/95fb9733/seed-0/2020-01-07_14-28-36/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-only-scene-speech-audio-flow-rgb-inst-r2p1d/95fb9733/seed-0/2020-01-07_14-28-36/summary-seed-0_seed-1_seed-2.json) |\n| Prev. + OCR    | t2v  | <sub><sup>10.0<sub>(0.1)</sub></sup></sub> | <sub><sup>28.8<sub>(0.2)</sub></sup></sub> | <sub><sup>40.9<sub>(0.2)</sub></sup></sub> | <sub><sup>16.7<sub>(0.6)</sub></sup></sub> | 165.33M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-speech-audio-flow-rgb-inst-r2p1d-ocr/282b618c/seed-0/2020-01-07_14-28-36/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-speech-audio-flow-rgb-inst-r2p1d-ocr/282b618c/seed-0/2020-01-07_14-28-36/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-only-scene-speech-audio-flow-rgb-inst-r2p1d-ocr/282b618c/seed-0/2020-01-07_14-28-36/summary-seed-0_seed-1_seed-2.json) |\n| Prev. + Face    | t2v  | <sub><sup>10.0<sub>(0.1)</sub></sup></sub> | <sub><sup>29.0<sub>(0.3)</sub></sup></sub> | <sub><sup>41.2<sub>(0.2)</sub></sup></sub> | <sub><sup>16.0<sub>(0.0)</sub></sup></sub> | 183.45M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-speech-audio-flow-rgb-inst-r2p1d-ocr-face/d89c1126/seed-0/2020-01-07_14-28-36/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-speech-audio-flow-rgb-inst-r2p1d-ocr-face/d89c1126/seed-0/2020-01-07_14-28-36/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-only-scene-speech-audio-flow-rgb-inst-r2p1d-ocr-face/d89c1126/seed-0/2020-01-07_14-28-36/summary-seed-0_seed-1_seed-2.json) |\n| Scene    | v2t  | <sub><sup>5.6<sub>(0.6)</sub></sup></sub> | <sub><sup>18.2<sub>(0.6)</sub></sup></sub> | <sub><sup>27.7<sub>(0.3)</sub></sup></sub> | <sub><sup>39.0<sub>(0.0)</sub></sup></sub> | 19.46M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene/812545a9/seed-0/2020-01-07_14-51-54/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene/812545a9/seed-0/2020-01-07_14-51-54/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-only-scene/812545a9/seed-0/2020-01-07_14-51-54/summary-seed-0_seed-1_seed-2.json) |\n| Prev. + Speech    | v2t  | <sub><sup>6.0<sub>(0.2)</sub></sup></sub> | <sub><sup>20.4<sub>(0.5)</sub></sup></sub> | <sub><sup>30.3<sub>(1.0)</sub></sup></sub> | <sub><sup>33.0<sub>(2.0)</sub></sup></sub> | 43.94M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-speech/be8b25b5/seed-0/2020-01-07_14-30-08/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-speech/be8b25b5/seed-0/2020-01-07_14-30-08/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-only-scene-speech/be8b25b5/seed-0/2020-01-07_14-30-08/summary-seed-0_seed-1_seed-2.json) |\n| Prev. + Audio    | v2t  | <sub><sup>8.6<sub>(0.2)</sub></sup></sub> | <sub><sup>26.1<sub>(0.6)</sub></sup></sub> | <sub><sup>37.8<sub>(0.8)</sub></sup></sub> | <sub><sup>19.8<sub>(0.8)</sub></sup></sub> | 62.45M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-speech-audio/9e758bc6/seed-0/2020-01-07_14-30-08/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-speech-audio/9e758bc6/seed-0/2020-01-07_14-30-08/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-only-scene-speech-audio/9e758bc6/seed-0/2020-01-07_14-30-08/summary-seed-0_seed-1_seed-2.json) |\n| Prev. + Flow    | v2t  | <sub><sup>9.9<sub>(0.4)</sub></sup></sub> | <sub><sup>28.6<sub>(0.7)</sub></sup></sub> | <sub><sup>41.7<sub>(0.8)</sub></sup></sub> | <sub><sup>15.7<sub>(0.6)</sub></sup></sub> | 80.96M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-speech-audio-flow/b7e54cca/seed-0/2020-01-07_14-28-36/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-speech-audio-flow/b7e54cca/seed-0/2020-01-07_14-28-36/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-only-scene-speech-audio-flow/b7e54cca/seed-0/2020-01-07_14-28-36/summary-seed-0_seed-1_seed-2.json) |\n| Prev. + RGB    | v2t  | <sub><sup>11.2<sub>(0.3)</sub></sup></sub> | <sub><sup>32.1<sub>(0.8)</sub></sup></sub> | <sub><sup>45.4<sub>(0.6)</sub></sup></sub> | <sub><sup>13.7<sub>(0.6)</sub></sup></sub> | 100.26M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-speech-audio-flow-rgb/7ace4a8c/seed-0/2020-01-07_14-32-06/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-speech-audio-flow-rgb/7ace4a8c/seed-0/2020-01-07_14-32-06/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-only-scene-speech-audio-flow-rgb/7ace4a8c/seed-0/2020-01-07_14-32-06/summary-seed-0_seed-1_seed-2.json) |\n| Prev. + Inst.    | v2t  | <sub><sup>14.7<sub>(0.6)</sub></sup></sub> | <sub><sup>38.9<sub>(0.8)</sub></sup></sub> | <sub><sup>53.1<sub>(1.0)</sub></sup></sub> | <sub><sup>9.3<sub>(0.6)</sub></sup></sub> | 119.56M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-speech-audio-flow-rgb-inst/f580d35f/seed-0/2020-01-07_14-29-58/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-speech-audio-flow-rgb-inst/f580d35f/seed-0/2020-01-07_14-29-58/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-only-scene-speech-audio-flow-rgb-inst/f580d35f/seed-0/2020-01-07_14-29-58/summary-seed-0_seed-1_seed-2.json) |\n| Prev. + R2P1D    | v2t  | <sub><sup>15.5<sub>(0.6)</sub></sup></sub> | <sub><sup>40.1<sub>(1.2)</sub></sup></sub> | <sub><sup>54.4<sub>(1.3)</sub></sup></sub> | <sub><sup>8.7<sub>(0.6)</sub></sup></sub> | 137.67M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-speech-audio-flow-rgb-inst-r2p1d/95fb9733/seed-0/2020-01-07_14-28-36/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-speech-audio-flow-rgb-inst-r2p1d/95fb9733/seed-0/2020-01-07_14-28-36/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-only-scene-speech-audio-flow-rgb-inst-r2p1d/95fb9733/seed-0/2020-01-07_14-28-36/summary-seed-0_seed-1_seed-2.json) |\n| Prev. + OCR    | v2t  | <sub><sup>15.2<sub>(0.1)</sub></sup></sub> | <sub><sup>41.1<sub>(0.6)</sub></sup></sub> | <sub><sup>54.6<sub>(0.7)</sub></sup></sub> | <sub><sup>8.5<sub>(0.5)</sub></sup></sub> | 165.33M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-speech-audio-flow-rgb-inst-r2p1d-ocr/282b618c/seed-0/2020-01-07_14-28-36/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-speech-audio-flow-rgb-inst-r2p1d-ocr/282b618c/seed-0/2020-01-07_14-28-36/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-only-scene-speech-audio-flow-rgb-inst-r2p1d-ocr/282b618c/seed-0/2020-01-07_14-28-36/summary-seed-0_seed-1_seed-2.json) |\n| Prev. + Face    | v2t  | <sub><sup>15.6<sub>(0.3)</sub></sup></sub> | <sub><sup>40.9<sub>(1.4)</sub></sup></sub> | <sub><sup>55.2<sub>(1.0)</sub></sup></sub> | <sub><sup>8.3<sub>(0.6)</sub></sup></sub> | 183.45M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-speech-audio-flow-rgb-inst-r2p1d-ocr-face/d89c1126/seed-0/2020-01-07_14-28-36/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-scene-speech-audio-flow-rgb-inst-r2p1d-ocr-face/d89c1126/seed-0/2020-01-07_14-28-36/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-only-scene-speech-audio-flow-rgb-inst-r2p1d-ocr-face/d89c1126/seed-0/2020-01-07_14-28-36/summary-seed-0_seed-1_seed-2.json) |\n\n\n**Importance of Model Capacity**: The next ablation investigates the value of the shared embedding dimension used by CE.\n\n| Dimension | Task | R@1 | R@5 | R@10 | MdR | Params | Links |\n| -----   | :--: | :-: | :-: | :--: | :-: | :----: | :---: |\n| 384    | t2v  | <sub><sup>9.4<sub>(0.2)</sub></sup></sub> | <sub><sup>27.8<sub>(0.4)</sub></sup></sub> | <sub><sup>39.8<sub>(0.4)</sub></sup></sub> | <sub><sup>17.7<sub>(0.6)</sub></sup></sub> | 88.62M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-384/777513d3/seed-0/2020-01-24_07-59-43/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-384/777513d3/seed-0/2020-01-24_07-59-43/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-384/777513d3/seed-0/2020-01-24_07-59-43/summary-seed-0_seed-1_seed-2.json) |\n| 512    | t2v  | <sub><sup>9.8<sub>(0.3)</sub></sup></sub> | <sub><sup>28.6<sub>(0.4)</sub></sup></sub> | <sub><sup>40.6<sub>(0.4)</sub></sup></sub> | <sub><sup>17.0<sub>(0.0)</sub></sup></sub> | 119.51M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-512/e1b0e018/seed-0/2020-01-24_07-10-30/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-512/e1b0e018/seed-0/2020-01-24_07-10-30/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-512/e1b0e018/seed-0/2020-01-24_07-10-30/summary-seed-0_seed-1_seed-2.json) |\n| 640    | t2v  | <sub><sup>10.1<sub>(0.1)</sub></sup></sub> | <sub><sup>28.8<sub>(0.1)</sub></sup></sub> | <sub><sup>40.9<sub>(0.2)</sub></sup></sub> | <sub><sup>16.7<sub>(0.6)</sub></sup></sub> | 151.12M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-640/364b2ecc/seed-0/2020-01-24_07-13-18/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-640/364b2ecc/seed-0/2020-01-24_07-13-18/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-640/364b2ecc/seed-0/2020-01-24_07-13-18/summary-seed-0_seed-1_seed-2.json) |\n| 768    | t2v  | <sub><sup>10.0<sub>(0.1)</sub></sup></sub> | <sub><sup>29.0<sub>(0.3)</sub></sup></sub> | <sub><sup>41.2<sub>(0.2)</sub></sup></sub> | <sub><sup>16.0<sub>(0.0)</sub></sup></sub> | 183.45M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce/074e6b24/seed-0/2020-01-07_15-24-30/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce/074e6b24/seed-0/2020-01-07_15-24-30/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce/074e6b24/seed-0/2020-01-07_15-24-30/summary-seed-0_seed-1_seed-2.json) |\n| 1024    | t2v  | <sub><sup>9.9<sub>(0.1)</sub></sup></sub> | <sub><sup>28.6<sub>(0.3)</sub></sup></sub> | <sub><sup>40.7<sub>(0.4)</sub></sup></sub> | <sub><sup>17.0<sub>(0.0)</sub></sup></sub> | 250.27M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-1024/e25a1038/seed-0/2020-01-24_07-10-29/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-1024/e25a1038/seed-0/2020-01-24_07-10-29/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-1024/e25a1038/seed-0/2020-01-24_07-10-29/summary-seed-0_seed-1_seed-2.json) |\n| 384    | v2t  | <sub><sup>14.0<sub>(0.5)</sub></sup></sub> | <sub><sup>38.7<sub>(0.5)</sub></sup></sub> | <sub><sup>52.7<sub>(1.4)</sub></sup></sub> | <sub><sup>9.3<sub>(0.6)</sub></sup></sub> | 88.62M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-384/777513d3/seed-0/2020-01-24_07-59-43/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-384/777513d3/seed-0/2020-01-24_07-59-43/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-384/777513d3/seed-0/2020-01-24_07-59-43/summary-seed-0_seed-1_seed-2.json) |\n| 512    | v2t  | <sub><sup>14.8<sub>(0.4)</sub></sup></sub> | <sub><sup>40.4<sub>(0.6)</sub></sup></sub> | <sub><sup>53.9<sub>(0.4)</sub></sup></sub> | <sub><sup>8.8<sub>(0.3)</sub></sup></sub> | 119.51M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-512/e1b0e018/seed-0/2020-01-24_07-10-30/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-512/e1b0e018/seed-0/2020-01-24_07-10-30/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-512/e1b0e018/seed-0/2020-01-24_07-10-30/summary-seed-0_seed-1_seed-2.json) |\n| 640    | v2t  | <sub><sup>15.6<sub>(0.6)</sub></sup></sub> | <sub><sup>41.3<sub>(0.7)</sub></sup></sub> | <sub><sup>55.0<sub>(0.5)</sub></sup></sub> | <sub><sup>8.3<sub>(0.6)</sub></sup></sub> | 151.12M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-640/364b2ecc/seed-0/2020-01-24_07-13-18/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-640/364b2ecc/seed-0/2020-01-24_07-13-18/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-640/364b2ecc/seed-0/2020-01-24_07-13-18/summary-seed-0_seed-1_seed-2.json) |\n| 768    | v2t  | <sub><sup>15.6<sub>(0.3)</sub></sup></sub> | <sub><sup>40.9<sub>(1.4)</sub></sup></sub> | <sub><sup>55.2<sub>(1.0)</sub></sup></sub> | <sub><sup>8.3<sub>(0.6)</sub></sup></sub> | 183.45M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce/074e6b24/seed-0/2020-01-07_15-24-30/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce/074e6b24/seed-0/2020-01-07_15-24-30/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce/074e6b24/seed-0/2020-01-07_15-24-30/summary-seed-0_seed-1_seed-2.json) |\n| 1024    | v2t  | <sub><sup>14.7<sub>(0.4)</sub></sup></sub> | <sub><sup>40.7<sub>(0.8)</sub></sup></sub> | <sub><sup>54.4<sub>(0.3)</sub></sup></sub> | <sub><sup>8.5<sub>(0.5)</sub></sup></sub> | 250.27M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-1024/e25a1038/seed-0/2020-01-24_07-10-29/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-1024/e25a1038/seed-0/2020-01-24_07-10-29/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-1024/e25a1038/seed-0/2020-01-24_07-10-29/summary-seed-0_seed-1_seed-2.json) |\n\n**Training with more captions:** Rather than varying the number of experts, we can also investigate how performance changes as we change the number of training captions available per-video.\n\n| Experts | Caps. | Task | R@1 | R@5 | R@10 | MdR | Params | Links |\n| -----   | :------: | :--: | :-: | :-: | :--: | :-: | :----: | :---: |\n| RGB   | 1 | t2v | <sub><sup>2.6<sub>(0.1)</sub></sup></sub> | <sub><sup>9.3<sub>(0.4)</sub></sup></sub> | <sub><sup>15.0<sub>(0.7)</sub></sup></sub> | <sub><sup>101.3<sub>(15.5)</sub></sup></sub> | 56.7M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-ablation-restrict-captions-only-rgb/f178e88e/seed-0/2020-01-07_15-21-50/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-ablation-restrict-captions-only-rgb/f178e88e/seed-0/2020-01-07_15-21-50/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-ablation-restrict-captions-only-rgb/f178e88e/seed-0/2020-01-07_15-21-50/summary-seed-0_seed-1_seed-2.json) |\n| RGB   | 20 | t2v  | <sub><sup>4.9<sub>(0.1)</sub></sup></sub> | <sub><sup>16.5<sub>(0.2)</sub></sup></sub> | <sub><sup>25.3<sub>(0.4)</sub></sup></sub> | <sub><sup>40.7<sub>(1.2)</sub></sup></sub> | 58.05M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-rgb/ee84d270/seed-0/2019-12-20_14-41-56/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-rgb/ee84d270/seed-0/2019-12-20_14-41-56/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-only-rgb/ee84d270/seed-0/2019-12-20_14-41-56/summary-seed-0_seed-1_seed-2.json) |\n| All   | 1 | t2v | <sub><sup>4.8<sub>(0.2)</sub></sup></sub> | <sub><sup>16.2<sub>(0.5)</sub></sup></sub> | <sub><sup>25.0<sub>(0.7)</sub></sup></sub> | <sub><sup>43.3<sub>(4.0)</sub></sup></sub> | 183.45M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-ablation-restrict-captions/517b81c9/seed-0/2020-01-07_15-20-29/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-ablation-restrict-captions/517b81c9/seed-0/2020-01-07_15-20-29/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-ablation-restrict-captions/517b81c9/seed-0/2020-01-07_15-20-29/summary-seed-0_seed-1_seed-2.json) |\n| All   | 20 | t2v | <sub><sup>10.0<sub>(0.1)</sub></sup></sub> | <sub><sup>29.0<sub>(0.3)</sub></sup></sub> | <sub><sup>41.2<sub>(0.2)</sub></sup></sub> | <sub><sup>16.0<sub>(0.0)</sub></sup></sub> | 183.45M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce/074e6b24/seed-0/2020-01-07_15-24-30/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce/074e6b24/seed-0/2020-01-07_15-24-30/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce/074e6b24/seed-0/2020-01-07_15-24-30/summary-seed-0_seed-1_seed-2.json) |\n| RGB   | 1 | v2t | <sub><sup>3.7<sub>(0.3)</sub></sup></sub> | <sub><sup>13.5<sub>(0.6)</sub></sup></sub> | <sub><sup>20.8<sub>(0.4)</sub></sup></sub> | <sub><sup>60.0<sub>(2.0)</sub></sup></sub> | 56.7M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-ablation-restrict-captions-only-rgb/f178e88e/seed-0/2020-01-07_15-21-50/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-ablation-restrict-captions-only-rgb/f178e88e/seed-0/2020-01-07_15-21-50/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-ablation-restrict-captions-only-rgb/f178e88e/seed-0/2020-01-07_15-21-50/summary-seed-0_seed-1_seed-2.json) |\n| RGB   | 20 | v2t  | <sub><sup>6.9<sub>(0.6)</sub></sup></sub> | <sub><sup>21.0<sub>(0.3)</sub></sup></sub> | <sub><sup>31.3<sub>(0.3)</sub></sup></sub> | <sub><sup>30.0<sub>(1.7)</sub></sup></sub> | 58.05M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-rgb/ee84d270/seed-0/2019-12-20_14-41-56/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-only-rgb/ee84d270/seed-0/2019-12-20_14-41-56/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-only-rgb/ee84d270/seed-0/2019-12-20_14-41-56/summary-seed-0_seed-1_seed-2.json) |\n| All   | 1 | v2t | <sub><sup>8.4<sub>(0.5)</sub></sup></sub> | <sub><sup>25.6<sub>(0.7)</sub></sup></sub> | <sub><sup>37.1<sub>(0.2)</sub></sup></sub> | <sub><sup>20.3<sub>(0.6)</sub></sup></sub> | 183.45M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-ablation-restrict-captions/517b81c9/seed-0/2020-01-07_15-20-29/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce-ablation-restrict-captions/517b81c9/seed-0/2020-01-07_15-20-29/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce-ablation-restrict-captions/517b81c9/seed-0/2020-01-07_15-20-29/summary-seed-0_seed-1_seed-2.json) |\n| All   | 20 | v2t | <sub><sup>15.6<sub>(0.3)</sub></sup></sub> | <sub><sup>40.9<sub>(1.4)</sub></sup></sub> | <sub><sup>55.2<sub>(1.0)</sub></sup></sub> | <sub><sup>8.3<sub>(0.6)</sub></sup></sub> | 183.45M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce/074e6b24/seed-0/2020-01-07_15-24-30/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/msrvtt-train-full-ce/074e6b24/seed-0/2020-01-07_15-24-30/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/msrvtt-train-full-ce/074e6b24/seed-0/2020-01-07_15-24-30/summary-seed-0_seed-1_seed-2.json) |\n\nSimilar ablation studies for the remaining datasets can be found [here](misc/ablations.md).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "**Importance of the model**:\n\n| Model | Task | R@1 | R@5 | R@10 | R@50 | MdR | MnR | Geom | params | Links |\n| ----- | ---- | --- | --- | ---- | ---- | --- | --- | ----- | -- | -- |\n| HowTo100m S3D | t2v  | <sub><sup>13.5<sub>(0.0)</sub></sup></sub> | <sub><sup>27.5<sub>(0.0)</sub></sup></sub> | <sub><sup>34.5<sub>(0.0)</sub></sup></sub> | <sub><sup>57.0<sub>(0.0)</sub></sup></sub> | <sub><sup>35.0<sub>(0.0)</sub></sup></sub> | <sub><sup>72.5<sub>(0.0)</sub></sup></sub> | <sub><sup>23.4<sub>(0.0)</sub></sup></sub> | 1 | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/queryd-train-full-mnnet/4f82d8b3/seed-0/2020-10-21_16-32-55/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/queryd-train-full-mnnet/4f82d8b3/seed-0/2020-10-21_16-32-55/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/queryd-train-full-mnnet/4f82d8b3/seed-0/2020-10-21_16-32-55/summary-seed-0_seed-1_seed-2.json) |\n| CE - P,CG | t2v  | <sub><sup>11.6<sub>(1.3)</sub></sup></sub> | <sub><sup>30.2<sub>(3.0)</sub></sup></sub> | <sub><sup>43.2<sub>(3.1)</sub></sup></sub> | <sub><sup>74.8<sub>(1.7)</sub></sup></sub> | <sub><sup>14.2<sub>(1.6)</sub></sup></sub> | <sub><sup>42.7<sub>(2.6)</sub></sup></sub> | <sub><sup>24.7<sub>(1.9)</sub></sup></sub> | 57.75M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/queryd-train-full-moee/d75cfe35/seed-0/2020-10-21_16-33-40/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/queryd-train-full-moee/d75cfe35/seed-0/2020-10-21_16-33-40/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/queryd-train-full-moee/d75cfe35/seed-0/2020-10-21_16-33-40/summary-seed-0_seed-1_seed-2.json) |\n| CE    | t2v  | <sub><sup>13.9<sub>(0.8)</sub></sup></sub> | <sub><sup>37.6<sub>(1.2)</sub></sup></sub> | <sub><sup>48.3<sub>(1.4)</sub></sup></sub> | <sub><sup>78.8<sub>(0.7)</sub></sup></sub> | <sub><sup>11.3<sub>(0.6)</sub></sup></sub> | <sub><sup>35.1<sub>(1.6)</sub></sup></sub> | <sub><sup>29.3<sub>(0.8)</sub></sup></sub> | 30.82M |[config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/queryd-train-full-ce/bf393a74/seed-0/2020-10-21_16-29-49/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/queryd-train-full-ce/bf393a74/seed-0/2020-10-21_16-29-49/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/queryd-train-full-ce/bf393a74/seed-0/2020-10-21_16-29-49/summary-seed-0_seed-1_seed-2.json) |\n| HowTo100m S3D | v2t  | <sub><sup>12.4<sub>(0.0)</sub></sup></sub> | <sub><sup>23.8<sub>(0.0)</sub></sup></sub> | <sub><sup>30.8<sub>(0.0)</sub></sup></sub> | <sub><sup>57.0<sub>(0.0)</sub></sup></sub> | <sub><sup>33.0<sub>(0.0)</sub></sup></sub> | <sub><sup>73.4<sub>(0.0)</sub></sup></sub> | <sub><sup>20.9<sub>(0.0)</sub></sup></sub> | 1 | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/queryd-train-full-mnnet/4f82d8b3/seed-0/2020-10-21_16-32-55/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/queryd-train-full-mnnet/4f82d8b3/seed-0/2020-10-21_16-32-55/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/queryd-train-full-mnnet/4f82d8b3/seed-0/2020-10-21_16-32-55/summary-seed-0_seed-1_seed-2.json) |\n| CE - P,CG | v2t  | <sub><sup>13.0<sub>(3.1)</sub></sup></sub> | <sub><sup>30.9<sub>(2.0)</sub></sup></sub> | <sub><sup>43.0<sub>(2.8)</sub></sup></sub> | <sub><sup>73.2<sub>(0.1)</sub></sup></sub> | <sub><sup>14.5<sub>(1.8)</sub></sup></sub> | <sub><sup>42.6<sub>(1.5)</sub></sup></sub> | <sub><sup>25.7<sub>(2.3)</sub></sup></sub> | 57.75M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/queryd-train-full-moee/d75cfe35/seed-0/2020-10-21_16-33-40/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/queryd-train-full-moee/d75cfe35/seed-0/2020-10-21_16-33-40/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/queryd-train-full-moee/d75cfe35/seed-0/2020-10-21_16-33-40/summary-seed-0_seed-1_seed-2.json) |\n| CE    | v2t  | <sub><sup>13.7<sub>(0.7)</sub></sup></sub> | <sub><sup>35.2<sub>(2.7)</sub></sup></sub> | <sub><sup>46.9<sub>(3.2)</sub></sup></sub> | <sub><sup>78.3<sub>(2.8)</sub></sup></sub> | <sub><sup>12.3<sub>(1.5)</sub></sup></sub> | <sub><sup>35.8<sub>(2.4)</sub></sup></sub> | <sub><sup>28.3<sub>(1.5)</sub></sup></sub> | 30.82M |[config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/queryd-train-full-ce/bf393a74/seed-0/2020-10-21_16-29-49/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/queryd-train-full-ce/bf393a74/seed-0/2020-10-21_16-29-49/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/queryd-train-full-ce/bf393a74/seed-0/2020-10-21_16-29-49/summary-seed-0_seed-1_seed-2.json) |\n\nThe influence of different pretrained experts for the performance of the CE model trained on QuerYD is studied. The value and cumulative effect of different experts for scene clas-sification (SCENE), ambient sound classification (AUDIO),image classification (OBJECT), and action recognition (ACTION) are presented. PREV. denotes the experts used in the previous row.\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "We conduct several ablation studies to investigate the importance of different components in the Collaborative Experts design.  Each ablation is conducted on the QuerYD dataset. \n\n\n| Experts | Task | R@1 | R@5 | R@10 | R@50 | MdR | MnR | Geom | params | Links |\n| ----- | ---- | --- | --- | ---- | ---- | --- | --- | ----- | -- | -- |\n| Scene    | t2v  | <sub><sup>8.7<sub>(0.4)</sub></sup></sub> | <sub><sup>26.3<sub>(1.1)</sub></sup></sub> | <sub><sup>37.1<sub>(0.7)</sub></sup></sub> | <sub><sup>68.5<sub>(2.2)</sub></sup></sub> | <sub><sup>22.2<sub>(1.6)</sub></sup></sub> | <sub><sup>52.3<sub>(3.0)</sub></sup></sub> | <sub><sup>20.4<sub>(0.1)</sub></sup></sub> | 7.51M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/queryd-train-full-ce-only-scene/7a747dc4/seed-0/2020-10-21_16-41-01/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/queryd-train-full-ce-only-scene/7a747dc4/seed-0/2020-10-21_16-41-01/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/queryd-train-full-ce-only-scene/7a747dc4/seed-0/2020-10-21_16-41-01/summary-seed-0_seed-1_seed-2.json) |\n| Scene + Inst. | t2v  | <sub><sup>11.7<sub>(1.4)</sub></sup></sub> | <sub><sup>31.6<sub>(0.9)</sub></sup></sub> | <sub><sup>43.4<sub>(1.3)</sub></sup></sub> | <sub><sup>74.5<sub>(0.9)</sub></sup></sub> | <sub><sup>14.0<sub>(1.0)</sub></sup></sub> | <sub><sup>41.1<sub>(2.1)</sub></sup></sub> | <sub><sup>25.2<sub>(0.8)</sub></sup></sub> | 17.25M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/queryd-train-full-ce-only-scene-inst/bb0be767/seed-0/2020-10-21_16-36-00/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/queryd-train-full-ce-only-scene-inst/bb0be767/seed-0/2020-10-21_16-36-00/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/queryd-train-full-ce-only-scene-inst/bb0be767/seed-0/2020-10-21_16-36-00/summary-seed-0_seed-1_seed-2.json) |\n| Scene + r2p1d | t2v  | <sub><sup>11.7<sub>(2.1)</sub></sup></sub> | <sub><sup>32.1<sub>(3.0)</sub></sup></sub> | <sub><sup>45.3<sub>(3.3)</sub></sup></sub> | <sub><sup>74.6<sub>(0.4)</sub></sup></sub> | <sub><sup>13.7<sub>(1.9)</sub></sup></sub> | <sub><sup>42.9<sub>(2.2)</sub></sup></sub> | <sub><sup>25.7<sub>(2.4)</sub></sup></sub> | 16.07M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/queryd-train-full-ce-only-scene-r2p1d/86e43b8b/seed-0/2020-10-21_16-38-35/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/queryd-train-full-ce-only-scene-r2p1d/86e43b8b/seed-0/2020-10-21_16-38-35/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/queryd-train-full-ce-only-scene-r2p1d/86e43b8b/seed-0/2020-10-21_16-38-35/summary-seed-0_seed-1_seed-2.json) |\n| Scene + Audio | t2v  | <sub><sup>7.6<sub>(2.7)</sub></sup></sub> | <sub><sup>27.4<sub>(1.4)</sub></sup></sub> | <sub><sup>40.4<sub>(0.9)</sub></sup></sub> | <sub><sup>69.1<sub>(0.9)</sub></sup></sub> | <sub><sup>17.0<sub>(1.7)</sub></sup></sub> | <sub><sup>49.0<sub>(1.9)</sub></sup></sub> | <sub><sup>20.2<sub>(2.3)</sub></sup></sub> | 17.25M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/queryd-train-full-ce-only-scene-audio/c87311b1/seed-0/2020-10-21_16-32-58/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/queryd-train-full-ce-only-scene-audio/c87311b1/seed-0/2020-10-21_16-32-58/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/queryd-train-full-ce-only-scene-audio/c87311b1/seed-0/2020-10-21_16-32-58/summary-seed-0_seed-1_seed-2.json) |\n| Scene    | v2t  | <sub><sup>9.1<sub>(0.8)</sub></sup></sub> | <sub><sup>25.4<sub>(0.9)</sub></sup></sub> | <sub><sup>35.3<sub>(1.5)</sub></sup></sub> | <sub><sup>68.2<sub>(2.2)</sub></sup></sub> | <sub><sup>23.2<sub>(0.3)</sub></sup></sub> | <sub><sup>52.6<sub>(2.6)</sub></sup></sub> | <sub><sup>20.1<sub>(0.5)</sub></sup></sub> | 7.51M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/queryd-train-full-ce-only-scene/7a747dc4/seed-0/2020-10-21_16-41-01/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/queryd-train-full-ce-only-scene/7a747dc4/seed-0/2020-10-21_16-41-01/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/queryd-train-full-ce-only-scene/7a747dc4/seed-0/2020-10-21_16-41-01/summary-seed-0_seed-1_seed-2.json) |\n| Scene + Inst. | v2t  | <sub><sup>11.9<sub>(0.5)</sub></sup></sub> | <sub><sup>31.0<sub>(3.6)</sub></sup></sub> | <sub><sup>43.5<sub>(2.7)</sub></sup></sub> | <sub><sup>74.8<sub>(1.8)</sub></sup></sub> | <sub><sup>14.5<sub>(0.9)</sub></sup></sub> | <sub><sup>40.8<sub>(2.1)</sub></sup></sub> | <sub><sup>25.2<sub>(1.1)</sub></sup></sub> | 17.25M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/queryd-train-full-ce-only-scene-inst/bb0be767/seed-0/2020-10-21_16-36-00/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/queryd-train-full-ce-only-scene-inst/bb0be767/seed-0/2020-10-21_16-36-00/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/queryd-train-full-ce-only-scene-inst/bb0be767/seed-0/2020-10-21_16-36-00/summary-seed-0_seed-1_seed-2.json) |\n| Scene + r2p1d | v2t  | <sub><sup>12.7<sub>(1.4)</sub></sup></sub> | <sub><sup>30.9<sub>(2.8)</sub></sup></sub> | <sub><sup>44.0<sub>(1.8)</sub></sup></sub> | <sub><sup>74.3<sub>(1.2)</sub></sup></sub> | <sub><sup>14.3<sub>(1.2)</sub></sup></sub> | <sub><sup>42.8<sub>(1.7)</sub></sup></sub> | <sub><sup>25.8<sub>(1.7)</sub></sup></sub> | 16.07M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/queryd-train-full-ce-only-scene-r2p1d/86e43b8b/seed-0/2020-10-21_16-38-35/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/queryd-train-full-ce-only-scene-r2p1d/86e43b8b/seed-0/2020-10-21_16-38-35/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/queryd-train-full-ce-only-scene-r2p1d/86e43b8b/seed-0/2020-10-21_16-38-35/summary-seed-0_seed-1_seed-2.json) |\n| Scene + Audio | v2t  | <sub><sup>10.1<sub>(1.2)</sub></sup></sub> | <sub><sup>25.7<sub>(1.5)</sub></sup></sub> | <sub><sup>37.5<sub>(1.2)</sub></sup></sub> | <sub><sup>69.8<sub>(1.6)</sub></sup></sub> | <sub><sup>20.0<sub>(1.3)</sub></sup></sub> | <sub><sup>48.9<sub>(2.0)</sub></sup></sub> | <sub><sup>21.3<sub>(1.1)</sub></sup></sub> | 17.25M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/queryd-train-full-ce-only-scene-audio/c87311b1/seed-0/2020-10-21_16-32-58/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/queryd-train-full-ce-only-scene-audio/c87311b1/seed-0/2020-10-21_16-32-58/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/queryd-train-full-ce-only-scene-audio/c87311b1/seed-0/2020-10-21_16-32-58/summary-seed-0_seed-1_seed-2.json) |\n\nWe can also study their cumulative effect:\n\n| Experts | Task | R@1 | R@5 | R@10 | R@50 | MdR | MnR | Geom | params | Links |\n| ----- | ---- | --- | --- | ---- | ---- | --- | --- | ----- | -- | -- |\n| Scene    | t2v  | <sub><sup>8.7<sub>(0.4)</sub></sup></sub> | <sub><sup>26.3<sub>(1.1)</sub></sup></sub> | <sub><sup>37.1<sub>(0.7)</sub></sup></sub> | <sub><sup>68.5<sub>(2.2)</sub></sup></sub> | <sub><sup>22.2<sub>(1.6)</sub></sup></sub> | <sub><sup>52.3<sub>(3.0)</sub></sup></sub> | <sub><sup>20.4<sub>(0.1)</sub></sup></sub> | 7.51M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/queryd-train-full-ce-only-scene/7a747dc4/seed-0/2020-10-21_16-41-01/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/queryd-train-full-ce-only-scene/7a747dc4/seed-0/2020-10-21_16-41-01/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/queryd-train-full-ce-only-scene/7a747dc4/seed-0/2020-10-21_16-41-01/summary-seed-0_seed-1_seed-2.json) |\n| Prev. + Audio    | t2v  | <sub><sup>7.6<sub>(2.7)</sub></sup></sub> | <sub><sup>27.4<sub>(1.4)</sub></sup></sub> | <sub><sup>40.4<sub>(0.9)</sub></sup></sub> | <sub><sup>69.1<sub>(0.9)</sub></sup></sub> | <sub><sup>17.0<sub>(1.7)</sub></sup></sub> | <sub><sup>49.0<sub>(1.9)</sub></sup></sub> | <sub><sup>20.2<sub>(2.3)</sub></sup></sub> | 17.25M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/queryd-train-full-ce-only-scene-audio/c87311b1/seed-0/2020-10-21_16-32-58/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/queryd-train-full-ce-only-scene-audio/c87311b1/seed-0/2020-10-21_16-32-58/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/queryd-train-full-ce-only-scene-audio/c87311b1/seed-0/2020-10-21_16-32-58/summary-seed-0_seed-1_seed-2.json) |\n| Prev. + Inst    | t2v  | <sub><sup>12.7<sub>(1.7)</sub></sup></sub> | <sub><sup>34.8<sub>(1.7)</sub></sup></sub> | <sub><sup>47.0<sub>(1.3)</sub></sup></sub> | <sub><sup>78.0<sub>(1.0)</sub></sup></sub> | <sub><sup>12.3<sub>(0.6)</sub></sup></sub> | <sub><sup>37.6<sub>(2.1)</sub></sup></sub> | <sub><sup>27.5<sub>(1.5)</sub></sup></sub> | 24.63M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/queryd-train-full-ce-only-scene-audio-inst/f302d191/seed-0/2020-10-21_16-29-36/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/queryd-train-full-ce-only-scene-audio-inst/f302d191/seed-0/2020-10-21_16-29-36/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/queryd-train-full-ce-only-scene-audio-inst/f302d191/seed-0/2020-10-21_16-29-36/summary-seed-0_seed-1_seed-2.json) |\n| Prev. + R2P1D    | t2v  | <sub><sup>14.3<sub>(0.3)</sub></sup></sub> | <sub><sup>37.5<sub>(1.3)</sub></sup></sub> | <sub><sup>48.6<sub>(0.8)</sub></sup></sub> | <sub><sup>78.8<sub>(0.3)</sub></sup></sub> | <sub><sup>11.3<sub>(0.6)</sub></sup></sub> | <sub><sup>35.2<sub>(1.8)</sub></sup></sub> | <sub><sup>29.7<sub>(0.3)</sub></sup></sub> | 30.82M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/queryd-train-full-ce-only-scene-audio-inst-r2p1d/70feaf3c/seed-0/2020-10-21_16-25-50/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/queryd-train-full-ce-only-scene-audio-inst-r2p1d/70feaf3c/seed-0/2020-10-21_16-25-50/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/queryd-train-full-ce-only-scene-audio-inst-r2p1d/70feaf3c/seed-0/2020-10-21_16-25-50/summary-seed-0_seed-1_seed-2.json) |\n| Scene    | v2t  | <sub><sup>9.1<sub>(0.8)</sub></sup></sub> | <sub><sup>25.4<sub>(0.9)</sub></sup></sub> | <sub><sup>35.3<sub>(1.5)</sub></sup></sub> | <sub><sup>68.2<sub>(2.2)</sub></sup></sub> | <sub><sup>23.2<sub>(0.3)</sub></sup></sub> | <sub><sup>52.6<sub>(2.6)</sub></sup></sub> | <sub><sup>20.1<sub>(0.5)</sub></sup></sub> | 7.51M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/queryd-train-full-ce-only-scene/7a747dc4/seed-0/2020-10-21_16-41-01/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/queryd-train-full-ce-only-scene/7a747dc4/seed-0/2020-10-21_16-41-01/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/queryd-train-full-ce-only-scene/7a747dc4/seed-0/2020-10-21_16-41-01/summary-seed-0_seed-1_seed-2.json) |\n| Prev. + Audio    | v2t  | <sub><sup>10.1<sub>(1.2)</sub></sup></sub> | <sub><sup>25.7<sub>(1.5)</sub></sup></sub> | <sub><sup>37.5<sub>(1.2)</sub></sup></sub> | <sub><sup>69.8<sub>(1.6)</sub></sup></sub> | <sub><sup>20.0<sub>(1.3)</sub></sup></sub> | <sub><sup>48.9<sub>(2.0)</sub></sup></sub> | <sub><sup>21.3<sub>(1.1)</sub></sup></sub> | 17.25M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/queryd-train-full-ce-only-scene-audio/c87311b1/seed-0/2020-10-21_16-32-58/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/queryd-train-full-ce-only-scene-audio/c87311b1/seed-0/2020-10-21_16-32-58/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/queryd-train-full-ce-only-scene-audio/c87311b1/seed-0/2020-10-21_16-32-58/summary-seed-0_seed-1_seed-2.json) |\n| Prev. + Inst.    | v2t  | <sub><sup>12.8<sub>(1.3)</sub></sup></sub> | <sub><sup>33.5<sub>(2.8)</sub></sup></sub> | <sub><sup>46.6<sub>(1.0)</sub></sup></sub> | <sub><sup>76.7<sub>(1.7)</sub></sup></sub> | <sub><sup>11.8<sub>(0.8)</sub></sup></sub> | <sub><sup>37.6<sub>(1.9)</sub></sup></sub> | <sub><sup>27.1<sub>(0.6)</sub></sup></sub> | 24.63M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/queryd-train-full-ce-only-scene-audio-inst/f302d191/seed-0/2020-10-21_16-29-36/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/queryd-train-full-ce-only-scene-audio-inst/f302d191/seed-0/2020-10-21_16-29-36/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/queryd-train-full-ce-only-scene-audio-inst/f302d191/seed-0/2020-10-21_16-29-36/summary-seed-0_seed-1_seed-2.json) |\n| Prev. + R2P1D    | v2t  | <sub><sup>14.0<sub>(0.3)</sub></sup></sub> | <sub><sup>35.4<sub>(2.9)</sub></sup></sub> | <sub><sup>47.2<sub>(2.8)</sub></sup></sub> | <sub><sup>78.7<sub>(2.4)</sub></sup></sub> | <sub><sup>12.3<sub>(1.5)</sub></sup></sub> | <sub><sup>35.8<sub>(2.4)</sub></sup></sub> | <sub><sup>28.6<sub>(1.2)</sub></sup></sub> | 30.82M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/queryd-train-full-ce-only-scene-audio-inst-r2p1d/70feaf3c/seed-0/2020-10-21_16-25-50/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/queryd-train-full-ce-only-scene-audio-inst-r2p1d/70feaf3c/seed-0/2020-10-21_16-25-50/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/queryd-train-full-ce-only-scene-audio-inst-r2p1d/70feaf3c/seed-0/2020-10-21_16-25-50/summary-seed-0_seed-1_seed-2.json) |\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "**Importance of the model**:\n\n| Model | Task | R@1 | R@5 | R@10 | R@50 | MdR | MnR | Geom | params | Links |\n| ----- | ---- | --- | --- | ---- | ---- | --- | --- | ----- | -- | -- |\n| HowTo100m S3D | t2v  | <sub><sup>6.7<sub>(0.0)</sub></sup></sub> | <sub><sup>14.7<sub>(0.0)</sub></sup></sub> | <sub><sup>20.4<sub>(0.0)</sub></sup></sub> | <sub><sup>36.6<sub>(0.0)</sub></sup></sub> | <sub><sup>133.0<sub>(0.0)</sub></sup></sub> | <sub><sup>342.0<sub>(0.0)</sub></sup></sub> | <sub><sup>12.6<sub>(0.0)</sub></sup></sub> | 1 | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/querydsegments-train-full-mnnet/84b0801c/seed-1/2020-10-21_18-28-28/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/querydsegments-train-full-mnnet/84b0801c/seed-1/2020-10-21_18-28-28/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/querydsegments-train-full-mnnet/84b0801c/seed-1/2020-10-21_18-28-28/summary-seed-1_seed-2_seed-3.json) |\n| CE - P,CG | t2v  | <sub><sup>19.0<sub>(0.8)</sub></sup></sub> | <sub><sup>38.9<sub>(1.0)</sub></sup></sub> | <sub><sup>47.9<sub>(0.7)</sub></sup></sub> | <sub><sup>68.0<sub>(0.4)</sub></sup></sub> | <sub><sup>12.0<sub>(1.0)</sub></sup></sub> | <sub><sup>127.4<sub>(5.9)</sub></sup></sub> | <sub><sup>32.8<sub>(0.6)</sub></sup></sub> | 57.75M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/querydsegments-train-full-moee/87a1436a/seed-1/2020-10-21_18-29-07/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/querydsegments-train-full-moee/87a1436a/seed-1/2020-10-21_18-29-07/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/querydsegments-train-full-moee/87a1436a/seed-1/2020-10-21_18-29-07/summary-seed-1_seed-2_seed-3.json) |\n| CE    | t2v  | <sub><sup>18.2<sub>(0.5)</sub></sup></sub> | <sub><sup>38.1<sub>(0.8)</sub></sup></sub> | <sub><sup>46.8<sub>(0.4)</sub></sup></sub> | <sub><sup>67.3<sub>(0.7)</sub></sup></sub> | <sub><sup>13.3<sub>(0.6)</sub></sup></sub> | <sub><sup>127.5<sub>(3.9)</sub></sup></sub> | <sub><sup>31.9<sub>(0.4)</sub></sup></sub> | 30.82M |[config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/querydsegments-train-full-ce/d7737672/seed-1/2020-10-21_18-19-39/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/querydsegments-train-full-ce/d7737672/seed-1/2020-10-21_18-19-39/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/querydsegments-train-full-ce/d7737672/seed-1/2020-10-21_18-19-39/summary-seed-1_seed-2_seed-3.json) |\n| HowTo100m S3D | v2t  | <sub><sup>8.4<sub>(0.0)</sub></sup></sub> | <sub><sup>15.4<sub>(0.0)</sub></sup></sub> | <sub><sup>19.8<sub>(0.0)</sub></sup></sub> | <sub><sup>34.2<sub>(0.0)</sub></sup></sub> | <sub><sup>154.5<sub>(0.0)</sub></sup></sub> | <sub><sup>363.0<sub>(0.0)</sub></sup></sub> | <sub><sup>13.7<sub>(0.0)</sub></sup></sub> | 1 | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/querydsegments-train-full-mnnet/84b0801c/seed-1/2020-10-21_18-28-28/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/querydsegments-train-full-mnnet/84b0801c/seed-1/2020-10-21_18-28-28/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/querydsegments-train-full-mnnet/84b0801c/seed-1/2020-10-21_18-28-28/summary-seed-1_seed-2_seed-3.json) |\n| CE - P,CG | v2t  | <sub><sup>19.8<sub>(0.2)</sub></sup></sub> | <sub><sup>39.6<sub>(0.6)</sub></sup></sub> | <sub><sup>47.6<sub>(0.1)</sub></sup></sub> | <sub><sup>67.9<sub>(0.5)</sub></sup></sub> | <sub><sup>13.0<sub>(0.0)</sub></sup></sub> | <sub><sup>124.3<sub>(5.5)</sub></sup></sub> | <sub><sup>33.4<sub>(0.2)</sub></sup></sub> | 57.75M | [config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/querydsegments-train-full-moee/87a1436a/seed-1/2020-10-21_18-29-07/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/querydsegments-train-full-moee/87a1436a/seed-1/2020-10-21_18-29-07/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/querydsegments-train-full-moee/87a1436a/seed-1/2020-10-21_18-29-07/summary-seed-1_seed-2_seed-3.json) |\n| CE    | v2t  | <sub><sup>18.1<sub>(0.6)</sub></sup></sub> | <sub><sup>37.3<sub>(0.5)</sub></sup></sub> | <sub><sup>45.9<sub>(0.6)</sub></sup></sub> | <sub><sup>67.2<sub>(0.2)</sub></sup></sub> | <sub><sup>14.0<sub>(1.0)</sub></sup></sub> | <sub><sup>123.9<sub>(3.3)</sub></sup></sub> | <sub><sup>31.4<sub>(0.4)</sub></sup></sub> | 30.82M |[config](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/querydsegments-train-full-ce/d7737672/seed-1/2020-10-21_18-19-39/config.json), [model](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/models/querydsegments-train-full-ce/d7737672/seed-1/2020-10-21_18-19-39/trained_model.pth), [log](http:/www.robots.ox.ac.uk/~vgg/research/collaborative-experts/data/log/querydsegments-train-full-ce/d7737672/seed-1/2020-10-21_18-19-39/summary-seed-1_seed-2_seed-3.json) |\n\n",
      "technique": "Header extraction"
    }
  ],
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "video-retrieval",
      "deep-neural-networks"
    ],
    "technique": "GitHub API"
  }
}