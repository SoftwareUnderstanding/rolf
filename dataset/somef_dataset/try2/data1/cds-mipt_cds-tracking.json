{
  "citation": [
    {
      "confidence": [
        0.9746139683026899,
        0.9997089747468236,
        0.9904682582335301
      ],
      "excerpt": "A Simple Baseline for Multi-Object Tracking,           \nYifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng, Wenyu Liu,       \narXiv technical report (arXiv 2004.01888) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/cds-mipt/cds-tracking",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-07-20T14:32:37Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-08-18T09:10:19Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "\u041c\u044b \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0442\u0435 \u0436\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f, \u043a\u0430\u043a \u0438 JDE. \u041f\u043e\u0436\u0430\u043b\u0443\u0439\u0441\u0442\u0430, \u043e\u0431\u0440\u0430\u0442\u0438\u0442\u0435\u0441\u044c \u043a \u0438\u0445 DATA ZOO \u0447\u0442\u043e\u0431\u044b \u0437\u0430\u0433\u0440\u0443\u0437\u0438\u0442\u044c \u0438 \u043f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u0438\u0442\u044c \u0432\u0441\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f, \u0432\u043a\u043b\u044e\u0447\u0430\u044f Caltech Pedestrian, CityPersons, CUHK-SYSU, PRW, ETHZ, MOT17 \u0438 MOT16. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8144965960308795,
        0.860059181823877
      ],
      "excerpt": "HRNetV2 ImageNet pretrained model: HRNetV2-W18 official, HRNetV2-W32 official. \n\u041f\u043e\u0441\u043b\u0435 \u0437\u0430\u0433\u0440\u0443\u0437\u043a\u0438 \u0432\u044b \u0434\u043e\u043b\u0436\u043d\u044b \u043f\u043e\u043c\u0435\u0441\u0442\u0438\u0442\u044c Pretrained model \u0432 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0443\u044e \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0443: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8725110104722592
      ],
      "excerpt": "Results of the test set all need to be evaluated on the MOT challenge server. You can see the tracking results on the training set by setting --val_motxx True and run the tracking code. We set 'conf_thres' 0.4 for MOT16 and MOT17. We set 'conf_thres' 0.3 for 2DMOT15 and MOT20. You can also use the SOTA MOT20 pretrained model here [Google]: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "One-shot multi-object tracking with Bird eye view visualization",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/cds-mipt/cds-tracking/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Fri, 24 Dec 2021 19:22:36 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/cds-mipt/cds-tracking/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "cds-mipt/cds-tracking",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/cds-mipt/cds-tracking/master/build/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/cds-mipt/cds-tracking/master/MOT15_dataset_down.sh",
      "https://raw.githubusercontent.com/cds-mipt/cds-tracking/master/model_down_script.sh",
      "https://raw.githubusercontent.com/cds-mipt/cds-tracking/master/experiments/all_hrnet.sh",
      "https://raw.githubusercontent.com/cds-mipt/cds-tracking/master/experiments/ft_mot15_dla34.sh",
      "https://raw.githubusercontent.com/cds-mipt/cds-tracking/master/experiments/all_dla34.sh",
      "https://raw.githubusercontent.com/cds-mipt/cds-tracking/master/experiments/all_res50.sh",
      "https://raw.githubusercontent.com/cds-mipt/cds-tracking/master/experiments/ft_mot20_dla34.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "| Dataset    |  MOTA | IDF1 | IDS | MT | ML | FPS |\n|--------------|-----------|--------|-------|----------|----------|--------|\n|2DMOT15  | 59.0 | 62.2 |  582 | 45.6% | 11.5% | 30.5 |\n|MOT16       | 68.7 | 70.4 | 953 | 39.5% | 19.0% | 25.9 |\n|MOT17       | 67.5 | 69.8 | 2868 | 37.7% | 20.8% | 25.9 |\n|MOT20       | 58.7 | 63.7 | 6013 | 66.3% | 8.5% | 13.2 |\n\n All of the results are obtained on the [MOT challenge](https://motchallenge.net) evaluation server under the \u201cprivate detector\u201d protocol. We rank first among all the trackers on 2DMOT15, MOT17 and the recently released (2020.02.29) MOT20. Note that our IDF1 score remarkably outperforms other one-shot MOT trackers by more than **10 points**. The tracking speed of the entire system can reach up to **30 FPS**.\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "<img src=\"assets/MOT15.gif\" width=\"400\"/>   <img src=\"assets/MOT16.gif\" width=\"400\"/>\n<img src=\"assets/MOT17.gif\" width=\"400\"/>   <img src=\"assets/MOT20.gif\" width=\"400\"/>\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9708119013170028,
        0.9012786611276753,
        0.9770335174395833,
        0.9865441524656512,
        0.9906248903846466,
        0.9979947896609701,
        0.9683757157018965
      ],
      "excerpt": "\u0423\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u0442\u0435 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438. \u041f\u0440\u0438 \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043b\u0441\u044f Python 3.7 \u0438 pytorch >= 1.2.0. \nconda create -n FairMOT \nconda activate FairMOT \nconda install pytorch==1.2.0 torchvision==0.4.0 cudatoolkit=10.0 -c pytorch \ncd ${FAIRMOT_ROOT} \npip install -r requirements.txt \ncd src/lib/models/networks/DCNv2_new sh make.sh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9452797457628369
      ],
      "excerpt": "cd src \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9164640256136145
      ],
      "excerpt": "sh experiments/all_dla34.sh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9452797457628369
      ],
      "excerpt": "cd src \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9452797457628369
      ],
      "excerpt": "cd src \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9452797457628369
      ],
      "excerpt": "cd src \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9164640256136145,
        0.9164640256136145
      ],
      "excerpt": "sh experiments/ft_mot15_dla34.sh \nsh experiments/ft_mot20_dla34.sh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9452797457628369
      ],
      "excerpt": "cd src \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8589534893990137,
        0.8633989807152664
      ],
      "excerpt": "   |        \u2514\u2014\u2014\u2014\u2014\u2014\u2014train \n   |        \u2514\u2014\u2014\u2014\u2014\u2014\u2014test \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": "            \u2514\u2014\u2014\u2014\u2014\u2014\u2014train(empty) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137,
        0.8633989807152664
      ],
      "excerpt": "   |        \u2514\u2014\u2014\u2014\u2014\u2014\u2014train \n   |        \u2514\u2014\u2014\u2014\u2014\u2014\u2014test \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137,
        0.9664150841647992,
        0.8629400568140789,
        0.9246227682586091,
        0.9246227682586091
      ],
      "excerpt": "            \u2514\u2014\u2014\u2014\u2014\u2014\u2014train(empty) \n\u0417\u0430\u0442\u0435\u043c \u0432\u044b \u043c\u043e\u0436\u0435\u0442\u0435 \u0438\u0437\u043c\u0435\u043d\u0438\u0442\u044c seq_root \u0438 label_root \u0432 src/gen_labels_15.py \u0438 \u0435\u0449\u0435 src/gen_labels_20.py \u0438 \u0437\u0430\u043f\u0443\u0441\u0442\u0438\u0442\u0435: \ncd src \npython gen_labels_15.py \npython gen_labels_20.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8245539886860519
      ],
      "excerpt": "Pretrained models \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8282633617843045
      ],
      "excerpt": "\u041f\u043e\u0441\u043b\u0435 \u0437\u0430\u0433\u0440\u0443\u0437\u043a\u0438 \u0432\u044b \u0434\u043e\u043b\u0436\u043d\u044b \u043f\u043e\u043c\u0435\u0441\u0442\u0438\u0442\u044c Pretrained model \u0432 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0443\u044e \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0443: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8812180368472974
      ],
      "excerpt": "\u0418\u0437\u043c\u0435\u043d\u0438\u0442\u0435 \u0434\u0435\u0440\u0438\u043a\u0442\u043e\u0440\u0438\u0439 \u043d\u0430\u0431\u043e\u0440\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 'root' \u0432 src/lib/cfg/data.json \u0438 'data_dir' \u0432 src/lib/opts.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8629400568140789,
        0.9300244143002546,
        0.971162495160033
      ],
      "excerpt": "cd src \npython track.py mot --load_model ../models/all_dla34.pth --conf_thres 0.6 \n\u0447\u0442\u043e\u0431\u044b \u0443\u0432\u0438\u0434\u0435\u0442\u044c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u043e\u0442\u0441\u043b\u0435\u0436\u0438\u0432\u0430\u043d\u0438\u044f (76.1 MOTA \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c \u0431\u0430\u0437\u043e\u0432\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 DLA-34). \u0412\u044b \u0442\u0430\u043a\u0436\u0435 \u043c\u043e\u0436\u0435\u0442\u0435 \u0432\u044b\u0431\u0440\u0430\u0442\u044c save_images=True \u0432 src/track.py \u0434\u043b\u044f \u0441\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u0438\u044f \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043a\u0430\u0434\u0440\u0430.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8629400568140789,
        0.9359196317249231
      ],
      "excerpt": "cd src \npython track.py mot --load_model ../models/all_hrnet_v2_w18.pth --conf_thres 0.6 --arch hrnet_18 --reid_dim 128 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8629400568140789,
        0.9544536113645911,
        0.9544536113645911,
        0.8184796022068788
      ],
      "excerpt": "cd src \npython track.py mot --test_mot17 True --load_model ../models/all_dla34.pth --conf_thres 0.4 \npython track.py mot --test_mot16 True --load_model ../models/all_dla34.pth --conf_thres 0.4 \n\u0438 \u043e\u0442\u043f\u0440\u0430\u0432\u044c\u0442\u0435 txt-\u0444\u0430\u0439\u043b\u044b \u0432 MOT challenge \u043e\u0446\u0435\u043d\u043e\u0447\u043d\u044b\u0439 \u0441\u0435\u0440\u0432\u0435\u0440 \u0434\u043b\u044f \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u044f \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432. ((\u0412\u044b \u043c\u043e\u0436\u0435\u0442\u0435 \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u043e\u0431\u0449\u0438\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b 67.5 MOTA \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u043c \u043d\u0430\u0431\u043e\u0440\u0435 MOT17, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u0431\u0430\u0437\u043e\u0432\u0443\u044e \u043c\u043e\u0434\u0435\u043b\u044c 'all_dla34.pth'.) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8629400568140789,
        0.9516623759856101,
        0.9516623759856101
      ],
      "excerpt": "cd src \npython track.py mot --test_mot15 True --load_model your_mot15_model.pth --conf_thres 0.3 \npython track.py mot --test_mot20 True --load_model your_mot20_model.pth --conf_thres 0.3 --K 500 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9572001618458429
      ],
      "excerpt": "python track.py mot --test_mot20 True --load_model ../models/mot20_dla34.pth --reid_dim 128 --conf_thres 0.3 --K 500 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991
      ],
      "excerpt": "\u041f\u043e\u0441\u043b\u0435 \u0437\u0430\u043f\u0443\u0441\u043a\u0430 track_birdeye_vis.py, \u043e\u0442\u043a\u0440\u043e\u0435\u0442\u0441\u044f \u043e\u043a\u043d\u043e \u043f\u0435\u0440\u0432\u043e\u0433\u043e \u043a\u0430\u0434\u0440\u0430 \u0432 \u0432\u0438\u0434\u0435\u043e. \u0412 \u044d\u0442\u043e\u0442 \u043c\u043e\u043c\u0435\u043d\u0442 \u043a\u043e\u0434 \u043e\u0436\u0438\u0434\u0430\u0435\u0442, \u0447\u0442\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c \u043e\u0442\u043c\u0435\u0442\u0438\u0442 6 \u0442\u043e\u0447\u0435\u043a, \u0449\u0435\u043b\u043a\u043d\u0443\u0432 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0435 \u043f\u043e\u0437\u0438\u0446\u0438\u0438 \u043d\u0430 \u043a\u0430\u0434\u0440\u0435. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/cds-mipt/cds-tracking/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Dockerfile",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 YifuZhang, Youshaa Murhij\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "cds-tracking",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "cds-tracking",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "cds-mipt",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/cds-mipt/cds-tracking/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 5,
      "date": "Fri, 24 Dec 2021 19:22:36 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "tracking"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "<img src=\"assets/MOT15.gif\" width=\"400\"/>   <img src=\"assets/MOT16.gif\" width=\"400\"/>\n<img src=\"assets/MOT17.gif\" width=\"400\"/>   <img src=\"assets/MOT20.gif\" width=\"400\"/>\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "You can input a raw video and get the demo video by running src/demo.py and get the mp4 format of the demo video:\n```\ncd src\npython demo.py mot --load_model ../models/all_dla34.pth --conf_thres 0.4\n```\nYou can change --input-video and --output-root to get the demos of your own videos.\n\nIf you have difficulty building DCNv2 and thus cannot use the DLA-34 baseline model, you can run the demo with the HRNetV2_w18 baseline model (don't forget to comment lines with 'dcn' in src/libs/models/model.py if you do not build DCNv2): \n```\ncd src\npython demo.py mot --load_model ../models/all_hrnet_v2_w18.pth --arch hrnet_18 --reid_dim 128 --conf_thres 0.4\n```\n--conf_thres can be set from 0.3 to 0.7 depending on your own videos.\n\n",
      "technique": "Header extraction"
    }
  ]
}