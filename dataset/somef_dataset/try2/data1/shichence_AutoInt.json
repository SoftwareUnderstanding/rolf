{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This implementation gets inspirations from Kyubyong Park's [transformer](https://github.com/Kyubyong/transformer) and Chenglong Chen' [DeepFM](https://github.com/ChenglongChen/tensorflow-DeepFM).\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1810.11921, 2018.\n\n## Requirements: \n* **Tensorflow 1.4.0-rc1**\n* Python 3\n* CUDA 8.0+ (For GPU)\n\n## Introduction\n\nAutoInt\uff1aAn effective and efficient algorithm to\nautomatically learn high-order feature interactions for (sparse) categorical and numerical features.\n\n<div align=center>\n  <img src=\"https://github.com/shichence/AutoInt/blob/master/figures/model.png\" width = 50% height = 50% />\n</div>\nThe illustration of AutoInt. We first project all sparse features\n(both categorical and numerical features) into the low-dimensional space. Next, we feed embeddings of all fields into stacked multiple interacting layers implemented by self-attentive neural network. The output of the final interacting layer is the low-dimensional representation of learnt combinatorial features, which is further used for estimating the CTR via sigmoid function.\n\n## Usage\n### Input Format\nAutoInt requires the input data in the following format:\n* train_x: matrix with shape *(num_sample, num_field)*. train_x[s][t] is the feature value of feature field t of sample s in the dataset. The default value for categorical feature is 1.\n* train_i: matrix with shape *(num_sample, num_field)*. train_i[s][t] is the feature index of feature field t of sample s in the dataset. The maximal value of train_i is the feature size.\n* train_y: label of each sample in the dataset.\n\nIf you want to know how to preprocess the data, please refer to `./Dataprocess/Criteo/preprocess.py`\n\n### Example\nWe use four public real-world datasets(Avazu, Criteo, KDD12, MovieLens-1M) in our experiments. Since the first three datasets are super huge, they can not be fit into the memory as a whole. In our implementation, we split the whole dataset into 10 parts and we use the first file as test set and the second file as valid set. We provide the codes for preprocessing these three datasets in `./Dataprocess`. If you want to reuse these codes, you should first run `preprocess.py` to generate `train_x.txt, train_i.txt, train_y.txt` as described in `Input Format`. Then you should run `./Dataprocesss/Kfold_split/StratifiedKfold.py` to split the whole dataset into ten folds. Finally you can run `scale.py` to scale the numerical value(optional).\n\nTo help test the correctness of the code and familarize yourself with the code, we upload the first `10000` samples of `Criteo` dataset in `train_examples.txt`. And we provide the scripts for preprocessing and training.(Please refer to `\tsample_preprocess.sh` and `test_code.sh`, you may need to modify the path in `config.py` and `test_code.sh`). \n\nAfter you run the `test_code.sh`, you should get a folder named `Criteo` which contains `part*, feature_size.npy, fold_index.npy, train_*.txt`. `feature_size.npy` contains the number of total features which will be used to initialize the model. `train_*.txt` is the whole dataset. If you use other small dataset, say `MovieLens-1M`, you only need to modify the function `_run_` in `train.py`.\n\nHere's how to run the preprocessing.\n```\nmkdir Criteo\npython ./Dataprocess/Criteo/preprocess.py\npython ./Dataprocess/Kfold_split/stratifiedKfold.py\npython ./Dataprocess/Criteo/scale.py\n```\n\nHere's how to run the training.\n```\npython -u train.py \\\n                       --data \"Criteo\"  --blocks 3 --heads 2  --block_shape \"[64, 64, 64]\" \\\n                       --is_save \"True\" --save_path \"./test_code/Criteo/b3h2_64x64x64/\"  \\\n                       --field_size 39  --run_times 1 --data_path \"./\" \\\n                       --epoch 3 --has_residual \"True\"  --has_wide \"False\" \\\n                       --batch_size 1024 \\\n                       > test_code_single.out &\n```\n\nYou should see output like this:\n\n```\n...\ntrain logs\n...\nstart testing!...\nrestored from ./test_code/Criteo/b3h2_dnn_dropkeep1_400x2/1/\ntest-result = 0.8088, test-logloss = 0.4430\ntest_auc [0.8088305055534442]\ntest_log_loss [0.44297631300399626]\navg_auc 0.8088305055534442\navg_log_loss 0.44297631300399626\n```\n\n## Citation\nIf you find AutoInt useful for your research, please consider citing the following paper:\n```\n@article{weiping2018autoint,\n  title={AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks",
      "https://arxiv.org/abs/1810.11921"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find AutoInt useful for your research, please consider citing the following paper:\n```\n@article{weiping2018autoint,\n  title={AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks},\n  author={Weiping, Song and Chence, Shi and Zhiping, Xiao and Zhijian, Duan and Yewen, Xu and Ming, Zhang and Jian, Tang},\n  journal={arXiv preprint arXiv:1810.11921},\n  year={2018}\n}\n```\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{weiping2018autoint,\n  title={AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks},\n  author={Weiping, Song and Chence, Shi and Zhiping, Xiao and Zhijian, Duan and Yewen, Xu and Ming, Zhang and Jian, Tang},\n  journal={arXiv preprint arXiv:1810.11921},\n  year={2018}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9999999464098309
      ],
      "excerpt": "Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang and Jian Tang. AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks. arXiv preprint arXiv:1810.11921, 2018. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/shichence/AutoInt",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you have questions related to the code, feel free to contact Weiping Song (`songweiping@pku.edu.cn`), Chence Shi (`chenceshi@pku.edu.cn`) and Zhijian Duan (`zjduan@pku.edu.cn`).\n\n",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-11-25T07:13:25Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-10T08:00:46Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "AutoInt\uff1aAn effective and efficient algorithm to\nautomatically learn high-order feature interactions for (sparse) categorical and numerical features.\n\n<div align=center>\n  <img src=\"https://github.com/shichence/AutoInt/blob/master/figures/model.png\" width = 50% height = 50% />\n</div>\nThe illustration of AutoInt. We first project all sparse features\n(both categorical and numerical features) into the low-dimensional space. Next, we feed embeddings of all fields into stacked multiple interacting layers implemented by self-attentive neural network. The output of the final interacting layer is the low-dimensional representation of learnt combinatorial features, which is further used for estimating the CTR via sigmoid function.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9952517627004194
      ],
      "excerpt": "This is a TenforFlow implementation of AutoInt for CTR prediction task, as described in our paper: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9311577934933575,
        0.9511992449686509
      ],
      "excerpt": "* train_x: matrix with shape (num_sample, num_field). train_x[s][t] is the feature value of feature field t of sample s in the dataset. The default value for categorical feature is 1. \n* train_i: matrix with shape (num_sample, num_field). train_i[s][t] is the feature index of feature field t of sample s in the dataset. The maximal value of train_i is the feature size. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Implementation of AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/shichence/AutoInt/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 43,
      "date": "Sun, 26 Dec 2021 22:51:50 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/shichence/AutoInt/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "shichence/AutoInt",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/shichence/AutoInt/master/sample_preprocess.sh",
      "https://raw.githubusercontent.com/shichence/AutoInt/master/test_code.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8295469788071923
      ],
      "excerpt": "We have moved the repo to https://github.com/DeepGraphLearning/RecommenderSystems/tree/master/featureRec. Please check out the latest version there. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/shichence/AutoInt/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2018 Chence Shi\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Note",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "AutoInt",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "shichence",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/shichence/AutoInt/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* **Tensorflow 1.4.0-rc1**\n* Python 3\n* CUDA 8.0+ (For GPU)\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 169,
      "date": "Sun, 26 Dec 2021 22:51:50 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We use four public real-world datasets(Avazu, Criteo, KDD12, MovieLens-1M) in our experiments. Since the first three datasets are super huge, they can not be fit into the memory as a whole. In our implementation, we split the whole dataset into 10 parts and we use the first file as test set and the second file as valid set. We provide the codes for preprocessing these three datasets in `./Dataprocess`. If you want to reuse these codes, you should first run `preprocess.py` to generate `train_x.txt, train_i.txt, train_y.txt` as described in `Input Format`. Then you should run `./Dataprocesss/Kfold_split/StratifiedKfold.py` to split the whole dataset into ten folds. Finally you can run `scale.py` to scale the numerical value(optional).\n\nTo help test the correctness of the code and familarize yourself with the code, we upload the first `10000` samples of `Criteo` dataset in `train_examples.txt`. And we provide the scripts for preprocessing and training.(Please refer to `\tsample_preprocess.sh` and `test_code.sh`, you may need to modify the path in `config.py` and `test_code.sh`). \n\nAfter you run the `test_code.sh`, you should get a folder named `Criteo` which contains `part*, feature_size.npy, fold_index.npy, train_*.txt`. `feature_size.npy` contains the number of total features which will be used to initialize the model. `train_*.txt` is the whole dataset. If you use other small dataset, say `MovieLens-1M`, you only need to modify the function `_run_` in `train.py`.\n\nHere's how to run the preprocessing.\n```\nmkdir Criteo\npython ./Dataprocess/Criteo/preprocess.py\npython ./Dataprocess/Kfold_split/stratifiedKfold.py\npython ./Dataprocess/Criteo/scale.py\n```\n\nHere's how to run the training.\n```\npython -u train.py \\\n                       --data \"Criteo\"  --blocks 3 --heads 2  --block_shape \"[64, 64, 64]\" \\\n                       --is_save \"True\" --save_path \"./test_code/Criteo/b3h2_64x64x64/\"  \\\n                       --field_size 39  --run_times 1 --data_path \"./\" \\\n                       --epoch 3 --has_residual \"True\"  --has_wide \"False\" \\\n                       --batch_size 1024 \\\n                       > test_code_single.out &\n```\n\nYou should see output like this:\n\n```\n...\ntrain logs\n...\nstart testing!...\nrestored from ./test_code/Criteo/b3h2_dnn_dropkeep1_400x2/1/\ntest-result = 0.8088, test-logloss = 0.4430\ntest_auc [0.8088305055534442]\ntest_log_loss [0.44297631300399626]\navg_auc 0.8088305055534442\navg_log_loss 0.44297631300399626\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}