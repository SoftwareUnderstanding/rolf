{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1911.05722",
      "https://arxiv.org/abs/2002.05709",
      "https://arxiv.org/abs/2011.10566",
      "https://arxiv.org/abs/2103.03230",
      "https://arxiv.org/abs/2006.07733",
      "https://arxiv.org/abs/2104.14548",
      "https://arxiv.org/abs/2006.09882",
      "https://arxiv.org/abs/2002.05709",
      "https://arxiv.org/abs/1911.05722",
      "https://arxiv.org/abs/2006.09882",
      "https://arxiv.org/abs/2008.05659"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{susmelj2020lightly,\n  title={Lightly},\n  author={Igor Susmelj, Matthias Heller, Philipp Wirth, Jeremy Prescott, Malte Ebner et al.},\n  journal={GitHub. Note: https://github.com/lightly-ai/lightly},\n  year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9222383658450612
      ],
      "excerpt": "MoCo, 2019 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9265979907090705
      ],
      "excerpt": "- Momentum Contrast for Unsupervised Visual Representation Learning (2020) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lightly-ai/lightly",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "How to contribute to lightly?\nEveryone is welcome to contribute, and we value everybody's contribution. Code is thus not the only way to help the community. Answering questions, helping others, reaching out and improving the documentations are immensely valuable to the community.\nIt also helps us if you spread the word: reference the library from blog posts on the awesome projects it made possible, shout out on Twitter every time it has helped you, or simply star the repo to say \"thank you\".\nYou can contribute in so many ways!\nThere are 4 ways you can contribute to lightly:\n* Fixing outstanding issues with the existing code;\n* Implementing new models;\n* Contributing to the examples or to the documentation;\n* Submitting issues related to bugs or desired new features.\nAll are equally valuable to the community.\nSubmitting a new issue or feature request\nDo your best to follow these guidelines when submitting an issue or a feature\nrequest. It will make it easier for us to come back to you quickly and with good\nfeedback.\nDid you find a bug?\nFirst, please make sure the bug was not already reported (use the search bar on Github under Issues).\n\nInclude your OS type and version, the versions of Python, PyTorch, and PyTorch Lightning.\nA code snippet that allows us to reproduce the bug in less than 30s.\nProvide the full traceback if an exception is raised.\n\nDo you want to implement a new self-supervised model?\nAwesome! Please provide the following information:\n\nShort description of the model and link to the paper;\nLink to the implementation if it's open source;\n\nIf you are willing to contribute the model yourself, let us know so we can best\nguide you.\nDo you want a new feature (that is not a model)?\nA world-class feature request addresses the following points:\n\nMotivation first:\nIs it related to a problem/frustration with the library? If so, please explain\n    why. Providing a code snippet that demonstrates the problem is best.\nIs it related to something you would need for a project? We'd love to hear\n    about it!\nIs it something you worked on and think could benefit the community?\n    Awesome! Tell us what problem it solved for you.\nProvide a code snippet that demonstrates its future use;\nAttach any additional information (drawings, screenshots, etc.) you think may help.\n\nPull Requests\nBefore writing code, we strongly advise you to search through the exising PRs or\nissues to make sure that nobody is already working on the same thing. If you are\nunsure, it is always a good idea to open an issue to get some feedback.\nFollow these steps to start contributing:\n\n\nFork the repository by\n   clicking on the 'Fork' button on the repository's page. This creates a copy of the code\n   under your GitHub user account.\n\n\nClone your fork to your local disk, and add the base repository as a remote:\n\n\nbash\n   $ git clone git@github.com:lightly-ai/lightly.git\n   $ cd lightly\n   $ git remote add upstream https://github.com/lightly-ai/lightly.git\n\nCreate a new branch to hold your development changes:\n\nbash\n   $ git checkout -b a_descriptive_name_for_my_changes\ndo not work on the master branch.\n\nSet up a development environment by running the following command in a virtual environment:\n\nbash\n   $ pip install -e \".[dev]\"\n\nDevelop the features on your branch.\n\nAs you work on the features, you should make sure that the test suite\n   passes:\nbash\n   $ make test\nIf you're modifying documents under docs/source, make sure to validate that\n   they can still be built. This check also runs in CI. \nbash\n   $ cd docs\n   $ make html\n   Once you're happy with your changes, add changed files using git add and\n   make a commit with git commit to record your changes locally:\nbash\n   $ git add modified_file.py\n   $ git commit\nPlease write good commit messages.\nIt is a good idea to sync your copy of the code with the original\n   repository regularly. This way you can quickly account for changes:\nbash\n   $ git fetch upstream\n   $ git rebase upstream/develop\nPush the changes to your account using:\nbash\n   $ git push -u origin a_descriptive_name_for_my_changes\n\n\nOnce you are satisfied, go to the webpage of your fork on GitHub.\n   Click on 'Pull request' to send your changes to the project maintainers for review.\n\n\nIt's ok if maintainers ask you for changes. It happens to core contributors\n   too! So everyone can see the changes in the Pull request, work in your local\n   branch and push the changes to your fork. They will automatically appear in\n   the pull request.\n\n\nStyle guide\nlightly follows the Google styleguide and the PyTorch styleguide by Igor Susmelj.\nCheck our documentation writing guide for more information.\nThis guide was inspired by Transformers transformers guide to contributing which was influenced by Scikit-learn scikit-learn guide to contributing.",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-10-13T13:02:56Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-24T10:57:35Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8000909301015634
      ],
      "excerpt": "Lightly offers features like \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8725093338586994
      ],
      "excerpt": "easy to use and written in a PyTorch like style \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9562594301137136
      ],
      "excerpt": "way using the full power of PyTorch. Experiment with different backbones, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "collate_fn = data.ImageCollateFunction(input_size=32, cj_prob=0.5) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9047376949275113
      ],
      "excerpt": "    batch_size=128,         #: a large batch size helps with the learning \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.850251757267249
      ],
      "excerpt": "model = models.SimCLR(resnet, num_ftrs=512) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.850251757267249
      ],
      "excerpt": "model = models.SimSiam(resnet, num_ftrs=512) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "    model, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "    model, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9802367805745531,
        0.9084043988449451
      ],
      "excerpt": "Currently implemented models and their accuracy on cifar10 and imagenette. All models have been evaluated using kNN. We report the max test accuracy over the epochs as well as the maximum GPU memory consumption. All models in this benchmark use the same augmentations as well as the same ResNet-18 backbone. Training precision is set to FP32 and SGD is used as an optimizer with cosineLR. \nOne epoch on cifar10 takes ~35 seconds on a V100 GPU. Learn more about the cifar10 and imagenette benchmark here \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9615132321000759
      ],
      "excerpt": "Below you can see a schematic overview of the different concepts present in the lightly Python package. The terms in bold are explained in more detail in our documentation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8732285139503251
      ],
      "excerpt": "Head to the documentation and see the things you can achieve with Lightly! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8946863648263463
      ],
      "excerpt": "For more information about how to contribute have a look here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9308975961902155
      ],
      "excerpt": "- A Simple Framework for Contrastive Learning of Visual Representations (2020) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.953804018419108
      ],
      "excerpt": "- Unsupervised Learning of Visual Features by Contrasting Cluster Assignments (2020) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A python library for self-supervised learning on images.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lightly-ai/lightly/releases",
    "technique": "GitHub API"
  },
  "faq": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Why should I care about self-supervised learning? Aren't pre-trained models from ImageNet much better for transfer learning?\n  - Self-supervised learning has become increasingly popular among scientists over the last year because the learned representations perform extraordinarily well on downstream tasks. This means that they capture the important information in an image better than other types of pre-trained models. By training a self-supervised model on *your* dataset, you can make sure that the representations have all the necessary information about your images.\n\n- How can I contribute?\n  - Create an issue if you encounter bugs or have ideas for features we should implement. You can also add your own code by forking this repository and creating a PR. More details about how to contribute with code is in our [contribution guide](CONTRIBUTING.md).\n\n- Is this framework for free?\n  - Yes, this framework completely free to use and we provide the code. We believe that\n  we need to make training deep learning models more data efficient to achieve widespread adoption. One step to achieve this goal is by leveraging self-supervised learning. The company behind lightly commited to keep this framework open-source.\n\n- If this framework is free, how is the company behind lightly making money?\n  - Training self-supervised models is only part of the solution. The company behind lightly focuses on processing and analyzing embeddings created by self-supervised models. \n  By building, what we call a self-supervised active learning loop we help companies understand and work with their data more efficiently. This framework acts as an interface\n  for our platform to easily upload and download datasets, embeddings and models. Whereas \n  the platform will cost for additional features this frameworks will always remain free of charge (even for commercial use).\n\n",
      "technique": "Header extraction"
    }
  ],
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 87,
      "date": "Fri, 24 Dec 2021 22:01:46 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/lightly-ai/lightly/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "lightly-ai/lightly",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/lightly-ai/lightly/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/lightly-ai/lightly/master/tests/UNMOCKED_end2end_tests/run_all_unmocked_tests.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You can install Lightly and its dependencies from PyPI with:\n```\npip3 install lightly\n```\n\nWe strongly recommend that you install Lightly in a dedicated virtualenv, to avoid conflicting with your system packages.\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8529305331482478
      ],
      "excerpt": "support for multi-gpu training using PyTorch Lightning \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9757929232696467
      ],
      "excerpt": ": build a PyTorch dataloader \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9715504682727676
      ],
      "excerpt": ": get a PyTorch optimizer \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9507229266754984
      ],
      "excerpt": "the following command: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9087800607962331,
        0.9792496481089042,
        0.9826756917960479
      ],
      "excerpt": "To install dev dependencies (for example to contribute to the framework) \nyou can use the following command: \npip3 install -e \".[dev]\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8227017514904269
      ],
      "excerpt": "We provide a Pylint config following the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9571845771611432
      ],
      "excerpt": "You can run the linter from your terminal either on a folder \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9133368656218674,
        0.8762729030460817
      ],
      "excerpt": "import torchvision \nimport lightly.models as models \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.913207809812685
      ],
      "excerpt": "import lightly.data as data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8021975146594292
      ],
      "excerpt": "dataset = data.LightlyDataset(input_dir='./my/cute/cats/dataset/') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8038366701029329
      ],
      "excerpt": "model = models.SimCLR(resnet, num_ftrs=512) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8038366701029329
      ],
      "excerpt": "model = models.SimSiam(resnet, num_ftrs=512) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8148794364043156
      ],
      "excerpt": "To train a SimCLR model on a folder of images you can simply run \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": "lightly-train input_dir=/mydataset \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.809846912782638
      ],
      "excerpt": "| Model       | Epochs | Batch Size | Test Accuracy | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.809846912782638
      ],
      "excerpt": "| Model       | Epochs | Batch Size | Test Accuracy | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8038086824892735
      ],
      "excerpt": "| SimSiam     |  800   | 128        | 0.80          | \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/lightly-ai/lightly/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell",
      "Makefile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "## Features",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "lightly",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "lightly-ai",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lightly-ai/lightly/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "philippmwirth",
        "body": "# Bug fixes and documentation updates\r\n\r\n## Bug fixes\r\nWe fixed the bug that `lightly-download` with the option `exclude_parent_tag` didn't work and a bug in the `api_workflow_upload_metadata` introduced by the change which made the apis private members.\r\n\r\n## Documentation Updates\r\nThe docs have received a fresh new look! Additionally, we have added tutorials about how to use [Lightly with data hosted on S3](https://docs.lightly.ai/getting_started/platform.html#how-to-use-s3-with-lightly) and how to [export data directly to Labelstudio](https://docs.lightly.ai/tutorials/platform/tutorial_label_studio_export.html) (no download needed!)\r\n\r\n## CLI\r\nThe CLI now stores important results such as the `dataset_id`, `path_to_embeddings` and `path_to_checkpoint` in the environment variables `LIGHTLY_LAST_CHECKPOINT_PATH`, `LIGHTLY_LAST_EMBEDDING_PATH`, and `LIGHTLY_LAST_DATASET_ID`.\r\n\r\n## Models\r\n- [Bootstrap your own latent: A new approach to self-supervised Learning, 2020](https://arxiv.org/abs/2006.07733)\r\n- [Barlow Twins: Self-Supervised Learning via Redundancy Reduction, 2021](https://arxiv.org/abs/2103.03230)\r\n- [SimSiam: Exploring Simple Siamese Representation Learning, 2020](https://arxiv.org/abs/2011.10566)\r\n- [MoCo: Momentum Contrast for Unsupervised Visual Representation Learning, 2019](https://arxiv.org/abs/1911.05722)\r\n- [SimCLR: A Simple Framework for Contrastive Learning of Visual Representations, 2020](https://arxiv.org/abs/2002.05709)\r\n- [NNCLR: Nearest-Neighbor Contrastive Learning of Visual Representations, 2021](https://arxiv.org/pdf/2104.14548.pdf)\r\n- [SwAV: Unsupervised Learning of Visual Features by Contrasting Cluster Assignments, M. Caron, 2020](https://arxiv.org/abs/2006.09882)\r\n",
        "dateCreated": "2021-12-14T14:16:24Z",
        "datePublished": "2021-12-14T14:30:04Z",
        "html_url": "https://github.com/lightly-ai/lightly/releases/tag/v1.2.1",
        "name": "Bug fixes and documentation updates",
        "tag_name": "v1.2.1",
        "tarball_url": "https://api.github.com/repos/lightly-ai/lightly/tarball/v1.2.1",
        "url": "https://api.github.com/repos/lightly-ai/lightly/releases/55279374",
        "zipball_url": "https://api.github.com/repos/lightly-ai/lightly/zipball/v1.2.1"
      },
      {
        "authorType": "User",
        "author_name": "guarin",
        "body": "## Low-Level Building Blocks\r\nTo improve the flexibility of the lightly framework we refactored our models into smaller building blocks. These blocks can now easily be assembled into novel model architectures and allow lightly to better integrate with other deep learning libraries such as PyTorch Lightning.\r\n\r\n### Examples\r\nWe provide example implementations using low-level building blocks for all the models we support in the new [Examples](https://docs.lightly.ai/examples/models.html) section in our documentation.\r\n\r\nWe also updated the other parts of the documentation to use the new building blocks. We hope that this makes it easier to integrate lightly into your own project!\r\n\r\n### Deprecation\r\nAs part of this refactoring have added a deprecation warning to all old models under `lightly/models`. We intend to remove those models in version 1.3.0.\r\n\r\n## Detectron2 Pretraining Tutorial\r\nWe created a new [tutorial](https://docs.lightly.ai/tutorials/package/tutorial_pretrain_detectron2.html) which shows how to use lightly to pre-train an object detection model with the [Detectron2](https://github.com/facebookresearch/detectron2) framework.\r\n\r\n## Models\r\n- [Bootstrap your own latent: A new approach to self-supervised Learning, 2020](https://arxiv.org/abs/2006.07733)\r\n- [Barlow Twins: Self-Supervised Learning via Redundancy Reduction, 2021](https://arxiv.org/abs/2103.03230)\r\n- [SimSiam: Exploring Simple Siamese Representation Learning, 2020](https://arxiv.org/abs/2011.10566)\r\n- [MoCo: Momentum Contrast for Unsupervised Visual Representation Learning, 2019](https://arxiv.org/abs/1911.05722)\r\n- [SimCLR: A Simple Framework for Contrastive Learning of Visual Representations, 2020](https://arxiv.org/abs/2002.05709)\r\n- [NNCLR: Nearest-Neighbor Contrastive Learning of Visual Representations, 2021](https://arxiv.org/pdf/2104.14548.pdf)\r\n- [SwAV: Unsupervised Learning of Visual Features by Contrasting Cluster Assignments, M. Caron, 2020](https://arxiv.org/abs/2006.09882)\r\n",
        "dateCreated": "2021-12-01T09:12:41Z",
        "datePublished": "2021-12-01T16:05:36Z",
        "html_url": "https://github.com/lightly-ai/lightly/releases/tag/v1.2.0",
        "name": "Refactoring Models",
        "tag_name": "v1.2.0",
        "tarball_url": "https://api.github.com/repos/lightly-ai/lightly/tarball/v1.2.0",
        "url": "https://api.github.com/repos/lightly-ai/lightly/releases/54397281",
        "zipball_url": "https://api.github.com/repos/lightly-ai/lightly/zipball/v1.2.0"
      },
      {
        "authorType": "User",
        "author_name": "guarin",
        "body": "## Bugfix: Upload Embeddings\r\nUploading embeddings to a new dataset through `lightly-magic` or `lightly-upload` raised an error. This is now fixed. Thanks to @natejenkins for the help!\r\n\r\n## Bugfix: `lightly-download` with integer tag names\r\n`lightly-download` now supports downloading datasets with integer tag names.\r\n\r\n## Readme Overview Image \r\nThe overview image in the readme file should now point again to the right address. Thanks @vnshanmukh for the contribution!\r\n\r\n## VideoDatasets are more efficient\r\nVideoDatasets now precompute the dataset length instead of recalculating it every time a frame is accessed.\r\n\r\n## Other\r\nAdded `scikit-learn` and `pandas` to dev dependencies.\r\nAdded more API tests.\r\n\r\n## Models\r\n- [Bootstrap your own latent: A new approach to self-supervised Learning, 2020](https://arxiv.org/abs/2006.07733)\r\n- [Barlow Twins: Self-Supervised Learning via Redundancy Reduction, 2021](https://arxiv.org/abs/2103.03230)\r\n- [SimSiam: Exploring Simple Siamese Representation Learning, 2020](https://arxiv.org/abs/2011.10566)\r\n- [MoCo: Momentum Contrast for Unsupervised Visual Representation Learning, 2019](https://arxiv.org/abs/1911.05722)\r\n- [SimCLR: A Simple Framework for Contrastive Learning of Visual Representations, 2020](https://arxiv.org/abs/2002.05709)\r\n- [NNCLR: Nearest-Neighbor Contrastive Learning of Visual Representations, 2021](https://arxiv.org/pdf/2104.14548.pdf)\r\n- [SwAV: Unsupervised Learning of Visual Features by Contrasting Cluster Assignments, M. Caron, 2020](https://arxiv.org/abs/2006.09882)",
        "dateCreated": "2021-11-24T15:25:54Z",
        "datePublished": "2021-11-24T16:01:49Z",
        "html_url": "https://github.com/lightly-ai/lightly/releases/tag/v1.1.23",
        "name": "Bugfixes, Better VideoDatasets",
        "tag_name": "v1.1.23",
        "tarball_url": "https://api.github.com/repos/lightly-ai/lightly/tarball/v1.1.23",
        "url": "https://api.github.com/repos/lightly-ai/lightly/releases/54007865",
        "zipball_url": "https://api.github.com/repos/lightly-ai/lightly/zipball/v1.1.23"
      },
      {
        "authorType": "User",
        "author_name": "MalteEbner",
        "body": "# Dataset Upsizing, Bugfixes\r\n## Dataset Upsizing\r\nYou can now add new samples and embedding to an existing dataset. Just run the usual `lightly-upload` or `lightly-magic` command with the `dataset_id` of an existing dataset and it will upload all new images to it. The embeddings are also updated.\r\n\r\n## Bugfix: ResnetGenerator now uses the argument `num_classes` correctly.\r\nBefore the fix, it was hardcoded to 10 classes. Thanks to @smartdanny for finding and fixing this bug! \r\n\r\n## Bugfix: NNCLR\r\nNNCLR had a bug that the projection and prediction head were not connected correctly. Thanks to @HBU-Lin-Li  for finding this bug!\r\n\r\n## Bugfix: Version check timeout\r\nWhen lightly starts, it checks if a newer version is available. This check could occur multiple times due to circular imports and it could take long if your don't have an internet connect. We fixed this to do only one version check and restrict its duration to 1s. Thanks to @luzuku for finding this bug!\r\n\r\n## Models\r\n- [Bootstrap your own latent: A new approach to self-supervised Learning, 2020](https://arxiv.org/abs/2006.07733)\r\n- [Barlow Twins: Self-Supervised Learning via Redundancy Reduction, 2021](https://arxiv.org/abs/2103.03230)\r\n- [SimSiam: Exploring Simple Siamese Representation Learning, 2020](https://arxiv.org/abs/2011.10566)\r\n- [MoCo: Momentum Contrast for Unsupervised Visual Representation Learning, 2019](https://arxiv.org/abs/1911.05722)\r\n- [SimCLR: A Simple Framework for Contrastive Learning of Visual Representations, 2020](https://arxiv.org/abs/2002.05709)\r\n- [NNCLR: Nearest-Neighbor Contrastive Learning of Visual Representations, 2021](https://arxiv.org/pdf/2104.14548.pdf)\r\n- [SwAV: Unsupervised Learning of Visual Features by Contrasting Cluster Assignments, M. Caron, 2020](https://arxiv.org/abs/2006.09882)\r\n",
        "dateCreated": "2021-11-02T13:52:40Z",
        "datePublished": "2021-11-02T14:33:23Z",
        "html_url": "https://github.com/lightly-ai/lightly/releases/tag/v1.1.22",
        "name": "Dataset Upsizing, Bugfixes",
        "tag_name": "v1.1.22",
        "tarball_url": "https://api.github.com/repos/lightly-ai/lightly/tarball/v1.1.22",
        "url": "https://api.github.com/repos/lightly-ai/lightly/releases/52525126",
        "zipball_url": "https://api.github.com/repos/lightly-ai/lightly/zipball/v1.1.22"
      },
      {
        "authorType": "User",
        "author_name": "MalteEbner",
        "body": "# Video Datasets with Subfolders, Specify Relevant Files\r\n\r\n## Video Datasets with Subfolders\r\nJust like for image datasets, now also video datasets with the videos in subfolders are supported. E.g. you can have the following input directory:\r\n```\r\n/path/to/data/\r\n    L subfolder_1/\r\n      L my-video-1-1.mp4\r\n      L my-video-1-2.mp4\r\n    L subfolder_2/\r\n       L my-video-2-1.mp4\r\n```\r\n\r\n## Specify relevant files\r\nWhen creating a `LightlyDataset` you can now also specify the argument `filenames`. It must be a list of filenames relative to the input directory. Then the dataset only uses the files specified and ignores all other files. E.g. using \r\n```python\r\nLightlyDataset(input_dir='/path/to/data', filenames=['subfolder_1/my-video-1-1.mp4', 'subfolder_2/my-video-2-1.mp4'])\r\n```\r\nwill only create a dataset out of the two specified files and ignore the third file.\r\n\r\n## Other\r\nWe added the SwAV model to the README, it was already added to the documentation.\r\n\r\n## Models\r\n- [Bootstrap your own latent: A new approach to self-supervised Learning, 2020](https://arxiv.org/abs/2006.07733)\r\n- [Barlow Twins: Self-Supervised Learning via Redundancy Reduction, 2021](https://arxiv.org/abs/2103.03230)\r\n- [SimSiam: Exploring Simple Siamese Representation Learning, 2020](https://arxiv.org/abs/2011.10566)\r\n- [MoCo: Momentum Contrast for Unsupervised Visual Representation Learning, 2019](https://arxiv.org/abs/1911.05722)\r\n- [SimCLR: A Simple Framework for Contrastive Learning of Visual Representations, 2020](https://arxiv.org/abs/2002.05709)\r\n- [NNCLR: Nearest-Neighbor Contrastive Learning of Visual Representations, 2021](https://arxiv.org/pdf/2104.14548.pdf)\r\n- [SwAV: Unsupervised Learning of Visual Features by Contrasting Cluster Assignments, M. Caron, 2020](https://arxiv.org/abs/2006.09882)",
        "dateCreated": "2021-10-19T08:31:23Z",
        "datePublished": "2021-10-19T08:43:58Z",
        "html_url": "https://github.com/lightly-ai/lightly/releases/tag/v1.1.21",
        "name": "Video Datasets with Subfolders, Specify Relevant Files",
        "tag_name": "v1.1.21",
        "tarball_url": "https://api.github.com/repos/lightly-ai/lightly/tarball/v1.1.21",
        "url": "https://api.github.com/repos/lightly-ai/lightly/releases/51599421",
        "zipball_url": "https://api.github.com/repos/lightly-ai/lightly/zipball/v1.1.21"
      },
      {
        "authorType": "User",
        "author_name": "MalteEbner",
        "body": "# Refactor Models, SwAV Model, S3-Bucket Integration\r\n## Refactor Models\r\nThis release will make it much easier to implement new models or adapt existing models by using basic building blocks. E.g. you can define your own model out of blocks like a backbone, projection head, momentum encoder, nearest neighbour memory bank and more. \r\nWe want you to see easily how the models in current papers are build and that different papers often only differ in one or two of these blocks.\r\nCompatible examples of all models are shown in the benchmarking scripts for [imagenette](https://github.com/lightly-ai/lightly/blob/master/docs/source/getting_started/benchmarks/imagenette_benchmark.py) and [cifar10](https://github.com/lightly-ai/lightly/blob/master/docs/source/getting_started/benchmarks/cifar10_benchmark.py).\r\n\r\nAs part of this refactoring to improve flexibility of the framework we have added a deprecation warning to all old models under `lightly/models`, e.g.:\r\n\r\n```\r\nThe high-level building block NNCLR will be deprecated in version 1.2.0. \r\nUse low-level building blocks instead. \r\nSee https://docs.lightly.ai/lightly.models.html for more information\r\n```\r\n\r\nThese models will be removed with the upcoming version 1.2. The necessity of the refactoring stems from a lack of flexibility which makes it difficult to keep up with the latest publications.\r\n\r\n## SwAV Model\r\nLightly now supports the [Swapping assignment between views (SWaV) paper](https://arxiv.org/abs/2006.09882). Thanks to the new system with building blocks, we could implement it more easily. \r\n\r\n## S3Bucket Integration\r\n- We added documentation on how to use an S3Bucket as input directory for lightly. It allows you to train your model and create embeddings without needing to download all your data.\r\n\r\n## Other\r\n- When uploading the embeddings to the Lightly Platform, no file `embeddings_sorted.csv` is created anymore, as it was only used internally. We also made the upload of large embeddings files faster. \r\n\r\n## Models\r\n- [Bootstrap your own latent: A new approach to self-supervised Learning, 2020](https://arxiv.org/abs/2006.07733)\r\n- [Barlow Twins: Self-Supervised Learning via Redundancy Reduction, 2021](https://arxiv.org/abs/2103.03230)\r\n- [SimSiam: Exploring Simple Siamese Representation Learning, 2020](https://arxiv.org/abs/2011.10566)\r\n- [MoCo: Momentum Contrast for Unsupervised Visual Representation Learning, 2019](https://arxiv.org/abs/1911.05722)\r\n- [SimCLR: A Simple Framework for Contrastive Learning of Visual Representations, 2020](https://arxiv.org/abs/2002.05709)\r\n- [NNCLR: Nearest-Neighbor Contrastive Learning of Visual Representations, 2021](https://arxiv.org/pdf/2104.14548.pdf)\r\n- [SwAV: Unsupervised Learning of Visual Features by Contrasting Cluster Assignments, M. Caron, 2020](https://arxiv.org/abs/2006.09882)",
        "dateCreated": "2021-10-04T08:25:12Z",
        "datePublished": "2021-10-04T09:22:07Z",
        "html_url": "https://github.com/lightly-ai/lightly/releases/tag/v1.1.20",
        "name": "Refactor Models, SwAV Model, S3-Bucket Integration",
        "tag_name": "v1.1.20",
        "tarball_url": "https://api.github.com/repos/lightly-ai/lightly/tarball/v1.1.20",
        "url": "https://api.github.com/repos/lightly-ai/lightly/releases/50722407",
        "zipball_url": "https://api.github.com/repos/lightly-ai/lightly/zipball/v1.1.20"
      },
      {
        "authorType": "User",
        "author_name": "philippmwirth",
        "body": "# Refactored Prediction Heads and Jigsaw\r\n\r\n## Refactored Prediction Heads\r\nExcited to bring the newly refactored prediction and projection heads to you! The new abstractions are easy to understand and \r\ncan be extended to arbitrary projection head implementations - making the framework more flexible. Additionally, the implementation of each projection head is now based on a direct citation from the respective paper. Check it out [here](https://github.com/lightly-ai/lightly/blob/master/lightly/models/modules/heads.py).\r\n\r\n**Breaking Changes:**\r\n- The argument `num_mlp_layers` was removed from SimSiam and NNCLR and defaults to 3 (as in the respective papers).\r\n- The projection heads and prediction heads of the models are now separate modules which might break old checkpoints. However, the following function helps loading old checkpoints: [`load_from_state_dict`](https://github.com/lightly-ai/lightly/blob/0af0563e02d55ee8363769a18a1a36d2a1408f64/lightly/cli/_helpers.py#L180)\r\n\r\n## Jigsaw (@shikharmn)\r\nLightly now features the jigsaw augmentation! Thanks a lot @shikharmn for your contribution.\r\n\r\n## Documentation Updates\r\nParts of the [documentation](https://docs.lightly.ai/) have been refactored to give a clearer overview of the features lightly provides. Additionally, external tutorials have been linked so that everything is in one place.\r\n\r\n## Bug Fixes\r\n- The `lightly-crop` feature now has a smaller memory footprint\r\n- Filenames containing commas are now ignored\r\n- Checks for the latest pip version occur less often\r\n\r\n## Models\r\n- [Bootstrap your own latent: A new approach to self-supervised Learning, 2020](https://arxiv.org/abs/2006.07733)\r\n- [Barlow Twins: Self-Supervised Learning via Redundancy Reduction, 2021](https://arxiv.org/abs/2103.03230)\r\n- [SimSiam: Exploring Simple Siamese Representation Learning, 2020](https://arxiv.org/abs/2011.10566)\r\n- [MoCo: Momentum Contrast for Unsupervised Visual Representation Learning, 2019](https://arxiv.org/abs/1911.05722)\r\n- [SimCLR: A Simple Framework for Contrastive Learning of Visual Representations, 2020](https://arxiv.org/abs/2002.05709)\r\n- [NNCLR: Nearest-Neighbor Contrastive Learning of Visual Representations, 2021](https://arxiv.org/pdf/2104.14548.pdf)",
        "dateCreated": "2021-09-16T15:01:07Z",
        "datePublished": "2021-09-16T15:34:08Z",
        "html_url": "https://github.com/lightly-ai/lightly/releases/tag/v1.1.19",
        "name": "Refactored Prediction Heads and Jigsaw",
        "tag_name": "v1.1.19",
        "tarball_url": "https://api.github.com/repos/lightly-ai/lightly/tarball/v1.1.19",
        "url": "https://api.github.com/repos/lightly-ai/lightly/releases/49687519",
        "zipball_url": "https://api.github.com/repos/lightly-ai/lightly/zipball/v1.1.19"
      },
      {
        "authorType": "User",
        "author_name": "MalteEbner",
        "body": "## Custom Metadata\r\nLightly now supports uploading custom metadata, which can be used in the [Lightly Web-app](https://app.lightly.ai).\r\n\r\n## Tutorial on custom metadata\r\nWe added a new [tutorial](https://docs.lightly.ai/tutorials/platform/tutorial_aquarium_custom_metadata.html) on how to create and use custom metadata  to understand your dataset even better.\r\n\r\n## Tutorial to use lightly to find false negatives in object detection. \r\nDo you have problems with your object detector not finding all objects? Lightly can help to you to find these false negatives. We created a [tutorial](https://docs.lightly.ai/tutorials/platform/tutorial_cropped_objects_metadata.html) describing how to do it.\r\n\r\n## Tutorial to embed the Lightly docker into a Dagster pipeline\r\nDo you want to use the Lightly Docker as part of a bigger data pipeline, e.g. with [Dagster](https://dagster.io)? We added a [tutorial](https://docs.lightly.ai/docker/integration/dagster_aws.html) on how to do it.\r\n\r\n## Models\r\n- [Bootstrap your own latent: A new approach to self-supervised Learning, 2020](https://arxiv.org/abs/2006.07733)\r\n- [Barlow Twins: Self-Supervised Learning via Redundancy Reduction, 2021](https://arxiv.org/abs/2103.03230)\r\n- [SimSiam: Exploring Simple Siamese Representation Learning, 2020](https://arxiv.org/abs/2011.10566)\r\n- [MoCo: Momentum Contrast for Unsupervised Visual Representation Learning, 2019](https://arxiv.org/abs/1911.05722)\r\n- [SimCLR: A Simple Framework for Contrastive Learning of Visual Representations, 2020](https://arxiv.org/abs/2002.05709)\r\n- [NNCLR: Nearest-Neighbor Contrastive Learning of Visual Representations, 2021](https://arxiv.org/pdf/2104.14548.pdf)",
        "dateCreated": "2021-08-25T08:08:25Z",
        "datePublished": "2021-08-25T08:23:41Z",
        "html_url": "https://github.com/lightly-ai/lightly/releases/tag/v1.1.18",
        "name": "Custom Metadata, 3 New Tutorials",
        "tag_name": "v1.1.18",
        "tarball_url": "https://api.github.com/repos/lightly-ai/lightly/tarball/v1.1.18",
        "url": "https://api.github.com/repos/lightly-ai/lightly/releases/48409451",
        "zipball_url": "https://api.github.com/repos/lightly-ai/lightly/zipball/v1.1.18"
      },
      {
        "authorType": "User",
        "author_name": "philippmwirth",
        "body": "# Active Learning Score Upload\r\n\r\n## Active Learning Score Upload\r\nThe lightly `ActiveLearningAgent` now supports an easy way to upload active learning scores to the [Lightly Web-app](https://app.lightly.ai).\r\n\r\n## Register Datasets before Upload\r\nThe refactored dataset upload now registers a dataset in the web-app before uploading the samples. This makes the upload more efficient and stable. Additionally, the progress of the upload can now be observed in the [Lightly Web-app](https://app.lightly.ai).\r\n\r\n## Documentation Updates\r\nThe [lightly on-premise documentation](https://docs.lightly.ai/docker/overview.html) was updated.\r\n\r\n## Models\r\n\r\n- [Bootstrap your own latent: A new approach to self-supervised Learning, 2020](https://arxiv.org/abs/2006.07733)\r\n- [Barlow Twins: Self-Supervised Learning via Redundancy Reduction, 2021](https://arxiv.org/abs/2103.03230)\r\n- [SimSiam: Exploring Simple Siamese Representation Learning, 2020](https://arxiv.org/abs/2011.10566)\r\n- [MoCo: Momentum Contrast for Unsupervised Visual Representation Learning, 2019](https://arxiv.org/abs/1911.05722)\r\n- [SimCLR: A Simple Framework for Contrastive Learning of Visual Representations, 2020](https://arxiv.org/abs/2002.05709)\r\n- [NNCLR: Nearest-Neighbor Contrastive Learning of Visual Representations, 2021](https://arxiv.org/pdf/2104.14548.pdf)",
        "dateCreated": "2021-08-06T16:04:48Z",
        "datePublished": "2021-08-06T16:11:50Z",
        "html_url": "https://github.com/lightly-ai/lightly/releases/tag/v1.1.17",
        "name": "Active Learning Score Upload",
        "tag_name": "v1.1.17",
        "tarball_url": "https://api.github.com/repos/lightly-ai/lightly/tarball/v1.1.17",
        "url": "https://api.github.com/repos/lightly-ai/lightly/releases/47439261",
        "zipball_url": "https://api.github.com/repos/lightly-ai/lightly/zipball/v1.1.17"
      },
      {
        "authorType": "User",
        "author_name": "philippmwirth",
        "body": "# Improved Tutorial and Bug Fix in Masked Select\r\n\r\n## Improved Tutorial\r\nThe \"Sunflowers\" Tutorial has been overhauled and provides a great starting point for anyone trying to clean up their data.\r\n\r\n## Bug Fix in Masked Select\r\nMajor bug fix which solves confusion about little and big endian representation of the bit masks used for active learning.\r\n\r\n## Updated Requirements\r\n`lightly` now requires the latest minor version (`0.0.*`) of the `lightly-utils` package instead of a fixed version. This allows quicker bug fixes and updates from the maintainers.\r\n\r\n## Models\r\n\r\n- [Bootstrap your own latent: A new approach to self-supervised Learning, 2020](https://arxiv.org/abs/2006.07733)\r\n- [Barlow Twins: Self-Supervised Learning via Redundancy Reduction, 2021](https://arxiv.org/abs/2103.03230)\r\n- [SimSiam: Exploring Simple Siamese Representation Learning, 2020](https://arxiv.org/abs/2011.10566)\r\n- [MoCo: Momentum Contrast for Unsupervised Visual Representation Learning, 2019](https://arxiv.org/abs/1911.05722)\r\n- [SimCLR: A Simple Framework for Contrastive Learning of Visual Representations, 2020](https://arxiv.org/abs/2002.05709)\r\n- [NNCLR: Nearest-Neighbor Contrastive Learning of Visual Representations, 2021](https://arxiv.org/pdf/2104.14548.pdf)",
        "dateCreated": "2021-07-22T10:54:15Z",
        "datePublished": "2021-07-22T11:01:40Z",
        "html_url": "https://github.com/lightly-ai/lightly/releases/tag/v1.1.16",
        "name": "Improved Tutorial and Bug Fix in Masked Select",
        "tag_name": "v1.1.16",
        "tarball_url": "https://api.github.com/repos/lightly-ai/lightly/tarball/v1.1.16",
        "url": "https://api.github.com/repos/lightly-ai/lightly/releases/46596657",
        "zipball_url": "https://api.github.com/repos/lightly-ai/lightly/zipball/v1.1.16"
      },
      {
        "authorType": "User",
        "author_name": "philippmwirth",
        "body": "# Resume Upload and Minor Updates\r\n\r\n## Resume Upload\r\nThe upload of a dataset can now be resumed if interrupted, as the `lightly-upload` and `lightly-magic` commands will skip files which are already on the platform. \r\n\r\n## Minor Updates\r\nFilenames of images which are uploaded to the platform can now be up to 255 characters long.\r\nLightly can now be [cited](https://github.com/lightly-ai/lightly#bibtex) :)\r\n\r\n## Models\r\n\r\n- [Bootstrap your own latent: A new approach to self-supervised Learning, 2020](https://arxiv.org/abs/2006.07733)\r\n- [Barlow Twins: Self-Supervised Learning via Redundancy Reduction, 2021](https://arxiv.org/abs/2103.03230)\r\n- [SimSiam: Exploring Simple Siamese Representation Learning, 2020](https://arxiv.org/abs/2011.10566)\r\n- [MoCo: Momentum Contrast for Unsupervised Visual Representation Learning, 2019](https://arxiv.org/abs/1911.05722)\r\n- [SimCLR: A Simple Framework for Contrastive Learning of Visual Representations, 2020](https://arxiv.org/abs/2002.05709)\r\n- [NNCLR: Nearest-Neighbor Contrastive Learning of Visual Representations, 2021](https://arxiv.org/pdf/2104.14548.pdf)",
        "dateCreated": "2021-07-08T09:11:30Z",
        "datePublished": "2021-07-08T09:18:20Z",
        "html_url": "https://github.com/lightly-ai/lightly/releases/tag/v1.1.15",
        "name": "Resume Upload and Minor Updates",
        "tag_name": "v1.1.15",
        "tarball_url": "https://api.github.com/repos/lightly-ai/lightly/tarball/v1.1.15",
        "url": "https://api.github.com/repos/lightly-ai/lightly/releases/45891010",
        "zipball_url": "https://api.github.com/repos/lightly-ai/lightly/zipball/v1.1.15"
      },
      {
        "authorType": "User",
        "author_name": "MalteEbner",
        "body": "# Lightly-Crop Command, Much Faster Upload, Faster Ntxent Loss\r\n\r\nThe `lightly-crop` CLI command crops objects out of the input images based on labels and copies them into an output folder. This is very useful for doing SSL on an object-level instead of an image level. For more information, look at the documentation at https://docs.lightly.ai/getting_started/command_line_tool.html#crop-images-using-labels-or-predictions\r\n\r\nWe made the upload to the Lightly platform via `lightly-upload`or `lightly-magic`much faster. It should be at least 2 times faster (for smaller images) and even faster for large and compressed images like large jpegs. \r\n\r\nThe ntxent loss has a higher performance by optimising transfer between CPU and GPU\r\n\r\n## Models\r\n\r\n- [Bootstrap your own latent: A new approach to self-supervised Learning, 2020](https://arxiv.org/abs/2006.07733)\r\n- [Barlow Twins: Self-Supervised Learning via Redundancy Reduction, 2021](https://arxiv.org/abs/2103.03230)\r\n- [SimSiam: Exploring Simple Siamese Representation Learning, 2020](https://arxiv.org/abs/2011.10566)\r\n- [MoCo: Momentum Contrast for Unsupervised Visual Representation Learning, 2019](https://arxiv.org/abs/1911.05722)\r\n- [SimCLR: A Simple Framework for Contrastive Learning of Visual Representations, 2020](https://arxiv.org/abs/2002.05709)\r\n- [NNCLR: Nearest-Neighbor Contrastive Learning of Visual Representations, 2021](https://arxiv.org/pdf/2104.14548.pdf)",
        "dateCreated": "2021-06-29T14:53:00Z",
        "datePublished": "2021-06-29T15:05:21Z",
        "html_url": "https://github.com/lightly-ai/lightly/releases/tag/v1.1.14",
        "name": "Lightly-Crop Command, Much Faster Upload, Faster Ntxent Loss",
        "tag_name": "v1.1.14",
        "tarball_url": "https://api.github.com/repos/lightly-ai/lightly/tarball/v1.1.14",
        "url": "https://api.github.com/repos/lightly-ai/lightly/releases/45431670",
        "zipball_url": "https://api.github.com/repos/lightly-ai/lightly/zipball/v1.1.14"
      },
      {
        "authorType": "User",
        "author_name": "MalteEbner",
        "body": "# More CLI parameters, Bugfixes, Documentation\r\n\r\nThis release adds the new CLI parameter `trainer.weights_summary` allowing you to set the respective parameter of the pytorch lightning trainer for controlling how much information about your embedding model should be printed. \r\n\r\nIt also includes some bugfixes and documentation improvements. \r\n\r\n## Models\r\n\r\n- [Bootstrap your own latent: A new approach to self-supervised Learning, 2020](https://arxiv.org/abs/2006.07733)\r\n- [Barlow Twins: Self-Supervised Learning via Redundancy Reduction, 2021](https://arxiv.org/abs/2103.03230)\r\n- [SimSiam: Exploring Simple Siamese Representation Learning, 2020](https://arxiv.org/abs/2011.10566)\r\n- [MoCo: Momentum Contrast for Unsupervised Visual Representation Learning, 2019](https://arxiv.org/abs/1911.05722)\r\n- [SimCLR: A Simple Framework for Contrastive Learning of Visual Representations, 2020](https://arxiv.org/abs/2002.05709)\r\n- [NNCLR: Nearest-Neighbor Contrastive Learning of Visual Representations, 2021](https://arxiv.org/pdf/2104.14548.pdf)",
        "dateCreated": "2021-06-18T16:10:14Z",
        "datePublished": "2021-06-18T16:20:00Z",
        "html_url": "https://github.com/lightly-ai/lightly/releases/tag/v1.1.13",
        "name": "More CLI parameters, Bugfixes, Documentation",
        "tag_name": "v1.1.13",
        "tarball_url": "https://api.github.com/repos/lightly-ai/lightly/tarball/v1.1.13",
        "url": "https://api.github.com/repos/lightly-ai/lightly/releases/44875884",
        "zipball_url": "https://api.github.com/repos/lightly-ai/lightly/zipball/v1.1.13"
      },
      {
        "authorType": "User",
        "author_name": "IgorSusmelj",
        "body": "# New ImageNette Benchmarks and Faster Dataset Indexing\r\n\r\nThis release contains smaller fixes on the data processing side:\r\n\r\n- Dataset indexing is now up to twice as fast when working with larger datasets\r\n- By default, we don't use `0` workers anymore. The default argument of `-1` automatically detects the number of available cores and uses them. This can speed up the loading of data as well as the uploading of data to the Lighlty Platform.\r\n\r\n## New ImageNette Benchmarks\r\nWe added new benchmarks for the ImageNette dataset.\r\n\r\n| Model       | Epochs | Batch Size | Test Accuracy |\r\n|-------------|--------|------------|---------------|\r\n| MoCo           |  800    | 256        | 0.827         |\r\n| SimCLR        |  800    | 256        | 0.847         |\r\n| SimSiam       |  800    | 256        | 0.827          |\r\n| BarlowTwins |  800    | 256        | 0.801         |\r\n| BYOL            |  800    | 256        | 0.851         |\r\n\r\n\r\n## Models\r\n\r\n- [Bootstrap your own latent: A new approach to self-supervised Learning, 2020](https://arxiv.org/abs/2006.07733)\r\n- [Barlow Twins: Self-Supervised Learning via Redundancy Reduction, 2021](https://arxiv.org/abs/2103.03230)\r\n- [SimSiam: Exploring Simple Siamese Representation Learning, 2020](https://arxiv.org/abs/2011.10566)\r\n- [MoCo: Momentum Contrast for Unsupervised Visual Representation Learning, 2019](https://arxiv.org/abs/1911.05722)\r\n- [SimCLR: A Simple Framework for Contrastive Learning of Visual Representations, 2020](https://arxiv.org/abs/2002.05709)\r\n- [NNCLR: Nearest-Neighbor Contrastive Learning of Visual Representations, 2021](https://arxiv.org/pdf/2104.14548.pdf)",
        "dateCreated": "2021-06-10T11:35:56Z",
        "datePublished": "2021-06-10T11:55:13Z",
        "html_url": "https://github.com/lightly-ai/lightly/releases/tag/v1.1.12",
        "name": "New ImageNette Benchmarks and Faster Dataset Indexing",
        "tag_name": "v1.1.12",
        "tarball_url": "https://api.github.com/repos/lightly-ai/lightly/tarball/v1.1.12",
        "url": "https://api.github.com/repos/lightly-ai/lightly/releases/44408337",
        "zipball_url": "https://api.github.com/repos/lightly-ai/lightly/zipball/v1.1.12"
      },
      {
        "authorType": "User",
        "author_name": "philippmwirth",
        "body": "# Nearest Neighbour Contrastive Learning of Representations (NNCLR)\r\n\r\n## New NNCLR model \r\nNNCLR[0] is basically SimCLR, but replaces samples by their nearest neighbours as an additional \"augmentation\" step.\r\nAs part of it, a Nearest Neighbour Memory Bank Module was implemented, which could also be used for other models.\r\n\r\n[0] [With a Little Help from My Friends: Nearest-Neighbor Contrastive Learning of Visual Representations](https://arxiv.org/abs/2104.14548v1)\r\n\r\n```python\r\nresnet = torchvision.models.resnet18()\r\nbackbone = nn.Sequential(\r\n    *list(resnet.children())[:-1],\r\n    nn.AdaptiveAvgPool2d(1),\r\n)\r\n\r\n# NNCLR\r\nmodel = NNCLR(backbone)\r\ncriterion = NTXentLoss()\r\n\r\n# Prefer SimSiam with nearest neighbour?\r\n# model = SimSiam(backbone)\r\n# criterion = SymNegCosineSimilarityLoss()\r\n\r\n# Prefer BYOL with nearest neighbour?\r\n# model = BYOL(backbone)\r\n# criterion = SymNegCosineSimilarityLoss()\r\n\r\nnn_replacer = NNMemoryBankModule(size=2 ** 16)\r\n\r\n# forward pass\r\n(z0, p0), (z1, p1) = model(x0, x1)\r\nz0 = nn_replacer(z0.detach(), update=False)\r\nz1 = nn_replacer(z1.detach(), update=True)\r\nloss = 0.5 * (criterion(z0, p1) + criterion(z1, p0))\r\n```\r\n\r\n## Models\r\n\r\n- [Bootstrap your own latent: A new approach to self-supervised Learning, 2020](https://arxiv.org/abs/2006.07733)\r\n- [Barlow Twins: Self-Supervised Learning via Redundancy Reduction, 2021](https://arxiv.org/abs/2103.03230)\r\n- [SimSiam: Exploring Simple Siamese Representation Learning, 2020](https://arxiv.org/abs/2011.10566)\r\n- [MoCo: Momentum Contrast for Unsupervised Visual Representation Learning, 2019](https://arxiv.org/abs/1911.05722)\r\n- [SimCLR: A Simple Framework for Contrastive Learning of Visual Representations, 2020](https://arxiv.org/abs/2002.05709)\r\n- [NNCLR: Nearest-Neighbor Contrastive Learning of Visual Representations, 2021](https://arxiv.org/pdf/2104.14548.pdf)",
        "dateCreated": "2021-05-28T06:58:00Z",
        "datePublished": "2021-05-28T07:01:22Z",
        "html_url": "https://github.com/lightly-ai/lightly/releases/tag/v1.1.11",
        "name": " Nearest Neighbour Contrastive Learning of Representations (NNCLR)",
        "tag_name": "v1.1.11",
        "tarball_url": "https://api.github.com/repos/lightly-ai/lightly/tarball/v1.1.11",
        "url": "https://api.github.com/repos/lightly-ai/lightly/releases/43746700",
        "zipball_url": "https://api.github.com/repos/lightly-ai/lightly/zipball/v1.1.11"
      },
      {
        "authorType": "User",
        "author_name": "MalteEbner",
        "body": "# Documentation updates and Miscellaneous\r\n\r\n## Documentation Updates\r\n- Added two new tutorials to the docs.\r\n\r\n## Miscellaneous\r\n- In the warning if a newer lightly version is available, the current version is also shown.\r\n\r\n## Models\r\n\r\n- [Bootstrap your own latent: A new approach to self-supervised Learning, 2020](https://arxiv.org/abs/2006.07733)\r\n- [Barlow Twins: Self-Supervised Learning via Redundancy Reduction, 2021](https://arxiv.org/abs/2103.03230)\r\n- [SimSiam: Exploring Simple Siamese Representation Learning, 2020](https://arxiv.org/abs/2011.10566)\r\n- [MoCo: Momentum Contrast for Unsupervised Visual Representation Learning, 2019](https://arxiv.org/abs/1911.05722)\r\n- [SimCLR: A Simple Framework for Contrastive Learning of Visual Representations, 2020](https://arxiv.org/abs/2002.05709)",
        "dateCreated": "2021-05-27T12:45:13Z",
        "datePublished": "2021-05-27T16:17:45Z",
        "html_url": "https://github.com/lightly-ai/lightly/releases/tag/v1.1.10",
        "name": "Documentation updates and Miscellaneous",
        "tag_name": "v1.1.10",
        "tarball_url": "https://api.github.com/repos/lightly-ai/lightly/tarball/v1.1.10",
        "url": "https://api.github.com/repos/lightly-ai/lightly/releases/43711351",
        "zipball_url": "https://api.github.com/repos/lightly-ai/lightly/zipball/v1.1.10"
      },
      {
        "authorType": "User",
        "author_name": "philippmwirth",
        "body": "# Additional Support for Videos, Minor Bug Fixes, and Documentation Updates\r\n\r\n## Additional Video Formats\r\nThe `LightlyDataset` now works with `.mpg`, `.hevc`, `.m4v`, `.webm`, and `.mpeg` videos.\r\n\r\n## Bug Fixes\r\n- Replaced the `squeeze` operation with `flatten` in the model forward passes. Thanks @guarin for noticing and for the fix!\r\n- Made `lightly` compatible with `pytorch-lightning>=1.3.0`.\r\n\r\n## Documentation Updates\r\n\r\n- The `lightly-magic` command is finally featured in the docs. Thanks @pranavsinghps1!\r\n- Big update on the docker docs.\r\n\r\n## Models\r\n\r\n- [Bootstrap your own latent: A new approach to self-supervised Learning, 2020](https://arxiv.org/abs/2006.07733)\r\n- [Barlow Twins: Self-Supervised Learning via Redundancy Reduction, 2021](https://arxiv.org/abs/2103.03230)\r\n- [SimSiam: Exploring Simple Siamese Representation Learning, 2020](https://arxiv.org/abs/2011.10566)\r\n- [MoCo: Momentum Contrast for Unsupervised Visual Representation Learning, 2019](https://arxiv.org/abs/1911.05722)\r\n- [SimCLR: A Simple Framework for Contrastive Learning of Visual Representations, 2020](https://arxiv.org/abs/2002.05709)",
        "dateCreated": "2021-05-20T11:32:18Z",
        "datePublished": "2021-05-20T11:53:06Z",
        "html_url": "https://github.com/lightly-ai/lightly/releases/tag/v1.1.9",
        "name": "Additional Support for Videos, Minor Bug Fixes, and Documentation Updates",
        "tag_name": "v1.1.9",
        "tarball_url": "https://api.github.com/repos/lightly-ai/lightly/tarball/v1.1.9",
        "url": "https://api.github.com/repos/lightly-ai/lightly/releases/43281737",
        "zipball_url": "https://api.github.com/repos/lightly-ai/lightly/zipball/v1.1.9"
      },
      {
        "authorType": "User",
        "author_name": "IgorSusmelj",
        "body": "# BYOL model, Refactoring and New Tutorial for Active Learning\r\n\r\n## New Model: BYOL\r\n\r\n- This release adds a new model for self-supervised learning: BYOL (see https://arxiv.org/abs/2006.07733)\r\n- Thanks @pranavsinghps1 for your contribution!\r\n\r\n## Improvements\r\n\r\n- Refactored NTXent Loss. The new code is shorter and easier to understand.\r\n- Added a scorer for semantic segmentation to do active learning with image segmentation\r\n- Added color highlighting in CLI\r\n- CLI returns now the `dataset_id` when creating a new dataset\r\n\r\n## New Active Learning Turorial using Detectron2\r\n\r\n- This tutorial shows the full power of the lightly self-supervised embedding and active learning scorers\r\n - Check it out here: https://docs.lightly.ai/tutorials/platform/tutorial_active_learning_detectron2.html\r\n\r\n## Models\r\n\r\n- [Bootstrap your own latent: A new approach to self-supervised Learning, 2020](https://arxiv.org/abs/2006.07733)\r\n- [Barlow Twins: Self-Supervised Learning via Redundancy Reduction, 2021](https://arxiv.org/abs/2103.03230)\r\n- [SimSiam: Exploring Simple Siamese Representation Learning, 2020](https://arxiv.org/abs/2011.10566)\r\n- [MoCo: Momentum Contrast for Unsupervised Visual Representation Learning, 2019](https://arxiv.org/abs/1911.05722)\r\n- [SimCLR: A Simple Framework for Contrastive Learning of Visual Representations, 2020](https://arxiv.org/abs/2002.05709)",
        "dateCreated": "2021-05-07T07:56:03Z",
        "datePublished": "2021-05-07T08:26:23Z",
        "html_url": "https://github.com/lightly-ai/lightly/releases/tag/1.1.8",
        "name": "BYOL model, Refactoring and New Tutorial for Active Learning",
        "tag_name": "1.1.8",
        "tarball_url": "https://api.github.com/repos/lightly-ai/lightly/tarball/1.1.8",
        "url": "https://api.github.com/repos/lightly-ai/lightly/releases/42588191",
        "zipball_url": "https://api.github.com/repos/lightly-ai/lightly/zipball/1.1.8"
      },
      {
        "authorType": "User",
        "author_name": "philippmwirth",
        "body": "# Active Learning Refactoring and Minor Improvements\r\n\r\n## Instantiate shuffle tensor directly on device\r\nThis change makes our momentum encoders more efficient by directly instantiating temporary tensors on device instead of moving them there after instantiation. Thanks a lot to @guarin for pointing out the problem and swiftly fixing it!\r\n\r\n## Active Learning Refactoring\r\nThe new strategy of uploading active learning scores to a query tag instead of the preselected tag is enforced making our framework more flexible, easier to use, and allowing users to make several samplings with the same set of scores at the cost of little computational overhead. \r\nAdditionally, active learning scores were renamed to match the current literature. We now support uncertainty sampling with the least confidence, margin and entropy variant as described in http://burrsettles.com/pub/settles.activelearning.pdf, page 12f, chapter 3.1.\r\n\r\n## Minor Bug Fixes and Improvements\r\nBetter handling of edge cases when doing active learning for object detection.\r\n\r\n## Models\r\n\r\n- [Barlow Twins: Self-Supervised Learning via Redundancy Reduction, 2021](https://arxiv.org/abs/2103.03230)\r\n- [SimSiam: Exploring Simple Siamese Representation Learning, 2020](https://arxiv.org/abs/2011.10566)\r\n- [MoCo: Momentum Contrast for Unsupervised Visual Representation Learning, 2019](https://arxiv.org/abs/1911.05722)\r\n- [SimCLR: A Simple Framework for Contrastive Learning of Visual Representations, 2020](https://arxiv.org/abs/2002.05709)",
        "dateCreated": "2021-04-29T14:33:22Z",
        "datePublished": "2021-04-29T14:42:55Z",
        "html_url": "https://github.com/lightly-ai/lightly/releases/tag/v1.1.7",
        "name": "Active Learning Refactoring and Minor Improvements",
        "tag_name": "v1.1.7",
        "tarball_url": "https://api.github.com/repos/lightly-ai/lightly/tarball/v1.1.7",
        "url": "https://api.github.com/repos/lightly-ai/lightly/releases/42208423",
        "zipball_url": "https://api.github.com/repos/lightly-ai/lightly/zipball/v1.1.7"
      },
      {
        "authorType": "User",
        "author_name": "MalteEbner",
        "body": "More Powerful CLI Commands, Stability Improvements and Updated Documentation\r\n## Create a new dataset directly when running`lightly-upload` and `lightly-magic`\r\nJust replace the argument `dataset_id=\"your_dataset_id\"` with the argument `new_dataset_name=\"your_dataset_name\"`. To learn more, look at the docs,\r\n## Get only the newly added samples from a tag\r\n#### `lightly-download` has the flag `exclude_parent_tag`\r\nIf this flag is set, the samples in the parent tag are excluded from being downloaded. This is very practical when doing active learning and you only want the filenames newly added to the tag.\r\n#### `ActiveLearningAgent` has new attribute `added_set`\r\nIf you prefer getting the newly added samples from the active learning agent, just access its new attribute `added_set`\r\n\r\n## Minor Updates and Fixes\r\nUpdated documentation and docstrings to make working with lightly simpler.\r\nMinor bug fixes and improvements.\r\n\r\n## Models\r\n\r\n- [Barlow Twins: Self-Supervised Learning via Redundancy Reduction, 2021](https://arxiv.org/abs/2103.03230)\r\n- [SimSiam: Exploring Simple Siamese Representation Learning, 2020](https://arxiv.org/abs/2011.10566)\r\n- [MoCo: Momentum Contrast for Unsupervised Visual Representation Learning, 2019](https://arxiv.org/abs/1911.05722)\r\n- [SimCLR: A Simple Framework for Contrastive Learning of Visual Representations, 2020](https://arxiv.org/abs/2002.05709)\r\n",
        "dateCreated": "2021-04-22T13:56:30Z",
        "datePublished": "2021-04-22T14:07:55Z",
        "html_url": "https://github.com/lightly-ai/lightly/releases/tag/v1.1.6",
        "name": "More Powerful CLI Commands, Stability Improvements and Updated Documentation",
        "tag_name": "v1.1.6",
        "tarball_url": "https://api.github.com/repos/lightly-ai/lightly/tarball/v1.1.6",
        "url": "https://api.github.com/repos/lightly-ai/lightly/releases/41844580",
        "zipball_url": "https://api.github.com/repos/lightly-ai/lightly/zipball/v1.1.6"
      },
      {
        "authorType": "User",
        "author_name": "MalteEbner",
        "body": "# Hypersphere Loss,  Stability Improvements and Updated Documentation\r\n\r\n## Hypersphere Loss (@EelcoHoogendoorn)\r\nImplemented the loss function [described here](https://arxiv.org/abs/2005.10242), which achieves competitive results with more cited ones (symmetric negative cosine similarity & contrastive loss) while providing better interpretability.\r\n\r\nYou can use the loss in combination with all other losses supported by lightly:\r\n```python\r\n# initialize loss function\r\nloss_fn = HypersphereLoss()\r\n\r\n# generate two random transforms of images\r\nt0 = transforms(images)\r\nt1 = transforms(images)\r\n\r\n# feed through (e.g. SimSiam) model\r\nout0, out1 = model(t0, t1)\r\n\r\n# calculate loss\r\nloss = loss_fn(out0, out1)\r\n```\r\n\r\nThank you, @EelcoHoogendoorn, for your contribution\r\n\r\n## Minor Updates and Fixes\r\nUpdated documentation and docstrings to make working with lightly simpler.\r\nMinor bug fixes and improvements.\r\n\r\n## Models\r\n\r\n- [Barlow Twins: Self-Supervised Learning via Redundancy Reduction, 2021](https://arxiv.org/abs/2103.03230)\r\n- [SimSiam: Exploring Simple Siamese Representation Learning, 2020](https://arxiv.org/abs/2011.10566)\r\n- [MoCo: Momentum Contrast for Unsupervised Visual Representation Learning, 2019](https://arxiv.org/abs/1911.05722)\r\n- [SimCLR: A Simple Framework for Contrastive Learning of Visual Representations, 2020](https://arxiv.org/abs/2002.05709)\r\n",
        "dateCreated": "2021-04-15T08:48:14Z",
        "datePublished": "2021-04-15T08:56:30Z",
        "html_url": "https://github.com/lightly-ai/lightly/releases/tag/v1.1.5",
        "name": "Hypersphere Loss,  Stability Improvements and Updated Documentation",
        "tag_name": "v1.1.5",
        "tarball_url": "https://api.github.com/repos/lightly-ai/lightly/tarball/v1.1.5",
        "url": "https://api.github.com/repos/lightly-ai/lightly/releases/41465651",
        "zipball_url": "https://api.github.com/repos/lightly-ai/lightly/zipball/v1.1.5"
      },
      {
        "authorType": "User",
        "author_name": "philippmwirth",
        "body": "# Consistency Regularization, CLI update, and API client update\r\n\r\n## Consistency Regularization\r\nThis release contains an implementation of the [CO2 (consistency contrast) regularization](https://arxiv.org/abs/2010.02217) which can be used together with our contrastive loss function. We observed consistent (although marginal) improvements when applying the regularizer to our models!\r\n\r\n## lightly-version\r\nA new CLI command was added to enable users to easily check the installed version from the command line. This is especially useful when working with different environments and it's not clear which version of lightly is being used. The command is:\r\n```\r\n> lightly-version\r\n1.0.4\r\n```\r\n\r\n## API client\r\nMinor updates to the API client were made enabling lightly to send `exif` data of images to the API and to make sampling requests with the sampling method `ACTIVE_LEARNING` which simply returns the samples with the highest active learning score.\r\n\r\n## Models\r\n\r\n- [Barlow Twins: Self-Supervised Learning via Redundancy Reduction, 2021](https://arxiv.org/abs/2103.03230)\r\n- [SimSiam: Exploring Simple Siamese Representation Learning, 2020](https://arxiv.org/abs/2011.10566)\r\n- [MoCo: Momentum Contrast for Unsupervised Visual Representation Learning, 2019](https://arxiv.org/abs/1911.05722)\r\n- [SimCLR: A Simple Framework for Contrastive Learning of Visual Representations, 2020](https://arxiv.org/abs/2002.05709)\r\n",
        "dateCreated": "2021-04-01T08:38:26Z",
        "datePublished": "2021-04-01T08:51:34Z",
        "html_url": "https://github.com/lightly-ai/lightly/releases/tag/v1.1.4",
        "name": "Consistency Regularization, CLI update, and API client update",
        "tag_name": "v1.1.4",
        "tarball_url": "https://api.github.com/repos/lightly-ai/lightly/tarball/v1.1.4",
        "url": "https://api.github.com/repos/lightly-ai/lightly/releases/40806789",
        "zipball_url": "https://api.github.com/repos/lightly-ai/lightly/zipball/v1.1.4"
      },
      {
        "authorType": "User",
        "author_name": "philippmwirth",
        "body": "# New Augmentation (Solarization) and Updates to README and Docs\r\n\r\n## Solarization\r\nSolarization is an augmentation which inverts all pixels above a given threshold. It is being applied in many papers about self-supervised learning. For example, in [BYOL](https://arxiv.org/abs/2006.07733) and [Barlow Twins](https://arxiv.org/abs/2103.03230).\r\n\r\n## Updates to README and Docs (multi GPU training)\r\nThe README received a code example to show how to use lightly. The documentation was polished and received a section about how to use lightly with multiple GPUs.\r\n\r\n## Experimental: Active Learning Scorers for Object Detection\r\nScorers for active learning with object detection were added. These scorers will not work with the API yet and are therefore also not yet documented.\r\n\r\n## Models\r\n\r\n- [Barlow Twins: Self-Supervised Learning via Redundancy Reduction, 2021](https://arxiv.org/abs/2103.03230)\r\n- [SimSiam: Exploring Simple Siamese Representation Learning, 2020](https://arxiv.org/abs/2011.10566)\r\n- [MoCo: Momentum Contrast for Unsupervised Visual Representation Learning, 2019](https://arxiv.org/abs/1911.05722)\r\n- [SimCLR: A Simple Framework for Contrastive Learning of Visual Representations, 2020](https://arxiv.org/abs/2002.05709)\r\n",
        "dateCreated": "2021-03-23T08:57:35Z",
        "datePublished": "2021-03-23T09:08:39Z",
        "html_url": "https://github.com/lightly-ai/lightly/releases/tag/v1.1.3",
        "name": "New Augmentation (Solarization) and Updates to README and Docs",
        "tag_name": "v1.1.3",
        "tarball_url": "https://api.github.com/repos/lightly-ai/lightly/tarball/v1.1.3",
        "url": "https://api.github.com/repos/lightly-ai/lightly/releases/40208863",
        "zipball_url": "https://api.github.com/repos/lightly-ai/lightly/zipball/v1.1.3"
      },
      {
        "authorType": "User",
        "author_name": "IgorSusmelj",
        "body": "# Barlow Twins,  a New Benchmarking Module and Updated Documentation\r\n\r\n## Barlow Twins (@AdrianArnaiz)\r\nAn implementation of the Barlow Twins architecture and loss for self-supervised learning is added. The approach measures the cross-correlation matrix between the outputs of two identical networks and making it as similar to the unit matrix as possible.\r\n\r\nThank you @AdrianArnaiz for your contribution\r\n\r\n## Benchmarking Module\r\nA [benchmarking module](https://docs.lightly.ai/lightly.utils.html#module-lightly.utils.benchmarking) is added for simpler evaluation of models using kNN callback. \r\n\r\n## API Updates: Lightly Platform\r\nYou can now easily download your datasets from the [Lightly Platform](https://app.lightly.ai) using the CLI:\r\n```bash\r\nlightly-download token=123 dataset_id=xyz output_dir=store/dataset/here\r\nlightly-download token=123 dataset_id=xyz tag_name=my-tag output_dir=store/tag/here\r\n```\r\n## Minor Updates and Fixes\r\nUpdated documentation and docstrings to make working with lightly simpler.\r\n`transforms` can now be passed directly to the `LightlyDataset`. [Learn more here](https://docs.lightly.ai/lightly.data.html#module-lightly.data.dataset).\r\nMinor bug fixes and improvements.\r\n\r\n## Models\r\n\r\n- [Barlow Twins: Self-Supervised Learning via Redundancy Reduction, 2021](https://arxiv.org/abs/2103.03230)\r\n- [SimSiam: Exploring Simple Siamese Representation Learning, 2020](https://arxiv.org/abs/2011.10566)\r\n- [MoCo: Momentum Contrast for Unsupervised Visual Representation Learning, 2019](https://arxiv.org/abs/1911.05722)\r\n- [SimCLR: A Simple Framework for Contrastive Learning of Visual Representations, 2020](https://arxiv.org/abs/2002.05709)\r\n",
        "dateCreated": "2021-03-18T11:29:33Z",
        "datePublished": "2021-03-18T11:34:04Z",
        "html_url": "https://github.com/lightly-ai/lightly/releases/tag/v1.1.2",
        "name": "Barlow Twins,  a New Benchmarking Module and Updated Documentation",
        "tag_name": "v1.1.2",
        "tarball_url": "https://api.github.com/repos/lightly-ai/lightly/tarball/v1.1.2",
        "url": "https://api.github.com/repos/lightly-ai/lightly/releases/39971130",
        "zipball_url": "https://api.github.com/repos/lightly-ai/lightly/zipball/v1.1.2"
      },
      {
        "authorType": "User",
        "author_name": "philippmwirth",
        "body": "# Fix Imports\r\nFixes a bug introduced in the last release.\r\n\r\n**Possible in v1.1.1 but not in v1.1.0:**\r\n```Python\r\nimport lightly\r\ndataset = lightly.data.LightlyDataset(input_dir='my/dataset')\r\n```\r\n\r\n## Models\r\n- [SimSiam: Exploring Simple Siamese Representation Learning, 2020](https://arxiv.org/abs/2011.10566)\r\n- [MoCo: Momentum Contrast for Unsupervised Visual Representation Learning, 2019](https://arxiv.org/abs/1911.05722)\r\n- [SimCLR: A Simple Framework for Contrastive Learning of Visual Representations, 2020](https://arxiv.org/abs/2002.05709)",
        "dateCreated": "2021-03-12T11:01:56Z",
        "datePublished": "2021-03-12T11:05:38Z",
        "html_url": "https://github.com/lightly-ai/lightly/releases/tag/v1.1.1",
        "name": "Fix Imports",
        "tag_name": "v1.1.1",
        "tarball_url": "https://api.github.com/repos/lightly-ai/lightly/tarball/v1.1.1",
        "url": "https://api.github.com/repos/lightly-ai/lightly/releases/39708773",
        "zipball_url": "https://api.github.com/repos/lightly-ai/lightly/zipball/v1.1.1"
      },
      {
        "authorType": "User",
        "author_name": "philippmwirth",
        "body": "# Lightly gets support for Active-Learning\r\nWe're excited to offer our [new active-learning functionality](https://docs.lightly.ai/getting_started/active_learning.html)! Use the strong representations learned in a self-supervised fashion together with model predictions to further improve the data selection process.\r\n\r\nThis release introduces breaking changes with respect to the API calls.\r\n\r\n## Active-Learning\r\nThe self-supervised representations together with the model predictions provide a great basis for deciding which samples should be annotated and which ones are redundant.\r\n\r\nThis release brings a completely new interface with which you can add active-learning to your ML project with just a few lines of code.:\r\n\r\n- ApiWorkflowClient: `lightly.api.api_workflow_client.ApiWorkflowClient`\r\n    The ApiWorkflowClient is used to connect to our API. The API handles the selection of the images based on embeddings and active- learning scores. To initialize the ApiWorkflowClient, you will need the datasetId and the token from the Lightly Platform.\r\n- ActiveLearningAgent: `lightly.active_learning.agents.agent.ActiveLearningAgent`\r\n    The ActiveLearningAgent builds the client interface of our active-learning framework. It helps with indicating which images are preselected and which ones to sample from. Furthermore, one can query it to get a new batch of images. To initialize an ActiveLearningAgent you need an ApiWorkflowClient.\r\n- SamplerConfig: `lightly.active_learning.config.sampler_config.SamplerConfig`\r\n    The SamplerConfig allows the configuration of a sampling request. In particular, you can set the number of samples, the name of the resulting selection, and the SamplingMethod. Currently, you can set the SamplingMethod to one of the following:\r\n    - Random: Selects samples uniformly at random.\r\n    - Coreset: Selects samples that are diverse.\r\n    - Coral: Combines Coreset with scores to do active-learning.\r\n- Scorer: `lightly.active_learning.scorers.scorer.Scorer`\r\n    The Scorer takes as input the predictions of a pre-trained model on the set of unlabeled images. It evaluates different scores based on how certain the model is about the images and passes them to the API so the sampler can use them with Coral.\r\n\r\nCheck out our [documentation](https://docs.lightly.ai/index.html) to learn more!\r\n\r\n## API (breaking)\r\nWith the refactoring of our API, we are switching to using a generated Python client. This leads to clearer and unified endpoints, fewer errors, and better error messages. Unfortunately, this means that previous versions of the package are no longer compatible with our new API.\r\n**Note that this only affects all API calls. Using the package for self-supervised learning is unaffected.**\r\n\r\n## Models\r\n- [SimSiam: Exploring Simple Siamese Representation Learning, 2020](https://arxiv.org/abs/2011.10566)\r\n- [MoCo: Momentum Contrast for Unsupervised Visual Representation Learning, 2019](https://arxiv.org/abs/1911.05722)\r\n- [SimCLR: A Simple Framework for Contrastive Learning of Visual Representations, 2020](https://arxiv.org/abs/2002.05709)",
        "dateCreated": "2021-03-11T11:21:40Z",
        "datePublished": "2021-03-11T12:18:24Z",
        "html_url": "https://github.com/lightly-ai/lightly/releases/tag/v1.1.0",
        "name": "Self-supervised Active Learning",
        "tag_name": "v1.1.0",
        "tarball_url": "https://api.github.com/repos/lightly-ai/lightly/tarball/v1.1.0",
        "url": "https://api.github.com/repos/lightly-ai/lightly/releases/39648578",
        "zipball_url": "https://api.github.com/repos/lightly-ai/lightly/zipball/v1.1.0"
      },
      {
        "authorType": "User",
        "author_name": "philippmwirth",
        "body": "# SimSiam and Refactoring of Models and Dataset \r\n\r\nThis release contains breaking changes. The models `SimCLR` and `MoCo`, the `LightlyDataset`, and the `BaseCollateFunction` were refactored. These changes were necessary to make the code base better understandable.\r\n\r\n## SimSiam (@busycalibrating)\r\nAn implementation of the [SimSiam](https://arxiv.org/abs/2011.10566) self-supervised framework is introduced. It relies on a siamese network architecture and aims to maximize similarity between two augmentations of one image.\r\n\r\n## Refactoring: LightlyDataset\r\nThe `LightlyDataset` is refactored such that the constructor now always expects an input directory `input_dir` which indicates where the images are stored. To use a `LightlyDataset` with any PyTorch dataset, the class method `LightlyDataset.from_torch_dataset` can be used.\r\n\r\n**1.0.7 (incompatible)**\r\n```python \r\n>>> dataset = LightlyDataset(from_folder='path/to/data')\r\n>>>\r\n>>> dataset = LightlyDataset(root='./', name='cifar10', download=True)\r\n```\r\n\r\n**1.0.8**\r\n```python\r\n>>> dataset = LightlyDataset(input_dir='path/to/data')\r\n>>>\r\n>>> torch_dataset = torchvision.datasets.CIFAR10(root='./', download=True)\r\n>>> dataset = LightlyDataset.from_torch_dataset(torch_dataset)\r\n```\r\n\r\n## Refactoring: BaseCollateFunction\r\nThe `BaseCollateFunction` now returns a tuple of augmented image batches along with the labels and filenames `(aug0, aug1), labels, filenames` where `aug0` and `aug1` are both of shape `bsz x channels x H x W`.\r\n\r\n## Refactoring: SimCLR, MoCo and NTXentLoss\r\nIn accordance with the changes of the `BaseCollateFunction`, `SimCLR` and `MoCo` will expect the augmented images seperately now instead of as a single batch. Similarly, the `NTXentLoss` now requires a separate batch of representations as inputs.\r\n\r\n**1.0.7 (incompatible)**\r\n```python\r\n>>> # batch size is 128\r\n>>> batch, labels, filenames = next(iter(dataloader))\r\n>>> batch.shape\r\ntorch.Size([256, 3, 32, 32]) \r\n>>> # number of features is 64\r\n>>> y = simclr(batch)\r\n>>> y.shape\r\ntorch.Size([256, 64])\r\n>>> loss = ntx_ent_loss(y)\r\n```\r\n\r\n**1.0.8**\r\n```python\r\n>>> # batch size is 128\r\n>>> (batch0, batch1), labels, filenames = next(iter(dataloader))\r\n>>> batch0.shape\r\ntorch.Size([128, 3, 32, 32])    \r\n>>> batch1.shape\r\ntorch.Size([128, 3, 32, 32]) \r\n>>> # number of features is 64\r\n>>> y0, y1 = simclr(batch0, batch1)\r\n>>> y0.shape\r\ntorch.Size([128, 64])\r\n>>> y1.shape\r\ntorch.Size([128, 64])\r\n>>> loss = ntx_ent_loss(y0, y1)\r\n```\r\n## Documentation Updates\r\nA tutorial about how to use the SimSiam model is added along with some minor changes and improvements.\r\n\r\n## Minor Changes\r\nPrivate functions are hidden from autocompletion.\r\n\r\n## Models\r\n- [SimSiam: Exploring Simple Siamese Representation Learning](https://arxiv.org/abs/2011.10566)\r\n- [MoCo: Momentum Contrast for Unsupervised Visual Representation Learning, 2019](https://arxiv.org/abs/1911.05722)\r\n- [SimCLR: A Simple Framework for Contrastive Learning of Visual Representations, 2020](https://arxiv.org/abs/2002.05709)\r\n",
        "dateCreated": "2021-01-11T10:17:20Z",
        "datePublished": "2021-01-11T11:30:44Z",
        "html_url": "https://github.com/lightly-ai/lightly/releases/tag/v1.0.8",
        "name": "SimSiam and Refactoring of Models and Dataset ",
        "tag_name": "v1.0.8",
        "tarball_url": "https://api.github.com/repos/lightly-ai/lightly/tarball/v1.0.8",
        "url": "https://api.github.com/repos/lightly-ai/lightly/releases/36219072",
        "zipball_url": "https://api.github.com/repos/lightly-ai/lightly/zipball/v1.0.8"
      },
      {
        "authorType": "User",
        "author_name": "philippmwirth",
        "body": "# Video Reader Backend\r\n\r\n## Torchvision Video Reader Compatability\r\nIf available, the video loader can use the torchvision video reader backend to load frames quicker.\r\nThe new sequential video loader also allows much faster iteration through frames when they are processed in order.\r\n\r\n## Continous Testing\r\nIntegration of continuous unit testing with a badge in the README.\r\n\r\n## New SimCLR Tutorial\r\nSimCLR with only a few lines of code - tutorial on a clothing dataset.\r\n\r\n## Models\r\n- MoCo: [Momentum Contrast for Unsupervised Visual Representation Learning, 2019](https://arxiv.org/abs/1911.05722)\r\n- SimCLR: [A Simple Framework for Contrastive Learning of Visual Representations, 2020](https://arxiv.org/abs/2002.05709)\r\n",
        "dateCreated": "2020-12-17T17:21:07Z",
        "datePublished": "2020-12-17T17:29:23Z",
        "html_url": "https://github.com/lightly-ai/lightly/releases/tag/v1.0.7",
        "name": "Video Reader Backend",
        "tag_name": "v1.0.7",
        "tarball_url": "https://api.github.com/repos/lightly-ai/lightly/tarball/v1.0.7",
        "url": "https://api.github.com/repos/lightly-ai/lightly/releases/35431235",
        "zipball_url": "https://api.github.com/repos/lightly-ai/lightly/zipball/v1.0.7"
      },
      {
        "authorType": "User",
        "author_name": "philippmwirth",
        "body": "# Custom Backbones Made Easy\r\n\r\n## Decoupling Self-Supervised Models from ResNet\r\nThe implementation of SimCLR and MoCo have been changed such that they can now be constructed from an arbitrary backbone network.\r\nFurthermore, the backbone of the self-supervised models is now called `backbone` instead of `features`.\r\n\r\n## Big Documentation Update\r\nThe documentation has received a lot of love and improvements to make working with `lightly` easier.\r\nThere is also a new tutorial on [how to train MoCo on Cifar-10](https://docs.lightly.ai/tutorials/package/tutorial_moco_memory_bank.html).\r\n\r\n## Minor Changes\r\nThe `LightlyDataset` can now be passed a list of indices marking relevant samples. Non-relevant samples will be ignored during further processing of the data.\r\n\r\n## Models\r\n- MoCo: [Momentum Contrast for Unsupervised Visual Representation Learning, 2019](https://arxiv.org/abs/1911.05722)\r\n- SimCLR: [A Simple Framework for Contrastive Learning of Visual Representations, 2020](https://arxiv.org/abs/2002.05709)\r\n",
        "dateCreated": "2020-12-10T15:05:04Z",
        "datePublished": "2020-12-10T15:15:54Z",
        "html_url": "https://github.com/lightly-ai/lightly/releases/tag/v1.0.6",
        "name": "Custom Backbones Made Easy",
        "tag_name": "v1.0.6",
        "tarball_url": "https://api.github.com/repos/lightly-ai/lightly/tarball/v1.0.6",
        "url": "https://api.github.com/repos/lightly-ai/lightly/releases/35105203",
        "zipball_url": "https://api.github.com/repos/lightly-ai/lightly/zipball/v1.0.6"
      },
      {
        "authorType": "User",
        "author_name": "philippmwirth",
        "body": "# Customizable Checkpoint Callbacks, Batch Shuffling and More\r\n\r\nFixed download speed for image datasets.\r\n`lightly-magic` can now be used with `trainer.max_epochs=0`.\r\nFixed the pytorch-lightning warning: \"Passing a ModelCheckpoint instance to Trainer(checkpoint_callbacks=...) is deprecated since v1.1 and will no longer be supported in v1.3.\"\r\n\r\n## Customizable Checkpoint Callback\r\nCheckpoint callbacks are now customizable (even from the command-line):\r\n```bash\r\n# save the 5 best models\r\nlightly-train input_dir='data/' checkpoint_callback.save_top_k=5\r\n\r\n# don't save the model of the last epoch\r\nlightly-train input_dir='data/' checkpoint_callback.save_last=False\r\n```\r\n## Batch Shuffling\r\nAdded [batch shuffling to MoCo](https://github.com/facebookresearch/moco/blob/master/moco/builder.py) and `SplitBatchNorm` to simulate multi-gpu behaviour.\r\n\r\n## Image Resizing\r\nImages can be resized before uploading them to the web-app:\r\n```bash\r\n# no resizing (default)\r\nlightly-upload input_dir='data/' dataset_id='XYZ' token='123' resize=-1\r\n\r\n# resize such that shortest edge of the image is 128\r\nlightly-upload input_dir='data/' dataset_id='XYZ' token='123' resize=128\r\n\r\n# resize images to (128, 128)\r\nlightly-upload input_dir='data/' dataset_id='XYZ' token='123' resize=[128,128]\r\n```\r\n\r\n## Models\r\n- MoCo: [Momentum Contrast for Unsupervised Visual Representation Learning, 2019](https://arxiv.org/abs/1911.05722)\r\n- SimCLR: [A Simple Framework for Contrastive Learning of Visual Representations, 2020](https://arxiv.org/abs/2002.05709)\r\n",
        "dateCreated": "2020-11-27T10:31:06Z",
        "datePublished": "2020-11-27T10:46:14Z",
        "html_url": "https://github.com/lightly-ai/lightly/releases/tag/v1.0.5",
        "name": "Customizable Checkpoint Callbacks, Batch Shuffling and More",
        "tag_name": "v1.0.5",
        "tarball_url": "https://api.github.com/repos/lightly-ai/lightly/tarball/v1.0.5",
        "url": "https://api.github.com/repos/lightly-ai/lightly/releases/34498547",
        "zipball_url": "https://api.github.com/repos/lightly-ai/lightly/zipball/v1.0.5"
      }
    ],
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- hydra-core>=1.0.0\n- numpy>=1.18.1\n- pytorch_lightning>=1.0.4 \n- requests>=2.23.0\n- torchvision\n- tqdm\n\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Unit tests are within the `tests` folder and we recommend to run them using \n[pytest](https://docs.pytest.org/en/stable/).\nThere are two test configurations available. By default only a subset will be run.\nThis is faster and should take less than a minute. You can run it using\n```\npython -m pytest -s -v\n```\n\nTo run all tests (including the slow ones) you can use the following command.\n```\npython -m pytest -s -v --runslow\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1357,
      "date": "Fri, 24 Dec 2021 22:01:46 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "deep-learning",
      "self-supervised-learning",
      "machine-learning",
      "computer-vision",
      "pytorch",
      "embeddings",
      "contrastive-learning",
      "active-learning"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Want to jump to the tutorials and see lightly in action?\n\n- [Train MoCo on CIFAR-10](https://docs.lightly.ai/tutorials/package/tutorial_moco_memory_bank.html)\n- [Train SimCLR on clothing data](https://docs.lightly.ai/tutorials/package/tutorial_simclr_clothing.html)\n- [Train SimSiam on satellite images](https://docs.lightly.ai/tutorials/package/tutorial_simsiam_esa.html)\n- [Use lightly with custom augmentations](https://docs.lightly.ai/tutorials/package/tutorial_custom_augmentations.html)\n\nTutorials of using the lightly packge together with the Lightly Platform:\n\n- [Active Learning using Detectron2 on Comma10k](https://docs.lightly.ai/tutorials/platform/tutorial_active_learning_detectron2.html)\n- [Active Learning with the Nvidia TLT](https://github.com/lightly-ai/NvidiaTLTActiveLearning)\n\nCommunity and partner projects:\n\n- [On-Device Deep Learning with Lightly on an ARM microcontroller](https://github.com/ARM-software/EndpointAI/tree/master/ProofOfConcepts/Vision/OpenMvMaskDefaults)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Lightly requires **Python 3.6+**. We recommend installing Lightly in a **Linux** or **OSX** environment.\n\n",
      "technique": "Header extraction"
    }
  ]
}