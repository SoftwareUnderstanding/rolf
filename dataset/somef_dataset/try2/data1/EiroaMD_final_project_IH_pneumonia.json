{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1409.1556](https://arxiv.org/abs/1409.1556",
      "https://arxiv.org/abs/1409.1556"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[https://web.stanford.edu/class/cs231a/lectures/intro_cnn.pdf](https://web.stanford.edu/class/cs231a/lectures/intro_cnn.pdf)\n\n[https://towardsdatascience.com/simply-deep-learning-an-effortless-introduction-45591a1c4abb](https://towardsdatascience.com/simply-deep-learning-an-effortless-introduction-45591a1c4abb)\n\n[https://towardsdatascience.com/basics-of-image-classification-with-keras-43779a299c8b](https://towardsdatascience.com/basics-of-image-classification-with-keras-43779a299c8b)\n\n[https://developers.google.com/machine-learning/practica/image-classification](https://developers.google.com/machine-learning/practica/image-classification?authuser=2)\n\n[https://www.freecodecamp.org/news/how-to-build-the-best-image-classifier-3c72010b3d55/](https://www.freecodecamp.org/news/how-to-build-the-best-image-classifier-3c72010b3d55/)\n\n[https://medium.com/merantix/deep-learning-from-natural-to-medical-images-74827bf51d6b](https://medium.com/merantix/deep-learning-from-natural-to-medical-images-74827bf51d6b)\n\n[https://towardsdatascience.com/running-jupyter-notebook-in-google-cloud-platform-in-15-min-61e16da34d52](https://towardsdatascience.com/running-jupyter-notebook-in-google-cloud-platform-in-15-min-61e16da34d52)\n\n[https://thispointer.com/python-how-to-move-files-and-directories/](https://thispointer.com/python-how-to-move-files-and-directories/)\n\n[https://arxiv.org/abs/1409.1556](https://arxiv.org/abs/1409.1556)\n\n[https://towardsdatascience.com/neural-style-transfer-4d7c8138e7f6](https://towardsdatascience.com/neural-style-transfer-4d7c8138e7f6)\n\n[https://medium.com/@vijayabhaskar96/multi-label-image-classification-tutorial-with-keras-imagedatagenerator-cd541f8eaf24](https://medium.com/@vijayabhaskar96/multi-label-image-classification-tutorial-with-keras-imagedatagenerator-cd541f8eaf24)\n\n[https://towardsdatascience.com/transfer-learning-in-tensorflow-9e4f7eae3bb4](https://towardsdatascience.com/transfer-learning-in-tensorflow-9e4f7eae3bb4)\n\n[https://marubon-ds.blogspot.com/2017/09/vgg-fine-tuning-model.html](https://marubon-ds.blogspot.com/2017/09/vgg-fine-tuning-model.html)\n\n[https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/](https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/)\n\n[https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a)\n\n[https://machinelearningmastery.com/how-to-calculate-precision-recall-f1-and-more-for-deep-learning-models/](https://machinelearningmastery.com/how-to-calculate-precision-recall-f1-and-more-for-deep-learning-models/)\n\n[https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/](https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/)\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "  - Validation subset (10%) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/EiroaMD/final_project_IH_pneumonia",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-11-16T09:56:02Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-02-26T11:15:22Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "26684 [DICOM](https://es.wikipedia.org/wiki/DICOM) files. (3.53 GB)\n\n     - stage_2_train_labels.csv (26684, 6) (1.49 MB)\n        - Patient Id: nominal variable. File names correspond to patient Id.\n        - X and Y: the center of the boxes for the segmentation part.\n        - height and width: the dimension of the box for the segmentation part.\n        - target: binary. 0-normal and 1-pneumonia.\n        \n    - stage_2_detailed_class_info.csv (26684, 2) (1.69 MB)\n        - Patient Id: nominal variable. File names correspond to patient Id.\n        - Class: nominal variable with three possible values:\n            - 'Normal'\n            - 'Not normal/Not pneumonia'\n            - 'Pneumonia'\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8225126298788366,
        0.999659749458849,
        0.9862223783668171,
        0.9642920002289929,
        0.8979411005071259,
        0.8979411005071259,
        0.9637680887969974,
        0.8795556503480305,
        0.8910185959776531
      ],
      "excerpt": "Final Project | Dani Eiroa | IH BCN Data Analytics PT 2019 \nA pneumonia is an acute infection of the lung and is characterized by the appearance of fever and respiratory symptoms, together with the presence of lung opacities on Chest-X-Rays (CXR). \nAccording to the Spanish Society or Radiology (SERAM), there are no reliable data on the number of (CXR) not reported in Spain, although there is a widespread conviction that, with exceptions, radiology services have never reported 100% of them. There are hospitals that, in fact, have stopped reporting CXR, as the workload has been inclined towards the reporting of more complicated techniques, such as CT and MRI. \nThroughout the process described below, we aim to develop a tool that classifies a given CXR in one of this two classes: Normal and Pneumonia. \nData Preparation \nData Ingestion \nData Wrangling and Cleaning \nData Analysis \nData Exploration and Visualization \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8838344713293736,
        0.9560108096274608,
        0.847390981436405,
        0.8897008071782261
      ],
      "excerpt": "Dataset was downloaded and unzziped using python functions defined in all three files of step 2, as different strategies were carried out to try to solve the problem. \nCSVs were loaded using pandas .read_csv() method and explored accordingly (see 'Data Wrangling and Cleaning'). \nThe original kaggle challenge included a segmentation task. For this reason, there were columns showing the coordinates of the pneumonic opacities and patient Id's that were repeated (some X-rays contained more than one pneumonic opacities). Those columns and duplicate rows were dropped. The two .csv files were joined by patiend Id. \nUsing specific python libraries (see 'Tools' below), the following metadata from DICOM files were extracted and added to the dataframe: age, sex and X-ray projection/view (antero-posterior or postero-anterior). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8645838399194606
      ],
      "excerpt": "        - Just in case the information had to be fed to the algorithm in that format. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9833292001434002,
        0.9280329400822998,
        0.9402251356983695,
        0.9640475125130538
      ],
      "excerpt": "Throughout the process, a few .csvfiles were generated, to keep record of the files belonging to the different subsets. They are stored in the /data folder of the github repository. \nThe heft of the files was stored on a Google Cloud virtual machine with GPUs, where the  model training was carried out. \nAn Exploratory Data Analysis was executed. Overall, data was clean and tidy. There were no NaNs and only five outliers in the only numerical variable present (age). After exploring the ages of the outliers (around 150 years old) and their CXRs, they were corrected by subtracting 100 years. \nAnalysis of the value counts of the different categorical variables was done. Special attention was given to the possible target variables, the binary (target) and the ternary (class). There are two possible classifications of the X-Rays: normal|pneumonia and normal|not-normal-not-pneumonia|pneumonia. The image below shows that any of the two possible classifications yields unbalanced data, which was a pain point during the whole process. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8519410058507305,
        0.8519410058507305,
        0.9739886789133845,
        0.8348945726081531,
        0.8850191494579739,
        0.9193704560771838,
        0.9790335738670007,
        0.9778118615909568,
        0.9185056108366116,
        0.9799429226143431,
        0.9955678849418395,
        0.839820948556802,
        0.8011976468233448
      ],
      "excerpt": "<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\large&space;H0:&space;\\bar{x}&space;^{normal}&space;=&space;\\bar{x}&space;^{pneumonia}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\large&space;H0:&space;\\bar{x}&space;^{normal}&space;=&space;\\bar{x}&space;^{pneumonia}\" title=\"\\large H0: \\bar{x} ^{normal} = \\bar{x} ^{pneumonia}\" /></a> \n<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\large&space;H1:&space;\\bar{x}&space;^{normal}&space;\\neq&space;\\bar{x}&space;^{pneumonia}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\large&space;H1:&space;\\bar{x}&space;^{normal}&space;\\neq&space;\\bar{x}&space;^{pneumonia}\" title=\"\\large H1: \\bar{x} ^{normal} \\neq \\bar{x} ^{pneumonia}\" /></a> \np-value of 3.27e-13. H0 is rejected. We can't conclude that there are no differences between mean age of people with pneumonia, compared to healthy subjects. \nFor the Ternary target - ANOVA: \n<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\large&space;H0:&space;\\bar{x}&space;^{normal}&space;=&space;\\bar{x}&space;^{nnnp}&space;=&space;\\bar{x}&space;^{pneumonia}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\large&space;H0:&space;\\bar{x}&space;^{normal}&space;=&space;\\bar{x}&space;^{nnnp}&space;=&space;\\bar{x}&space;^{pneumonia}\" title=\"\\large H0: \\bar{x} ^{normal} = \\bar{x} ^{nnnp} = \\bar{x} ^{pneumonia}\" /></a> \n<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\large&space;H1:&space;Means\\hspace{2mm}&space;are\\hspace{2mm}&space;not\\hspace{2mm}&space;all\\hspace{2mm}&space;equal.\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\large&space;H1:&space;Means\\hspace{2mm}&space;are\\hspace{2mm}&space;not\\hspace{2mm}&space;all\\hspace{2mm}&space;equal.\" title=\"\\large H1: Means\\hspace{2mm} are\\hspace{2mm} not\\hspace{2mm} all\\hspace{2mm} equal.\" /></a> \np-value of 5.82e-90. H0 is rejected. We can't conclude that there are no differences between mean age of people with pneumonia, compared with the other two groups. \nInitially, the three-category approach was decided, because the unbalance was not as big as with the binary approach, but a good result couldn't be achieved, the accuracy was not bad, but the rest of the metrics did not satisfy our standards and the model overfitted widely, so finally a binary classification with undersampling of the normal. \nIn brief, we ended up with a total of 12024 images (6012 normal - 6012 pneumonia) and a dataframe with a shape of (12024, 12), as well as three subsets of it (train, validation, and test). \nAfter some research on the matter, we came to the conclusion that the best models for image classification tasks are Convolutional Neural Networks (CNN) \nBriefly, a CNN consists on an input, layers of filters that apply mathematical operations to the pixel values of the image (convolution and pooling) and  a cluster of fully connected or dense layers on the end of the network. This layers bound together produce an output, that in our cases is a category: normal or pneumonia. \nBefore feeding the images to the CNN, they had to be preprocessed using the ImageDataGenerator function from keras. \nSummary of the most important parameters used: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.889582173575955,
        0.9888064741798299,
        0.9479247767395829,
        0.8011976468233448
      ],
      "excerpt": "A lot of parameter hypertuning was performed throughout the process, although the most relevant to the final results were the addition of dropout layers, which dramatically decreased overfitting and the selection of the softmax function on the output layer (the initial output function chosen was the sigmoid). \nAlthough satisfying results are yielded by this model (around 90% accuracy and F1-score) with a low proportion of false negatives (which in this particular case would be worse than false positives - i.e. it is better to treat a patient with no pneumonia that leave a sick patient with no treatment), there was quite a bit of overfitting, despite extensive hypertuning. \nFor that reason it was decided to try to apply Transfer Learning techniques to the problem. Layers of the pretrained VGG-19 network were used, combined with self-added fully connected layers. \nSummary of the most important parameters used: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9100689084892593,
        0.9750797111772573
      ],
      "excerpt": "Although clean and tidy, the data presented a problem of imbalance, with a relatively low number of pneumonia images (6012/26684). \nThere is a slight difference in terms of age, between subjects that have a normal X-Ray and subjects with pneumonia. That difference is statistically significant. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9848040004156846,
        0.9867879890181229
      ],
      "excerpt": "A CNN was built from scratch with relatively good performance (f1-score of 0.89), and a low number of false negatives, which in this particular case (a person with pneumonia leaving the hospital without treatment) \nFurthermore, with techniques of transfer-learning, another CNN was built and trained, achieving better metrics (f1-score of 0.92), and minimizing the overfitting. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/EiroaMD/final_project_IH_pneumonia/releases",
    "technique": "GitHub API"
  },
  "faq": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The main pain points throughout the process were:\n- Dealing with unbalance: after trying to solve the problem with the unbalanced raw data and discarding the idea of oversampling, as we are dealing with medical images, I finally decided to do an undersampling of the normal images to balance both classes.\n    - **Learning:** when dealing with classification problems, better quality than quantity. From now on, I'd rather have a small good, balanced dataset than a big one that I have to undersample anyway.\n- DICOM: I learnt the hard way that `.dcm` files could not be fed to a CNN. I used some libraries (see \"Tools\") to convert and move images using Python.\n    - **Learning:** DICOM is a very versatile format and you can do a lot of file management just by using Python.\n- Accuracy is not everything. Actually, it's nothing: first time I trained the CNN, with unbalanced binary classes, I got an accuracy of almost 75%. It turned out that the CNN classified all X-rays as normal (75% of the train image set was normal).\n    - **Learning:** looking at all the metrics is vital, as they provide different information and help to identify problems within the model.\n- Overfitting: it never ceases to amaze me the difficulty of adequately dealing and correcting overfitting.\n    - **Learning:** hypertune and iterate until you die.\n",
      "technique": "Header extraction"
    }
  ],
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Thu, 23 Dec 2021 05:51:26 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/EiroaMD/final_project_IH_pneumonia/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "EiroaMD/final_project_IH_pneumonia",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/EiroaMD/final_project_IH_pneumonia/master/3%20EDA/3_EDA.ipynb",
      "https://raw.githubusercontent.com/EiroaMD/final_project_IH_pneumonia/master/4%20Model%20Training/4_3_Model_Binary_Transfer_VGG19_10_epochs.ipynb",
      "https://raw.githubusercontent.com/EiroaMD/final_project_IH_pneumonia/master/4%20Model%20Training/4_4_Model_Binary_Transfer_VGG19_7_epochs.ipynb",
      "https://raw.githubusercontent.com/EiroaMD/final_project_IH_pneumonia/master/4%20Model%20Training/4_2_Model_Binary_from_directory_balanced.ipynb",
      "https://raw.githubusercontent.com/EiroaMD/final_project_IH_pneumonia/master/1%20Data%20Cleaning/1_data_cleaning.ipynb",
      "https://raw.githubusercontent.com/EiroaMD/final_project_IH_pneumonia/master/5%20Testing/5_3_load_model_predict.ipynb",
      "https://raw.githubusercontent.com/EiroaMD/final_project_IH_pneumonia/master/5%20Testing/5_4_load_model_predict.ipynb",
      "https://raw.githubusercontent.com/EiroaMD/final_project_IH_pneumonia/master/5%20Testing/5_2_load_model_predict.ipynb",
      "https://raw.githubusercontent.com/EiroaMD/final_project_IH_pneumonia/master/2%20Image%20Management/2_file_classifier_t_v_t_3_classes.ipynb",
      "https://raw.githubusercontent.com/EiroaMD/final_project_IH_pneumonia/master/2%20Image%20Management/2_file_classifier_t_v_t.ipynb",
      "https://raw.githubusercontent.com/EiroaMD/final_project_IH_pneumonia/master/2%20Image%20Management/2_file_classifier_t_v_t_balanced.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The dataset is comprised of a total 26684 Chest X-Rays (CXR), some of them of patients affected by pneumonia and others with no pneumonia.\n\nThe dataset also contains two .csv files:\n  - One with the patient Id (same as original image file name), a Target column (0 for normal and 1 for pneumonia) and the coordinates for the opacities consistent with pneumonia, as the original challenge included a segmentation task.\n  - Another containing the class of the image (Opacity, Normal, Not-normal/No opacity) as well as patient Id.\n  \nThe dataset was obtained from Kaggle and provided and labeled by the Radiological Society of North America (RSNA). [Link](https://www.kaggle.com/c/rsna-pneumonia-detection-challenge/data)\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8233588558014837,
        0.833782873776028
      ],
      "excerpt": "- Numpy \n- Pandas \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8262157230141619
      ],
      "excerpt": "Data Preparation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8276092113759073
      ],
      "excerpt": "  - Training subset (80%) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8828192451660339
      ],
      "excerpt": "  - Test subset (10%) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8409016216669487
      ],
      "excerpt": "In brief, we ended up with a total of 12024 images (6012 normal - 6012 pneumonia) and a dataframe with a shape of (12024, 12), as well as three subsets of it (train, validation, and test). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8235451612644769
      ],
      "excerpt": "Softmax for output layer. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8235451612644769
      ],
      "excerpt": "Softmax for output layer. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/EiroaMD/final_project_IH_pneumonia/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": ":hospital: |Pneumonia| :hospital:",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "final_project_IH_pneumonia",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "EiroaMD",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/EiroaMD/final_project_IH_pneumonia/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Thu, 23 Dec 2021 05:51:26 GMT"
    },
    "technique": "GitHub API"
  }
}