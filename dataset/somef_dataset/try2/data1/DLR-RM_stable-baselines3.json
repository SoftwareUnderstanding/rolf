{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The initial work to develop Stable Baselines3 was partially funded by the project *Reduced Complexity Models* from the *Helmholtz-Gemeinschaft Deutscher Forschungszentren*.\n\nThe original version, Stable Baselines, was created in the [robotics lab U2IS](http://u2is.ensta-paristech.fr/index.php?lang=en) ([INRIA Flowers](https://flowers.inria.fr/) team) at [ENSTA ParisTech](http://www.ensta-paristech.fr/en).\n\n\nLogo credits: [L.M. Tenkes](https://www.instagram.com/lucillehue/)\n",
      "technique": "Header extraction"
    }
  ],
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To cite this repository in publications:\n\n```bibtex\n@article{stable-baselines3,\n  author  = {Antonin Raffin and Ashley Hill and Adam Gleave and Anssi Kanervisto and Maximilian Ernestus and Noah Dormann},\n  title   = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},\n  journal = {Journal of Machine Learning Research},\n  year    = {2021},\n  volume  = {22},\n  number  = {268},\n  pages   = {1-8},\n  url     = {http://jmlr.org/papers/v22/20-1364.html}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{stable-baselines3,\n  author  = {Antonin Raffin and Ashley Hill and Adam Gleave and Anssi Kanervisto and Maximilian Ernestus and Noah Dormann},\n  title   = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},\n  journal = {Journal of Machine Learning Research},\n  year    = {2021},\n  volume  = {22},\n  number  = {268},\n  pages   = {1-8},\n  url     = {http://jmlr.org/papers/v22/20-1364.html}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9752782612812023
      ],
      "excerpt": "Please take a look at the Roadmap and Milestones. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9809845813829983
      ],
      "excerpt": "Github repo: https://github.com/DLR-RM/rl-baselines3-zoo \n",
      "technique": "Supervised classification"
    }
  ],
  "codeOfConduct": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://raw.githubusercontent.com/DLR-RM/stable-baselines3/master/CODE_OF_CONDUCT.md",
    "technique": "File Exploration"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/DLR-RM/stable-baselines3",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contributing to Stable-Baselines3\nIf you are interested in contributing to Stable-Baselines, your contributions will fall\ninto two categories:\n1. You want to propose a new Feature and implement it\n    - Create an issue about your intended feature, and we shall discuss the design and\n    implementation. Once we agree that the plan looks good, go ahead and implement it.\n2. You want to implement a feature or bug-fix for an outstanding issue\n    - Look at the outstanding issues here: https://github.com/DLR-RM/stable-baselines3/issues\n    - Pick an issue or feature and comment on the task that you want to work on this feature.\n    - If you need more context on a particular issue, please ask and we shall provide.\nOnce you finish implementing a feature or bug-fix, please send a Pull Request to\nhttps://github.com/DLR-RM/stable-baselines3\nIf you are not familiar with creating a Pull Request, here are some guides:\n- http://stackoverflow.com/questions/14680711/how-to-do-a-github-pull-request\n- https://help.github.com/articles/creating-a-pull-request/\nDeveloping Stable-Baselines3\nTo develop Stable-Baselines3 on your machine, here are some tips:\n\nClone a copy of Stable-Baselines3 from source:\n\nbash\ngit clone https://github.com/DLR-RM/stable-baselines3\ncd stable-baselines3/\n\nInstall Stable-Baselines3 in develop mode, with support for building the docs and running tests:\n\nbash\npip install -e .[docs,tests,extra]\nCodestyle\nWe are using black codestyle (max line length of 127 characters) together with isort to sort the imports.\nPlease run make format to reformat your code. You can check the codestyle using make check-codestyle and make lint.\nPlease document each function/method and type them using the following template:\n```python\ndef my_function(arg1: type1, arg2: type2) -> returntype:\n    \"\"\"\n    Short description of the function.\n:param arg1: describe what is arg1\n:param arg2: describe what is arg2\n:return: describe what is returned\n\"\"\"\n...\nreturn my_variable\n\n```\nPull Request (PR)\nBefore proposing a PR, please open an issue, where the feature will be discussed. This prevent from duplicated PR to be proposed and also ease the code review process.\nEach PR need to be reviewed and accepted by at least one of the maintainers (@hill-a, @araffin, @ernestum, @AdamGleave or @Miffyli).\nA PR must pass the Continuous Integration tests to be merged with the master branch.\nTests\nAll new features must add tests in the tests/ folder ensuring that everything works fine.\nWe use pytest.\nAlso, when a bug fix is proposed, tests should be added to avoid regression.\nTo run tests with pytest:\nmake pytest\nType checking with pytype:\nmake type\nCodestyle check with black, isort and flake8:\nmake check-codestyle\nmake lint\nTo run pytype, format and lint in one command:\nmake commit-checks\nBuild the documentation:\nmake doc\nCheck documentation spelling (you need to install sphinxcontrib.spelling package for that):\nmake spelling\nChangelog and Documentation\nPlease do not forget to update the changelog (docs/misc/changelog.rst) and add documentation if needed.\nYou should add your username next to each changelog entry that you added. If this is your first contribution, please add your username at the bottom too.\nA README is present in the docs/ folder for instructions on how to build the documentation.\nCredits: this contributing guide is based on the PyTorch one.",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-05-05T05:52:26Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-29T17:03:59Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9872122208710024,
        0.9054255514738898,
        0.9906992316522041,
        0.9499944502180233,
        0.873415436632079
      ],
      "excerpt": "Stable Baselines3 (SB3) is a set of reliable implementations of reinforcement learning algorithms in PyTorch. It is the next major version of Stable Baselines. \nYou can read a detailed presentation of Stable Baselines3 in the v1.0 blog post or our JMLR paper. \nThese algorithms will make it easier for the research community and industry to replicate, refine, and identify new ideas, and will create good baselines to build projects on top of. We expect these tools will be used as a base around which new ideas can be added, and as a tool for comparing a new approach against existing ones. We also hope that the simplicity of these tools will allow beginners to experiment with a more advanced toolset, without being buried in implementation details. \nNote: despite its simplicity of use, Stable Baselines3 (SB3) assumes you have some knowledge about Reinforcement Learning (RL). You should not utilize this library without some practice. To that extent, we provide good resources in the documentation to get started with RL. \nThe performance of each algorithm was tested (see Results section in their respective page), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9240568874637005
      ],
      "excerpt": "| State of the art RL methods | :heavy_check_mark: | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8648084703823735,
        0.822749199304693
      ],
      "excerpt": "| Dict observation space support  | :heavy_check_mark: | \n| Ipython / Notebook friendly | :heavy_check_mark: | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.915090955984636
      ],
      "excerpt": "RL Baselines3 Zoo is a training framework for Reinforcement Learning (RL). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9712676365497585,
        0.9808772988276458
      ],
      "excerpt": "In addition, it includes a collection of tuned hyperparameters for common environments and RL algorithms, and agents trained with those settings. \nGoals of this repository: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9642285328979444,
        0.9343719621260289
      ],
      "excerpt": "We implement experimental features in a separate contrib repository: SB3-Contrib \nThis allows SB3 to maintain a stable and compact core, while still providing the latest features, like Truncated Quantile Critics (TQC), Quantile Regression DQN (QR-DQN) or PPO with invalid action masking (Maskable PPO). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9028514791447523
      ],
      "excerpt": "<b id=\"f1\">1</b>: Implemented in SB3 Contrib GitHub repository. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8460360420896518,
        0.9847221675530855
      ],
      "excerpt": " * MultiBinary: A list of possible actions, where each timestep any of the actions can be used in any combination. \nWe try to maintain a list of project using stable-baselines3 in the documentation, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9557807277020227
      ],
      "excerpt": "Stable-Baselines3 is currently maintained by Ashley Hill (aka @hill-a), Antonin Raffin (aka @araffin), Maximilian Ernestus (aka @ernestum), Adam Gleave (@AdamGleave) and Anssi Kanervisto (@Miffyli). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9199839793855858,
        0.9782073173060808
      ],
      "excerpt": "Please post your question on the RL Discord, Reddit or Stack Overflow in that case. \nTo any interested in making the baselines better, there is still some documentation that needs to be done. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "PyTorch version of Stable Baselines, reliable implementations of reinforcement learning algorithms. ",
      "technique": "GitHub API"
    }
  ],
  "documentation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Documentation is available online: [https://stable-baselines3.readthedocs.io/](https://stable-baselines3.readthedocs.io/)\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/DLR-RM/stable-baselines3/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 573,
      "date": "Thu, 30 Dec 2021 03:01:42 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/DLR-RM/stable-baselines3/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "DLR-RM/stable-baselines3",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/DLR-RM/stable-baselines3/master/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/DLR-RM/stable-baselines3/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/DLR-RM/stable-baselines3/master/scripts/run_docker_cpu.sh",
      "https://raw.githubusercontent.com/DLR-RM/stable-baselines3/master/scripts/run_docker_gpu.sh",
      "https://raw.githubusercontent.com/DLR-RM/stable-baselines3/master/scripts/build_docker.sh",
      "https://raw.githubusercontent.com/DLR-RM/stable-baselines3/master/scripts/run_tests.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "All unit tests in stable baselines3 can be run using `pytest` runner:\n```\npip install pytest pytest-cov\nmake pytest\n```\n\nYou can also do a static type check using `pytype`:\n```\npip install pytype\nmake type\n```\n\nCodestyle check with `flake8`:\n```\npip install flake8\nmake lint\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Install the Stable Baselines3 package:\n```\npip install stable-baselines3[extra]\n```\n**Note:** Some shells such as Zsh require quotation marks around brackets, i.e. `pip install 'stable-baselines3[extra]'` ([More Info](https://stackoverflow.com/a/30539963)).\n\nThis includes an optional dependencies like Tensorboard, OpenCV or `atari-py` to train on atari games. If you do not need those, you can use:\n```\npip install stable-baselines3\n```\n\nPlease read the [documentation](https://stable-baselines3.readthedocs.io/) for more details and alternatives (from source, using docker).\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "**Note:** Stable-Baselines3 supports PyTorch >= 1.8.1.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "A migration guide from SB2 to SB3 can be found in the [documentation](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8707094128560938
      ],
      "excerpt": "Github repo: https://github.com/DLR-RM/rl-baselines3-zoo \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8573889943382627
      ],
      "excerpt": "To install stable-baselines on Windows, please look at the documentation. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/DLR-RM/stable-baselines3/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell",
      "Dockerfile",
      "Makefile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'The MIT License\\n\\nCopyright (c) 2019 Antonin Raffin\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in\\nall copies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\\nTHE SOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Stable Baselines3",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "stable-baselines3",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "DLR-RM",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/DLR-RM/stable-baselines3/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "araffin",
        "body": "**WARNING: This version will be the last one supporting Python 3.6 (end of life in Dec 2021).\r\n  We highly recommend you to upgrade to Python >= 3.7.**\r\n\r\nSB3-Contrib changelog: https://github.com/Stable-Baselines-Team/stable-baselines3-contrib/releases/tag/v1.3.0\r\n\r\n## Breaking Changes:\r\n\r\n- ``sde_net_arch`` argument in policies is deprecated and will be removed in a future version.\r\n- ``_get_latent`` (``ActorCriticPolicy``) was removed\r\n- All logging keys now use underscores instead of spaces (@timokau). Concretely this changes:\r\n\r\n    - ``time/total timesteps`` to ``time/total_timesteps`` for off-policy algorithms (PPO and A2C) and the eval callback (on-policy algorithms already used the underscored version),\r\n    - ``rollout/exploration rate`` to ``rollout/exploration_rate`` and\r\n    - ``rollout/success rate`` to ``rollout/success_rate``.\r\n\r\n## New Features:\r\n\r\n- Added methods ``get_distribution`` and ``predict_values`` for ``ActorCriticPolicy`` for A2C/PPO/TRPO (@cyprienc)\r\n- Added methods ``forward_actor`` and ``forward_critic`` for ``MlpExtractor``\r\n- Added ``sb3.get_system_info()`` helper function to gather version information relevant to SB3 (e.g., Python and PyTorch version)\r\n- Saved models now store system information where agent was trained, and load functions have ``print_system_info`` parameter to help debugging load issues.\r\n\r\n## Bug Fixes:\r\n\r\n- Fixed ``dtype`` of observations for ``SimpleMultiObsEnv``\r\n- Allow `VecNormalize` to wrap discrete-observation environments to normalize reward\r\n  when observation normalization is disabled.\r\n- Fixed a bug where ``DQN`` would throw an error when using ``Discrete`` observation and stochastic actions\r\n- Fixed a bug where sub-classed observation spaces could not be used\r\n- Added ``force_reset`` argument to ``load()`` and ``set_env()`` in order to be able to call ``learn(reset_num_timesteps=False)`` with a new environment\r\n\r\n## Others:\r\n\r\n- Cap gym max version to 0.19 to avoid issues with atari-py and other breaking changes\r\n- Improved error message when using dict observation with the wrong policy\r\n- Improved error message when using ``EvalCallback`` with two envs not wrapped the same way.\r\n- Added additional infos about supported python version for PyPi in ``setup.py``\r\n\r\n## Documentation:\r\n\r\n- Add Rocket League Gym to list of supported projects (@AechPro)\r\n- Added gym-electric-motor to project page (@wkirgsn)\r\n- Added policy-distillation-baselines to project page (@CUN-bjy)\r\n- Added ONNX export instructions (@batu)\r\n- Update read the doc env (fixed ``docutils`` issue)\r\n- Fix PPO environment name (@IljaAvadiev)\r\n- Fix custom env doc and add env registration example\r\n- Update algorithms from SB3 Contrib\r\n- Use underscores for numeric literals in examples to improve clarity\r\n",
        "dateCreated": "2021-10-23T15:07:00Z",
        "datePublished": "2021-10-23T15:15:01Z",
        "html_url": "https://github.com/DLR-RM/stable-baselines3/releases/tag/v1.3.0",
        "name": "SB3 v1.3.0 : Bug fixes and improvements for the user",
        "tag_name": "v1.3.0",
        "tarball_url": "https://api.github.com/repos/DLR-RM/stable-baselines3/tarball/v1.3.0",
        "url": "https://api.github.com/repos/DLR-RM/stable-baselines3/releases/51912572",
        "zipball_url": "https://api.github.com/repos/DLR-RM/stable-baselines3/zipball/v1.3.0"
      },
      {
        "authorType": "User",
        "author_name": "araffin",
        "body": "## Breaking Changes:\r\n\r\n- SB3 now requires PyTorch >= 1.8.1\r\n- ``VecNormalize`` ``ret`` attribute was renamed to ``returns``\r\n\r\n## Bug Fixes:\r\n\r\n- Hotfix for ``VecNormalize`` where the observation filter was not updated at reset (thanks @vwxyzjn)\r\n- Fixed model predictions when using batch normalization and dropout layers by calling ``train()`` and ``eval()`` (@davidblom603)\r\n- Fixed model training for DQN, TD3 and SAC so that their target nets always remain in evaluation mode (@ayeright)\r\n- Passing ``gradient_steps=0`` to an off-policy algorithm will result in no gradient steps being taken (vs as many gradient steps as steps done in the environment\r\n  during the rollout in previous versions)\r\n\r\n## Others:\r\n\r\n- Enabled Python 3.9 in GitHub CI\r\n- Fixed type annotations\r\n- Refactored ``predict()`` by moving the preprocessing to ``obs_to_tensor()`` method\r\n\r\n## Documentation:\r\n\r\n- Updated multiprocessing example\r\n- Added example of ``VecEnvWrapper``\r\n- Added a note about logging to tensorboard more often\r\n- Added warning about simplicity of examples and link to RL zoo (@MihaiAnca13)\r\n",
        "dateCreated": "2021-09-08T10:30:20Z",
        "datePublished": "2021-09-08T10:34:51Z",
        "html_url": "https://github.com/DLR-RM/stable-baselines3/releases/tag/v1.2.0",
        "name": "SB3 v1.2.0: Hotfix for VecNormalize, training/eval mode support",
        "tag_name": "v1.2.0",
        "tarball_url": "https://api.github.com/repos/DLR-RM/stable-baselines3/tarball/v1.2.0",
        "url": "https://api.github.com/repos/DLR-RM/stable-baselines3/releases/49188452",
        "zipball_url": "https://api.github.com/repos/DLR-RM/stable-baselines3/zipball/v1.2.0"
      },
      {
        "authorType": "User",
        "author_name": "araffin",
        "body": "\r\n## Breaking Changes\r\n\r\n- All customs environments (e.g. the ``BitFlippingEnv`` or ``IdentityEnv``) were moved to ``stable_baselines3.common.envs`` folder\r\n- Refactored ``HER`` which is now the ``HerReplayBuffer`` class that can be passed to any off-policy algorithm\r\n- Handle timeout termination properly for off-policy algorithms (when using ``TimeLimit``)\r\n- Renamed ``_last_dones`` and ``dones`` to ``_last_episode_starts`` and ``episode_starts`` in ``RolloutBuffer``.\r\n- Removed ``ObsDictWrapper`` as ``Dict`` observation spaces are now supported\r\n\r\n```python\r\n  her_kwargs = dict(n_sampled_goal=2, goal_selection_strategy=\"future\", online_sampling=True)\r\n  # SB3 < 1.1.0\r\n  # model = HER(\"MlpPolicy\", env, model_class=SAC, **her_kwargs)\r\n  # SB3 >= 1.1.0:\r\n  model = SAC(\"MultiInputPolicy\", env, replay_buffer_class=HerReplayBuffer, replay_buffer_kwargs=her_kwargs)\r\n```\r\n\r\n- Updated the KL Divergence estimator in the PPO algorithm to be positive definite and have lower variance (@09tangriro)\r\n- Updated the KL Divergence check in the PPO algorithm to be before the gradient update step rather than after end of epoch (@09tangriro)\r\n- Removed parameter ``channels_last`` from ``is_image_space`` as it can be inferred.\r\n- The logger object is now an attribute ``model.logger`` that be set by the user using ``model.set_logger()``\r\n- Changed the signature of ``logger.configure`` and ``utils.configure_logger``, they now return a ``Logger`` object\r\n- Removed ``Logger.CURRENT`` and ``Logger.DEFAULT``\r\n- Moved ``warn(), debug(), log(), info(), dump()`` methods to the ``Logger`` class\r\n- ``.learn()`` now throws an import error when the user tries to log to tensorboard but the package is not installed\r\n\r\n## New Features\r\n\r\n- Added support for single-level ``Dict`` observation space (@JadenTravnik)\r\n- Added ``DictRolloutBuffer`` ``DictReplayBuffer`` to support dictionary observations (@JadenTravnik)\r\n- Added ``StackedObservations`` and ``StackedDictObservations`` that are used within ``VecFrameStack``\r\n- Added simple 4x4 room Dict test environments\r\n- ``HerReplayBuffer`` now supports ``VecNormalize`` when ``online_sampling=False``\r\n- Added [VecMonitor](https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/common/vec_env/vec_monitor.py) and [VecExtractDictObs](https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/common/vec_env/vec_extract_dict_obs.py) wrappers to handle gym3-style vectorized environments (@vwxyzjn)\r\n- Ignored the terminal observation if the it is not provided by the environment\r\n  such as the gym3-style vectorized environments. (@vwxyzjn)\r\n- Added policy_base as input to the OnPolicyAlgorithm for more flexibility (@09tangriro)\r\n- Added support for image observation when using ``HER``\r\n- Added ``replay_buffer_class`` and ``replay_buffer_kwargs`` arguments to off-policy algorithms\r\n- Added ``kl_divergence`` helper for ``Distribution`` classes (@09tangriro)\r\n- Added support for vector environments with ``num_envs > 1`` (@benblack769)\r\n- Added ``wrapper_kwargs`` argument to ``make_vec_env`` (@amy12xx)\r\n\r\n## Bug Fixes\r\n\r\n- Fixed potential issue when calling off-policy algorithms with default arguments multiple times (the size of the replay buffer would be the same)\r\n- Fixed loading of ``ent_coef`` for ``SAC`` and ``TQC``, it was not optimized anymore (thanks @Atlis)\r\n- Fixed saving of ``A2C`` and ``PPO`` policy when using gSDE (thanks @liusida)\r\n- Fixed a bug where no output would be shown even if ``verbose>=1`` after passing ``verbose=0`` once\r\n- Fixed observation buffers dtype in DictReplayBuffer (@c-rizz)\r\n- Fixed EvalCallback tensorboard logs being logged with the incorrect timestep. They are now written with the timestep at which they were recorded. (@skandermoalla)\r\n\r\n\r\n## Others\r\n\r\n- Added ``flake8-bugbear`` to tests dependencies to find likely bugs\r\n- Updated ``env_checker`` to reflect support of dict observation spaces\r\n- Added Code of Conduct\r\n- Added tests for GAE and lambda return computation\r\n- Updated distribution entropy test (thanks @09tangriro)\r\n- Added sanity check ``batch_size > 1`` in PPO to avoid NaN in advantage normalization\r\n\r\n## Documentation:\r\n\r\n- Added gym pybullet drones project (@JacopoPan)\r\n- Added link to SuperSuit in projects (@justinkterry)\r\n- Fixed DQN example (thanks @ltbd78)\r\n- Clarified channel-first/channel-last recommendation\r\n- Update sphinx environment installation instructions (@tom-doerr)\r\n- Clarified pip installation in Zsh (@tom-doerr)\r\n- Clarified return computation for on-policy algorithms (TD(lambda) estimate was used)\r\n- Added example for using ``ProcgenEnv``\r\n- Added note about advanced custom policy example for off-policy algorithms\r\n- Fixed DQN unicode checkmarks\r\n- Updated migration guide (@juancroldan)\r\n- Pinned ``docutils==0.16`` to avoid issue with rtd theme\r\n- Clarified callback ``save_freq`` definition\r\n- Added doc on how to pass a custom logger\r\n- Remove recurrent policies from ``A2C`` docs (@bstee615)\r\n",
        "dateCreated": "2021-07-02T09:21:09Z",
        "datePublished": "2021-07-02T10:07:45Z",
        "html_url": "https://github.com/DLR-RM/stable-baselines3/releases/tag/v1.1.0",
        "name": "SB3 v1.1.0: Dictionary observation support, timeout handling and refactored HER buffer",
        "tag_name": "v1.1.0",
        "tarball_url": "https://api.github.com/repos/DLR-RM/stable-baselines3/tarball/v1.1.0",
        "url": "https://api.github.com/repos/DLR-RM/stable-baselines3/releases/45613337",
        "zipball_url": "https://api.github.com/repos/DLR-RM/stable-baselines3/zipball/v1.1.0"
      },
      {
        "authorType": "User",
        "author_name": "araffin",
        "body": "**First Major Version**\r\n\r\nBlog post: https://araffin.github.io/post/sb3/\r\n\r\n100+ pre-trained models in the zoo: https://github.com/DLR-RM/rl-baselines3-zoo\r\n\r\n## Breaking Changes:\r\n\r\n- Removed `stable_baselines3.common.cmd_util` (already deprecated), please use `env_util` instead\r\n\r\n<div class=\"warning\">\r\n\r\n<div class=\"admonition-title\">\r\n\r\n**Warning**\r\n\r\n</div>\r\n\r\nA refactoring of the `HER` algorithm is planned together with support for dictionary observations (see [PR #243](https://github.com/DLR-RM/stable-baselines3/pull/243) and\r\n[#351](https://github.com/DLR-RM/stable-baselines3/pull/351))\r\nThis will be a backward incompatible change (model trained with previous version of `HER` won't work with the new version).\r\n\r\n</div>\r\n\r\n## New Features:\r\n\r\n- Added support for `custom_objects` when loading models\r\n\r\n## Bug Fixes:\r\n\r\n- Fixed a bug with `DQN` predict method when using `deterministic=False` with image space\r\n\r\n## Documentation:\r\n\r\n- Fixed examples\r\n- Added new project using SB3: rl\\_reach (@PierreExeter)\r\n- Added note about slow-down when switching to PyTorch\r\n- Add a note on continual learning and resetting environment\r\n- Updated RL-Zoo to reflect the fact that is it more than a collection of trained agents\r\n- Added images to illustrate the training loop and custom policies (created with <https://excalidraw.com/>)\r\n- Updated the custom policy section\r\n\r\n\r\n",
        "dateCreated": "2021-03-17T13:20:31Z",
        "datePublished": "2021-03-17T14:26:17Z",
        "html_url": "https://github.com/DLR-RM/stable-baselines3/releases/tag/v1.0",
        "name": "Stable-Baselines3 v1.0",
        "tag_name": "v1.0",
        "tarball_url": "https://api.github.com/repos/DLR-RM/stable-baselines3/tarball/v1.0",
        "url": "https://api.github.com/repos/DLR-RM/stable-baselines3/releases/39945513",
        "zipball_url": "https://api.github.com/repos/DLR-RM/stable-baselines3/zipball/v1.0"
      },
      {
        "authorType": "User",
        "author_name": "araffin",
        "body": "Second release candidate",
        "dateCreated": "2021-03-06T13:17:43Z",
        "datePublished": "2021-03-06T13:27:02Z",
        "html_url": "https://github.com/DLR-RM/stable-baselines3/releases/tag/v1.0rc1",
        "name": "v1.0rc1",
        "tag_name": "v1.0rc1",
        "tarball_url": "https://api.github.com/repos/DLR-RM/stable-baselines3/tarball/v1.0rc1",
        "url": "https://api.github.com/repos/DLR-RM/stable-baselines3/releases/39377978",
        "zipball_url": "https://api.github.com/repos/DLR-RM/stable-baselines3/zipball/v1.0rc1"
      },
      {
        "authorType": "User",
        "author_name": "araffin",
        "body": "",
        "dateCreated": "2021-03-01T12:35:21Z",
        "datePublished": "2021-03-01T12:35:45Z",
        "html_url": "https://github.com/DLR-RM/stable-baselines3/releases/tag/v1.0rc0",
        "name": "v1.0rc0: Beta is over =)! ",
        "tag_name": "v1.0rc0",
        "tarball_url": "https://api.github.com/repos/DLR-RM/stable-baselines3/tarball/v1.0rc0",
        "url": "https://api.github.com/repos/DLR-RM/stable-baselines3/releases/39050669",
        "zipball_url": "https://api.github.com/repos/DLR-RM/stable-baselines3/zipball/v1.0rc0"
      },
      {
        "authorType": "User",
        "author_name": "araffin",
        "body": "\r\n## Breaking Changes:\r\n\r\n- ``evaluate_policy`` now returns rewards/episode lengths from a ``Monitor`` wrapper if one is present,\r\n  this allows to return the unnormalized reward in the case of Atari games for instance.\r\n- Renamed ``common.vec_env.is_wrapped`` to ``common.vec_env.is_vecenv_wrapped`` to avoid confusion\r\n  with the new ``is_wrapped()`` helper\r\n- Renamed ``_get_data()`` to ``_get_constructor_parameters()`` for policies (this affects independent saving/loading of policies)\r\n- Removed ``n_episodes_rollout`` and merged it with ``train_freq``, which now accepts a tuple ``(frequency, unit)``:\r\n- ``replay_buffer`` in ``collect_rollout`` is no more optional\r\n\r\n```python\r\n\r\n  # SB3 < 0.11.0\r\n  # model = SAC(\"MlpPolicy\", env, n_episodes_rollout=1, train_freq=-1)\r\n  # SB3 >= 0.11.0:\r\n  model = SAC(\"MlpPolicy\", env, train_freq=(1, \"episode\"))\r\n```\r\n\r\n## New Features:\r\n\r\n- Add support for ``VecFrameStack`` to stack on first or last observation dimension, along with\r\n  automatic check for image spaces.\r\n- ``VecFrameStack`` now has a ``channels_order`` argument to tell if observations should be stacked\r\n  on the first or last observation dimension (originally always stacked on last).\r\n- Added ``common.env_util.is_wrapped`` and ``common.env_util.unwrap_wrapper`` functions for checking/unwrapping\r\n  an environment for specific wrapper.\r\n- Added ``env_is_wrapped()`` method for ``VecEnv`` to check if its environments are wrapped\r\n  with given Gym wrappers.\r\n- Added ``monitor_kwargs`` parameter to ``make_vec_env`` and ``make_atari_env``\r\n- Wrap the environments automatically with a ``Monitor`` wrapper when possible.\r\n- ``EvalCallback`` now logs the success rate when available (``is_success`` must be present in the info dict)\r\n- Added new wrappers to log images and matplotlib figures to tensorboard. (@zampanteymedio)\r\n- Add support for text records to ``Logger``. (@lorenz-h)\r\n\r\n## Bug Fixes:\r\n\r\n- Fixed bug where code added VecTranspose on channel-first image environments (thanks @qxcv)\r\n- Fixed ``DQN`` predict method when using single ``gym.Env`` with ``deterministic=False``\r\n- Fixed bug that the arguments order of ``explained_variance()`` in ``ppo.py`` and ``a2c.py`` is not correct (@thisray)\r\n- Fixed bug where full ``HerReplayBuffer`` leads to an index error. (@megan-klaiber)\r\n- Fixed bug where replay buffer could not be saved if it was too big (> 4 Gb) for python<3.8 (thanks @hn2)\r\n- Added informative ``PPO`` construction error in edge-case scenario where ``n_steps * n_envs = 1`` (size of rollout buffer),\r\n  which otherwise causes downstream breaking errors in training (@decodyng)\r\n- Fixed discrete observation space support when using multiple envs with A2C/PPO (thanks @ardabbour)\r\n- Fixed a bug for TD3 delayed update (the update was off-by-one and not delayed when ``train_freq=1``)\r\n- Fixed numpy warning (replaced ``np.bool`` with ``bool``)\r\n- Fixed a bug where ``VecNormalize`` was not normalizing the terminal observation\r\n- Fixed a bug where ``VecTranspose`` was not transposing the terminal observation\r\n- Fixed a bug where the terminal observation stored in the replay buffer was not the right one for off-policy algorithms\r\n- Fixed a bug where ``action_noise`` was not used when using ``HER`` (thanks @ShangqunYu)\r\n- Fixed a bug where ``train_freq`` was not properly converted when loading a saved model\r\n\r\n## Others:\r\n\r\n- Add more issue templates\r\n- Add signatures to callable type annotations (@ernestum)\r\n- Improve error message in ``NatureCNN``\r\n- Added checks for supported action spaces to improve clarity of error messages for the user\r\n- Renamed variables in the ``train()`` method of ``SAC``, ``TD3`` and ``DQN`` to match SB3-Contrib.\r\n- Updated docker base image to Ubuntu 18.04\r\n- Set tensorboard min version to 2.2.0 (earlier version are apparently not working with PyTorch)\r\n- Added warning for ``PPO`` when ``n_steps * n_envs`` is not a multiple of ``batch_size`` (last mini-batch truncated) (@decodyng)\r\n- Removed some warnings in the tests\r\n\r\n## Documentation:\r\n\r\n- Updated algorithm table\r\n- Minor docstring improvements regarding rollout (@stheid)\r\n- Fix migration doc for ``A2C`` (epsilon parameter)\r\n- Fix ``clip_range`` docstring\r\n- Fix duplicated parameter in ``EvalCallback`` docstring (thanks @tfederico)\r\n- Added example of learning rate schedule\r\n- Added SUMO-RL as example project (@LucasAlegre)\r\n- Fix docstring of classes in atari_wrappers.py which were inside the constructor (@LucasAlegre)\r\n- Added SB3-Contrib page\r\n- Fix bug in the example code of DQN (@AptX395)\r\n- Add example on how to access the tensorboard summary writer directly. (@lorenz-h)\r\n- Updated migration guide\r\n- Updated custom policy doc (separate policy architecture recommended)\r\n- Added a note about OpenCV headless version\r\n- Corrected typo on documentation (@mschweizer)\r\n- Provide the environment when loading the model in the examples (@lorepieri8)",
        "dateCreated": "2021-02-27T18:53:13Z",
        "datePublished": "2021-02-27T19:31:33Z",
        "html_url": "https://github.com/DLR-RM/stable-baselines3/releases/tag/v0.11.1",
        "name": "Bug fixes, better image support and last release before v1.0",
        "tag_name": "v0.11.1",
        "tarball_url": "https://api.github.com/repos/DLR-RM/stable-baselines3/tarball/v0.11.1",
        "url": "https://api.github.com/repos/DLR-RM/stable-baselines3/releases/39000502",
        "zipball_url": "https://api.github.com/repos/DLR-RM/stable-baselines3/zipball/v0.11.1"
      },
      {
        "authorType": "User",
        "author_name": "araffin",
        "body": "\r\n## Breaking Changes\r\n\r\n- **Warning:** Renamed ``common.cmd_util`` to ``common.env_util`` for clarity (affects ``make_vec_env`` and ``make_atari_env`` functions)\r\n\r\n## New Features\r\n\r\n- Allow custom actor/critic network architectures using ``net_arch=dict(qf=[400, 300], pi=[64, 64])`` for off-policy algorithms (SAC, TD3, DDPG)\r\n- Added Hindsight Experience Replay ``HER``. (@megan-klaiber)\r\n- ``VecNormalize`` now supports ``gym.spaces.Dict`` observation spaces\r\n- Support logging videos to Tensorboard (@SwamyDev)\r\n- Added ``share_features_extractor`` argument to ``SAC`` and ``TD3`` policies\r\n\r\n## Bug Fixes\r\n\r\n- Fix GAE computation for on-policy algorithms (off-by one for the last value) (thanks @Wovchena)\r\n- Fixed potential issue when loading a different environment\r\n- Fix ignoring the exclude parameter when recording logs using json, csv or log as logging format (@SwamyDev)\r\n- Make ``make_vec_env`` support the ``env_kwargs`` argument when using an env ID str (@ManifoldFR)\r\n- Fix model creation initializing CUDA even when `device=\"cpu\"` is provided\r\n- Fix ``check_env`` not checking if the env has a Dict actionspace before calling ``_check_nan`` (@wmmc88)\r\n- Update the check for spaces unsupported by Stable Baselines 3 to include checks on the action space (@wmmc88)\r\n- Fixed feature extractor bug for target network where the same net was shared instead\r\n  of being separate. This bug affects ``SAC``, ``DDPG`` and ``TD3`` when using ``CnnPolicy`` (or custom feature extractor)\r\n- Fixed a bug when passing an environment when loading a saved model with a ``CnnPolicy``, the passed env was not wrapped properly\r\n  (the bug was introduced when implementing ``HER`` so it should not be present in previous versions)\r\n\r\n## Others\r\n\r\n- Improved typing coverage\r\n- Improved error messages for unsupported spaces\r\n- Added ``.vscode`` to the gitignore\r\n\r\n## Documentation\r\n\r\n- Added first draft of migration guide\r\n- Added intro to [imitation](https://github.com/HumanCompatibleAI/imitation) library (@shwang)\r\n- Enabled doc for ``CnnPolicies``\r\n- Added advanced saving and loading example\r\n- Added base doc for exporting models\r\n- Added example for getting and setting model parameters\r\n",
        "dateCreated": "2020-10-28T12:02:55Z",
        "datePublished": "2020-10-28T12:05:21Z",
        "html_url": "https://github.com/DLR-RM/stable-baselines3/releases/tag/v0.10.0",
        "name": "HER with online and offline sampling, bug fixes for features extraction",
        "tag_name": "v0.10.0",
        "tarball_url": "https://api.github.com/repos/DLR-RM/stable-baselines3/tarball/v0.10.0",
        "url": "https://api.github.com/repos/DLR-RM/stable-baselines3/releases/33159585",
        "zipball_url": "https://api.github.com/repos/DLR-RM/stable-baselines3/zipball/v0.10.0"
      },
      {
        "authorType": "User",
        "author_name": "araffin",
        "body": "\r\n### Breaking Changes:\r\n\r\n- Removed ``device`` keyword argument of policies; use ``policy.to(device)`` instead. (@qxcv)\r\n- Rename ``BaseClass.get_torch_variables`` -> ``BaseClass._get_torch_save_params`` and\r\n    ``BaseClass.excluded_save_params`` -> ``BaseClass._excluded_save_params``\r\n- Renamed saved items ``tensors`` to ``pytorch_variables`` for clarity\r\n- ``make_atari_env``, ``make_vec_env`` and ``set_random_seed`` must be imported with (and not directly from ``stable_baselines3.common``):\r\n\r\n```python\r\nfrom stable_baselines3.common.cmd_util import make_atari_env, make_vec_env\r\nfrom stable_baselines3.common.utils import set_random_seed\r\n```\r\n\r\n### New Features:\r\n\r\n- Added ``unwrap_vec_wrapper()`` to ``common.vec_env`` to extract ``VecEnvWrapper`` if needed\r\n- Added ``StopTrainingOnMaxEpisodes`` to callback collection (@xicocaio)\r\n- Added ``device`` keyword argument to ``BaseAlgorithm.load()`` (@liorcohen5)\r\n- Callbacks have access to rollout collection locals as in SB2. (@PartiallyTyped)\r\n- Added ``get_parameters`` and ``set_parameters`` for accessing/setting parameters of the agent\r\n- Added actor/critic loss logging for TD3. (@mloo3)\r\n\r\n### Bug Fixes:\r\n\r\n- Fixed a bug where the environment was reset twice when using ``evaluate_policy``\r\n- Fix logging of ``clip_fraction`` in PPO (@diditforlulz273)\r\n- Fixed a bug where cuda support was wrongly checked when passing the GPU index, e.g., ``device=\"cuda:0\"`` (@liorcohen5)\r\n- Fixed a bug when the random seed was not properly set on cuda when passing the GPU index\r\n\r\n\r\n### Others:\r\n\r\n- Improve typing coverage of the ``VecEnv``\r\n- Fix type annotation of ``make_vec_env`` (@ManifoldFR)\r\n- Removed ``AlreadySteppingError`` and ``NotSteppingError`` that were not used\r\n- Fixed typos in SAC and TD3\r\n- Reorganized functions for clarity in ``BaseClass`` (save/load functions close to each other, private\r\n    functions at top)\r\n- Clarified docstrings on what is saved and loaded to/from files\r\n- Simplified ``save_to_zip_file`` function by removing duplicate code\r\n- Store library version along with the saved models\r\n- DQN loss is now logged\r\n\r\n### Documentation:\r\n\r\n- Added ``StopTrainingOnMaxEpisodes`` details and example (@xicocaio)\r\n- Updated custom policy section (added custom feature extractor example)\r\n- Re-enable ``sphinx_autodoc_typehints``\r\n- Updated doc style for type hints and remove duplicated type hints",
        "dateCreated": "2020-10-04T15:13:39Z",
        "datePublished": "2020-10-04T15:29:45Z",
        "html_url": "https://github.com/DLR-RM/stable-baselines3/releases/tag/v0.9.0",
        "name": "Bug fixes, get/set parameters  and improved docs",
        "tag_name": "v0.9.0",
        "tarball_url": "https://api.github.com/repos/DLR-RM/stable-baselines3/tarball/v0.9.0",
        "url": "https://api.github.com/repos/DLR-RM/stable-baselines3/releases/32139833",
        "zipball_url": "https://api.github.com/repos/DLR-RM/stable-baselines3/zipball/v0.9.0"
      },
      {
        "authorType": "User",
        "author_name": "araffin",
        "body": "## Breaking Changes:\r\n\r\n- ``AtariWrapper`` and other Atari wrappers were updated to match SB2 ones\r\n- ``save_replay_buffer`` now receives as argument the file path instead of the folder path (@tirafesi)\r\n- Refactored ``Critic`` class for ``TD3`` and ``SAC``, it is now called ``ContinuousCritic``\r\n  and has an additional parameter ``n_critics``\r\n- ``SAC`` and ``TD3`` now accept an arbitrary number of critics (e.g. ``policy_kwargs=dict(n_critics=3)``)\r\n    instead of only 2 previously\r\n\r\n## New Features:\r\n\r\n- Added ``DQN`` Algorithm (@Artemis-Skade)\r\n- Buffer dtype is now set according to action and observation spaces for ``ReplayBuffer``\r\n- Added warning when allocation of a buffer may exceed the available memory of the system\r\n  when ``psutil`` is available\r\n- Saving models now automatically creates the necessary folders and raises appropriate warnings (@PartiallyTyped)\r\n- Refactored opening paths for saving and loading to use strings, pathlib or io.BufferedIOBase (@PartiallyTyped)\r\n- Added ``DDPG`` algorithm as a special case of ``TD3``.\r\n- Introduced ``BaseModel`` abstract parent for ``BasePolicy``, which critics inherit from.\r\n\r\n## Bug Fixes:\r\n\r\n- Fixed a bug in the ``close()`` method of ``SubprocVecEnv``, causing wrappers further down in the wrapper stack to not be closed. (@NeoExtended)\r\n- Fix target for updating q values in SAC: the entropy term was not conditioned by terminals states\r\n- Use ``cloudpickle.load`` instead of ``pickle.load`` in ``CloudpickleWrapper``. (@shwang)\r\n- Fixed a bug with orthogonal initialization when `bias=False` in custom policy (@rk37)\r\n- Fixed approximate entropy calculation in PPO and A2C. (@andyshih12)\r\n- Fixed DQN target network sharing feature extractor with the main network.\r\n- Fixed storing correct ``dones`` in on-policy algorithm rollout collection. (@andyshih12)\r\n- Fixed number of filters in final convolutional layer in NatureCNN to match original implementation.\r\n\r\n\r\n## Others:\r\n\r\n- Refactored off-policy algorithm to share the same ``.learn()`` method\r\n- Split the ``collect_rollout()`` method for off-policy algorithms\r\n- Added ``_on_step()`` for off-policy base class\r\n- Optimized replay buffer size by removing the need of ``next_observations`` numpy array\r\n- Optimized polyak updates (1.5-1.95 speedup) through inplace operations (@PartiallyTyped)\r\n- Switch to ``black`` codestyle and added ``make format``, ``make check-codestyle`` and ``commit-checks``\r\n- Ignored errors from newer pytype version\r\n- Added a check when using ``gSDE``\r\n- Removed codacy dependency from Dockerfile\r\n- Added ``common.sb2_compat.RMSpropTFLike`` optimizer, which corresponds closer to the implementation of RMSprop from Tensorflow.\r\n\r\n## Documentation:\r\n\r\n- Updated notebook links\r\n- Fixed a typo in the section of Enjoy a Trained Agent, in RL Baselines3 Zoo README. (@blurLake)\r\n- Added Unity reacher to the projects page (@koulakis)\r\n- Added PyBullet colab notebook\r\n- Fixed typo in PPO example code (@joeljosephjin)\r\n- Fixed typo in custom policy doc (@RaphaelWag)\r\n",
        "dateCreated": "2020-08-03T20:39:40Z",
        "datePublished": "2020-08-03T20:42:01Z",
        "html_url": "https://github.com/DLR-RM/stable-baselines3/releases/tag/v0.8.0",
        "name": "Added DQN and DDPG, bug fixes and performance matching for Atari games",
        "tag_name": "v0.8.0",
        "tarball_url": "https://api.github.com/repos/DLR-RM/stable-baselines3/tarball/v0.8.0",
        "url": "https://api.github.com/repos/DLR-RM/stable-baselines3/releases/29246029",
        "zipball_url": "https://api.github.com/repos/DLR-RM/stable-baselines3/zipball/v0.8.0"
      },
      {
        "authorType": "User",
        "author_name": "araffin",
        "body": "\r\n## Breaking Changes:\r\n\r\n- ``render()`` method of ``VecEnvs`` now only accept one argument: ``mode``\r\n- Created new file common/torch_layers.py, similar to SB refactoring\r\n\r\n  - Contains all PyTorch network layer definitions and feature extractors: ``MlpExtractor``, ``create_mlp``, ``NatureCNN``\r\n\r\n- Renamed ``BaseRLModel`` to ``BaseAlgorithm`` (along with offpolicy and onpolicy variants)\r\n- Moved on-policy and off-policy base algorithms to ``common/on_policy_algorithm.py`` and ``common/off_policy_algorithm.py``, respectively.\r\n- Moved ``PPOPolicy`` to ``ActorCriticPolicy`` in common/policies.py\r\n- Moved ``PPO`` (algorithm class) into ``OnPolicyAlgorithm`` (``common/on_policy_algorithm.py``), to be shared with A2C\r\n- Moved following functions from ``BaseAlgorithm``:\r\n\r\n  - ``_load_from_file`` to ``load_from_zip_file`` (save_util.py)\r\n  - ``_save_to_file_zip`` to ``save_to_zip_file`` (save_util.py)\r\n  - ``safe_mean`` to ``safe_mean`` (utils.py)\r\n  - ``check_env`` to ``check_for_correct_spaces`` (utils.py. Renamed to avoid confusion with environment checker tools)\r\n\r\n- Moved static function ``_is_vectorized_observation`` from common/policies.py to common/utils.py under name ``is_vectorized_observation``.\r\n- Removed ``{save,load}_running_average`` functions of ``VecNormalize`` in favor of ``load/save``.\r\n- Removed ``use_gae`` parameter from ``RolloutBuffer.compute_returns_and_advantage``.\r\n\r\n## Bug Fixes:\r\n\r\n- Fixed ``render()`` method for ``VecEnvs``\r\n- Fixed ``seed()`` method for ``SubprocVecEnv``\r\n- Fixed loading on GPU for testing when using gSDE and ``deterministic=False``\r\n- Fixed ``register_policy`` to allow re-registering same policy for same sub-class (i.e. assign same value to same key).\r\n- Fixed a bug where the gradient was passed when using ``gSDE`` with ``PPO``/``A2C``, this does not affect ``SAC``\r\n\r\n## Others:\r\n\r\n- Re-enable unsafe ``fork`` start method in the tests (was causing a deadlock with tensorflow)\r\n- Added a test for seeding ``SubprocVecEnv`` and rendering\r\n- Fixed reference in NatureCNN (pointed to older version with different network architecture)\r\n- Fixed comments saying \"CxWxH\" instead of \"CxHxW\" (same style as in torch docs / commonly used)\r\n- Added bit further comments on register/getting policies (\"MlpPolicy\", \"CnnPolicy\").\r\n- Renamed ``progress`` (value from 1 in start of training to 0 in end) to ``progress_remaining``.\r\n- Added ``policies.py`` files for A2C/PPO, which define MlpPolicy/CnnPolicy (renamed ActorCriticPolicies).\r\n- Added some missing tests for ``VecNormalize``, ``VecCheckNan`` and ``PPO``.\r\n\r\n## Documentation:\r\n\r\n- Added a paragraph on \"MlpPolicy\"/\"CnnPolicy\" and policy naming scheme under \"Developer Guide\"\r\n- Fixed second-level listing in changelog\r\n",
        "dateCreated": "2020-06-10T16:59:30Z",
        "datePublished": "2020-06-10T17:01:45Z",
        "html_url": "https://github.com/DLR-RM/stable-baselines3/releases/tag/v0.7.0",
        "name": "Hotfix for PPO/A2C + gSDE, internal refactoring and bug fixes",
        "tag_name": "v0.7.0",
        "tarball_url": "https://api.github.com/repos/DLR-RM/stable-baselines3/tarball/v0.7.0",
        "url": "https://api.github.com/repos/DLR-RM/stable-baselines3/releases/27421260",
        "zipball_url": "https://api.github.com/repos/DLR-RM/stable-baselines3/zipball/v0.7.0"
      },
      {
        "authorType": "User",
        "author_name": "araffin",
        "body": "\r\n## Breaking Changes:\r\n\r\n- Remove State-Dependent Exploration (SDE) support for ``TD3``\r\n- Methods were renamed in the logger:\r\n  - ``logkv`` -> ``record``, ``writekvs`` -> ``write``, ``writeseq`` ->  ``write_sequence``,\r\n  - ``logkvs`` -> ``record_dict``, ``dumpkvs`` -> ``dump``,\r\n  - ``getkvs`` -> ``get_log_dict``, ``logkv_mean`` -> ``record_mean``,\r\n\r\n\r\n## New Features:\r\n\r\n- Added env checker (Sync with Stable Baselines)\r\n- Added ``VecCheckNan`` and ``VecVideoRecorder`` (Sync with Stable Baselines)\r\n- Added determinism tests\r\n- Added ``cmd_util`` and ``atari_wrappers``\r\n- Added support for ``MultiDiscrete`` and ``MultiBinary`` observation spaces (@rolandgvc)\r\n- Added ``MultiCategorical`` and ``Bernoulli`` distributions for PPO/A2C (@rolandgvc)\r\n- Added support for logging to tensorboard (@rolandgvc)\r\n- Added ``VectorizedActionNoise`` for continuous vectorized environments (@PartiallyTyped)\r\n- Log evaluation in the ``EvalCallback`` using the logger\r\n\r\n## Bug Fixes:\r\n\r\n- Fixed a bug that prevented model trained on cpu to be loaded on gpu\r\n- Fixed version number that had a new line included\r\n- Fixed weird seg fault in docker image due to FakeImageEnv by reducing screen size\r\n- Fixed ``sde_sample_freq`` that was not taken into account for SAC\r\n- Pass logger module to ``BaseCallback`` otherwise they cannot write in the one used by the algorithms\r\n\r\n## Others:\r\n\r\n- Renamed to Stable-Baseline3\r\n- Added Dockerfile\r\n- Sync ``VecEnvs`` with Stable-Baselines\r\n- Update requirement: ``gym>=0.17``\r\n- Added ``.readthedoc.yml`` file\r\n- Added ``flake8`` and ``make lint`` command\r\n- Added Github workflow\r\n- Added warning when passing both ``train_freq`` and ``n_episodes_rollout`` to Off-Policy Algorithms\r\n\r\n## Documentation:\r\n\r\n- Added most documentation (adapted from Stable-Baselines)\r\n- Added link to CONTRIBUTING.md in the README (@kinalmehta)\r\n- Added gSDE project and update docstrings accordingly\r\n- Fix ``TD3`` example code block",
        "dateCreated": "2020-06-01T11:10:26Z",
        "datePublished": "2020-06-01T11:13:13Z",
        "html_url": "https://github.com/DLR-RM/stable-baselines3/releases/tag/v0.6.0",
        "name": "Tensorboard support, refactored logger",
        "tag_name": "v0.6.0",
        "tarball_url": "https://api.github.com/repos/DLR-RM/stable-baselines3/tarball/v0.6.0",
        "url": "https://api.github.com/repos/DLR-RM/stable-baselines3/releases/27092068",
        "zipball_url": "https://api.github.com/repos/DLR-RM/stable-baselines3/zipball/v0.6.0"
      }
    ],
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Stable Baselines3 requires python 3.7+.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2640,
      "date": "Thu, 30 Dec 2021 03:01:42 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "reinforcement-learning",
      "reinforcement-learning-algorithms",
      "machine-learning",
      "gym",
      "openai",
      "baselines",
      "toolbox",
      "stable-baselines",
      "python",
      "pytorch",
      "robotics",
      "sde",
      "gsde",
      "sb3"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Most of the library tries to follow a sklearn-like syntax for the Reinforcement Learning algorithms.\n\nHere is a quick example of how to train and run PPO on a cartpole environment:\n```python\nimport gym\n\nfrom stable_baselines3 import PPO\n\nenv = gym.make(\"CartPole-v1\")\n\nmodel = PPO(\"MlpPolicy\", env, verbose=1)\nmodel.learn(total_timesteps=10000)\n\nobs = env.reset()\nfor i in range(1000):\n    action, _states = model.predict(obs, deterministic=True)\n    obs, reward, done, info = env.step(action)\n    env.render()\n    if done:\n      obs = env.reset()\n\nenv.close()\n```\n\nOr just train a model with a one liner if [the environment is registered in Gym](https://github.com/openai/gym/wiki/Environments) and if [the policy is registered](https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html):\n\n```python\nfrom stable_baselines3 import PPO\n\nmodel = PPO('MlpPolicy', 'CartPole-v1').learn(10000)\n```\n\nPlease read the [documentation](https://stable-baselines3.readthedocs.io/) for more examples.\n\n\n",
      "technique": "Header extraction"
    }
  ]
}