{
  "citation": [
    {
      "confidence": [
        0.82000071191569
      ],
      "excerpt": "Head to the Github releases page at https://github.com/gcp/leela-zero/releases, \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/l1t1/lz19",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-01-04T01:21:04Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-01-04T01:26:51Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.979219551298523,
        0.9173765602124146,
        0.9531983271841112
      ],
      "excerpt": "This is a fairly faithful reimplementation of the system described \nin the Alpha Go Zero paper \"Mastering the Game of Go without Human Knowledge\". \nFor all intents and purposes, it is an open source AlphaGo Zero. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8965561406895736
      ],
      "excerpt": "be an engine that is far stronger than the top humans. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8363207968336344,
        0.8861790690159256,
        0.8635575203630618
      ],
      "excerpt": "One reason for publishing this program is that we are running a public, \ndistributed effort to repeat the work. Working together, and especially \nwhen starting on a smaller scale, it will take less than 1700 years to get \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8818572653162655
      ],
      "excerpt": "the server automatically and do its work in the background, uploading results \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8943072643172953,
        0.8569366557992855,
        0.9242625561857859
      ],
      "excerpt": "Follow the instructions below to compile the leelaz binary, then go into \nthe autogtp subdirectory and follow the instructions there \nto build the autogtp binary. Copy the leelaz binary into the autogtp dir, and \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9922776317749896
      ],
      "excerpt": "that are usable for helping the leela-zero project. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9571141677975775
      ],
      "excerpt": "And head to the Usage section of this README. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9877238155879763,
        0.877617903499411,
        0.8591719591674217
      ],
      "excerpt": "The layout of the network is as in the AlphaGo Zero paper, but any number of \nresidual blocks is allowed, and any number of outputs (filters) per layer, \nas long as the latter is the same for all layers. The program will autodetect \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9495743027549155,
        0.9151633939648772,
        0.8592780487879873,
        0.9410046557775178
      ],
      "excerpt": "head. All convolution filters are 3x3 except for the ones at the start of the policy and value head, which are 1x1 (as in the paper). \nThere are 18 inputs to the first layer, instead of 17 as in the paper. The \noriginal AlphaGo Zero design has a slight imbalance in that it is easier \nfor the black player to see the board edge (due to how padding works in \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8068953168871238
      ],
      "excerpt": "1) Side to move stones at time T=0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8256387107811768
      ],
      "excerpt": "18) All 1 if white is to move, 0 otherwise \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9189749338775963
      ],
      "excerpt": "description of the full 40 residual block design, in (NVIDIA)-Caffe protobuff \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9330386999014089
      ],
      "excerpt": "because they are followed by a batchnorm layer, which is supposed to normalize \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9094219622414114,
        0.9488671820622203
      ],
      "excerpt": "operation in the batchnorm layer, corrected for the effect of the batchnorm mean/variance adjustment. At inference time, Leela Zero will fuse the channel \nbias into the batchnorm mean, thereby offsetting it and performing the center operation. This roundabout construction exists solely for backwards \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.916311975937522,
        0.9025586935682142
      ],
      "excerpt": "Leela can convert a database of concatenated SGF games into a datafile suitable \nfor learning: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9684473347564445
      ],
      "excerpt": "The training data consists of files with the following data, all in text \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8096892841717471
      ],
      "excerpt": "16 lines of hexadecimal strings, each 361 bits longs, corresponding to the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9742557693696156,
        0.9398384857726668,
        0.9538663115488041,
        0.9693233393948762
      ],
      "excerpt": "(visit counts) at the end of the search for the move in question. The last \nnumber is the probability of passing. \n1 line with either 1 or -1, corresponding to the outcome of the game for the \nplayer to move \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9235253062837733
      ],
      "excerpt": "well as snapshots of the learning state numbered by the batch number. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8821926934974886,
        0.8886034359529978
      ],
      "excerpt": "[ ] Root filtering for handicap play. \nMore backends: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8789339176452443
      ],
      "excerpt": "Status page of the distributed effort: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "copy gcp leela-zero 2019",
      "technique": "GitHub API"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "    msvc\\x64\\Release\\leelaz.exe --weights best-network\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/l1t1/lz19/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 28 Dec 2021 23:18:53 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/l1t1/lz19/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "l1t1/lz19",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "    brew install boost cmake\n\n    ",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "    sudo apt install libboost-dev libboost-program-options-dev libboost-filesystem-dev opencl-headers ocl-icd-libopencl1 ocl-icd-opencl-dev zlib1g-dev\n\n    ",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9518582487054953
      ],
      "excerpt": "You need a PC with a GPU, i.e. a discrete graphics card made by NVIDIA or AMD, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9578948819182579
      ],
      "excerpt": "Follow the instructions below to compile the leelaz binary, then go into \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9879863063452118,
        0.9906248903846466,
        0.8270538936472002,
        0.9879863063452118,
        0.9906248903846466,
        0.8270538936472002,
        0.9879863063452118,
        0.9906248903846466,
        0.8270538936472002,
        0.9906248903846466
      ],
      "excerpt": "git clone https://github.com/gcp/leela-zero \ncd leela-zero \ngit submodule update --init --recursive \ngit clone https://github.com/gcp/leela-zero \ncd leela-zero \ngit submodule update --init --recursive \ngit clone https://github.com/gcp/leela-zero \ncd leela-zero \ngit submodule update --init --recursive \ncd msvc \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9304772622292063
      ],
      "excerpt": "to the Visual Studio version you have. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8229637852726345
      ],
      "excerpt": "This requires a working installation of TensorFlow 1.4 or later: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9748709027320682
      ],
      "excerpt": "[ ] Implement GPU batching. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9532312362739296
      ],
      "excerpt": "[ ] CUDA specific version using cuDNN or cuBLAS. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8615764604075481
      ],
      "excerpt": "The weights file is a text file with each line containing a row of coefficients. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8289669050403863
      ],
      "excerpt": "    2) output biases \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9221361021541947
      ],
      "excerpt": "in the tfprocess.py file. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8454553703666403
      ],
      "excerpt": "dump_supervised sgffile.sgf train.txt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8866584713902795
      ],
      "excerpt": "starting with the name train.txt and containing training data generated from \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8090460578397023
      ],
      "excerpt": "The training data consists of files with the following data, all in text \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8142829148706594
      ],
      "excerpt": "first 16 input planes from the previous section \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8612138100060728
      ],
      "excerpt": "1 line with 362 (19x19 + 1) floating point numbers, indicating the search probabilities \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8522043822775097,
        0.8291400007888242
      ],
      "excerpt": "src/leelaz -w weights.txt \ndump_supervised bigsgf.sgf train.out \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9231761409144816
      ],
      "excerpt": "training/tf/parse.py train.out \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9323578162161054
      ],
      "excerpt": "training/tf/parse.py train.out leelaz-model-batchnumber \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/l1t1/lz19/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "C++",
      "Python",
      "CMake",
      "C",
      "Makefile",
      "Batchfile",
      "QMake"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "GNU General Public License v3.0",
      "url": "https://api.github.com/licenses/gpl-3.0"
    },
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "What",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "lz19",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "l1t1",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/l1t1/lz19/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* GCC, Clang or MSVC, any C++14 compiler\n* Boost 1.58.x or later, headers and program_options, filesystem and system libraries (libboost-dev, libboost-program-options-dev and libboost-filesystem-dev on Debian/Ubuntu)\n* zlib library (zlib1g & zlib1g-dev on Debian/Ubuntu)\n* Standard OpenCL C headers (opencl-headers on Debian/Ubuntu, or at\nhttps://github.com/KhronosGroup/OpenCL-Headers/tree/master/CL)\n* OpenCL ICD loader (ocl-icd-libopencl1 on Debian/Ubuntu, or reference implementation at https://github.com/KhronosGroup/OpenCL-ICD-Loader)\n* An OpenCL capable device, preferably a very, very fast GPU, with recent\ndrivers is strongly recommended (OpenCL 1.1 support is enough).\nIf you do not have a GPU, add the define \"USE_CPU_ONLY\", for example\nby adding -DUSE_CPU_ONLY=1 to the cmake command line.\n* Optional: BLAS Library: OpenBLAS (libopenblas-dev) or Intel MKL\n* The program has been tested on Windows, Linux and macOS.\n\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "    ",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "    ",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "    ",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "For training a new network, you can use an existing framework (Caffe,\nTensorFlow, PyTorch, Theano), with a set of training data as described above.\nYou still need to contruct a model description (2 examples are provided for\nCaffe), parse the input file format, and outputs weights in the proper format.\n\nThere is a complete implementation for TensorFlow in the training/tf directory.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 28 Dec 2021 23:18:53 GMT"
    },
    "technique": "GitHub API"
  },
  "support": [
    {
      "confidence": [
        1
      ],
      "excerpt": "    sudo apt install clinfo && clinfo\n\n    ",
      "technique": "Header extraction"
    }
  ],
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "    ",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "    mkdir build && cd build\n    cmake ..\n    cmake --build .\n    ./tests\n    curl -O https://zero.sjeng.org/best-network\n    ./leelaz --weights best-network\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "    ",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "    mkdir build && cd build\n    cmake ..\n    cmake --build .\n    ./tests\n    curl -O https://zero.sjeng.org/best-network\n    ./leelaz --weights best-network\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "    ",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "The engine supports the [GTP protocol, version 2](https://www.lysator.liu.se/~gunnar/gtp/gtp2-spec-draft2/gtp2-spec.html).\n\nLeela Zero is not meant to be used directly. You need a graphical interface\nfor it, which will interface with Leela Zero through the GTP protocol.\n\n[Lizzie](https://github.com/featurecat/lizzie/releases) is a client specifically\nfor Leela Zero which shows live search probilities, a win rate graph, and has\nan automatic game analysis mode. Has binaries for Windows, Mac, and Linux.\n\n[Sabaki](http://sabaki.yichuanshen.de/) is a very nice looking GUI with GTP 2\ncapability.\n\n[LeelaSabaki](https://github.com/SabakiHQ/LeelaSabaki) is modified to\nshow variations and winning statistics in the game tree, as well as a heatmap\non the game board.\n\nA lot of go software can interface to an engine via GTP,\nso look around.\n\nAdd the --gtp commandline option on the engine command line to enable Leela\nZero's GTP support. You will need a weights file, specify that with the -w option.\n\nAll required commands are supported, as well as the tournament subset, and\n\"loadsgf\". The full set can be seen with \"list_commands\". The time control\ncan be specified over GTP via the time\\_settings command. The kgs-time\\_settings\nextension is also supported. These have to be supplied by the GTP 2 interface,\nnot via the command line!\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "At the end of the game, you can send Leela Zero a \"dump\\_training\" command,\nfollowed by the winner of the game (either \"white\" or \"black\") and a filename,\ne.g:\n\n    dump_training white train.txt\n\nThis will save (append) the training data to disk, in the format described below,\nand compressed with gzip.\n\nTraining data is reset on a new game.\n\n",
      "technique": "Header extraction"
    }
  ]
}