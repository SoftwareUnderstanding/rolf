{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1706.05587\n\nIn this article, dilated convolution is mainly used to extract more compact features by removing the downsampling operation of the last few layers of the network and the upsampling operation of the corresponding filter kernel, without adding new additional learning parameters.\n\n![goingdeeper](https://github.com/akkaze/cnn-without-any-downsampling/blob/master/assets/goingdeeper.png"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8207940084462922
      ],
      "excerpt": "batch_normalization (BatchNo (None, 32, 32, 16)        64 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8444342525991423,
        0.8444342525991423,
        0.8444342525991423,
        0.9559715772848645,
        0.9559715772848645,
        0.9559715772848645,
        0.8955886365383559,
        0.8955886365383559,
        0.8955886365383559
      ],
      "excerpt": "conv2d_1 (Conv2D)            (None, 32, 32, 24)        3480 \nbatch_normalization_1 (Batch (None, 32, 32, 24)        96 \nactivation_1 (Activation)    (None, 32, 32, 24)        0 \nconv2d_2 (Conv2D)            (None, 32, 32, 32)        6944 \nbatch_normalization_2 (Batch (None, 32, 32, 32)        128 \nactivation_2 (Activation)    (None, 32, 32, 32)        0 \nconv2d_3 (Conv2D)            (None, 32, 32, 48)        13872 \nbatch_normalization_3 (Batch (None, 32, 32, 48)        192 \nactivation_3 (Activation)    (None, 32, 32, 48)        0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9120203064182911
      ],
      "excerpt": "dense (Dense)                (None, 10)                490 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "| 10  | 0.9200 |0.6346| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "| 30  | 0.7293 |0.7193| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "input_1 (InputLayer)            [(None, 32, 32, 3)]  0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8207940084462922
      ],
      "excerpt": "batch_normalization (BatchNorma (None, 32, 32, 16)   64          conv2d[0][0] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8207940084462922
      ],
      "excerpt": "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8207940084462922
      ],
      "excerpt": "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8207940084462922
      ],
      "excerpt": "batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_3[0][0] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8207940084462922
      ],
      "excerpt": "batch_normalization_4 (BatchNor (None, 32, 32, 16)   64          conv2d_4[0][0] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8207940084462922
      ],
      "excerpt": "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8207940084462922
      ],
      "excerpt": "batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8207940084462922
      ],
      "excerpt": "batch_normalization_7 (BatchNor (None, 32, 32, 16)   64          conv2d_7[0][0] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8207940084462922
      ],
      "excerpt": "batch_normalization_8 (BatchNor (None, 32, 32, 16)   64          conv2d_8[0][0] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8207940084462922
      ],
      "excerpt": "batch_normalization_9 (BatchNor (None, 32, 32, 16)   64          conv2d_9[0][0] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8207940084462922
      ],
      "excerpt": "batch_normalization_10 (BatchNo (None, 32, 32, 16)   64          conv2d_10[0][0] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8207940084462922
      ],
      "excerpt": "batch_normalization_11 (BatchNo (None, 32, 32, 16)   64          conv2d_11[0][0] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8207940084462922
      ],
      "excerpt": "batch_normalization_12 (BatchNo (None, 32, 32, 16)   64          conv2d_12[0][0] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645,
        0.9559715772848645,
        0.9559715772848645,
        0.9559715772848645,
        0.9559715772848645,
        0.9559715772848645,
        0.9015781078351486
      ],
      "excerpt": "conv2d_13 (Conv2D)              (None, 32, 32, 32)   4640        activation_12[0][0] \nbatch_normalization_13 (BatchNo (None, 32, 32, 32)   128         conv2d_13[0][0] \nactivation_13 (Activation)      (None, 32, 32, 32)   0           batch_normalization_13[0][0] \nconv2d_14 (Conv2D)              (None, 32, 32, 32)   9248        activation_13[0][0] \nconv2d_15 (Conv2D)              (None, 32, 32, 32)   4640        activation_12[0][0] \nbatch_normalization_14 (BatchNo (None, 32, 32, 32)   128         conv2d_14[0][0] \nadd_6 (Add)                     (None, 32, 32, 32)   0           conv2d_15[0][0] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645,
        0.9559715772848645,
        0.9559715772848645,
        0.9559715772848645,
        0.9559715772848645,
        0.9559715772848645,
        0.9015781078351486
      ],
      "excerpt": "activation_14 (Activation)      (None, 32, 32, 32)   0           add_6[0][0] \nconv2d_16 (Conv2D)              (None, 32, 32, 32)   9248        activation_14[0][0] \nbatch_normalization_15 (BatchNo (None, 32, 32, 32)   128         conv2d_16[0][0] \nactivation_15 (Activation)      (None, 32, 32, 32)   0           batch_normalization_15[0][0] \nconv2d_17 (Conv2D)              (None, 32, 32, 32)   9248        activation_15[0][0] \nbatch_normalization_16 (BatchNo (None, 32, 32, 32)   128         conv2d_17[0][0] \nadd_7 (Add)                     (None, 32, 32, 32)   0           activation_14[0][0] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645,
        0.9559715772848645,
        0.9559715772848645,
        0.9559715772848645,
        0.9559715772848645,
        0.9559715772848645,
        0.9015781078351486
      ],
      "excerpt": "activation_16 (Activation)      (None, 32, 32, 32)   0           add_7[0][0] \nconv2d_18 (Conv2D)              (None, 32, 32, 32)   9248        activation_16[0][0] \nbatch_normalization_17 (BatchNo (None, 32, 32, 32)   128         conv2d_18[0][0] \nactivation_17 (Activation)      (None, 32, 32, 32)   0           batch_normalization_17[0][0] \nconv2d_19 (Conv2D)              (None, 32, 32, 32)   9248        activation_17[0][0] \nbatch_normalization_18 (BatchNo (None, 32, 32, 32)   128         conv2d_19[0][0] \nadd_8 (Add)                     (None, 32, 32, 32)   0           activation_16[0][0] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645,
        0.9559715772848645,
        0.9559715772848645,
        0.9559715772848645,
        0.9559715772848645,
        0.9559715772848645,
        0.9015781078351486
      ],
      "excerpt": "activation_18 (Activation)      (None, 32, 32, 32)   0           add_8[0][0] \nconv2d_20 (Conv2D)              (None, 32, 32, 32)   9248        activation_18[0][0] \nbatch_normalization_19 (BatchNo (None, 32, 32, 32)   128         conv2d_20[0][0] \nactivation_19 (Activation)      (None, 32, 32, 32)   0           batch_normalization_19[0][0] \nconv2d_21 (Conv2D)              (None, 32, 32, 32)   9248        activation_19[0][0] \nbatch_normalization_20 (BatchNo (None, 32, 32, 32)   128         conv2d_21[0][0] \nadd_9 (Add)                     (None, 32, 32, 32)   0           activation_18[0][0] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645,
        0.9559715772848645,
        0.9559715772848645,
        0.9559715772848645,
        0.9559715772848645,
        0.9559715772848645,
        0.9015781078351486
      ],
      "excerpt": "activation_20 (Activation)      (None, 32, 32, 32)   0           add_9[0][0] \nconv2d_22 (Conv2D)              (None, 32, 32, 32)   9248        activation_20[0][0] \nbatch_normalization_21 (BatchNo (None, 32, 32, 32)   128         conv2d_22[0][0] \nactivation_21 (Activation)      (None, 32, 32, 32)   0           batch_normalization_21[0][0] \nconv2d_23 (Conv2D)              (None, 32, 32, 32)   9248        activation_21[0][0] \nbatch_normalization_22 (BatchNo (None, 32, 32, 32)   128         conv2d_23[0][0] \nadd_10 (Add)                    (None, 32, 32, 32)   0           activation_20[0][0] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645,
        0.9559715772848645,
        0.9559715772848645,
        0.9559715772848645,
        0.9559715772848645,
        0.9559715772848645,
        0.9015781078351486
      ],
      "excerpt": "activation_22 (Activation)      (None, 32, 32, 32)   0           add_10[0][0] \nconv2d_24 (Conv2D)              (None, 32, 32, 32)   9248        activation_22[0][0] \nbatch_normalization_23 (BatchNo (None, 32, 32, 32)   128         conv2d_24[0][0] \nactivation_23 (Activation)      (None, 32, 32, 32)   0           batch_normalization_23[0][0] \nconv2d_25 (Conv2D)              (None, 32, 32, 32)   9248        activation_23[0][0] \nbatch_normalization_24 (BatchNo (None, 32, 32, 32)   128         conv2d_25[0][0] \nadd_11 (Add)                    (None, 32, 32, 32)   0           activation_22[0][0] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645,
        0.9156566588472104,
        0.9156566588472104,
        0.9156566588472104,
        0.9156566588472104,
        0.9156566588472104,
        0.9156566588472104,
        0.8207940084462922
      ],
      "excerpt": "activation_24 (Activation)      (None, 32, 32, 32)   0           add_11[0][0] \nconv2d_26 (Conv2D)              (None, 32, 32, 64)   18496       activation_24[0][0] \nbatch_normalization_25 (BatchNo (None, 32, 32, 64)   256         conv2d_26[0][0] \nactivation_25 (Activation)      (None, 32, 32, 64)   0           batch_normalization_25[0][0] \nconv2d_27 (Conv2D)              (None, 32, 32, 64)   36928       activation_25[0][0] \nconv2d_28 (Conv2D)              (None, 32, 32, 64)   18496       activation_24[0][0] \nbatch_normalization_26 (BatchNo (None, 32, 32, 64)   256         conv2d_27[0][0] \nadd_12 (Add)                    (None, 32, 32, 64)   0           conv2d_28[0][0] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9156566588472104,
        0.9156566588472104,
        0.9156566588472104,
        0.9156566588472104,
        0.9156566588472104,
        0.9156566588472104,
        0.8207940084462922
      ],
      "excerpt": "activation_26 (Activation)      (None, 32, 32, 64)   0           add_12[0][0] \nconv2d_29 (Conv2D)              (None, 32, 32, 64)   36928       activation_26[0][0] \nbatch_normalization_27 (BatchNo (None, 32, 32, 64)   256         conv2d_29[0][0] \nactivation_27 (Activation)      (None, 32, 32, 64)   0           batch_normalization_27[0][0] \nconv2d_30 (Conv2D)              (None, 32, 32, 64)   36928       activation_27[0][0] \nbatch_normalization_28 (BatchNo (None, 32, 32, 64)   256         conv2d_30[0][0] \nadd_13 (Add)                    (None, 32, 32, 64)   0           activation_26[0][0] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9156566588472104,
        0.9156566588472104,
        0.9156566588472104,
        0.9156566588472104,
        0.9156566588472104,
        0.9156566588472104,
        0.8207940084462922
      ],
      "excerpt": "activation_28 (Activation)      (None, 32, 32, 64)   0           add_13[0][0] \nconv2d_31 (Conv2D)              (None, 32, 32, 64)   36928       activation_28[0][0] \nbatch_normalization_29 (BatchNo (None, 32, 32, 64)   256         conv2d_31[0][0] \nactivation_29 (Activation)      (None, 32, 32, 64)   0           batch_normalization_29[0][0] \nconv2d_32 (Conv2D)              (None, 32, 32, 64)   36928       activation_29[0][0] \nbatch_normalization_30 (BatchNo (None, 32, 32, 64)   256         conv2d_32[0][0] \nadd_14 (Add)                    (None, 32, 32, 64)   0           activation_28[0][0] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9156566588472104,
        0.9156566588472104,
        0.9156566588472104,
        0.9156566588472104,
        0.9156566588472104,
        0.9156566588472104,
        0.8207940084462922
      ],
      "excerpt": "activation_30 (Activation)      (None, 32, 32, 64)   0           add_14[0][0] \nconv2d_33 (Conv2D)              (None, 32, 32, 64)   36928       activation_30[0][0] \nbatch_normalization_31 (BatchNo (None, 32, 32, 64)   256         conv2d_33[0][0] \nactivation_31 (Activation)      (None, 32, 32, 64)   0           batch_normalization_31[0][0] \nconv2d_34 (Conv2D)              (None, 32, 32, 64)   36928       activation_31[0][0] \nbatch_normalization_32 (BatchNo (None, 32, 32, 64)   256         conv2d_34[0][0] \nadd_15 (Add)                    (None, 32, 32, 64)   0           activation_30[0][0] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9156566588472104,
        0.9156566588472104,
        0.9156566588472104,
        0.9156566588472104,
        0.9156566588472104,
        0.9156566588472104,
        0.8207940084462922
      ],
      "excerpt": "activation_32 (Activation)      (None, 32, 32, 64)   0           add_15[0][0] \nconv2d_35 (Conv2D)              (None, 32, 32, 64)   36928       activation_32[0][0] \nbatch_normalization_33 (BatchNo (None, 32, 32, 64)   256         conv2d_35[0][0] \nactivation_33 (Activation)      (None, 32, 32, 64)   0           batch_normalization_33[0][0] \nconv2d_36 (Conv2D)              (None, 32, 32, 64)   36928       activation_33[0][0] \nbatch_normalization_34 (BatchNo (None, 32, 32, 64)   256         conv2d_36[0][0] \nadd_16 (Add)                    (None, 32, 32, 64)   0           activation_32[0][0] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9156566588472104,
        0.9156566588472104,
        0.9156566588472104,
        0.9156566588472104,
        0.9156566588472104,
        0.9156566588472104,
        0.8207940084462922
      ],
      "excerpt": "activation_34 (Activation)      (None, 32, 32, 64)   0           add_16[0][0] \nconv2d_37 (Conv2D)              (None, 32, 32, 64)   36928       activation_34[0][0] \nbatch_normalization_35 (BatchNo (None, 32, 32, 64)   256         conv2d_37[0][0] \nactivation_35 (Activation)      (None, 32, 32, 64)   0           batch_normalization_35[0][0] \nconv2d_38 (Conv2D)              (None, 32, 32, 64)   36928       activation_35[0][0] \nbatch_normalization_36 (BatchNo (None, 32, 32, 64)   256         conv2d_38[0][0] \nadd_17 (Add)                    (None, 32, 32, 64)   0           activation_34[0][0] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9156566588472104
      ],
      "excerpt": "activation_36 (Activation)      (None, 32, 32, 64)   0           add_17[0][0] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9823492606796287
      ],
      "excerpt": "Similar ideas first appeared in paper of deeplab, [Rethinking Atrous Convolution for Semantic Image Segmentation]: https://arxiv.org/abs/1706.05587 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/akkaze/cnn-without-any-downsampling",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-11-30T14:19:52Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-10T21:27:28Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8433363942749937,
        0.8956767615413057
      ],
      "excerpt": "The accuracy curve on validation dataset is shown below, \nThe final accuracy rate reached 76%. The accuracy rate of a convolutional network with vgg structure with the same parameters is basically around this. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9836541455347556,
        0.9141436035489462,
        0.9788010031625837
      ],
      "excerpt": "This prompted us to think, is sampling really necessary? Of course, from an engineering point of view, sampling can greatly reduce the size of the feature map, thereby greatly reducing the amount of calculation. However, in this experimental surface, sampling does not help improve the performance of convolution neural network. Max pooling has the effect of suppressing noise, so it is useful , But max pooling can also be implemented without any downsampling, which is just like traditional median filers. \n<center>A typical median filtering. Here the size of the convolution kernel is 20. Note that the output image size has not been changed. Max pooing is similar to median filtering. Both can be used to suppress noise.</center> \nThis also shows that each  convolution layer is used to  encoding spatial correlations, shallow features encode short-range correlations, and deeper convolution layers encode longer-range spatial correlations. At a certain level, there is no longer Spatial correlation in the statistical sense (this depends on the size of meaningful objects in image). At this layer, you can use GAP to aggregate spatial features. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8604861344131268
      ],
      "excerpt": "As far as I know, I was the first one to use dilated convolution combined with global avergage pooling for image classification and segmentation. Even if there is no performance improvement (but basically no worsing). Note that dilated convolution is not necessary. A larger kernel size Convolution can replace it, but this will inevitably introduce more parameters, which may lead to overfitting. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A project demonstrate that downsampling(upsaming) in cnn are not nesscessary",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/akkaze/cnn-without-any-downsampling/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 7,
      "date": "Wed, 22 Dec 2021 18:11:33 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/akkaze/cnn-without-any-downsampling/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "akkaze/cnn-without-any-downsampling",
    "technique": "GitHub API"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/akkaze/cnn-without-any-downsampling/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 zhengankun\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "## How to  use this code\n\nThis repo requires **tensorflow-gpu-1.5.0** or other compatible version of tensorflow,\n\nyou can run it simple by type \n\n**python cifar10.py**\n\n### Does CNN really need downsampling (upsampling)?\n\nIn common convolutional neural networks, sampling is almost ubiquitous, formerly max_pooling, and now strided convolution.\nTake the vgg network as an example, which uses quite a lot of max_pooling,\n\n![vgg](https://github.com/akkaze/cnn-without-any-downsampling/blob/master/assets/vgg.png)\n<center>The input side is below, you can see that a lot of 2x2 pooling is used in the network</center>\nAlso, when doing semantic segmentation or object detection, we use quite a lot of upsampling, or transposed convolution.\n\n![fcn](https://github.com/akkaze/cnn-without-any-downsampling/blob/master/assets/fcn.png)\n<center>Typical fcn structure, pay attention to the decovolution distinguished by red</center>\nPreviously, we used fc in the last few layers of the classification network. Later, fc was proved to have too many parameters and poor generalization performance. It was replaced by global average pooling and it first appeared in network in network.\n\n![gap](https://github.com/akkaze/cnn-without-any-downsampling/blob/master/assets/gap.jpg)\n<center>GAP aggregates spatial features directly into a scalar</center>\nSince then, the paradigm of classification networks behaves like(Relu has been integrated into conv and deconv, without considering any shortcut),\n\n```\nInput-->Conv-->DownSample_x_2-->Conv-->DownSample_x_2-->Conv-->DownSample_x_2-->GAP-->Conv1x1-->Softmax-->Output\n```\n\nAnd the paradigm of semantic segmentation network behaves like,\n\n```\nInput-->Conv-->DownSample_x_2-->Conv-->DownSample_x_2-->Conv-->DownSample_x_2-->Deconv_x_2-->Deconv_x_2-->Deconv_x_2-->Softmax-->Output\n```\n\nHowever, we have to think about it. Is downsampling and upsampling really necessary? Is it impossible to remove it?\n\nOn the classification task of cifar10, I tried to remove the downsampling, change the convolution to a dilated convolution, and the dialation rate increased respectively. The model structure is shown below.\n\n```python\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "cnn-without-any-downsampling",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "akkaze",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/akkaze/cnn-without-any-downsampling/blob/master/Readme.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 72,
      "date": "Wed, 22 Dec 2021 18:11:33 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "convolutional-neural-network",
      "deep-learning",
      "keras",
      "tensorflow"
    ],
    "technique": "GitHub API"
  }
}