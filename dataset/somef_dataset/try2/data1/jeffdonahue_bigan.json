{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1605.09782",
      "https://arxiv.org/abs/1605.09782"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{donahue2016bigan,\n  Author = {Donahue, Jeff and Kr\\\"ahenb\\\"uhl, Philipp and Darrell, Trevor},\n  Journal = {arXiv preprint arXiv:1605.09782},\n  Title = {Adversarial Feature Learning},\n  Year = {2016}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9998710384112545,
        0.9933537248551054,
        0.9999537354568557,
        0.9824840693628997,
        0.9954488832581693
      ],
      "excerpt": "Please consider citing Adversarial Feature Learning if you use this code in your work: \n  Author = {Donahue, Jeff and Kr\\\"ahenb\\\"uhl, Philipp and Darrell, Trevor}, \n  Journal = {arXiv preprint arXiv:1605.09782}, \n  Title = {Adversarial Feature Learning}, \n  Year = {2016} \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8252756551763197
      ],
      "excerpt": "NND/100: 13.54  NND/10: 13.48  NND: 13.44  NNC_e: 91.50%  NNC_e-: 96.84%  CLS_e-: 91.39%  EGr: 13.64  EGr_b: 13.64  EGg: 3.00  EGg_b: 3.00 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9725140271112827
      ],
      "excerpt": "NND/100: 7.37  NND/10: 7.26  NND: 7.19  NNC_e: 89.94%  NNC_e-: 92.56%  CLS_e-: 86.72%  EGr: 8.70  EGr_b: 9.55  EGg: 3.77  EGg_b: 5.84 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9372151656983196
      ],
      "excerpt": "NND/100: 5.54  NND/10: 4.98  NND: 4.61  NNC_e: 95.41%  NNC_e-: 96.28%  CLS_e-: 91.33%  EGr: 7.29  EGr_b: 9.51  EGg: 5.24  EGg_b: 7.79 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9904682582335301
      ],
      "excerpt": "NND/100: 5.41  NND/10: 4.66  NND: 4.14  NNC_e: 92.10%  NNC_e-: 97.35%  CLS_e-: 79.48%  EGr: 5.95  EGr_b: 9.60  EGg: 5.20  EGg_b: 9.15 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9372151656983196
      ],
      "excerpt": "NND/100: 54.81  NND/10: 53.49  NND: 52.74  NNC_e: 31.78%  NNC_e-: 35.97%  CLS_e-: 48.80%  EGr: 59.23  EGr_b: 59.26  EGg: 14.03  EGg_b: 13.94 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9372151656983196
      ],
      "excerpt": "NND/100: 55.91  NND/10: 54.21  NND: 53.45  NNC_e: 19.26%  NNC_e-: 23.23%  CLS_e-: 37.91%  EGr: 65.22  EGr_b: 66.69  EGg: 38.27  EGg_b: 38.42 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9975061413939055
      ],
      "excerpt": "NND/100: 42.16  NND/10: 39.78  NND: 37.99  NNC_e: 30.34%  NNC_e-: 31.55%  CLS_e-: 46.10%  EGr: 60.35  EGr_b: 75.33  EGg: 47.99  EGg_b: 62.76 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8272368526802568
      ],
      "excerpt": "NND/100: 42.80  NND/10: 40.06  NND: 37.69  NNC_e: 34.67%  NNC_e-: 34.74%  CLS_e-: 51.80%  EGr: 65.58  EGr_b: 92.14  EGg: 64.16  EGg_b: 92.81 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9228174472672006
      ],
      "excerpt": "NND/100: 42.11  NND/10: 39.13  NND: 36.93  NNC_e: 39.89%  NNC_e-: 42.55%  CLS_e-: 58.34%  EGr: 54.51  EGr_b: 79.90  EGg: 50.90  EGg_b: 78.35 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8252756551763226
      ],
      "excerpt": "NND/100: 42.11  NND/10: 39.09  NND: 36.67  NNC_e: 34.31%  NNC_e-: 48.05%  CLS_e-: 58.40%  EGr: 50.84  EGr_b: 81.20  EGg: 47.38  EGg_b: 79.76 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.881848456479086
      ],
      "excerpt": "NND/100: 41.96  NND/10: 38.97  NND: 36.57  NNC_e: 32.09%  NNC_e-: 48.54%  CLS_e-: 52.21%  EGr: 50.72  EGr_b: 80.74  EGg: 47.39  EGg_b: 79.31 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "    --max_labels 100 --epochs 100 --decay_epochs 100 --disp_interval 5 --save_interval 10 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9454111819844788
      ],
      "excerpt": "NND/100: 55.33  NND/10: 53.66  NND: 52.97  NNC_e: 33.15%  NNC_e-: 34.10%  CLS_e-: 52.66%  EGr: 60.46  EGr_b: 61.04 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9372151656983196
      ],
      "excerpt": "NND/100: 57.75  NND/10: 56.89  NND: 55.36  NNC_e: 29.67%  NNC_e-: 25.55%  CLS_e-: 41.33%  EGr: 71.01  EGr_b: 69.69 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "NND/100: 44.22  NND/10: 40.68  NND: 38.06  NNC_e: 42.06%  NNC_e-: 39.54%  CLS_e-: 63.26%  EGr: 55.03  EGr_b: 80.23 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9728768603164633
      ],
      "excerpt": "NND/100: 43.76  NND/10: 40.14  NND: 37.51  NNC_e: 33.76%  NNC_e-: 47.17%  CLS_e-: 63.29%  EGr: 52.96  EGr_b: 80.15 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9996778142843665
      ],
      "excerpt": "NND/100: 43.77  NND/10: 40.08  NND: 37.46  NNC_e: 34.30%  NNC_e-: 48.60%  CLS_e-: 54.37%  EGr: 53.20  EGr_b: 80.37 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8670498468297771
      ],
      "excerpt": "wget 'https://people.eecs.berkeley.edu/~jdonahue/pretrained_bigan_weights.zip' \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9848179726092176,
        0.9974719242546096
      ],
      "excerpt": "100) JD: 0.0003  E: 10.0109  G: 10.0109 \nNND/100: 48.05  NND/10: 44.93  NND: 42.61  NNC_e: 2.30%  NNC_e-: 3.78%  CLS_e-: 9.39%  EGr: 64.98  EGr_b: 82.64  EGg: 63.28  EGg_b: 84.10 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9659166110223061
      ],
      "excerpt": "NND/100: 50.76  NND/10: 46.44  NND: 43.74  NNC_e: 2.27%  NNC_e-: 3.84%  CLS_e-: 12.25%  EGr: 66.49  EGr_b: 81.08 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9881764689005513
      ],
      "excerpt": "0.425175004158    0.70 0.43 0.35 0.46 0.14 0.40 0.64 0.42 0.43 0.18 0.38 0.33 0.64 0.52 0.78 0.19 0.28 0.30 0.60 0.32 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jeffdonahue/bigan",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-05-03T07:53:39Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-21T02:09:53Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.958207066477898,
        0.9037335221382273
      ],
      "excerpt": "This is the official code release for Adversarial Feature Learning (arXiv), including code to train and evaluate BiGANs \u2014 Bidirectional Generative Adversarial Networks \u2014 as well as the alternative GAN-based approaches to feature learning we evaluated. \nThe training code requires Theano and is based on the official DCGAN code from Alec Radford et al. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9712107736721353
      ],
      "excerpt": "Here the encoder and generator losses are always equal, but this is not always the case (as in the latent regressor below). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8362466107086602,
        0.9477035047347335,
        0.8866542395865437,
        0.8464794631738501,
        0.8935299894465151,
        0.9629017067400456
      ],
      "excerpt": "NNC* and CLS* measure \"feature\" quality by either a 1-nearest-neighbor (NNC) or logistic regression (CLS) classifier (higher is better). \n*_e and *_e- denote the feature space, with _e being E(x) itself, and _e- being the layer of encoder features immediately before the output. (The latter normally works better.) \nEG* measures reconstruction error (lower is better). \nEGr is L2 error || x - G(E(x)) ||, averaged across real data samples x ~ p(x) \nEGg is also L2 error, but averaged across generated samples x = G(z), z ~ p(z): || G(z) - G(E(G(z))) || \nThe corresponding *_b measures are \"baselines\", where the reconstruction error is computed against a random input, i.e. || x' - G(E(x)) || where x and x' are each random samples. The ratio EGr / EGr_b gives a more meaningful notion of reconstruction accuracy than EGr alone; e.g., if EGr ~= EGr_b as in epoch 0 above, no meaningful reconstruction is happening. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9289397443264116
      ],
      "excerpt": "400.png contains generated samples G(z) at the end of training (400 epochs): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9023199197596387
      ],
      "excerpt": "To also learn a \"latent regressor\" encoder E by minimizing reconstruction error L(z, E(G(z))), set a non-zero encode_weight. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8577903013004905,
        0.9501040637901483
      ],
      "excerpt": "Finally, we can set a non-zero encode_gen_weight to jointly optimize the generator to both fool the discriminator and reconstruct z per the latent regressor loss. \n(Here we set the weight to 0.25; a weight of 1 results in a degenerate solution.) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8544849601034479
      ],
      "excerpt": "For the (joint) latent regressor baselines, change the OBJECTIVE=... setting appropriately (see MNIST instructions above). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8200551084260383,
        0.9445942714925436
      ],
      "excerpt": "With more classes, each epoch takes proportionately longer, \nso we suggest also training for fewer epochs and evaluating/saving more frequently: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9217045830560764,
        0.8098248789238716
      ],
      "excerpt": "The only difference is that we append the arguments --raw_size 128 --crop_size 112 --crop_resize 64 specifying the larger encoder input size (see train_imagenet_highres_encoder.sh). \nDue to the higher resolution encoder inputs, a single training epoch takes a bit longer: ~28 seconds on a Titan X (vs. ~24 seconds for a standard BiGAN). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9122839340629234
      ],
      "excerpt": "To train on more than 10 classes, see the additional arguments from the \"More data\" subsection above. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8601878887922155,
        0.8204290125567466
      ],
      "excerpt": "This file includes both the standard and generalized weights, with the raw NumPy weights saved by train_gan.py, as well as the converted and magic-init'ed caffemodels used for the PASCAL VOC feature learning experiments. \nTo download and install these weights at the locations assumed in eval_model.sh (see below), do the following from the root of this repository: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8359571171995739
      ],
      "excerpt": "After training a BiGAN (or other model) as shown above, it can be evaluated by transferring the encoder weights to auxiliary supervised learning tasks like classification and detection. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.866561930876877,
        0.8336914098490965,
        0.9768134710544497
      ],
      "excerpt": "for detection experiments, use rbgirshick's version of Caffe submoduled in Fast R-CNN (see below) \n\"Magic\" (AKA data-dependent) initializations: magic-init by @philkr (with a few modifications) \nused for the random initializations of the fully connected layers fc6-8, and recalibration of the conv layer scales for more effective fine-tuning \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8288062747677384
      ],
      "excerpt": "train_cls.py uses voc-classification to train the model for VOC classification and evaluate it.) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "code for \"Adversarial Feature Learning\"",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jeffdonahue/bigan/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 49,
      "date": "Tue, 21 Dec 2021 09:05:58 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jeffdonahue/bigan/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "jeffdonahue/bigan",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/jeffdonahue/bigan/master/theanosetup.sh",
      "https://raw.githubusercontent.com/jeffdonahue/bigan/master/train_imagenet_highres_encoder.sh",
      "https://raw.githubusercontent.com/jeffdonahue/bigan/master/train_imagenet.sh",
      "https://raw.githubusercontent.com/jeffdonahue/bigan/master/train_mnist.sh",
      "https://raw.githubusercontent.com/jeffdonahue/bigan/master/eval_model.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Create a directory `./data/imagenet` under the root of this repository.\nThis directory should contain:\n * `train.txt`\n * `train/`\n * `val.txt`\n * `val/`\n\nThe `*.txt` files are lists of labeled images as used in Caffe.\nSee the [Caffe ImageNet tutorial](http://caffe.berkeleyvision.org/gathered/examples/imagenet.html) (specifically the `get_ilsvrc_aux.sh` script) to download them, or prepare them yourself as follows.\n`train.txt` lists image paths relative to `./data/imagenet/train` and integer labels (`val.txt` is analogous):\n\n    n01440764/n01440764_10026.JPEG 0\n    n01440764/n01440764_10027.JPEG 0\n    n01440764/n01440764_10029.JPEG 0\n    n01440764/n01440764_10040.JPEG 0\n    [...]\n    n15075141/n15075141_9933.JPEG 999\n    n15075141/n15075141_9942.JPEG 999\n    n15075141/n15075141_999.JPEG 999\n    n15075141/n15075141_9993.JPEG 999\n\nRelative to the root of this repository, the first image listed above should be located at `./data/imagenet/train/n01440764/n01440764_10026.JPEG`.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Create a directory `./data/mnist` under the root of this repository.\nThis directory should contain the MNIST data files (or symlinks to them) with these names:\n\n    t10k-images.idx3-ubyte\n    t10k-labels.idx1-ubyte\n    train-images.idx3-ubyte\n    train-labels.idx1-ubyte\n\nThe `train_mnist.sh` script trains a \"permutation-invariant\" BiGAN (by default) on the MNIST dataset.\nMNIST training takes about 30 minutes on a Titan X GPU (400 epochs at ~3.3 seconds per epoch).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9752480306872585
      ],
      "excerpt": "You should see output like the following: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8167894585395217
      ],
      "excerpt": "For the (joint) latent regressor baselines, change the OBJECTIVE=... setting appropriately (see MNIST instructions above). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9752480306872585
      ],
      "excerpt": "You should see output like the following: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8882842272445765
      ],
      "excerpt": "To download and install these weights at the locations assumed in eval_model.sh (see below), do the following from the root of this repository: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8125126379591333
      ],
      "excerpt": "for classification experiments, use philkr's \"future\" version of Caffe linked from voc-classification (see below) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9481770552450954
      ],
      "excerpt": "To run eval_model.sh yourself, follow these steps: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8510237012496932
      ],
      "excerpt": "Modify the variables (CAFFE_DIR, MAGIC_DIR, CLASS_DIR) near the top of eval_model.sh specifying the paths where you installed these packages. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8156048215014386
      ],
      "excerpt": "NND/100: 5.41  NND/10: 4.66  NND: 4.14  NNC_e: 92.10%  NNC_e-: 97.35%  CLS_e-: 79.48%  EGr: 5.95  EGr_b: 9.60  EGg: 5.20  EGg_b: 9.15 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8531546400693047
      ],
      "excerpt": "NND/100: 55.91  NND/10: 54.21  NND: 53.45  NNC_e: 19.26%  NNC_e-: 23.23%  CLS_e-: 37.91%  EGr: 65.22  EGr_b: 66.69  EGg: 38.27  EGg_b: 38.42 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8399924733648418
      ],
      "excerpt": "NND/100: 42.16  NND/10: 39.78  NND: 37.99  NNC_e: 30.34%  NNC_e-: 31.55%  CLS_e-: 46.10%  EGr: 60.35  EGr_b: 75.33  EGg: 47.99  EGg_b: 62.76 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8648165619324122
      ],
      "excerpt": "NND/100: 42.80  NND/10: 40.06  NND: 37.69  NNC_e: 34.67%  NNC_e-: 34.74%  CLS_e-: 51.80%  EGr: 65.58  EGr_b: 92.14  EGg: 64.16  EGg_b: 92.81 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8369026202930907
      ],
      "excerpt": "NND/100: 42.11  NND/10: 39.09  NND: 36.67  NNC_e: 34.31%  NNC_e-: 48.05%  CLS_e-: 58.40%  EGr: 50.84  EGr_b: 81.20  EGg: 47.38  EGg_b: 79.76 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.802628903544772
      ],
      "excerpt": "    --max_labels 100 --epochs 100 --decay_epochs 100 --disp_interval 5 --save_interval 10 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8533812536771774
      ],
      "excerpt": "NND/100: 57.75  NND/10: 56.89  NND: 55.36  NNC_e: 29.67%  NNC_e-: 25.55%  CLS_e-: 41.33%  EGr: 71.01  EGr_b: 69.69 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.847380065085022
      ],
      "excerpt": "NND/100: 44.48  NND/10: 41.47  NND: 39.31  NNC_e: 33.15%  NNC_e-: 35.50%  CLS_e-: 50.67%  EGr: 71.20  EGr_b: 88.41 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8941718192476898
      ],
      "excerpt": "NND/100: 45.09  NND/10: 41.66  NND: 39.16  NNC_e: 36.93%  NNC_e-: 39.74%  CLS_e-: 56.34%  EGr: 58.49  EGr_b: 81.76 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8240802018684753
      ],
      "excerpt": "NND/100: 43.76  NND/10: 40.14  NND: 37.51  NNC_e: 33.76%  NNC_e-: 47.17%  CLS_e-: 63.29%  EGr: 52.96  EGr_b: 80.15 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8205720125861328
      ],
      "excerpt": "You can download the pretrained BiGAN ImageNet weights used in the paper from here (zip file, 530 MB). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8082022887640018,
        0.8017285540429218
      ],
      "excerpt": "unzip pretrained_bigan_weights.zip \nrm pretrained_bigan_weights.zip \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8358842810983622
      ],
      "excerpt": "Loading 26 params from: ./exp/imagenet_1000_size72_u-200_bigan/models/100_encode_params.jl \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8712482456573534
      ],
      "excerpt": "Loading 23 params from: ./exp/imagenet_1000_size72_u-200_bigan/models/100_joint_discrim_params.jl \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8253272493896298
      ],
      "excerpt": "100) JD: 0.0003  E: 10.0109  G: 10.0109 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8358842810983622
      ],
      "excerpt": "Loading 26 params from: ./exp/imagenet_1000_size128_resize64_u-200_bigan/models/100_encode_params.jl \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8712482456573534
      ],
      "excerpt": "Loading 23 params from: ./exp/imagenet_1000_size128_resize64_u-200_bigan/models/100_joint_discrim_params.jl \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.80868514521432
      ],
      "excerpt": "NND/100: 50.76  NND/10: 46.44  NND: 43.74  NNC_e: 2.27%  NNC_e-: 3.84%  CLS_e-: 12.25%  EGr: 66.49  EGr_b: 81.08 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8321731569327117,
        0.928208928818935
      ],
      "excerpt": "0.588507234158    0.80 0.58 0.63 0.68 0.33 0.53 0.75 0.63 0.57 0.48 0.51 0.45 0.67 0.64 0.83 0.38 0.57 0.52 0.68 0.53 \ntest       10 100%|##############################################################################|Time: 0:01:52 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9274811899200657
      ],
      "excerpt": "train      10 100%|##############################################################################|Time: 0:01:54 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jeffdonahue/bigan/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other",
      "url": "https://raw.githubusercontent.com/jeffdonahue/bigan/master/LICENSE"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'UC Berkeley\\'s Standard Copyright and Disclaimer Notice:\\n\\nCopyright (c) 2016-2017, Jeff Donahue and The Regents of the University of California (Regents). All Rights Reserved.\\n\\nPermission to use, copy, modify, and distribute this software and its documentation for educational, research, and not-for-profit purposes, without fee and without a signed licensing agreement, is hereby granted, provided that the above copyright notice, this paragraph and the following two paragraphs appear in all copies, modifications, and distributions. Contact The Office of Technology Licensing, UC Berkeley, 2150 Shattuck Avenue, Suite 510, Berkeley, CA 94720-1620, (510) 643-7201, for commercial licensing opportunities.\\n\\nIN NO EVENT SHALL REGENTS BE LIABLE TO ANY PARTY FOR DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES, INCLUDING LOST PROFITS, ARISING OUT OF THE USE OF THIS SOFTWARE AND ITS DOCUMENTATION, EVEN IF REGENTS HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. REGENTS SPECIFICALLY DISCLAIMS ANY WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE SOFTWARE AND ACCOMPANYING DOCUMENTATION, IF ANY, PROVIDED HEREUNDER IS PROVIDED \"AS IS\". REGENTS HAS NO OBLIGATION TO PROVIDE MAINTENANCE, SUPPORT, UPDATES, ENHANCEMENTS, OR MODIFICATIONS.\\n\\n--------------------------------------------------------\\n\\nAdapted from the Theano DCGAN software (https://github.com/Newmu/dcgan_code) by Alec Radford.\\nThe License for this software is as follows:\\n\\nThe MIT License (MIT)\\n\\nCopyright (c) 2015 Alec Radford\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Adversarial Feature Learning",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "bigan",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "jeffdonahue",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jeffdonahue/bigan/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 217,
      "date": "Tue, 21 Dec 2021 09:05:58 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "    python resize_imageset.py -r -j 4 ${SIZE} ./data/imagenet ./data/imagenet${SIZE}\n\nWith an argument of `--raw_size 72` (for example), `train_gan.py` will automatically check if the presized image directory `./data/imagenet72` exists before falling back to `./data/imagenet`.\n\n",
      "technique": "Header extraction"
    }
  ]
}