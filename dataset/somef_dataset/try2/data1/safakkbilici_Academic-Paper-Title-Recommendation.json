{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1412.6980",
      "https://arxiv.org/abs/1412.6980"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (Raffel et al., 2019)\n- BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension (Lewis et al., 2019)\n- GloVe: Global Vectors for Word Representation (Pennington et al., 2014)\n- Long Short-Term Memory (Hochreiter and Schmidhuber, 1997)\n- arXiv Dataset, Cornell University.\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{safakb/eness,\n  author = {M. Safak BILICI, E. Sadi UYSAL},\n  title = {Generating Titles With Sequential Models},\n  year = {2020},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/safakkbilici/Academic-Paper-Title-Recommendation}}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9568193957733768
      ],
      "excerpt": "Title: Sparsity-certifying Graph Decompositions \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8995754874345715
      ],
      "excerpt": "graphs. Special instances of sparse graphs appear in rigidity theory and have \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8231581381982848
      ],
      "excerpt": "pebbles generalize and strengthen the previous results of Lee and Streinu and \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "Top 10 categories \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9930701451585323
      ],
      "excerpt": "TOP 10 AUTHOR BY PUBLICATON: \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/safakkbilici/Academic-Paper-Title-Recommendation",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-12-11T07:47:36Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-07T17:53:32Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9203200924398439,
        0.9426291299910415
      ],
      "excerpt": "[['Using Language-Based Summarization Methods', \n  'A New Method for Language Modeling', \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9017947925559381,
        0.9927390389858326
      ],
      "excerpt": "Sequence to sequence Natural Processing Language tasks are progressing rapidly. They can used for Language Modeling, Machine Translation, Question Answering, and Summarization. \nIn this project, our aim is to create a supervised summarization model that generates titles from academic papers (their abstracts). Paper titles are exiciting section of papers, some of them relevent with paper content, some of them are not (e.g Attention Is All You Need). Titles motivate people to read the papers, have a big role on accepted submission to conferences. Maybe it can help people write more creative titles. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8440364436613044,
        0.8136894909683914,
        0.9372342368616713
      ],
      "excerpt": "We create a crawler for collecting papers' titles and abstracts from arXiv which is a open pre-print and e-print website for papers and free. You can find the source code and manual for the crawler in README.md file under the crawler directory of repository. It can be used for collecting data from specific subfield of arXiv papers. However we used a kaggle dataset when we train our models. So pretrained models are also trained with this dataset. \nKaggle dataset is in .json format, we provided scripts for parsing data. Also tokenization and other preprocessing scripts are provided, they are explained in tutorial section. \nThe cleaned, parsed and tokenized version of data is provided in ./data folder as compressed. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9116753379120259,
        0.9413710281881306,
        0.9714184679175076
      ],
      "excerpt": "Abstract:   We describe a new algorithm, the $(k,\\ell)$-pebble game with colors, and use \nit obtain a characterization of the family of $(k,\\ell)$-sparse graphs and \nalgorithmic solutions to a family of problems concerning tree decompositions of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9683840212640369,
        0.9926284700672866,
        0.9693013106287357,
        0.9675458919947385
      ],
      "excerpt": "pebbles generalize and strengthen the previous results of Lee and Streinu and \ngive a new proof of the Tutte-Nash-Williams characterization of arboricity. We \nalso present a new decomposition that certifies sparsity based on the \n$(k,\\ell)$-pebble game with colors. Our work also exposes connections between \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.908925214220865
      ],
      "excerpt": "Westermann and Hendrickson. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.908925214220865,
        0.9942364023378256
      ],
      "excerpt": "Authors Parsed: Ileana Streinu and Louis Theran \nSeq2Seq LSTM is written with Keras. The summary of model looks like: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8577302739126875
      ],
      "excerpt": "decoder_dense (Dense)           (None, None, 2001)   202101      decoder_lstm[0][0]                \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9131331214634584,
        0.9193453694289209
      ],
      "excerpt": "We trained seq2seq LSTM on Colab's Tesla K80, with 11 GB of GPU RAM for 11 hours. \nThe Seq2Seq LSTM model can learn wording of academic titles and capture the main topic of the paper, but it cannot specify the title. Results are not sufficient. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123675387138547
      ],
      "excerpt": "!!Colab is highly recommended!! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9490487377047591,
        0.9906566696261683,
        0.9611343541077212
      ],
      "excerpt": "The T5 model was presented in Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer by Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu. \nT5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks and for which each task is converted into a text-to-text format. T5 works well on a variety of tasks out-of-the-box by prepending a different prefix to the input corresponding to each task, e.g., for translation: translate English to German, ... for summarization: summarize... \nT5 is an encoder-decoder model and converts all NLP problems into a text-to-text format. It is trained using teacher forcing. This means that for training we always need an input sequence and a target sequence. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9492697922687282
      ],
      "excerpt": "We trained T5 Base model on Colab's Tesla K80, with 11 GB of GPU RAM for 8 hours.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9252072499685648,
        0.9037840169563277
      ],
      "excerpt": "Best results are from T5. It can learn wordings well, and it also can learn the topic detailed (also it can show what is new in the paper with recommended title). Even though the dataset mainly contains Physics papers, it can learn the context, wording, what's novel in paper etc in various sub-field of science. \nThe checkpoints of our model is provided in the drive link. Create a shortcut of this shared file then run the script \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8073843580965777
      ],
      "excerpt": "The prediction comes with: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8943071148807478,
        0.9832049249105174,
        0.8846224442291497,
        0.8500367948098034,
        0.9115221161097238
      ],
      "excerpt": "abss =[\"summarize: \"+\"\"\"We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes.  \nOn the test data, we achieved top-1 and top-5 error rates of 39.7% and 18.9% which is considerably better than the previous state-of-the-art results.  \nThe neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers,  \nand two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets.  \nTo reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.\"\"\"] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.873434696406716
      ],
      "excerpt": "Top 50 Used Words In Abstracts With Word Length > 3 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8490037945672047
      ],
      "excerpt": "   The BABAR Collaboration: 193 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Supervised text summarization (title generation/recommendation) based on academic paper abstracts, with Seq2Seq LSTM and T5.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/safakkbilici/Academic-Paper-Title-Recommendation/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Tue, 28 Dec 2021 03:40:20 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/safakkbilici/Academic-Paper-Title-Recommendation/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "safakkbilici/Academic-Paper-Title-Recommendation",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/safakkbilici/Academic-Paper-Title-Recommendation/tree/main/docs"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/safakkbilici/Academic-Paper-Title-Recommendation/main/build.sh",
      "https://raw.githubusercontent.com/safakkbilici/Academic-Paper-Title-Recommendation/main/T5/build_local.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Dowload the [arXiv dataset from Kaggle](https://www.kaggle.com/Cornell-University/arxiv).\n\nSave this in arbitrary path. Then run\n\n```bash\npython3 prep_data.py --datapath /path/to/your_json_file.json\n```\n\nThis script parses your .json file then converts into .csv file, after that it does NLP techniques for abstracts.\nAfter this script, you will have ./data/df_to_model.csv file. You can use it for training from scratch. \n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8238404648107593,
        0.8238404648107593
      ],
      "excerpt": "train_df['prefix'] = \"summarize\" \neval_df['prefix'] = \"summarize\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9748709027320682
      ],
      "excerpt": "GPU Utilization \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9287190341490074
      ],
      "excerpt": "A sample example from json data: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.906614234580968
      ],
      "excerpt": "Layer (type)                    Output Shape         Param \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8934745846565956
      ],
      "excerpt": "Total params: 1,623,501 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8851298975315551
      ],
      "excerpt": "First extract the .csv file from ./data/df_to_model.tar.gz to ./data folder (or create it from stracth). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991,
        0.9104031697361714
      ],
      "excerpt": "python3 train_lstm.py \n./models folder contains checkpoints for specific epochs. Move your .json, .npy and .h5 file into ./modes (default = epoch 100). Then run \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8268911223583573
      ],
      "excerpt": "python3 generate_lstm.py --abstract /path/to/your_abstract_file.txt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8723008126209506
      ],
      "excerpt": "<img src=\"./docs/img/t5.jpg\" alt=\"drawing\" width=\"700\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.844148656202008
      ],
      "excerpt": "<img src=\"./docs/img/training_loss.png\" alt=\"drawing\" width=\"700\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.844148656202008
      ],
      "excerpt": "<img src=\"./docs/img/gpu_util.png\" alt=\"drawing\" width=\"700\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.844148656202008
      ],
      "excerpt": "<img src=\"./docs/img/gpu_powerusage.png\" alt=\"drawing\" width=\"700\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "from simpletransformers.t5 import T5Model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984,
        0.8594142235991984
      ],
      "excerpt": "    \"reprocess_input_data\": True, \n    \"overwrite_output_dir\": True, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "    \"do_sample\": True, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8525645461651434
      ],
      "excerpt": "predicted_title = model.predict(abss) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.844148656202008
      ],
      "excerpt": "<img src=\"./docs/img/stats1.png\" alt=\"drawing\" width=\"700\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.844148656202008
      ],
      "excerpt": "<img src=\"./docs/img/stats2.png\" alt=\"drawing\" width=\"700\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.844148656202008
      ],
      "excerpt": "<img src=\"./docs/img/stats3.png\" alt=\"drawing\" width=\"700\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.844148656202008
      ],
      "excerpt": "<img src=\"./docs/img/stats5.png\" alt=\"drawing\" width=\"700\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.844148656202008
      ],
      "excerpt": "<img src=\"./docs/img/stats4.png\" alt=\"drawing\" width=\"700\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8182384578601853
      ],
      "excerpt": "<img src=\"./docs/img/missing.jpeg\" alt=\"drawing\" width=\"500\"/> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/safakkbilici/Academic-Paper-Title-Recommendation/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "JavaScript",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Academic Paper Title Recommendation",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Academic-Paper-Title-Recommendation",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "safakkbilici",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/safakkbilici/Academic-Paper-Title-Recommendation/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 14,
      "date": "Tue, 28 Dec 2021 03:40:20 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "t5",
      "natural-language-processing",
      "summarization",
      "transfer-learning",
      "transfer-learning-nlp",
      "lstm",
      "seq2seq",
      "transformer",
      "crawler",
      "arxiv-dataset",
      "arxiv",
      "arxiv-crawler"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- [Adam: A Method for Stochastic Optimization (Kingma et al., 2014)](https://arxiv.org/abs/1412.6980)\n\n        [['A Stochastic Optimization Theory Method for Non-stationary Strategies',\n          'A Probabilistic Algorithm for Stochastic Optimization using Adaptive Moments',\n          'Algorithm for gradient-based optimization of stochastic functions']]\n          \n- [Language Models are Few-Shot Learners (Brown et al., 2020)](https://arxiv.org/abs/1412.6980)\n\n        [['Train-off: scaling up language models for precision data sets',\n          'GPT-3: A Comparable Evaluation of Task and Benchmarking',\n          'Testing performance of fine-tuning languages']]\n\nAlso it can recommend very useful titles for you papers. For example based on our paper called \"Data Communication Protocols In Wireless Sensor\nNetworks\", based on this abstract:\n\n        This paper describes the concept of sensor networks which has been made viable by the convergence of micro-\n        electro-mechanical systems technology, wireless communications and digital electronics. First, the sensing tasks and the\n        potential sensor networks applications are explored, and a review of factors influencing the design of sensor networks is\n        provided. Then, the communication architecture for sensor networks is outlined, and the algorithms and protocols\n        developed for each layer in the literature are explored. Open research issues for the realization of sensor networks are\n        also discussed.\n\nthis titles are recommended/generated:\n\n        ['Sensor networks: The novel concepts and perspectives',\n         'Sensor Networks: Convergence and Applications',\n         'The Applied Field Theory Theory of Sensor Networks for e+ e- to Sensing Networks']]\n         \nYou can find other examples in ./T5 directory (in the README.md).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "<img src=\"./docs/gif/demo.gif\" width=\"700\" height=\"400\" />\n\nLocal server for demo is created by Flask API. Just run \n```\ncd demo\nconda activate simpletransformers\npython3 app.py\n```\nand play!\n\n",
      "technique": "Header extraction"
    }
  ]
}