{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This repo borrows code from several repos, like [maskrcnn-benchmark](https://github.com/facebookresearch/maskrcnn-benchmark), [PSMNet](https://github.com/JiaRenChang/PSMNet), [FCOS](https://github.com/tianzhi0549/FCOS) and [kitti-object-eval-python](https://github.com/traveller59/kitti-object-eval-python).\n\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2001.03398"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find our work useful in your research, please consider citing:\n```\n@article{chen2020dsgn,\n  title={DSGN: Deep Stereo Geometry Network for 3D Object Detection},\n  author={Chen, Yilun and Liu, Shu and Shen, Xiaoyong and Jia, Jiaya},\n  journal={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},\n  year={2020}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{chen2020dsgn,\n  title={DSGN: Deep Stereo Geometry Network for 3D Object Detection},\n  author={Chen, Yilun and Liu, Shu and Shen, Xiaoyong and Jia, Jiaya},\n  journal={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},\n  year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.99992522158855,
        0.9554441738822752
      ],
      "excerpt": "Yilun Chen, Shu Liu, Xiaoyong Shen, Jiaya Jia. <br/> \n[Paper]&nbsp;  [Video]&nbsp;  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8200061201136728
      ],
      "excerpt": "and visualize your training process by accessing https://localhost:6666 on your browser. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8302556419090275
      ],
      "excerpt": "            <td><a href=\"https://drive.google.com/open?id=1pbvyRGOknlovmIK96MwEyvV0_z76Bfks\"> GoogleDrive </a></td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8302556419090275
      ],
      "excerpt": "            <td rowspan=2><a href=\"https://drive.google.com/open?id=1L14QisrQMyIbowhSSOf_FjaOx9CVe0oF\"> GoogleDrive </a></td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "            <td>10.0</td> \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dvlab-research/DSGN",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you have any questions or suggestions about this repo, please feel free to contact me (chenyilun95@gmail.com).\n\n",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-12-20T14:09:23Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-16T02:51:44Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9941580874716932
      ],
      "excerpt": "This is the official implementation of DSGN (CVPR 2020), a strong 3D object detector proposed to jointly estimate scene depth and detect 3D objects in 3D world with only input of a stereo image pair. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9978543418783007
      ],
      "excerpt": "Most state-of-the-art 3D object detectors heavily rely on LiDAR sensors and there remains a large gap in terms of performance between image-based and LiDAR-based methods, caused by inappropriate representation for the prediction in 3D scenarios. Our method, called Deep Stereo Geometry Network (DSGN), reduces this gap significantly by detecting 3D objects on a differentiable volumetric representation \u2013 3D geometric volume, which effectively encodes 3D geometric structure for 3D regular space. With this representation, we learn depth information and semantic cues simultaneously. For the first time, we provide a simple and effective one-stage stereo-based 3D detection pipeline that jointly estimates the depth and detects 3D objects in an end-to-end learning manner. Our approach outperforms previous stereo-based 3D detectors (about 10 higher in terms of AP) and even achieves comparable performance with a few LiDAR-based methods on the KITTI 3D object detection leaderboard. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8020643493312405
      ],
      "excerpt": "The training models, configuration and logs will be saved in the model folder. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8546183985523813
      ],
      "excerpt": "We provide several pretrained models for our experiments, which are evaluated on KITTI val set. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8214646476162296
      ],
      "excerpt": "            <!-- <th>Inference Time(s/im)</th> --> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9196241412598118
      ],
      "excerpt": "            <th>3D AP</th> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "DSGN: Deep Stereo Geometry Network for 3D Object Detection (CVPR 2020)",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/chenyilun95/DSGN/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 45,
      "date": "Mon, 27 Dec 2021 03:36:17 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/dvlab-research/DSGN/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "dvlab-research/DSGN",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/chenyilun95/DSGN/master/scripts/mptrain_disp.sh",
      "https://raw.githubusercontent.com/chenyilun95/DSGN/master/scripts/batch_test.sh",
      "https://raw.githubusercontent.com/chenyilun95/DSGN/master/scripts/mptrain_human.sh",
      "https://raw.githubusercontent.com/chenyilun95/DSGN/master/scripts/mptrain_disp_trainval.sh",
      "https://raw.githubusercontent.com/chenyilun95/DSGN/master/scripts/mptrain_human_trainval.sh",
      "https://raw.githubusercontent.com/chenyilun95/DSGN/master/scripts/mptrain_car.sh",
      "https://raw.githubusercontent.com/chenyilun95/DSGN/master/scripts/mptrain_car_trainval.sh",
      "https://raw.githubusercontent.com/chenyilun95/DSGN/master/dsgn/utils/rotate_iou/compile.sh",
      "https://raw.githubusercontent.com/chenyilun95/DSGN/master/dsgn/eval/kitti-object-eval-python/eval.sh",
      "https://raw.githubusercontent.com/chenyilun95/DSGN/master/dsgn/eval/kitti-object-eval-python/eval_dist.sh",
      "https://raw.githubusercontent.com/chenyilun95/DSGN/master/dsgn/eval/kitti/eval_05.sh",
      "https://raw.githubusercontent.com/chenyilun95/DSGN/master/dsgn/eval/kitti/eval.sh",
      "https://raw.githubusercontent.com/chenyilun95/DSGN/master/dsgn/eval/kitti/compile.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "(1) Please download the KITTI dataset and create the model folders. KITTI dataset is avaible [here](http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d). Download KITTI [point clouds](https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_velodyne.zip), [left images](https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_image_2.zip), [right images](https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_image_3.zip), [calibrations matrices](https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_calib.zip) and [object labels](https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_label_2.zip). \n```\nln -s /path/to/KITTI_DATA_PATH ./data/kitti/\nln -s /path/to/OUTPUT_PATH ./outputs/\n```\n\n(2) Generate the depth map from the ground-truth LiDAR point cloud and save them in ./data/kitti/training/depth/.\n```\npython3 preprocessing/generate_disp.py --data_path ./data/kitti/training/ --split_file ./data/kitti/trainval.txt \npython3 preprocessing/generate_disp.py --data_path ./data/kitti/training/ --split_file ./data/kitti/trainval.txt --right_calib\n```\n\n(3) Pre-compute the bbox targets in pre-defined grid and save them in ./outputs/temp/.\n```\npython3 tools/generate_targets.py --cfg CONFIG_PATH\n```\n\nAfter training the models, the overall directory will look like below:\n```\n.                                           (root directory)\n|-- dsgn                                    (dsgn library file)\n|-- configs                                 (model configurations folder)\n|-- ...\n|-- data\n|   |-- kitti                               (dataset directory)\n|       |-- train.txt                       (KITTI train images list (3712 samples))\n|       |-- val.txt                         (KITTI val images list (3769 samples))\n|       |-- test.txt                        (KITTI test images list (7518 samples))\n|       |-- training\n|       |   |-- image_2\n|       |   |-- image_3\n|       |   |-- ...\n|       |-- testing\n|       |-- depth                           (generated depth map)\n|-- outputs\n    |-- MODEL_DSGN_v1                       (Model config and snapshots should be saved in the same model folder)\n        |-- finetune_53.tar                 (saved model)\n        |-- save_config.py                  (saved model configuration file)\n        |-- save_config.py.tmp              (automatic generated copy of previous configuration)\n        |-- training.log                    (full training log)\n        |-- result_kitti_finetune_53.txt    (kitti evaluated results for the saved model)\n        |-- kitti_output                    (kitti detection results folder)\n    |-- MODEL_DSGN_v2\n    |-- temp                                (temporary folder for saving the pre-computed bbox targets)\n        |-- ...                             (pre-computed bbox targets under some specific configurations)\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "(1) Clone this repository.\n```\ngit clone https://github.com/chenyilun95/DSGN.git && cd DSGN\n```\n\n(2) Setup Python environment.\n```\nconda activate -n dsgn\npip install -r requirements.txt --user\n\n#:#: conda deactivate dsgn\n```\n\n(3) Compile the rotated IoU library. \n```\ncd dsgn/utils/rotate_iou && bash compile.sh & cd ../../../\n```\n\n(4) Compile and install DSGN library.\n```\n#: the following will install the lib with symbolic links, so that\n#: you can modify the file if you want and won't need to re-build it.\npython3 setup.py build develop --user\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9414119019616536
      ],
      "excerpt": "bash scripts/mptrain_xxx.sh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.84093609179227
      ],
      "excerpt": "Besides, you can start a tensorboard session by \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8836704212480256
      ],
      "excerpt": " <img src=\"doc/sample_result.png\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8347328371885108
      ],
      "excerpt": " <img src=\"doc/pipeline.png\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9162779728121012
      ],
      "excerpt": "<p align=\"center\"> <img src=\"./doc/result.jpg\" width=\"80%\"></p> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8256683821319274
      ],
      "excerpt": "python3 tools/train_net.py --cfg ./configs/config_xxx.py --savemodel ./outputs/MODEL_NAME -btrain 4 -d 0-3 --multiprocessing-distributed \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8119277887598771,
        0.8053944180809512
      ],
      "excerpt": "The training models, configuration and logs will be saved in the model folder. \nTo load some pretrained model, you can run \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8082656709902393,
        0.8093180256491097
      ],
      "excerpt": "            <th>Train Mem (GB/Img)</th> \n            <th>Test Mem (GB/Img)</th> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8435705523010719
      ],
      "excerpt": "[x] Multiprocessing GPU training \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8398776326968947
      ],
      "excerpt": "[x] Reduce training GPU memory usage \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/dvlab-research/DSGN/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "C++",
      "Cuda",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2018 \\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "DSGN",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "DSGN",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "dvlab-research",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dvlab-research/DSGN/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "All the codes are tested in the following environment:\n* Ubuntu 16.04\n* Python 3.7\n* PyTorch 1.1.0 or 1.2.0 or 1.3.0\n* Torchvision 0.2.2 or 0.4.1\n\nThe models reported in paper are trained with 4 *NVIDIA Tesla V100* (32G) GPUs with batch-size 4. The training GPU memory requirement is close to 29G and the testing GPU memory requirement is feasible for a normal *NVIDIA TITAN* (12G) GPU. One full image pair is fed into the network and used to construct the 3D volume. For your reference, PSMNet is trained with input patch size of 512x256. Please note your GPU memory. \n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 266,
      "date": "Mon, 27 Dec 2021 03:36:17 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "3d-detection",
      "stereo-vision",
      "depth-estimation",
      "cvpr2020"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We provide a video demo for showing the result of DSGN. Here we show the predicted depth map and 3D detection results on both front view (the left camera view) and bird's eye view (the ground-truth point cloud).\n\n<p align=\"center\"> <a href=\"https://www.youtube.com/watch?v=u6mQW89wBbo\"><img src=\"./doc/demo_cover.jpg\" width=\"50%\"></a> </p>\n\n",
      "technique": "Header extraction"
    }
  ]
}