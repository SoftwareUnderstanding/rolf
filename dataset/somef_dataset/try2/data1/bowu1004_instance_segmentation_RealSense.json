{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1703.06870",
      "https://arxiv.org/abs/1703.06870",
      "https://arxiv.org/abs/1602.00763</br>\nOriginal python implementation of SORT by Alex Bewley: https://github.com/abewley/sort\n\nSORT proposes using a Kalman filter to predict the trajectory of previously identified objects, and then match them with newly identified objects. In this program, when an object is matched with a detection, the real-world position and distance from camera are added as attributes to the KalmanBoxTracker object. When the same object is tracked to the next frame, linear speed, velocity, real-world distance, and time until impact are all added under the same object. Each KalmanBoxTracker is added to the appropriate DetectedObject as the attribute DetectredObject.track. This means all the data can be passed to an API using a single DetectedObject.\n\n### 4.2 Velocity Vector Arrows\n\nOptionally, vector arrows can be superimposed on the image. These vector arrows show the direction the object is moving in 3D space. Each arrow is represented through the Arrow3D class, which essentially is the same as the FancyArrowPatch class from matplotlib, with additional 3D support."
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9562026397941471
      ],
      "excerpt": "Simple Online and Real-time Tracking (SORT) paper: https://arxiv.org/abs/1602.00763</br> \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/bowu1004/instance_segmentation_RealSense",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-10-20T17:12:32Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-30T06:23:36Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9837250694633097,
        0.9388269022194616,
        0.974963428753929
      ],
      "excerpt": "Detectron2's Mask R_CNN with a ReSNet-101-FPN backbone was determined to be the optimal model. Upon comparing Detectron2 to MMDetection's models, which won first place in the 2018 segmentation COCO challenge, it is evident that the choice of model is appropriate for high-speed real-time video. \nWhen comparing Detectron2's Mask R_CNN to MMDetection's Mask R_CNN, Detectron2 outperforms in both mask AP (38.6 vs 35.9) and inference time (0.070 s/im vs 0.105 s/im). MMDetectron does have models that are slightly more accurate than Detectron2's Mask R_CNN implementation, such as the Hybrid Task Cascade model (HTC) however these often result in models that output masks at less than 4 fps. When adding the time to ouput the superimposed images, this would be insufficient for real-time. \nDetectron2's Model Zoo displays the inference time and Mask AP for each model provided. For the Mask R_CNN models, the FPN model with a ResNet101 backbone has the best Mask AP for the short time it takes for inferences. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.856591008901412
      ],
      "excerpt": "The SCORE_THRESHOLD or cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST line specifies the lower threshold for when the instance segmentation mask is shown to the user. For example, set SCORE_THRESHOLD=0.65 or cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.65. If Detectron2 is at least 65% confident the object detected belongs to a class name, the mask is superimposed onto the image. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9355608827508441,
        0.9552332473488468,
        0.9484286546297099
      ],
      "excerpt": "The cfg.INPUT.MIN_SIZE_TEST line specifies the size of the smallest size of the image during testing/inference. If this is set to 0, resizing is disabled. \nThe RESOLUTION_X(i.e. 640, 1280) and RESOLUTION_Y(360(BW:cannot work in this PC, min:480),480, 720) specify the resolution of camera streams from D435. \nAccording to Intel's paper, Best-Known-Methods for Tuning Intel\u00ae RealSense\u2122 D400 Depth Cameras for Best Performance, The depth RMS (root mean square) error increases rapidly when placing objects further away, especially when the distance is greater than 3m. The orange line on the graph below represents the depth RMS error on a D435 with HFOV=90deg, Xres=848, baseline=50mm and for subpixel=0.08. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9603330543424115
      ],
      "excerpt": "When the object is too close to the camera, the depth values will return 0m. This threshold is known as MinZ. The formula for calculating MinZ is \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.890112329508483,
        0.9204249101493783,
        0.9837008141801825,
        0.9879145366121396,
        0.9236440080120041
      ],
      "excerpt": "Therefore with a depth resolution of 848x480, the MinZ is ~16.8cm. If the object is within this distance, no value is returned. \nSimilar to MinZ, MaxZ exists too. For the D435, the MaxZ is approximately 10m. Any object outside this range will also be recorded as 0m. \nSometimes objects can be recorded as 0m even though they are inside the MinZ and MaxZ threshold. This usually occurs when there is too much noise on the depth image. This can occur when the target is not well textured. For more information on how to configure the D435 for specific environments and objects, refer to this paper. \nTo find the distance of each object, the median depth pixel is used. All pixels associated to the object are abstracted to a histogram with a max distance of 10m (Max range of the D435), and 500 bins. The bins are looped through until the bin which contains the median is found. This means that the depth values will change with intervals of 0.02m. \nFor smaller intervals of 0.01m, change the NUM_BINS constant to 1000, and change \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9871125637572288,
        0.9903955584185618
      ],
      "excerpt": "The purpose of this project is to propose where objects exists in the environment around a robot. In addition to this, it would be ideal to understand the movement of each object. \nThe velocity, linear speed (between camera and object), and time to impact were all calculated using an altered version of Chris Fotache's implementation of SORT with PyTorch, created by Alex Bewley, Zongyuan Ge, Lionel Ott, Fabio Ramos and Ben Upcroft. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Realtime instance segmentation by Detectron2 and Intel RealSense D435.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/bowu1004/instance_segmentation_RealSense/releases",
    "technique": "GitHub API"
  },
  "faq": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Testing was performed on this program, where the real distances of objects from the D435 were compared to the distance measured by the stereo sensors on the D435. The true distance was found by measuring the distance between a box (with a flat front) and the parallel plane of the imagers.\n\n<img src=\"img/depth_vs_range.png\"/>\n\nfrom [Intel D400 Datasheet](https://www.mouser.ca/pdfdocs/Intel_D400_Series_Datasheet.pdf)\n\nThe D435 recordings were measured on the [realsense-viewer](https://github.com/IntelRealSense/librealsense/tree/master/tools/realsense-viewer) program. The stereo resolution was set to 1280 x 720. Rather than the depth RMS error, the absolute depth error was compared to the real distance of the object to the D435.\n![d435_error_table](img/d435_error_table.png)\n![d435_error_graph](img/d435_error_graph.png)\n\nThis graph shows that the absolute error appears to exponentially increases when the distance increases. This means the depth recordings will be most accurate when the object is closer to the camera.\n\n",
      "technique": "Header extraction"
    }
  ],
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sat, 25 Dec 2021 10:10:32 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/bowu1004/instance_segmentation_RealSense/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "bowu1004/instance_segmentation_RealSense",
    "technique": "GitHub API"
  },
  "invocation": [
    {
      "confidence": [
        0.8314823419435533
      ],
      "excerpt": "<img src=\"img/detectron2_model_zoo.png\" /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8144517307817205
      ],
      "excerpt": "The SCORE_THRESHOLD or cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST line specifies the lower threshold for when the instance segmentation mask is shown to the user. For example, set SCORE_THRESHOLD=0.65 or cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.65. If Detectron2 is at least 65% confident the object detected belongs to a class name, the mask is superimposed onto the image. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8314823419435533
      ],
      "excerpt": "<img src=\"img/d435_rms_error.png\" /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8339556183526199
      ],
      "excerpt": "centre_depth = \"{:.2f}m\".format(x / 100) \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/bowu1004/instance_segmentation_RealSense/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# 1. Usage",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "instance_segmentation_RealSense",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "bowu1004",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/bowu1004/instance_segmentation_RealSense/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 8,
      "date": "Sat, 25 Dec 2021 10:10:32 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "**Requirements/Dependencies**\n\n- Windows10 (You can find the solution for Linux or macOS [here](https://github.com/ErikGDev/instance-segmentation))\n- Python \u2265 3.6\n- PyTorch \u2265 1.3\n- [torchvision](https://github.com/pytorch/vision/) that matches the PyTorch installation.\n\tYou can install them together at [pytorch.org](https://pytorch.org) to make sure of this. Please ensure that your version of CUDA is also compatible when installing. You can run this code without CUDA, but it will be much slower, e.g. 10x slower but not really test it.\n- OpenCV `pip3 install opencv-python`\n- Intel RealSense SDK 2.0 Installation: [here](https://www.intelrealsense.com/sdk-2/)\n- PyRealSense `pip3 install pyrealsense2`\n- Build Detectron2:\n  + **Build Detectron2 from Source**\n\n     [Windows] Install Visual C++ Build tools form [this link](https://answers.microsoft.com/en-us/windows/forum/windows_10-windows_install/microsoft-visual-c-140-is-required-in-windows-10/f0445e6b-d461-4e40-b44f-962622628de7).  Then restart your PC, then you also need to upgrade Python setup tools, by running this command: `pip3 install --upgrade setuptools`.\n\n     Then you can install Detectron2 from source by running:\n     ```bash\n     [Note: This should be the easiest way to build Detectron2 in Windows10!]\n     pip install git+https://github.com/facebookresearch/detectron2.git\n     #: (add --user if you don't have permission)\n\n     #: Or, to install it from a local clone:\n     git clone https://github.com/facebookresearch/detectron2.git\n     cd detectron2 && pip3 install -e .\n\n     #: Or if you are on macOS\n     #: CC=clang CXX=clang++ pip install -e .\n     ```\n\n     If the installation is not proper, you may see the error of \"cannot import name '_C' #157\" when running the `main_xxx.py`.\n\n     For more details on the installation of Detectron2 and its dependencies, please refer to the [official Detectron2 GitHub](https://github.com/facebookresearch/detectron2).\n\n**After Installation**\n\n1. Clone or download this repository.\n2. To perform instance segmentation straight from a D435 camera attached to a USB port:\n  * Run one of the two python files i.e. `main_xxx_win10.py`\n  * If using .bag files:\n    * Type 'python3 main_xxx_win10.py --file={filename}' where {filename} is the name of the input .bag file. To create .bag files, use d435_to_file.py in [this repository](https://github.com/ErikGDev/instance-segmentation/tree/master/tools).\n\n---\n\n_(For conveniently recalling the background, I here copy and paste most of the content from [this awesome rep](https://github.com/ErikGDev/instance-segmentation) as below.)_\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "|  | Backbone | AP | AP<sub>50</sub> | AP<sub>75</sub> | AP<sub>S</sub> | AP<sub>M</sub> | AP<sub>L</sub> |\n| :--- | :--- | :---: | :---: | :---: |  :---:  | :---: | :---: |\n| Original Mask R-CNN   | ResNet-101-FPN  | 35.7 | 58.0 | 37.8 | 15.5 | 38.1 | 52.4 |\n| Matterport Mask R-CNN | ReSNet-101-FPN | 38.0 | 55.8 | <b>41.3</b> | 17.9 | <b>45.5</b> | <b>55.9</b> |\n| Detectron2 Mask R-CNN | ReSNet-101-FPN | <b>38.6</b> | <b>60.4</b> | <b>41.3</b> | <b>19.5</b> | 41.3 | 55.3 |\n\nValidation tests were perfomed on the segmentation masks created on the **2017 COCO** validation dataset. The standard COCO validation metrics include average AP over IoU thresholds, AP<sub>50</sub>, AP<sub>75</sub>, and AP<sub>S</sub>, AP<sub>M</sub> and AP<sub>L</sub> (AP at different scales). These results were then compared to COCO validation results from the [original paper](https://arxiv.org/abs/1703.06870) and a popular [Mask R-CNN implementation by Matterport](https://github.com/matterport/Mask_RCNN). Clearly, Detectron2's Mask R_CNN outperforms the original Mask R_CNN and Matterport's Mask R_CNN with respect to average precision. It also outperformed SOTA COCO segmentation competition winners from the [2015 and 2016 challenge](http://cocodataset.org/#detection-leaderboard).\n\n",
      "technique": "Header extraction"
    }
  ]
}