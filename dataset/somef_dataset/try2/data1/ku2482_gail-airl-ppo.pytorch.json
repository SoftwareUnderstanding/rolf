{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1710.11248",
      "https://arxiv.org/abs/1707.06347",
      "https://arxiv.org/abs/1801.01290",
      "https://arxiv.org/abs/1812.05905",
      "https://arxiv.org/abs/1710.11248 (2017).\n\n[[3]](https://arxiv.org/abs/1707.06347) Schulman, John, et al. \"Proximal policy optimization algorithms.\" arXiv preprint https://arxiv.org/abs/1707.06347 (2017).\n\n[[4]](https://arxiv.org/abs/1801.01290) Haarnoja, Tuomas, et al. \"Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.\" arXiv preprint https://arxiv.org/abs/1801.01290 (2018).\n\n[[5]](https://arxiv.org/abs/1812.05905) Haarnoja, Tuomas, et al. \"Soft actor-critic algorithms and applications.\" arXiv preprint https://arxiv.org/abs/1812.05905 (2018).",
      "https://arxiv.org/abs/1707.06347 (2017).\n\n[[4]](https://arxiv.org/abs/1801.01290) Haarnoja, Tuomas, et al. \"Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.\" arXiv preprint https://arxiv.org/abs/1801.01290 (2018).\n\n[[5]](https://arxiv.org/abs/1812.05905) Haarnoja, Tuomas, et al. \"Soft actor-critic algorithms and applications.\" arXiv preprint https://arxiv.org/abs/1812.05905 (2018).",
      "https://arxiv.org/abs/1801.01290 (2018).\n\n[[5]](https://arxiv.org/abs/1812.05905) Haarnoja, Tuomas, et al. \"Soft actor-critic algorithms and applications.\" arXiv preprint https://arxiv.org/abs/1812.05905 (2018).",
      "https://arxiv.org/abs/1812.05905 (2018)."
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[[1]](http://papers.nips.cc/paper/6391-generative-adversarial-imitation-learning) Ho, Jonathan, and Stefano Ermon. \"Generative adversarial imitation learning.\" Advances in neural information processing systems. 2016.\n\n[[2]](https://arxiv.org/abs/1710.11248) Fu, Justin, Katie Luo, and Sergey Levine. \"Learning robust rewards with adversarial inverse reinforcement learning.\" arXiv preprint arXiv:1710.11248 (2017).\n\n[[3]](https://arxiv.org/abs/1707.06347) Schulman, John, et al. \"Proximal policy optimization algorithms.\" arXiv preprint arXiv:1707.06347 (2017).\n\n[[4]](https://arxiv.org/abs/1801.01290) Haarnoja, Tuomas, et al. \"Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.\" arXiv preprint arXiv:1801.01290 (2018).\n\n[[5]](https://arxiv.org/abs/1812.05905) Haarnoja, Tuomas, et al. \"Soft actor-critic algorithms and applications.\" arXiv preprint arXiv:1812.05905 (2018).\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ku2482/gail-airl-ppo.pytorch",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-08-23T19:08:39Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-23T11:26:11Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9668474724662603
      ],
      "excerpt": "This is a PyTorch implementation of Generative Adversarial Imitation Learning(GAIL)[1] and Adversarial Inverse Reinforcement Learning(AIRL)[2] based on PPO[3]. I tried to make it easy for readers to understand the algorithm. Please let me know if you have any questions. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9511120671742012
      ],
      "excerpt": "You need to collect demonstraions using trained expert's weight. Note that --std specifies the standard deviation of the gaussian noise add to the action, and --p_rand specifies the probability the expert acts randomly. We set std to 0.01 not to collect too similar trajectories. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8281453905730819
      ],
      "excerpt": "Mean returns of experts we use in the experiments are listed below. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A PyTorch implementation of GAIL and AIRL based on PPO.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ku2482/gail-airl-ppo.pytorch/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 14,
      "date": "Fri, 24 Dec 2021 19:20:24 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ku2482/gail-airl-ppo.pytorch/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "ku2482/gail-airl-ppo.pytorch",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You can install Python liblaries using `pip install -r requirements.txt`. Note that you need a MuJoCo license. Please follow the instruction in [mujoco-py](https://github.com/openai/mujoco-py\n) for help.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8411004553040458
      ],
      "excerpt": "    --cuda --env_id InvertedPendulum-v2 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8411004553040458
      ],
      "excerpt": "    --algo gail --cuda --env_id InvertedPendulum-v2 \\ \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python collect_demo.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8828665034782968
      ],
      "excerpt": "    --buffer_size 1000000 --std 0.01 --p_rand 0.0 --seed 0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8835493258479138,
        0.9046910653595559
      ],
      "excerpt": "| InvertedPendulum-v2.pth | 0.01 | 0.0 | 1000(1000)  | \n| Hopper-v3.pth | 0.01 | 0.0 | 2534(2791) | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python train_imitation.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8381535334701395
      ],
      "excerpt": "    --buffer buffers/InvertedPendulum-v2/size1000000_std0.01_prand0.0.pth \\ \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ku2482/gail-airl-ppo.pytorch/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 Toshiki Watanabe\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "GAIL and AIRL in PyTorch",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "gail-airl-ppo.pytorch",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "ku2482",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ku2482/gail-airl-ppo.pytorch/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 39,
      "date": "Fri, 24 Dec 2021 19:20:24 GMT"
    },
    "technique": "GitHub API"
  }
}