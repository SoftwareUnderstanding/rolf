{
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/sthalles/deeplab_v3",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-12-18T16:38:50Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-23T05:35:14Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9137209231222988,
        0.9670250975238693
      ],
      "excerpt": "Implementation of the Semantic Segmentation DeepLab_V3 CNN as described at Rethinking Atrous Convolution for Semantic Image Segmentation. \nFor a complete documentation of this implementation, check out the blog post. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8780502957492957
      ],
      "excerpt": "The custom_train.txt file contains the name of the images selected for training. This file is designed to use the Pascal VOC 2012 set as a TESTING set. Therefore, it doesn't contain any images from the VOC 2012 val dataset. For more info, see the Training section of Deeplab Image Semantic Segmentation Network. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9853018970142913
      ],
      "excerpt": "For full documentation on serving this Semantic Segmentation CNN, refer to How to deploy TensorFlow models to production using TF Serving. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.895512839501481
      ],
      "excerpt": "To export the model and to perform client requests do the following: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Tensorflow Implementation of the Semantic Segmentation DeepLab_V3 CNN",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/sthalles/deeplab_v3/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 288,
      "date": "Mon, 27 Dec 2021 11:24:43 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/sthalles/deeplab_v3/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "sthalles/deeplab_v3",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/sthalles/deeplab_v3/master/dataset/CreateTfRecord.ipynb",
      "https://raw.githubusercontent.com/sthalles/deeplab_v3/master/serving/deeplab_client.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8597133716358103
      ],
      "excerpt": "To use a different dataset, you just need to modify the CreateTfRecord.ipynb notebook inside the dataset/ folder, to suit your needs. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.902530862251927
      ],
      "excerpt": "Note: You do not need both datasets. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.821791110497746
      ],
      "excerpt": "All the serving scripts are placed inside: ./serving/. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.957409158501759
      ],
      "excerpt": "Create a python3 virtual environment and install the dependencies from the serving_requirements.txt file; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9482974219888054,
        0.8773050938564111
      ],
      "excerpt": "Create a python2 virtual environment and install the dependencies from the client_requirements.txt file; \nFrom the python2 env, run the deeplab_client.ipynb notebook; \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.812205556494373,
        0.9377921601705367,
        0.9196755985763088,
        0.911337624156172
      ],
      "excerpt": "Once you have the training and validation TfRefords files, just run the command bellow. Before running Deeplab_v3, the code will look for the proper ResNets checkpoints inside ./resnet/checkpoints, if the folder does not exist, it will first be downloaded. \npython train.py --starting_learning_rate=0.00001 --batch_norm_decay=0.997 --crop_size=513 --gpu_id=0 --resnet_model=resnet_v2_50 \nCheck out the train.py file for more input argument options. Each run produces a folder inside the tboard_logs directory (create it if not there). \nTo evaluate the model, run the test.py file passing to it the model_id parameter (the name of the folder created inside tboard_logs during training). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9515752551715031
      ],
      "excerpt": "python test.py --model_id=16645 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8462986696028474
      ],
      "excerpt": "The custom_train.txt file contains the name of the images selected for training. This file is designed to use the Pascal VOC 2012 set as a TESTING set. Therefore, it doesn't contain any images from the VOC 2012 val dataset. For more info, see the Training section of Deeplab Image Semantic Segmentation Network. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8261803081547449
      ],
      "excerpt": "Using the python3 env, run deeplab_saved_model.py. The exported model should reside into ./serving/model/; \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/sthalles/deeplab_v3/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "DeepLab_V3 Image Semantic Segmentation Network",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "deeplab_v3",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "sthalles",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/sthalles/deeplab_v3/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "sthalles",
        "body": "",
        "dateCreated": "2018-05-16T12:43:33Z",
        "datePublished": "2018-05-17T12:24:25Z",
        "html_url": "https://github.com/sthalles/deeplab_v3/releases/tag/v1.0.0",
        "name": "First release of Tensorflow implementation of DeepLab_V3",
        "tag_name": "v1.0.0",
        "tarball_url": "https://api.github.com/repos/sthalles/deeplab_v3/tarball/v1.0.0",
        "url": "https://api.github.com/repos/sthalles/deeplab_v3/releases/11046690",
        "zipball_url": "https://api.github.com/repos/sthalles/deeplab_v3/zipball/v1.0.0"
      }
    ],
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Python 3.x\n- Numpy\n- Tensorflow 1.10.1\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 790,
      "date": "Mon, 27 Dec 2021 11:24:43 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "tensorflow",
      "deeplab-v3",
      "semantic-segmentation",
      "deep-learning",
      "python"
    ],
    "technique": "GitHub API"
  }
}