{
  "citation": [
    {
      "confidence": [
        0.9738274355964066
      ],
      "excerpt": "For more on SMOTE you can visit this link  https://arxiv.org/pdf/1106.1813.pdf \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8043073075947352
      ],
      "excerpt": "if(aggs.shape == (6,9)): \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Stamatis-Ilias/PLAsTiCC-Astronomical-Classification",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-05-25T18:18:00Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-01-17T22:46:38Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9827232678075372,
        0.9574704687555694,
        0.9750965257083789,
        0.9875084983053539
      ],
      "excerpt": "The data that was used in this project came from the LSST dataset, an astronomical dataset made by  values based on the survey data of the telescope, that when analyzed will help classify the astronomical data that the telescope will collect in the future. \nThe Data consists of a train set and a test set, containing 7848 and 3490000 objects respectively, making the train to test size ratio rather small. This was the biggest challenge that we faced in this competition, and it was an understandable hurdle regarding the nature of the data. Precision was the goal of this competition and we believe we did a rather good job in that area. \nDue to the huge size of the test set 20GB we used a google console cloud linux system, equiped with 56GB of RAM and a 10-core processor, to help do the computations as fast as possible. Even so, many ideas we had in mind didn't have the time to be implemented for the competition but we will describe them briefly later. \nThe goal of the challenge is to classify each object to a target, based on features we will extract from the above data. Even though a simple classification problem sounds easy, the fact that the data ratio is skewed is not the only problem that prevents the problem from being simple. Looking at the graph below Taken from Study.ipynb, we can see that the distribution of the targets are not equal. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9981817820315089
      ],
      "excerpt": "For this reason, when classifying the Data we preproccessed it with SMOTE, which is a certain algorithm for providing the training set with more data, as to stabilize the disparities between the unequal size classes. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9972664599566387
      ],
      "excerpt": "We can see that it is divided in 3 to 4 different time ranges, making the study of such a timeseries harder than normal as it is missing values in certain places. To compat this, we used an Autoencoder later in the challenge, that was train on the data and made it's own normalized and time invariant data. This could help in 2 ways \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9469779160667403
      ],
      "excerpt": "The first helped our results quite a lot, while the second was one of those things that we didn't have the time to finish, as the autoencoded model just for the training set upped its size by 200%. (And that's because it was filling gaps in the data). This means that the test set variant would be quite huge. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9841449335749717
      ],
      "excerpt": "Another way we tried to compat the above was, by dividing the flux series in 3-4 pieces based on a threshold and then applying feature extraction in these places seperately. This worked great in the training set, but failed to work in the test set cause we didn't took into account that the data there was seperated a lot differently. The code for the above is given below:<br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9695001360072597
      ],
      "excerpt": "    threshold = 120 #:Threshold found as the minimum of all the \"dead-time-spaces\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9099408635739442,
        0.9804369543001832
      ],
      "excerpt": "Because the above failed to work in the test-set, we just did statistical analysis to the set without any further preprocessing. \nIn most of the files we have implemented our own algorithms for extracting features, which are the following.<br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8539943039240899
      ],
      "excerpt": "2) Find the Dynamic Time Wrapping (DTW) between each passband for one curve (meaning 15 features for all of 6 passbands).<br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9988279247559337
      ],
      "excerpt": "In the data we have two kinds of values that help find the host galaxy of the star, and these are hostgal_specz and (hostgal_photoz, hostgal_photoz_err). The first of these values, is of greater importance and could help our work greatly, but it is not available in the whole of the test set. For this, we tried to fit the hostgal_photoz of each star and it's distmod to the hostgal_specz, and obtained the following equation that we used to calculate this new feature: <br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.963265797280408
      ],
      "excerpt": "For what concerns the distances of each star, we didn't do much as they are universal for all classes  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9609587159458854
      ],
      "excerpt": "Even so we used the following haversine function to compare the galactic and earthly distances of the two for any diviations \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9693233393948762
      ],
      "excerpt": "    #:Convert decimal degrees to Radians: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.985790421965302
      ],
      "excerpt": "Another problem with the data, that didn't help with the classification was that target 99 was absent from the test data, for this reason we had to calculate the probability of one object being there based on the weighted probabilities of the other classes, and for that reason we used the following function.<br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9899777797871522,
        0.8979411005071259,
        0.8979411005071259
      ],
      "excerpt": ": Get the median, mean and max of the other probabilities and infer the  probability of class 99 based on these \ndef GenUnknown(data): \n    return ((((((data[\"mymedian\"]) + (((data[\"mymean\"]) / 2.0)))/2.0)) + (((((1.0) - (((data[\"mymax\"]) * (((data[\"mymax\"]) * (data[\"mymax\"]))))))) / 2.0)))/2.0) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9978928386765036
      ],
      "excerpt": "Maybe the only thing that we didn't have time to implement and could improve our score for the best was to fit Gaussian Processes to the time-series data, so we could infer statistical features for every different set of observations. A gaussian process is a set of functions that can fit data without using any parameters. This means that we can feed any curve in a good algorithm and it will return the posterior parameters best suited for the fit. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9389331875273732
      ],
      "excerpt": "For more on GP visit this link http://katbailey.github.io/post/gaussian-processes-for-dummies/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9937186458829677,
        0.9190549702112751,
        0.9862010310681346
      ],
      "excerpt": "Generally this is a time consuming problem, and for that we have to constraint the function to only fit processes of certain range, curvature, noise and other such characteristics. \nIn the file Deep Learning.ipynb we used a simple LGB Random Forest to make predictions on the data.<br> \nThe reason that we used it besides it generally giving the best predictions from all other models (RNN, LSTMNN, Machine Learning tasks), was because of its speed. The speed in this competition was key, and by using this algorithm that could train in less than a minute for 10-fold CV we managed to save a lot of much needed time, for our other calculations \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Stamatis-Ilias/PLAsTiCC-Astronomical-Classification/releases",
    "technique": "GitHub API"
  },
  "faq": [
    {
      "confidence": [
        1
      ],
      "excerpt": "In *Study.ipynb* we plotted 20 fluxes for each target class to see how they compare, and by that analysis we got most of the ideas on how to tackle the classification problem.\n\n<img src=\"img/flux_target.png\">\n\nWe can see that same-class objects look a lot like each other, as they have the **same peak-patterns** the **same passband-patterns** and sometimes **same times of observations.**\n\nFor that reason, we extracted the following statistical features for each object_id, by sorting for each different passband and by not.<br>\n*Most of the work for Feature Extraction, can be found in the kernel Feature Extraction_neg.ipynb and in the file agg_test.py, the code is given below*\n<br>\n\n```python\ndef process_flux(df):\n    #: Get the squared ratio of the flux and flux_err as a feature\n    flux_ratio_sq = np.power(df['flux'].values / df['flux_err'].values, 2.0)\n\n    df_flux = pd.DataFrame({\n        'flux_ratio_sq': flux_ratio_sq, \n        'flux_by_flux_ratio_sq': df['flux'].values * flux_ratio_sq,}, \n        index=df.index)\n    \n    return pd.concat([df, df_flux], axis=1)\n\n\ndef process_flux_agg(df):\n    #: Create more flux features by the statistical deviations between the flux, its error and the squared\n    #: error from before\n    flux_w_mean = df['flux_by_flux_ratio_sq_sum'].values / df['flux_ratio_sq_sum'].values\n    flux_diff = df['flux_max'].values - df['flux_min'].values\n    \n    df_flux_agg = pd.DataFrame({\n        'flux_w_mean': flux_w_mean,\n        'flux_diff1': flux_diff,\n        'flux_diff2': flux_diff / df['flux_mean'].values,       \n        'flux_diff3': flux_diff /flux_w_mean,\n        }, index=df.index)\n    \n    return pd.concat([df, df_flux_agg], axis=1)\n\ndef process_meta(filename):\n    meta_df = pd.read_csv(filename)\n    \n    meta_dict = dict()\n    #: distance\n    meta_dict.update(haversine_plus(meta_df['ra'].values, meta_df['decl'].values, \n                   meta_df['gal_l'].values, meta_df['gal_b'].values))\n    #:\n    meta_dict['hostgal_photoz_certain'] = np.multiply(\n            meta_df['hostgal_photoz'].values, \n             np.exp(meta_df['hostgal_photoz_err'].values))\n    \n    meta_df = pd.concat([meta_df, pd.DataFrame(meta_dict, index=meta_df.index)], axis=1)\n    return meta_df\n\n#: General aggregates\naggs_all = {\n        'flux': ['min', 'max', 'mean', 'median', 'std', 'skew'],\n        'flux_err': ['min', 'max', 'mean', 'median', 'std', 'skew'],\n        'detected': ['mean'],\n        'flux_ratio_sq':['sum', 'skew'],\n        'flux_by_flux_ratio_sq':['sum','skew'],\n    }\n\naggs_pb = {\n        'flux': ['min', 'max', 'mean', 'median', 'std', 'skew'],\n        'flux_err': ['min', 'max', 'mean', 'median', 'std', 'skew'],\n        'flux_ratio_sq':['sum', 'skew'],\n        'flux_by_flux_ratio_sq':['sum','skew'],\n    }\n\n#: Aggregates for tsfresh. Tsfresh is a python modeling from timeseries feature extraction!\nfcp = {\n        'flux': {\n            'longest_strike_above_mean': None,\n            'longest_strike_below_mean': None,\n            'mean_change': None,\n            'mean_abs_change': None,\n            'length': None,\n        },\n                \n        'flux_by_flux_ratio_sq': {\n            'longest_strike_above_mean': None,\n            'longest_strike_below_mean': None,       \n        },\n                \n        'flux_passband': {\n            'fft_coefficient': [\n                    {'coeff': 0, 'attr': 'abs'}, \n                    {'coeff': 1, 'attr': 'abs'}\n                ],\n            'kurtosis' : None, \n            'skewness' : None,\n        },\n                \n        'mjd': {\n            'maximum': None, \n            'minimum': None,\n            'mean_change': None,\n            'mean_abs_change': None,\n        },\n    }\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 23 Dec 2021 08:39:59 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Stamatis-Ilias/PLAsTiCC-Astronomical-Classification/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Stamatis-Ilias/PLAsTiCC-Astronomical-Classification",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Stamatis-Ilias/PLAsTiCC-Astronomical-Classification/master/Deep%20Learning.ipynb",
      "https://raw.githubusercontent.com/Stamatis-Ilias/PLAsTiCC-Astronomical-Classification/master/Study.ipynb",
      "https://raw.githubusercontent.com/Stamatis-Ilias/PLAsTiCC-Astronomical-Classification/master/README.ipynb",
      "https://raw.githubusercontent.com/Stamatis-Ilias/PLAsTiCC-Astronomical-Classification/master/Untitled.ipynb",
      "https://raw.githubusercontent.com/Stamatis-Ilias/PLAsTiCC-Astronomical-Classification/master/Autoencoder.ipynb",
      "https://raw.githubusercontent.com/Stamatis-Ilias/PLAsTiCC-Astronomical-Classification/master/Feature%20Extraction_neg.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "For doing this we used a library created by an other competitor, that can be found in the read_test_files folder\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8837680365796365
      ],
      "excerpt": "python      \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8314823419435533
      ],
      "excerpt": "<img src=\"img/targets.png\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8314823419435533
      ],
      "excerpt": "<img src = \"img/flux_example.png\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8997243352845468,
        0.821044763650378,
        0.936606094659785
      ],
      "excerpt": "    diffs = np.diff(series.mjd) \n    diff_ids = (np.where(diffs>threshold)) \n    print(diff_ids) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8942628989657678,
        0.865011735779095,
        0.936606094659785
      ],
      "excerpt": "    mjd_diff = np.max(series.mjd) - np.min(series.mjd) \n    mjd_diff_div =int(np.ceil(mjd_diff/4)) \n    #:print(mjd_diff, mjd_diff_div) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.936606094659785
      ],
      "excerpt": "    #:print(agg1) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8067244127819766
      ],
      "excerpt": "meta_df[\"hostgal_photoz_certain_mine\"] = -6.09*(10**(-14))*np.exp(0.6713*meta_df[\"distmod\"])+0.04902+meta_df[\"hostgal_photoz\"] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8314823419435533
      ],
      "excerpt": "<img src=\"img/position_example.png\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8997243352845468,
        0.8997243352845468,
        0.8997243352845468,
        0.8997243352845468
      ],
      "excerpt": "    lon1 = np.radians(lon1) \n    lat1 = np.radians(lat1) \n    lon2 = np.radians(lon2) \n    lat2 = np.radians(lat2) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8997243352845468,
        0.8997243352845468,
        0.8938802086218193,
        0.9166808280272533,
        0.9166808280272533,
        0.8760544343585684,
        0.9169301470659292
      ],
      "excerpt": "dlon = np.subtract(lon2, lon1) \ndlat = np.subtract(lat2, lat1) \na = np.add(np.power(np.sin(np.divide(dlat, 2)), 2),   \n                      np.multiply(np.cos(lat1),  \n                                  np.multiply(np.cos(lat2),  \n                                              np.power(np.sin(np.divide(dlon, 2)), 2)))) \nhaversine = np.multiply(2, np.arcsin(np.sqrt(a))) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9132220487264048
      ],
      "excerpt": "    'latlon1': np.subtract(np.multiply(lon1, lat1), np.multiply(lon2, lat2)), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8066634711824444
      ],
      "excerpt": "    return ((((((data[\"mymedian\"]) + (((data[\"mymean\"]) / 2.0)))/2.0)) + (((((1.0) - (((data[\"mymax\"]) * (((data[\"mymax\"]) * (data[\"mymax\"]))))))) / 2.0)))/2.0) \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Stamatis-Ilias/PLAsTiCC-Astronomical-Classification/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Introduction",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "PLAsTiCC-Astronomical-Classification",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Stamatis-Ilias",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Stamatis-Ilias/PLAsTiCC-Astronomical-Classification/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 23 Dec 2021 08:39:59 GMT"
    },
    "technique": "GitHub API"
  },
  "support": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The data was divided into two sets. One for the **timeseries data** of each object, that contained the *Julian Dates, the flux and the corresponding passband* of each observation, while also having the *flux error* of the observation and a *detected* value that was equal to 1 if the object's brightness is significantly different at the 3-sigma level relative to the reference template.\n\n\n\n```python\nimport pandas as pd\ndf = pd.read_csv(\"../all/training_set.csv\")\ndf_meta = pd.read_csv(\"../all/training_set_metadata.csv\").set_index(\"object_id\")\ndf.head()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>object_id</th>\n      <th>mjd</th>\n      <th>passband</th>\n      <th>flux</th>\n      <th>flux_err</th>\n      <th>detected</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>615</td>\n      <td>59750.4229</td>\n      <td>2</td>\n      <td>-544.810303</td>\n      <td>3.622952</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>615</td>\n      <td>59750.4306</td>\n      <td>1</td>\n      <td>-816.434326</td>\n      <td>5.553370</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>615</td>\n      <td>59750.4383</td>\n      <td>3</td>\n      <td>-471.385529</td>\n      <td>3.801213</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>615</td>\n      <td>59750.4450</td>\n      <td>4</td>\n      <td>-388.984985</td>\n      <td>11.395031</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>615</td>\n      <td>59752.4070</td>\n      <td>2</td>\n      <td>-681.858887</td>\n      <td>4.041204</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\nThe other data file contains the metadata of each star, differentiating the stars by an object_id.\n\n\n\n```python\ndf_meta.head()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ra</th>\n      <th>decl</th>\n      <th>gal_l</th>\n      <th>gal_b</th>\n      <th>ddf</th>\n      <th>hostgal_specz</th>\n      <th>hostgal_photoz</th>\n      <th>hostgal_photoz_err</th>\n      <th>distmod</th>\n      <th>mwebv</th>\n      <th>target</th>\n    </tr>\n    <tr>\n      <th>object_id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>615</th>\n      <td>349.046051</td>\n      <td>-61.943836</td>\n      <td>320.796530</td>\n      <td>-51.753706</td>\n      <td>1</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>NaN</td>\n      <td>0.017</td>\n      <td>92</td>\n    </tr>\n    <tr>\n      <th>713</th>\n      <td>53.085938</td>\n      <td>-27.784405</td>\n      <td>223.525509</td>\n      <td>-54.460748</td>\n      <td>1</td>\n      <td>1.8181</td>\n      <td>1.6267</td>\n      <td>0.2552</td>\n      <td>45.4063</td>\n      <td>0.007</td>\n      <td>88</td>\n    </tr>\n    <tr>\n      <th>730</th>\n      <td>33.574219</td>\n      <td>-6.579593</td>\n      <td>170.455585</td>\n      <td>-61.548219</td>\n      <td>1</td>\n      <td>0.2320</td>\n      <td>0.2262</td>\n      <td>0.0157</td>\n      <td>40.2561</td>\n      <td>0.021</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>745</th>\n      <td>0.189873</td>\n      <td>-45.586655</td>\n      <td>328.254458</td>\n      <td>-68.969298</td>\n      <td>1</td>\n      <td>0.3037</td>\n      <td>0.2813</td>\n      <td>1.1523</td>\n      <td>40.7951</td>\n      <td>0.007</td>\n      <td>90</td>\n    </tr>\n    <tr>\n      <th>1124</th>\n      <td>352.711273</td>\n      <td>-63.823658</td>\n      <td>316.922299</td>\n      <td>-51.059403</td>\n      <td>1</td>\n      <td>0.1934</td>\n      <td>0.2415</td>\n      <td>0.0176</td>\n      <td>40.4166</td>\n      <td>0.024</td>\n      <td>90</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n",
      "technique": "Header extraction"
    }
  ]
}