{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1710.10903](https://arxiv.org/abs/1710.10903",
      "https://arxiv.org/abs/1710.10903"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you make advantage of the GAT model in your research, please cite the following in your manuscript:\n\n```\n@article{\n  velickovic2018graph,\n  title=\"{Graph Attention Networks}\",\n  author={Veli{\\v{c}}kovi{\\'{c}}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\\`{o}}, Pietro and Bengio, Yoshua},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJXMpikCZ},\n  note={accepted as poster},\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{\n  velickovic2018graph,\n  title=\"{Graph Attention Networks}\",\n  author={Veli{\\v{c}}kovi{\\'{c}}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\\`{o}}, Pietro and Bengio, Yoshua},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJXMpikCZ},\n  note={accepted as poster},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9999005642056139
      ],
      "excerpt": "Graph Attention Networks (Veli\u010dkovi\u0107 et al., ICLR 2018): https://arxiv.org/abs/1710.10903 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8981780929348538
      ],
      "excerpt": "  --residual            use residual connections \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/handasontam/GAT-with-edgewise-attention",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-03-03T17:55:33Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-11-10T09:30:28Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9711020782113616,
        0.8252855287165325,
        0.9467084383162095
      ],
      "excerpt": "Here we provide the implementation of a Graph Attention Network (GAT) layer in TensorFlow, along with a minimal execution example (on the Cora dataset). The repository is organised as follows: \n- data/ contains the necessary dataset files for Cora; \n- models/ contains the implementation of the GAT network (gat.py); \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9737133844921698
      ],
      "excerpt": "An experimental sparse version is also available, working only when the batch size is equal to 1. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8245992191658201
      ],
      "excerpt": "                        ratio of data used for training (the rest will be used \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.819455713178842
      ],
      "excerpt": "- id corresponds to the node id (zero indexing) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "GAT with edgewise attention",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/handasontam/GAT-with-edgewise-attention/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Sat, 25 Dec 2021 15:57:00 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/handasontam/GAT-with-edgewise-attention/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "handasontam/GAT-with-edgewise-attention",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/handasontam/GAT-with-edgewise-attention/master/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/handasontam/GAT-with-edgewise-attention/master/data/20groups.ipynb"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.8529270852770672,
        0.8318585798103063
      ],
      "excerpt": "The sparse model may be found at models/sp_gat.py. \nYou may execute a full training run of the sparse model on Cora through execute.py. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8137517737036888
      ],
      "excerpt": "a csv file containing the node label. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/handasontam/GAT-with-edgewise-attention/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python",
      "Dockerfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2018 Petar Veli\\xc4\\x8dkovi\\xc4\\x87\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# GAT modified\nGraph Attention Networks (Veli\u010dkovi\u0107 *et al.*, ICLR 2018): [https://arxiv.org/abs/1710.10903](https://arxiv.org/abs/1710.10903)\n\n![](./img/tensorboard.png)\n\n## Overview\nHere we provide the implementation of a Graph Attention Network (GAT) layer in TensorFlow, along with a minimal execution example (on the Cora dataset). The repository is organised as follows:\n- `data/` contains the necessary dataset files for Cora;\n- `models/` contains the implementation of the GAT network (`gat.py`);\n- `pre_trained/` store model checkpoint);\n- `utils/` contains:\n    * an implementation of an attention head, along with an experimental sparse version (`layers.py`);\n    * preprocessing subroutines (`process.py`);\n\n\n## Sparse version\nAn experimental sparse version is also available, working only when the batch size is equal to 1.\nThe sparse model may be found at `models/sp_gat.py`.\n\nYou may execute a full training run of the sparse model on Cora through `execute.py`.\n\n## Command line argument\n```\noptional arguments:\n  -h, --help            show this help message and exit\n  -s, --sparse          use sparse operation to reduce memory consumption\n  --epochs EPOCHS       number of epochs\n  --lr LR               learning rate\n  --patience PATIENCE   for early stopping\n  --l2_coef L2_COEF     l2 regularization coefficient\n  --hid_units HID_UNITS [HID_UNITS ...]\n                        numbers of hidden units per each attention head in\n                        each layer\n  --n_heads N_HEADS [N_HEADS ...]\n                        number of attention head\n  --residual            use residual connections\n  --attention_drop ATTENTION_DROP\n                        dropout probability for attention layer\n  --edge_attr_directory EDGE_ATTR_DIRECTORY\n                        directory storing all edge attribute (.npz file) which\n                        stores the sparse adjacency matrix\n  --node_features_path NODE_FEATURES_PATH\n                        csv file path for the node features\n  --label_path LABEL_PATH\n                        csv file path for the ground truth label\n  --log_directory LOG_DIRECTORY\n                        directory for logging to tensorboard\n  --train_ratio TRAIN_RATIO\n                        ratio of data used for training (the rest will be used\n                        for testing)\n```\n\n## data preparation\n### edge_attr_directory\nThe directory that contains multiple .npz file. \n- Each .npz file stores the scipy sparse matrix (N, N) - the adjacency matrix of the edge attribute.\n- reference: [https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.save_npz.html](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.save_npz.html)\n\n### node_features_path\na csv file containing the node attribute.\n- The first row contains the features name\n- should have the same node ordering as the edge attribute adjacency matrix\n\n### label_path\na csv file containing the node label.\n- The first row must be \"id, label\"\n- id corresponds to the node id (zero indexing)\n- label can be any string\n\n## example (train)\n\n```bash\n$ git clone https://github.com/handasontam/GAT-with-edgewise-attention.git\n\n$ cd data\n\n$ curl https://transfer.sh/11fhgc/eth.tar.gz -o eth.tar.gz  # md5: 62aef8b070d7be703152419f16e830d1\n\n$ tar -zxvf eth.tar.gz\n\n$ cd ../\n\n$ python execute.py \\\n--sparse \\\n--epochs 100000 \\\n--lr 0.008 \\\n--patience 50 \\\n--l2_coef 0.005 \\\n--hid_units 5 \\\n--n_heads 2 1 \\\n--residual \\\n--attention_drop 0.0 \\\n--edge_attr ./data/eth/edges \\\n--node_features_path ./data/eth/node_features.csv \\\n--log_directory /tmp/tensorboard \\\n--label_path ./data/eth/label.csv\n\n$ tensorboard --logdir=/tmp/tensorboard  # to run tensorboard\n```\nOnce TensorBoard is running, navigate your web browser to localhost:6006 to view the TensorBoard\n\n## example (load model)\n\n``` bash\n$ curl https://transfer.sh/iMacq/pre_trained.tar.gz -o pre_trained.tar.gz  # md5: 041de9eb6e7dcd4ca74267c30a58ad70\n\n$ tar -zxvf pre_trained.tar.gz\n\n$ python load_model.py \\\n--sparse \\\n--hid_units 5 \\\n--n_heads 2 1 \\\n--residual \\\n--edge_attr./data/eth/edges \\\n--node_features_path ./data/eth/node_features.csv \\\n--label_path ./data/eth/label.csv \\\n--train_ratio 0.8 \\\n--model_path ./pre_trained/mod_test.ckpt\n\n$ # should print: Test loss: 0.579380989074707 ; Test accuracy: 0.86021488904953\n\n```\n\n## Dependencies\n\nThe script has been tested running under Python 3.5.2, with the following packages installed (along with their dependencies):\n\n- `numpy==1.14.1`\n- `scipy==1.0.0`\n- `networkx==2.1`\n- `tensorflow-gpu==1.6.0`\n- `pandas==0.23.4`\n\nIn addition, CUDA 9.0 and cuDNN 7 have been used.\n\n## Reference\nIf you make advantage of the GAT model in your research, please cite the following in your manuscript:\n\n```\n@article{\n  velickovic2018graph,\n  title=\"{Graph Attention Networks}\",\n  author={Veli{\\v{c}}kovi{\\'{c}}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\\`{o}}, Pietro and Bengio, Yoshua},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJXMpikCZ},\n  note={accepted as poster},\n}\n```\n\n## License\nMIT",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "GAT-with-edgewise-attention",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "handasontam",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/handasontam/GAT-with-edgewise-attention/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The script has been tested running under Python 3.5.2, with the following packages installed (along with their dependencies):\n\n- `numpy==1.14.1`\n- `scipy==1.0.0`\n- `networkx==2.1`\n- `tensorflow-gpu==1.6.0`\n- `pandas==0.23.4`\n\nIn addition, CUDA 9.0 and cuDNN 7 have been used.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Sat, 25 Dec 2021 15:57:00 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```bash\n$ git clone https://github.com/handasontam/GAT-with-edgewise-attention.git\n\n$ cd data\n\n$ curl https://transfer.sh/11fhgc/eth.tar.gz -o eth.tar.gz  #: md5: 62aef8b070d7be703152419f16e830d1\n\n$ tar -zxvf eth.tar.gz\n\n$ cd ../\n\n$ python execute.py \\\n--sparse \\\n--epochs 100000 \\\n--lr 0.008 \\\n--patience 50 \\\n--l2_coef 0.005 \\\n--hid_units 5 \\\n--n_heads 2 1 \\\n--residual \\\n--attention_drop 0.0 \\\n--edge_attr ./data/eth/edges \\\n--node_features_path ./data/eth/node_features.csv \\\n--log_directory /tmp/tensorboard \\\n--label_path ./data/eth/label.csv\n\n$ tensorboard --logdir=/tmp/tensorboard  #: to run tensorboard\n```\nOnce TensorBoard is running, navigate your web browser to localhost:6006 to view the TensorBoard\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "``` bash\n$ curl https://transfer.sh/iMacq/pre_trained.tar.gz -o pre_trained.tar.gz  #: md5: 041de9eb6e7dcd4ca74267c30a58ad70\n\n$ tar -zxvf pre_trained.tar.gz\n\n$ python load_model.py \\\n--sparse \\\n--hid_units 5 \\\n--n_heads 2 1 \\\n--residual \\\n--edge_attr./data/eth/edges \\\n--node_features_path ./data/eth/node_features.csv \\\n--label_path ./data/eth/label.csv \\\n--train_ratio 0.8 \\\n--model_path ./pre_trained/mod_test.ckpt\n\n$ #: should print: Test loss: 0.579380989074707 ; Test accuracy: 0.86021488904953\n\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}