{
  "citation": [
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "Emily Chen \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8586718319963604
      ],
      "excerpt": "\ud83c\udf89Happy New Year!\ud83c\udf89 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/collectionslab/Omniscribe",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-12-17T23:22:38Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-09-23T22:53:53Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9220405538628439,
        0.9925646378443037
      ],
      "excerpt": "Omniscribe was developed to detect annotations (marginalia, interlinear markings, provenance marks, etc.) in digitized printed books hosted via the International Image Interoperability Framework (IIIF). Why do we care about finding annotations? Annotations and other marks are traces left behind by previous readers and owners. Through these markings, we can better understand how readers in the past have used their books or interpreted their contents. \nFormed in December 2018, the project is a Collections Lab/BuildUCLA collaboration with a team of UCLA Digital Library staff and UCLA students. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.916470920250408
      ],
      "excerpt": "Fig. 1 A sample image generated using our Mask-RCNN model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9432944570471317,
        0.997107903265057,
        0.98771857555818,
        0.9684496908619047,
        0.9993082240207625
      ],
      "excerpt": "The team has met bi-weekly throughout the winter quarter and is transitioning from R&D workflows to a product development workflow. \nUp until recent, the only data we had readily available for training came from one of the members of the team annotating a hundred or so regions, which is hardly representative of all the possible annotations that exist in the world. To gather more data, we used Zooniverse to crowdsource more data. We received over 2000 responses and gathered several hundred regions of annotations and made that data readily available to train. Thus, we now have two models: the model that was trained on a smaller set of annotations (call it m<sub>small</sub>), and another model trained based on the data gathered from zooniverse (call it m<sub>zoo</sub>). \nThere are various ways that we can evaluate these models. Ideally, we would have these models see a test set, know the total amount of annotated regions in this test set, and perhaps compute an F1 score and an accuracy score. However, what makes something a \"region\" for us is arguably blurred. For example, when considering a whole page of handwriting, m<sub>small</sub> would detect multiple regions, stratisfying the page. m<sub>zoo</sub> however, would see the entire page as one region of annotation. Both models are correct, but accuracy score would not account for the difference in their predictions. We also should not use F1 because there are uncountably many regions that are not annotated, which makes for an indefinite amount of True Negatives in the F1 calculation. For now, we have settled on using a True Positive / False Positive ratio (TP/FP) as a metric of evaluation. That is, the higher the TP/FP, the better a model is performing. \nIn our test set, m<sub>small</sub> scored a TP/FP of 140/14, while m<sub>zoo</sub> received a higher TP/FP of 87/7. However, a qualitative anaylsis suggests that neither of these models should be used alone. We found that m<sub>small</sub> detected interlinear annotations and rarely detected tiny annotations (e.g. a lone \nWe are also now looking to wrap up these models in a nice package for others to use. The plan is to integrate Flask with our machine learning tools to achieve the goal of usability. In the meantime, we are also planning to upload more of our data onto Zooniverse so that we can later train a third model that may improve on our current choice of having two models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.926843236146716
      ],
      "excerpt": "for integrating IIIF manifests as input for our tool. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9616930536191388,
        0.8016415938261183,
        0.9711943411129581
      ],
      "excerpt": "and outputting a copy of the image with a color spash on any annotation found; \nhowever, a large collection of annotated books are actually archived over IIIF \nservers. With this in mind, we are looking into porting this tool to a web \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9801449150118191,
        0.8933756327050151,
        0.8011126675413861,
        0.8557208936935338
      ],
      "excerpt": "Mask-RCNN is currently our best solution to detecting annotations. It is \nstate-of-the-art in computer vision as it is the result of continuous \nadvancements in using convolutional neural networks for image segmentation. \nMore information of its upbringing as well as its predecessors are found below. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8001532450053255,
        0.8845028101185117,
        0.975525823206246,
        0.8165061065977329,
        0.8589284963132094
      ],
      "excerpt": "Perhaps what makes Mask-RCNN the most feasible solution for us over other \napproaches is the fact that we can treat our images as regions and consider \nannotations as regions of interest (ROIs). This is in contrast of treating \nour problem as a binary classification problem (i.e. annotated v.s. not \nannotated). After training the Mask-RCNN on our dataset, we get desirable \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.866990304047308,
        0.9443637022613137
      ],
      "excerpt": "Auto-Keras is currently the best open-source library for automatically \ngenerating machine learning models for your datasets. It competes directly with \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9715084040228723
      ],
      "excerpt": "From our experience, we believe that Auto-Keras is a great tool for those who \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9367172975172345
      ],
      "excerpt": "they generate are generally dense and complex to achieve desirable results \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9787954280836079,
        0.9812681172906407,
        0.9941420101774248
      ],
      "excerpt": "better. For the more experienced, we recommend using the models that Auto-Keras \ngenerates as benchmarks for your own models. It is also important to note that \nonly image classification is supported as of 1/3/19, with ML features for text, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9591904940406711
      ],
      "excerpt": "Vanilla CNN (implemented in PyTorch) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9738160359066036,
        0.9695620850231621
      ],
      "excerpt": "The following link discusses our experience with the approaches aforementioned, \nincluding project overview, data preparation, data pre-processing, and \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/collectionslab/annotations-computervision/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 4,
      "date": "Tue, 21 Dec 2021 10:25:48 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/collectionslab/Omniscribe/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "collectionslab/Omniscribe",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/collectionslab/annotations-computervision/master/proofOfConcept/mask-rcnn/visualize-model.ipynb",
      "https://raw.githubusercontent.com/collectionslab/annotations-computervision/master/proofOfConcept/mask-rcnn/visualize-data.ipynb"
    ],
    "technique": "File Exploration"
  },
  "identifier": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "https://zenodo.org/badge/latestdoi/114576206",
      "technique": "Regular expression"
    }
  ],
  "installation": [
    {
      "confidence": [
        0.8814584376174757
      ],
      "excerpt": "better. For the more experienced, we recommend using the models that Auto-Keras \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/collectionslab/Omniscribe/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'Mask R-CNN\\n\\nThe MIT License (MIT)\\n\\nCopyright (c) 2017 Matterport, Inc.\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in\\nall copies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\\nTHE SOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Omniscribe",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Omniscribe",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "collectionslab",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/collectionslab/Omniscribe/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "kirschbombe",
        "body": "[![DOI](https://zenodo.org/badge/114576206.svg)](https://zenodo.org/badge/latestdoi/114576206)\r\n\r\nThis is a command line interface for detecting annotations in IIIF-hosted printed books. Give the script a list of IIIF manifest URLs (either saved locally or hosted elsewhere) and it will generate a `resultsManifest.json` file that is IIIF-compliant that can be displayed via IIIF Viewers like Mirador. Other outputs available are an HTML gallery and plain text file. \r\n\r\n### Release notes\r\n\r\n**Support for smaller-than-max inference images**\r\n\r\nSometimes IIIF image servers limit the dimensions of the largest \"full\" image they make available (so people can't just download the full-res version); the code to highlight the detected handwriting regions via IIIF annotations now handles these situations.\r\nThere's a new command-line option, `--max_pages=N`, which can be used to avoid processing all of the images/pages on a given manifest, if for example the manifest is really long and you only want to detect handwriting on the first N pages.\r\nThe generated IIIF annotations have been streamlined considerably, thanks to suggestions from @glenrobson.\r\n\r\n**Add option to generate IIIF annotation lists**\r\n\r\nIf the user specifies `--annotate` when running `inferencer.py` with the `--manifest` flag, IIIF annotation list files are created in the `annotations/` folder and referenced in the resulting manifest file. IIIF-compatible viewers like Mirador can visualize the detected handwriting/notes on the input images when they load the generated manifest.\r\n\r\n**Other notes/contributions**\r\n\r\nThere's also an optional `--iiif_root` argument to specify the web address where the manifest of the detected annotations will be posted, with the `annotations/` folder for the optional IIIF annotation lists within it.\r\nThe detected handwriting/notes are highlighted in the IIIF annotation overlays when viewed in Mirador via a dashed rectangle bounding box with a dashed mask path within it. Mousing over the detected annotations displays a tag and the confidence level.\r\n\r\n### Installing Omniscribe\r\n\r\n**Requires Python 3.6.x**\r\n\r\n1. Download the Source Code package, unzip it, and save the `Omniscribe-1.1` folder to your local machine or server.\r\n2. Download the `model.h5` and save to the `Omniscribe-1.1` folder.\r\n3. Using the command line, navigate to the `Omniscribe-1.1` folder.\r\n4. Install dependencies by running the command `pip install -r requirements.txt`. <br> <br> NOTE: We recommend setting up a virtual environment to install Omniscribe. For more information on setting up a virtual environment, please refer to https://packaging.python.org/guides/installing-using-pip-and-virtualenv/ up to the **Leaving the virtualenv** section of this documentation.\r\n\r\n### Usage\r\nRun `inferencer.py` with the manifest URLs where you wish to detect annotations:\r\n\r\n`python3 inferencer.py [ export ] [ confidence ] [ manifest-url/path ]`\r\n\r\nExport options:\r\n* `--manifest` - exports `resultsManifest.json`, a IIIF manifest listing the images with detected annotations.\r\n* `--text` - exports `resultsURIs.txt`, a text file that contains URLs of images with detected annotations.\r\n* `--html` - exports `resultsImages.html`, a simple HTML gallery of images with detected annotations.\r\n* `--max_pages=N` - limits the number of images/pages processed in a given manifest, if for example the manifest is really long and you only want to detect handwriting on the first N pages.\r\n* `--annotate` (when running `inferencer.py` with the `--manifest` flag) - IIIF annotation list files are created in the `annotations/` folder and referenced in the resulting manifest file.\r\n* `--iiif_root` - specify the web address where the manifest of the detected annotations will be posted, with the `annotations/` folder for the optional IIIF annotation lists within it.\r\n\r\nThe default export format is `resultsManifest.json` if no export options are specified.\r\n\r\n* `--confidence=VALUE` - adjust this value for any values between 0 and 1 (inclusive).\r\n\r\nE.g. `--confidence=0.91` sets the threshold to 0.91. This means that any region that receives a score of 0.91 or higher from our model will be inferred as an annotation.\r\n\r\nThe default confidence level is 0.95 if no confidence value is specified.\r\n\r\n#### Gauging a \"Good\" Confidence Value\r\nWe found that marginalia are often detected with a confidence value of 0.90 and higher, but detecting interlinear annotations require lower confidence values, somewhere between 0.70-0.85. This means that setting a confidence score of `--confidence=0.90` will detect marginalia, but will be less effective in detecting interlinear annotations since these often receive scores below the threshold of 0.90. Setting a confidence score of `--confidence=0.70` will detect both interlinear annotations and marginalia (as both types of annotations will receive scores that are equal or higher than the confidence score); however, using the lower confidence threshold will likely result in more false positives.\r\n\r\n### Operating on Multiple Manifests\r\nThe manifests can be hosted or local IIIF manifest files. You can input multiple manifest URLs or paths, and the application will crawl through all the images from each manifest such that the resulting export is a single conglomerate of all the sub-results from every manifest.\r\n* `https://marinus.library.ucla.edu/iiif/annotated/uclaclark_SB322S53-short.json`\r\n* `path/to/a/localManifestFile.json`\r\n\r\n### Example Command Lines\r\n\r\npython3 inferencer.py --manifest --confidence=0.93 manifest1.json\r\npython3 inferencer.py --html --confidence=0.90 manifest1.json\r\npython3 inferencer.py --text --confidence=0.94 manifest1.json\r\npython3 inferencer.py --manifest --html --confidence=0.92 manifest1.json\r\npython3 inferencer.py --text --manifest --confidence=0.97 manifest1.json\r\npython3 inferencer.py --html --text --confidence=0.93 manifest1.json\r\npython3 inferencer.py --html --manifest --text --confidence=0.91 manifest1.json\r\npython3 inferencer.py --confidence=0.95 manifest1.json\r\npython3 inferencer.py --text manifest1.json\r\npython3 inferencer.py manifest1.json\r\npython3 inferencer.py --manifest --text --html --confidence=0.96 manifest1.json manifest2.json\r\npython3 inferencer.py --manifest --text manifest1.json manifest2.json manifest3.json manifest4.json\r\n\r\nNote that omitting the confidence option will be interpreted as setting the confidence score to 0.95. Additionally, omitting all export options will be interpreted as setting the export to a manifest file.\r\n\r\n### Collecting the Results\r\n\r\nAfter `inferencer.py` is done processing all the images, you will see the message `Finished detecting annotations`.\r\n\r\nAll the export files will be saved in the `Omniscribe-1.1` folder.\r\n\r\n### Sample Images\r\n\r\nCommand Line will typically display this as it processes through all the images.\r\n[<img src=\"https://imgur.com/56RFHD3.png\">](https://imgur.com/56RFHD3.png)\r\n\r\nTensorFlow may automatically use any available GPUs to do the predictions on the images as shown below.\r\n[<img src=\"https://i.imgur.com/kDwYaNP.png\">](https://i.imgur.com/kDwYaNP.png)\r\n\r\nHTML Gallery\r\n[<img src=\"https://imgur.com/mXWEuCF.png\">](https://imgur.com/mXWEuCF.png)\r\n\r\nDisplaying `resultsManifest.json` through Mirador, an image viewing client that supports IIIF.\r\n[<img src=\"https://imgur.com/qrPaNJrl.png\">](https://imgur.com/qrPaNJrl.png)",
        "dateCreated": "2020-01-22T17:07:16Z",
        "datePublished": "2020-03-27T19:33:43Z",
        "html_url": "https://github.com/collectionslab/Omniscribe/releases/tag/v1.1",
        "name": "Omniscribe",
        "tag_name": "v1.1",
        "tarball_url": "https://api.github.com/repos/collectionslab/Omniscribe/tarball/v1.1",
        "url": "https://api.github.com/repos/collectionslab/Omniscribe/releases/24945390",
        "zipball_url": "https://api.github.com/repos/collectionslab/Omniscribe/zipball/v1.1"
      },
      {
        "authorType": "User",
        "author_name": "kirschbombe",
        "body": "[![DOI](https://zenodo.org/badge/114576206.svg)](https://zenodo.org/badge/latestdoi/114576206)\r\n\r\nThanks for using our package!  This is a command line interface for detecting annotations in IIIF-hosted printed books. Give the script a list of IIIF manifest URLs (either saved locally or hosted elsewhere) and it will generate a `resultsManifest.json` file that is IIIF-compliant that can be displayed via IIIF Viewers like Mirador. Other outputs available are an HTML gallery and plain text file. \r\n\r\n### Installing Omniscribe\r\n\r\n**Requires Python 3.6.x**\r\n\r\n1. Download the Source Code package, unzip it, and save the `Omniscribe-1.0.2` folder to your local machine or server.\r\n2. Download the `model.h5` and save to the `Omniscribe-1.0.2` folder.\r\n3. Using the command line, navigate to the `Omniscribe-1.0.2` folder.\r\n4. Install dependencies by running the command `pip install -r requirements.txt`. <br> <br> NOTE: We recommend setting up a virtual environment to install Omniscribe. For more information on setting up a virtual environment, please refer to https://packaging.python.org/guides/installing-using-pip-and-virtualenv/ up to the **Leaving the virtualenv** section of this documentation.\r\n\r\n\r\n### Usage\r\nRun `inferencer.py` with the manifest URLs where you wish to detect annotations:\r\n\r\n`python3 inferencer.py [ export ] [ confidence ] [ manifest-url/path ]`\r\n\r\nExport options:\r\n* `--manifest` - exports `resultsManifest.json`, a IIIF manifest listing the images with detected annotations.\r\n* `--text` - exports `resultsURIs.txt`, a text file that contains URLs of images with detected annotations.\r\n* `--html` - exports `resultsImages.html`, a simple HTML gallery of images with detected annotations.\r\n\r\nThe default export format is `resultsManifest.json` if no export options are specified.\r\n\r\n* `--confidence=VALUE` - adjust this value for any values between 0 and 1 (inclusive).\r\n\r\nE.g. `--confidence=0.91` sets the threshold to 0.91. This means that any region that receives a score of 0.91 or higher from our model will be inferred as an annotation.\r\n\r\nThe default confidence level is 0.95 if no confidence value is specified.\r\n\r\n#### Gauging a \"Good\" Confidence Value\r\nWe found that marginalia are often detected with a confidence value of 0.90 and higher, but detecting interlinear annotations require lower confidence values, somewhere between 0.70-0.85. This means that setting a confidence score of `--confidence=0.90` will detect marginalia, but will be less effective in detecting interlinear annotations since these often receive scores below the threshold of 0.90. Setting a confidence score of `--confidence=0.70` will detect both interlinear annotations and marginalia (as both types of annotations will receive scores that are equal or higher than the confidence score); however, using the lower confidence threshold will likely result in more false positives.\r\n\r\n### Operating on Multiple Manifests\r\nThe manifests can be hosted or local IIIF manifest files. You can input multiple manifest URLs or paths, and the application will crawl through all the images from each manifest such that the resulting export is a single conglomerate of all the sub-results from every manifest.\r\n* `https://marinus.library.ucla.edu/iiif/annotated/uclaclark_SB322S53-short.json`\r\n* `path/to/a/localManifestFile.json`\r\n\r\n### Example Command Lines\r\n\r\npython3 inferencer.py --manifest --confidence=0.93 manifest1.json\r\npython3 inferencer.py --html --confidence=0.90 manifest1.json\r\npython3 inferencer.py --text --confidence=0.94 manifest1.json\r\npython3 inferencer.py --manifest --html --confidence=0.92 manifest1.json\r\npython3 inferencer.py --text --manifest --confidence=0.97 manifest1.json\r\npython3 inferencer.py --html --text --confidence=0.93 manifest1.json\r\npython3 inferencer.py --html --manifest --text --confidence=0.91 manifest1.json\r\npython3 inferencer.py --confidence=0.95 manifest1.json\r\npython3 inferencer.py --text manifest1.json\r\npython3 inferencer.py manifest1.json\r\npython3 inferencer.py --manifest --text --html --confidence=0.96 manifest1.json manifest2.json\r\npython3 inferencer.py --manifest --text manifest1.json manifest2.json manifest3.json manifest4.json\r\n\r\nNote that omitting the confidence option will be interpreted as setting the confidence score to 0.95. Additionally, omitting all export options will be interpreted as setting the export to a manifest file.\r\n\r\n### Collecting the Results\r\n\r\nAfter `inferencer.py` is done processing all the images, you will see the message `Finished detecting annotations`.\r\n\r\nAll the export files will be saved in the `Omniscribe-1.0.2` folder.\r\n\r\n### Sample Images\r\n\r\nCommand Line will typically display this as it processes through all the images.\r\n[<img src=\"https://imgur.com/56RFHD3.png\">](https://imgur.com/56RFHD3.png)\r\n\r\nTensorFlow may automatically use any available GPUs to do the predictions on the images as shown below.\r\n[<img src=\"https://i.imgur.com/kDwYaNP.png\">](https://i.imgur.com/kDwYaNP.png)\r\n\r\nHTML Gallery\r\n[<img src=\"https://imgur.com/mXWEuCF.png\">](https://imgur.com/mXWEuCF.png)\r\n\r\nDisplaying `resultsManifest.json` through Mirador, an image viewing client that supports IIIF.\r\n[<img src=\"https://imgur.com/qrPaNJrl.png\">](https://imgur.com/qrPaNJrl.png)",
        "dateCreated": "2019-05-03T18:04:02Z",
        "datePublished": "2019-05-19T18:15:44Z",
        "html_url": "https://github.com/collectionslab/Omniscribe/releases/tag/v1.0.2",
        "name": "Omniscribe v1.0.2",
        "tag_name": "v1.0.2",
        "tarball_url": "https://api.github.com/repos/collectionslab/Omniscribe/tarball/v1.0.2",
        "url": "https://api.github.com/repos/collectionslab/Omniscribe/releases/17449119",
        "zipball_url": "https://api.github.com/repos/collectionslab/Omniscribe/zipball/v1.0.2"
      },
      {
        "authorType": "User",
        "author_name": "jquach12",
        "body": "Thanks for using our package!  This is a command line interface for detecting annotations in IIIF-hosted printed books. Give the script a list of IIIF manifest URLs (either saved locally or hosted elsewhere) and it will generate a `resultsManifest.json` file that is IIIF-compliant that can be displayed via IIIF Viewers like Mirador. Other outputs available are an HTML gallery and plain text file. \r\n\r\n### Installing Omniscribe\r\n\r\n**Requires Python 3.6.x**\r\n\r\n1. Download the Source Code package, unzip it, and save the `Omniscribe-1.0.1` folder to your local machine or server.\r\n2. Download the `model.h5` and save to the `Omniscribe-1.0.1` folder.\r\n3. Using the command line, navigate to the `Omniscribe-1.0.1` folder.\r\n4. Install dependencies by running the command `pip install -r requirements.txt`. <br> <br> NOTE: we recommend setting up a virtual environment to install these dependencies in. For more information on setting up a virtual environment, please refer to https://packaging.python.org/guides/installing-using-pip-and-virtualenv/ up to the **Leaving the virtualenv** section of this documentation.\r\n\r\n\r\n### Usage\r\nRun `inferencer.py` with the manifest URLs where you wish to detect annotations:\r\n\r\n`python3 inferencer.py [ export ] [ confidence ] [ manifest ]`\r\n\r\nExport options:\r\n* `--manifest` - exports `resultsManifest.json`, a IIIF manifest that contains images with annotations.\r\n* `--text` - exports `resultsURIs.txt`, a text file that contains URIs of images with annotations.\r\n* `--html` - exports `resultsImages.html`, a simple HTML gallery of images with annotations.\r\n\r\nThe default export format is `resultsManifest.json` if no export options are specified.\r\n\r\n* `--confidence=VALUE` - adjust this value for any values between 0 and 1 (inclusive).\r\n\r\nE.g. `--confidence=0.91` sets the threshold to 0.91. This means that any region that receives a score of 0.91 or higher from our model will be inferred as an annotation.\r\n\r\nThe default confidence level is 0.95 if no confidence value is specified.\r\n\r\n#### Gauging a \"Good\" Confidence Value\r\nWe found in our experience that interlinear annotations get picked up with values 0.70-0.85, while marginalia are often 0.90+. That is, setting a confidence score of `--confidence=0.70` will pick up both interlinear annotations and marginalia (as both types of annotations will receive scores that are equal or higher than the confidence score); in turn, setting a confidence score of `--confidence=0.90` will only detect marginalia (as interlinear annotations often receive scores below the threshold of 0.90). <br><br> NOTE: The lower the confidence threshold, the more likely the model may give more false positives (accuracy increases, precision decreases). Similarly, increasing the confidence threshold might give less true positives (accuracy decreases, precision increases).\r\n\r\n### Operating on Multiple Manifests\r\nThe manifests can be hosted or local IIIF manifest files. You can input multiple manifest URLs or paths, and the application will crawl through all the images from each manifest such that the resulting export is a single conglomerate of all the sub-results from every manifest.\r\n* `https://marinus.library.ucla.edu/iiif/annotated/uclaclark_SB322S53-short.json`\r\n* `path/to/a/localManifestFile.json`\r\n\r\n### Example Command Lines\r\n\r\npython3 inferencer.py --manifest --confidence=0.93 manifest1.json\r\npython3 inferencer.py --html --confidence=0.90 manifest1.json\r\npython3 inferencer.py --text --confidence=0.94 manifest1.json\r\npython3 inferencer.py --manifest --html --confidence=0.92 manifest1.json\r\npython3 inferencer.py --text --manifest --confidence=0.97 manifest1.json\r\npython3 inferencer.py --html --text --confidence=0.93 manifest1.json\r\npython3 inferencer.py --html --manifest --text --confidence=0.91 manifest1.json\r\npython3 inferencer.py --confidence=0.95 manifest1.json\r\npython3 inferencer.py --text manifest1.json\r\npython3 inferencer.py manifest1.json\r\npython3 inferencer.py --manifest --text --html --confidence=0.96 manifest1.json manifest2.json\r\npython3 inferencer.py --manifest --text manifest1.json manifest2.json manifest3.json manifest4.json\r\n\r\nNote that omitting the confidence option will be interpreted as setting the confidence score to 0.95. Additionally, omitting all export options will be interpreted as setting the export to a manifest file.\r\n\r\n### Collecting the Results\r\n\r\nAfter `inferencer.py` is done processing all the images, you will see the message `Finished detecting annotations`.\r\n\r\nAll the export files will be saved in the `Omniscribe-1.0.1` folder.\r\n\r\n### Sample Images\r\n\r\nCommand Line will typically display this as it processes through all the images.\r\n[<img src=\"https://imgur.com/56RFHD3.png\">](https://imgur.com/56RFHD3.png)\r\n\r\nTensorFlow may automatically use any available GPUs to do the predictions on the images as shown below.\r\n[<img src=\"https://i.imgur.com/kDwYaNP.png\">](https://i.imgur.com/kDwYaNP.png)\r\n\r\nHTML Gallery\r\n[<img src=\"https://imgur.com/mXWEuCF.png\">](https://imgur.com/mXWEuCF.png)\r\n\r\nDisplaying `resultsManifest.json` through Mirador, an image viewing client that supports IIIF.\r\n[<img src=\"https://imgur.com/cRC9uSN.png\">](https://imgur.com/cRC9uSN.png)",
        "dateCreated": "2019-04-17T21:34:56Z",
        "datePublished": "2019-04-18T02:18:46Z",
        "html_url": "https://github.com/collectionslab/Omniscribe/releases/tag/v1.0.1",
        "name": "Omniscribe",
        "tag_name": "v1.0.1",
        "tarball_url": "https://api.github.com/repos/collectionslab/Omniscribe/tarball/v1.0.1",
        "url": "https://api.github.com/repos/collectionslab/Omniscribe/releases/16839039",
        "zipball_url": "https://api.github.com/repos/collectionslab/Omniscribe/zipball/v1.0.1"
      },
      {
        "authorType": "User",
        "author_name": "jquach12",
        "body": "Thanks for using our package!  This is a command line interface for detecting annotations in IIIF-hosted printed books. Give the script a list of IIIF manifest URLs (either saved locally or hosted elsewhere) and it will generate a `resultsManifest.json` file that is IIIF-compliant that can be displayed via IIIF Viewers like Mirador. Other outputs available are an HTML gallery and plain text file. \r\n\r\n### Install Omniscribe\r\n\r\n**Requires Python 3.6.x**\r\n\r\n1. Download the Source Code package, unzip it, and save the `Omniscribe-1.0` folder to your local machine or server.\r\n2. Download the `model.h5` and save to the `Omniscribe-1.0` folder.\r\n3. Using the command line, navigate to the `Omniscribe-1.0` folder.\r\n4. Install dependencies by running the command `pip3 install -r requirements.txt`.\r\n\r\n### Usage\r\nRun `inferencer.py` with the manifest URLs where you wish to detect annotations:\r\n\r\n`python3 inferencer.py [ output ] [ confidence ] [ manifest ]`\r\n\r\nOutput options:\r\n* `--manifest` (default) - outputs a IIIF manifest\r\n* `--text` - outputs a text file\r\n* `--html` - outputs an HTML gallery\r\n\r\nThe default confidence level is 0.95. If you wish to change the confidence level, you can use:\r\n* `--confidence=0.91` - adjust the percent value as needed\r\n\r\nThe manifests can be hosted or local IIIF manifest files. You can input multiple manifest URLs or paths.\r\n* `https://marinus.library.ucla.edu/iiif/annotated/uclaclark_SB322S53-short.json`\r\n* `path/to/a/localManifestFile.json`\r\n\r\nAfter `inferencer.py` is done processing all the images, you will see the message `Finished detecting annotations'.\r\n\r\nAll the export files will be saved in the `Omniscribe-1.0` folder.\r\n\r\nSample Images:\r\n\r\nCommand Line will typically display this as it processes through all the images.\r\n[<img src=\"https://imgur.com/K4HoLP1.png\">](https://imgur.com/K4HoLP1)\r\n\r\nTensorFlow may automatically use any available GPUs to do the predictions on the images as shown below.\r\n[<img src=\"https://i.imgur.com/kDwYaNP.png\">](https://i.imgur.com/kDwYaNP.png)",
        "dateCreated": "2019-04-16T19:57:28Z",
        "datePublished": "2019-04-17T21:06:43Z",
        "html_url": "https://github.com/collectionslab/Omniscribe/releases/tag/v1.0",
        "name": "Omniscribe",
        "tag_name": "v1.0",
        "tarball_url": "https://api.github.com/repos/collectionslab/Omniscribe/tarball/v1.0",
        "url": "https://api.github.com/repos/collectionslab/Omniscribe/releases/16806040",
        "zipball_url": "https://api.github.com/repos/collectionslab/Omniscribe/zipball/v1.0"
      },
      {
        "authorType": "User",
        "author_name": "jquach12",
        "body": "Thanks for using our package!  This is a command line interface for detecting annotations in IIIF-hosted printed books. Give the script a list of IIIF manifest URLs (either saved locally or hosted elsewhere) and it will generate a `resultsManifest.json` file that is IIIF-compliant that can be displayed via Presentation APIs like Mirador.\r\n\r\n*Note: This is a pre-release for testing. We are planning to deploy a production-ready release sometime in April.\r\n\r\n### How to Use:\r\n\r\n**Requires Python 3.6.x**\r\n\r\n1. Download the Source Code package, unzip it, and save the `Omniscribe-v0.4-alpha` folder to your local machine or server.\r\n2. Download the `model.h5` and save them to the `Omniscribe-v0.4-alpha` folder.\r\n3. Using the command line, navigate to the `Omniscribe-v0.4-alpha` folder.\r\n4. Install dependencies by running the command `pip3 install -r requirements.txt`.\r\n5. Run `inferencer.py` with the manifest URLs where you wish to detect annotations.\r\n\r\nExample syntax: a web-hosted manifest:\r\n\r\n```\r\n$ python3 inferencer.py https://marinus.library.ucla.edu/iiif/annotated/uclaclark_BF1681A441713.json\r\n```\r\nExample syntax: local manifest\r\n\r\n```\r\n$ python3 inferencer.py path/to/a/localManifestFile.json\r\n```\r\n\r\nExample syntax: multiple manifests\r\n\r\n```\r\n$ python3 inferencer.py manifest1.json manifest2.json manifest3.json\r\n```\r\n6. After `inferencer.py` is done processing all the images, you will see prompted a message of form `Finished detecting annotations on the manifest(s).\r\n\r\n7. Done! All the export files will be saved in the `Omniscribe-v0.4-alpha` folder.\r\n\r\n\r\nSample Images:\r\n\r\nCommand Line will typically display this as it processes through all the images.\r\n[<img src=\"https://i.imgur.com/O23r28W.png\">](https://i.imgur.com/O23r28W.png)\r\n\r\nTensorFlow may automatically use any available GPUs to do the predictions on the images as shown below.\r\n[<img src=\"https://i.imgur.com/kDwYaNP.png\">](https://i.imgur.com/kDwYaNP.png)",
        "dateCreated": "2019-04-16T19:57:28Z",
        "datePublished": "2019-04-16T20:01:20Z",
        "html_url": "https://github.com/collectionslab/Omniscribe/releases/tag/v0.4-alpha",
        "name": "Omniscribe",
        "tag_name": "v0.4-alpha",
        "tarball_url": "https://api.github.com/repos/collectionslab/Omniscribe/tarball/v0.4-alpha",
        "url": "https://api.github.com/repos/collectionslab/Omniscribe/releases/16805494",
        "zipball_url": "https://api.github.com/repos/collectionslab/Omniscribe/zipball/v0.4-alpha"
      },
      {
        "authorType": "User",
        "author_name": "jquach12",
        "body": "Thanks for using our package!  This is a command line interface for detecting annotations in IIIF-hosted printed books. Give the script a list of IIIF manifest URLs (either saved locally or hosted elsewhere) and it will generate a `resultsURIs.txt` file listing all the page images that were predicted to have at least one annotation.\r\n\r\n*Note: This is a pre-release for testing. We are planning to deploy a production-ready release sometime in April.\r\n\r\n### How to Use:\r\n\r\n**Requires Python 3.6.x**\r\n\r\n1. Download the Source Code package, unzip it, and save the `Omniscribe-v0.3-alpha` folder to your local machine or server.\r\n2. Download the `model.h5` and save them to the `Omniscribe-v0.3-alpha` folder.\r\n3. Using the command line, navigate to the `Omniscribe-v0.3-alpha` folder.\r\n4. Install dependencies by running the command `pip3 install -r requirements.txt`.\r\n5. Run `inferencer.py` with the manifest URLs where you wish to detect annotations.\r\n\r\nExample syntax: a web-hosted manifest:\r\n\r\n```\r\n$ python3 inferencer.py https://marinus.library.ucla.edu/iiif/annotated/uclaclark_BF1681A441713.json\r\n```\r\nExample syntax: local manifest\r\n\r\n```\r\n$ python3 inferencer.py path/to/a/localManifestFile.json\r\n```\r\n\r\nExample syntax: multiple manifests\r\n\r\n```\r\n$ python3 inferencer.py manifest1.json manifest2.json manifest3.json\r\n```\r\n6. After `inferencer.py` is done processing all the images, you will see prompted a message of form `FINISHED PROCESSING MANIFESTS. SAVED [EXPORT FILE] TO [CURRENT DIRECTORY]`.\r\n\r\n7. Done! a `resultsURIs.txt` file containing a list of all the image URIs that were predicted to have at least one annotation will be saved in the `Omniscribe-v0.3-alpha` folder.\r\n\r\n\r\nSample Images:\r\n\r\nCommand Line will typically display this as it processes through all the images.\r\n[<img src=\"https://i.imgur.com/YzzqAtr.png\">](https://i.imgur.com/YzzqAtr.png)\r\n\r\nTensorFlow may automatically use any available GPUs to do the predictions on the images as shown below.\r\n[<img src=\"https://i.imgur.com/kDwYaNP.png\">](https://i.imgur.com/kDwYaNP.png)",
        "dateCreated": "2019-04-15T19:06:23Z",
        "datePublished": "2019-04-15T19:14:28Z",
        "html_url": "https://github.com/collectionslab/Omniscribe/releases/tag/v0.3-alpha",
        "name": "Omniscribe",
        "tag_name": "v0.3-alpha",
        "tarball_url": "https://api.github.com/repos/collectionslab/Omniscribe/tarball/v0.3-alpha",
        "url": "https://api.github.com/repos/collectionslab/Omniscribe/releases/16775990",
        "zipball_url": "https://api.github.com/repos/collectionslab/Omniscribe/zipball/v0.3-alpha"
      },
      {
        "authorType": "User",
        "author_name": "kirschbombe",
        "body": "Thanks for using our package!  This is a command line interface for detecting annotations in IIIF-hosted printed books. Give the script a list of IIIF manifest URLs (either saved locally or hosted elsewhere) and it will generate a `regionURIs.txt` file listing all the page images that were predicted to have at least one annotation.\r\n\r\n*Note: This is a pre-release for testing. We are still fine-tuning our model for the best performance and accuracy. Check back for a more robust beta version soon.*\r\n\r\n### How to Use:\r\n\r\n**Requires Python 3.6 or higher**\r\n\r\n1. Download the Source Code package, unzip it, and save the `book-annotation-classification-v0.1-alpha` folder to your local machine or server.\r\n2. Download the `w_smallData.h5` and `w_bigData.h5` files and save them to the `book-annotation-classification-v0.1-alpha` folder.\r\n3. Using the command line, navigate to the `book-annotation-classification-v0.1-alpha` folder.\r\n4. Install dependencies by running the command `pip3 install -r requirements.txt`.\r\n5. Run `inferencer.py` with the manifest URLs where you wish to detect annotations.\r\n\r\nExample syntax: a web-hosted manifest:\r\n\r\n```\r\n$ python3 inferencer.py https://marinus.library.ucla.edu/iiif/annotated/uclaclark_BF1681A441713.json\r\n```\r\nExample syntax: local manifest\r\n\r\n```\r\n$ python3 inferencer.py path/to/a/localManifestFile.json\r\n```\r\n\r\nExample syntax: multiple manifests\r\n\r\n```\r\n$ python3 inferencer.py manifest1.json manifest2.json manifest3.json\r\n```\r\n6. After `inferencer.py` is done processing all the images, you will see the message `FINISHED PROCESSING MANIFESTS. SAVED regionURIS.txt TO CURRENT DIRECTORY`\r\n7. Done! a `regionURIs.txt` file containing a list of all the image URIs that were predicted to have at least one annotation will be saved in the `book-annotation-classification-v0.1-alpha` folder.\r\n\r\n\r\nSample Images:\r\n\r\nCommand Line will typically display this as it processes through all the images.\r\n[<img src=\"https://i.imgur.com/YzzqAtr.png\">](https://i.imgur.com/YzzqAtr.png)\r\n\r\nTensorFlow may automatically use any available GPUs to do the predictions on the images as shown below.\r\n[<img src=\"https://i.imgur.com/kDwYaNP.png\">](https://i.imgur.com/kDwYaNP.png)",
        "dateCreated": "2019-03-09T02:03:10Z",
        "datePublished": "2019-03-09T02:17:44Z",
        "html_url": "https://github.com/collectionslab/Omniscribe/releases/tag/v0.2-alpha",
        "name": "Book Annotation Detection",
        "tag_name": "v0.2-alpha",
        "tarball_url": "https://api.github.com/repos/collectionslab/Omniscribe/tarball/v0.2-alpha",
        "url": "https://api.github.com/repos/collectionslab/Omniscribe/releases/16008514",
        "zipball_url": "https://api.github.com/repos/collectionslab/Omniscribe/zipball/v0.2-alpha"
      },
      {
        "authorType": "User",
        "author_name": "jquach12",
        "body": "Thanks for using our package! May you find the annotations that we find helpful and interesting.\r\n\r\nThis is a command line interface for annotation detection! Give the script a list of IIIF manifest files (either saved locally or hosted elsewhere) and it will generate a **regionURIs.txt** text file that contains a list of all images that were predicted to have at least one annotation.\r\n\r\nHow to Use:\r\n\r\nStep 1. Download the Source Code package, unzip it, and save the **book-annotation-classification-v0.1-alpha** folder to a place of your choosing.\r\n\r\nStep 2. Download the **w_smallData.h5** and **w_bigData.h5** files and save them in the **book-annotation-classification-v0.1-alpha** folder.\r\n\r\nStep 3. Using a command line, navigate to the **book-annotation-classification-v0.1-alpha** folder.\r\n\r\nStep 4. Install dependencies by running the command *pip3 install -r requirements.txt*.\r\n\r\nStep 5. Run **inferencer.py** with the manifests that you are interested in detecting annotations from.\r\n\r\nStep 6. After **inferencer.py** is done processing all the manifest, you will see the message \"FINISHED PROCESSING MANIFESTS. SAVED **regionURIS.txt** TO CURRENT DIRECTORY\" \r\n\r\nStep 7. Done! a **regionURIs.txt** file containing a list of all the image URIs that were predicted to have at least one annotation will be saved in the **book-annotation-classification-v0.1-alpha** folder.\r\n\r\nExamples of How to Invoke Our Script:\r\n\r\n$ python3 inferencer.py https://marinus.library.ucla.edu/iiif/annotated/uclaclark_BF1681A441713.json\r\n$ python3 inferencer.py path/to/a/localManifestFile.json\r\n$ python3 inferencer.py foo.json https://marinus.library.ucla.edu/iiif/annotated/uclaclark_SB322S53.json\r\n\r\nSample Image of How Your Set Up Should Look Like:\r\n\r\nCommand Line will typically display this as it processes through all the images.\r\n[<img src=\"https://i.imgur.com/YzzqAtr.png\">](https://i.imgur.com/YzzqAtr.png)\r\n\r\nTensorFlow may automatically use any available GPUs to do the predictions on the images as shown below.\r\n[<img src=\"https://i.imgur.com/kDwYaNP.png\">](https://i.imgur.com/kDwYaNP.png)\r\n",
        "dateCreated": "2019-03-07T13:21:47Z",
        "datePublished": "2019-03-07T13:24:28Z",
        "html_url": "https://github.com/collectionslab/Omniscribe/releases/tag/v0.1-alpha",
        "name": "Annotation Detector on Command (ADC)",
        "tag_name": "v0.1-alpha",
        "tarball_url": "https://api.github.com/repos/collectionslab/Omniscribe/tarball/v0.1-alpha",
        "url": "https://api.github.com/repos/collectionslab/Omniscribe/releases/15970536",
        "zipball_url": "https://api.github.com/repos/collectionslab/Omniscribe/zipball/v0.1-alpha"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 9,
      "date": "Tue, 21 Dec 2021 10:25:48 GMT"
    },
    "technique": "GitHub API"
  }
}