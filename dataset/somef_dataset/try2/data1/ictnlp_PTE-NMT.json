{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\n@inproceedings{GuFX21,\n  author    = {Shuhao Gu and\n               Yang Feng and\n               Wanying Xie},\n  title     = {Pruning-then-Expanding Model for Domain Adaptation of Neural Machine\n               Translation},\n  booktitle = {Proceedings of the 2021 Conference of the North American Chapter of\n               the Association for Computational Linguistics: Human Language Technologies,\n               {NAACL-HLT} 2021, Online, June 6-11, 2021},\n  pages     = {3942--3952},\n  year      = {2021},\n  url       = {https://www.aclweb.org/anthology/2021.naacl-main.308/},\n}\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{GuFX21,\n  author    = {Shuhao Gu and\n               Yang Feng and\n               Wanying Xie},\n  title     = {Pruning-then-Expanding Model for Domain Adaptation of Neural Machine\n               Translation},\n  booktitle = {Proceedings of the 2021 Conference of the North American Chapter of\n               the Association for Computational Linguistics: Human Language Technologies,\n               {NAACL-HLT} 2021, Online, June 6-11, 2021},\n  pages     = {3942--3952},\n  year      = {2021},\n  url       = {https://www.aclweb.org/anthology/2021.naacl-main.308/},\n}",
      "technique": "Regular expression"
    }
  ],
  "codeOfConduct": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://raw.githubusercontent.com/ictnlp/PTE-NMT/master/CODE_OF_CONDUCT.md",
    "technique": "File Exploration"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ictnlp/PTE-NMT",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contributing to FAIR Sequence-to-Sequence Toolkit (PyTorch)\nWe want to make contributing to this project as easy and transparent as\npossible.\nPull Requests\nWe actively welcome your pull requests.\n\nFork the repo and create your branch from master.\nIf you've added code that should be tested, add tests.\nIf you've changed APIs, update the documentation.\nEnsure the test suite passes.\nMake sure your code lints.\nIf you haven't already, complete the Contributor License Agreement (\"CLA\").\n\nContributor License Agreement (\"CLA\")\nIn order to accept your pull request, we need you to submit a CLA. You only need\nto do this once to work on any of Facebook's open source projects.\nComplete your CLA here: https://code.facebook.com/cla\nIssues\nWe use GitHub issues to track public bugs. Please ensure your description is\nclear and has sufficient instructions to be able to reproduce the issue.\nCoding Style\nWe try to follow the PEP style guidelines and encourage you to as well.\nLicense\nBy contributing to FAIR Sequence-to-Sequence Toolkit, you agree that your contributions will be licensed\nunder the LICENSE file in the root directory of this source tree.",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-03-19T02:47:35Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-08T12:10:51Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Source code for the NAACL 2021 paper: Pruning-then-Expanding Model for Domain Adaptation of Neural Machine Translation",
      "technique": "GitHub API"
    }
  ],
  "documentation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "https://fairseq.readthedocs.io/",
      "technique": "Regular expression"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ictnlp/PTE-NMT/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Thu, 23 Dec 2021 17:50:17 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ictnlp/PTE-NMT/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "ictnlp/PTE-NMT",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/ictnlp/PTE-NMT/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/ictnlp/PTE-NMT/master/train.ft.sh",
      "https://raw.githubusercontent.com/ictnlp/PTE-NMT/master/train.kd.sh",
      "https://raw.githubusercontent.com/ictnlp/PTE-NMT/master/par_mask.sh",
      "https://raw.githubusercontent.com/ictnlp/PTE-NMT/master/scripts/compound_split_bleu.sh",
      "https://raw.githubusercontent.com/ictnlp/PTE-NMT/master/scripts/sacrebleu_pregen.sh",
      "https://raw.githubusercontent.com/ictnlp/PTE-NMT/master/examples/language_model/prepare-wikitext-103.sh",
      "https://raw.githubusercontent.com/ictnlp/PTE-NMT/master/examples/translation/prepare-wmt14en2fr.sh",
      "https://raw.githubusercontent.com/ictnlp/PTE-NMT/master/examples/translation/prepare-iwslt14.sh",
      "https://raw.githubusercontent.com/ictnlp/PTE-NMT/master/examples/translation/prepare-iwslt17-multilingual.sh",
      "https://raw.githubusercontent.com/ictnlp/PTE-NMT/master/examples/translation/prepare-wmt14en2de.sh"
    ],
    "technique": "File Exploration"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ictnlp/PTE-NMT/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Lua",
      "C++",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other",
      "url": "https://raw.githubusercontent.com/ictnlp/PTE-NMT/master/LICENSE"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'BSD License\\n\\nFor fairseq software\\n\\nCopyright (c) 2017-present, Facebook, Inc. All rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without modification,\\nare permitted provided that the following conditions are met:\\n\\n * Redistributions of source code must retain the above copyright notice, this\\n    list of conditions and the following disclaimer.\\n\\n * Redistributions in binary form must reproduce the above copyright notice,\\n    this list of conditions and the following disclaimer in the documentation\\n       and/or other materials provided with the distribution.\\n\\n * Neither the name Facebook nor the names of its contributors may be used to\\n    endorse or promote products derived from this software without specific\\n       prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR\\nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\\nANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "PTE-NMT",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "PTE-NMT",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "ictnlp",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ictnlp/PTE-NMT/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This system has been tested in the following environment.\n+ OS: Ubuntu 16.04.1 LTS 64 bits\n+ Python version \\>=3.7\n+ Pytorch version \\>=1.0\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 6,
      "date": "Thu, 23 Dec 2021 17:50:17 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Build\n```\npython setup.py build develop\n```\n\n- Preprocess the training data. Pretrain the general-domain model with the general-domain data. Read [here](https://fairseq.readthedocs.io/en/latest/getting_started.html#training-a-new-model) for more instructions.\n\n- Evaluate the importance of the parameters and prune the general domain model.\n\n```\nbash par_mask.sh\n```\n\nor\n\n```\n#:!/bin/bash \n\n#: the pre-trained general-domain checkpoint\nckt=\n\n#: path to save the pruned checkpoint\nsave_ckt=\n\n#: path to save the mask matrix \nsave_mask=\n\n#: prune ratio\nratio=0.3\n\npython magnitude.py --pre-ckt-path $ckt --save-ckt-path $save_ckt \\\n            --save-mask-path $save_mask --prune-ratio $ratio\n```\n\n- Train the pruned model with knowledge distillation\n\n```\nbash train.kd.sh\n```\n\nor\n\n```\n#: save dir\nsave=\n\n#: the pruned checkpoint\nckt=\n\n#: the general domain checkpoint\nteacher_ckt=\n\n#: the absolute path to the mask file\nmask=\n\nCUDA_VISIBLE_DEVICES=0  python3  train.py --ddp-backend=no_c10d  data-bin/{in-domain-data}\\\n    --arch transformer_wmt_en_de  --fp16 --reset-optimizer \\\n      --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n       --mask-file $mask  --restore-teacher-file $teacher_ckt --knowledge-distillation \\\n         --lr-scheduler fixed --restore-file $ckt  \\\n          --lr 7.5e-5 --dropout 0.1\\\n           --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --weight-decay 0.0\\\n        --max-tokens  4096  --save-dir checkpoints/$save  --save-interval 1 \\\n        --update-freq 1 --no-progress-bar --log-format json --log-interval 25  \n```\n\n- Generate the general-domain translation \n\n```\npython generate.py {General-domain-data} --path $MODEL \\\n    --gen-subset test --beam 4 --batch-size 128 \\\n    --remove-bpe --lenpen {float} \\\n```\n\nThe length penalty is set as 1.4 for the zh-en experiments and 0.6 for the en-de and en-fr experiments.\n\n- Fine-tuning the model \n\n```\nbash train.ft.sh\n```\n\nor\n\n```\n#:save dir\nsave=\n\n#: the last checkpoint after knowledge distillation\nckt=checkpoint_last.pt\n\n#: mask file\nmask=\n\nCUDA_VISIBLE_DEVICES=1  python3  train.py --ddp-backend=no_c10d  data-bin/{in-domain-data}\\\n    --arch transformer_wmt_en_de  --fp16 --reset-optimizer \\\n      --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n       --mask-file $mask  --restore-file $ckt --lr-scheduler fixed \\\n          --lr 7.5e-5 --dropout 0.1\\\n           --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --weight-decay 0.0\\\n        --max-tokens  4096  --save-dir checkpoints/$save   --save-interval 1 \\\n        --update-freq 1 --no-progress-bar --log-format json --log-interval 25 \n```\n\n- Generate the in-domain translation\n\n```\npython generate.py {In-domain-data} --path $MODEL \\\n    --gen-subset test --beam 4 --batch-size 128 \\\n    --remove-bpe --lenpen {float} \\\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}