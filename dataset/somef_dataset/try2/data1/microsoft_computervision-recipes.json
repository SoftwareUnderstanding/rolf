{
  "citation": [
    {
      "confidence": [
        0.9056674988540252
      ],
      "excerpt": "Vision Services \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9056674988540252
      ],
      "excerpt": "Custom Vision \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9195926162616405
      ],
      "excerpt": "Azure Machine Learning service (AzureML) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/microsoft/computervision-recipes",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contribution Guidelines\nWe appreciate all contributions. If you are planning to contribute back bug-fixes, please do so without any further discussion. If you plan to contribute new features, utility functions or extensions, please first open an issue and discuss the feature with us.\nHere are a few more things to know:\n- Microsoft Contributor License Agreement\n- Steps to Contributing\n- Working with Notebooks\n- Coding Guidelines\n- Code of Conduct\n    - Do not point fingers\n    - Provide code feedback based on evidence\n    - Ask questions do not give answers\nMicrosoft Contributor License Agreement\nMost contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.microsoft.com.\nWhen you submit a pull request, a CLA-bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.\nSteps to Contributing\nHere are the basic steps to get started with your first contribution. Please reach out with any questions.\n1. Use open issues to discuss the proposed changes. Create an issue describing changes if necessary to collect feedback. Also, please use provided labels to tag issues so everyone can easily sort issues of interest.\n1. Fork the repo so you can make and test local changes.\n1. Create a new branch for the issue. We suggest prefixing the branch with your username and then a descriptive title: (e.g. gramhagen/update_contributing_docs)\n1. Create a test that replicates the issue.\n1. Make code changes.\n1. Ensure unit tests pass and code style / formatting is consistent, and follows the Zen of Python.\n1. We use pre-commit package to run our pre-commit hooks. We use black formatter and flake8 linting on each commit. In order to set up pre-commit on your machine, follow the steps here, please note that you only need to run these steps the first time you use pre-commit for this project.\n\nUpdate your conda environment, pre-commit is part of the yaml file or just do  \n$ pip install pre-commit    \nSet up pre-commit by running following command, this will put pre-commit under your .git/hooks directory.\n   $ pre-commit install\n   $ git commit -m \"message\"\n\nEach time you commit, git will run the pre-commit hooks (black and flake8 for now) on any python files that are getting committed and are part of the git index.  If black modifies/formats the file, or if flake8 finds any linting errors, the commit will not succeed. You will need to stage the file again if black changed the file, or fix the issues identified by flake8 and and stage it again.\n\n\nTo run pre-commit on all files just run\n   $ pre-commit run --all-files\n\nCreate a pull request against <b>staging</b> branch.\n\nNote: We use the staging branch to land all new features, so please remember to create the Pull Request against staging.\nWorking with Notebooks\nWhen you pull updates from remote there might be merge conflicts with jupyter notebooks. The tool nbdime can help fix such problems.\n* To install nbdime\npip install ndime\n* To do diff between notebooks\nnbdiff notebook_1.ipynb notebook_2.ipynb\nCoding Guidelines\nWe strive to maintain high quality code to make the utilities in the repository easy to understand, use, and extend. We also work hard to maintain a friendly and constructive environment. We've found that having clear expectations on the development process and consistent style helps to ensure everyone can contribute and collaborate effectively.\nWe follow the Google docstring guidlines outlined on this styleguide page. For example:\n```python\n  def bite(n:int, animal:animal_object) -> bool:\n      \"\"\"\n      This function will perform n bites on animal.\n  Args:\n      n (int): the number of bites to do\n      animal (Animal): the animal to bite\n\n  Raises:\n      Exception: biting animal has no teeth\n\n  Returns:\n      bool: whether or not bite was successful\n  \"\"\"\n\n```\nSince we take a strong dependency on fast.ai, variable naming should follow the standards of fast.ai which are described in this abbreviation guide. For example, in computer vision cases, an image should always be abbreviated with im and not i, img, imag, image, etc. The one exception to this guide is that variable names should be as self-explanatory as possible. For example, the meaning of the variable batch_size is clearer than bs to refer to batch size.\nThe main variables and abbreviations are given in the table below:\n| Abbreviation | Description |\n| ------------ | ----------- |\n| im                    | Image\n| fig                    | Figure\n| pt                     | 2D point (column,row)\n| rect                   | Rectangle (order: left, top, right, bottom)\n| width, height, w, h  | Image dimensions\n| scale                  | Image up/down scaling factor\n| angle                  | Rotation angle in degree\n| table                  | 2D row/column matrix implemented using a list of lists\n| row, list1D             | Single row in a table, i.e. single 1D-list\n| rowItem                | Single item in a row\n| line, string            | Single string\n| lines, strings          | List of strings\n| list1D                 | List of items, not necessarily strings\n| -s    | Multiple of something (plural) should be indicated by appending an s to an abbreviation.\nCode of Conduct\nThis project has adopted the Microsoft Open Source Code of Conduct.\nFor more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.\nApart from the official Code of Conduct developed by Microsoft, we adopt the following behaviors, to ensure a great working environment:\nDo not point fingers\nLet\u2019s be constructive. For example: \"This method is missing docstrings\" instead of \"YOU forgot to put docstrings\".\nProvide code feedback based on evidence\nWhen making code reviews, try to support your ideas based on evidence (papers, library documentation, stackoverflow, etc) rather than your personal preferences. For example: \"When reviewing this code, I saw that the Python implementation the metrics are based on classes, however, scikit-learn and tensorflow use functions. We should follow the standard in the industry.\"\nAsk questions do not give answers\nTry to be empathic. For example: \"Would it make more sense if ...?\" or \"Have you considered this ... ?\"",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-02-11T16:23:51Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-28T16:07:57Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8873134720377608,
        0.9893536242998925,
        0.9895109669560826
      ],
      "excerpt": "In recent years, we've see an extra-ordinary growth in Computer Vision, with applications in face recognition, image understanding, search, drones, mapping, semi-autonomous and autonomous vehicles. A key part to many of these applications are visual recognition tasks such as image classification, object detection and image similarity. \nThis repository provides examples and best practice guidelines for building computer vision systems. The goal of this repository is to build a comprehensive set of tools and examples that leverage recent advances in Computer Vision algorithms, neural architectures, and operationalizing such systems. Rather than creating implementations from scratch, we draw from existing state-of-the-art libraries and build additional utility around loading image data, optimizing and evaluating models, and scaling up to the cloud. In addition, having worked in this space for many years, we aim to answer common questions, point out frequently observed pitfalls, and show how to use the cloud for training and deployment. \nWe hope that these examples and utilities can significantly reduce the \u201ctime to market\u201d by simplifying the experience from defining the business problem to development of solution by orders of magnitude. In addition, the example notebooks would serve as guidelines and showcase best practices and usage of the tools in a wide variety of languages. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9819026140853003,
        0.9948808741766716
      ],
      "excerpt": "Our target audience for this repository includes data scientists and machine learning engineers with varying levels of Computer Vision knowledge as our content is source-only and targets custom machine learning modelling. The utilities and examples provided are intended to be solution accelerators for real-world vision problems. \nThe following is a summary of commonly used Computer Vision scenarios that are covered in this repository. For each of the main scenarios (\"base\"), we provide the tools to effectively build your own model. This includes simple tasks such as fine-tuning your own model on your own data, to more complex tasks such as hard-negative mining and even model deployment. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.942927155595129,
        0.8162669333767489
      ],
      "excerpt": "| Classification | Base | Image Classification is a supervised machine learning technique to learn and predict the category of a given image. | \n| Similarity  | Base | Image Similarity is a way to compute a similarity score given a pair of images. Given an image, it allows you to identify the most similar image in a given dataset.  | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8255639328281056
      ],
      "excerpt": "| Keypoints | Base | Keypoint detection can be used to detect specific points on an object. A pre-trained model is provided to detect body joints for human pose estimation. | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8263953568982882
      ],
      "excerpt": "| Tracking | Base | Tracking allows to detect and track multiple objects in a video sequence over time. | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9644202924526787,
        0.8894842903779363
      ],
      "excerpt": "We separate the supported CV scenarios into two locations: (i) base: code and notebooks within the \"utils_cv\" and \"scenarios\" folders which follow strict coding guidelines, are well tested and maintained; (ii) contrib: code and other assets within the \"contrib\" folder, mainly covering less common CV scenarios using bleeding edge state-of-the-art approaches. Code in \"contrib\" is not regularly tested or maintained. \nNote that for certain computer vision problems, you may not need to build your own models. Instead, pre-built or easily customizable solutions exist on Azure which do not require any custom coding or machine learning expertise. We strongly recommend evaluating if these can sufficiently solve your problem. If these solutions are not applicable, or the accuracy of these solutions is not sufficient, then resorting to more complex and time-consuming custom approaches may be necessary. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9326330946021544
      ],
      "excerpt": "are a set of pre-trained REST APIs which can be called for image tagging, face recognition, OCR, video analytics, and more. These APIs work out of the box and require minimal expertise in machine learning, but have limited customization capabilities. See the various demos available to get a feel for the functionality (e.g. Computer Vision). The service can be used through API calls or through SDKs (available in .NET, Python, Java, Node and Go languages) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9551715276942417
      ],
      "excerpt": "is a SaaS service to train and deploy a model as a REST API given a user-provided training set. All steps including image upload, annotation, and model deployment can be performed using an intuitive UI or through SDKs (available in .NEt, Python, Java, Node and Go languages). Training image classification or object detection models can be achieved with minimal machine learning expertise. The Custom Vision offers more flexibility than using the pre-trained cognitive services APIs, but requires the user to bring and annotate their own data. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9746182619213485
      ],
      "excerpt": "is a service that helps users accelerate the training and deploying of machine learning models. While not specific for computer vision workloads, the AzureML Python SDK can be used for scalable and reliable training and deployment of machine learning solutions to the cloud. We leverage Azure Machine Learning in several of the notebooks within this repository (e.g. deployment to Azure Kubernetes Service) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9352451214439763
      ],
      "excerpt": "provide a set of examples (backed by code) of how to build common AI-oriented workloads that leverage multiple cloud components. While not computer vision specific, these reference architectures cover several machine learning workloads such as model deployment or batch scoring. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Best Practices, code samples, and documentation for Computer Vision.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/microsoft/computervision-recipes/releases",
    "technique": "GitHub API"
  },
  "executable_example": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "https://mybinder.org/v2/gh/PatrickBue/computervision-recipes/master?filepath=scenarios%2Fclassification%2F01_training_introduction_BINDER.ipynb",
      "technique": "Regular expression"
    }
  ],
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1006,
      "date": "Tue, 28 Dec 2021 21:59:57 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/microsoft/computervision-recipes/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "microsoft/computervision-recipes",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/microsoft/computervision-recipes/master/docker/Dockerfile",
      "https://raw.githubusercontent.com/microsoft/computervision-recipes/master/contrib/crowd_counting/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/microsoft/computervision-recipes/master/scenarios/similarity/00_webcam.ipynb",
      "https://raw.githubusercontent.com/microsoft/computervision-recipes/master/scenarios/similarity/11_exploring_hyperparameters.ipynb",
      "https://raw.githubusercontent.com/microsoft/computervision-recipes/master/scenarios/similarity/01_training_and_evaluation_introduction.ipynb",
      "https://raw.githubusercontent.com/microsoft/computervision-recipes/master/scenarios/similarity/02_state_of_the_art.ipynb",
      "https://raw.githubusercontent.com/microsoft/computervision-recipes/master/scenarios/similarity/12_fast_retrieval.ipynb",
      "https://raw.githubusercontent.com/microsoft/computervision-recipes/master/scenarios/segmentation/11_exploring_hyperparameters.ipynb",
      "https://raw.githubusercontent.com/microsoft/computervision-recipes/master/scenarios/segmentation/01_training_introduction.ipynb",
      "https://raw.githubusercontent.com/microsoft/computervision-recipes/master/scenarios/tracking/01_training_introduction.ipynb",
      "https://raw.githubusercontent.com/microsoft/computervision-recipes/master/scenarios/tracking/02_mot_challenge.ipynb",
      "https://raw.githubusercontent.com/microsoft/computervision-recipes/master/scenarios/action_recognition/00_webcam.ipynb",
      "https://raw.githubusercontent.com/microsoft/computervision-recipes/master/scenarios/action_recognition/10_video_transformation.ipynb",
      "https://raw.githubusercontent.com/microsoft/computervision-recipes/master/scenarios/action_recognition/02_training_hmdb.ipynb",
      "https://raw.githubusercontent.com/microsoft/computervision-recipes/master/scenarios/action_recognition/01_training_introduction.ipynb",
      "https://raw.githubusercontent.com/microsoft/computervision-recipes/master/scenarios/detection/00_webcam.ipynb",
      "https://raw.githubusercontent.com/microsoft/computervision-recipes/master/scenarios/detection/02_mask_rcnn.ipynb",
      "https://raw.githubusercontent.com/microsoft/computervision-recipes/master/scenarios/detection/11_exploring_hyperparameters_on_azureml.ipynb",
      "https://raw.githubusercontent.com/microsoft/computervision-recipes/master/scenarios/detection/03_keypoint_rcnn.ipynb",
      "https://raw.githubusercontent.com/microsoft/computervision-recipes/master/scenarios/detection/01_training_introduction.ipynb",
      "https://raw.githubusercontent.com/microsoft/computervision-recipes/master/scenarios/detection/20_deployment_on_kubernetes.ipynb",
      "https://raw.githubusercontent.com/microsoft/computervision-recipes/master/scenarios/detection/04_coco_accuracy_vs_speed.ipynb",
      "https://raw.githubusercontent.com/microsoft/computervision-recipes/master/scenarios/detection/12_hard_negative_sampling.ipynb",
      "https://raw.githubusercontent.com/microsoft/computervision-recipes/master/scenarios/classification/23_aci_aks_web_service_testing.ipynb",
      "https://raw.githubusercontent.com/microsoft/computervision-recipes/master/scenarios/classification/00_webcam.ipynb",
      "https://raw.githubusercontent.com/microsoft/computervision-recipes/master/scenarios/classification/03_training_accuracy_vs_speed.ipynb",
      "https://raw.githubusercontent.com/microsoft/computervision-recipes/master/scenarios/classification/10_image_annotation.ipynb",
      "https://raw.githubusercontent.com/microsoft/computervision-recipes/master/scenarios/classification/11_exploring_hyperparameters.ipynb",
      "https://raw.githubusercontent.com/microsoft/computervision-recipes/master/scenarios/classification/21_deployment_on_azure_container_instances.ipynb",
      "https://raw.githubusercontent.com/microsoft/computervision-recipes/master/scenarios/classification/22_deployment_on_azure_kubernetes_service.ipynb",
      "https://raw.githubusercontent.com/microsoft/computervision-recipes/master/scenarios/classification/20_azure_workspace_setup.ipynb",
      "https://raw.githubusercontent.com/microsoft/computervision-recipes/master/scenarios/classification/24_exploring_hyperparameters_on_azureml.ipynb",
      "https://raw.githubusercontent.com/microsoft/computervision-recipes/master/scenarios/classification/01_training_introduction.ipynb",
      "https://raw.githubusercontent.com/microsoft/computervision-recipes/master/scenarios/classification/02_multilabel_classification.ipynb",
      "https://raw.githubusercontent.com/microsoft/computervision-recipes/master/scenarios/classification/12_hard_negative_sampling.ipynb",
      "https://raw.githubusercontent.com/microsoft/computervision-recipes/master/scenarios/classification/25_deployment_on_azure_apps_service_and_setting_CORS_policies.ipynb",
      "https://raw.githubusercontent.com/microsoft/computervision-recipes/master/contrib/html_demo/JupyterCode/2_upload_ui.ipynb",
      "https://raw.githubusercontent.com/microsoft/computervision-recipes/master/contrib/html_demo/JupyterCode/3_deployment_to_azure_app_service.ipynb",
      "https://raw.githubusercontent.com/microsoft/computervision-recipes/master/contrib/html_demo/JupyterCode/4_train_and_deploy_custom_image_similarity_webapp.ipynb",
      "https://raw.githubusercontent.com/microsoft/computervision-recipes/master/contrib/html_demo/JupyterCode/1_image_similarity_export.ipynb",
      "https://raw.githubusercontent.com/microsoft/computervision-recipes/master/contrib/document_cleanup/confidence_based_Sauvola_binarization/Modified-Sauvola_Binarization.ipynb",
      "https://raw.githubusercontent.com/microsoft/computervision-recipes/master/contrib/document_cleanup/light_weight_document_cleanup_ICDAR2021/DocumentCleanup_ICDAR2021.ipynb",
      "https://raw.githubusercontent.com/microsoft/computervision-recipes/master/contrib/crowd_counting/crowdcounting/examples/tutorial.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/microsoft/computervision-recipes/master/contrib/vmss_builder/vm_user_env_setup.sh",
      "https://raw.githubusercontent.com/microsoft/computervision-recipes/master/contrib/action_recognition/i3d/download_models.sh",
      "https://raw.githubusercontent.com/microsoft/computervision-recipes/master/contrib/crowd_counting/crowdcounting/data/models/download.sh",
      "https://raw.githubusercontent.com/microsoft/computervision-recipes/master/utils_cv/tracking/references/fairmot/models/networks/DCNv2/make.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9748590769222945,
        0.889981732836477,
        0.8725533897273386
      ],
      "excerpt": "| Linux GPU | master | <b>PAUSED</b>  | | staging | <b>PAUSED</b>  | \n| Linux CPU | master | <b>PAUSED</b>  | | staging | <b>PAUSED</b>  | \n| Notebook unit GPU | master | <b>PAUSED</b>  | | staging | <b>PAUSED</b>  | \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/microsoft/computervision-recipes/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python",
      "Cuda",
      "HTML",
      "JavaScript",
      "C++",
      "C",
      "Dockerfile",
      "CSS",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'    MIT License\\r\\n\\r\\n    Copyright (c) Microsoft Corporation. All rights reserved.\\r\\n\\r\\n    Permission is hereby granted, free of charge, to any person obtaining a copy\\r\\n    of this software and associated documentation files (the \"Software\"), to deal\\r\\n    in the Software without restriction, including without limitation the rights\\r\\n    to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\r\\n    copies of the Software, and to permit persons to whom the Software is\\r\\n    furnished to do so, subject to the following conditions:\\r\\n\\r\\n    The above copyright notice and this permission notice shall be included in all\\r\\n    copies or substantial portions of the Software.\\r\\n\\r\\n    THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\r\\n    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\r\\n    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\r\\n    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\r\\n    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\r\\n    OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\r\\n    SOFTWARE\\r\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Computer Vision",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "computervision-recipes",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "microsoft",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/microsoft/computervision-recipes/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "PatrickBue",
        "body": "# Highlights\r\n\r\n## Scenarios added\r\n    \r\n### Tracking:\r\n- Added state-of-the-art support for multi-object tracking based on the FairMOT approach described in the 2020 paper [\"A Simple Baseline for Multi-Object Tracking\"](https://arxiv.org/pdf/2004.01888.pdf).\r\n- Reproduced published accuracies on the popular MOT research benchmark datasets.\r\n- Notebooks for training using a custom dataset, and for reproducing results on the MOT dataset.\r\n   \r\n### Action Recognition \r\n- Added state-of-the-art support for action recognition from video based on the R(2+1)D approach described in the 2019 paper [\"Large-scale weakly-supervised pre-training for video action recognition\"](https://arxiv.org/abs/1905.00561).\r\n- Reproduced published accuracies on the popular HMDB-51 research benchmark dataset.\r\n- Notebooks for training using a custom dataset, and for reproducing results on the HMDB-51 dataset.",
        "dateCreated": "2020-07-10T20:53:12Z",
        "datePublished": "2020-07-10T20:54:09Z",
        "html_url": "https://github.com/microsoft/computervision-recipes/releases/tag/1.2",
        "name": "Release version 1.2",
        "tag_name": "1.2",
        "tarball_url": "https://api.github.com/repos/microsoft/computervision-recipes/tarball/1.2",
        "url": "https://api.github.com/repos/microsoft/computervision-recipes/releases/28453295",
        "zipball_url": "https://api.github.com/repos/microsoft/computervision-recipes/zipball/1.2"
      },
      {
        "authorType": "User",
        "author_name": "PatrickBue",
        "body": "# Highlights\r\n\r\n## Scenarios added or expanded\r\n    \r\n### Similarity:\r\n- Implemented state-of-the-art approach for image retrieval based on the BMVC 2019 paper [\"Classification is a Strong Baseline for Deep Metric Learning\"](https://arxiv.org/abs/1811.12649).\r\n- Implemented popular re-ranking approach based on the CVPR 2017 paper [\"Re-ranking Person Re-identification with k-reciprocal Encoding\"](http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhong_Re-Ranking_Person_Re-Identification_CVPR_2017_paper.pdf).\r\n- Reproduced published accuracies on three popular research benchmark datasets (CARS196, CUB200, and SOP).\r\n- Notebook added which shows how to train and evaluate the approaches on a custom dataset.\r\n   \r\n### Detection:\r\n- Added Mask-RCNN functionality to detect and segment objects.\r\n- Added speed vs. accuracy trade-off analysis using the COCO dataset for benchmarking.\r\n- Improved visualization of e.g. predictions, ground truth, or annotation statistics.\r\n- Notebooks added which show how to: (i) run and train a Mask-RCNN model; (ii) evaluate on the COCO dataset; (iii) perform active learning via hard-negative sampling.\r\n\r\n### Keypoint:\r\n- New scenario.\r\n- Notebook added which shows: (i) how to run a pre-trained model for human pose estimation; and (ii) how to train a keypoint model on a custom dataset.\r\n\r\n### Action (in 'contrib' folder):\r\n- New scenario.\r\n- Added two state-of-the-art approaches for action recognition from video: (i) I3D from the famous 2017 [\"Quo Vadis\"](https://arxiv.org/pdf/1705.07750.pdf) paper; and (ii) R(2+1)D described in the 2019 paper [\"Large-scale weakly-supervised pre-training for video action recognition\"](https://arxiv.org/abs/1905.00561).\r\n- Functionality and documentation how to annotate own video data.\r\n\r\n",
        "dateCreated": "2020-03-26T22:56:59Z",
        "datePublished": "2020-03-27T13:52:55Z",
        "html_url": "https://github.com/microsoft/computervision-recipes/releases/tag/1.1",
        "name": "Release version 1.1",
        "tag_name": "1.1",
        "tarball_url": "https://api.github.com/repos/microsoft/computervision-recipes/tarball/1.1",
        "url": "https://api.github.com/repos/microsoft/computervision-recipes/releases/24931169",
        "zipball_url": "https://api.github.com/repos/microsoft/computervision-recipes/zipball/1.1"
      },
      {
        "authorType": "User",
        "author_name": "jiata",
        "body": "# Scenarios\r\n\r\n### Classification:\r\n  - Introduction notebooks that include the basics of training a cutting edge classification model, how to do multi-label classification, and evaluating speed vs accuracy\r\n  - Advanced topic notebooks that include hard-negative mining, and basic exploration of parameters\r\n  - Notebooks that show how to use Azure ML to operationalize your model, and Azure ML Hyperdrive to perform exhaustive testing on your model\r\n\r\n### Similarity:\r\n - Introduction notebooks that performs basic training and evaluation for image similarity\r\n - Notebooks that show how to use Azure ML hyperdrive to perform exhaustive testing on your model\r\n\r\n### Detection:\r\n - Introduction notebooks that performs basic training and evaluation for object detection\r\n - Notebooks that show how to use Azure ML hyperdrive to perform exhaustive testing on your model\r\n",
        "dateCreated": "2019-09-30T16:59:20Z",
        "datePublished": "2019-09-30T17:09:00Z",
        "html_url": "https://github.com/microsoft/computervision-recipes/releases/tag/v2019.09",
        "name": "Computer Vision Repo 2019.09",
        "tag_name": "v2019.09",
        "tarball_url": "https://api.github.com/repos/microsoft/computervision-recipes/tarball/v2019.09",
        "url": "https://api.github.com/repos/microsoft/computervision-recipes/releases/20359693",
        "zipball_url": "https://api.github.com/repos/microsoft/computervision-recipes/zipball/v2019.09"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 8228,
      "date": "Tue, 28 Dec 2021 21:59:57 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "machine-learning",
      "computer-vision",
      "deep-learning",
      "python",
      "jupyter-notebook",
      "operationalization",
      "kubernetes",
      "azure",
      "microsoft",
      "data-science",
      "tutorial",
      "artificial-intelligence",
      "image-classification",
      "image-processing",
      "similarity",
      "object-detection",
      "convolutional-neural-networks"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This repository supports various Computer Vision scenarios which either operate on a single image:\n<p align=\"center\">\n  <img src=\"./scenarios/media/cv_overview.jpg\" height=\"350\" alt=\"Some supported CV scenarios\"/>\n</p>\n\nAs well as scenarios such as action recognition which take a video sequence as input:\n<p align=\"center\">\n  <img src=/scenarios/action_recognition/media/action_recognition2.gif \"Example of action recognition\"/>\n</p>\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "To get started, navigate to the [Setup Guide](SETUP.md), which lists\ninstructions on how to setup the compute environment and dependencies needed to run the\nnotebooks in this repo. Once your environment is setup, navigate to the\n[Scenarios](scenarios) folder and start exploring the notebooks. We recommend to start with the *image classification* notebooks, since this introduces concepts which are also used by the other scenarios (e.g. pre-training on ImageNet).\n\nAlternatively, we support Binder\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/PatrickBue/computervision-recipes/master?filepath=scenarios%2Fclassification%2F01_training_introduction_BINDER.ipynb)\nwhich makes it easy to try one of our notebooks in a web-browser simply by following this link. However, Binder is free, and as a result only comes with limited CPU compute power and without GPU support. Expect the notebook to run very slowly (this is somewhat improved by reducing image resolution to e.g. 60 pixels but at the cost of low accuracies).\n\n",
      "technique": "Header extraction"
    }
  ]
}