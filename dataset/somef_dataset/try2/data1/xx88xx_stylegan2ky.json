{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\nWe thank Ming-Yu Liu for an early review, Timo Viitanen for his help with code release, and Tero Kuosmanen for compute infrastructure.\r\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1706.08500",
      "https://arxiv.org/abs/1606.03498",
      "https://arxiv.org/abs/1812.04948",
      "https://arxiv.org/abs/1812.04948",
      "https://arxiv.org/abs/1812.04948",
      "https://arxiv.org/abs/1812.04948",
      "https://arxiv.org/abs/1812.04948",
      "https://arxiv.org/abs/1812.04948",
      "https://arxiv.org/abs/1904.06991"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\n```\r\n@article{Karras2019stylegan2,\r\n  title   = {Analyzing and Improving the Image Quality of {StyleGAN}},\r\n  author  = {Tero Karras and Samuli Laine and Miika Aittala and Janne Hellsten and Jaakko Lehtinen and Timo Aila},\r\n  journal = {CoRR},\r\n  volume  = {abs/1912.04958},\r\n  year    = {2019},\r\n}\r\n```\r\n\r\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{Karras2019stylegan2,\n  title   = {Analyzing and Improving the Image Quality of {StyleGAN}},\n  author  = {Tero Karras and Samuli Laine and Miika Aittala and Janne Hellsten and Jaakko Lehtinen and Timo Aila},\n  journal = {CoRR},\n  volume  = {abs/1912.04958},\n  year    = {2019},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9105368110547479
      ],
      "excerpt": "* https://github.com/NVlabs/stylegan2 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9941533744942845
      ],
      "excerpt": "Paper: http://arxiv.org/abs/1912.04958<br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8334710283794773,
        0.8753150968738145
      ],
      "excerpt": "For business inquiries, please contact researchinquiries@nvidia.com<br> \nFor press and other inquiries, please contact Hector Marinez at hmarinez@nvidia.com<br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8956259629933797
      ],
      "excerpt": "| &boxvr;&nbsp; stylegan2-video.mp4 | High-quality version of the video \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8422862053358879
      ],
      "excerpt": "| pr50k3    | 0.689 / 0.492  | 26 min | 17 min  | 12 min | Precision and Recall \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/xx88xx/stylegan2ky",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-05-05T01:00:29Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-05-05T01:04:14Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8005202681752692
      ],
      "excerpt": "Added image and network snapshot frequencies as explicit args, with defaults: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9309988369043362
      ],
      "excerpt": "Use these with run_training.py - for ex, to save an image and network snapshot every tick: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.816512601824012
      ],
      "excerpt": "Various Improvements to make StyleGAN2 more suitible to be trained on Google Colab \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8246712264400082
      ],
      "excerpt": "* Optimized dataset creation and access for non-progressive training and for colab training, which includes: create only the maximum size tfrecord; use raw JPEG instead of decoded numpy array, which reduce both tfrecord creation time and dataset size dramatically. (* Only tested for config-e and config-f, as no-progressive for these configurations) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8567196543930558
      ],
      "excerpt": "Create training image set. Instead of image size of 2^n * 2^n, now you can process your image size as of (min_h x 2^n) X (min_w * 2^n) natually. For example, 640x384, min_h = 5, min_w =3, n=7. Please make sure all your raw images are preprocessed to the exact same size. To reduce the training set size, JPEG format is preferred. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9493885846771477
      ],
      "excerpt": "* Tar your raw data and upload to google drive, share it as data_url \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9873767197644192
      ],
      "excerpt": "You may also save a generated tfrecord directly in your google drive, and pin your dataset dir to your google drive. The benefit of creating a new tfrecord everytime is: Google colab disconnects after around 9-12 hours, since there is no true randomness for tfrecord, you may end up using some data more often then others. Also, the read/transfer speed from mounted google drive is kind of slow. It only takes about 2 min to gdown and create dataset for 30k/2G jpeg files. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8802695721951891
      ],
      "excerpt": "Analyzing and Improving the Image Quality of StyleGAN<br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9835764315336535
      ],
      "excerpt": "Abstract: The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent vectors to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably detect if an image is generated by a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8051988754586414
      ],
      "excerpt": "| &boxvr;&nbsp; stylegan2-paper.pdf | High-quality version of the paper \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8885141820379509
      ],
      "excerpt": "We have verified that the results match the paper when training with 1, 2, 4, or 8 GPUs. Note that training FFHQ at 1024&times;1024 resolution requires GPU(s) with at least 16 GB of memory. The following table lists typical training times using NVIDIA DGX-1 with 8 Tesla V100 GPUs: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8950816142632219,
        0.8826059627549617
      ],
      "excerpt": "For other configurations, see the StyleGAN2 Google Drive folder. \nNote that the metrics are evaluated using a different random seed each time, so the results will vary between runs. In the paper, we reported the average result of running each metric 10 times. The following table lists the available metrics along with their expected runtimes and random variation: \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/xx88xx/stylegan2ky/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 30 Dec 2021 05:49:08 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/xx88xx/stylegan2ky/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "xx88xx/stylegan2ky",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/xx88xx/stylegan2ky/master/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/xx88xx/stylegan2ky/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\nDatasets are stored as multi-resolution TFRecords, similar to the [original StyleGAN](https://github.com/NVlabs/stylegan). Each dataset consists of multiple `*.tfrecords` files stored under a common directory, e.g., `~/datasets/ffhq/ffhq-r*.tfrecords`. In the following sections, the datasets are referenced using a combination of `--dataset` and `--data-dir` arguments, e.g., `--dataset=ffhq --data-dir=~/datasets`.\r\n\r\n**FFHQ**. To download the [Flickr-Faces-HQ](https://github.com/NVlabs/ffhq-dataset) dataset as multi-resolution TFRecords, run:\r\n\r\n```.bash\r\npushd ~\r\ngit clone https://github.com/NVlabs/ffhq-dataset.git\r\ncd ffhq-dataset\r\npython download_ffhq.py --tfrecords\r\npopd\r\npython dataset_tool.py display ~/ffhq-dataset/tfrecords/ffhq\r\n```\r\n\r\n**LSUN**. Download the desired LSUN categories in LMDB format from the [LSUN project page](https://www.yf.io/p/lsun). To convert the data to multi-resolution TFRecords, run:\r\n\r\n```.bash\r\npython dataset_tool.py create_lsun_wide ~/datasets/car ~/lsun/car_lmdb --width=512 --height=384\r\npython dataset_tool.py create_lsun ~/datasets/cat ~/lsun/cat_lmdb --resolution=256\r\npython dataset_tool.py create_lsun ~/datasets/church ~/lsun/church_outdoor_train_lmdb --resolution=256\r\npython dataset_tool.py create_lsun ~/datasets/horse ~/lsun/horse_lmdb --resolution=256\r\n```\r\n\r\n**Custom**. Create custom datasets by placing all training images under a single directory. The images must be square-shaped and they must all have the same power-of-two dimensions. To convert the images to multi-resolution TFRecords, run:\r\n\r\n```.bash\r\npython dataset_tool.py create_from_images ~/datasets/my-custom-dataset ~/my-custom-images\r\npython dataset_tool.py display ~/datasets/my-custom-dataset\r\n```\r\n\r\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8882288000331093
      ],
      "excerpt": "Create tfrecord, clone this repo, then \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8817415589474913
      ],
      "excerpt": "* Clone this repo \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9893272198983933,
        0.9906248903846466
      ],
      "excerpt": "!git clone https://github.com/skyflynil/stylegan2.git \n%cd stylegan2 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8289038044865998,
        0.8606309057550272
      ],
      "excerpt": "create your dataset for train \n!mkdir dataset \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8918974083095406,
        0.8399828158449817
      ],
      "excerpt": "* https://github.com/NVlabs/stylegan2 \n* https://github.com/akanimax/msg-stylegan-tf \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8072695082808222,
        0.9148471852535786
      ],
      "excerpt": "Added --resume_pkl option to explicitly set the pkl file to resume training from. Defaults to 'latest', but should otherwise be the path to your desired pkl file: \n!python run_training.py --num-gpus=1 --data-dir=./dataset --config=config-f --dataset=your_dataset_name --metric=none --min-h=4 --min-w=4 --res-log2=8 --resume_pkl=results/00000-pretrained/network-snapshot-10000.pkl \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.944396168487928
      ],
      "excerpt": "!python run_training.py --num-gpus=1 --data-dir=./dataset --config=config-f --dataset=your_dataset_name --mirror-augment=true --metric=none --total-kimg=20000 --min-h=5 --min-w=3 --res-log2=7 --image_snapshot_frequency=1 --network_snapshot_frequency=1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091,
        0.9066863560554895,
        0.9440365628796447
      ],
      "excerpt": "python dataset_tool.py create_from_images_raw dataset_dir raw_image_dir \nTo train, for example, 640x384 training set \npython run_training.py --num-gpus=your_gpu_num --data-dir=your_data_dir --config=config-e(or config_f) --dataset=your_data_set --mirror-augment=true --metric=none --total-kimg=12000 --min-h=5 --min-w=3 --res-log2=7 --result-dir=your_result_dir \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.925671696398174
      ],
      "excerpt": "import tensorflow as tf \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8896609176004759
      ],
      "excerpt": "print('Tensorflow version: {}'.format(tf.version) ) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560890156292937
      ],
      "excerpt": "print('GPU Identified at: {}'.format(tf.test.gpu_device_name())) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8055205149521806
      ],
      "excerpt": "download raw dataset to colab using  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8138030899163907
      ],
      "excerpt": "create your dataset for train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.921108411272225,
        0.8407651001510851,
        0.9400607743859524
      ],
      "excerpt": "!python dataset_tool.py create_from_images_raw ./dataset/dataset_name untared_raw_image_dir \nstart training \n!python run_training.py --num-gpus=1 --data-dir=./dataset --config=config-f --dataset=your_dataset_name --mirror-augment=true --metric=none --total-kimg=20000 --min-h=5 --min-w=3 --res-log2=7 --result-dir=\"/content/drive/My Drive/stylegan2/results\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.921108411272225,
        0.9400607743859524
      ],
      "excerpt": "!python dataset_tool.py create_from_images_raw --res_log2=8 ./dataset/dataset_name untared_raw_image_dir \n!python run_training.py --num-gpus=1 --data-dir=./dataset --config=config-f --dataset=your_dataset_name --mirror-augment=true --metric=none --total-kimg=20000 --min-h=5 --min-w=3 --res-log2=8 --result-dir=\"/content/drive/My Drive/stylegan2/results\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8146449015423624
      ],
      "excerpt": "Also, exposed resume_with_new_nets to command line. The example usage would be, network trained without attention, but now you want to try the network with attention module, you can specify --resume_with_new_nets=true to copy weights from checkpoints. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8114488624004937
      ],
      "excerpt": "| StyleGAN2 | Main Google Drive folder \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8027133024627905
      ],
      "excerpt": ": Generate uncurated car images \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8344311767521461
      ],
      "excerpt": "  --dataset=car --data-dir=~/datasets \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9054017614873153,
        0.8782615320428928,
        0.9054017614873153,
        0.8761283009744154,
        0.9054017614873153,
        0.8761283009744154,
        0.9054017614873153,
        0.896198593558758,
        0.9054017614873153,
        0.896198593558758
      ],
      "excerpt": "python run_training.py --num-gpus=8 --data-dir=~/datasets --config=config-f \\ \n  --dataset=ffhq --mirror-augment=true \npython run_training.py --num-gpus=8 --data-dir=~/datasets --config=config-f \\ \n  --dataset=car --total-kimg=57000 \npython run_training.py --num-gpus=8 --data-dir=~/datasets --config=config-f \\ \n  --dataset=cat --total-kimg=88000 \npython run_training.py --num-gpus=8 --data-dir=~/datasets --config=config-f \\ \n  --dataset=church --total-kimg 88000 --gamma=100 \npython run_training.py --num-gpus=8 --data-dir=~/datasets --config=config-f \\ \n  --dataset=horse --total-kimg 100000 --gamma=100 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9226509042444692
      ],
      "excerpt": "python run_generator.py generate-images --seeds=0-999 --truncation-psi=1.0 \\ \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/xx88xx/stylegan2ky/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Cuda",
      "Dockerfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other"
    },
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Changes from skyflynil fork:",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "stylegan2ky",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "xx88xx",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/xx88xx/stylegan2ky/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\n* Both Linux and Windows are supported. Linux is recommended for performance and compatibility reasons.\r\n* 64-bit Python 3.6 installation. We recommend Anaconda3 with numpy 1.14.3 or newer.\r\n* TensorFlow 1.14 or 1.15 with GPU support. The code does not support TensorFlow 2.0.\r\n* On Windows, you need to use TensorFlow 1.14 &mdash; TensorFlow 1.15 will not work.\r\n* One or more high-end NVIDIA GPUs, NVIDIA drivers, CUDA 10.0 toolkit and cuDNN 7.5. To reproduce the results reported in the paper, you need an NVIDIA GPU with at least 16 GB of DRAM.\r\n* Docker users: use the [provided Dockerfile](./Dockerfile) to build an image with the required library dependencies.\r\n\r\nStyleGAN2 relies on custom TensorFlow ops that are compiled on the fly using [NVCC](https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html). To test that your NVCC installation is working correctly, run:\r\n\r\n```.bash\r\nnvcc test_nvcc.cu -o test_nvcc -run\r\n| CPU says hello.\r\n| GPU says hello.\r\n```\r\n\r\nOn Windows, the compilation requires Microsoft Visual Studio to be in `PATH`. We recommend installing [Visual Studio Community Edition](https://visualstudio.microsoft.com/vs/) and adding into `PATH` using `\"C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Auxiliary\\Build\\vcvars64.bat\"`.\r\n\r\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 30 Dec 2021 05:49:08 GMT"
    },
    "technique": "GitHub API"
  }
}