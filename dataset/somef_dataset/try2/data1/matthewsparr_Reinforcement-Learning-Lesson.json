{
  "citation": [
    {
      "confidence": [
        0.8998593429438314
      ],
      "excerpt": "Consider this: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8217240581807945
      ],
      "excerpt": "AIs capable of beating human performance (Deepmind, AlphaGo, AlphaZero), improving computer-controlled players \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "<center> </center> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8028046190715653,
        0.8043073075947367
      ],
      "excerpt": "        next_state, reward, done, info = env.step(action) \n        if done: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8199748500934674,
        0.9318073560667521,
        0.9717212838488315,
        0.9983395072806817
      ],
      "excerpt": "http://www.gatsby.ucl.ac.uk/~dayan/papers/cjch.pdf - paper on Q-learning<br> \nhttps://www.cs.toronto.edu/~vmnih/docs/dqn.pdf - paper on DQN<br> \nhttps://medium.com/machine-learning-for-humans/reinforcement-learning-6eacf258b265 - blog post on RL<br> \nhttps://arxiv.org/pdf/1509.02971.pdf - paper on DDPG<br> \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/matthewsparr/Reinforcement-Learning-Lesson",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-08-15T20:01:41Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-09-19T16:18:04Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "That concludes this brief introduction into the wonderful world of Reinforcement Learning. Hopefully this provides you with a good foundation of understanding of the topic and can help springboard you into starting RL projects of your own.\n<br><br>\nAddtionally, it is important to note that RL is extremely expensive computationally. To train the Atari games, for example, even with a high end GPU, it could take multiple days of constant training. That being said, more simple environments can be trained is as little as 30 minutes to 1 hour. \n<br> <br>\nAlso important to note is that RL models are highly unstable and unpredictable. Even using the same seed for random number generation, the results of training may vary wildly from one training session to another. While it may have only taken 500 games to train the first time, the second time it might take 1000 games to reach the same performance.\n<br><br>\nThere are also many hyperparameters to adjust, from the epsilon values to the layers of the neural network to the size of the batch in experience replay. Finding the right balance of these variables can be time-consuming, adding to the time sink that RL has the possibility to become.\n<br><br>\nThat being said, it is extremely fulfilling when you see that your AI has actually started to learn and you see scores start climbing. <br><br>\nBeware, you may just find yourself become emotional involved in the success of your model.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "You may have heard that the two types of learning are supervised and unsupervised - either you are training a model to correctly assign labels or training a model to group similar items together. There is, however, a third breed of learning: reinforcement learning. Reinforcement learning seeks to train a model make a sequence of decisions.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8710545132487385,
        0.8089042330509276,
        0.9597411228776799
      ],
      "excerpt": "Explain what reinforcement learning is and its uses \nImplement reinforcement learning in your own projects \nReinforcement is training a model to achieve a certain goal through the process of trial and error. In RL, there are 5 main components: the <b>environment</b>, the <b>agent</b>, <b>actions</b>, <b>rewards</b>, and <b>states</b>. Those five components interact in the way shown in the diagram below. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9687862241045397
      ],
      "excerpt": "The agent is simply the model or artifical intelligence being trained. The environment is the interactive world in which the agent acts. The states are different, distinct versions of the environment. Actions are performed by the agent on the environment. The rewards are positive or negative feedback values corresponding to the actions taken and the effect they have on the environment. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9386420404653536,
        0.8484081872052228
      ],
      "excerpt": "This image represents a possible environment. In this environment, the penguin in the bottom left corner would be the <b>agent</b>. It ultimately wants to end up at the fish in the upper right corner. However, there are obstacles in its way there - the shark and the mines. The penguin must make a series of <b>actions</b> that will allow it to reach the fish while avoiding the obstacles. The <b>rewards</b> in this case could be as follows:<br> <br> \n<center>+1 if the penguin moves to an empty ocean space<br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8872589943852638
      ],
      "excerpt": "-50 if the penguin moves to the shark space<br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9705433777774725
      ],
      "excerpt": "So as the penguin chooses an <b>action</b> each turn, it is then presented with a new <b>state</b> and given a <b>reward</b> depending on what type of space it moved onto. With this new information, the penguin or <b>agent</b> will then evaluate the new <b>state</b> and select the next<b>action</b>. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9643439438782968,
        0.9755749808508395,
        0.9801860378549215
      ],
      "excerpt": "Over time, through enough trial and error (and the penguin hopefully narrowly escaping death by shark or mine), the penguin would ultimately learn the best route to the highest <b>reward</b> - navigating to the fish while avoiding the mines and the shark. \nThis is a very simple environment but it demonstrates the fundamentals of reinforcement learning. \nSo you understand the basics of RL and how it applies to our penguin's world above, but when does it make sense to use this type of learning in the real world? \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9968029537584643,
        0.8405010266792654
      ],
      "excerpt": "Allocation of limited computational resources. \nPortfolio optimization, risk management, stock trading. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.972148337239804,
        0.9157745433344461,
        0.9244344826422779
      ],
      "excerpt": "And so many more! The field of reinforcement learning will continue to grow and can be applicable to any field where there is a need to improve decision making. \nNow for the exciting part - learning how to implement RL! \nThere are many different reinforcement learning algorithms. These include <b>Q-Learning</b>, <b>State-Action-Reward-State-Action (SARSA)</b>, <b>Deep Q Network (DQN)</b>, and <b>Deep Deterministic Policy Gradient (DDPG)</b>, as well a myriad of different iterations of each. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8260595774264914
      ],
      "excerpt": "Q-Learning seeks to maximize what's known as the <b>Q-value</b> in order to select the most optimal action at any given state. <br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9647026926477519
      ],
      "excerpt": "As an agent makes actions, it updates the Q-values in the <b>Q-table</b>. The Q-table consists of all possible combinations of states and actions. So for the penguin example above, the environment is 5x5 which  means there are 25 possible states in which the penguin could be. At each of those states, the penguin has 4 possible moves - up, down, left, or right. Together, that gives a total Q-table size of 100. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8463990870517027
      ],
      "excerpt": "Now for the Q-values, they are calculated with the formula below. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.836674674249641,
        0.9285303012689345,
        0.9394935564834569,
        0.9787978754115377,
        0.988055964539576
      ],
      "excerpt": "<b>\u03b3</b> (or gamma) is a discount factor<br><br> \n<b>maxQ(s(t+1),a(t+1))</b> is the maximum Q-value at the next state (s(t+1)) which is found by testing all possible actions \nWhat this function does is allow the agent to learn to select not just the action that will give the highest immediate reward, but it will take into account future rewards as well. \nThe gamma value is necessary because it helps to balance immediate v.s. future reward. We would like the agent to value immediate rewards slightly more because they are more guaranteed than the predicted rewards of the next state. \nThe main issue with Q-Learning is that, while it is great at being able to select actions for states it has seen before, it is not able to predict anything in new states that it hasn't seen before. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.950167614957099,
        0.9171190280838613
      ],
      "excerpt": "This is where <b>Deep Q-Learning</b> comes into play. \nDQNs take the basic principles of Q-Learning and improves upon them by estimating Q-values through the use of a neural network. DQNs, unlike regular Q-Learning, are able to handle never-before-seen states. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.89566604325312
      ],
      "excerpt": "In a DQN, the input is usually the current state while the output is the Q-values for each of the possible actions. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9436758221488037,
        0.9045373477336557
      ],
      "excerpt": "There are three essential components that allow a DQN to work. \nIn order to gain experiences properly when starting off, a DQN employs a policy in which a value, epsilon, is used to tell the network whether to take a random action or predict an action. This value usually starts off at 1 and decays over time according to some decay rate (usually a decimal close to 1, i.e. 0.99). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.929420295449202
      ],
      "excerpt": "What this means is that at the beginning of training, with epsilon at 1, the network will always perform random actions. Since it hasn't really been trained yet, this is the best strategy to <b>explore</b> the environment. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9425064984807084
      ],
      "excerpt": "Then, as epsilon decays below 1, there is a growing chance each time of predicting an action instead of choosing randomly. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9223477070300881
      ],
      "excerpt": "As the network is trained more and more, it will start to predict actions more as well. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.986340894716544
      ],
      "excerpt": "When humans learn something by trial-and-error, we don't just look at our most recent attempt and base our next decision solely off of that. Instead, we rely on our memory of all our past attempts. DQNs must do something similar. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.969329506997107
      ],
      "excerpt": "Experience replay means that when the network is trained, it is not trained on each action it takes, as it takes them. Instead, a history of all states, actions, and corresponding rewards are stored in a memory. Then, at given intervals, the network is trained on a random sample of that memory according to some batch size. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9513820139149691,
        0.9246578567984548
      ],
      "excerpt": "This helps to decouple the temporal relationship between subsequent turns and greatly improves the stability of training. \nNormally, when training the network, we have to perform predictions using the same network to calculate the Q-value updates. This causes an issue where, as we are training the network, the Q-values are constantly shifting. This can create a feedback loop where the  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.929420295449202
      ],
      "excerpt": "What this means is that at the beginning of training, with epsilon at 1, the network will always perform random actions. Since it hasn't really been trained yet, this is the best strategy to <b>explore</b> the environment. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9425064984807084
      ],
      "excerpt": "Then, as epsilon decays below 1, there is a growing chance each time of predicting an action instead of choosing randomly. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9223477070300881
      ],
      "excerpt": "As the network is trained more and more, it will start to predict actions more as well. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9218054434265432
      ],
      "excerpt": "Here is a basic rundown of how training happens in a DQN. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8328761548765665
      ],
      "excerpt": "The easiest way to quickly start experimenting with RL is to train models to play video games. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8445281244658759
      ],
      "excerpt": "Fortunately, in Python there is a great library that enables this - Gym by OpenAi. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9498773760199912
      ],
      "excerpt": "Gym is a library designed specifically to provide numerous game environments for training a reinforcement learning model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9671210007368621
      ],
      "excerpt": "Environments are the games provide by the library. These include simple text based games, 2D and 3D games, and even classic Atari games. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9905547295335165
      ],
      "excerpt": "To get started, all you have to do is select an environment and then plug it into the code below. This will play out a number of games and moves, taking random actions at each step. (This code specifically using MsPacman but feel free to try any of the other games.) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8419285590766353
      ],
      "excerpt": "Calling <b>reset()</b> on the environment will do just that - reset it to a new game. <br> <br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9018879026464444
      ],
      "excerpt": "The <b>step()</b> command causes the game to play out one step or move, which in this case is a random action given by the action_space.sample() line. This function returns the next state of the game, the reward for the given move, a boolean value 'done' telling us whether or not the game ended, and any additional info the specific game might provide.<br><br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8670846918841228,
        0.9439836911963241
      ],
      "excerpt": "<b>Note:</b> playing the game in this way, moves happen very quickly. To slow down the render to a more human-level speed, you can <b>import time</b> and run <b>time.sleep(0.1)</b> to add a brief pause after each frame. \nThere is a lot to tweak within a Gym environment. You can modify different parameters such as the maximum number of moves in a game, the number of lives, and even the physics of some games. You can also create your own custom Gym environments. <br><br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8092769414294833
      ],
      "excerpt": "http://www.gatsby.ucl.ac.uk/~dayan/papers/cjch.pdf - paper on Q-learning<br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8228633591791039
      ],
      "excerpt": "https://medium.com/machine-learning-for-humans/reinforcement-learning-6eacf258b265 - blog post on RL<br> \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/matthewsparr/Reinforcement-Learning-Lesson/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 7,
      "date": "Wed, 29 Dec 2021 09:18:52 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/matthewsparr/Reinforcement-Learning-Lesson/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "matthewsparr/Reinforcement-Learning-Lesson",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/matthewsparr/Reinforcement-Learning-Lesson/master/Reinforcement%20Learning%20-%20Lesson.ipynb",
      "https://raw.githubusercontent.com/matthewsparr/Reinforcement-Learning-Lesson/master/FrozenLake-Exercise-Solution.ipynb",
      "https://raw.githubusercontent.com/matthewsparr/Reinforcement-Learning-Lesson/master/Reinforcement%20Learning%20-%20Lab%20-%20Solution.ipynb",
      "https://raw.githubusercontent.com/matthewsparr/Reinforcement-Learning-Lesson/master/FrozenLake-Exercise.ipynb",
      "https://raw.githubusercontent.com/matthewsparr/Reinforcement-Learning-Lesson/master/Reinforcement%20Learning%20-%20Lab%20-%20Main.ipynb",
      "https://raw.githubusercontent.com/matthewsparr/Reinforcement-Learning-Lesson/master/.ipynb_checkpoints/Reinforcement%20Learning%20-%20Lesson-checkpoint.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9978517649029499,
        0.999746712887969
      ],
      "excerpt": "Gym can be install using pip: \n<b>pip install gym</b> <br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9374546618784233,
        0.8298312993774783
      ],
      "excerpt": "Windows users, if you want to run Atari environments, you will have to also install this: <br><br> \n<b>pip install --no-index -f https://github.com/Kojoley/atari-py/releases atari_py</b> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8507488652753381
      ],
      "excerpt": "env = gym.make('MsPacman-v4') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8676658487553607
      ],
      "excerpt": "Calling <b>reset()</b> on the environment will do just that - reset it to a new game. <br> <br> \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8314823419435533
      ],
      "excerpt": "<img src = \"rl_chart.png\"></img> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8314823419435533
      ],
      "excerpt": "<img src = \"penguin_game.png\"></img> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8314823419435533
      ],
      "excerpt": "<img src = \"q_formula.png\"></img> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.836809527649333
      ],
      "excerpt": "Here is a basic rundown of how training happens in a DQN. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8659259975398079
      ],
      "excerpt": "<img src=\"gym_games.jpg\"></img><br><br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9133368656218674,
        0.8396948529258378
      ],
      "excerpt": "import gym \nimport time \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/matthewsparr/Reinforcement-Learning-Lesson/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Reinforcement Learning",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Reinforcement-Learning-Lesson",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "matthewsparr",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/matthewsparr/Reinforcement-Learning-Lesson/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 29 Dec 2021 09:18:52 GMT"
    },
    "technique": "GitHub API"
  }
}