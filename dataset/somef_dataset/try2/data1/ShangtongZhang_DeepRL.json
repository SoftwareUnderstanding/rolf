{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1602.01783",
      "https://arxiv.org/abs/1509.06461",
      "https://arxiv.org/abs/1511.06581",
      "https://arxiv.org/abs/1312.5602",
      "https://arxiv.org/abs/1106.5730",
      "https://arxiv.org/abs/1509.02971",
      "https://arxiv.org/abs/1506.02438",
      "https://arxiv.org/abs/1706.04208",
      "https://arxiv.org/abs/1502.05477",
      "https://arxiv.org/abs/1707.06347",
      "https://arxiv.org/abs/1707.02286",
      "https://arxiv.org/abs/1507.08750",
      "https://arxiv.org/abs/1707.06887",
      "https://arxiv.org/abs/1710.10044",
      "https://arxiv.org/abs/1609.05140",
      "https://arxiv.org/abs/1802.09477",
      "https://arxiv.org/abs/1801.00690",
      "https://arxiv.org/abs/2101.08862",
      "https://arxiv.org/abs/2101.02808",
      "https://arxiv.org/abs/2004.10888",
      "https://arxiv.org/abs/2007.06703",
      "https://arxiv.org/abs/1911.04384",
      "https://arxiv.org/abs/2001.11113",
      "https://arxiv.org/abs/1905.01072",
      "https://arxiv.org/abs/1903.11329",
      "https://arxiv.org/abs/1904.12691",
      "https://arxiv.org/abs/1811.02073",
      "https://arxiv.org/abs/1811.02696"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* [Human Level Control through Deep Reinforcement Learning](https://www.nature.com/nature/journal/v518/n7540/full/nature14236.html)\n* [Asynchronous Methods for Deep Reinforcement Learning](https://arxiv.org/abs/1602.01783)\n* [Deep Reinforcement Learning with Double Q-learning](https://arxiv.org/abs/1509.06461)\n* [Dueling Network Architectures for Deep Reinforcement Learning](https://arxiv.org/abs/1511.06581)\n* [Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602)\n* [HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent](https://arxiv.org/abs/1106.5730)\n* [Deterministic Policy Gradient Algorithms](http://proceedings.mlr.press/v32/silver14.pdf)\n* [Continuous control with deep reinforcement learning](https://arxiv.org/abs/1509.02971)\n* [High-Dimensional Continuous Control Using Generalized Advantage Estimation](https://arxiv.org/abs/1506.02438)\n* [Hybrid Reward Architecture for Reinforcement Learning](https://arxiv.org/abs/1706.04208)\n* [Trust Region Policy Optimization](https://arxiv.org/abs/1502.05477)\n* [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)\n* [Emergence of Locomotion Behaviours in Rich Environments](https://arxiv.org/abs/1707.02286)\n* [Action-Conditional Video Prediction using Deep Networks in Atari Games](https://arxiv.org/abs/1507.08750)\n* [A Distributional Perspective on Reinforcement Learning](https://arxiv.org/abs/1707.06887)\n* [Distributional Reinforcement Learning with Quantile Regression](https://arxiv.org/abs/1710.10044)\n* [The Option-Critic Architecture](https://arxiv.org/abs/1609.05140)\n* [Addressing Function Approximation Error in Actor-Critic Methods](https://arxiv.org/abs/1802.09477)\n* Some hyper-parameters are from [DeepMind Control Suite](https://arxiv.org/abs/1801.00690), [OpenAI Baselines](https://github.com/openai/baselines) and [Ilya Kostrikov](https://github.com/ikostrikov/pytorch-a2c-ppo-acktr)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{deeprl,\n  author = {Zhang, Shangtong},\n  title = {Modularized Implementation of Deep RL Algorithms in PyTorch},\n  year = {2018},\n  publisher = {GitHub},\n  journal = {GitHub Repository},\n  howpublished = {\\url{https://github.com/ShangtongZhang/DeepRL}},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8714162992508173
      ],
      "excerpt": "* Quantile Regression DQN (QR-DQN) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.834695500396296
      ],
      "excerpt": "* Mean-Variance Policy Iteration for Risk-Averse Reinforcement Learning [MVPI] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8052858479707351
      ],
      "excerpt": "* Deep Residual Reinforcement Learning [Bi-Res-DDPG] \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ShangtongZhang/DeepRL",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-04-20T19:59:53Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-17T14:05:17Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9379141539547785
      ],
      "excerpt": "Modularized implementation of popular deep RL algorithms in PyTorch. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8503829381739865
      ],
      "excerpt": "* (Double/Dueling/Prioritized) Deep Q-Learning (DQN) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9303014446896113,
        0.8382561901252524
      ],
      "excerpt": "The DQN agent, as well as C51 and QR-DQN, has an asynchronous actor for data generation and an asynchronous replay buffer for transferring data to GPU. \nUsing 1 RTX 2080 Ti and 3 threads, the DQN agent runs for 10M steps (40M frames, 2.5M gradient updates) for Breakout within 6 hours. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9452294674962353,
        0.8454002143388467
      ],
      "excerpt": "They are located in other branches of this repo and seem to be good examples for using this codebase. \n* Breaking the Deadly Triad with a Target Network [TargetNetwork] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8158152188955269,
        0.8703726064242825
      ],
      "excerpt": "* Mean-Variance Policy Iteration for Risk-Averse Reinforcement Learning [MVPI] \n* Learning Retrospective Knowledge with Reverse Reinforcement Learning [ReverseRL] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Modularized Implementation of Deep RL Algorithms in PyTorch",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ShangtongZhang/DeepRL/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 599,
      "date": "Thu, 23 Dec 2021 15:02:14 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ShangtongZhang/DeepRL/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "ShangtongZhang/DeepRL",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/ShangtongZhang/DeepRL/master/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/ShangtongZhang/DeepRL/master/docker_python.sh",
      "https://raw.githubusercontent.com/ShangtongZhang/DeepRL/master/docker_clean.sh",
      "https://raw.githubusercontent.com/ShangtongZhang/DeepRL/master/docker_build.sh",
      "https://raw.githubusercontent.com/ShangtongZhang/DeepRL/master/docker_shell.sh",
      "https://raw.githubusercontent.com/ShangtongZhang/DeepRL/master/docker_batch.sh",
      "https://raw.githubusercontent.com/ShangtongZhang/DeepRL/master/docker_stop.sh"
    ],
    "technique": "File Exploration"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ShangtongZhang/DeepRL/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Dockerfile",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 Shangtong Zhang\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "DeepRL",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "DeepRL",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "ShangtongZhang",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ShangtongZhang/DeepRL/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "ShangtongZhang",
        "body": "",
        "dateCreated": "2020-03-23T15:59:45Z",
        "datePublished": "2020-03-23T16:06:48Z",
        "html_url": "https://github.com/ShangtongZhang/DeepRL/releases/tag/v1.4",
        "name": "Upgrade to PyTorch v1.4.0",
        "tag_name": "v1.4",
        "tarball_url": "https://api.github.com/repos/ShangtongZhang/DeepRL/tarball/v1.4",
        "url": "https://api.github.com/repos/ShangtongZhang/DeepRL/releases/24778933",
        "zipball_url": "https://api.github.com/repos/ShangtongZhang/DeepRL/zipball/v1.4"
      },
      {
        "authorType": "User",
        "author_name": "ShangtongZhang",
        "body": "",
        "dateCreated": "2019-05-02T07:57:52Z",
        "datePublished": "2019-05-02T08:01:01Z",
        "html_url": "https://github.com/ShangtongZhang/DeepRL/releases/tag/v1.1",
        "name": "Upgrade to PyTorch v1.1.0",
        "tag_name": "v1.1",
        "tarball_url": "https://api.github.com/repos/ShangtongZhang/DeepRL/tarball/v1.1",
        "url": "https://api.github.com/repos/ShangtongZhang/DeepRL/releases/17103070",
        "zipball_url": "https://api.github.com/repos/ShangtongZhang/DeepRL/zipball/v1.1"
      },
      {
        "authorType": "User",
        "author_name": "ShangtongZhang",
        "body": "The main update is to use TensorBoard to fully replace the plotting system from Open AI baselines. The latter has turned out to be a bad choice.\r\n\r\nPreviously, there is an internal version for me to run experiments on servers, including many helping scripts. I find recently it is intractable to maintain two versions at the same time. So from now on they are merged together.\r\n\r\nI really don't like shell. So my philosophy is to use python as much as possible. I don't like the common style that we pass a loooooong arg list to a script to specify hyper-parameters.\r\n\r\nCurrently I cannot upgrade to Pytorch v1.0.1 as many of my ongoing projects are still based on v0.4.0. I will do this as soon as possible.",
        "dateCreated": "2019-03-19T09:20:04Z",
        "datePublished": "2019-03-19T09:32:00Z",
        "html_url": "https://github.com/ShangtongZhang/DeepRL/releases/tag/v1.0",
        "name": "Get rid of Open AI baselines' plotting system",
        "tag_name": "v1.0",
        "tarball_url": "https://api.github.com/repos/ShangtongZhang/DeepRL/tarball/v1.0",
        "url": "https://api.github.com/repos/ShangtongZhang/DeepRL/releases/16199597",
        "zipball_url": "https://api.github.com/repos/ShangtongZhang/DeepRL/zipball/v1.0"
      },
      {
        "authorType": "User",
        "author_name": "ShangtongZhang",
        "body": "",
        "dateCreated": "2018-10-06T14:58:08Z",
        "datePublished": "2018-10-31T14:54:34Z",
        "html_url": "https://github.com/ShangtongZhang/DeepRL/releases/tag/v0.4",
        "name": "The last version with action-conditional video prediction",
        "tag_name": "v0.4",
        "tarball_url": "https://api.github.com/repos/ShangtongZhang/DeepRL/tarball/v0.4",
        "url": "https://api.github.com/repos/ShangtongZhang/DeepRL/releases/13757431",
        "zipball_url": "https://api.github.com/repos/ShangtongZhang/DeepRL/zipball/v0.4"
      },
      {
        "authorType": "User",
        "author_name": "ShangtongZhang",
        "body": "After this release, there is no official support for Python 2, although I expect most of the code will still work well in Python 2.",
        "dateCreated": "2018-06-24T00:29:17Z",
        "datePublished": "2018-06-30T03:25:07Z",
        "html_url": "https://github.com/ShangtongZhang/DeepRL/releases/tag/v0.3",
        "name": "End of official support for Python 2",
        "tag_name": "v0.3",
        "tarball_url": "https://api.github.com/repos/ShangtongZhang/DeepRL/tarball/v0.3",
        "url": "https://api.github.com/repos/ShangtongZhang/DeepRL/releases/11721439",
        "zipball_url": "https://api.github.com/repos/ShangtongZhang/DeepRL/zipball/v0.3"
      },
      {
        "authorType": "User",
        "author_name": "ShangtongZhang",
        "body": "After this release, all the codes are incompatible with PyTorch v0.3.x",
        "dateCreated": "2018-04-25T03:49:25Z",
        "datePublished": "2018-04-25T05:19:20Z",
        "html_url": "https://github.com/ShangtongZhang/DeepRL/releases/tag/v0.2",
        "name": "End of PyTorch v0.3",
        "tag_name": "v0.2",
        "tarball_url": "https://api.github.com/repos/ShangtongZhang/DeepRL/tarball/v0.2",
        "url": "https://api.github.com/repos/ShangtongZhang/DeepRL/releases/10707022",
        "zipball_url": "https://api.github.com/repos/ShangtongZhang/DeepRL/zipball/v0.2"
      },
      {
        "authorType": "User",
        "author_name": "ShangtongZhang",
        "body": "I found the current Atari wrapper I used is not fully compatible with the one in OpenAI baselines, resulting a dropped performance for most games (except for Pong). So I plan to do a major update to fix this issue. (To be more specific, OpenAI baselines track the return of the original episode which usually has more than one lives, however I track the return of the episode that only has one life)\r\n\r\nMoreover, asynchronous methods are getting deprecated nowadays, so I will remove them and switch to A2C style algorithms in next version.\r\n\r\nI made this tag in case someone may still want some old stuff.\r\n\r\nTo be more specific, following are implemented algorithms in this release:\r\n* Deep Q-Learning (DQN)\r\n* Double DQN\r\n* Dueling DQN\r\n* (Async) Advantage Actor Critic (A3C / A2C)\r\n* Async One-Step Q-Learning\r\n* Async One-Step Sarsa \r\n* Async N-Step Q-Learning\r\n* Continuous A3C\r\n* Distributed Deep Deterministic Policy Gradient (Distributed DDPG, aka D3PG)\r\n* Parallelized Proximal Policy Optimization (P3O, similar to DPPO)\r\n* Action Conditional Video Prediction\r\n* Categorical DQN (C51, Distributional DQN with KL Distance)\r\n* Quantile Regression DQN (Distributional DQN with Wasserstein Distance)\r\n* N-Step DQN (similar to A2C)\r\n\r\nMost of them are compatible with both Python2 and Python3, however almost all the async methods can only work in Python2.",
        "dateCreated": "2018-04-02T21:22:56Z",
        "datePublished": "2018-04-04T21:05:55Z",
        "html_url": "https://github.com/ShangtongZhang/DeepRL/releases/tag/v0.1",
        "name": "End of Asynchronous Methods",
        "tag_name": "v0.1",
        "tarball_url": "https://api.github.com/repos/ShangtongZhang/DeepRL/tarball/v0.1",
        "url": "https://api.github.com/repos/ShangtongZhang/DeepRL/releases/10399047",
        "zipball_url": "https://api.github.com/repos/ShangtongZhang/DeepRL/zipball/v0.1"
      }
    ],
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* PyTorch v1.5.1\n* See ```Dockerfile``` and ```requirements.txt``` for more details\n\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "![Loading...](https://raw.githubusercontent.com/ShangtongZhang/DeepRL/master/images/Breakout.png)\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2640,
      "date": "Thu, 23 Dec 2021 15:02:14 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "pytorch",
      "deep-reinforcement-learning",
      "dqn",
      "dueling-network-architecture",
      "double-dqn",
      "deeprl",
      "ddpg",
      "ppo",
      "categorical-dqn",
      "option-critic",
      "quantile-regression",
      "td3",
      "a2c",
      "option-critic-architecture",
      "prioritized-experience-replay",
      "rainbow"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```examples.py``` contains examples for all the implemented algorithms.  \n```Dockerfile``` contains the environment for generating the curves below.  \nPlease use this bibtex if you want to cite this repo\n```\n@misc{deeprl,\n  author = {Zhang, Shangtong},\n  title = {Modularized Implementation of Deep RL Algorithms in PyTorch},\n  year = {2018},\n  publisher = {GitHub},\n  journal = {GitHub Repository},\n  howpublished = {\\url{https://github.com/ShangtongZhang/DeepRL}},\n}\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}