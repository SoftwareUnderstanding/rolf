{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1802.09477, 2018.](https://arxiv.org/pdf/1802.09477.pdf)\n2. [OpenAI \u2014 Spinning Up](https://spinningup.openai.com/en/latest/algorithms/td3.html) \n3. [Solving-CarRacing-with-DDPG](https://github.com/lzhan144/Solving-CarRacing-with-DDPG/blob/master/TD3.py)\n4. [TD3: Learning To Run With AI](https://towardsdatascience.com/td3-learning-to-run-with-ai-40dfc512f93)\n    \n"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. [Fujimoto, S., van Hoof, H., and Meger, D. Addressing function approximation error in actor-critic methods. arXiv preprint arXiv:1802.09477, 2018.](https://arxiv.org/pdf/1802.09477.pdf)\n2. [OpenAI \u2014 Spinning Up](https://spinningup.openai.com/en/latest/algorithms/td3.html) \n3. [Solving-CarRacing-with-DDPG](https://github.com/lzhan144/Solving-CarRacing-with-DDPG/blob/master/TD3.py)\n4. [TD3: Learning To Run With AI](https://towardsdatascience.com/td3-learning-to-run-with-ai-40dfc512f93)\n    \n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9251255279142222
      ],
      "excerpt": "2. On touching walls : -10.0 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/GauravPatel89/Car-Navigation-Simulation-using-TD3",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-06-19T11:18:16Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-08-24T11:23:53Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9658102327696575,
        0.9622890858902178,
        0.8821178082917323
      ],
      "excerpt": "Just like humans Car needs to go through tons of experiences, learn which actions are rewarding and which are penalizing, experiment, make mistakes, improvise. It's a lot. But its doable using Deep Reinforcement Learning. \nIn this submission we have used one of the most powerful RL algorithm, Twin Delayed DDPG aka TD3. Detailed explanation on working of this algorithm can be found in previous session submissions and at references listed at the end of this page.    \nComing back to implementation, Car's navigation knowledge lies in TD3 algorithm's Actor and Critic models. To train these models there are two options \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9196995757156778,
        0.963972273160331,
        0.9846158769300614,
        0.850146799863051,
        0.8005260972930622
      ],
      "excerpt": "To simulate the car navigation task an environment has been defined. Environment definition is similar to standard Gym environments. Important functions like step(),reset(),render() have been provided for ease of use. This way the training algorithm need not worry about car movement,visualization, reward generations etc. Actor has to just query the environment for current state and based on that provide action to the environment. Environment in turn takes provided action and generates next state and also informs Actor about reward for that step.  \nSome of the important components of the Environment are explained below. \nState for the environment should be such that it satisfies markov model, meaning at any point of time the state should be able to represent environment's current setup irrespective of past actions and states. In simple terms we must define 'State' such that it conveys to the model all the information about the Environment in order to take an appropriate action to achieve the specified target. \nFor our problem, the target can be simplified into 2 tasks. \n1. Stay on the Road \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9678301168503209,
        0.9350358821597736,
        0.9232618232910185
      ],
      "excerpt": "For achieving first task we must define our state such that model can ascertain whether it is on road and if not how and where to turn to get back on road. \nFor second task, we must have have information as to how far is the Goal and where does it lie wrt the car. \nKeeping this in mind we have 4 components in our 'State' \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9342557757806653
      ],
      "excerpt": "Current state image for the environment is a cropped view of road network in car's front view i.e. how car is viewing the area around it. In this view car is always facing front but area around it changes as car navigates as shown below.     \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8768633072435976
      ],
      "excerpt": "Currently crop size has been selected to be 40x40 . This image is estimated by first cropping an area twice the required crop size, rotating it by (90-car.angle) then again cropping it to required crop size. One such example is illustrated below. Car angle is assumed to be 10 degrees. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.851941320103402
      ],
      "excerpt": "This value corresponds orientation of Goal wrt to Car's current inclination angle. It is calculated as angular difference between vector1, joining car location and goal location, and vector2, representing Car's pointing direction. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9469957898460363
      ],
      "excerpt": "Action space for the environment defines kind of actions environment allows an Actor to take. For our environment action space is 1 dimensional i.e. just one value, 'angle of rotation'.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9274241320855222,
        0.9735980551969565,
        0.8297394019724771,
        0.8696600747289791
      ],
      "excerpt": "Max size of this rotation has been limited to 5 degrees thus during any step car can rotate by maximum of -5.0 or 5.0 degrees. \nRuns of environment are classified as episodes. In each episode, car tries to reach 3 number of randomly selected goals. Episode is ended based on 3 criteria. \nAll the 3 Goals achieved. \nCar is at the boundary and its position has not changed for last 50 steps \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9087960341846711,
        0.8045547719942087
      ],
      "excerpt": "The reward system plays most important role in conveying to the Actor which actions in particular state are rewarding and which are not.  \nFor our environment we have following reward system. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.805514848087018
      ],
      "excerpt": "3. For being on road and moving towards Goal: 0.5 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8636320877247143,
        0.8715449000831114,
        0.938112272816165
      ],
      "excerpt": "The environment provides 3 Goals during each episode. In order to prevent Actor from memorizing (Overfitting) path to Goals it is important to randomize them. In our environment 3 Goals are selected at random during each episode from a list of 10 Goals. \nAt the heart Navigation learning is Twin Delayed DDPG (TD3) algorithm. It's implementation has been explained in detail in previous session here.  \nTD3 consists of 2 types of Neural Networks.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9912689671270366
      ],
      "excerpt": "Task of an Actor is to predict an 'Action' for given 'State'. In our implementation Actor network is as shown below. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9894169736914181
      ],
      "excerpt": "Task of a Critic is to predict a Q-Value for given 'State'  and 'Action' (Generated by Actor). In our implementation Critic network is as shown below.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9399332875099985
      ],
      "excerpt": "This file contains definition of Actor and Critic DNNs, ReplayBuffer used for storing step transitions and TD3 class which implements TD3 algorithm. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8717544765231088
      ],
      "excerpt": "This file contains code for evaluating the trained models. This is done by instantiating TD3 class object, loading saved Actor and Critic models and repeatedly generating action and taking action on defined environment to generate visualization. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.907898249861744
      ],
      "excerpt": "This file contains some of the utilities used by other files. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9230318596599654
      ],
      "excerpt": "This file has Environment definition similar to 'endGameEnv.py'. Difference is 'endGameEnv.py' environment's episode runs for only 3 random goal values while environment defined in this file runs the episode untill all the goals in the goalList have been achieved or car gets stuck to boundaries. This file is useful for evaluating how model is working for all the goal values. It was used for generating the submission video. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8010200513293553
      ],
      "excerpt": "This is the Google Colab file for TD3 training. 'endgameTD3.py' is simple .py version of this file. This file can be accessed on Google Colab here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9469504546189675
      ],
      "excerpt": "a. 'car.png': Used for visualization of Car on city map.  \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/GauravPatel89/Car-Navigation-Simulation-using-TD3/releases",
    "technique": "GitHub API"
  },
  "faq": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Teach car to reach different goals on the city map while traveling on road using **Twin Delayed DDPG (TD3)** algorithm. \n\n",
      "technique": "Header extraction"
    }
  ],
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Fri, 24 Dec 2021 16:44:02 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/GauravPatel89/Car-Navigation-Simulation-using-TD3/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "GauravPatel89/Car-Navigation-Simulation-using-TD3",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/GauravPatel89/Car-Navigation-Simulation-using-TD3/master/endGameTD3.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.985821082170063,
        0.9663056094827679
      ],
      "excerpt": "cd to 'EndGame' directory    \ncd path-to-EndGame-folder \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9071592223557394
      ],
      "excerpt": "For our environment we have following reward system. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8539305815027328
      ],
      "excerpt": "This directory contains image files used by the carEndgameEnv environment. It has following files \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.805798944429871
      ],
      "excerpt": "Download entire EndGame folder \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9364541555520881,
        0.9246227682586091
      ],
      "excerpt": "Run endGameTD3.py  \npython endGameTD3.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9106896921769732
      ],
      "excerpt": "upload files ('car.png','citymap.png','MASK1.png','endGameEnv.py','endGameModels.py','endGameUtilities.py') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8238408973214699
      ],
      "excerpt": "The training process will go on for a while. Meanwhile intermediate training information and evaluations will be shown to convey how training is progressing. Training will periodically store trained Actor, Critic models to 'pytorch_models' directory.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991
      ],
      "excerpt": "'endGameEnv.py' : \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991
      ],
      "excerpt": "'endGameModels.py' : \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991
      ],
      "excerpt": "'endGameInference.py' : \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991
      ],
      "excerpt": "'endGameUtilities.py' : \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991
      ],
      "excerpt": "'endGameEnvAllGoals.py' : \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991
      ],
      "excerpt": "'endgameTD3.py' : \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8033649895403507
      ],
      "excerpt": "This is the Google Colab file for TD3 training. 'endgameTD3.py' is simple .py version of this file. This file can be accessed on Google Colab here. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/GauravPatel89/Car-Navigation-Simulation-using-TD3/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Project EndGame: Car Navigation using Deep Reinforcement Learning",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Car-Navigation-Simulation-using-TD3",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "GauravPatel89",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/GauravPatel89/Car-Navigation-Simulation-using-TD3/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Now that our Car has learned so much, we must see how it performs. This step must be run on local machine as follows.\n- cd to 'EndGame' directory   \n\n        cd path-to-EndGame-folder\n\n\n- Copy Actor, Critic models to be evaluated to 'pytorch_models' with names 'TD3_carEndGameEnv_0_actor.pth' and 'TD3_carEndGameEnv_0_critic.pth' respectively.\n- Run [endGameInference.py](https://github.com/GauravPatel89/EVA-Track3-Assignments/blob/master/EndGame/endGameInference.py)\n\n        python endGameInference.py\n\n- 'endGameInference.py' provides options for live car running view, video generation and average reward visualization.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Fri, 24 Dec 2021 16:44:02 GMT"
    },
    "technique": "GitHub API"
  }
}