{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1512.02325\u00a0\n\nMarkDetector.py uses a CNN to identify the landmark locations on given faces. It firstly crops faces from input image by expanding the bonding boxes from the FaceDetector. Then, these cropped faces are sent to CNN to identify the facial landmarks. Finally, these landmark locations are fitted back into the coordinates of the  original image. \n\nPoseEstimator.py estimates the head pose by mapping the detected 2D facial landmarks from MarkDetector to the average 3D facial landmarks. Two ways to estimate head pose are implemented, one uses 6 landmarks, and the other uses all 68 landmarks detected. \n\nkf_2points.py is a Kalman filter for bonding boxes. Each bonding box has 2 2D points, and these 2 points kalman filters simultaneously with kf_2points.py\n\nMarkStablizer.py is is a Kalman filter implemetation for 1D and 2D points, which are used for landmarks (2D points"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/EricYang3721/faces",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-06-08T19:40:58Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-14T03:20:27Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9287030906839163,
        0.8902433162383191
      ],
      "excerpt": "This is an implementation of a multi-face tracking system using kalman filter to track multiple faces in video/webcam, including faces and landmarks detection, head pose estimation. \nThe implementation is organized as following \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9863259634016291,
        0.9100483664460068,
        0.8404986073555486,
        0.9755245744201697,
        0.9712475597013379
      ],
      "excerpt": "MarkDetector.py uses a CNN to identify the landmark locations on given faces. It firstly crops faces from input image by expanding the bonding boxes from the FaceDetector. Then, these cropped faces are sent to CNN to identify the facial landmarks. Finally, these landmark locations are fitted back into the coordinates of the  original image.  \nPoseEstimator.py estimates the head pose by mapping the detected 2D facial landmarks from MarkDetector to the average 3D facial landmarks. Two ways to estimate head pose are implemented, one uses 6 landmarks, and the other uses all 68 landmarks detected.  \nkf_2points.py is a Kalman filter for bonding boxes. Each bonding box has 2 2D points, and these 2 points kalman filters simultaneously with kf_2points.py \nMarkStablizer.py is is a Kalman filter implemetation for 1D and 2D points, which are used for landmarks (2D points), and head pose (considering each entry in rotation vector and translation vector in 3D to 2D mapping as a 1D point).  \ncam_head_tracking.py is the application which integrates all above functions for real time face tracking & analysis based on a webcam. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8764969342382964,
        0.9039521594624738,
        0.8577768811740268,
        0.9217238426655447
      ],
      "excerpt": "The yellow rectangle is the original bonding box directly detection, the number at the bottom right corner is the confidence of this detection.  \nThe red box is the face bonding box after kalman filter, and the number on the top left corner is the id of the face identified in current streaming.  \nThe green dots are facial landmarks, and the blue wedge is the head pose direction, both are results after kalman filter.  \nAll detections/estimation on face, landmarks and head pose could independently turned on or off as explained in Section 2. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9186304449208441
      ],
      "excerpt": "Face re-identification is also implemented in case of occlusions. This re-identification relies on the IOU score. If a track loses its detection less than certain number of frames, it could be re-identified as the same face. This number of frames is defined  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9235144799433664,
        0.8938139524885983
      ],
      "excerpt": "   MAX_AGE = 45   #: no.of consecutive unmatched detection before a track is deleted \nThe number of images to draw a tracking is defined as \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/EricYang3721/faces/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3,
      "date": "Sun, 26 Dec 2021 20:24:30 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/EricYang3721/faces/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "EricYang3721/faces",
    "technique": "GitHub API"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/EricYang3721/faces/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Multi-face tracking application",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "faces",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "EricYang3721",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/EricYang3721/faces/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This implementation depends on Python 3.6, OpenCV 3.2.0, tensorflow 1.8.0, numpy. The codes are compile in Ubuntu 16.04 LTS.\n\nAll source code are located in the folder of ./faces/. \n\nThe ssd face detection model (res10_300x300_ssd_iter_140000.caffemodel and res10_300x300_ssd_iter_140000.prototxt) should be put in the folder of ./faces/models/face_detector. The model could be download from \n\n[1]: https://github.com/thegopieffect/computer_vision/blob/master/CAFFE_DNN/res10_300x300_ssd_iter_140000.caffemodel\n[2]: https://github.com/thegopieffect/computer_vision/blob/master/CAFFE_DNN/deploy.prototxt.txt\n\nThe landmark detector model (frozen_inference_graph.pb) should be saved in the folder of  faces/models/landmark_detector. The model for the landmark detector could be downloaded from \n\n[3]: https://github.com/yinguobing/head-pose-estimation/blob/master/assets/frozen_inference_graph.pb\n\nThe 68 3D facial landmarks for an average face in model_landmark.txt should be save in the folder of /faces/models/\n\nTo run the code, just run following code:\n\n```python\npython cam_head_tracking.py\n```\n\nThe detection/estimation of faces, landmarks and head pose could be independently turn on or off in FaceVar.py by setting the true of false with following constants.  \n\n```python\nDRAW_ORIG_BBOX = True   #: drawn the original bonding box from the FaceDetector()\nDRAW_DETECTION_BOX = True  #: drawn tracked face bonding box\nLADNMARK_ON = True   #: turn on landmark tracking\nHEADPOSE_ON = True   #: turn on head pose estimation\n```\n\nOther parameters controlling the tracking system could also be adjusted inside the FaceVar.py. Please refer to the annotation in the file for more details.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 7,
      "date": "Sun, 26 Dec 2021 20:24:30 GMT"
    },
    "technique": "GitHub API"
  }
}