{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Credits:\n- [Baseline common-sense reasoning implementation for images](https://github.com/Wangt-CN/VC-R-CNN) by Tan Wang.\n- The [Mask R-CNN](https://github.com/facebookresearch/maskrcnn-benchmark/) repository helped us workaround a lot of the issues we faced with the VC R-CNN codebase.\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2011.07735",
      "https://arxiv.org/abs/2003.07758\">Iashin et al.</a> as our DVC baseline as it is the current state-of-the-art.</p>\n\nThe figure below outlines the goals of iPerceive VideoQA: (i",
      "https://arxiv.org/abs/2002.12204"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you found our work interesting, please cite it as:\n\n```\n@article{Chadha2020iPerceive,\n  title={{i}{P}erceive: Applying Common-Sense Reasoning to Multi-Modal Dense Video Captioning and Video Question Answering},\n  author={Chadha, Aman and Arora, Gurneet and Kaloty, Navpreet},\n  journal={2021 IEEE Winter Conference on Applications of Computer Vision (WACV)},\n  pages={1-13},\n  year={2021},\n  publisher={IEEE}\n}\n```\n\n```\nA. Chadha, G. Arora and N. Kaloty. iPerceive: Applying Common-Sense Reasoning to Multi-Modal Dense Video Captioning and Video Question Answering, pp. 1\u201313, 2021.\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{Chadha2020iPerceive,\n  title={{i}{P}erceive: Applying Common-Sense Reasoning to Multi-Modal Dense Video Captioning and Video Question Answering},\n  author={Chadha, Aman and Arora, Gurneet and Kaloty, Navpreet},\n  journal={2021 IEEE Winter Conference on Applications of Computer Vision (WACV)},\n  pages={1-13},\n  year={2021},\n  publisher={IEEE}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.999997599733159
      ],
      "excerpt": "Project for Stanford CS231n: CS231n: Convolutional Neural Networks for Visual Recognition. Published in IEEE Winter Conference on Applications of Computer Vision (WACV) 2021. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8656070203791273,
        0.8295750146413969,
        0.956411698907347
      ],
      "excerpt": "Python3 | PyTorch | CNNs | Causality | Reasoning \nWe\u2019re #1 on the Video Question-Answering and #3 on the Dense Video Captioning leaderboard on PapersWithCode! \n<a href=\"https://paperswithcode.com/sota/video-question-answering-on-tvqa\"><p align=center><img src=\"https://github.com/amanchadha/iPerceive/blob/master/vidqa.jpg\" width=\"600px\" height=\"400px\"/></p></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8090016440670298
      ],
      "excerpt": "class MyDataset(object): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9783595426586629
      ],
      "excerpt": "    boxes = [[0, 0, 10, 10], [10, 20, 50, 50]] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8550101043698384
      ],
      "excerpt": "    labels = torch.tensor([10, 20]) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/amanchadha/iPerceive",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-06-10T06:13:34Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-25T15:55:02Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9965135827183605
      ],
      "excerpt": "This is the official PyTorch implementation of our paper. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8550753319022757
      ],
      "excerpt": "We\u2019re #1 on the Video Question-Answering and #3 on the Dense Video Captioning leaderboard on PapersWithCode! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9992791351911494
      ],
      "excerpt": "Most of the previous works in visual understanding, rely solely on understanding the \"what\" (e.g., object recognition) and \"where\" (e.g., event localization), which in some cases, fails to describe correct contextual relationships between events or leads to incorrect underlying visual attention. Part of what defines us as human and fundamentally different from machines is our instinct to seek causality behind any association, say an event Y that happened as a direct result of event X. To this end, we propose iPerceive, a framework capable of understanding the \"why\" between events in a video by building a common-sense knowledge base using contextual cues. We demonstrate the effectiveness of our technique to the dense video captioning (DVC) and video question answering (VideoQA) tasks. Furthermore, while most prior art in DVC and VideoQA relies solely on visual information, other modalities such as audio and speech are vital for a human observer's perception of an environment. We formulate DVC and VideoQA tasks as machine translation problems that utilize multiple modalities. Another common drawback of current methods is that they train the event proposal and captioning model either separately or in alternation, which prevents direct influence of the proposal based on the caption. To address this, we adopt an end-to-end Transformer architecture. Using ablation studies, we demonstrate a considerable contribution from audio and speech components suggesting that these modalities contain substantial complementary information to video frames. By evaluating the performance of iPerceive DVC and iPerceive VideoQA on the ActivityNet Captions and TVQA datasets respectively, we show that our approach furthers the state-of-the-art. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9863556796806824,
        0.9758723451754912,
        0.9768945610764915,
        0.9766542818516515
      ],
      "excerpt": "<p align=\"center\">Figure 1. Top: An example of a cognitive error in DVC. While the girl tries to block the boy's dunking attempt, him *jumping* (event X) eventually *leads* to him dunking the basketball through the hoop (event Y). Bottom: An example of an incorrect attended region where conventional DVC approaches correlate a chef and steak to the activity of cooking without even attending to the nearby oven. We used <a href=\"https://arxiv.org/abs/2003.07758\">Iashin et al.</a> as our DVC baseline as it is the current state-of-the-art.</p> \nThe figure below outlines the goals of iPerceive VideoQA: (i) build a knowledge base for common-sense reasoning, (ii) supplement features extracted from input modalities: video and text (in the form of dense captions, subtitles and QA) and, (iii) implement the relevant-frames selection problem as a multi-label classification task. As such, we apply a two-stage approach. \n<p align=\"center\">Figure 2. Architectural overview of iPerceive DVC. iPerceive DVC generates common-sense vectors from the temporal events that the proposal module localizes (left). Features from all modalities are sent to the corresponding encoder-decoder Transformers (middle). Upon fusing the processed features we finally output the next word in the caption using the distribution over the vocabulary (right).</p> \n<p align=\"center\">Figure 3. Architectural overview of iPerceive VideoQA. Our model consists of two main components: feature fusion and frame selection. For feature fusion, we encode features using a convolutional encoder, generate common-sense vectors from the input video sequence, and use iPerceive DVC for dense captions (left). Features from all modalities (video, dense captions, QA and subtitles) are then fed to dual-layer attention: word/object and frame-level (middle). Upon fusing the attended features, we calculate frame-relevance scores (right).</p> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9582731172716724
      ],
      "excerpt": "Parameters about iPerceive: They are in the end of default.py with annotations. Users can make changes according to their own situation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8648089538652813
      ],
      "excerpt": "For learning iPerceive feature on your own dataset, the crux is to make your own dataset COCO-style (can refer to the data format in detection task) and design the dataloader file, for example coco.py and openimages.py. Here we provide an example for reference. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8774325842748769
      ],
      "excerpt": "    #: add the labels to the boxlist \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9331646658337033
      ],
      "excerpt": "    #: Here you can also add many other characters to the boxlist in addition to the labels, for example `image_id', `category_id' and so on. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8593993976119897
      ],
      "excerpt": "    #: return the image, the boxlist and the idx in your dataset \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9165259544213658,
        0.9385084359224708
      ],
      "excerpt": "    #: we want to split the batches according to the aspect ratio \n    #: of the image, as it can be more efficient than loading the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8597209804970991
      ],
      "excerpt": "2. Extracting features of customized dataset \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8145123614762984,
        0.9506944972133711,
        0.9416385392361971
      ],
      "excerpt": "You don't need to! We can take care of that for you. Details from the paper below: \nNote that since the architecture proposed in Wang et al. essentially serves as an improved visual region encoder given a region of interest (RoI) in an image, it assumes that an RoI exists and is available at test time, which reduces its effectiveness and limits its usability with new datasets that the model has never seen before.  \nWe adapt their work by utilizing a pre-trained Mask R-CNN model to generate bounding boxes for RoIs for frames within each event that has been localized by the event proposal module, before feeding it to the common-sense module. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Applying Common-Sense Reasoning to Multi-Modal Dense Video Captioning and Video Question Answering | Python3 | PyTorch | CNNs | Causality | Reasoning | LSTMs | Transformers | Multi-Head Self Attention | Published in IEEE Winter Conference on Applications of Computer Vision (WACV) 2021",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/amanchadha/iPerceive/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 13,
      "date": "Mon, 27 Dec 2021 20:28:37 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/amanchadha/iPerceive/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "amanchadha/iPerceive",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/amanchadha/iPerceive/master/docker/Dockerfile",
      "https://raw.githubusercontent.com/amanchadha/iPerceive/master/docker/docker-jupyter/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/amanchadha/iPerceive/master/install.sh",
      "https://raw.githubusercontent.com/amanchadha/iPerceive/master/run.sh",
      "https://raw.githubusercontent.com/amanchadha/iPerceive/master/iPerceiveVideoQA/train.sh",
      "https://raw.githubusercontent.com/amanchadha/iPerceive/master/iPerceiveVideoQA/inference.sh",
      "https://raw.githubusercontent.com/amanchadha/iPerceive/master/iPerceiveVideoQA/download.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "**1. Using your own model**\n\nSince the goal of iPerceive is to extract visual common-sense representations using self-supervised learning, we have no metrics for evaluation and we treat it as the feature extraction process.\n\nSpecifically, you can just run the following command to extract common-sense features.\n\n```bash\npython -m torch.distributed.launch --nproc_per_node=$NGPUS tools/test_net.py --config-file \"path/to/config/file.yaml\" TEST.IMS_PER_BATCH images_per_gpu x $GPUS\n```\n\nPlease note that before running, you need to set the suitable path for `BOUNDINGBOX_FILE` and `FEATURE_SAVE_PATH` in `default.py`. (Recall that just given image and bounding box coordinate, our iPerceive can extract the iPerceive Feature)\n\n**2. Using our pre-trained iPerceive model on COCO**\n\n- You can use our pre-trained iPerceive [model](https://drive.google.com/drive/folders/1y44pwGVVzRTr11tDKGnNEabOrif01QX4?usp=sharing). \n- Move it into the model dictionary and set the `last_checkpoint` with the absolute path of `model_final.pth`. \n- The following script is a push-button mechanism to use iPerceive for feature extraction with a pre-trained model (without any need for bounding-box data):\n\n```bash\nrun.sh\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "- Step 0: First, you need to download the COCO dataset and annotations into `/path_to_COCO_dataset/`\n\n- Step 1: Modify the path in `iPerceive/config/paths_catalog.py`, containing the `DATA_DIR` and `DATASETS` path.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9890580655571799,
        0.9884094970874368
      ],
      "excerpt": "pytorch pytorch-nightly torchvision cudatoolkit ninja yacs cython matplotlib tqdm opencv-python h5py lmdb \ninstall.sh installs everything for you (Python packages + CUDA toolkit). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9935459261120722
      ],
      "excerpt": "pip3 install -r requirements.txt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9181517445955639
      ],
      "excerpt": "Next, you need to modify the following files: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8677234995849012
      ],
      "excerpt": "default.py: OUTPUT_DIR denotes the model output dir. TENSORBOARD_EXPERIMENT is the tensorboard loger output dir. Another parameter the user may need notice is the SOLVER.IMS_PER_BATCH which denotes the number of total images per batch. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8006903047238545
      ],
      "excerpt": "1. Training on customized dataset \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9069620884340471
      ],
      "excerpt": "iPerceive/data/datasets/__init__.py: add your dataset to __all__ \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/amanchadha/iPerceive/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Cuda",
      "C++",
      "Dockerfile",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'Mask R-CNN\\n\\nThe MIT License (MIT)\\n\\nCopyright (c) 2017 Matterport, Inc.\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in\\nall copies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\\nTHE SOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "iPerceive: Applying Common-Sense Reasoning to Multi-Modal Dense Video Captioning and Video Question Answering",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "iPerceive",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "amanchadha",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/amanchadha/iPerceive/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Most of the configuration files that we provide assume that we are running 2x images on each GPU with 8x GPUs. In order to be able to run it on fewer GPUs, there are a few possibilities: \n\n**1. Single GPU Training:** \nYou can use ```configs/e2e_mask_rcnn_R_101_FPN_1x.yaml``` as a template. You can override parameters by specifying them as part of your command. \nExample:\n\n```\npython tools/train_net.py --config-file \"configs/e2e_mask_rcnn_R_101_FPN_1x.yaml\" SOLVER.IMS_PER_BATCH 2 SOLVER.BASE_LR 0.0025 SOLVER.MAX_ITER 720000 SOLVER.STEPS \"(480000, 640000)\" TEST.IMS_PER_BATCH 1 MODEL.RPN.FPN_POST_NMS_TOP_N_TRAIN 2000\n```\n\nP.S.: To increase your batch size on your GPU, please check for instructions in the [Mask R-CNN](https://github.com/facebookresearch/maskrcnn-benchmark/) repository.\n \n**2. Multi-GPU Training:**\n\nWe support multi-GPU training using `torch.distributed.launch`. Example command below (change $NGPUS to the number of GPUs you'd like to use):\n\n```bash\npython -m torch.distributed.launch --nproc_per_node=$NGPUS tools/train_net.py --config-file \"path/to/config/file.yaml\" MODEL.RPN.FPN_POST_NMS_TOP_N_TRAIN images_per_gpu x 1000\n```\n\n**Notes**: \n\n- In our experiments, we adopted `e2e_mask_rcnn_R_101_FPN_1x.yaml` **without the Mask Branch** (set False) as our config file.\n\n- The `MODEL.RPN.FPN_POST_NMS_TOP_N_TRAIN` denotes that the proposals are selected for per the batch rather than per image in the default training. The value is calculated by **1000 x images-per-gpu**. Here we have 2 images per GPU, therefore we set the number as 1000 x 2 = 2000. If we have 8 images per GPU, the value should be set as 8000. See [#672@maskrcnn-benchmark](https://github.com/facebookresearch/maskrcnn-benchmark/issues/672) for more details.\n\n- Please note that the learning rate and iteration change rule follows the [scheduling rules from Detectron](https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14-L30), which means the LR needs to be set 2x if the number of GPUs become 2x. In our implementation, the learning rate is set for 4 GPUs and each GPU has 2 images.\n- In our observations, \"optimizing\" the learning rate is a challenging task since iPerceive training is self-supervised model and you cannot measure the goodness of the iPerceive model from training procedure by observing a particular metric. We have provided a generally suitable learning rate and leave it to the end user to tune it to their application.\n- You can turn on the **TensorBoard** logger by adding `--use-tensorboard` into command (Need to install `tensorflow` and `tensorboardx` first).\n- The confounder dictionary `dic_coco.npy` and the prior `stat_prob.npy` are located inside [tools](tools).\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 47,
      "date": "Mon, 27 Dec 2021 20:28:37 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "python",
      "python3",
      "video",
      "captioning",
      "captioning-videos",
      "dense-captioning",
      "convolutional-neural-networks",
      "resnets",
      "videoqa",
      "question-answering",
      "common-sense",
      "causality",
      "distilling-the-knowledge",
      "multi-modal",
      "transformers",
      "reasoning",
      "lstm",
      "self-attention",
      "attention",
      "pytorch"
    ],
    "technique": "GitHub API"
  }
}