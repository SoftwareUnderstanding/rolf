{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Thanks to the University of Edinburgh and EPSRC research council for funding this research.\n \n \n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1810.09502",
      "https://arxiv.org/abs/1703.03400",
      "https://arxiv.org/abs/1810.09502"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/AntreasAntoniou/HowToTrainYourMAMLPytorch",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-10-20T19:39:44Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-22T13:27:48Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Welcome to the code repository of [How to train your MAML](https://arxiv.org/abs/1810.09502). This repository includes code for training both MAML\nand MAML++ models, as well as data providers and the datasets for both. By using this codebase you agree to the terms \nand conditions in the [LICENSE](https://github.com/AntreasAntoniou/HowToTrainYourMAMLPytorch/blob/master/LICENSE) file. If you choose to use the Mini-Imagenet dataset, you must abide by the terms and conditions in the [ImageNet LICENSE](https://github.com/AntreasAntoniou/HowToTrainYourMAMLPytorch/blob/master/imagenet_license.md)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.971268680515619
      ],
      "excerpt": "A replication of the paper \"How to train your MAML\", along with a replication of the original \"Model Agnostic Meta Learning\" (MAML) paper. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9532944961876512
      ],
      "excerpt": "substantially larger than github's limit, we chose to upload it on gdrive using pbzip (parallel zip) compression, which \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9279268043199173
      ],
      "excerpt": "as today's large scale datasets). We have automated the unzipping and usage of the dataset, all one needs to do is download it \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8562393917576245
      ],
      "excerpt": "Note: By downloading and using the mini-imagenet datasets, you accept terms and conditions found in imagenet_license.md \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "custom data provider. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8841327621042574,
        0.9042705153189646
      ],
      "excerpt": "All of the scripts are automatically generated using the script_generation_tools/generate_configs.py script. \nexperiment_scripts: Contains scripts that can reproduce every results in the paper. Each script is easily runnable \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8081559553853153
      ],
      "excerpt": "experiment_template_config: Contains the template configuration files. These files have variables declared in their  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8169153168219712,
        0.9353938780378869
      ],
      "excerpt": "utils: Contains utilities for dataset extraction, parser argument extraction and storage of statistics and others. \ndata.py: Contains the data providers for the few shot meta learning task generation. The data provider is agnostic \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8508031348216382
      ],
      "excerpt": "of that class, as illustrated below: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9394449182630016
      ],
      "excerpt": "samples for class_0    samples for class_1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9394449182630016
      ],
      "excerpt": "samples for class_0    samples for class_1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9064632362804972,
        0.9166446918599273,
        0.8366754422612593,
        0.9225483231159496,
        0.8653920954087263,
        0.8494588572115952
      ],
      "excerpt": "few_shot_learning_system.py: Contains the meta_learning_system class which is where most of MAML and MAML++ are actually \nimplemented. It takes care of inner and outer loop optimization, checkpointing, reloading and statistics generation, as  \nwell as setting the rng seeds in pytorch. \nmeta_neural_network_architectures: Contains new pytorch layers which are capable of utilizing either internal  \nparameter or externally passed parameters. This is very useful in a meta-learning setting where inner-loop update  \nsteps are applied on the internal parameters. By allowing layers to receive weight which they will only use for the  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.881295670469634
      ],
      "excerpt": "without having to reload the internal parameters at every step. Essentially at the technical level, the meta-layers  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8254304725882634
      ],
      "excerpt": "subsequent step just pass the new inner loop/dynamic weights to the network. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8253301553134136
      ],
      "excerpt": " to the experiment builder to run an experiment. Also takes care of automated extraction of data if they are not  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "The original code for the paper \"How to train your MAML\" along with a replication of the original \"Model Agnostic Meta Learning\" (MAML) paper in Pytorch.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/AntreasAntoniou/HowToTrainYourMAMLPytorch/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 110,
      "date": "Thu, 23 Dec 2021 22:08:59 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/AntreasAntoniou/HowToTrainYourMAMLPytorch/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "AntreasAntoniou/HowToTrainYourMAMLPytorch",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/AntreasAntoniou/HowToTrainYourMAMLPytorch/master/install.sh",
      "https://raw.githubusercontent.com/AntreasAntoniou/HowToTrainYourMAMLPytorch/master/experiment_scripts/omniglot_maml-omniglot_5_8_0.1_64_5_2_few_shot.sh",
      "https://raw.githubusercontent.com/AntreasAntoniou/HowToTrainYourMAMLPytorch/master/experiment_scripts/omniglot_maml%2B%2B-omniglot_5_8_0.1_64_20_2_few_shot.sh",
      "https://raw.githubusercontent.com/AntreasAntoniou/HowToTrainYourMAMLPytorch/master/experiment_scripts/mini-imagenet_maml%2B%2B-mini-imagenet_5_2_0.01_48_5_0_few_shot.sh",
      "https://raw.githubusercontent.com/AntreasAntoniou/HowToTrainYourMAMLPytorch/master/experiment_scripts/omniglot_maml-omniglot_1_8_0.1_64_20_0_few_shot.sh",
      "https://raw.githubusercontent.com/AntreasAntoniou/HowToTrainYourMAMLPytorch/master/experiment_scripts/omniglot_maml-omniglot_1_8_0.1_64_5_2_few_shot.sh",
      "https://raw.githubusercontent.com/AntreasAntoniou/HowToTrainYourMAMLPytorch/master/experiment_scripts/omniglot_maml-omniglot_1_8_0.1_64_5_0_few_shot.sh",
      "https://raw.githubusercontent.com/AntreasAntoniou/HowToTrainYourMAMLPytorch/master/experiment_scripts/omniglot_maml%2B%2B-omniglot_5_8_0.1_64_20_0_few_shot.sh",
      "https://raw.githubusercontent.com/AntreasAntoniou/HowToTrainYourMAMLPytorch/master/experiment_scripts/omniglot_maml%2B%2B-omniglot_1_8_0.1_64_5_1_few_shot.sh",
      "https://raw.githubusercontent.com/AntreasAntoniou/HowToTrainYourMAMLPytorch/master/experiment_scripts/mini-imagenet_maml%2B%2B-mini-imagenet_5_2_0.01_48_5_1_few_shot.sh",
      "https://raw.githubusercontent.com/AntreasAntoniou/HowToTrainYourMAMLPytorch/master/experiment_scripts/mini-imagenet_maml-mini-imagenet_5_2_0.01_48_5_2_few_shot.sh",
      "https://raw.githubusercontent.com/AntreasAntoniou/HowToTrainYourMAMLPytorch/master/experiment_scripts/mini-imagenet_maml%2B%2B-mini-imagenet_1_2_0.01_48_5_2_few_shot.sh",
      "https://raw.githubusercontent.com/AntreasAntoniou/HowToTrainYourMAMLPytorch/master/experiment_scripts/omniglot_maml%2B%2B-omniglot_1_8_0.1_64_20_0_few_shot.sh",
      "https://raw.githubusercontent.com/AntreasAntoniou/HowToTrainYourMAMLPytorch/master/experiment_scripts/omniglot_maml%2B%2B-omniglot_1_8_0.1_64_20_2_few_shot.sh",
      "https://raw.githubusercontent.com/AntreasAntoniou/HowToTrainYourMAMLPytorch/master/experiment_scripts/mini-imagenet_maml%2B%2B-mini-imagenet_1_2_0.01_48_5_1_few_shot.sh",
      "https://raw.githubusercontent.com/AntreasAntoniou/HowToTrainYourMAMLPytorch/master/experiment_scripts/omniglot_maml%2B%2B-omniglot_5_8_0.1_64_5_0_few_shot.sh",
      "https://raw.githubusercontent.com/AntreasAntoniou/HowToTrainYourMAMLPytorch/master/experiment_scripts/mini-imagenet_maml%2B%2B-mini-imagenet_1_2_0.01_48_5_0_few_shot.sh",
      "https://raw.githubusercontent.com/AntreasAntoniou/HowToTrainYourMAMLPytorch/master/experiment_scripts/omniglot_maml%2B%2B-omniglot_1_8_0.1_64_20_1_few_shot.sh",
      "https://raw.githubusercontent.com/AntreasAntoniou/HowToTrainYourMAMLPytorch/master/experiment_scripts/mini-imagenet_maml%2B%2B-mini-imagenet_5_2_0.01_48_5_2_few_shot.sh",
      "https://raw.githubusercontent.com/AntreasAntoniou/HowToTrainYourMAMLPytorch/master/experiment_scripts/omniglot_maml-omniglot_5_8_0.1_64_5_0_few_shot.sh",
      "https://raw.githubusercontent.com/AntreasAntoniou/HowToTrainYourMAMLPytorch/master/experiment_scripts/omniglot_maml%2B%2B-omniglot_5_8_0.1_64_5_1_few_shot.sh",
      "https://raw.githubusercontent.com/AntreasAntoniou/HowToTrainYourMAMLPytorch/master/experiment_scripts/mini-imagenet_maml-mini-imagenet_1_2_0.01_48_5_0_few_shot.sh",
      "https://raw.githubusercontent.com/AntreasAntoniou/HowToTrainYourMAMLPytorch/master/experiment_scripts/mini-imagenet_maml-mini-imagenet_5_2_0.01_48_5_0_few_shot.sh",
      "https://raw.githubusercontent.com/AntreasAntoniou/HowToTrainYourMAMLPytorch/master/experiment_scripts/omniglot_maml%2B%2B-omniglot_1_8_0.1_64_5_2_few_shot.sh",
      "https://raw.githubusercontent.com/AntreasAntoniou/HowToTrainYourMAMLPytorch/master/experiment_scripts/mini-imagenet_maml-mini-imagenet_5_2_0.01_48_5_1_few_shot.sh",
      "https://raw.githubusercontent.com/AntreasAntoniou/HowToTrainYourMAMLPytorch/master/experiment_scripts/omniglot_maml-omniglot_1_8_0.1_64_20_2_few_shot.sh",
      "https://raw.githubusercontent.com/AntreasAntoniou/HowToTrainYourMAMLPytorch/master/experiment_scripts/omniglot_maml%2B%2B-omniglot_1_8_0.1_64_5_0_few_shot.sh",
      "https://raw.githubusercontent.com/AntreasAntoniou/HowToTrainYourMAMLPytorch/master/experiment_scripts/omniglot_maml-omniglot_5_8_0.1_64_20_0_few_shot.sh",
      "https://raw.githubusercontent.com/AntreasAntoniou/HowToTrainYourMAMLPytorch/master/experiment_scripts/mini-imagenet_maml-mini-imagenet_1_2_0.01_48_5_2_few_shot.sh",
      "https://raw.githubusercontent.com/AntreasAntoniou/HowToTrainYourMAMLPytorch/master/experiment_scripts/omniglot_maml-omniglot_5_8_0.1_64_20_1_few_shot.sh",
      "https://raw.githubusercontent.com/AntreasAntoniou/HowToTrainYourMAMLPytorch/master/experiment_scripts/omniglot_maml-omniglot_5_8_0.1_64_20_2_few_shot.sh",
      "https://raw.githubusercontent.com/AntreasAntoniou/HowToTrainYourMAMLPytorch/master/experiment_scripts/mini-imagenet_maml-mini-imagenet_1_2_0.01_48_5_1_few_shot.sh",
      "https://raw.githubusercontent.com/AntreasAntoniou/HowToTrainYourMAMLPytorch/master/experiment_scripts/omniglot_maml%2B%2B-omniglot_5_8_0.1_64_20_1_few_shot.sh",
      "https://raw.githubusercontent.com/AntreasAntoniou/HowToTrainYourMAMLPytorch/master/experiment_scripts/omniglot_maml%2B%2B-omniglot_5_8_0.1_64_5_2_few_shot.sh",
      "https://raw.githubusercontent.com/AntreasAntoniou/HowToTrainYourMAMLPytorch/master/experiment_scripts/omniglot_maml-omniglot_1_8_0.1_64_20_1_few_shot.sh",
      "https://raw.githubusercontent.com/AntreasAntoniou/HowToTrainYourMAMLPytorch/master/experiment_scripts/omniglot_maml-omniglot_1_8_0.1_64_5_1_few_shot.sh",
      "https://raw.githubusercontent.com/AntreasAntoniou/HowToTrainYourMAMLPytorch/master/experiment_scripts/omniglot_maml-omniglot_5_8_0.1_64_5_1_few_shot.sh",
      "https://raw.githubusercontent.com/AntreasAntoniou/HowToTrainYourMAMLPytorch/master/script_generation_tools/local_run_template_script.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The code uses Pytorch to run, along with many other smaller packages. To take care of everything at once, we recommend \nusing the conda package management library. More specifically, \n[miniconda3](https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh), as it is lightweight and fast to install.\nIf you have an existing miniconda3 installation please start at step 3. \nIf you want to  install both conda and the required packages, please run:\n 1. ```wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh```\n 2. Go through the installation.\n 3. Activate conda\n 4. conda create -n meta_learning_pytorch_env python=3.6.\n 5. conda activate meta_learning_pytorch_env\n 6. At this stage you need to choose which version of pytorch you need by visiting [here](https://pytorch.org/get-started/locally/)\n 7. Choose and install the pytorch variant of your choice using the conda commands.\n 8. Then run ```bash install.sh```\n\nTo execute an installation script simply run:\n```bash <installation_file_name>```\n\nTo activate your conda installations simply run:\n```conda activate```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.80949435888777
      ],
      "excerpt": "substantially larger than github's limit, we chose to upload it on gdrive using pbzip (parallel zip) compression, which \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8651786053369876
      ],
      "excerpt": "as mini_imagenet, you can instead use: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.800988743481017
      ],
      "excerpt": "datasets folder: Contains the dataset pbzip files and folders containing the images in a structure readable by the  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8080740668152102
      ],
      "excerpt": "In this case the data provider will split the data into 3 sets, train, val and test using the train_val_test_split  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8826507538749373
      ],
      "excerpt": "Train   Val  Test \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/AntreasAntoniou/HowToTrainYourMAMLPytorch/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other",
      "url": "https://raw.githubusercontent.com/AntreasAntoniou/HowToTrainYourMAMLPytorch/master/LICENSE"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2018 Antreas Antoniou\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "How to train your MAML in Pytorch",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "HowToTrainYourMAMLPytorch",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "AntreasAntoniou",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/AntreasAntoniou/HowToTrainYourMAMLPytorch/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To run an experiment from the paper on Omniglot:\n1. Activate your conda environment ```conda activate pytorch_meta_learning_env```\n2. cd experiment_scripts\n3. Find which experiment you want to run.\n4. ```bash experiment_script.sh gpu_ids_separated_by_spaces```\n\nNote: By downloading and using the mini-imagenet datasets, you accept terms and conditions found in [imagenet_license.md](https://github.com/AntreasAntoniou/HowToTrainYourMAMLPytorch/blob/master/imagenet_license.md) \n\nTo run an experiment from the paper on Mini-Imagenet:\n1. Activate your conda environment ```conda activate pytorch_meta_learning_env```\n2. Download the mini_imagenet dataset from the [mini_imagenet gdrive folder](https://drive.google.com/file/d/1qQCoGoEJKUCQkk8roncWH7rhPN7aMfBr/view?usp=sharing)\n3. copy the .pbzip file in datasets\n4. cd experiment_scripts\n5. Find which experiment you want to run.\n6. ```bash experiment_script.sh gpu_ids_separated_by_spaces```\n\nTo run a custom/new experiment on any dataset:\n1. Activate your conda environment ```conda activate pytorch_meta_learning_env```\n2. Make sure your data is in datasets/ in a folder structure the data provider can read.\n3. cd experiment_template_config\n4. Find an experiment close to what you want to do and open its config file.\n5. For example let's take an omniglot experiment on maml++. Make changes to the hyperparameters such that your \nexperiment takes form. Note that the variables in $<variable>$ are hyperparameters automatically filled by the config\ngeneration script. If you add any new of those, you'll have to change the generate_configs.py file in order to tell it\nwhat to fill those with.\n6.\n    ```json\n    {\n      \"batch_size\":16,\n      \"image_height\":28,\n      \"image_width\":28,\n      \"image_channels\":1,\n      \"gpu_to_use\":0,\n      \"num_dataprovider_workers\":8,\n      \"max_models_to_save\":5,\n      \"dataset_name\":\"omniglot_dataset\",\n      \"dataset_path\":\"omniglot_dataset\",\n      \"reset_stored_paths\":false,\n      \"experiment_name\":\"MAML++_Omniglot_$num_classes$_way_$samples_per_class$_shot_$train_update_steps$_filter_multi_step_loss_with_max_pooling_seed_$train_seed$\",\n      \"train_seed\": $train_seed$, \"val_seed\": $val_seed$,\n      \"train_val_test_split\": [0.70918052988, 0.03080714725, 0.2606284658],\n      \"indexes_of_folders_indicating_class\": [-3, -2],\n      \"sets_are_pre_split\": false,\n    \n      \"total_epochs\": 150,\n      \"total_iter_per_epoch\":500, \"continue_from_epoch\": -2,\n    \n      \"max_pooling\": true,\n      \"per_step_bn_statistics\": true,\n      \"learnable_batch_norm_momentum\": false,\n    \n      \"dropout_rate_value\":0.0,\n      \"min_learning_rate\":0.00001,\n      \"meta_learning_rate\":0.001,   \"total_epochs_before_pause\": 150,\n      \"task_learning_rate\":-1,\n      \"init_task_learning_rate\":0.4,\n      \"first_order_to_second_order_epoch\":80,\n    \n      \"norm_layer\":\"batch_norm\",\n      \"cnn_num_filters\":64,\n      \"num_stages\":4,\n      \"number_of_training_steps_per_iter\":$train_update_steps$,\n      \"number_of_evaluation_steps_per_iter\":$val_update_steps$,\n      \"cnn_blocks_per_stage\":1,\n      \"num_classes_per_set\":$num_classes$,\n      \"num_samples_per_class\":$samples_per_class$,\n      \"num_target_samples\": $target_samples_per_class$,\n    \n      \"second_order\": true,\n      \"use_multi_step_loss_optimization\":true,\n      \"use_gdrive\":false\n    }\n    \n    ```\n7. ```cd script_generation_tools```\n8. ```python generate_configs.py; python generate_scripts.py```\n9. Your new scripts can be found in the experiment_scripts, ready to be run.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 536,
      "date": "Thu, 23 Dec 2021 22:08:59 GMT"
    },
    "technique": "GitHub API"
  }
}