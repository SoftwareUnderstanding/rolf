{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please cite SSD in your publications if it helps your research:\n\n    @inproceedings{liu2016ssd,\n      title = {{SSD}: Single Shot MultiBox Detector},\n      author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},\n      booktitle = {ECCV},\n      year = {2016}\n    }\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{liu2016ssd,\n  title = {{SSD}: Single Shot MultiBox Detector},\n  author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},\n  booktitle = {ECCV},\n  year = {2016}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8721897611844946
      ],
      "excerpt": "Set gpus and batch_size if needed. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9988794968804929
      ],
      "excerpt": "By Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488,
        0.8356013927728488
      ],
      "excerpt": "07+12: SSD300*, SSD512* \n07++12: SSD300*, SSD512* \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Coldmooon/SSD-on-Custom-Dataset",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contributing\nIssues\nSpecific Caffe design and development issues, bugs, and feature requests are maintained by GitHub Issues.\nPlease do not post usage, installation, or modeling questions, or other requests for help to Issues.\nUse the caffe-users list instead. This helps developers maintain a clear, uncluttered, and efficient view of the state of Caffe.\nWhen reporting a bug, it's most helpful to provide the following information, where applicable:\n\nWhat steps reproduce the bug?\nCan you reproduce the bug using the latest master, compiled with the DEBUG make option?\nWhat hardware and operating system/distribution are you running?\nIf the bug is a crash, provide the backtrace (usually printed by Caffe; always obtainable with gdb).\n\nTry to give your issue a title that is succinct and specific. The devs will rename issues as needed to keep track of them.\nPull Requests\nCaffe welcomes all contributions.\nSee the contributing guide for details.\nBriefly: read commit by commit, a PR should tell a clean, compelling story of one improvement to Caffe. In particular:\n\nA PR should do one clear thing that obviously improves Caffe, and nothing more. Making many smaller PRs is better than making one large PR; review effort is superlinear in the amount of code involved.\nSimilarly, each commit should be a small, atomic change representing one step in development. PRs should be made of many commits where appropriate.\nPlease do rewrite PR history to be clean rather than chronological. Within-PR bugfixes, style cleanups, reversions, etc. should be squashed and should not appear in merged PR history.\nAnything nonobvious from the code should be explained in comments, commit messages, or the PR description, as appropriate.",
    "technique": "File Exploration"
  },
  "contributors": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contributors\nCaffe is developed by a core set of BVLC members and the open-source community.\nWe thank all of our contributors!\nFor the detailed history of contributions of a given file, try\ngit blame file\n\nto see line-by-line credits and\ngit log --follow file\n\nto see the change log even across renames and rewrites.\nPlease refer to the acknowledgements on the Caffe site for further details.\nCopyright is held by the original contributor according to the versioning history; see LICENSE.",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-12-22T07:02:23Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-19T03:34:36Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "SSD is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details, please refer to our [arXiv paper](http://arxiv.org/abs/1512.02325) and our [slide](http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf).\n\n<p align=\"center\">\n<img src=\"http://www.cs.unc.edu/~wliu/papers/ssd.png\" alt=\"SSD Framework\" width=\"600px\">\n</p>\n\n| System | VOC2007 test *mAP* | **FPS** (Titan X) | Number of Boxes | Input resolution\n|:-------|:-----:|:-------:|:-------:|:-------:|\n| [Faster R-CNN (VGG16)](https://github.com/ShaoqingRen/faster_rcnn) | 73.2 | 7 | ~6000 | ~1000 x 600 |\n| [YOLO (customized)](http://pjreddie.com/darknet/yolo/) | 63.4 | 45 | 98 | 448 x 448 |\n| SSD300* (VGG16) | 77.2 | 46 | 8732 | 300 x 300 |\n| SSD512* (VGG16) | **79.8** | 19 | 24564 | 512 x 512 |\n\n\n<p align=\"left\">\n<img src=\"http://www.cs.unc.edu/~wliu/papers/ssd_results.png\" alt=\"SSD results on multiple datasets\" width=\"800px\">\n</p>\n\n_Note: SSD300* and SSD512* are the latest models. Current code should reproduce these results._\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9873292523611165
      ],
      "excerpt": "SSD is simple to use but inconvenient to modify codes. In this repo, I list all the files and codes needed to be changed when using a new dataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9807851573667816
      ],
      "excerpt": "SSD provides two scripts to convert any VOC-format dataset to LMDB database. But before doing this, we need to take some efforts to modify necessary codes for processing our new dataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259,
        0.8766740537482074,
        0.9387520117803673
      ],
      "excerpt": "cp data/VOC0712/* data/MELON/ \nNext, modify the data/MELON/create_list.sh. In this script, replace the extension of image files with yours (e.g., png). \nIn the second loop of the script, replace the keywords VOC2007 and VOC2012 with MELON since we have only one dataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "mv data/MELON/labelmap_voc.prototxt data/MELON/labelmap_melon.prototxt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "vim data/MELON/labelmap_melon.prototxt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9678258976658242
      ],
      "excerpt": "replace the keyword dataset_name with MELON, and labelmap_voc.prototxt with labelmap_melon.prototxt. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9193461655058173,
        0.9730996123079696
      ],
      "excerpt": "Change train_data and test_data to our new dataset. \nReplace all the keywords related to voc with melon. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9554025773396336
      ],
      "excerpt": "Modify the num_test_image (important!) and test_batch_size. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8493112611157456
      ],
      "excerpt": "Train your model and evaluate the model on the fly. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9662618183644768
      ],
      "excerpt": "To train on other dataset, please refer to data/OTHERDATASET for more details. We currently add support for COCO and ILSVRC2016. We recommend using examples/ssd.ipynb to check whether the new dataset is prepared correctly. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8011561644547963
      ],
      "excerpt": "PASCAL VOC models: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Train SSD on custom dataset.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Coldmooon/SSD-on-Custom-Dataset/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 21,
      "date": "Sun, 26 Dec 2021 01:54:08 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Coldmooon/SSD-on-Custom-Dataset/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Coldmooon/SSD-on-Custom-Dataset",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/docker/standalone/gpu/Dockerfile",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/docker/standalone/cpu/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/Coldmooon/SSD-on-Custom-Dataset/tree/ssd/docs"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/examples/ssd.ipynb",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/examples/02-fine-tuning.ipynb",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/examples/pascal-multilabel-with-datalayer.ipynb",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/examples/net_surgery.ipynb",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/examples/detection.ipynb",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/examples/01-learning-lenet.ipynb",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/examples/brewing-logreg.ipynb",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/examples/inceptionv3.ipynb",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/examples/ssd_detect.ipynb",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/examples/00-classification.ipynb",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/examples/convert_model.ipynb",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/examples/siamese/mnist_siamese.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/scripts/deploy_docs.sh",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/scripts/build_docs.sh",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/scripts/download_model_from_gist.sh",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/scripts/upload_model_to_gist.sh",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/scripts/gather_examples.sh",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/scripts/travis/install-python-deps.sh",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/scripts/travis/test.sh",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/scripts/travis/defaults.sh",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/scripts/travis/setup-venv.sh",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/scripts/travis/configure-make.sh",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/scripts/travis/configure.sh",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/scripts/travis/build.sh",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/scripts/travis/install-deps.sh",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/scripts/travis/configure-cmake.sh",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/data/MELON/create_data.sh",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/data/MELON/create_list.sh",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/data/ilsvrc12/get_ilsvrc_aux.sh",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/data/VOC0712/create_data.sh",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/data/VOC0712/create_list.sh",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/data/cifar10/get_cifar10.sh",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/data/ILSVRC2016/create_data.sh",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/data/coco/create_data.sh",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/data/mnist/get_mnist.sh",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/examples/imagenet/create_imagenet.sh",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/examples/imagenet/resume_training.sh",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/examples/imagenet/train_caffenet.sh",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/examples/imagenet/make_imagenet_mean.sh",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/examples/cifar10/create_cifar10.sh",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/examples/cifar10/train_quick.sh",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/examples/cifar10/train_full_sigmoid.sh",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/examples/cifar10/train_full_sigmoid_bn.sh",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/examples/cifar10/train_full.sh",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/examples/mnist/train_lenet_adam.sh",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/examples/mnist/train_lenet_consolidated.sh",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/examples/mnist/train_lenet_docker.sh",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/examples/mnist/train_mnist_autoencoder_adagrad.sh",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/examples/mnist/train_lenet_rmsprop.sh",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/examples/mnist/create_mnist.sh",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/examples/mnist/train_mnist_autoencoder_nesterov.sh",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/examples/mnist/train_lenet.sh",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/examples/mnist/train_mnist_autoencoder.sh",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/examples/mnist/train_mnist_autoencoder_adadelta.sh",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/examples/siamese/create_mnist_siamese.sh",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/examples/siamese/train_mnist_siamese.sh",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/tools/extra/parse_log.sh",
      "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/tools/extra/launch_resize_and_crop_images.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Download [fully convolutional reduced (atrous) VGGNet](https://gist.github.com/weiliu89/2ed6e13bfd5b57cf81d6). By default, we assume the model is stored in `$CAFFE_ROOT/models/VGGNet/`\n\n2. Download VOC2007 and VOC2012 dataset. By default, we assume the data is stored in `$HOME/data/`\n  ```Shell\n  #: Download the data.\n  cd $HOME/data\n  wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\n  wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n  wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\n  #: Extract the data.\n  tar -xvf VOCtrainval_11-May-2012.tar\n  tar -xvf VOCtrainval_06-Nov-2007.tar\n  tar -xvf VOCtest_06-Nov-2007.tar\n  ```\n\n3. Create the LMDB file.\n  ```Shell\n  cd $CAFFE_ROOT\n  #: Create the trainval.txt, test.txt, and test_name_size.txt in data/VOC0712/\n  ./data/VOC0712/create_list.sh\n  #: You can modify the parameters in create_data.sh if needed.\n  #: It will create lmdb files for trainval and test with encoded original image:\n  #:   - $HOME/data/VOCdevkit/VOC0712/lmdb/VOC0712_trainval_lmdb\n  #:   - $HOME/data/VOCdevkit/VOC0712/lmdb/VOC0712_test_lmdb\n  #: and make soft links at examples/VOC0712/\n  ./data/VOC0712/create_data.sh\n  ```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Get the code. We will call the directory that you cloned Caffe into `$CAFFE_ROOT`\n  ```Shell\n  git clone https://github.com/weiliu89/caffe.git\n  cd caffe\n  git checkout ssd\n  ```\n\n2. Build the code. Please follow [Caffe instruction](http://caffe.berkeleyvision.org/installation.html) to install all necessary packages and build it.\n  ```Shell\n  #: Modify Makefile.config according to your Caffe installation.\n  cp Makefile.config.example Makefile.config\n  make -j8\n  #: Make sure to include $CAFFE_ROOT/python to your PYTHONPATH.\n  make py\n  make test -j8\n  #: (Optional)\n  make runtest -j8\n  ```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "For convenience, please follow the VOC dataset format to make the new dataset. Click [here](https://drive.google.com/open?id=11nA6c_NUgV4TyuXK1roLW27K2gMDqFMZ) to download the MELON dataset I made for this repo.\n\n```\ncd ~/data/VOCdevkit\n```\n```\nmkdir MELON\n```\n\nPut all training/test images in `MELON/JPEGImages`\n\nPut all xml-format labels in `MELON/Annotations`\n\nAdd all the training/val samples in `MELON/ImageSets/Main/trainval.txt`\n\nAdd all the test samples in `MELON/ImageSets/Main/test.txt`\n\nThe final directory structure is like this:\n\n```\nVOCdevkit\n\u251c\u2500\u2500 MELON\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Annotations\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ImageSets\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 Main\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 JPEGImages\n\u251c\u2500\u2500 VOC2007\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Annotations\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ImageSets\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 Layout\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 Main\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 Segmentation\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 JPEGImages\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 SegmentationClass\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 SegmentationObject\n\u2514\u2500\u2500 VOC2012\n    \u251c\u2500\u2500 Annotations\n    \u251c\u2500\u2500 ImageSets\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 Action\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 Layout\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 Main\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 Segmentation\n    \u251c\u2500\u2500 JPEGImages\n    \u251c\u2500\u2500 SegmentationClass\n    \u2514\u2500\u2500 SegmentationObject\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9830381496987749
      ],
      "excerpt": "Following the original instructions to compile SSD. Make sure that you can run it successfully. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9244436634133352
      ],
      "excerpt": "First cd to the SSD root directory. Then, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8727487047449518,
        0.8111111152111801,
        0.8111111152111801,
        0.8773473436922751
      ],
      "excerpt": "COCO<sup>[1]</sup>: SSD300*, SSD512* \n07+12+COCO: SSD300*, SSD512* \n07++12+COCO: SSD300*, SSD512* \nCOCO models: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8470716424955363
      ],
      "excerpt": "Run data/MELON/create_list.sh to generate test_name_size.txt, test.txt, and trainval.txt in data/MELON/. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.941440458593965
      ],
      "excerpt": "In examples/ssd/ssd_pascal.py. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8107805693592931
      ],
      "excerpt": "  #: and job file, log file, and the python script in: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9563277988753164
      ],
      "excerpt": "  python examples/ssd/ssd_pascal.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9563277988753164
      ],
      "excerpt": "  python examples/ssd/score_ssd_pascal.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9563277988753164,
        0.8231387946236283,
        0.8742997505534339
      ],
      "excerpt": "  python examples/ssd/ssd_pascal_webcam.py \n  Here is a demo video of running a SSD500 model trained on MSCOCO dataset. \nCheck out examples/ssd_detect.ipynb or examples/ssd/ssd_detect.cpp on how to detect objects using a SSD model. Check out examples/ssd/plot_detections.py on how to plot detection results output by ssd_detect.cpp. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Coldmooon/SSD-on-Custom-Dataset/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "C++",
      "Python",
      "Cuda",
      "CMake",
      "MATLAB",
      "Makefile",
      "Shell",
      "Dockerfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other",
      "url": "https://raw.githubusercontent.com/Coldmooon/SSD-on-Custom-Dataset/ssd/LICENSE"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'COPYRIGHT\\n\\nAll new contributions compared to the original branch:\\nCopyright (c) 2015, 2016 Wei Liu (UNC Chapel Hill), Dragomir Anguelov (Zoox),\\nDumitru Erhan (Google), Christian Szegedy (Google), Scott Reed (UMich Ann Arbor),\\nCheng-Yang Fu (UNC Chapel Hill), Alexander C. Berg (UNC Chapel Hill).\\nAll rights reserved.\\n\\nAll contributions by the University of California:\\nCopyright (c) 2014, 2015, The Regents of the University of California (Regents)\\nAll rights reserved.\\n\\nAll other contributions:\\nCopyright (c) 2014, 2015, the respective contributors\\nAll rights reserved.\\n\\nCaffe uses a shared copyright model: each contributor holds copyright over\\ntheir contributions to Caffe. The project versioning records all such\\ncontribution and copyright details. If a contributor wants to further mark\\ntheir specific copyright on a particular contribution, they should indicate\\ntheir copyright solely in the commit message of the change when it is\\ncommitted.\\n\\nLICENSE\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met: \\n\\n1. Redistributions of source code must retain the above copyright notice, this\\n   list of conditions and the following disclaimer. \\n2. Redistributions in binary form must reproduce the above copyright notice,\\n   this list of conditions and the following disclaimer in the documentation\\n   and/or other materials provided with the distribution. \\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR\\nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\\nON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n\\nCONTRIBUTION AGREEMENT\\n\\nBy contributing to the BVLC/caffe repository through pull-request, comment,\\nor otherwise, the contributor releases their content to the\\nlicense and copyright terms herein.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Train SSD on Custom Dataset",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "SSD-on-Custom-Dataset",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Coldmooon",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Coldmooon/SSD-on-Custom-Dataset/blob/ssd/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Run `python examples/ssd/ssd_pascal.py` to train a new model.\n\nUse `python examples/ssd/score_ssd_pascal.py` to evaluate the model.\n\nBelow is the original content.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 56,
      "date": "Sun, 26 Dec 2021 01:54:08 GMT"
    },
    "technique": "GitHub API"
  }
}