{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2007.06346",
      "https://arxiv.org/abs/2007.06346",
      "https://arxiv.org/abs/2002.05709",
      "https://arxiv.org/abs/2006.07733",
      "https://arxiv.org/abs/2007.06346",
      "https://arxiv.org/abs/2007.06346](https://arxiv.org/abs/2007.06346)\n\nIt includes 3 types of losses:\n- W-MSE [arXiv](https://arxiv.org/abs/2007.06346)\n- Contrastive [SimCLR arXiv](https://arxiv.org/abs/2002.05709)\n- BYOL [arXiv](https://arxiv.org/abs/2006.07733)\n\nAnd 5 datasets:\n- CIFAR-10 and CIFAR-100\n- STL-10\n- Tiny ImageNet\n- ImageNet-100\n\nCheckpoints are stored in `data` each 100 epochs during training.\n\nThe implementation is optimized for a single GPU, although multiple are also supported. It includes fast evaluation: we pre-compute embeddings for the entire dataset and then train a classifier on top. The evaluation of the ResNet-18 encoder takes about one minute.\n\n## Installation\n\nThe implementation is based on PyTorch. Logging works on [wandb.ai](https://wandb.ai/). See `docker/Dockerfile`.\n\n#### ImageNet-100\nTo get this dataset, take the original ImageNet and filter out [this subset of classes](https://github.com/HobbitLong/CMC/blob/master/imagenet100.txt). We do not use augmentations during testing, and loading big images with resizing on the fly is slow, so we can preprocess classifier train and test images. We recommend [mogrify](https://imagemagick.org/script/mogrify.php) for it. First, you need to resize to 256 (just like `torchvision.transforms.Resize(256)`) and then crop to 224 (like `torchvision.transforms.CenterCrop(224)`). Finally, put the original images to `train`, and resized to `clf` and `test`.\n\n## Usage\n\nDetailed settings are good by default, to see all options:\n```\npython -m train --help\npython -m test --help\n```\n\nTo reproduce the results from [table 1](https://arxiv.org/abs/2007.06346):\n#### W-MSE 4\n```\npython -m train --dataset cifar10 --epoch 1000 --lr 3e-3 --num_samples 4 --bs 256 --emb 64 --w_size 128\npython -m train --dataset cifar100 --epoch 1000 --lr 3e-3 --num_samples 4 --bs 256 --emb 64 --w_size 128\npython -m train --dataset stl10 --epoch 2000 --lr 2e-3 --num_samples 4 --bs 256 --emb 128 --w_size 256\npython -m train --dataset tiny_in --epoch 1000 --lr 2e-3 --num_samples 4 --bs 256 --emb 128 --w_size 256\n```\n\n#### W-MSE 2\n```\npython -m train --dataset cifar10 --epoch 1000 --lr 3e-3 --emb 64 --w_size 128\npython -m train --dataset cifar100 --epoch 1000 --lr 3e-3 --emb 64 --w_size 128\npython -m train --dataset stl10 --epoch 2000 --lr 2e-3 --emb 128 --w_size 256 --w_iter 4\npython -m train --dataset tiny_in --epoch 1000 --lr 2e-3 --emb 128 --w_size 256 --w_iter 4\n```\n\n#### Contrastive\n```\npython -m train --dataset cifar10 --epoch 1000 --lr 3e-3 --emb 64 --method contrastive\npython -m train --dataset cifar100 --epoch 1000 --lr 3e-3 --emb 64 --method contrastive\npython -m train --dataset stl10 --epoch 2000 --lr 2e-3 --emb 128 --method contrastive\npython -m train --dataset tiny_in --epoch 1000 --lr 2e-3 --emb 128 --method contrastive\n```\n\n#### BYOL\n```\npython -m train --dataset cifar10 --epoch 1000 --lr 3e-3 --emb 64 --method byol\npython -m train --dataset cifar100 --epoch 1000 --lr 3e-3 --emb 64 --method byol\npython -m train --dataset stl10 --epoch 2000 --lr 2e-3 --emb 128 --method byol\npython -m train --dataset tiny_in --epoch 1000 --lr 2e-3 --emb 128 --method byol\n```\n\n#### ImageNet-100\n```\npython -m train --dataset imagenet --epoch 240 --lr 2e-3 --emb 128 --w_size 256 --crop_s0 0.08 --cj0 0.8 --cj1 0.8 --cj2 0.8 --cj3 0.2 --gs_p 0.2\npython -m train --dataset imagenet --epoch 240 --lr 2e-3 --num_samples 4 --bs 256 --emb 128 --w_size 256 --crop_s0 0.08 --cj0 0.8 --cj1 0.8 --cj2 0.8 --cj3 0.2 --gs_p 0.2\n```\n\nUse `--no_norm` to disable normalization (for Euclidean distance).\n\n## Citation\n```\n@article{ermolov2020whitening,\n  title={Whitening for Self-Supervised Representation Learning",
      "https://arxiv.org/abs/2007.06346"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\n@article{ermolov2020whitening,\n  title={Whitening for Self-Supervised Representation Learning}, \n  author={Aleksandr Ermolov and Aliaksandr Siarohin and Enver Sangineto and Nicu Sebe},\n  journal={arXiv preprint arXiv:2007.06346},\n  year={2020}\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{ermolov2020whitening,\n  title={Whitening for Self-Supervised Representation Learning}, \n  author={Aleksandr Ermolov and Aliaksandr Siarohin and Enver Sangineto and Nicu Sebe},\n  journal={arXiv preprint arXiv:2007.06346},\n  year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9490753289412834,
        0.9490753289412834,
        0.9490753289412834
      ],
      "excerpt": "- W-MSE arXiv \n- Contrastive SimCLR arXiv \n- BYOL arXiv \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9392901210467604,
        0.9030859728368266
      ],
      "excerpt": "- CIFAR-10 and CIFAR-100 \n- STL-10 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/htdt/self-supervised",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-01-31T17:23:31Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-07T10:20:30Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9676912386234988
      ],
      "excerpt": "Official repository of the paper Whitening for Self-Supervised Representation Learning \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.910799136109525
      ],
      "excerpt": "It includes 3 types of losses: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8415591833446245
      ],
      "excerpt": "And 5 datasets: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9227390896906613
      ],
      "excerpt": "The implementation is optimized for a single GPU, although multiple are also supported. It includes fast evaluation: we pre-compute embeddings for the entire dataset and then train a classifier on top. The evaluation of the ResNet-18 encoder takes about one minute. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Whitening for Self-Supervised Representation Learning | Official repository",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/htdt/self-supervised/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 14,
      "date": "Mon, 27 Dec 2021 19:52:30 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/htdt/self-supervised/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "htdt/self-supervised",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/htdt/self-supervised/master/docker/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The implementation is based on PyTorch. Logging works on [wandb.ai](https://wandb.ai/). See `docker/Dockerfile`.\n\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8331028142694584
      ],
      "excerpt": "Checkpoints are stored in data each 100 epochs during training. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9181665525024284,
        0.9096447229895428,
        0.8763715792781845,
        0.8763715792781845,
        0.9171972355961544,
        0.9091830889225923,
        0.8976923287641212,
        0.8976923287641212,
        0.8437365989225407,
        0.8203843048064217,
        0.8499326987778074,
        0.8499326987778074,
        0.8437365989225407,
        0.8203843048064217,
        0.8499326987778074,
        0.8499326987778074,
        0.8970498394537235,
        0.8791476546246391
      ],
      "excerpt": "python -m train --dataset cifar10 --epoch 1000 --lr 3e-3 --num_samples 4 --bs 256 --emb 64 --w_size 128 \npython -m train --dataset cifar100 --epoch 1000 --lr 3e-3 --num_samples 4 --bs 256 --emb 64 --w_size 128 \npython -m train --dataset stl10 --epoch 2000 --lr 2e-3 --num_samples 4 --bs 256 --emb 128 --w_size 256 \npython -m train --dataset tiny_in --epoch 1000 --lr 2e-3 --num_samples 4 --bs 256 --emb 128 --w_size 256 \npython -m train --dataset cifar10 --epoch 1000 --lr 3e-3 --emb 64 --w_size 128 \npython -m train --dataset cifar100 --epoch 1000 --lr 3e-3 --emb 64 --w_size 128 \npython -m train --dataset stl10 --epoch 2000 --lr 2e-3 --emb 128 --w_size 256 --w_iter 4 \npython -m train --dataset tiny_in --epoch 1000 --lr 2e-3 --emb 128 --w_size 256 --w_iter 4 \npython -m train --dataset cifar10 --epoch 1000 --lr 3e-3 --emb 64 --method contrastive \npython -m train --dataset cifar100 --epoch 1000 --lr 3e-3 --emb 64 --method contrastive \npython -m train --dataset stl10 --epoch 2000 --lr 2e-3 --emb 128 --method contrastive \npython -m train --dataset tiny_in --epoch 1000 --lr 2e-3 --emb 128 --method contrastive \npython -m train --dataset cifar10 --epoch 1000 --lr 3e-3 --emb 64 --method byol \npython -m train --dataset cifar100 --epoch 1000 --lr 3e-3 --emb 64 --method byol \npython -m train --dataset stl10 --epoch 2000 --lr 2e-3 --emb 128 --method byol \npython -m train --dataset tiny_in --epoch 1000 --lr 2e-3 --emb 128 --method byol \npython -m train --dataset imagenet --epoch 240 --lr 2e-3 --emb 128 --w_size 256 --crop_s0 0.08 --cj0 0.8 --cj1 0.8 --cj2 0.8 --cj3 0.2 --gs_p 0.2 \npython -m train --dataset imagenet --epoch 240 --lr 2e-3 --num_samples 4 --bs 256 --emb 128 --w_size 256 --crop_s0 0.08 --cj0 0.8 --cj1 0.8 --cj2 0.8 --cj3 0.2 --gs_p 0.2 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/htdt/self-supervised/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Dockerfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Self-Supervised Representation Learning",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "self-supervised",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "htdt",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/htdt/self-supervised/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 79,
      "date": "Mon, 27 Dec 2021 19:52:30 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "self-supervised-learning",
      "representation-learning"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Detailed settings are good by default, to see all options:\n```\npython -m train --help\npython -m test --help\n```\n\nTo reproduce the results from [table 1](https://arxiv.org/abs/2007.06346):\n",
      "technique": "Header extraction"
    }
  ]
}