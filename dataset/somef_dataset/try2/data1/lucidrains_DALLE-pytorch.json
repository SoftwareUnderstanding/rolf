{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2102.12092\">paper</a>",
      "https://arxiv.org/abs/2001.04451\n"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```bibtex\n@misc{ramesh2021zeroshot,\n    title   = {Zero-Shot Text-to-Image Generation}, \n    author  = {Aditya Ramesh and Mikhail Pavlov and Gabriel Goh and Scott Gray and Chelsea Voss and Alec Radford and Mark Chen and Ilya Sutskever},\n    year    = {2021},\n    eprint  = {2102.12092},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n```bibtex\n@misc{unpublished2021clip,\n    title  = {CLIP: Connecting Text and Images},\n    author = {Alec Radford, Ilya Sutskever, Jong Wook Kim, Gretchen Krueger, Sandhini Agarwal},\n    year   = {2021}\n}\n```\n\n```bibtex\n@misc{kitaev2020reformer,\n    title   = {Reformer: The Efficient Transformer},\n    author  = {Nikita Kitaev and \u0141ukasz Kaiser and Anselm Levskaya},\n    year    = {2020},\n    eprint  = {2001.04451},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG}\n}\n```\n\n```bibtex\n@misc{esser2021taming,\n    title   = {Taming Transformers for High-Resolution Image Synthesis},\n    author  = {Patrick Esser and Robin Rombach and Bj\u00f6rn Ommer},\n    year    = {2021},\n    eprint  = {2012.09841},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n```bibtex\n@misc{ding2021cogview,\n    title   = {CogView: Mastering Text-to-Image Generation via Transformers},\n    author  = {Ming Ding and Zhuoyi Yang and Wenyi Hong and Wendi Zheng and Chang Zhou and Da Yin and Junyang Lin and Xu Zou and Zhou Shao and Hongxia Yang and Jie Tang},\n    year    = {2021},\n    eprint  = {2105.13290},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n```bibtex\n@software{peng_bo_2021_5196578,\n    author       = {PENG Bo},\n    title        = {BlinkDL/RWKV-LM: 0.01},\n    month        = {aug},\n    year         = {2021},\n    publisher    = {Zenodo},\n    version      = {0.01},\n    doi          = {10.5281/zenodo.5196578},\n    url          = {https://doi.org/10.5281/zenodo.5196578}\n}\n```\n\n```bibtex\n@misc{su2021roformer,\n    title   = {RoFormer: Enhanced Transformer with Rotary Position Embedding},\n    author  = {Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu},\n    year    = {2021},\n    eprint  = {2104.09864},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}\n```\n\n*Those who do not want to imitate anything, produce nothing.* - Dali\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{su2021roformer,\n    title   = {RoFormer: Enhanced Transformer with Rotary Position Embedding},\n    author  = {Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu},\n    year    = {2021},\n    eprint  = {2104.09864},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@software{peng_bo_2021_5196578,\n    author       = {PENG Bo},\n    title        = {BlinkDL/RWKV-LM: 0.01},\n    month        = {aug},\n    year         = {2021},\n    publisher    = {Zenodo},\n    version      = {0.01},\n    doi          = {10.5281/zenodo.5196578},\n    url          = {https://doi.org/10.5281/zenodo.5196578}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{ding2021cogview,\n    title   = {CogView: Mastering Text-to-Image Generation via Transformers},\n    author  = {Ming Ding and Zhuoyi Yang and Wenyi Hong and Wendi Zheng and Chang Zhou and Da Yin and Junyang Lin and Xu Zou and Zhou Shao and Hongxia Yang and Jie Tang},\n    year    = {2021},\n    eprint  = {2105.13290},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{esser2021taming,\n    title   = {Taming Transformers for High-Resolution Image Synthesis},\n    author  = {Patrick Esser and Robin Rombach and Bj\u00f6rn Ommer},\n    year    = {2021},\n    eprint  = {2012.09841},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{kitaev2020reformer,\n    title   = {Reformer: The Efficient Transformer},\n    author  = {Nikita Kitaev and \u0141ukasz Kaiser and Anselm Levskaya},\n    year    = {2020},\n    eprint  = {2001.04451},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{unpublished2021clip,\n    title  = {CLIP: Connecting Text and Images},\n    author = {Alec Radford, Ilya Sutskever, Jong Wook Kim, Gretchen Krueger, Sandhini Agarwal},\n    year   = {2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{ramesh2021zeroshot,\n    title   = {Zero-Shot Text-to-Image Generation}, \n    author  = {Aditya Ramesh and Mikhail Pavlov and Gabriel Goh and Scott Gray and Chelsea Voss and Alec Radford and Mark Chen and Ilya Sutskever},\n    year    = {2021},\n    eprint  = {2102.12092},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8537951306561473,
        0.8618121291573836
      ],
      "excerpt": "  <a href=\"https://colab.research.google.com/gist/afiaka87/b29213684a1dd633df20cab49d05209d/train_dalle_pytorch.ipynb\"> \n         <img alt=\"Train DALL-E w/ DeepSpeed\" src=\"https://colab.research.google.com/assets/colab-badge.svg\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9318073560667521
      ],
      "excerpt": "  <a href=\"https://discord.gg/xBPBXfcFHd\"><img alt=\"Join us on Discord\" src=\"https://img.shields.io/discord/823813159592001537?color=5865F2&logo=discord&logoColor=white\"></a></br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8029948720864918,
        0.9279628677675321
      ],
      "excerpt": "  <a href=\"https://github.com/rom1504/dalle-service\">Web-Hostable DALLE Checkpoints</a></br> \n  <a href=\"https://www.youtube.com/watch?v=j4xgkjWlfL4\">Yannic Kilcher's video</a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9195380841997004
      ],
      "excerpt": "This library could not have been possible without the contributions of <a href=\"https://github.com/janEbert\">janEbert</a>, <a href=\"https://github.com/afiaka87\">Clay</a>, <a href=\"https://github.com/robvanvolt\">robvanvolt</a>, and <a href=\"https://github.com/rom1504\">Romaine</a>! \ud83d\ude4f \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9126472096896313
      ],
      "excerpt": ": do your topk here, in paper they sampled 512 and chose top 32 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9958355183014763
      ],
      "excerpt": "    reversible = True  #: &lt;-- reversible networks https://arxiv.org/abs/2001.04451 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lucidrains/DALLE-pytorch",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-01-05T20:35:16Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-24T03:14:10Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9388293473786885
      ],
      "excerpt": "You can also skip the training of the VAE altogether, using the pretrained model released by OpenAI! The wrapper class should take care of downloading and caching the model for you auto-magically. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.945970850543856,
        0.9831056092204365
      ],
      "excerpt": "You can also use the pretrained VAE offered by the authors of <a href=\"https://github.com/CompVis/taming-transformers\">Taming Transformers</a>! Currently only the VAE with a codebook size of 1024 is offered, with the hope that it may train a little faster than OpenAI's, which has a size of 8192. \nIn contrast to OpenAI's VAE, it also has an extra layer of downsampling, so the image sequence length is 256 instead of 1024 (this will lead to a 16 reduction in training costs, when you do the math). Whether it will generalize as well as the original DALL-E is up to the citizen scientists out there to discover. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.978417717699621
      ],
      "excerpt": "In the blog post, they used 64 layers to achieve their results. I added reversible networks, from the <a href=\"https://github.com/lucidrains/reformer-pytorch\">Reformer</a> paper, in order for users to attempt to scale depth at the cost of compute. Reversible networks allow you to scale to any depth at no memory cost, but a little over 2x compute cost (each layer is rerun on the backward pass). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9845799166584046
      ],
      "excerpt": "The blogpost alluded to a mixture of different types of sparse attention, used mainly on the image (while the text presumably had full causal attention). I have done my best to replicate these types of sparse attention, on the scant details released. Primarily, it seems as though they are doing causal axial row / column attention, combined with a causal convolution-like attention. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8395468787915461,
        0.8395468787915461,
        0.8212735411460657,
        0.9433321929105853
      ],
      "excerpt": "axial_row axial attention, along the rows of the image feature map \naxial_col axial attention, along the columns of the image feature map \nconv_like convolution-like attention, for the image feature map \nThe sparse attention only applies to the image. Text will always receive full attention, as said in the blogpost. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9344007631495299
      ],
      "excerpt": "    attn_types = ('full', 'axial_row', 'axial_col', 'conv_like')  #: cycles between these four types of attention \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8006651489540569
      ],
      "excerpt": "You can of course open up the training script at ./train_vae.py, where you can modify the constants, what is passed to Weights & Biases, or any other tricks you know to make the VAE learn better. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8749485481399795
      ],
      "excerpt": "Weights and Biases will allow you to monitor the temperature annealing, image reconstructions (encoder and decoder working properly), as well as to watch out for codebook collapse (where the network decides to only use a few tokens out of what you provide it). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8044020626971446,
        0.9868252689569403
      ],
      "excerpt": "A black and white cat curled up next to the fireplace \nA fireplace, with a cat sleeping next to it \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.938795168404481
      ],
      "excerpt": "column key after the --wds argument. The ---image_text_folder points to your .tar(.gz) file instead of the datafolder. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8152298642354604
      ],
      "excerpt": "To generate multiple images, just pass in your text with '|' character as a separator. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9629015634745284
      ],
      "excerpt": "Note that DALL-E is a full image+text language model. As a consequence you can also generate text using a dalle model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8159077151529762,
        0.9659421328099079
      ],
      "excerpt": "for each one. See the DeepSpeed configuration \ndocs for more \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9366005328476021
      ],
      "excerpt": "Automatic mixed precision is a stable alternative to fp16 which still provides a decent speedup. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8876136289487182,
        0.8448891934992762
      ],
      "excerpt": "This repository supports custom tokenization with <a href=\"https://github.com/VKCOM/YouTokenToMe\">YouTokenToMe</a>, if you wish to use it instead of the default simple tokenizer. Simply pass in an extra --bpe_path when invoking train_dalle.py and generate.py, with the path to your BPE model file. \nThe only requirement is that you use 0 as the padding during tokenization \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Implementation / replication of DALL-E, OpenAI's Text to Image Transformer, in Pytorch",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lucidrains/DALLE-pytorch/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 374,
      "date": "Fri, 24 Dec 2021 06:33:21 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/lucidrains/DALLE-pytorch/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "lucidrains/DALLE-pytorch",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/lucidrains/DALLE-pytorch/main/docker/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/lucidrains/DALLE-pytorch/main/examples/rainbow_dalle.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/lucidrains/DALLE-pytorch/main/install_apex.sh",
      "https://raw.githubusercontent.com/lucidrains/DALLE-pytorch/main/install_deepspeed.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```bash\n$ pip install dalle-pytorch\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9117070200704508
      ],
      "excerpt": "[Quick Start](https://github.com/lucidrains/DALLE-pytorch/wiki) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8841082029669962,
        0.8030141784853604
      ],
      "excerpt": "- (3/15/21) <a href=\"https://github.com/afiaka87\">afiaka87</a> has managed one epoch using a reversible DALL-E and the dVaE <a href=\"https://github.com/lucidrains/DALLE-pytorch/issues/86#issue-832121328\">here</a> \n- <a href=\"https://github.com/robvanvolt\">TheodoreGalanos</a> has trained on 150k layouts with the following results \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9038937349751167
      ],
      "excerpt": "Update - <a href=\"https://github.com/lucidrains/DALLE-pytorch/discussions/131\">it works!</a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.839826065199372
      ],
      "excerpt": "The default VQGan is the codebook size 1024 one trained on imagenet. If you wish to use a different one, you can use the vqgan_model_path and vqgan_config_path to pass the .ckpt file and the .yaml file. These options can be used both in train-dalle script or as argument of VQGanVAE class. Other pretrained VQGAN can be found in taming transformers readme. If you want to train a custom one you can follow this guide \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8128685981857151
      ],
      "excerpt": "To get the similarity scores from your trained Clipper, just do \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.893160939132218,
        0.9023697225149864,
        0.9728961943251753,
        0.999746712887969,
        0.8927630469455259
      ],
      "excerpt": "First, you need to install Deepspeed with Sparse Attention \n$ sh install_deepspeed.sh \nNext, you need to install the pip package triton. It will need to be a version &lt; 1.0 because that's what Microsoft used. \n$ pip install triton==0.4.2 \nIf both of the above succeeded, now you can train with Sparse Attention! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.999746712887969
      ],
      "excerpt": "$ pip install wandb \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8894756355925916
      ],
      "excerpt": "To train the VAE, you just need to run \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9106498385544827
      ],
      "excerpt": "If you installed everything correctly, a link to the experiments page should show up in your terminal. You can follow your link there and customize your experiment, like the example layout below. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8048786874864003
      ],
      "excerpt": "(https://www.youtube.com/watch?v=v_PacO-3OGQ here are given 4 examples, or a little helper script which also supports splitting your dataset \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8497286135169608
      ],
      "excerpt": "$ python train_dalle.py --image_text_folder /path/to/coco/dataset \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8497286135169608
      ],
      "excerpt": "$ python train_dalle.py --image_text_folder /path/to/coco/dataset --taming \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8722359482542814
      ],
      "excerpt": "To build: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8255742485198816
      ],
      "excerpt": "Thanks to <a href=\"https://github.com/janEbert\">janEbert</a>, the repository is now equipped so you can train DALL-E with Microsoft's <a href=\"https://www.deepspeed.ai/\">Deepspeed</a>! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.804524160972973
      ],
      "excerpt": "for each one. See the DeepSpeed configuration \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.802537351905227
      ],
      "excerpt": "As of DeepSpeed version 0.3.16, ZeRO optimizations can be used with \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8503599412694465
      ],
      "excerpt": "version, you'll have to pass the --fp16 flag to be able to enable \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9515126333991091,
        0.9941611694592876
      ],
      "excerpt": "In order to run with Apex AMP (through DeepSpeed), you will need to install DeepSpeed using either the Dockerfile or the bash script. \nThen you will need to install apex from source.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9023697225149864
      ],
      "excerpt": "sh install_apex.sh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.999746712887969
      ],
      "excerpt": "$ pip install youtokentome \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8536753758407865
      ],
      "excerpt": "<img src=\"./images/landscape.png\"></img> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8627594245369156
      ],
      "excerpt": "<img src=\"./images/birds.png\" width=\"256\"></img> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8873152197607692,
        0.8873152197607692
      ],
      "excerpt": "  <img src=\"./images/layouts-1.jpg\" width=\"256\"></img> \n  <img src=\"./images/layouts-2.jpg\" width=\"256\"></img> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8671904599658984
      ],
      "excerpt": "<img src=\"./images/clothing.png\" width=\"420\"></img> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "from dalle_pytorch import OpenAIDiscreteVAE, DALLE \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8180979537954312
      ],
      "excerpt": "    num_text_tokens = 10000,    #: vocab size for text \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8875940045760212
      ],
      "excerpt": "loss = dalle(text, images, mask = mask, return_loss = True) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "from dalle_pytorch import VQGanVAE \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": "Train CLIP \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "from dalle_pytorch import CLIP \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8995853271734886
      ],
      "excerpt": "loss = clip(text, images, text_mask = mask, return_loss = True) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.821782239690004
      ],
      "excerpt": "images, scores = dalle.generate_images(text, mask = mask, clip = clip) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8329937481166215
      ],
      "excerpt": "    depth = 64, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8329937481166215
      ],
      "excerpt": "    depth = 64, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "    reversible = True, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8329937481166215
      ],
      "excerpt": "    depth = 64, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.818171695265806
      ],
      "excerpt": " Train in Colab \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8755822258377478
      ],
      "excerpt": "$ python train_vae.py --image_folder /path/to/your/images \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8671904599658984
      ],
      "excerpt": "<img src=\"./images/wb.png\" width=\"700px\"></img> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8182183459539593,
        0.8670206413668038
      ],
      "excerpt": "Now you just have to invoke the ./train_dalle.py script, indicating which VAE model you would like to use, as well as the path to your folder if images and text. \nThe dataset I am currently working with contains a folder of images and text files, arbitraily nested in subfolders, where text file name corresponds with the image name, and where each text file contains multiple descriptions, delimited by newlines. The script will find and pair all the image and text files with the same names, and randomly select one of the textual descriptions during batch creation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639986685036579
      ],
      "excerpt": " \u2523 \ud83d\udcdcdog.jpg \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9024377697052569
      ],
      "excerpt": "$ python train_dalle.py --vae_path ./vae.pt --image_text_folder /path/to/data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9024377697052569
      ],
      "excerpt": "$ python train_dalle.py --dalle_path ./dalle.pt --image_text_folder /path/to/data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8075638663213955
      ],
      "excerpt": "$ python train_dalle.py --wds img,cap --image_text_folder /path/to/data.tar(.gz) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8383399689090096
      ],
      "excerpt": "$ deepspeed train_dalle.py --wds img,cap --image_text_folder /path/to/shardfolder --fp16 --deepspeed \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8797176904315078
      ],
      "excerpt": "$ deepspeed train_dalle.py --image_text_folder \"http://storage.googleapis.com/nvdata-openimages/openimages-train-{000000..000554}.tar\" --wds jpg,json --taming --truncate_captions --random_resize_crop_lower_ratio=0.8 --attn_types=full --epochs=2 --fp16 --deepspeed \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8124393234403227
      ],
      "excerpt": "$ python train_dalle.py --image_text_folder /path/to/coco/dataset \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8124393234403227
      ],
      "excerpt": "$ python train_dalle.py --image_text_folder /path/to/coco/dataset --taming \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8511443332695361
      ],
      "excerpt": "$ python generate.py --dalle_path ./dalle.pt --text 'fireflies in a field under a full moon' \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8089985062892021
      ],
      "excerpt": "To generate multiple images, just pass in your text with '|' character as a separator. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8924934134089838
      ],
      "excerpt": "$ python generate.py --dalle_path ./dalle.pt --text 'a dog chewing a bone|a cat chasing mice|a frog eating a fly' \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9401176859360765,
        0.8085005129550544
      ],
      "excerpt": "$ python generate.py --dalle_path ./dalle.pt --text 'a dog chewing a bone' --gentext \nThis will complete the provided text, save it in a caption.txt and generate the corresponding images. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8645561602142441
      ],
      "excerpt": "docker run --gpus all -it --mount src=\"$(pwd)\",target=/workspace/dalle,type=bind dalle:latest bash \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8286120695811436
      ],
      "excerpt": "Modify the deepspeed_config dictionary in train_dalle.py or \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.918834725396504
      ],
      "excerpt": "Now, run train_dalle.py with deepspeed instead of python as done here: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991
      ],
      "excerpt": "deepspeed train_dalle.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8597606641010321
      ],
      "excerpt": "experiments. This will multiply your effective batch size per training \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8615054810856908,
        0.8228700413403821
      ],
      "excerpt": "$ python train_dalle.py --image_text_folder ./path/to/data --bpe_path ./path/to/bpe.model \nTo create a BPE model file from scratch, firstly \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8916702812589763
      ],
      "excerpt": "$ yttm bpe --vocab_size 8000 --data ./path/to/big/text/file.txt --model ./path/to/bpe.model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9024377697052569
      ],
      "excerpt": "$ python train_dalle.py --chinese --image_text_folder ./path/to/data \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/lucidrains/DALLE-pytorch/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Dockerfile",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2021 Phil Wang\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "DALL-E in Pytorch",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "DALLE-pytorch",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "lucidrains",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lucidrains/DALLE-pytorch/blob/main/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-12-01T20:33:50Z",
        "datePublished": "2021-12-01T20:35:26Z",
        "html_url": "https://github.com/lucidrains/DALLE-pytorch/releases/tag/1.1.5",
        "name": "1.1.5",
        "tag_name": "1.1.5",
        "tarball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/tarball/1.1.5",
        "url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/releases/54443620",
        "zipball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/zipball/1.1.5"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-11-12T06:01:26Z",
        "datePublished": "2021-11-12T06:01:47Z",
        "html_url": "https://github.com/lucidrains/DALLE-pytorch/releases/tag/1.1.4",
        "name": "1.1.4",
        "tag_name": "1.1.4",
        "tarball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/tarball/1.1.4",
        "url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/releases/53218706",
        "zipball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/zipball/1.1.4"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-11-09T01:21:18Z",
        "datePublished": "2021-11-09T01:21:47Z",
        "html_url": "https://github.com/lucidrains/DALLE-pytorch/releases/tag/1.1.2",
        "name": "1.1.2",
        "tag_name": "1.1.2",
        "tarball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/tarball/1.1.2",
        "url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/releases/52966719",
        "zipball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/zipball/1.1.2"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-11-06T19:16:53Z",
        "datePublished": "2021-11-06T19:17:07Z",
        "html_url": "https://github.com/lucidrains/DALLE-pytorch/releases/tag/1.1.1",
        "name": "1.1.1",
        "tag_name": "1.1.1",
        "tarball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/tarball/1.1.1",
        "url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/releases/52852026",
        "zipball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/zipball/1.1.1"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-10-19T20:51:03Z",
        "datePublished": "2021-10-19T20:51:20Z",
        "html_url": "https://github.com/lucidrains/DALLE-pytorch/releases/tag/1.1.0",
        "name": "1.1.0",
        "tag_name": "1.1.0",
        "tarball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/tarball/1.1.0",
        "url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/releases/51653205",
        "zipball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/zipball/1.1.0"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-09-29T22:33:59Z",
        "datePublished": "2021-09-29T22:34:18Z",
        "html_url": "https://github.com/lucidrains/DALLE-pytorch/releases/tag/1.0.8",
        "name": "1.0.8",
        "tag_name": "1.0.8",
        "tarball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/tarball/1.0.8",
        "url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/releases/50510303",
        "zipball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/zipball/1.0.8"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-09-01T23:55:44Z",
        "datePublished": "2021-09-01T23:56:02Z",
        "html_url": "https://github.com/lucidrains/DALLE-pytorch/releases/tag/1.0.7",
        "name": "1.0.7",
        "tag_name": "1.0.7",
        "tarball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/tarball/1.0.7",
        "url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/releases/48854229",
        "zipball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/zipball/1.0.7"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-08-23T21:10:19Z",
        "datePublished": "2021-08-23T21:10:38Z",
        "html_url": "https://github.com/lucidrains/DALLE-pytorch/releases/tag/1.0.6",
        "name": "1.0.6",
        "tag_name": "1.0.6",
        "tarball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/tarball/1.0.6",
        "url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/releases/48310168",
        "zipball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/zipball/1.0.6"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-08-17T15:28:17Z",
        "datePublished": "2021-08-17T15:28:36Z",
        "html_url": "https://github.com/lucidrains/DALLE-pytorch/releases/tag/1.0.5",
        "name": "1.0.5",
        "tag_name": "1.0.5",
        "tarball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/tarball/1.0.5",
        "url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/releases/47986054",
        "zipball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/zipball/1.0.5"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-08-17T14:33:19Z",
        "datePublished": "2021-08-17T15:21:04Z",
        "html_url": "https://github.com/lucidrains/DALLE-pytorch/releases/tag/1.0.4",
        "name": "1.0.4",
        "tag_name": "1.0.4",
        "tarball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/tarball/1.0.4",
        "url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/releases/47985522",
        "zipball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/zipball/1.0.4"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-08-16T23:59:28Z",
        "datePublished": "2021-08-17T00:00:34Z",
        "html_url": "https://github.com/lucidrains/DALLE-pytorch/releases/tag/1.0.3",
        "name": "1.0.3",
        "tag_name": "1.0.3",
        "tarball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/tarball/1.0.3",
        "url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/releases/47941951",
        "zipball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/zipball/1.0.3"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-08-16T18:48:59Z",
        "datePublished": "2021-08-16T18:49:16Z",
        "html_url": "https://github.com/lucidrains/DALLE-pytorch/releases/tag/1.0.2",
        "name": "1.0.2",
        "tag_name": "1.0.2",
        "tarball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/tarball/1.0.2",
        "url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/releases/47928873",
        "zipball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/zipball/1.0.2"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-08-16T15:59:14Z",
        "datePublished": "2021-08-16T16:00:03Z",
        "html_url": "https://github.com/lucidrains/DALLE-pytorch/releases/tag/1.0.1",
        "name": "1.0.1",
        "tag_name": "1.0.1",
        "tarball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/tarball/1.0.1",
        "url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/releases/47920052",
        "zipball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/zipball/1.0.1"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-08-16T02:43:52Z",
        "datePublished": "2021-08-16T02:44:37Z",
        "html_url": "https://github.com/lucidrains/DALLE-pytorch/releases/tag/1.0.0",
        "name": "1.0.0",
        "tag_name": "1.0.0",
        "tarball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/tarball/1.0.0",
        "url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/releases/47882602",
        "zipball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/zipball/1.0.0"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-07-15T02:24:44Z",
        "datePublished": "2021-07-15T02:25:04Z",
        "html_url": "https://github.com/lucidrains/DALLE-pytorch/releases/tag/0.14.3",
        "name": "",
        "tag_name": "0.14.3",
        "tarball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/tarball/0.14.3",
        "url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/releases/46224857",
        "zipball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/zipball/0.14.3"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-07-08T18:58:03Z",
        "datePublished": "2021-07-08T18:58:13Z",
        "html_url": "https://github.com/lucidrains/DALLE-pytorch/releases/tag/0.14.2",
        "name": "0.14.2",
        "tag_name": "0.14.2",
        "tarball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/tarball/0.14.2",
        "url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/releases/45926336",
        "zipball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/zipball/0.14.2"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-07-01T15:47:24Z",
        "datePublished": "2021-07-01T15:47:35Z",
        "html_url": "https://github.com/lucidrains/DALLE-pytorch/releases/tag/0.14.1",
        "name": "0.14.1",
        "tag_name": "0.14.1",
        "tarball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/tarball/0.14.1",
        "url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/releases/45571224",
        "zipball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/zipball/0.14.1"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-06-30T17:15:52Z",
        "datePublished": "2021-06-30T17:16:03Z",
        "html_url": "https://github.com/lucidrains/DALLE-pytorch/releases/tag/0.14.0",
        "name": "0.14.0",
        "tag_name": "0.14.0",
        "tarball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/tarball/0.14.0",
        "url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/releases/45507519",
        "zipball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/zipball/0.14.0"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-06-29T02:10:45Z",
        "datePublished": "2021-06-29T02:10:59Z",
        "html_url": "https://github.com/lucidrains/DALLE-pytorch/releases/tag/0.13.4",
        "name": "0.13.4",
        "tag_name": "0.13.4",
        "tarball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/tarball/0.13.4",
        "url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/releases/45393759",
        "zipball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/zipball/0.13.4"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-06-23T22:57:50Z",
        "datePublished": "2021-06-23T22:58:06Z",
        "html_url": "https://github.com/lucidrains/DALLE-pytorch/releases/tag/0.13.2",
        "name": "0.13.2",
        "tag_name": "0.13.2",
        "tarball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/tarball/0.13.2",
        "url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/releases/45135145",
        "zipball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/zipball/0.13.2"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-06-20T16:14:58Z",
        "datePublished": "2021-06-20T16:15:08Z",
        "html_url": "https://github.com/lucidrains/DALLE-pytorch/releases/tag/0.13.1",
        "name": "0.13.1",
        "tag_name": "0.13.1",
        "tarball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/tarball/0.13.1",
        "url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/releases/44925007",
        "zipball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/zipball/0.13.1"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-06-16T22:13:04Z",
        "datePublished": "2021-06-16T22:13:41Z",
        "html_url": "https://github.com/lucidrains/DALLE-pytorch/releases/tag/0.13.0",
        "name": "",
        "tag_name": "0.13.0",
        "tarball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/tarball/0.13.0",
        "url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/releases/44758597",
        "zipball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/zipball/0.13.0"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-05-28T23:18:46Z",
        "datePublished": "2021-05-28T23:19:00Z",
        "html_url": "https://github.com/lucidrains/DALLE-pytorch/releases/tag/0.12.5",
        "name": "0.12.5",
        "tag_name": "0.12.5",
        "tarball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/tarball/0.12.5",
        "url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/releases/43792653",
        "zipball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/zipball/0.12.5"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-05-28T21:56:23Z",
        "datePublished": "2021-05-28T21:56:47Z",
        "html_url": "https://github.com/lucidrains/DALLE-pytorch/releases/tag/0.12.4",
        "name": "0.12.4",
        "tag_name": "0.12.4",
        "tarball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/tarball/0.12.4",
        "url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/releases/43790766",
        "zipball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/zipball/0.12.4"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-05-28T20:13:46Z",
        "datePublished": "2021-05-28T20:14:01Z",
        "html_url": "https://github.com/lucidrains/DALLE-pytorch/releases/tag/0.12.3",
        "name": "0.12.3",
        "tag_name": "0.12.3",
        "tarball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/tarball/0.12.3",
        "url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/releases/43787265",
        "zipball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/zipball/0.12.3"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-05-28T19:54:36Z",
        "datePublished": "2021-05-28T19:54:54Z",
        "html_url": "https://github.com/lucidrains/DALLE-pytorch/releases/tag/0.12.2",
        "name": "0.12.2",
        "tag_name": "0.12.2",
        "tarball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/tarball/0.12.2",
        "url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/releases/43786519",
        "zipball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/zipball/0.12.2"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-05-28T19:50:55Z",
        "datePublished": "2021-05-28T19:51:14Z",
        "html_url": "https://github.com/lucidrains/DALLE-pytorch/releases/tag/0.12.1",
        "name": "0.12.1",
        "tag_name": "0.12.1",
        "tarball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/tarball/0.12.1",
        "url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/releases/43786400",
        "zipball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/zipball/0.12.1"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-05-19T21:30:15Z",
        "datePublished": "2021-05-19T21:30:31Z",
        "html_url": "https://github.com/lucidrains/DALLE-pytorch/releases/tag/0.12.0",
        "name": "0.12.0",
        "tag_name": "0.12.0",
        "tarball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/tarball/0.12.0",
        "url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/releases/43248527",
        "zipball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/zipball/0.12.0"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-05-04T22:28:41Z",
        "datePublished": "2021-05-04T22:28:54Z",
        "html_url": "https://github.com/lucidrains/DALLE-pytorch/releases/tag/0.11.3",
        "name": "0.11.3",
        "tag_name": "0.11.3",
        "tarball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/tarball/0.11.3",
        "url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/releases/42444640",
        "zipball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/zipball/0.11.3"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-05-03T20:57:04Z",
        "datePublished": "2021-05-03T20:57:14Z",
        "html_url": "https://github.com/lucidrains/DALLE-pytorch/releases/tag/0.11.2",
        "name": "0.11.2",
        "tag_name": "0.11.2",
        "tarball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/tarball/0.11.2",
        "url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/releases/42381890",
        "zipball_url": "https://api.github.com/repos/lucidrains/DALLE-pytorch/zipball/0.11.2"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3675,
      "date": "Fri, 24 Dec 2021 06:33:21 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "artificial-intelligence",
      "deep-learning",
      "attention-mechanism",
      "text-to-image",
      "transformers",
      "multi-modal"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Train VAE\n\n```python\nimport torch\nfrom dalle_pytorch import DiscreteVAE\n\nvae = DiscreteVAE(\n    image_size = 256,\n    num_layers = 3,           #: number of downsamples - ex. 256 / (2 ** 3) = (32 x 32 feature map)\n    num_tokens = 8192,        #: number of visual tokens. in the paper, they used 8192, but could be smaller for downsized projects\n    codebook_dim = 512,       #: codebook dimension\n    hidden_dim = 64,          #: hidden dimension\n    num_resnet_blocks = 1,    #: number of resnet blocks\n    temperature = 0.9,        #: gumbel softmax temperature, the lower this is, the harder the discretization\n    straight_through = False, #: straight-through for gumbel softmax. unclear if it is better one way or the other\n)\n\nimages = torch.randn(4, 3, 256, 256)\n\nloss = vae(images, return_loss = True)\nloss.backward()\n\n#: train with a lot of data to learn a good codebook\n```\n\nTrain DALL-E with pretrained VAE from above\n\n```python\nimport torch\nfrom dalle_pytorch import DiscreteVAE, DALLE\n\nvae = DiscreteVAE(\n    image_size = 256,\n    num_layers = 3,\n    num_tokens = 8192,\n    codebook_dim = 1024,\n    hidden_dim = 64,\n    num_resnet_blocks = 1,\n    temperature = 0.9\n)\n\ndalle = DALLE(\n    dim = 1024,\n    vae = vae,                  #: automatically infer (1) image sequence length and (2) number of image tokens\n    num_text_tokens = 10000,    #: vocab size for text\n    text_seq_len = 256,         #: text sequence length\n    depth = 12,                 #: should aim to be 64\n    heads = 16,                 #: attention heads\n    dim_head = 64,              #: attention head dimension\n    attn_dropout = 0.1,         #: attention dropout\n    ff_dropout = 0.1            #: feedforward dropout\n)\n\ntext = torch.randint(0, 10000, (4, 256))\nimages = torch.randn(4, 3, 256, 256)\nmask = torch.ones_like(text).bool()\n\nloss = dalle(text, images, mask = mask, return_loss = True)\nloss.backward()\n\n#: do the above for a long time with a lot of data ... then\n\nimages = dalle.generate_images(text, mask = mask)\nimages.shape #: (4, 3, 256, 256)\n```\n\nTo prime with a starting crop of an image, simply pass two more arguments\n\n```python\nimg_prime = torch.randn(4, 3, 256, 256)\n\nimages = dalle.generate_images(\n    text,\n    mask = mask,\n    img = img_prime,\n    num_init_img_tokens = (14 * 32)  #: you can set the size of the initial crop, defaults to a little less than ~1/2 of the tokens, as done in the paper\n)\n\nimages.shape #: (4, 3, 256, 256)\n```\n\nYou may also want to generate text using DALL-E. For that call this function:\n\n```python\ntext_tokens, texts = dalle.generate_texts(tokenizer, text)\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}