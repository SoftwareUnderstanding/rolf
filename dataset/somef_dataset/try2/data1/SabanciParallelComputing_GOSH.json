{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1803.04742",
      "https://arxiv.org/abs/1803.04742",
      "https://arxiv.org/abs/1803.04742",
      "https://arxiv.org/abs/1803.04742"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{10.1145/3404397.3404456,\n    author = {Akyildiz, Taha Atahan and Aljundi, Amro Alabsi and Kaya, Kamer},\n    title = {GOSH: Embedding Big Graphs on Small Hardware},\n    year = {2020},\n    isbn = {9781450388160},\n    publisher = {Association for Computing Machinery},\n    address = {New York, NY, USA},\n    url = {https://doi.org/10.1145/3404397.3404456},\n    doi = {10.1145/3404397.3404456},\n    booktitle = {49th International Conference on Parallel Processing - ICPP},\n    articleno = {4},\n    numpages = {11},\n    keywords = {GPU, parallel graph algorithms, link prediction, Graph embedding, graph coarsening},\n    location = {Edmonton, AB, Canada},\n    series = {ICPP '20}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/SabanciParallelComputing/GOSH",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-06-09T08:57:12Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-25T20:31:41Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "An ultra-fast, GPU-based large graph embedding algorithm utilizing a novel coarsening algorithm requiring not more than a single GPU.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/SabanciParallelComputing/GOSH/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Fri, 24 Dec 2021 11:19:05 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/SabanciParallelComputing/GOSH/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "SabanciParallelComputing/GOSH",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.9118439783313135,
        0.9879863063452118,
        0.9906248903846466
      ],
      "excerpt": "You can build the executables of the project with a single command. Simply clone the repo, navigate to it and call the make command: \ngit clone https://github.com/SabanciParallelComputing/GOSH \ncd GOSH \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/SabanciParallelComputing/GOSH/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Cuda",
      "C++",
      "C",
      "Makefile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'Copyright (c) 2017, Hilton Bristow\\nAll rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met:\\n\\n Redistributions of source code must retain the above copyright notice, this\\n  list of conditions and the following disclaimer.\\n\\n Redistributions in binary form must reproduce the above copyright notice,\\n  this list of conditions and the following disclaimer in the documentation\\n  and/or other materials provided with the distribution.\\n\\n* Neither the name of the copyright holder nor the names of its\\n  contributors may be used to endorse or promote products derived from\\n  this software without specific prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# GOSH: Embedding Big Graphs on Small Hardware\n**GOSH** is a *GPU-based* graph embedding tool that takes a graph and produces *d*-dimensional vector for every node in the graph. The embeddings can then be used for a multitude of machine learning tasks including node classification, link prediction, graph visualization, and anomaly detection.\n\n![Embedding pipeline](emb.png)\n**GOSH** employs a novel *coarsening* algorithm (MultiEdgeCollapse) to compress the graph into smaller graphs and embeds the smaller graphs to produce very accurate embeddings very quickly. Besides, it uses a special scheduling algorithm to embed any graph using a single GPU - even if the memory requirement of the graph exceeds that of the GPU.\n\nGetting Started",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "GOSH",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "SabanciParallelComputing",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/SabanciParallelComputing/GOSH/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We compiled this program with `nvcc` using `CUDA 10.1`and ran it on `Ubuntu 4.4.0-159`. \n\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The most basic execution of GOSH can be done as follows:\n```\nexecs/gosh.out --input-graph $string --output-embedding $string --directed $number --epochs $number\n```\n- `--input-graph $string`:\nAn edge list file with the vertex IDs as zero-indexed integers. i.e, the file should be of the form:\n```\ni j\nk l\n...\na b\n```\nWhere `i, j, k, l, a, b` are vertex IDs from the graph. Please note that *all vertices will get embeddings - even those without any edges.*\n- `--output-embedding $string`:\nThe file to which the embeddings are to be printed. The embeddings can either be printed as an ASCII formatted text file or a binary file (can be triggered with the `--binary-output` flag).\nThe output format in ASCII text is as follows:\n```\nnum_vertices dimension\n0 e_1 e_2 e_3 ... e_dimension-1\n1 e_1 e_2 e_3 ... e_dimension-1\n...\nnum_vertices-1 e_1 e_2 e_3 ... e_dimension-1\n```\nWhere `num_vertices` and `dimension` are the number of vertices and the embedding dimensionality, respectively, and every line afterward corresponds to a vertex from the graph. A line starts with the ID of the vertex and is followed by its embeddings. All elements within a line are space-separated.\n- `--epochs $number`: The number of epochs to run on the entirety of the graph. Running an epoch on a graph Gi which has been coarsening *i* times corresponds to running |V*i*| positive samples on that graph. \n    - *Note:* the strategy in which the epochs are distributed across different levels of coarsening can be tuned using the options `--epoch-strategy` and `--smoothing-ratio` and are discussed further below.\n- `--directed $number`: Whether the graph is directed (1), undirected (0), or passed as in a Binary Compressed Sparse Row format (2).\n\n## Optional Parameters\nMany optional parameters can be used to fine-tune the embedding:\n### Global parameters\n- `-d --dimension $number`: an integer with the dimensionality of the embedding.\n- `-s --negative_samples $number`: number of negative samples used with every positive update.\n- `--negative-weight $float`: a scaling factor used with negative samples to scale the gradients to be used when updating embeddings during negative updates.\n- `--device-id $nummber`: the ID of the GPU device to be used during the embedding.\n- `--binary-output`: whether the outputs are printed in binary format for compactness and ease of processing on memory. The format of the binary output is as follows:\n    - The number of vertices as a signed integer\n    - The embedding dimension as s signed integer\n    - The embeddings of all the vertices printed sequentially as single precision floats in C++.\n### Sampling parameters\n- `--sampling-algorithm $number`: the method used to create positive samples during training. Currently, two sampling strategies are implemented: \n    - `--sampling-algorithm 0`: 1-hop neighbor sampling or PPR sampling as described in [VERSE](https://arxiv.org/abs/1803.04742). Depending on the value of `--alpha`. \n        - `--alpha 0`: positive samples for a node are sampled from its direct neighbors \n        - `--alpha > 0 && --alpha < 1`: positive samples of a node are nodes reached after performing a Personalized Page Rank random walk, with `--alpha` being the damping factor as defined in [VERSE](https://arxiv.org/abs/1803.04742).\n    - `--sampling-algorithm 1`: random-walk based sampling. With this method, random walks are generated on the CPU and samples are extracted from them and sent to the GPU. It is controlled with three parameters:\n        - `--walk-length $number`: length of each random walk.\n        - `--augmentation-distance $number`: within a walk, from each sequence of `$number$`, all the pairs of nodes are used as poisitive samples.\n        - `--sample-pool-size`: the number of samples to be added to the pool on the CPU which is copied to the GPU.\n- `-a --alpha $number`: A value for the positive sampling strategy to be used in the model based on [VERSE](https://arxiv.org/abs/1803.04742). `alpha = 0` will use an adjacency similarity positive sampling approach while `0 > alpha > 100` will use PPR with `alpha/100` as its damping factor.\n ### Learning Rate Parameters\n- `-l --learning_rate $float`: The global learning rate of the model.\n- `--learning-rate-decay-strategy $num`: The strategy used to decay the learning rate during a level and between different levels. There are four strategies (0/1/2/3), their differences are shown below:\n    - 0, 1: At every level, learning rate decays linearly from the initial learning rate starting at the first epoch until the last epoch based on the following equation:\n    ```\n    current_learning_rate = (max(1-current_epoch/total_epochs), 1e-4)*initial_learning_rate\n    ```\n    where `current_epoch` and `total_epochs` are the current and total epochs for the current coarsening level.\n    - 2, 3: The learning rate at the end of a level i is the same as it is at the beginning. No decay.\n    - 1, 3; initial learning rate for every coarsening level differs based on the following heuristic:\n    ```\n    lr_i = lr;  if  |Vi| < LR_REV_NV\n    lr_i = lr/sqrt(|Vi|/ LR_REV_NV); otherwise\n    ```\n    Where:  \n    `lr` = input learning rate (global)  \n    `lri` = initial learning rate at coarsening level *i*.  \n    `LR_REV_NV` = tunable hyperparameter in the `src/main.cu`\n\n    - 0, 2: initial learning rate at each level is the same as the original learning rate given as input\n### Coarsening Parameters\n- `--no-coarsening`: will not apply any coarsening and will run the embedding directly on the original graph.\n- `--coarsening-stopping-threshold $num`: the number of vertices to stop coarsening at. i.e, when a graph G*i* is generated having |V*i*|< `$num`, it will be added to the coarsened set, but the coarsening will not continue.\n- `--coarsening-stopping-precision $float`: the accpetable shrinkage of a graph during coarsening. i.e, if graph G*i* is coarsened into graph G*i+1*, and |V*i+1*| > |V*i*| * `$float`, graph G*i+1* is *not* added to the coarsened set and coarsening will not continue.\n- `--coarsening-matching-threshold-ratio $num`: controls the total number of matches allowed per vertex. Given a graph G*i* coarsened *i* times, a vertex in G*i* is not allowed to match more than `i * i * ($num / |Vi|)` vertices.\n- `--coarsening-min-vertices-in-graph $num`: the minimum number of vertices acceptable in a graph to be added into the coarsened set, i.e, if a graph G*i* is coarsened into a graph G*i+1* and |V*i+1*| < `$num`, the graph G*i+1* is not added to the coarsened set.\n### Epoch Distribution Parameters\n- `--epoch-strategy`: choose the strategy to use to distribute epochs across levels, there are multiple strategies available:\n  - *fast*: The smallest graph is given half of the total epochs and the next level is given half of that, and so on.\n  - *s-fast*: `COARSE_SMOOTHING_RATIO * total_epochs` epochs are distributed equally across levels, while the remainder is distributed based on the *fast* rule.\n  - *normal*: equal distribution of epochs across levels.\n  - *accurate*: The opposite of fast; the biggest graph is given half of the total epochs and, the smaller level is given half of that, and so on.\n  - *s-accurate*: `COARSE_SMOOTHING_RATIO * total_epochs` epochs are distributed equally across levels, while the remainder is distributed based on the *accurate* rule.\n\n NOTE: in all of the aforementioned rules, a level is allocated a minimum of 1 epoch.\n- `--smoothing-ratio`: the smoothing ratio used in distributing epochs for the smooth strategies, i.e *s-fast* and *s-accurate*.\n\n\n### Large Graph Parameters\n- `--epoch-batch-size $num`: the number of epochs to run per large graph execution round, where a single round consists of a full rotation over all the embedding part pairs.\n- `--num-parts $num`: the number of embedding parts to store concurrently on the GPU.\n- `--num-pools $num`: the number of sample pools to store concurrently on the GPU.\n- `--sampling-threads $num`: the number of threads to work on sampling in parallel.\n- `--concurrent-samplers $num`: the number of sample pools to be sampled into concurrently, where a single pool can be sampled into by a maximum of `sampling-threads / concurrent-samplers` threads.\n- `--task-queue-threads $num`: the number of threads to execute tasks from the task queue.\n- `--num-sample-pool-sets $num`: number of sample pool sets.\n\n# Authors\n[Amro Alabsi Aljundi](https://github.com/AmroAlJundi), [Taha Atahan Akyildiz](https://github.com/Atahanak), and [Kamer Kaya](http://people.sabanciuniv.edu/kaya/).\n# Acknowldegments\n- Adapted the positive sampling and embedding updates from the [VERSE](https://arxiv.org/abs/1803.04742) paper written by [Anton Tsitsulin](http://tsitsul.in/), [Davide Mottin](https://mott.in/), [Panagiotis Karras](https://scholar.google.com/citations?user=B6C4aBoAAAAJ&hl=en), and  [Emmanuel M\u00fcller](https://scholar.google.com/citations?user=gg0HkEsAAAAJ&hl=en) \n- Used the Argparse header for C++ written by [Hilton Bristow](https://github.com/hbristow) to process CLI inputs: https://github.com/hbristow/argparse\n# Citation\nIf you find our code useful for your research, please cite the GOSH paper:\n```\n@inproceedings{10.1145/3404397.3404456,\n    author = {Akyildiz, Taha Atahan and Aljundi, Amro Alabsi and Kaya, Kamer},\n    title = {GOSH: Embedding Big Graphs on Small Hardware},\n    year = {2020},\n    isbn = {9781450388160},\n    publisher = {Association for Computing Machinery},\n    address = {New York, NY, USA},\n    url = {https://doi.org/10.1145/3404397.3404456},\n    doi = {10.1145/3404397.3404456},\n    booktitle = {49th International Conference on Parallel Processing - ICPP},\n    articleno = {4},\n    numpages = {11},\n    keywords = {GPU, parallel graph algorithms, link prediction, Graph embedding, graph coarsening},\n    location = {Edmonton, AB, Canada},\n    series = {ICPP '20}\n}\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 12,
      "date": "Fri, 24 Dec 2021 11:19:05 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "graph-embedding",
      "graph-algorithms",
      "high-performance-computing",
      "gpu-computing",
      "cuda"
    ],
    "technique": "GitHub API"
  }
}