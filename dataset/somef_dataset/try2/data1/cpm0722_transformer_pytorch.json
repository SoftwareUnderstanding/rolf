{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- ",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8498605443470728
      ],
      "excerpt": "\ucd9c\ucc98: Attention is All You Need [https://arxiv.org/pdf/1706.03762.pdf] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8283216015784888
      ],
      "excerpt": "Encoder\ub294 \uc65c \uc5ec\ub7ec \uac1c\uc758 layer\ub97c \uacb9\uccd0 \uc313\ub294 \uac83\uc77c\uae4c? \uac01 Encoder Layer\uc758 \uc5ed\ud560\uc740 \ubb34\uc5c7\uc77c\uae4c? \uacb0\ub860\ubd80\ud130 \ub9d0\ud558\uc790\uba74, \uac01 Encoder Layer\ub294 input\uc73c\ub85c \ub4e4\uc5b4\uc624\ub294 vector\uc5d0 \ub300\ud574 \ub354 \ub192\uc740 \ucc28\uc6d0(\ub113\uc740 \uad00\uc810)\uc5d0\uc11c\uc758 context\ub97c \ub2f4\ub294\ub2e4. \ub192\uc740 \ucc28\uc6d0\uc5d0\uc11c\uc758 context\ub77c\ub294 \uac83\uc740 \ub354 \ucd94\uc0c1\uc801\uc778 \uc815\ubcf4\ub77c\ub294 \uc758\ubbf8\uc774\ub2e4. Encoder Layer\ub294 \ub0b4\ubd80\uc801\uc73c\ub85c \uc5b4\ub5a0\ud55c Mechanism\uc744 \uc0ac\uc6a9\ud574 context\ub97c \ub2f4\uc544\ub0b4\ub294\ub370, Encoder Layer\uac00 \uacb9\uacb9\uc774 \uc313\uc774\ub2e4 \ubcf4\ub2c8 \ucc98\uc74c\uc5d0\ub294 \uc6d0\ubcf8 \ubb38\uc7a5\uc5d0 \ub300\ud55c \ub0ae\uc740 \uc218\uc900\uc758 context\uc600\uaca0\uc9c0\ub9cc \uc774\ud6c4 context\uc5d0 \ub300\ud55c context, context\uc758 context\uc5d0 \ub300\ud55c context ... \uc640 \uac19\uc740 \uc2dd\uc73c\ub85c \uc810\ucc28 \ub192\uc740 \ucc28\uc6d0\uc758 context\uac00 \uc800\uc7a5\ub418\uac8c \ub41c\ub2e4. Encoder Layer\uc758 \ub0b4\ubd80\uc801\uc778 \uc791\ub3d9 \ubc29\uc2dd\uc740 \uace7 \uc0b4\ud3b4\ubcfc \uac83\uc774\uae30\uc5d0, \uc5ec\uae30\uc11c\ub294 \uc9c1\uad00\uc801\uc73c\ub85c Encoder Layer\uc758 \uc5ed\ud560, Encoder \ub0b4\ubd80\uc758 \uc804\uccb4\uc801\uc778 \uad6c\uc870\ub9cc \uc774\ud574\ud558\uace0 \ub118\uc5b4\uac00\uc790. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9321597499656605
      ],
      "excerpt": "Multi-Head Attention\uc740 Self-Attention\uc744 \ubcd1\ub82c\uc801\uc73c\ub85c \uc5ec\ub7ec \uac1c \uc218\ud589\ud558\ub294 layer\uc774\ub2e4. \ub54c\ubb38\uc5d0 Multi-Head Attention\uc744 \uc774\ud574\ud558\uae30 \uc704\ud574\uc11c\ub294 Self-Attention\uc5d0 \ub300\ud574 \uba3c\uc800 \uc54c\uc544\uc57c\ub9cc \ud55c\ub2e4. Attention\uc774\ub77c\ub294 \uac83\uc740 \ub113\uc740 \ubc94\uc704\uc758 \uc804\uccb4 data\uc5d0\uc11c \ud2b9\uc815\ud55c \ubd80\ubd84\uc5d0 \uc9d1\uc911\ud55c\ub2e4\ub294 \uc758\ubbf8\uc774\ub2e4. \ub2e4\uc74c\uc758 \ubb38\uc7a5\uc744 \ud1b5\ud574 Attention\uc758 \uac1c\ub150\uc744 \uc774\ud574\ud574\ubcf4\uc790. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8109194328925066
      ],
      "excerpt": "Self-Attention\uc5d0\uc11c\ub294 \ucd1d 3\uac1c\uc758 vector\uac00 \uc0c8\ub85c \ub4f1\uc7a5\ud55c\ub2e4. Query, Key, Value\uc774\ub2e4. \uac01\uac01\uc758 \uc5ed\ud560\uc740 \ub2e4\uc74c\uacfc \uac19\ub2e4. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8714162992508173
      ],
      "excerpt": "Key: attention\uc744 \uad6c\ud558\uace0\uc790 \ud558\ub294 \ub300\uc0c1 token\uc744 \uc758\ubbf8 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999887851776386,
        0.971679416046096
      ],
      "excerpt": "Query, Key, Value\uac00 \uac01\uac01 \uc5b4\ub5a4 token\uc744 \uac00\ub9ac\ud0a4\ub294\uc9c0\ub294 \uc774\ud574\uac00 \ub410\uc744 \uac83\uc774\ub2e4. \ud558\uc9c0\ub9cc, \uadf8\ub798\uc11c Query, Key, Value\ub77c\ub294 \uc138 vector\uc758 \uad6c\uccb4\uc801\uc778 \uac12\uc740 \uc5b4\ub5bb\uac8c \ub9cc\ub4e4\uc5b4\uc9c0\ub294\uc9c0\ub294 \uc6b0\ub9ac\ub294 \uc544\uc9c1 \uc54c\uc9c0 \ubabb\ud55c\ub2e4. \uc815\ub9d0 \uac04\ub2e8\ud558\uac8c\ub3c4, input\uc73c\ub85c \ub4e4\uc5b4\uc624\ub294 token embedding vector\ub97c fully connected layer\uc5d0 \ub123\uc5b4 \uc138 vector\ub97c \ub9cc\ub4e4\uc5b4\ub0b8\ub2e4. \uc138 vector\ub97c \uc0dd\uc131\ud574\ub0b4\ub294 FC layer\ub294 \ubaa8\ub450 \ub2e4\ub974\uae30 \ub54c\ubb38\uc5d0, \uacb0\uad6d self-attention\uc5d0\uc11c\ub294 Query, Key, Value\ub97c \uad6c\ud558\uae30 \uc704\ud574 3\uac1c\uc758 \uc11c\ub85c \ub2e4\ub978 FC layer\uac00 \uc874\uc7ac\ud55c\ub2e4. \uc774 FC layer\ub4e4\uc740 \ubaa8\ub450 \uac19\uc740 input dimension, output dimension\uc744 \uac16\ub294\ub2e4. input dimension\uc774 \uac19\uc740 \uc774\uc720\ub294 \ub2f9\uc5f0\ud558\uac8c\ub3c4 \ubaa8\ub450 \ub2e4 token embedding vector\ub97c input\uc73c\ub85c \ubc1b\uae30 \ub54c\ubb38\uc774\ub2e4. \ud55c\ud3b8, \uc138 FC layer\uc758 output dimension\uc774 \uac19\ub2e4\ub294 \uac83\uc744 \ud1b5\ud574 \uac01\uac01 \ubcc4\uac1c\uc758 FC layer\ub85c \uad6c\ud574\uc9c4 Query, Key, Value\uac00 \uad6c\uccb4\uc801\uc778 \uac12\uc740 \ub2e4\ub97c\uc9c0\uc5b8\uc815 \uac19\uc740 dimension\uc744 \uac16\ub294 vector\uac00 \ub41c\ub2e4\ub294 \uac83\uc744 \uc54c \uc218 \uc788\ub2e4. \uc989, Query, Key, Value\uc758 shape\ub294 \ubaa8\ub450 \ub3d9\uc77c\ud558\ub2e4. \uc55e\uc73c\ub85c \uc774 \uc138 vector\uc758 dimension\uc744 $d_k$\ub85c \uba85\uba85\ud55c\ub2e4. \uc5ec\uae30\uc11c $k$\ub294 Key\ub97c \uc758\ubbf8\ud558\ub294\ub370, \uad73\uc774 Query, Key, Value \uc911 Key\ub97c \uc774\ub984\uc73c\ub85c \ucc44\ud0dd\ud55c \uc774\uc720\ub294 \ud2b9\ubcc4\ud788 \uc788\uc9c0 \uc54a\uace0, \ub2e8\uc9c0 \ub17c\ubb38\uc758 notation\uc5d0\uc11c \uc774\ub97c \ucc44\ud0dd\ud588\uae30 \ub54c\ubb38\uc774\ub2e4. \uc815\ub9ac\ud558\uc790\uba74, Query, Key, Value\ub294 \ubaa8\ub450 $d_k$\uc758 dimension\uc744 \uac16\ub294 vector\uc774\ub2e4. \uc774\uc81c \uc704\uc5d0\uc11c \uc598\uae30\ud588\ub358 Key, Value\uac00 \ub2e4\ub978 \uac12\uc744 \uac16\ub294 \uc774\uc720\ub97c \uc774\ud574\ud560 \uc218 \uc788\ub2e4. input\uc740 \uac19\uc740 token embedding vector\uc600\uc744\uc9c0\ub77c\ub3c4 \uc11c\ub85c \ub2e4\ub978 FC layer\ub97c \ud1b5\ud574\uc11c \uac01\uac01 Key, Value\uac00 \uad6c\ud574\uc9c0\uae30 \ub54c\ubb38\uc5d0 \uac19\uc740 token\uc744 \uac00\ub9ac\ud0a4\uba74\uc11c \ub2e4\ub978 \uac12\uc744 \uac16\ub294 \uac83\uc774\ub2e4. \n\uc774\uc81c Query, Key, Value\ub97c \ud65c\uc6a9\ud574 Attention\uc744 \uacc4\uc0b0\ud574\ubcf4\uc790. Attention\uc774\ub77c\uace0 \ud55c\ub2e4\uba74 \uc5b4\ub5a4 \uac83\uc5d0 \ub300\ud55c Attention\uc778\uc9c0 \ubd88\uba85\ud655\ud558\ub2e4. \uad6c\uccb4\uc801\uc73c\ub85c, Query\uc5d0 \ub300\ud55c Attention\uc774\ub2e4. \uc774 \uc810\uc744 \uaf2d \uc778\uc9c0\ud558\uace0 \ub118\uc5b4\uac00\uc790. \uc774\ud6c4\ubd80\ud130\ub294 Query, Key, Value\ub97c \uac01\uac01 $Q$, $K$, $V$\ub85c \ucd95\uc57d\ud574 \ubd80\ub978\ub2e4. Query\uc758 Attention\uc740 \ub2e4\uc74c\uacfc \uac19\uc740 \uc218\uc2dd\uc73c\ub85c \uacc4\uc0b0\ub41c\ub2e4. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8498605443470728
      ],
      "excerpt": "\ucd9c\ucc98: Attention is All You Need [https://arxiv.org/pdf/1706.03762.pdf] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8498605443470728
      ],
      "excerpt": "\ucd9c\ucc98: Attention is All You Need [https://arxiv.org/pdf/1706.03762.pdf] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8498605443470728,
        0.8184428348523783
      ],
      "excerpt": "\ucd9c\ucc98: Attention is All You Need [https://arxiv.org/pdf/1706.03762.pdf] \n\uc9c0\uae08\uae4c\uc9c0\uc758 Self-Attention\uc5d0 \ub300\ud55c \uac1c\ub150\uc740 \ubaa8\ub450 Multi-Head Attention Layer\ub97c \uc774\ud574\ud558\uae30 \uc704\ud55c \uac83\uc774\uc5c8\ub2e4. Attention \uacc4\uc0b0\uc744 \ub17c\ubb38\uc5d0\uc11c\ub294 Scaled Dot-Product Attention\uc774\ub77c\uace0 \uba85\uba85\ud55c\ub2e4. Transformer\ub294 Scaled Dot Attention\uc744 \ud55c Encoder Layer\ub9c8\ub2e4 1\ud68c\uc529 \uc218\ud589\ud558\ub294 \uac83\uc774 \uc544\ub2c8\ub77c $h$\ud68c \uc218\ud589\ud55c \ub4a4, \uadf8 \uacb0\uacfc\ub97c \uc885\ud569\ud574 \uc0ac\uc6a9\ud55c\ub2e4. \uc774 \uac83\uc774 Multi-Head Attention Layer\uc774\ub2e4. \uc774\ub7ec\ud55c \uc791\uc5c5\uc744 \uc218\ud589\ud558\ub294 \uc774\uc720\ub294 \uc5ec\ub7ec Attention\uc744 \uc798 \ubc18\uc601\ud558\uae30 \uc704\ud574\uc11c\uc774\ub2e4. \ub9cc\uc57d \ud558\ub098\uc758 Attention\ub9cc \ubc18\uc601\ud55c\ub2e4\uace0 \ud588\uc744 \ub54c, \uc608\uc2dc \ubb38\uc7a5\uc5d0\uc11c 'it'\uc758 Attention\uc5d0\ub294 'animal'\uc758 \uac83\uc774 \ub300\ubd80\ubd84\uc744 \ucc28\uc9c0\ud558\uac8c \ub420 \uac83\uc774\ub2e4. \ud558\uc9c0\ub9cc \uc5ec\ub7ec \uc885\ub958\uc758 attention\uc744 \ubc18\uc601\ud55c\ub2e4\uace0 \ud588\uc744 \ub54c 'tired'\uc5d0 \uc9d1\uc911\ud55c Attention\uae4c\uc9c0 \ubc18\uc601\ub41c\ub2e4\uba74, \ucd5c\uc885\uc801\uc778 'it'\uc758 Attention\uc5d0\ub294 'animal'\uc744 \uc9c0\uce6d\ud55c\ub2e4\ub294 \uc815\ubcf4, 'tired' \uc0c1\ud0dc\ub77c\ub294 \uc815\ubcf4\uae4c\uc9c0 \ubaa8\ub450 \ub2f4\uae30\uac8c \ub420 \uac83\uc774\ub2e4. \uc774 \uac83\uc774 Multi-Head Attention\uc744 \uc0ac\uc6a9\ud558\ub294 \uc774\uc720\uc774\ub2e4. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8714162992508173
      ],
      "excerpt": "Multi-Head Attention Layer\ub97c \uc2e4\uc81c code\ub85c \uad6c\ud604\ud574\ubcf4\uc790. \uc704\uc5d0\uc11c \uad6c\ud604\ud588\ub358 calculate_attention()\uc744 \uc0ac\uc6a9\ud55c\ub2e4. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9321597499656605
      ],
      "excerpt": "    key = transform(key, self.key_fc_layer) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9763162561870891
      ],
      "excerpt": "Encoder Layer\ub294 \uc704\uc5d0\uc11c \ub2e4\ub918\ub358 Multi-Head Attention Layer\uc640 Feed-Forwad Layer\ub85c \uad6c\uc131\ub41c\ub2e4. \uadf8\ub7ec\ub098 \uc0ac\uc2e4\uc740 Encoder Layer\ub97c \uad6c\uc131\ud558\ub294 \ub450 layer\ub294 Residual Connection\uc73c\ub85c \ub458\ub7ec\uc2f8\uc5ec \uc788\ub2e4. Residual Connection\uc774\ub77c\ub294 \uac83\uc740 \uc815\ub9d0 \ub2e8\uc21c\ud558\ub2e4. $y = f(x)$\ub97c $y=f(x)+x$\ub85c \ubcc0\uacbd\ud558\ub294 \uac83\uc774\ub2e4. \uc989, output\uc744 \uadf8\ub300\ub85c \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uace0, output\uc5d0 input\uc744 \ucd94\uac00\uc801\uc73c\ub85c \ub354\ud55c \uac12\uc744 \uc0ac\uc6a9\ud558\uac8c \ub41c\ub2e4. \uc774\ub85c \uc778\ud574 \uc5bb\uc744 \uc218 \uc788\ub294 \uc774\uc810\uc740 \uba85\ud655\ud558\ub2e4. Back Propagation \ub3c4\uc911 \ubc1c\uc0dd\ud560 \uc218 \uc788\ub294 Gradient Vanishing\uc744 \ubc29\uc9c0\ud560 \uc218 \uc788\ub2e4. \uac1c\ub150\uc801\uc73c\ub85c\ub294 \uc774 \uac83\uc774 \uc804\ubd80\uc774\ub2e4. \uc5ec\uae30\uc5d0 \ub354\ud574 \ub17c\ubb38\uc5d0\uc11c \ucc44\ud0dd\ud55c Layer Normalization\uae4c\uc9c0 \ucd94\uac00\ud55c\ub2e4. \uac04\ub2e8\ud558\uac8c \ucf54\ub4dc\ub85c \uad6c\ud604\ud574\ubcf4\uc790. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9982063141659211,
        0.9321597499656605
      ],
      "excerpt": "Decoder Layer\ub294 Encoder Layer\uc640 \ub2ec\ub9ac Multi-Head Attention Layer\uac00 2\uac1c\uac00 \uc874\uc7ac\ud55c\ub2e4. \uccab\ubc88\uc9f8 layer\ub294 Masked Multi-Head Attention Layer\ub77c\uace0 \ubd80\ub974\ub294\ub370, \uc774\ub294 \uc704\uc5d0\uc11c \uc5b8\uae09\ud588\ub358 subsequent masking\uc774 \uc801\uc6a9\ub418\uae30 \ub584\ubb38\uc774\ub2e4. \ub450\ubc88\uc9f8 layer\ub294 \ud2b9\uc9d5\uc774 Encoder\uc5d0\uc11c \ub118\uc5b4\uc628 Context\ub97c input\uc73c\ub85c \ubc1b\uc544 \uc0ac\uc6a9\ud55c\ub2e4\ub294 \uac83\uc774\ub2e4. \uc989, Encoder\uc758 Context\ub294 Decoder \ub0b4 \uac01 Decoder Layer\uc758 \ub450\ubc88\uc9f8 Multi-Head Attention Layer\uc5d0\uc11c \uc0ac\uc6a9\ub418\uac8c \ub41c\ub2e4. \ub9c8\uc9c0\ub9c9 Position-wise Feed-Forward Layer\ub294 Encoder Layer\uc758 \uac83\uacfc \uc644\uc804\ud788 \ub3d9\uc77c\ud558\ubbc0\ub85c \uc124\uba85\uc744 \uc0dd\ub7b5\ud55c\ub2e4. \uc774\uc81c \ub450 Multi-Head Attention Layer\uc5d0 \ub300\ud574\uc11c Encoder\uc758 \uac83\uacfc \ube44\uad50\ud558\uba70 \ud2b9\uc9d5\uc744 \uc0b4\ud3b4\ubcf4\uc790. \nMasked Multi-Head Attention Layer\uc5d0 \ub300\ud55c \uc124\uba85\uc740 \ud2b9\ubcc4\ud55c \uac83\uc774 \uc5c6\ub2e4. Encoder\uc758 \uac83\uacfc \uc644\uc804\ud788 \ub3d9\uc77c\ud55c\ub370 \ub2e4\ub9cc mask\ub85c \ub4e4\uc5b4\uc624\ub294 \uc778\uc790\uac00 \uc77c\ubc18\uc801\uc778 pad masking\uc5d0 \ub354\ud574 subsequent masking\uae4c\uc9c0 \uc801\uc6a9\ub418\uc5b4 \uc788\ub2e4\ub294 \uc810\ub9cc\uc774 \ucc28\uc774\uc77c \ubfd0\uc774\ub2e4. \uc989, \uc774 layer\ub294 Self-Attention\uc744 \uc218\ud589\ud558\ub294 layer\uc774\ub2e4. 'Self'\uc5d0 \uc8fc\ubaa9\ud558\uc790. \uac19\uc740 sentence \ub0b4 token\ub4e4 \uc0ac\uc774\uc758 attention\uc744 \ucc3e\ub294 \uac83\uc774\ub2e4. \uc774\ub294 \ub2e4\uc74c Multi-Head Attention Layer\uc640 \uac00\uc7a5 \ud070 \ucc28\uc774\uc810\uc774\ub2e4. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9734334817100387
      ],
      "excerpt": "\uc9c0\uae08 \uc640\uc11c Multi-Head Attention Layer\ub97c \ubcf4\uba74 \uc774\uc804\uacfc \ub2e4\ub974\uac8c \ubcf4\uc77c \uac83\uc774\ub2e4. query, key, value\ub97c \uad73\uc774 \uac01\uac01 \ubcc4\uac1c\uc758 \uc778\uc790\ub85c \ubc1b\uc740 \uc774\uc720\ub294 Decoder Layer \ub0b4\uc5d0\uc11c query\uc640 key, value\ub294 \uc11c\ub85c \ub2e4\ub978 embedding\uc5d0\uc11c \ub118\uc5b4\uc624\uae30 \ub54c\ubb38\uc774\ub2e4. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/cpm0722/transformer_pytorch",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-11-22T08:35:03Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-02T06:35:52Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.83718068102951
      ],
      "excerpt": "    def transform(x, fc_layer): #: reshape (n_batch, seq_len, d_embed) to (n_batch, h, seq_len, d_k) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877,
        0.8043749996699634,
        0.8588347943308506
      ],
      "excerpt": "\ud604\uc2e4\uc5d0\uc11c\ub294, \ud2b9\ud788\ub098 model \ud559\uc2b5 \ucd08\ucc3d\uae30\uc5d0\ub294 \uc704\ucc98\ub7fc \uc798\ubabb\ub41c token\uc744 \uc0dd\uc131\ud574\ub0b4\uace0, \uadf8 \uc774\ud6c4 \uacc4\uc18d\uc801\uc73c\ub85c \uc798\ubabb\ub41c token\uc774 \uc0dd\uc131\ub420 \uac83\uc774\ub2e4. \ucd08\ubc18\uc5d0 \ud558\ub098\uc758 token\uc774 \uc798\ubabb \ub3c4\ucd9c\ub418\uc5c8\ub2e4\uace0 \uc774\ud6c4 token\uc774 \ubaa8\ub450 \ub2e4 \uc798\ubabb\ub418\uac8c \ub098\uc628\ub2e4\uba74 \uc81c\ub300\ub85c \ub41c \ud559\uc2b5\uc774 \uc9c4\ud589\ub418\uae30 \ud798\ub4e4 \uac83\uc774\ub2e4. \ub530\ub77c\uc11c \uc774\ub97c \uc704\ud574 Teacher Forcing\uc744 \uc0ac\uc6a9\ud55c\ub2e4. \nTeacher Forcing\uc740 \uc2e4\uc81c labeled data(Ground Truth)\ub97c RNN cell\uc758 input\uc73c\ub85c \uc0ac\uc6a9\ud558\ub294 \uac83\uc774\ub2e4. \uc815\ud655\ud788\ub294 Ground Truth\uc758 [:-1]\ub85c slicing\uc744 \ud55c \uac83\uc774\ub2e4(\ub9c8\uc9c0\ub9c9 token\uc778 EOS token\uc744 \uc81c\uc678\ud558\ub294 \uac83\uc774\ub2e4). \uc774\ub97c \ud1b5\ud574\uc11c model\uc774 \uc798\ubabb\ub41c token\uc744 \uc0dd\uc131\ud574\ub0b4\ub354\ub77c\ub3c4 \uc774\ud6c4 \uc81c\ub300\ub85c \ub41c token\uc744 \uc0dd\uc131\ud574\ub0b4\ub3c4\ub85d \uc720\ub3c4\ud560 \uc218 \uc788\ub2e4. \n\ud558\uc9c0\ub9cc \uc774\ub294 model \ud559\uc2b5 \uacfc\uc815\uc5d0\uc11c Ground Truth\ub97c \ud3ec\ud568\ud55c dataset\uc744 \uac16\uace0 \uc788\uc744 \ub54c\uc5d0\ub098 \uac00\ub2a5\ud55c \uac83\uc774\uae30\uc5d0 Test\ub098 \uc2e4\uc81c\ub85c Real-World\uc5d0 Deliever\ub420 \ub54c\uc5d0\ub294 model\uc774 \uc0dd\uc131\ud574\ub0b8 \uc774\uc804 token\uc744 \uc0ac\uc6a9\ud558\uac8c \ub41c\ub2e4. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "Transformer\ub97c \uc0dd\uc131\ud558\ub294 \uc608\uc81c \ud568\uc218 make_model()\uc740 \ub2e4\uc74c\uacfc \uac19\uc774 \uc791\uc131\ud560 \uc218 \uc788\ub2e4. \uc2e4\uc81c \ub17c\ubb38 \uc0c1\uc5d0\uc11c\ub294 $d_{embed}$\uc640 $d_{model}$\uc744 \uad6c\ubd84\ud558\uc9c0 \uc54a\uace0 \ud1b5\ud569\ud574\uc11c $d_{model}$\ub85c \uc0ac\uc6a9\ud588\uc9c0\ub9cc, \uc774\ud574\ub97c \ub3d5\uae30 \uc704\ud574 \uc9c0\uae08\uae4c\uc9c0 \ubd84\ub9ac\ud574 \uc0ac\uc6a9\ud588\ub2e4. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877,
        0.860059181823877
      ],
      "excerpt": "#: \uc2e4\uc81c model \uc0dd\uc131 \nmodel = Transformer( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "return model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Transformer(Attention Is All You Need) Implementation in Pytorch",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/cpm0722/transformer_pytorch/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sat, 25 Dec 2021 00:47:11 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/cpm0722/transformer_pytorch/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "cpm0722/transformer_pytorch",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/cpm0722/transformer_pytorch/main/transformer.ipynb"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.907712520488279,
        0.8359299706379749
      ],
      "excerpt": "Transformer\ub294 input sentence\ub97c \ub123\uc5b4 output sentence\ub97c \uc0dd\uc131\ud574\ub0b4\ub294 model\uc774\ub2e4. input\uacfc \ub3d9\uc77c\ud55c sentence\ub97c \ub9cc\ub4e4\uc5b4\ub0bc \uc218\ub3c4, input\uc758 \uc5ed\ubc29\ud5a5 sentence\ub97c \ub9cc\ub4e4\uc5b4\ub0bc \uc218\ub3c4, \uac19\uc740 \uc758\ubbf8\uc758 \ub2e4\ub978 \uc5b8\uc5b4\ub85c \ub41c sentence\ub97c \ub9cc\ub4e4\uc5b4\ub0bc \uc218\ub3c4 \uc788\ub2e4. \uc774\ub294 model\uc758 train \uacfc\uc815\uc5d0\uc11c \uc815\ud574\uc9c0\ub294 \uac83\uc73c\ub85c, label\uc744 \uc5b4\ub5a4 sentence\ub85c \uc815\ud560 \uac83\uc778\uac00\uc5d0 \ub530\ub77c \ub2ec\ub77c\uc9c4\ub2e4. \uacb0\uad6d Transformer\ub294 sentence \ud615\ud0dc\uc758 input\uc744 \uc0ac\uc6a9\ud574 sentence \ud615\ud0dc\uc758 output\uc744 \ub9cc\ub4e4\uc5b4\ub0b4\ub294 \ud568\uc218\ub85c \uc774\ud574\ud560 \uc218 \uc788\ub2e4. \n$ \\text{Transformer}(x)\\x,\\ y\\text{ : sentence} $ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8163012035434292
      ],
      "excerpt": "Value: attention\uc744 \uad6c\ud558\uace0\uc790 \ud558\ub294 \ub300\uc0c1 token\uc744 \uc758\ubbf8 (Key\uc640 \ub3d9\uc77c\ud55c token) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8481449418903522
      ],
      "excerpt": "    #: query, key, value's shape: (n_batch, seq_len, d_k) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8629920582736479
      ],
      "excerpt": "calculate_attention()\uc758 \uc778\uc790\ub85c query, key, value, mask\ub97c \ubc1b\ub294\ub2e4. mask\ub294 pad mask matrix\uc77c \uac83\uc774\ub2e4. pad mask matrix\ub294 Transformer \uc678\ubd80 (\ub300\uac1c Batch class)\uc5d0\uc11c \uc0dd\uc131\ub418\uc5b4 Transformer\uc5d0 \uc778\uc790\ub85c \ub4e4\uc5b4\uc624\uac8c \ub41c\ub2e4. query, key, value\ub294 \uc11c\ub85c \ub2e4\ub978 FC Layer\ub97c \uac70\uccd0 $\\text{n_batch} \\times \\text{max_seq_len} \\times d_k$\ub85c \ubcc0\ud615\ub418\uc5c8\ub2e4. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.803564247763383,
        0.8481449418903522,
        0.8164255860924499
      ],
      "excerpt": "def forward(self, query, key, value, mask=None): \n    #: query, key, value's shape: (n_batch, seq_len, d_embed) \n    #: mask's shape: (n_batch, seq_len, seq_len) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.839290883268242
      ],
      "excerpt": "    out = self.calculate_attention(query, key, value, mask) #: out's shape: (n_batch, h, seq_len, d_k) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8060813455504295
      ],
      "excerpt": "\uc778\uc790\ub85c \ubc1b\uc740 query, key, value\ub294 \uc2e4\uc81c $Q$, $K$, $V$ matrix\uac00 \uc544\ub2c8\ub2e4. $Q$, $K$, $V$ \uacc4\uc0b0\uc744 \uc704\ud574\uc11c\ub294 \uac01\uac01 FC Layer \uc5d0 input\uc73c\ub85c sentence(\uc2e4\uc81c\ub85c\ub294 mini-batch\uc774\ubbc0\ub85c \ub2e4\uc218\uc758 sentence)\ub97c \ub123\uc5b4\uc918\uc57c \ud558\ub294\ub370, \uc774 sentence\ub97c \uc758\ubbf8\ud558\ub294 \uac83\uc774\ub2e4. Self-Attention\uc774\uae30\uc5d0 \ub2f9\uc5f0\ud788 $Q$, $K$, $V$\ub294 \uac19\uc740 sentence\uc5d0\uc11c \ub098\uc624\uac8c \ub418\ub294\ub370 \uc65c \ubcc4\uac1c\uc758 \uc778\uc790\ub85c \ubc1b\ub294\uc9c0 \uc758\ubb38\uc77c \uc218 \uc788\ub2e4. \uc774\ub294 Decoder\uc758 \uc791\ub3d9 \uc6d0\ub9ac\ub97c \uc54c\uace0 \ub098\uba74 \uc774\ud574\ud560 \uc218 \uc788\uc744 \uac83\uc774\ub2e4. \uc778\uc790\ub85c \ubc1b\uc740 query, key, value\ub294 sentence\uc774\ubbc0\ub85c shape\ub294 ($\\text{n_batch} \\times \\text{seq_len} \\times d_{embed}$)\uc774\ub2e4. mask matrix\ub294 \uae30\ubcf8\uc801\uc73c\ub85c \ud55c \ubb38\uc7a5\uc5d0 \ub300\ud574 ($\\text{seq_len} \\times \\text{seq_len}$)\uc758 shape\ub97c \uac16\ub294\ub370, mini-batch\uc774\ubbc0\ub85c ($\\text{n_batch} \\times \\text{seq_len} \\times \\text{seq_len}$)\uc758 shape\ub97c \uac16\ub294\ub2e4. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8481449418903522
      ],
      "excerpt": "    #: query, key, value's shape: (n_batch, seq_len, d_k) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8385182728255672
      ],
      "excerpt": "\uc6b0\uc120 $d_k$\ub97c \uc911\uc2ec\uc73c\ub85c $Q$\uc640 $K$ \uc0ac\uc774 \ud589\ub82c\uacf1 \uc5f0\uc0b0\uc744 \uc218\ud589\ud558\uae30 \ub54c\ubb38\uc5d0 $Q$, $K$, $V$\uc758 \ub9c8\uc9c0\ub9c9 dimension\uc740 \ubc18\ub4dc\uc2dc $d_k$\uc5ec\uc57c\ub9cc \ud55c\ub2e4. \ub610\ud55c attention_score\uc758 shape\ub294 \ub9c8\uc9c0\ub9c9 \ub450 dimension\uc774 \ubc18\ub4dc\uc2dc ($\\text{seq_len} \\times \\text{seq_len}$)\uc774\uc5b4\uc57c\ub9cc masking\uc774 \uc801\uc6a9\ub420 \uc218 \uc788\uae30 \ub54c\ubb38\uc5d0 $Q$, $K$, $V$\uc758 \ub9c8\uc9c0\ub9c9 \uc9c1\uc804 dimension(.shape[-2])\uc740 \ubc18\ub4dc\uc2dc $\\text{seq_len}$\uc774\uc5b4\uc57c\ub9cc \ud55c\ub2e4. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8193003071479553
      ],
      "excerpt": "    out = self.multi_head_attention_layer(query=x, key=x, value=x, mask=mask) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8179345893902109,
        0.8143966368397063
      ],
      "excerpt": "def forward(self, src, trg, mask): \n    encoder_output = self.encoder(src, mask) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8359299706379749
      ],
      "excerpt": "$\\text{FFN}(x)=\\text{max}(0, xW_1+b_1)W_2 + b_2$ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8462666485673652
      ],
      "excerpt": "\uac00\uc7a5 \ucc98\uc74c\uc5d0 Transformer\uc758 \uc804\uccb4 \uad6c\uc870\ub97c \uc774\uc57c\uae30\ud560 \ub54c \ubd24\ub358 Decoder\uc758 \uad6c\uc870\uc774\ub2e4. Context\uc640 Some Sentence\ub97c input\uc73c\ub85c \ubc1b\uc544 Output Sentence\ub97c \ucd9c\ub825\ud55c\ub2e4. Context\ub294 Encoder\uc758 output\uc774\ub77c\ub294 \uac83\uc740 \uc774\ud574\ud588\ub2e4. Transformer model\uc758 \ubaa9\uc801\uc744 \ub2e4\uc2dc \uc0c1\uae30\uc2dc\ucf1c \ubcf4\uc790. input sentence\ub97c \ubc1b\uc544\uc640 output sentence\ub97c \ub9cc\ub4e4\uc5b4\ub0b4\ub294 model\uc774\ub2e4. \ub300\ud45c\uc801\uc73c\ub85c \ubc88\uc5ed\uacfc \uac19\uc740 task\ub97c \ucc98\ub9ac\ud560 \uc218 \uc788\uc744 \uac83\uc774\ub2e4. \ubc88\uc5ed\uc774\ub77c\uace0 \uac00\uc815\ud55c\ub2e4\uba74, Encoder\ub294 Context\ub97c \uc0dd\uc131\ud574\ub0b4\ub294 \uac83, \uc989 input sentence\uc758 \uc815\ubcf4\ub97c \uc555\ucd95\ud574 \ub2f4\uc544\ub0b4\ub294 \uac83\uc744 \ubaa9\uc801\uc73c\ub85c \ud558\uace0, Decoder\ub294 Context\ub97c \ud65c\uc6a9\ud574 output sentence\ub97c \ub9cc\ub4e4\uc5b4\ub0b4\ub294 \uac83\uc744 \ubaa9\uc801\uc73c\ub85c \ud55c\ub2e4. \uadf8\ub807\ub2e4\uba74 Decoder\ub294 input\uc73c\ub85c Context\ub9cc \ubc1b\uc544\uc57c \ud558\uc9c0, \uc65c \ub2e4\ub978 \ucd94\uac00\uc801\uc778 sentence\ub97c \ubc1b\uc744\uae4c? \ub610 \uc774 sentence\ub294 \ub3c4\ub300\uccb4 \ubb34\uc5c7\uc77c\uae4c? \uc774\uc5d0 \ub300\ud574 \uc54c\uc544\ubcf4\uc790. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8892625917143743
      ],
      "excerpt": "    mask = np.triu(np.ones(atatn_shape), k=1).astype('uint8') #: masking with upper triangle matrix \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8189832825408754
      ],
      "excerpt": "make_std_mask()\ub294 subsequent_mask()\ub97c \ud638\ucd9c\ud574 subsequent mask\uc744 \uc0dd\uc131\ud558\uace0, \uc774\ub97c pad mask\uc640 \uacb0\ud569\ud55c\ub2e4. \uc704\uc758 code\ub294 Transformer \ub0b4\ubd80\uac00 \uc544\ub2cc Batch class \ub0b4\uc5d0\uc11c \uc2e4\ud589\ub418\ub294 \uac83\uc774 \ubc14\ub78c\uc9c1\ud560 \uac83\uc774\ub2e4. mask \uc0dd\uc131\uc740 Transformer \ub0b4\ubd80 \uc791\uc5c5\uc774 \uc544\ub2cc \uc804\ucc98\ub9ac \uacfc\uc815\uc5d0 \ud3ec\ud568\ub418\uae30 \ub54c\ubb38\uc774\ub2e4. \ub530\ub77c\uc11c Encoder\uc5d0 \uc801\uc6a9\ub418\ub294 pad mask\uc640 \ub3d9\uc77c\ud558\uac8c Batch class \ub0b4\uc5d0\uc11c \uc0dd\uc131\ub420 \uac83\uc774\ub2e4. \uc774\ub294 \uacb0\uad6d Transformer \uc678\ubd80\uc5d0\uc11c \ub118\uc5b4\uc640\uc57c \ud558\uae30 \ub54c\ubb38\uc5d0 Transformer code\uac00 \uc218\uc815\ub418\uc5b4\uc57c \ud55c\ub2e4. \uae30\uc874\uc5d0\ub294 Encoder\uc5d0\uc11c \uc0ac\uc6a9\ud558\ub294 pad mask(src_mask)\ub9cc\uc774 forward()\uc758 \uc778\uc790\ub85c \ub4e4\uc5b4\uc654\ub2e4\uba74, \uc774\uc81c\ub294 Decoder\uc5d0\uc11c \uc0ac\uc6a9\ud560 subsequent mask (trg_mask)\ub3c4 \ud568\uaed8 \uc8fc\uc5b4\uc9c4\ub2e4. \ub530\ub77c\uc11c forward()\uc758 \ucd5c\uc885 \uc778\uc790\ub294 src, trg, src_mask, trg_mask\uc774\ub2e4. \uac01\uac01 Encoder\uc758 input, Decoder\uc758 input, Encoder\uc758 mask, Decoder\uc758 mask\uc774\ub2e4. forward() \ub0b4\ubd80\uc5d0\uc11c decoder\uc758 forward()\ub97c \ud638\ucd9c\ud560 \ub54c \uc5ed\uc2dc \ubcc0\uacbd\ub418\ub294\ub370, trg_mask\uac00 \ucd94\uac00\uc801\uc73c\ub85c \uc778\uc790\ub85c \ub118\uc5b4\uac00\uac8c \ub41c\ub2e4. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.803564247763383
      ],
      "excerpt": "def forward(self, query, key, value, mask=None): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8025472048583212
      ],
      "excerpt": "        out = layer(x, mask, encoder_output, encoder_mask) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8193003071479553,
        0.803196181461259
      ],
      "excerpt": "    out = self.masked_multi_head_attention_layer(query=x, key=x, value=x, mask=mask) \n    out = self.multi_head_attention_layer(query=out, key=encoder_output, value=encoder_output, mask=encoder_mask) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8340992551989391
      ],
      "excerpt": "code\uac00 \ub09c\ud574\ud55c\ub370, \uc9c1\uad00\uc801\uc73c\ub85c \uc791\ub3d9 \uc6d0\ub9ac\ub9cc \uc774\ud574\ud558\uace0 \ub118\uc5b4\uac00\ub3c4 \ucda9\ubd84\ud558\ub2e4. PositionalEncoding\uc758 \ubaa9\uc801\uc740 positional\uc815\ubcf4(\ub300\ud45c\uc801\uc73c\ub85c token\uc758 \uc21c\uc11c, \uc989 index number)\ub97c \uc815\uaddc\ud654\uc2dc\ud0a4\uae30 \uc704\ud55c \uac83\uc774\ub2e4. \ub2e8\uc21c\ud558\uac8c index number\ub97c positionalEncoding\uc73c\ub85c \uc0ac\uc6a9\ud558\uac8c \ub420 \uacbd\uc6b0, \ub9cc\uc57d training data\uc5d0\uc11c\ub294 \ucd5c\ub300 \ubb38\uc7a5\uc758 \uae38\uc774\uac00 30\uc774\uc5c8\ub294\ub370 test data\uc5d0\uc11c \uae38\uc774 50\uc778 \ubb38\uc7a5\uc774 \ub098\uc624\uac8c \ub41c\ub2e4\uba74 30~49\uc758 index\ub294 model\uc774 \ud559\uc2b5\ud55c \uc801\uc774 \uc5c6\ub294 \uc815\ubcf4\uac00 \ub41c\ub2e4. \uc774\ub294 \uc81c\ub300\ub85c \ub41c \uc131\ub2a5\uc744 \uae30\ub300\ud558\uae30 \uc5b4\ub824\uc6b0\ubbc0\ub85c, positonal \uc815\ubcf4\ub97c \uc77c\uc815\ud55c \ubc94\uc704 \uc548\uc758 \uc2e4\uc218\ub85c \uc81c\uc57d\ud574\ub450\ub294 \uac83\uc774\ub2e4. \uc5ec\uae30\uc11c $sin$\ud568\uc218\uc640 $cos$\ud568\uc218\ub97c \uc0ac\uc6a9\ud558\ub294\ub370, \uc9dd\uc218 index\uc5d0\ub294 $sin$\ud568\uc218\ub97c, \ud640\uc218 index\uc5d0\ub294 $cos$\ud568\uc218\ub97c \uc0ac\uc6a9\ud558\uac8c \ub41c\ub2e4. \uc774\ub97c \uc0ac\uc6a9\ud560 \uacbd\uc6b0 \ud56d\uc0c1 -1\uc5d0\uc11c 1 \uc0ac\uc774\uc758 \uac12\ub9cc\uc774 positional \uc815\ubcf4\ub85c \uc0ac\uc6a9\ub418\uac8c \ub41c\ub2e4. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8524209969009525
      ],
      "excerpt": "\uc6b0\ub9ac\uac00 \uacb0\uad6d \ud574\ub0b4\uace0\uc790 \ud558\ub294 \ubaa9\ud45c\ub294 Decoder\uc758 output\uc774 sentence, \uc989 token\uc758 sequence\uac00 \ub418\ub294 \uac83\uc774\ub2e4. \uadf8\ub7f0\ub370 Decoder\uc758 output\uc740 \uadf8\uc800 ($\\text{n_batch} \\times \\text{seq_len} \\times \\text{d_model}$)\uc758 shape\ub97c \uac16\ub294 matrix\uc77c \ubfd0\uc774\ub2e4. \uc774\ub97c vocabulary\ub97c \uc0ac\uc6a9\ud574 \uc2e4\uc81c token\uc73c\ub85c \ubcc0\ud658\ud560 \uc218 \uc788\ub3c4\ub85d \ucc28\uc6d0\uc744 \uc218\uc815\ud574\uc57c \ud55c\ub2e4. \ub530\ub77c\uc11c FC Layer\ub97c \uac70\uccd0 \ub9c8\uc9c0\ub9c9 dimension\uc744 $\\text{d_model}$\uc5d0\uc11c $\\text{len(vocab)}$\uc73c\ub85c \ubcc0\uacbd\ud55c\ub2e4. \uadf8\ub798\uc57c \uc2e4\uc81c vocabulary \ub0b4 token\uc5d0 \ub300\uc751\uc2dc\ud0ac \uc218 \uc788\ub294 \uac12\uc774 \ub418\uae30 \ub54c\ubb38\uc774\ub2e4. \uc774\ud6c4 softmax \ud568\uc218\ub97c \uc0ac\uc6a9\ud574 \uac01 vocabulary\uc5d0 \ub300\ud55c \ud655\ub960\uac12\uc73c\ub85c \ubcc0\ud658\ud558\uac8c \ub418\ub294\ub370, \uc774 \ub54c log_softmax\ub97c \uc0ac\uc6a9\ud574 \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0a8\ub2e4. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8172342705654815
      ],
      "excerpt": "log_softmax\uc5d0\uc11c\ub294 dim=-1\uc774 \ub418\ub294\ub370, \ub9c8\uc9c0\ub9c9 dimension\uc778 len(vocab)\uc5d0 \ub300\ud55c \ud655\ub960\uac12\uc744 \uad6c\ud574\uc57c \ud558\uae30 \ub54c\ubb38\uc774\ub2e4. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/cpm0722/transformer_pytorch/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "## [\uc6d0\ubcf8 \ud3ec\uc2a4\ud2b8](https://cpm0722.github.io/pytorch-implementation/transformer)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "transformer_pytorch",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "cpm0722",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/cpm0722/transformer_pytorch/blob/main/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Machine Learning\uc5d0 \ub300\ud55c \uae30\ubcf8\uc801\uc778 \uc9c0\uc2dd(Back Propagation, Activation Function, Optimizer, Softmax, KL Divergence, Drop-out, Normalization, Regularization, RNN \ub4f1)\uacfc NLP\uc758 \uae30\ubcf8\uc801\uc778 \uc9c0\uc2dd(tokenizing, word embedding, vocabulary, Machine Translation, BLEU Score \ub4f1)\uc744 \uc548\ub2e4\uace0 \uac00\uc815\ud55c\ub2e4. \ub610\ud55c Python, pytorch\ub97c \uc0ac\uc6a9\ud574 \uac04\ub2e8\ud55c model\uc744 \ub9cc\ub4e4\uc5b4\ub0bc \uc218 \uc788\ub2e4\ub294 \uac83\uc744 \uc804\uc81c\ub85c \ud55c\ub2e4.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Sat, 25 Dec 2021 00:47:11 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "nlp",
      "transformer",
      "pytorch",
      "nlp-model"
    ],
    "technique": "GitHub API"
  }
}