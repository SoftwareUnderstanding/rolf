{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Thanks to Soumith Chintala, this pipeline is largely built on his example ImageNet training code available at:\n[https://github.com/soumith/imagenet-multiGPU.torch](https://github.com/soumith/imagenet-multiGPU.torch)\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1603.06937](http://arxiv.org/abs/1603.06937), 2016.\n\nA pretrained model is available on the [project site](http://www-personal.umich.edu/~alnewell/pose). You can use the option `-loadModel path/to/model` to try fine-tuning. \n\nTo run this code, make sure the following are installed:\n\n- [Torch7](https://github.com/torch/torch7)\n- hdf5\n- cudnn\n\n## Getting Started ##\n\nDownload the full [MPII Human Pose dataset](http://human-pose.mpi-inf.mpg.de), and place the `images` directory in `data/mpii`. From there, it is as simple as running `th main.lua -expID test-run` (the experiment ID is arbitrary). To run on [FLIC](http://bensapp.github.io/flic-dataset.html), again place the images in a directory `data/flic/images` then call `th main.lua -dataset flic -expID test-run`.\n\nMost of the command line options are pretty self-explanatory, and can be found in `src/opts.lua`. The `-expID` option will be used to save important information in a directory like `pose-hg-train/exp/mpii/test-run`. This directory will include snapshots of the trained model, training/validations logs with loss and accuracy information, and details of the options set for that particular experiment.\n\n## Running experiments ##\n\nThere are a couple features to make experiments a bit easier:\n\n- Experiment can be continued with `th main.lua -expID example-exp -continue` it will pick up where the experiment left off with all of the same options set. But let's say you want to change an option like the learning rate, then you can do the same call as above but add the option `-LR 1e-5` for example and it will preserve all old options except for the new learning rate.\n\n- In addition, the `-branch` option allows for the initialization of a new experiment directory leaving the original experiment intact. For example, if you have trained for a while and want to drop the learning rate but don't know what to change it to, you can do something like the following: `th main.lua -branch old-exp -expID new-exp-01 -LR 1e-5` and then compare to a separate experiment `th main.lua -branch old-exp -expID new-exp-02 -LR 5e-5`.\n\nIn `src/misc` there's a simple script for monitoring a set of experiments to visualize and compare training curves.\n\n#### Getting final predictions ####\n\nTo generate final test set predictions for MPII, you can call:\n\n`th main.lua -branch your-exp -expID final-preds -finalPredictions -nEpochs 0`\n\nThis assumes there is an experiment that has already been run. If you just want to provide a pre-trained model, that's fine too and you can call:\n\n`th main.lua -expID final-preds -finalPredictions -nEpochs 0 -loadModel /path/to/model`\n\n#### Training accuracy metric ####\n\nFor convenience during training, the accuracy function evaluates PCK by comparing the output heatmap of the network to the ground truth heatmap. The normalization in this case will be slightly different than the normalization done when officially evaluating on FLIC or MPII. So there will be some discrepancy between the numbers, but the heatmap-based accuracy still provides a good picture of how well the network is learning during training.\n\n## Final notes ##\n\nIn the paper, the training time reported was with an older version of cuDNN, and after switching to cuDNN 4, training time was cut in half. Now, with a Titan X NVIDIA GPU, training time from scratch is under 3 days for MPII, and about 1 day for FLIC.\n\n#### pypose/ ####\n\nIncluded in this repository is a folder with a bunch of old python code that I used. It hasn't been updated in a while, and might not be totally functional at the moment. There are a number of useful functions for doing evaluation and analysis on pose predictions and it is worth digging into. It will be updated and cleaned up soon.\n\n#### Questions? ####\n\nI am sure there is a lot not covered in the README at the moment so please get in touch if you run into any issues or have any questions!\n\n## Acknowledgements ##\n\nThanks to Soumith Chintala, this pipeline is largely built on his example ImageNet training code available at:\n[https://github.com/soumith/imagenet-multiGPU.torch](https://github.com/soumith/imagenet-multiGPU.torch)"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9846359944650075,
        0.9715300342475984,
        0.9929838729698188
      ],
      "excerpt": "Alejandro Newell, Kaiyu Yang, and Jia Deng, \nStacked Hourglass Networks for Human Pose Estimation, \narXiv:1603.06937, 2016. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/princeton-vl/pose-hg-train",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2016-04-19T02:44:28Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-26T07:17:26Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8682589829576759,
        0.908925214220865
      ],
      "excerpt": "This is the training pipeline used for: \nAlejandro Newell, Kaiyu Yang, and Jia Deng, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8646990932387094
      ],
      "excerpt": "A pretrained model is available on the project site. You can use the option -loadModel path/to/model to try fine-tuning.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9754156449394664,
        0.9219355490438736,
        0.9746235952174873
      ],
      "excerpt": "For convenience during training, the accuracy function evaluates PCK by comparing the output heatmap of the network to the ground truth heatmap. The normalization in this case will be slightly different than the normalization done when officially evaluating on FLIC or MPII. So there will be some discrepancy between the numbers, but the heatmap-based accuracy still provides a good picture of how well the network is learning during training. \nIn the paper, the training time reported was with an older version of cuDNN, and after switching to cuDNN 4, training time was cut in half. Now, with a Titan X NVIDIA GPU, training time from scratch is under 3 days for MPII, and about 1 day for FLIC. \nIncluded in this repository is a folder with a bunch of old python code that I used. It hasn't been updated in a while, and might not be totally functional at the moment. There are a number of useful functions for doing evaluation and analysis on pose predictions and it is worth digging into. It will be updated and cleaned up soon. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Training and experimentation code used for \"Stacked Hourglass Networks for Human Pose Estimation\"",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/umich-vl/pose-hg-train/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 191,
      "date": "Sun, 26 Dec 2021 21:21:45 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/princeton-vl/pose-hg-train/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "princeton-vl/pose-hg-train",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/umich-vl/pose-hg-train/master/src/itorch/demo.ipynb",
      "https://raw.githubusercontent.com/umich-vl/pose-hg-train/master/src/itorch/.ipynb_checkpoints/demo-checkpoint.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9237656103494654
      ],
      "excerpt": "To run this code, make sure the following are installed: \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/princeton-vl/pose-hg-train/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python",
      "Lua",
      "MATLAB"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other",
      "url": "https://raw.githubusercontent.com/umich-vl/pose-hg-train/master/LICENSE"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'Large portions of this code were built off:\\nhttps://github.com/soumith/imagenet-multiGPU.torch\\nCopyright (c) 2016, Soumith Chintala\\n\\nFor the rest of the code:\\nCopyright (c) 2016, University of Michigan\\nAll rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met:\\n\\n Redistributions of source code must retain the above copyright notice, this\\n  list of conditions and the following disclaimer.\\n\\n Redistributions in binary form must reproduce the above copyright notice,\\n  this list of conditions and the following disclaimer in the documentation\\n  and/or other materials provided with the distribution.\\n\\n* Neither the name of pose-hg-train nor the names of its\\n  contributors may be used to endorse or promote products derived from\\n  this software without specific prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Stacked Hourglass Networks for Human Pose Estimation (Training Code)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "pose-hg-train",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "princeton-vl",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/princeton-vl/pose-hg-train/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "There are a couple features to make experiments a bit easier:\n\n- Experiment can be continued with `th main.lua -expID example-exp -continue` it will pick up where the experiment left off with all of the same options set. But let's say you want to change an option like the learning rate, then you can do the same call as above but add the option `-LR 1e-5` for example and it will preserve all old options except for the new learning rate.\n\n- In addition, the `-branch` option allows for the initialization of a new experiment directory leaving the original experiment intact. For example, if you have trained for a while and want to drop the learning rate but don't know what to change it to, you can do something like the following: `th main.lua -branch old-exp -expID new-exp-01 -LR 1e-5` and then compare to a separate experiment `th main.lua -branch old-exp -expID new-exp-02 -LR 5e-5`.\n\nIn `src/misc` there's a simple script for monitoring a set of experiments to visualize and compare training curves.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 556,
      "date": "Sun, 26 Dec 2021 21:21:45 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Download the full [MPII Human Pose dataset](http://human-pose.mpi-inf.mpg.de), and place the `images` directory in `data/mpii`. From there, it is as simple as running `th main.lua -expID test-run` (the experiment ID is arbitrary). To run on [FLIC](http://bensapp.github.io/flic-dataset.html), again place the images in a directory `data/flic/images` then call `th main.lua -dataset flic -expID test-run`.\n\nMost of the command line options are pretty self-explanatory, and can be found in `src/opts.lua`. The `-expID` option will be used to save important information in a directory like `pose-hg-train/exp/mpii/test-run`. This directory will include snapshots of the trained model, training/validations logs with loss and accuracy information, and details of the options set for that particular experiment.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "To generate final test set predictions for MPII, you can call:\n\n`th main.lua -branch your-exp -expID final-preds -finalPredictions -nEpochs 0`\n\nThis assumes there is an experiment that has already been run. If you just want to provide a pre-trained model, that's fine too and you can call:\n\n`th main.lua -expID final-preds -finalPredictions -nEpochs 0 -loadModel /path/to/model`\n\n",
      "technique": "Header extraction"
    }
  ]
}