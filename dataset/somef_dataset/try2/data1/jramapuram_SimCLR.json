{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2002.05709",
      "https://arxiv.org/abs/2002.05709"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Cite the original authors on doing some great work:\n\n```\n@article{chen2020simple,\n  title={A Simple Framework for Contrastive Learning of Visual Representations},\n  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},\n  journal={arXiv preprint arXiv:2002.05709},\n  year={2020}\n}\n```\n\nLike this replication? Buy me [a beer](https://github.com/sponsors/jramapuram).\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{chen2020simple,\n  title={A Simple Framework for Contrastive Learning of Visual Representations},\n  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},\n  journal={arXiv preprint arXiv:2002.05709},\n  year={2020}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jramapuram/SimCLR",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-04-26T13:18:15Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-08-23T08:19:31Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9911442413127721,
        0.9902560282728152
      ],
      "excerpt": "An implementation of SimCLR with DistributedDataParallel (1GPU : 1Process) in pytorch. \nThis allows scalability to batch size of 4096 (suggested by authors) using 64 gpus, each with batch size of 64 at a resolution of 224x224x3 in FP32 (see below for FP16 support). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "SimCLR pytorch implementation using DistributedDataParallel.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jramapuram/SimCLR/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 22 Dec 2021 21:05:09 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jramapuram/SimCLR/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "jramapuram/SimCLR",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/jramapuram/SimCLR/master/docker/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/jramapuram/SimCLR/master/docker/run.sh",
      "https://raw.githubusercontent.com/jramapuram/SimCLR/master/slurm/run.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Grab imagenet, [do standard pre-processing](https://github.com/soumith/imagenet-multiGPU.torch#data-processing) and use `--data-dir=${DATA_DIR}`. **Note:** This SimCLR implementation expects two pytorch `imagefolder` locations: `train` and `test` as opposed to `val` in the preprocessor above.\n\n",
      "technique": "Header extraction"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jramapuram/SimCLR/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell",
      "Dockerfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2018 Jason Ramapuram\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "SimCLR-pytorch",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "SimCLR",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "jramapuram",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jramapuram/SimCLR/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 19,
      "date": "Wed, 22 Dec 2021 21:05:09 GMT"
    },
    "technique": "GitHub API"
  },
  "support": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you have GPUs that works well with FP16, you can try the `--half` flag.  \nThis will allow faster training with larger batch sizes (~95 with a 12Gb GPU memory).  \nIf training doesn't work well try chaning the [AMP optimization](https://nvidia.github.io/apex/amp.html#opt-levels) level [here](https://github.com/jramapuram/SimCLR/blob/master/main.py#L590).\n\n",
      "technique": "Header extraction"
    }
  ],
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "**NOTE0**: this will not produce SOTA results, but is good for debugging. The authors use a batch size of 4096+ for SOTA.    \n**NOTE1**: Setup your github ssh tokens; if you get an authentication issue from the git clone this is most likely it.\n\n\n``` bash\n> git clone --recursive git+ssh://git@github.com/jramapuram/SimCLR.git\n#: DATADIR is the location of imagenet or anything that works with imagefolder.\n> ./docker/run.sh \"python main.py --data-dir=$DATADIR \\  \n                                  --batch-size=64 \\  \n                                  --num-replicas=1 \\  \n                                  --epochs=100\" 0  #: add --debug-step to do a single minibatch\n```\nThe bash script `docker/run.sh` pulls the appropriate docker container.  \nIf you want to setup your own environment use:\n  - `environment.yml` (conda) in **addition** to\n  - `requirements.txt` (pip)  \n  \nor just take a look at the Dockerfile in `docker/Dockerfile`.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Setup stuff according to the [slurm bash script](./slurm/run.sh). Then:\n\n``` bash\n> cd slurm && sbatch run.sh\n```\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "  1. Start each replica worker pointing to the master using `--distributed-master=`.\n  2. Set the total number of replicas appropriately using `--num-replicas=`.\n  3. Set each node to have a unique `--distributed-rank=` ranging from `[0, num_replicas)`.\n  3. Ensure network connectivity between workers. You will get NCCL errors if there are resolution problems here.\n  4. Profit.\n  \nFor example, with a 2 node setup run the following on the master node:\n```bash\npython main.py \\\n     --epochs=100 \\\n     --data-dir=<YOUR_DATA_DIR> \\\n     --batch-size=128 \\                   #: divides into 64 per node\n     --convert-to-sync-bn \\\n     --visdom-url=http://MY_VISDOM_URL \\  #: optional, not providing uses tensorboard\n     --visdom-port=8097 \\                 #: optional, not providing uses tensorboard\n     --num-replicas=2 \\                   #: specifies total available nodes, 2 in this example     \n     --distributed-master=127.0.0.1 \\\n     --distributed-port=29301 \\\n     --distributed-rank=0 \\               #: rank-0 is the master\n     --uid=simclrv00_0\n```\n\nand the following on the child node:\n\n```bash\nexport MASTER=<IP_ADDR_OF_MASTER_ABOVE>\npython main.py \\\n     --epochs=100 \\\n     --data-dir=<YOUR_DATA_DIR> \\\n     --batch-size=128 \\                   #: divides into 64 per node\n     --convert-to-sync-bn \\\n     --visdom-url=http://MY_VISDOM_URL \\  #: optional, not providing uses tensorboard\n     --visdom-port=8097 \\                 #: optional, not providing uses tensorboard\n     --num-replicas=2 \\                   #: specifies total available nodes, 2 in this example\n     --distributed-master=$MASTER \\\n     --distributed-port=29301 \\\n     --distributed-rank=1 \\               #: rank-1 is this child, increment for extra nodes\n     --uid=simclrv00_0\n```\n\n\n",
      "technique": "Header extraction"
    }
  ]
}