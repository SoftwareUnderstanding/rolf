{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1512.02325",
      "https://arxiv.org/abs/1708.02002"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8028046190715653
      ],
      "excerpt": "    <td align=center width=\"25%\">07+12<br>0.5</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "    <td><b>SSD300 \"07+12\"</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8028046190715653
      ],
      "excerpt": "  wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8109194328925066
      ],
      "excerpt": "SSD300 Pascal VOC \"07+12\" training summary \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/pierluigiferrari/ssd_keras",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contributing Guidelines\n\nContributions to this repository are welcome, but before you create a pull request, consider the following guidelines:\n\nThe To-do list in the README of this repository defines the main topics for which contributions are welcome. If you want to contribute, ideally contribute to one of the topics listed there.\nIf you'd like to contribute features that are not mentioned on the to-do list in the README, make sure to explain why your proposed change adds value, i.e. what relevant use case it solves. The benefit of any new feature will be compared against the cost of maintaining it and your contribution will be accepter or rejected based on this trade-off.\nOne pull request should be about one specific feature or improvement, i.e. it should not contain multiple unrelated changes. If you want to contribute multiple features and/or improvements, create a separate pull request for every individual feature or improvement.\nWhen you create a pull request, make sure to explain properly\nwhy your propsed change adds value, i.e. what problem or use case it solves,\nall the API changes it will introduce, if any,\nall behavioral changes in any existing parts of the project it will introduce, if any.\n\n\nThis should go without saying, but you are responsible for updating any parts of the code or the tutorial notebooks that are affected by your introduced changes.\nAny submitted code must conform to the coding standards and style of this repository. There is no formal guide for coding standards and style, but here are a few things to note:\nAny new modules, classes or functions must provide proper docstrings unless they are trivial. These docstrings must have sections for Arguments, Returns, and Raises (if applicable). For every argument of a function, the docstring must explain precisely what the argument does, what data type it expects, whether or not it is optional, and any requirements for the range of values it expects. The same goes for the returns. Use existing docstrings as templates.\nNaming:\nClassNames consist of capitalized words without underscores.\nmodule_names.py consist of lower case words connected with underscores.\nfunction_names consist of lower case words connected with underscores.\nvariable_names consist of lower case words connected with underscores.\n\n\nAll module, class, function, and variable names must be descriptive in order to meet the goal that all code should always be as self-explanatory as possible. A longer and descriptive name is always preferable over a shorter and non-descriptive name. Abbreviations are generally to be avoided unless the full words would really make the name too long.\nMore in-line comments are better than fewer in-line comments and all comments should be precise and succinct.",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-04-02T16:20:32Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-29T17:38:52Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8655850181255891
      ],
      "excerpt": "How to fine-tune one of the trained models on your own dataset \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9598781254320428,
        0.9619661546391137,
        0.9971504672726538,
        0.9650103292423254
      ],
      "excerpt": "This is a Keras port of the SSD model architecture introduced by Wei Liu et al. in the paper SSD: Single Shot MultiBox Detector. \nPorts of the trained weights of all the original models are provided below. This implementation is accurate, meaning that both the ported weights and models trained from scratch produce the same mAP values as the respective models of the original Caffe implementation (see performance section below). \nThe main goal of this project is to create an SSD implementation that is well documented for those who are interested in a low-level understanding of the model. The provided tutorials, documentation and detailed comments hopefully make it a bit easier to dig into the code and adapt or build upon the model than with most other implementations out there (Keras or otherwise) that provide little to no documentation and comments. \nThe repository currently provides the following network architectures: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8532376281526585,
        0.8706495730706248
      ],
      "excerpt": "* SSD7: keras_ssd7.py - a smaller 7-layer version that can be trained from scratch relatively quickly even on a mid-tier GPU, yet is capable enough for less complex object detection tasks and testing. You're obviously not going to get state-of-the-art results with that one, but it's fast. \nIf you would like to use one of the provided trained models for transfer learning (i.e. fine-tune one of the trained models on your own dataset), there is a Jupyter notebook tutorial that helps you sub-sample the trained weights so that they are compatible with your dataset, see further below. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9616348486383504
      ],
      "excerpt": "Here are the mAP evaluation results of the ported weights and below that the evaluation results of a model trained from scratch using this implementation. All models were evaluated using the official Pascal VOC test server (for 2012 test) or the official Pascal VOC Matlab evaluation script (for 2007 test). In all cases the results match (or slightly surpass) those of the original Caffe models. Download links to all ported weights are available further below. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8640215378546516
      ],
      "excerpt": "Training an SSD300 from scratch to convergence on Pascal VOC 2007 trainval and 2012 trainval produces the same mAP on Pascal VOC 2007 test as the original Caffe SSD300 \"07+12\" model. You can find a summary of the training here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9700013920307615
      ],
      "excerpt": "The models achieve the following average number of frames per second (FPS) on Pascal VOC on an NVIDIA GeForce GTX 1070 mobile (i.e. the laptop version) and cuDNN v6. There are two things to note here. First, note that the benchmark prediction speeds of the original Caffe implementation were achieved using a TitanX GPU and cuDNN v4. Second, the paper says they measured the prediction speed at batch size 8, which I think isn't a meaningful way of measuring the speed. The whole point of measuring the speed of a detection model is to know how many individual sequential images the model can process per second, therefore measuring the prediction speed on batches of images and then deducing the time spent on each individual image in the batch defeats the purpose. For the sake of comparability, below you find the prediction speed for the original Caffe SSD implementation and the prediction speed for this implementation under the same conditions, i.e. at batch size 8. In addition you find the prediction speed for this implementation at batch size 1, which in my opinion is the more meaningful number. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8950599359340804,
        0.8872619759597105
      ],
      "excerpt": "The general training setup is layed out and explained in ssd7_training.ipynb and in ssd300_training.ipynb. The setup and explanations are similar in both notebooks for the most part, so it doesn't matter which one you look at to understand the general training setup, but the parameters in ssd300_training.ipynb are preset to copy the setup of the original Caffe implementation for training on Pascal VOC, while the parameters in ssd7_training.ipynb are preset to train on the Udacity traffic datasets. \nTo train the original SSD300 model on Pascal VOC: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8681487815616727,
        0.9712485756090703
      ],
      "excerpt": "Set the file paths for the datasets and model weights accordingly in ssd300_training.ipynb and execute the cells. \nThe procedure for training SSD512 is the same of course. It is imperative that you load the pre-trained VGG-16 weights when attempting to train an SSD300 or SSD512 from scratch, otherwise the training will probably fail. Here is a summary of a full training of the SSD300 \"07+12\" model for comparison with your own training: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9768618841626172,
        0.9506222461701407,
        0.9691450024810404,
        0.9361783901790706
      ],
      "excerpt": "The ssd_encoder_decoder sub-package contains all functions and classes related to encoding and decoding boxes. Encoding boxes means converting ground truth labels into the target format that the loss function needs during training. It is this encoding process in which the matching of ground truth boxes to anchor boxes (the paper calls them default boxes and in the original C++ code they are called priors - all the same thing) happens. Decoding boxes means converting raw model output back to the input label format, which entails various conversion and filtering processes such as non-maximum suppression (NMS). \nIn order to train the model, you need to create an instance of SSDInputEncoder that needs to be passed to the data generator. The data generator does the rest, so you don't usually need to call any of SSDInputEncoder's methods manually. \nModels can be created in 'training' or 'inference' mode. In 'training' mode, the model outputs the raw prediction tensor that still needs to be post-processed with coordinate conversion, confidence thresholding, non-maximum suppression, etc. The functions decode_detections() and decode_detections_fast() are responsible for that. The former follows the original Caffe implementation, which entails performing NMS per object class, while the latter performs NMS globally across all object classes and is thus more efficient, but also behaves slightly differently. Read the documentation for details about both functions. If a model is created in 'inference' mode, its last layer is the DecodeDetections layer, which performs all the post-processing that decode_detections() does, but in TensorFlow. That means the output of the model is already the post-processed output. In order to be trainable, a model must be created in 'training' mode. The trained weights can then later be loaded into a model that was created in 'inference' mode. \nA note on the anchor box offset coordinates used internally by the model: This may or may not be obvious to you, but it is important to understand that it is not possible for the model to predict absolute coordinates for the predicted bounding boxes. In order to be able to predict absolute box coordinates, the convolutional layers responsible for localization would need to produce different output values for the same object instance at different locations within the input image. This isn't possible of course: For a given input to the filter of a convolutional layer, the filter will produce the same output regardless of the spatial position within the image because of the shared weights. This is the reason why the model predicts offsets to anchor boxes instead of absolute coordinates, and why during training, absolute ground truth coordinates are converted to anchor box offsets in the encoding process. The fact that the model predicts offsets to anchor box coordinates is in turn the reason why the model contains anchor box layers that do nothing but output the anchor box coordinates so that the model's output tensor can include those. If the model's output tensor did not contain the anchor box coordinates, the information to convert the predicted offsets back to absolute coordinates would be missing in the model output. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8332534104174826,
        0.9644850720532333,
        0.8720355287547324,
        0.8732450767671174
      ],
      "excerpt": "The following things are on the to-do list, ranked by priority. Contributions are welcome, but please read the contributing guidelines. \nAdd model definitions and trained weights for SSDs based on other base networks such as MobileNet, InceptionResNetV2, or DenseNet. \nAdd support for the Theano and CNTK backends. Requires porting the custom layers and the loss function from TensorFlow to the abstract Keras backend. \nCurrently in the works: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9610834078384897
      ],
      "excerpt": "\"Labels\": For the purpose of this project, datasets consist of \"images\" and \"labels\". Everything that belongs to the annotations of a given image is the \"labels\" of that image: Not just object category labels, but also bounding box coordinates. \"Labels\" is just shorter than \"annotations\". I also use the terms \"labels\" and \"targets\" more or less interchangeably throughout the documentation, although \"targets\" means labels specifically in the context of training. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A Keras port of Single Shot MultiBox Detector",
      "technique": "GitHub API"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "In order to train an SSD300 or SSD512 from scratch, download the weights of the fully convolutionalized VGG-16 model trained to convergence on ImageNet classification here:\n\n[`VGG_ILSVRC_16_layers_fc_reduced.h5`](https://drive.google.com/open?id=1sBmajn6vOE7qJ8GnxUJt4fGPuffVUZox).\n\nAs with all other weights files below, this is a direct port of the corresponding `.caffemodel` file that is provided in the repository of the original Caffe implementation.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Here are the ported weights for all the original trained models. The filenames correspond to their respective `.caffemodel` counterparts. The asterisks and footnotes refer to those in the README of the [original Caffe implementation](https://github.com/weiliu89/caffe/tree/ssd#models).\n\n1. PASCAL VOC models:\n\n    * 07+12: [SSD300*](https://drive.google.com/open?id=121-kCXaOHOkJE_Kf5lKcJvC_5q1fYb_q), [SSD512*](https://drive.google.com/open?id=19NIa0baRCFYT3iRxQkOKCD7CpN6BFO8p)\n    * 07++12: [SSD300*](https://drive.google.com/open?id=1M99knPZ4DpY9tI60iZqxXsAxX2bYWDvZ), [SSD512*](https://drive.google.com/open?id=18nFnqv9fG5Rh_fx6vUtOoQHOLySt4fEx)\n    * COCO[1]: [SSD300*](https://drive.google.com/open?id=17G1J4zEpFwiOzgBmq886ci4P3YaIz8bY), [SSD512*](https://drive.google.com/open?id=1wGc368WyXSHZOv4iow2tri9LnB0vm9X-)\n    * 07+12+COCO: [SSD300*](https://drive.google.com/open?id=1vtNI6kSnv7fkozl7WxyhGyReB6JvDM41), [SSD512*](https://drive.google.com/open?id=14mELuzm0OvXnwjb0mzAiG-Ake9_NP_LQ)\n    * 07++12+COCO: [SSD300*](https://drive.google.com/open?id=1fyDDUcIOSjeiP08vl1WCndcFdtboFXua), [SSD512*](https://drive.google.com/open?id=1a-64b6y6xsQr5puUsHX_wxI1orQDercM)\n\n\n2. COCO models:\n\n    * trainval35k: [SSD300*](https://drive.google.com/open?id=1vmEF7FUsWfHquXyCqO17UaXOPpRbwsdj), [SSD512*](https://drive.google.com/open?id=1IJWZKmjkcFMlvaz2gYukzFx4d6mH3py5)\n\n\n3. ILSVRC models:\n\n    * trainval1: [SSD300*](https://drive.google.com/open?id=1VWkj1oQS2RUhyJXckx3OaDYs5fx2mMCq), [SSD500](https://drive.google.com/open?id=1LcBPsd9CJbuBw4KiSuE1o1fMA-Pz2Zvw)\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/pierluigiferrari/ssd_keras/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 920,
      "date": "Wed, 29 Dec 2021 20:26:23 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/pierluigiferrari/ssd_keras/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "pierluigiferrari/ssd_keras",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/pierluigiferrari/ssd_keras/master/ssd300_inference.ipynb",
      "https://raw.githubusercontent.com/pierluigiferrari/ssd_keras/master/ssd300_evaluation_COCO.ipynb",
      "https://raw.githubusercontent.com/pierluigiferrari/ssd_keras/master/ssd7_training.ipynb",
      "https://raw.githubusercontent.com/pierluigiferrari/ssd_keras/master/ssd512_inference.ipynb",
      "https://raw.githubusercontent.com/pierluigiferrari/ssd_keras/master/weight_sampling_tutorial.ipynb",
      "https://raw.githubusercontent.com/pierluigiferrari/ssd_keras/master/ssd300_evaluation.ipynb",
      "https://raw.githubusercontent.com/pierluigiferrari/ssd_keras/master/ssd300_training.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8494567821500104
      ],
      "excerpt": "Download the convolutionalized VGG-16 weights \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8020605654110581
      ],
      "excerpt": "If you would like to use one of the provided trained models for transfer learning (i.e. fine-tune one of the trained models on your own dataset), there is a Jupyter notebook tutorial that helps you sub-sample the trained weights so that they are compatible with your dataset, see further below. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8165637148334034
      ],
      "excerpt": "Download the weights for the convolutionalized VGG-16 or for one of the trained original models provided below. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8016808366254073
      ],
      "excerpt": "Download the original trained model weights \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991,
        0.9336801098518991
      ],
      "excerpt": "* SSD300: keras_ssd300.py \n* SSD512: keras_ssd512.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8014683806463279,
        0.8066519693843278
      ],
      "excerpt": "To train the original SSD300 model on Pascal VOC: \nDownload the datasets: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8164055470415279,
        0.8137355381761607
      ],
      "excerpt": "Set the file paths for the datasets and model weights accordingly in ssd300_training.ipynb and execute the cells. \nThe procedure for training SSD512 is the same of course. It is imperative that you load the pre-trained VGG-16 weights when attempting to train an SSD300 or SSD512 from scratch, otherwise the training will probably fail. Here is a summary of a full training of the SSD300 \"07+12\" model for comparison with your own training: \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/pierluigiferrari/ssd_keras/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Apache License 2.0",
      "url": "https://api.github.com/licenses/apache-2.0"
    },
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# SSD: Single-Shot MultiBox Detector implementation in Keras",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "ssd_keras",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "pierluigiferrari",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/pierluigiferrari/ssd_keras/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "pierluigiferrari",
        "body": "## Release 0.9.0\r\n\r\n### Breaking Changes\r\n\r\n- None\r\n\r\n### Major Features and Improvements\r\n\r\n- Added a new, flexible `Evaluator` class that computes average precision scores. Among other things, it can compute average precisions according to both the Pascal VOC pre-2010 and post-2010 algorithms.\r\n- Added two new features to `DataGenerator`:\r\n    1. Convert the dataset into an HDF5 file: This stores the images of a dataset as uncompressed arrays in a contiguous block of memory within an HDF5 file, which requires a lot of disk space but reduces the image loading times during the batch generation.\r\n    2. Load the entire dataset into memory: This loads all images of a dataset into memory, thereby eliminating image loading times altogether. Requires enough memory to hold the entire dataset.\r\n\r\nFor several minor other improvements please refer to the commits since the last release (v0.8.0).\r\n\r\n### Bug Fixes and Other Changes\r\n\r\n- Fixed a bug in `DataGenerator.parse_xml()`: Before the fix there were cases in which the XML parser would parse the wrong bounding boxes for some objects. The only known situation in which this bug occurred is for the 'person' class of the Pascal VOC datasets, where the ground truth provides not only a bounding box for the person itself, but also additional bounding boxes for various body parts such. Depending on the order of these ground truth boxes within the XML files, the parser would sometimes parse the bounding box of a body part instead of the bounding box of the person. The parser now loads the correct bounding boxes in these cases.\r\n- Provided a better training/validation split for the Udacity traffic dataset. The new split is much more balanced than the old one.\r\n\r\n### API Changes\r\n\r\n- None\r\n\r\n### Known Issues\r\n\r\n- None",
        "dateCreated": "2018-05-03T20:29:30Z",
        "datePublished": "2018-05-06T16:47:40Z",
        "html_url": "https://github.com/pierluigiferrari/ssd_keras/releases/tag/v0.9.0",
        "name": "Keras SSD v0.9.0",
        "tag_name": "v0.9.0",
        "tarball_url": "https://api.github.com/repos/pierluigiferrari/ssd_keras/tarball/v0.9.0",
        "url": "https://api.github.com/repos/pierluigiferrari/ssd_keras/releases/10871813",
        "zipball_url": "https://api.github.com/repos/pierluigiferrari/ssd_keras/zipball/v0.9.0"
      },
      {
        "authorType": "User",
        "author_name": "pierluigiferrari",
        "body": "## Release 0.8.0\r\n\r\n### Breaking Changes\r\n\r\n- None\r\n\r\n### Major Features and Improvements\r\n\r\n- Improved the matching algorithm. While the previous version had a few flaws, the new version is identical to the matching in the original Caffe implementation. Training a model with this new version reproduces the mAP results of the original Caffe SSD models exactly.\r\n- Added two new data augmentation chains: One for variable-size input images that produces effects similar to the original SSD data augmentation chain, but is a lot faster, and a second one for bird's eye-view datasets.\r\n\r\n### API Changes\r\n\r\n- None\r\n\r\n### Known Issues\r\n\r\n- None",
        "dateCreated": "2018-04-19T23:07:05Z",
        "datePublished": "2018-04-19T23:18:57Z",
        "html_url": "https://github.com/pierluigiferrari/ssd_keras/releases/tag/v0.8.0",
        "name": "Keras SSD v0.8.0",
        "tag_name": "v0.8.0",
        "tarball_url": "https://api.github.com/repos/pierluigiferrari/ssd_keras/tarball/v0.8.0",
        "url": "https://api.github.com/repos/pierluigiferrari/ssd_keras/releases/10634538",
        "zipball_url": "https://api.github.com/repos/pierluigiferrari/ssd_keras/zipball/v0.8.0"
      },
      {
        "authorType": "User",
        "author_name": "pierluigiferrari",
        "body": "## Release 0.7.0\r\n\r\n### Breaking Changes\r\n\r\n- Introduced a new data generator.\r\n\r\n### Major Features and Improvements\r\n\r\n- Introduced a new data generator that has several advantages over the old data generator:\r\n    - It can replicate the data augmentation pipeline of the original Caffe SSD implementation.\r\n    - It's very flexible: Image transformations are no longer hard-coded into the generator itself. Instead, the generator takes a list of transformation objects that it applies to the data. This allows you to realize arbitrary image processing chains. In particular, you can now put transformations in any order or even have multiple parallel transformation chains from which one chain is randomly chosen. The generator comes with a number of useful image transformation classes that can be used out of the box. Among them are most common photometric and geometric transformations, and, in particular, many useful patch sampling transformations.\r\n\r\n### API Changes\r\n\r\nThe API of the new data generator is not compatible with the old data generator.\r\n\r\n### Known Issues\r\n\r\nNone",
        "dateCreated": "2018-03-26T00:10:45Z",
        "datePublished": "2018-03-26T00:24:00Z",
        "html_url": "https://github.com/pierluigiferrari/ssd_keras/releases/tag/v0.7.0",
        "name": "Keras SSD v0.7.0",
        "tag_name": "v0.7.0",
        "tarball_url": "https://api.github.com/repos/pierluigiferrari/ssd_keras/tarball/v0.7.0",
        "url": "https://api.github.com/repos/pierluigiferrari/ssd_keras/releases/10251135",
        "zipball_url": "https://api.github.com/repos/pierluigiferrari/ssd_keras/zipball/v0.7.0"
      },
      {
        "authorType": "User",
        "author_name": "pierluigiferrari",
        "body": "## Release 0.6.0\r\n\r\n### Breaking Changes\r\n\r\n- Changed the repository structure: Modules are now arranged in packages.\r\n\r\n### Major Features and Improvements\r\n\r\n- Introduced a new `DecodeDetections` layer type that corresponds to the `DetectionOutput` layer type of the original Caffe implementation. It performs the decoding and filtering (confidence thresholding, NMS, etc.) of the raw model output and follows the exact procedure of the `decode_y()` function. The point is to move the computationally expensive decoding and filtering process from the CPU (`decode_y()`) to the GPU for faster prediction. Along with `DecodeDetections`, a second version `DecodeDetections2` has been added. It follows the exact procedure of `decode_y2()` and is significantly faster than `DecodeDetections`, but potentially at the cost of lower prediction accuracy - this has not been tested extensively. The introduction of this new layer type also means that the API of the model builder functions has been expanded: Models can now be built in one of three modes:\r\n    1. `training`: The default mode. Produces the same models as before, where the model outputs the raw predictions that need to be decoded by `decode_y()` or `decode_y2()`.\r\n    2. `inference`: Adds a `DecodeDetections` layer to the model as its final layer. The resulting model outputs predictions that are already decoded and filtered. However, since tensors are homogeneous in size along all axes, there will always be `top_k` predictions for each batch item, regardless of how many objects actually are in it, so the output still needs to be confidence-thresholded to remove the dummy entries among the predictions. The inference tutorials show how to do this.\r\n    3. `inference_fast`: Same as `inference`, but using a `DecodeDetections2` layer as the model's last layer.\r\n\r\n### Bug Fixes and Other Changes\r\n\r\n- Changed the repository structure: Modules are now arranged in packages.\r\n\r\n### API Changes\r\n\r\n- With the introduction of the new `DecodeDetections` layer type, the API of all model builder functions has changed to include a new `mode` parameter and `confidence_thresh`, `iou_threshold`, `top_k`, and `nms_max_output_size` parameters, all of which assume default values. `mode` defaults to `training`, in which case the resulting model is the same as before, so this is not a breaking change. `mode` can also be set to `inference` or `inference_fast` upon creation of the model though, in which case the resulting model has the `DecodeDetections` or `DecodeDetections2` layer as its last layer.\r\n\r\n### Known Issues\r\n\r\nNone",
        "dateCreated": "2018-03-05T15:07:11Z",
        "datePublished": "2018-03-05T15:41:06Z",
        "html_url": "https://github.com/pierluigiferrari/ssd_keras/releases/tag/v0.6.0",
        "name": "Keras SSD v0.6.0",
        "tag_name": "v0.6.0",
        "tarball_url": "https://api.github.com/repos/pierluigiferrari/ssd_keras/tarball/v0.6.0",
        "url": "https://api.github.com/repos/pierluigiferrari/ssd_keras/releases/9942575",
        "zipball_url": "https://api.github.com/repos/pierluigiferrari/ssd_keras/zipball/v0.6.0"
      },
      {
        "authorType": "User",
        "author_name": "pierluigiferrari",
        "body": "## Release 0.5.0\r\n\r\n### Breaking Changes\r\n\r\nNone\r\n\r\n### Major Features and Improvements\r\n\r\n- Ports of the weights of all trained original models\r\n- Evaluation results on Pascal VOC\r\n- Tools for evaluation on Pascal VOC and MS COCO\r\n- Tutorials for training, inference, evaluation, and weight sub-sampling\r\n\r\n### Bug Fixes and Other Changes\r\n\r\n- Fixed random sampling in the weight sub-sampling procedure\r\n\r\n### API Changes\r\n\r\nNone\r\n\r\n### Known Issues\r\n\r\nNone",
        "dateCreated": "2018-02-28T22:58:45Z",
        "datePublished": "2018-03-04T17:17:59Z",
        "html_url": "https://github.com/pierluigiferrari/ssd_keras/releases/tag/v0.5.0",
        "name": "Keras SSD v0.5.0",
        "tag_name": "v0.5.0",
        "tarball_url": "https://api.github.com/repos/pierluigiferrari/ssd_keras/tarball/v0.5.0",
        "url": "https://api.github.com/repos/pierluigiferrari/ssd_keras/releases/9930104",
        "zipball_url": "https://api.github.com/repos/pierluigiferrari/ssd_keras/zipball/v0.5.0"
      }
    ],
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* Python 3.x\n* Numpy\n* TensorFlow 1.x\n* Keras 2.x\n* OpenCV\n* Beautiful Soup 4.x\n\nThe Theano and CNTK backends are currently not supported.\n\nPython 2 compatibility: This implementation seems to work with Python 2.7, but I don't provide any support for it. It's 2018 and nobody should be using Python 2 anymore.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1765,
      "date": "Wed, 29 Dec 2021 20:26:23 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "ssd",
      "keras",
      "ssd-model",
      "object-detection",
      "computer-vision",
      "deep-learning",
      "fcn",
      "fully-convolutional-networks",
      "keras-models",
      "single-shot-multibox-detector"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Below are some prediction examples of the fully trained original SSD300 \"07+12\" model (i.e. trained on Pascal VOC2007 `trainval` and VOC2012 `trainval`). The predictions were made on Pascal VOC2007 `test`.\n\n| | |\n|---|---|\n| ![img01](./examples/trained_ssd300_pascalVOC2007_test_pred_05_no_gt.png) | ![img01](./examples/trained_ssd300_pascalVOC2007_test_pred_04_no_gt.png) |\n| ![img01](./examples/trained_ssd300_pascalVOC2007_test_pred_01_no_gt.png) | ![img01](./examples/trained_ssd300_pascalVOC2007_test_pred_02_no_gt.png) |\n\nHere are some prediction examples of an SSD7 (i.e. the small 7-layer version) partially trained on two road traffic datasets released by [Udacity](https://github.com/udacity/self-driving-car/tree/master/annotations) with roughly 20,000 images in total and 5 object categories (more info in [`ssd7_training.ipynb`](ssd7_training.ipynb)). The predictions you see below were made after 10,000 training steps at batch size 32. Admittedly, cars are comparatively easy objects to detect and I picked a few of the better examples, but it is nonetheless remarkable what such a small model can do after only 10,000 training iterations.\n\n| | |\n|---|---|\n| ![img01](./examples/ssd7_udacity_traffic_pred_01.png) | ![img01](./examples/ssd7_udacity_traffic_pred_02.png) |\n| ![img01](./examples/ssd7_udacity_traffic_pred_03.png) | ![img01](./examples/ssd7_udacity_traffic_pred_04.png) |\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "This repository provides Jupyter notebook tutorials that explain training, inference and evaluation, and there are a bunch of explanations in the subsequent sections that complement the notebooks.\n\nHow to use a trained model for inference:\n* [`ssd300_inference.ipynb`](ssd300_inference.ipynb)\n* [`ssd512_inference.ipynb`](ssd512_inference.ipynb)\n\nHow to train a model:\n* [`ssd300_training.ipynb`](ssd300_training.ipynb)\n* [`ssd7_training.ipynb`](ssd7_training.ipynb)\n\nHow to use one of the provided trained models for transfer learning on your own dataset:\n* [Read below](#how-to-fine-tune-one-of-the-trained-models-on-your-own-dataset)\n\nHow to evaluate a trained model:\n* In general: [`ssd300_evaluation.ipynb`](ssd300_evaluation.ipynb)\n* On MS COCO: [`ssd300_evaluation_COCO.ipynb`](ssd300_evaluation_COCO.ipynb)\n\nHow to use the data generator:\n* The data generator used here has its own repository with a detailed tutorial [here](https://github.com/pierluigiferrari/data_generator_object_detection_2d)\n\n",
      "technique": "Header extraction"
    }
  ]
}