{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1801.05746\n.. _`U-Net`: https://arxiv.org/abs/1505.04597\n.. _`AlbuNet34`: https://arxiv.org/abs/1803.01207\n.. _`LinkNet`: https://arxiv.org/abs/1707.03718\n.. _`google drive`: https://drive.google.com/drive/folders/1V_bLBTzsl_Z8Ln9Iq8gjcFDxodfiHxul\n\n.. |br| raw:: html\n\n   <br />\n\n.. |plusmn| raw:: html\n\n   &plusmn\n\n.. |times| raw:: html\n\n   &times\n\n.. |micro| raw:: html\n\n   &microm\n\n.. |y| image:: https://hsto.org/webt/jm/sn/i0/jmsni0y8mao8vnaij8a4eyuoqmu.gif\n.. |y_hat| image:: https://hsto.org/webt/xf/j2/a4/xfj2a4obgqhdzneysar5_us5pgk.gif\n.. |i| image:: https://hsto.org/webt/87/cc/ca/87cccaz4gjp2lgyeip17utljvvi.gif",
      "https://arxiv.org/abs/1505.04597\n.. _`AlbuNet34`: https://arxiv.org/abs/1803.01207\n.. _`LinkNet`: https://arxiv.org/abs/1707.03718\n.. _`google drive`: https://drive.google.com/drive/folders/1V_bLBTzsl_Z8Ln9Iq8gjcFDxodfiHxul\n\n.. |br| raw:: html\n\n   <br />\n\n.. |plusmn| raw:: html\n\n   &plusmn\n\n.. |times| raw:: html\n\n   &times\n\n.. |micro| raw:: html\n\n   &microm\n\n.. |y| image:: https://hsto.org/webt/jm/sn/i0/jmsni0y8mao8vnaij8a4eyuoqmu.gif\n.. |y_hat| image:: https://hsto.org/webt/xf/j2/a4/xfj2a4obgqhdzneysar5_us5pgk.gif\n.. |i| image:: https://hsto.org/webt/87/cc/ca/87cccaz4gjp2lgyeip17utljvvi.gif",
      "https://arxiv.org/abs/1803.01207\n.. _`LinkNet`: https://arxiv.org/abs/1707.03718\n.. _`google drive`: https://drive.google.com/drive/folders/1V_bLBTzsl_Z8Ln9Iq8gjcFDxodfiHxul\n\n.. |br| raw:: html\n\n   <br />\n\n.. |plusmn| raw:: html\n\n   &plusmn\n\n.. |times| raw:: html\n\n   &times\n\n.. |micro| raw:: html\n\n   &microm\n\n.. |y| image:: https://hsto.org/webt/jm/sn/i0/jmsni0y8mao8vnaij8a4eyuoqmu.gif\n.. |y_hat| image:: https://hsto.org/webt/xf/j2/a4/xfj2a4obgqhdzneysar5_us5pgk.gif\n.. |i| image:: https://hsto.org/webt/87/cc/ca/87cccaz4gjp2lgyeip17utljvvi.gif",
      "https://arxiv.org/abs/1707.03718\n.. _`google drive`: https://drive.google.com/drive/folders/1V_bLBTzsl_Z8Ln9Iq8gjcFDxodfiHxul\n\n.. |br| raw:: html\n\n   <br />\n\n.. |plusmn| raw:: html\n\n   &plusmn\n\n.. |times| raw:: html\n\n   &times\n\n.. |micro| raw:: html\n\n   &microm\n\n.. |y| image:: https://hsto.org/webt/jm/sn/i0/jmsni0y8mao8vnaij8a4eyuoqmu.gif\n.. |y_hat| image:: https://hsto.org/webt/xf/j2/a4/xfj2a4obgqhdzneysar5_us5pgk.gif\n.. |i| image:: https://hsto.org/webt/87/cc/ca/87cccaz4gjp2lgyeip17utljvvi.gif"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find this work useful for your publications, please consider citing::\n\n    @inproceedings{shvets2018angiodysplasia,\n    title={Angiodysplasia Detection and Localization using Deep Convolutional Neural Networks},\n    author={Shvets, Alexey A and Iglovikov, Vladimir I and Rakhlin, Alexander and Kalinin, Alexandr A},\n    booktitle={2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA)},\n    pages={612--617},\n    year={2018}\n    }\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{shvets2018angiodysplasia,\ntitle={Angiodysplasia Detection and Localization using Deep Convolutional Neural Networks},\nauthor={Shvets, Alexey A and Iglovikov, Vladimir I and Rakhlin, Alexander and Kalinin, Alexandr A},\nbooktitle={2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA)},\npages={612--617},\nyear={2018}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9753110628106604
      ],
      "excerpt": ".. figure:: https://habrastorage.org/webt/if/5p/tj/if5ptjnbzeswfgqderpcww0sstm.jpeg \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8374695145293155
      ],
      "excerpt": ".. figure:: https://hsto.org/webt/nq/3v/wf/nq3vwfqtoutrzmnbzmrnyligwym.png \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8374695145293155
      ],
      "excerpt": ".. figure:: https://habrastorage.org/webt/t3/p6/yy/t3p6yykecrvr9mim7fqgevodgu4.png \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8374695145293155
      ],
      "excerpt": ".. figure:: https://hsto.org/webt/vz/ok/wt/vzokwtntgqe6lb-g2oyhzj0qcyo.png \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9586571307183652
      ],
      "excerpt": "    &lt;img src=\"https://hsto.org/webt/r2/bt/ck/r2btckzdgnau9-kwsrzvryezclk.gif\" align=\"center\"/&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8906174419333412
      ],
      "excerpt": ".. figure:: https://habrastorage.org/webt/_8/wc/j1/_8wcj1to6ahxfsmb8s3nrxumqjy.gif \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8374695145293155
      ],
      "excerpt": ".. figure:: https://hsto.org/webt/mw/yj/-l/mwyj-l6ddk6xz-ykydduixzhrdk.png \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9278824608274014
      ],
      "excerpt": "U-Net         73.18     83.06     21 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8906174419333412
      ],
      "excerpt": "AlbuNet34     75.35     84.98     30 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ternaus/angiodysplasia-segmentation",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-03-29T03:35:07Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-08T09:30:48Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9833772534922407
      ],
      "excerpt": "Here we present our wining solution and its further development for MICCAI 2017 Endoscopic Vision Challenge Angiodysplasia Detection and Localization_. It addresses binary segmentation problem, where every pixel in image is labeled as an angiodysplasia lesions or background. Then, we analyze connected component of each predicted mask. Based on the analysis we developed a classifier that predict angiodysplasia lesions (binary variable) and a detector for their localization (center of a component). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9991490852536056
      ],
      "excerpt": "Angiodysplasias are degenerative lesions of previously healthy blood vessels, in which the bowel wall have microvascular abnormalities. These lesions are the most common source of small bowel bleeding in patients older than 50 years, and cause approximately 8% of all gastrointestinal bleeding episodes. Gold-standard examination for angiodysplasia detection and localization in the small bowel is performed using Wireless Capsule Endoscopy (WCE). Last generation of this pill-like device is able to acquire more than 60 000 images with a resolution of approximately 520*520 pixels. According to the latest state-of-the art, only 69% of angiodysplasias are detected by gastroenterologist experts during the reading of WCE videos, and blood indicator software (provided by WCE provider like Given Imaging), in the presence of angiodysplasias, presents sensitivity and specificity values of only 41% and 67%, respectively. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9757811746818097
      ],
      "excerpt": "The dataset consists of 1200 color images obtained with WCE. The images are in 24-bit PNG format, with 576 |times| 576 pixel resolution. The dataset is split into two equal parts, 600 images for training and 600 for evaluation. Each subset is composed of 300 images with apparent AD and 300 without any pathology. The training subset is annotated by human expert and contains 300 binary masks in JPEG format of the same 576 |times| 576 pixel resolution. White pixels in the masks correspond to lesion localization. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8173399381240779
      ],
      "excerpt": "First row corresponds to images without pathology, the second row to images with several AD lesions in every image, and the last row contains masks that correspond to the pathology images from the second row. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9123335692073884
      ],
      "excerpt": "We evaluate 4 different deep architectures for segmentation: U-Net (Ronneberger et al., 2015; Iglovikov et al., 2017a), 2 modifications of TernausNet (Iglovikov and Shvets, 2018), and AlbuNet34, a modifications of LinkNet (Chaurasia and Culurciello, 2017; Shvets et al., 2018). As an improvement over standard U-Net, we use similar networks with pre-trained encoders. TernausNet (Iglovikov and Shvets, 2018) is a U-Net-like architecture that uses relatively simple pre-trained VGG11 or VGG16 (Simonyan and Zisserman, 2014) networks as an encoder. VGG11 consists of seven convolutional layers, each followed by a ReLU activation function, and  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9164722400538353
      ],
      "excerpt": "Since an image consists of pixels, the expression can be adapted for discrete objects in the following way: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8938147275827837,
        0.8011076869449261
      ],
      "excerpt": "where |y| and |y_hat| are a binary value (label) and a predicted probability for the pixel |i|, respectively. \nSince image segmentation task can also be considered as a pixel classification problem, we additionally use common classification loss functions, denoted as H. For a binary segmentation problem H is a binary cross entropy, while for a multi-class segmentation problem H is a categorical cross entropy. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9736121938398399,
        0.8993077190452162
      ],
      "excerpt": "As an output of a model, we obtain an image, in which each pixel value corresponds to a probability of belonging to the area of interest or a class. The size of the output image matches the input image size. For binary segmentation, we use 0.3 as a threshold value (chosen using validation dataset) to binarize pixel probabilities. All pixel values below the speci \ned threshold are set to 0, while all values above the threshold are set to 255 to produce final prediction mask. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9887197412854084,
        0.9955881863842192
      ],
      "excerpt": "nd the coordinates of angiodysplasia lesions in the image. In the postprocessing step we use OpenCV implementation of connected component labeling function connectedComponentsWithStats. This function returns the number of connected components, their sizes (areas), and centroid coordinates of the corresponding connected component. In our detector we use another threshold to neglect all clusters with the size smaller than 300 pixels. Therefore, in order to establish the presence of the lesions, the number of found components should be higher than 0, otherwise the image corresponds to a normal condition. Then, for localization of angiodysplasia lesions we return centroid coordinates of all connected components. \nThe quantitative comparison of our models' performance is presented in the Table 1. For the segmentation task the best results is achieved by AlbuNet34 providing IoU = 0.754 and Dice = 0.850. When compared by the inference time, AlbuNet34 is also the fastest model due to the light encoder. In the segmentation task this network takes around 20ms \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9969780595772709
      ],
      "excerpt": "Prediction of our detector on the validation image. The left picture is original image, the central is ground truth mask, and the right is predicted mask. Green dots correspond to centroid coordinates that define localization of the angiodysplasia. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8222434557380591
      ],
      "excerpt": ".. table:: Table 1. Segmentation results per task. Intersection over Union, Dice coefficient and inference time, ms. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.917280909000726
      ],
      "excerpt": "Model         IOU, %    Dice, %   Inference time, ms \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Wining solution and its further development for MICCAI 2017 Endoscopic Vision Challenge Angiodysplasia Detection and Localization",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ternaus/angiodysplasia-segmentation/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 28,
      "date": "Wed, 22 Dec 2021 01:52:58 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ternaus/angiodysplasia-segmentation/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "ternaus/angiodysplasia-segmentation",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/ternaus/angiodysplasia-segmentation/master/Demo.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/ternaus/angiodysplasia-segmentation/master/train.sh"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.8016654907421421
      ],
      "excerpt": "    :scale: 45 % \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8366977552709938
      ],
      "excerpt": "As an output of a model, we obtain an image, in which each pixel value corresponds to a probability of belonging to the area of interest or a class. The size of the output image matches the input image size. For binary segmentation, we use 0.3 as a threshold value (chosen using validation dataset) to binarize pixel probabilities. All pixel values below the speci \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ternaus/angiodysplasia-segmentation/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2018 Vladimir Iglovikov, Alexey Shvets, Alexandr A. Kalinin, Alexander Rakhlin\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "angiodysplasia-segmentation",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "ternaus",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ternaus/angiodysplasia-segmentation/blob/master/README.rst",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* Python 3.6\n* PyTorch 0.3.1\n* TorchVision 0.1.9\n* numpy 1.14.0\n* opencv-python 3.3.0.10\n* tqdm 4.19.4\n\nThese dependencies can be installed by running::\n\n    pip install -r requirements.txt\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The dataset is organized in the folloing way::\n::\n\n    \u251c\u2500\u2500 data\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 test\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 train\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 angyodysplasia\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 images\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 masks\n    \u2502\u00a0\u00a0     \u2514\u2500\u2500 normal\n    \u2502\u00a0\u00a0         \u251c\u2500\u2500 images\n    \u2502\u00a0\u00a0         \u2514\u2500\u2500 masks\n    \u2502\u00a0\u00a0     .......................\n\nThe training dataset contains 2 sets of images, one with angyodysplasia and second without it. For training we used only the images with angyodysplasia, which were split in 5 folds.\n\n1. Training\n\nThe main file that is used to train all models -  ``train.py``. Running ``python train.py --help`` will return set of all possible input parameters.\nTo train all models we used the folloing bash script (batch size was chosen depending on how many samples fit into the GPU RAM, limit was adjusted accordingly to keep the same number of updates for every network)::\n\n    #!/bin/bash\n\n    for i in 0 1 2 3\n    do\n       python train.py --device-ids 0,1,2,3 --limit 10000 --batch-size 12 --fold $i --workers 12 --lr 0.0001 --n-epochs 10 --jaccard-weight 0.3 --model UNet11\n       python train.py --device-ids 0,1,2,3 --limit 10000 --batch-size 12 --fold $i --workers 12 --lr 0.00001 --n-epochs 15 --jaccard-weight 0.3 --model UNet11\n    done\n\n2. Mask generation.\n\nThe main file to generate masks is ``generate_masks.py``. Running ``python generate_masks.py --help`` will return set of all possible input parameters. Example::\n\n    python generate_masks.py --output_path predictions/UNet16 --model_type UNet16 --model_path data/models/UNet16 --fold -1 --batch-size 4\n\n3. Evaluation.\n\nThe evaluation is different for a binary and multi-class segmentation:\n\n[a] In the case of binary segmentation it calculates jaccard (dice) per image / per video and then the predictions are avaraged.\n\n[b] In the case of multi-class segmentation it calculates jaccard (dice) for every class independently then avaraged them for each image and then for every video::\n\n    python evaluate.py --target_path predictions/UNet16 --train_path data/train/angyodysplasia/masks\n\n4. Further Improvements.\n\nOur results can be improved further by few percentages using simple rules such as additional augmentation of train images and train the model for longer time. In addition, the cyclic learning rate or cosine annealing could be also applied. To do it one can use our pre-trained weights as initialization. To improve test prediction TTA technique could be used as well as averaging prediction from all folds.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 76,
      "date": "Wed, 22 Dec 2021 01:52:58 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "medical-imaging",
      "computer-vision",
      "image-segmentation",
      "deep-learning",
      "python",
      "pytorch"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You can start working with our models using the demonstration example: `Demo.ipynb`_\n\n..  _`Demo.ipynb`: Demo.ipynb\n.. _`Alexander Rakhlin`: https://www.linkedin.com/in/alrakhlin/\n.. _`Alexey Shvets`: https://www.linkedin.com/in/shvetsiya/\n.. _`Vladimir Iglovikov`: https://www.linkedin.com/in/iglovikov/\n.. _`Alexandr A. Kalinin`: https://alxndrkalinin.github.io/\n.. _`MICCAI 2017 Endoscopic Vision SubChallenge Angiodysplasia Detection and Localization`: https://endovissub2017-giana.grand-challenge.org/angiodysplasia-etisdb/\n.. _`TernausNet`: https://arxiv.org/abs/1801.05746\n.. _`U-Net`: https://arxiv.org/abs/1505.04597\n.. _`AlbuNet34`: https://arxiv.org/abs/1803.01207\n.. _`LinkNet`: https://arxiv.org/abs/1707.03718\n.. _`google drive`: https://drive.google.com/drive/folders/1V_bLBTzsl_Z8Ln9Iq8gjcFDxodfiHxul\n\n.. |br| raw:: html\n\n   <br />\n\n.. |plusmn| raw:: html\n\n   &plusmn\n\n.. |times| raw:: html\n\n   &times\n\n.. |micro| raw:: html\n\n   &microm\n\n.. |y| image:: https://hsto.org/webt/jm/sn/i0/jmsni0y8mao8vnaij8a4eyuoqmu.gif\n.. |y_hat| image:: https://hsto.org/webt/xf/j2/a4/xfj2a4obgqhdzneysar5_us5pgk.gif\n.. |i| image:: https://hsto.org/webt/87/cc/ca/87cccaz4gjp2lgyeip17utljvvi.gif\n",
      "technique": "Header extraction"
    }
  ]
}