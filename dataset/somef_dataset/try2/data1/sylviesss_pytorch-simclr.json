{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2002.05709",
      "https://arxiv.org/abs/2002.05709"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\n@article{chen2020simple,\n  title={A Simple Framework for Contrastive Learning of Visual Representations},\n  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},\n  journal={arXiv preprint arXiv:2002.05709},\n  year={2020}\n}\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{chen2020simple,\n  title={A Simple Framework for Contrastive Learning of Visual Representations},\n  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},\n  journal={arXiv preprint arXiv:2002.05709},\n  year={2020}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/sylviesss/pytorch-simclr",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-04-05T22:17:14Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-09-01T15:04:13Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9877389834234259,
        0.9741871506888429,
        0.9532031192257335
      ],
      "excerpt": "In this project, we present an implementation of SimCLR in PyTorch. We also form \nprobabilistic notion of the contrastive learning framework and derive a new loss function. The goal is to truly  \nunderstand how a contrastive learning model (SimCLR) learns, how to interpret learned representations, and to quantify \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8258378134088862,
        0.9303472504477089,
        0.814669833082445,
        0.8653620961683772,
        0.8842376363832175
      ],
      "excerpt": "Below is a sample of augmented CIFAR-10 data: \nAugmentation performed for this project followed exactly the same procedure as what was carried out in the paper.  \nFor clarity, we list the steps here: \n- Random cropping (inception-style: random crop size uniform from 0.08 to 1 in area and a random aspect ratio) and  \nresizing to original size with random flipping (p=50%); torchvision.transforms.RandomResizedCrop \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9486037315103771
      ],
      "excerpt": "- Random Gaussian blur (p=50%). Randomly sample volatility in [0.1, 2.0], and the kernel size is 10% of the image  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8799452357296393,
        0.9766323053664151,
        0.9077179006522182,
        0.8542701474720066
      ],
      "excerpt": "We modify the original resnet module in pytorch  \nby defining a wrapper on top of it in order to:  \n- replace the first 7x7 convolution layer of stride 2 (conv1) with a 3x3 convolution of stride 1, to adjust for  \nthe smaller resolution of images in CIFAR10. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9546408094325994
      ],
      "excerpt": "The basic model for pretraining consists of: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9472795251476872
      ],
      "excerpt": "Currently using a batch_size of 512 and gradient accumulation to allow (relatively) larger batch training  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9529870910267318
      ],
      "excerpt": "was used with a learning rate of 1e-3 and a weight_decay of 1e-6 in pretraining. Lars \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/sylviesss/pytorch-simclr/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 23 Dec 2021 17:50:02 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/sylviesss/pytorch-simclr/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "sylviesss/pytorch-simclr",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.883634242677826
      ],
      "excerpt": "on a single Nvidia Tesla GPU with 12GB RAM.  \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/sylviesss/pytorch-simclr/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "(Probabilistic) SimCLR",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "pytorch-simclr",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "sylviesss",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/sylviesss/pytorch-simclr/blob/main/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To pretrain the model with gradient accumulation with batch size = `n_accum * 64`, for a number of epochs = \n`num_of_epochs`, dataset = `\"cifar10\"/\"stl10\"`, path for saving the model and checkpoints = `\"/path/for/saving/\"`, use_new_loss, run\n```\npython3 pretrain.py --n_epoch=num_of_epochs --accum_steps=n_accum --dataset=dataset --path_for_saving=\"/path/for/saving/\" --new_loss=use_new_loss\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 23 Dec 2021 17:50:02 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "pytorch",
      "simclr",
      "representation-learning",
      "resnet",
      "deep-learning",
      "self-supervised-learning"
    ],
    "technique": "GitHub API"
  }
}