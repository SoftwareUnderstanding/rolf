{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1810.04805",
      "https://arxiv.org/abs/1706.03762",
      "https://arxiv.org/abs/1810.04805",
      "https://arxiv.org/abs/1901.08746",
      "https://arxiv.org/abs/1810.04805"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9311242139085347
      ],
      "excerpt": "Collaboration between Santosh Gupta, Alex Sheng, and Junpeng Ye \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9287913210266059
      ],
      "excerpt": "<div style=\"text-align:center\"><img src=\"https://i.imgur.com/wzWt039.png\" /></div> \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/re-search/DocProduct",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-05-06T19:56:35Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-10T09:19:34Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.891773182293089,
        0.9911342338005367,
        0.9986189780898524,
        0.9968614943673577
      ],
      "excerpt": "Winner Top 6 Finalist of the \u26a1#PoweredByTF 2.0 Challenge! https://devpost.com/software/nlp-doctor . Doc Product will be presented to the Tensorflow Engineering Team at Tensorflow Connect. Stay tuned for details.  \nWe wanted to use TensorFlow 2.0 to explore how well state-of-the-art natural language processing models like BERT and GPT-2 could respond to medical questions by retrieving and conditioning on relevant medical data, and this is the result. \nThe purpose of this project is to explore the capabilities of deep learning language models for scientific encoding and retrieval IT SHOULD NOT TO BE USED FOR ACTIONABLE MEDICAL ADVICE. \nAs a group of friends with diverse backgrounds ranging from broke undergrads to data scientists to top-tier NLP researchers, we drew inspiration for our design from various different areas of machine learning. By combining the power of transformer architectures, latent vector search, negative sampling, and generative pre-training within TensorFlow 2.0's flexible deep learning framework, we were able to come up with a novel solution to a difficult problem that at first seemed like a herculean task. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.956356575800752,
        0.9778708515719151,
        0.8472086903491735,
        0.9630152661710599,
        0.9992282995996445,
        0.9756812769576805,
        0.9954305344866715
      ],
      "excerpt": "Fine-tuned TF 2.0 GPT-2 with OpenAI's GPT-2-117M parameters for generating answers to new questions \nNetwork heads for mapping question and answer embeddings to metric space, made with a Keras.Model feedforward network \nOver a terabyte of TFRECORDS, CSV, and CKPT data \nIf you're interested in the whole story of how we built Doc Product and the details of our architecture, take a look at our GitHub README! \nOur project was wrought with too many challenges to count, from compressing astronomically large datasets, to re-implementing the entirety of BERT in TensorFlow 2.0, to running GPT-2 with 117 million parameters in Colaboratory, to rushing to get the last parts of our project ready with a few hours left until the submission deadline. Oddly enough, the biggest challenges were often when we had disagreements about the direction that the project should be headed. However, although we'd disagree about what the best course of action was, in the end we all had the same end goal of building something meaningful and potentially valuable for a lot of people. That being said, we would always eventually be able to sit down and come to an agreement and, with each other's support and late-night pep talks over Google Hangouts, rise to the challenges and overcome them together. \nAlthough Doc Product isn't ready for widespread commercial use, its surprisingly good performance shows that advancements in general language models like BERT and GPT-2 have made previously intractable problems like medical information processing accessible to deep NLP-based approaches. Thus, we hope that our work serves to inspire others to tackle these problems and explore the newly open NLP frontier themselves. \nNevertheless, we still plan to continue work on Doc Product, specifically expanding it to take advantage of the 345M, 762M, and 1.5B parameter versions of GPT-2 as OpenAI releases them as part of their staged release program. We also intend to continue training the model, since we still have quite a bit more data to go through. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8935198682647316,
        0.8271574432779506,
        0.9757662766051984,
        0.9706369204501986
      ],
      "excerpt": "Our BERT has been trained to encode medical questions and medical information. A user can type in a medical question, and our model will retrieve the most relevant medical information to that question. \nWe created datasets from several medical question and answering forums. The forums are WebMD, HealthTap, eHealthForums, iClinic, Question Doctors, and Reddit.com/r/AskDocs \nThe architecture consists of a fine-tuned bioBert (same for both questions and answers) to convert text input to an embedding representation. The embedding is then input into a FCNN (a different one for the questions and answers) to develop an embedding which is used for similarity lookup. The top similar questions and answers are then used by GPT-2 to generate an answer. The full architecture is shown below.  \nLets take a look at the first half of the diagram above above in more detail, the training of the BERT and the FCNNs. A detailed figure of this part is shown below \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8882532114954776,
        0.8140852010448396
      ],
      "excerpt": "During training, we take a batch of medical questions and their corresponding medical answers, and convert them to bioBert embeddings. The same Bert weights are used for both the questions and answers.  \nThese embeddings are then inputted into a FCNN layer. There are separate FCNN layers for both the question and answer embeddings. To recap, we use the same weights in the Bert layer, but the questions and answers each have their own seperate FCNN layer.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9879798679777962,
        0.9660150263510153,
        0.9900461089450603,
        0.9962059896528453,
        0.9838849380951197,
        0.9885262321883461,
        0.9876553271675401,
        0.9955781254701,
        0.9465698314261253,
        0.9350982602226207,
        0.9764801794274949,
        0.9870916448818061,
        0.992987158499647,
        0.9472688498673673,
        0.9783521691185644,
        0.9315103909216017
      ],
      "excerpt": "So instead of NCE loss, what we did was compute the dot product for every combination of the question and answer embeddings within our batch. This is shown in the figure below \nThen, a softmax is taken across the rows; for each question, all of it's answer combinations are softmaxed.  \nFinally, the loss used is cross entropy loss. The softmaxed matrix is compared to a ground truth matrix; the correct combinations of questions and answers are labeled with a '1', and all the other combinations are labeled with a '0'. \nThe data gathering was tricky because the formatting of all of the different medical sites was significantly different. Custom work needed to be done for each site in order to pull questions and answers from the correct portion of the HTML tags. Some of the sites also had the possibility of multiple doctors responding to a single question so we needed a method of gathering multiple responses to individual questions. In order to deal with this, we created multiple rows for every question-answer pair. From here we needed to run the model through BERT and store the outputs from one of the end layers in order to make BioBERT embeddings we could pass through the dense layers of our feed-forward neural network(FFNN). 768 dimension vectors were stored for both the question and answers and concatenated with the corresponding text in a CSV file. We tried various different formats for more compact and faster loading and sharing, but CSV ended up being the easiest and most flexible method. After the BioBERT embeddings were created and stored the similarity training process was done and then FFNN embeddings were created that would capture the similarity of questions to answers. These were also stored along with the BioBERT embeddings and source text for later visualization and querying. \nThe embedding models are built in TF 2.0 which utilizes the flexibility of eager execution of TF 2.0. However, GPT2 model that we use are are built in TF 1.X. Luckily, we can train two models separately. While inference, we need to maintain disable eager execution with tf.compat.v1.disable_eager_execution and maintain two separate sessions. We also need to take care of the GPU memory of two sessions to avoid OOM. \nOne obvious approach to retrieve answers based on user\u2019s questions is that we use a powerful encoder(BERT) to encode input questions and questions in our database and do a similarity search. There is no training involves and the performance of this approach totally rely on the encoder. Instead, we use separate Feed-forward networks for questions and answers and calculate cosine similarity between them. Inspired by the negative sampling of word2vec paper, we treat other answers in the same batch as negative samples and calculate cross entropy loss. This approach makes the questions embeddings and answers embeddings in one pair as close as possible in terms of Euclidean distance. It turns out that this approach yields more robust results than doing similarity search directly using BERT embedding vector. \nThe preprocessing of BERT is complicated and we totally have around 333K QA pairs and over 30 million tokens. Considering shuffle is very important in our training, we need the shuffle buffer sufficiently large to properly train our model. It took over 10 minutes to preprocess data before starting to train model in each epoch. So we used the tf.data and TFRecords to build a high-performance input pipeline. After the optimization, it only took around 20 seconds to start training and no GPU idle time.  \nAnother problem with BERT preprocessing is that it pads all data to a fixed length. Therefore, for short sequences, a lot of computation and GPU memory are wasted. This is very important especially with big models like BERT. So we rewrite the BERT preprocessing code and make use of tf.data.experimental.bucket_by_sequence_length to bucket sequences with different lengths and dynamically padding sequences. By doing this, we achieved a longer max sequence length and faster training. \nAfter some modification, the Keras-Bert is able to run in tf 2.0 environment. However, when we try to use the Keras-Bert as a sub-model in our embedding models, we found the following two problems. \n- It uses the functional API. Functional API is very flexible, however, it\u2019s still symbolic. That means even though eager execution is enabled, we still cannot use the traditional python debugging method at run time. In order to fully utilize the power of eager execution, we need to build the model using tf.keras.Model \n- We are not directly using the input layer of Keras-Bert and ran into this issue. It\u2019s not easy to avoid this bug without changing our input pipeline. \nAs a result, we decided to re-implement an imperative version of BERT. We used some components of Keras-Bert(Multihead Attention, Checkpoint weight loading, etc) and write the call method of Bert. Our implementation is easier to debug and compatible with both flexible eager mode and high-performance static graph mode. \nUsers may experience multiple symptoms in various condition, which makes the perfect answer might be a combination of multiple answers. To tackle that, we make use of the powerful GPT2 model and feed the model the questions from users along with Top K auxiliary answers that we retrieved from our data. The GPT2 model will be based on the question and the Top K answers and generate a better answer. To properly train the GPT2 model, we create the training data as following: we take every question in our dataset, do a similarity search to obtain top K+1 answer, use the original answer as target and other answers as auxiliary inputs. By doing this we get the same amount of GPT2 training data as the embedding model training data. \nBert is fantastic for encoding medical questions and answers, and developing robust vector representations of those questions/answers.  \nWe trained a fine-tuned version of our model which was initialized with Naver's bioBert. We also trained a version where the bioBert weights were frozen, and only trained the two FCNNs for the questions and answers. While we expected the fine-tuned version to work well, we were surprised at how robust later was. This suggests that bioBert has innate capabilities in being able to encode the means of medical questions and answers. \nExplore if there's any practical use of this project outside of research/exploratory purposes. A model like this should not be used in the public for obtaining medical information. But perhaps it can be used by trained/licenced medical professionals to gather information for vetting.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Medical Q&A with Deep Language Models",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/re-search/DocProduct/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 137,
      "date": "Thu, 30 Dec 2021 00:03:25 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/re-search/DocProduct/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "re-search/DocProduct",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/re-search/DocProduct/master/notebooks/RetrievalQADoc.ipynb",
      "https://raw.githubusercontent.com/re-search/DocProduct/master/notebooks/GenerateQADoc.ipynb",
      "https://raw.githubusercontent.com/re-search/DocProduct/master/notebooks/SG-QAFFN_cross_entropy.ipynb",
      "https://raw.githubusercontent.com/re-search/DocProduct/master/notebooks/visualization.ipynb",
      "https://raw.githubusercontent.com/re-search/DocProduct/master/notebooks/FaissEvalTopKColab.ipynb",
      "https://raw.githubusercontent.com/re-search/DocProduct/master/notebooks/webmd_data_gather.ipynb",
      "https://raw.githubusercontent.com/re-search/DocProduct/master/notebooks/DocProductPresentation.ipynb",
      "https://raw.githubusercontent.com/re-search/DocProduct/master/ffn_cross_entropy/SG-QAFFN_cross_entropy%20%281%29.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You can install *Doc Product* directly from pip and run it on your local machine. Here's the code to install *Doc Product*, along with TensorFlow 2.0 and FAISS:\n\n```\n!wget  https://anaconda.org/pytorch/faiss-cpu/1.2.1/download/linux-64/faiss-cpu-1.2.1-py36_cuda9.0.176_1.tar.bz2\n#:To use GPU FAISS use\n#: !wget  https://anaconda.org/pytorch/faiss-gpu/1.2.1/download/linux-64/faiss-gpu-1.2.1-py36_cuda9.0.176_1.tar.bz2\n!tar xvjf faiss-cpu-1.2.1-py36_cuda9.0.176_1.tar.bz2\n!cp -r lib/python3.6/site-packages/* /usr/local/lib/python3.6/dist-packages/\n!pip install mkl\n\n!pip install tensorflow-gpu==2.0.0-alpha0\nimport tensorflow as tf\n!pip install https://github.com/Santosh-Gupta/DocProduct/archive/master.zip\n```\n \nOur repo contains scripts for generating **.tfrefords** data, training *Doc Product* on your own Q&A data, and running *Doc Product* to get answers for medical questions. Please see the **Google Colaboratory demos** section below for code samples to load data/weights and run our models.\n\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8528716389136073
      ],
      "excerpt": "Download trained models and embedding file here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8044137844601341
      ],
      "excerpt": "Fine-tuned TF 2.0 BERT with pre-trained BioBERT weights for extracting representations from text \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8452064023529171
      ],
      "excerpt": "  <img src=\"https://snag.gy/WPdV5T.jpg\"> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/re-search/DocProduct/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "HTML",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 Santosh Gupta\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Doc Product: Medical Q&A with Deep Language Models",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "DocProduct",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "re-search",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/re-search/DocProduct/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "https://colab.research.google.com/drive/11hAr1qo7VCSmIjWREFwyTFblU2LVeh1R\n\n<p align=\"center\">\n  <img src=\"https://i.imgur.com/Z8DOXuJ.png\">\n</p>\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "The end-to-end *Doc Product* demo is still **experimental**, but feel free to try it out!\nhttps://colab.research.google.com/drive/1Bv7bpPxIImsMG4YWB_LWjDRgUHvi7pxx\n\n<p align=\"center\">\n  <img src=\"https://snag.gy/WU1YPE.jpg\">\n</p>\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 518,
      "date": "Thu, 30 Dec 2021 00:03:25 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "artificial-intelligence",
      "machine-learning",
      "deep-learning",
      "nlp",
      "medical",
      "health",
      "healthcare",
      "bert",
      "gpt-2",
      "tensorflow",
      "tensorflow-2"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[Take a look at our Colab demos!](https://drive.google.com/open?id=1kXqsE4N0MgfktsEJpZZn470yOZ3UQ10F) We plan on adding more demos as we go, allowing users to explore more of the functionalities of *Doc Product*. All new demos will be added to the same Google Drive folder.\n\nThe demos include code for installing *Doc Product* via pip, downloading/loading pre-trained weights, and running *Doc Product*'s retrieval functions and fine-tuning on your own Q&A data.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "https://colab.research.google.com/drive/11hAr1qo7VCSmIjWREFwyTFblU2LVeh1R\n\n<p align=\"center\">\n  <img src=\"https://i.imgur.com/Z8DOXuJ.png\">\n</p>\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "The end-to-end *Doc Product* demo is still **experimental**, but feel free to try it out!\nhttps://colab.research.google.com/drive/1Bv7bpPxIImsMG4YWB_LWjDRgUHvi7pxx\n\n<p align=\"center\">\n  <img src=\"https://snag.gy/WU1YPE.jpg\">\n</p>\n\n",
      "technique": "Header extraction"
    }
  ]
}