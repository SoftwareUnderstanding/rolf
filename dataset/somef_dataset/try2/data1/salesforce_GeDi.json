{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2009.06367",
      "https://arxiv.org/abs/1810.04805",
      "https://arxiv.org/abs/1703.01898",
      "https://arxiv.org/abs/1508.07909",
      "https://arxiv.org/abs/2009.06367"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\n@article{KrauseGeDi2020,\n  title={{GeDi: Generative Discriminator Guided Sequence Generation}},\n  author={Krause, Ben and Gotmare, Akhilesh Deepak and McCann, Bryan and Keskar, Nitish Shirish and Joty, Shafiq and Socher, Richard and Rajani, Nazneen Fatema},\n  journal={arXiv preprint arXiv:2009.06367},\n  year={2020}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{KrauseGeDi2020,\n  title={{GeDi: Generative Discriminator Guided Sequence Generation}},\n  author={Krause, Ben and Gotmare, Akhilesh Deepak and McCann, Bryan and Keskar, Nitish Shirish and Joty, Shafiq and Socher, Richard and Rajani, Nazneen Fatema},\n  journal={arXiv preprint arXiv:2009.06367},\n  year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.939574221882642
      ],
      "excerpt": "Sept 29, 2020: Adding support for GeDi-guided GPT-3 generation (API key needed) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeOfConduct": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://raw.githubusercontent.com/salesforce/GeDi/master/CODE_OF_CONDUCT.md",
    "technique": "File Exploration"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/salesforce/GeDi",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-09-02T22:47:22Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-15T03:12:11Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "GeDi is a method of using class-conditional language models (which we refer to as generative discriminators (GeDis)) to guide generation from other (potentially much larger) language models. This has several advantages over finetuning large language models directly including:\n\n* significantly less training computation.\n* maintaining the diversity of the original language model (If we finetune a large pretrained language model to a specific attribute dataset, we will likely reduce the broad generation capabilities of the model).\n* teaching the language model what not to generate. This is especially useful for applications like detoxification.\n\n\nGeDi is a form of discriminator guided generation. A discriminator that can classify an attribute could be used to guide language model generation towards that attribute by classifying the sequences that result from candidate next tokens. However, using a normal discriminator (such as [BERT](https://arxiv.org/abs/1810.04805)) to do this would be very computationally expensive during generation, since it would require feeding in every candidate next token one-by-one to the discriminator to be classified. However, using generative discriminators, we can very efficiently classify candidate next tokens during generation using Bayes rule (see Section 3.1 of the paper). As an added bonus, [generative discriminators can be used as zero shot classifiers](https://arxiv.org/abs/1703.01898), and can therefore be used to guide generation towards unseen topics.\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8687749175365307
      ],
      "excerpt": "This downloads and saves the topic, sentiment, and detoxifier models in the folder ../pretrained_models \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9878085332654565,
        0.8829641891435817,
        0.9941185585547703
      ],
      "excerpt": "--filter_p equal to 1 - \\rho in Equation 7 of the paper \n--target_p equal to \\tau from the paper \n--disc_weight exponent for posterior weighting (\\omega in Equation 6 of the paper) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9008799105361694,
        0.8539021656950752
      ],
      "excerpt": "You will be prompted to give a topic code. The model was trained on world, sports, business, and science, but can often generate other topics zero-shot, for instance space, fire, climate, education \nIf the topic code you give is more than one BPE token, the model often struggles because the 4 training topics were all 1 BPE token. You will be warned that this might not work, but can proceed by hitting enter again (or can type a new topic code). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9866046690527657,
        0.9698596659822672
      ],
      "excerpt": "The model can controllably generate positive or negative text. When generalizing to other domains such as stories, this often translates to positive/negative mood or tone of the story (since sentiment implies an opinion). \nThe model is set to positive sentiment by default. You will be prompted for the opportunity to change to negative sentiment by typing n. Note that the negative model can be very negative, and this sometimes results in toxic or offensive samples. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9577269317959132,
        0.9810465099895432
      ],
      "excerpt": "Two of the baselines we consider are generating from GPT-2 (will give same result regardless of control codes), and generating from the GeDi model directly as a class-conditional language model (instead of using it to guide generation from GPT-2). \nSet --gen_type gpt2 to generate from GPT-2, and --gen_type cclm to generate directly from the GeDi as a class-conditional language model. --gen_type cclm corresponds to all experiments in Section 5 of the paper, and the CC-LM baselines in Section 6.1. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9377348838175522,
        0.950694753516689,
        0.8680774431699041
      ],
      "excerpt": "This is somewhat limited, since the GPT-3 API only allow access to the top 100 next token log probabilities. \nReuses settings for controlling GPT-2 (which uses all next token log probs), retuning for GPT-3 could give better results. \nIt is also slow (up to 1 second per token) because modifying GPT-3 decoding requires calling the API one token at a time. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9280090271654243,
        0.8116038522730336
      ],
      "excerpt": "There are some differences in this training script and the one used to train the pretrained model. The pretrained model only used half of AG news, and there were some slight differences in preprocessing. \nThis runs in about 5 hours on a 16GB V100 GPU on GCP. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "GeDi: Generative Discriminator Guided Sequence Generation",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/salesforce/GeDi/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 35,
      "date": "Sun, 26 Dec 2021 09:53:21 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/salesforce/GeDi/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "salesforce/GeDi",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/salesforce/GeDi/master/GeDi_guided_GPT_2_XL.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/salesforce/GeDi/master/scripts/setup.sh",
      "https://raw.githubusercontent.com/salesforce/GeDi/master/scripts/get_data.sh",
      "https://raw.githubusercontent.com/salesforce/GeDi/master/scripts/run_generation.sh",
      "https://raw.githubusercontent.com/salesforce/GeDi/master/scripts/run_training.sh",
      "https://raw.githubusercontent.com/salesforce/GeDi/master/scripts/get_models.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9714207429223904,
        0.9465718491881494
      ],
      "excerpt": "cd scripts \nbash get_models.sh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8520482746333369
      ],
      "excerpt": "If you have your own GPT-3 API secret key, you can use GeDi to guide decoding from GPT-3. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.999746712887969
      ],
      "excerpt": "pip install openai \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9714207429223904,
        0.9465718491881494
      ],
      "excerpt": "cd scripts \nbash get_data.sh \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8949255813999306
      ],
      "excerpt": "First download the models: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8118514848017406
      ],
      "excerpt": "--gedi_model_name_or_path path to GeDi model. If unused, will assume you ran bash get_models.sh and infer model directory from --mode argument \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9100466275704214
      ],
      "excerpt": "python ../generate_GeDi.py --penalize_cond --gen_length 100 --mode sentiment --gpt3_api_key sk-xxxxxxxx \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9100466275704214
      ],
      "excerpt": "python ../generate_GeDi.py --penalize_cond --gen_length 100 --mode sentiment --gen_type gpt2 --gpt3_api_key sk-xxxxxxx \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8155572406269703
      ],
      "excerpt": "First, download and process the topic data: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.854402003594198
      ],
      "excerpt": "Then run training using: \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/salesforce/GeDi/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "BSD 3-Clause \"New\" or \"Revised\" License",
      "url": "https://api.github.com/licenses/bsd-3-clause"
    },
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "### Colab Notebook on controlling topic using GeDi [here](https://colab.research.google.com/github/salesforce/GeDi/blob/master/GeDi_guided_GPT_2_XL.ipynb)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "GeDi",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "salesforce",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/salesforce/GeDi/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Python 3.7, PyTorch 1.4\n(We recommend creating a container using the [pytorch/pytorch:1.4-cuda10.1-cudnn7-devel](https://hub.docker.com/layers/pytorch/pytorch/1.4-cuda10.1-cudnn7-devel/images/sha256-c612782acc39256aac0637d58d297644066c62f6f84f0b88cfdc335bb25d0d22?context=explore) official pytorch docker image.)\n- Run `scripts/setup.sh`:\n  ```\n  cd scripts\n  bash setup.sh\n  ```\n  This will install the following:\n\n  - Transformers v2.8.0 and its example-specific requirements mentioned [here](https://github.com/huggingface/transformers/tree/master/examples#important-note)\n  - (optional) Apex (details [here](https://github.com/NVIDIA/apex#linux)) for fp16 training and generation (installing apex takes a while; comment corresponding lines in `setup.sh` if you want to skip)\n  - `wget` and `unzip` (to download and unzip data and model checkpoints)\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 139,
      "date": "Sun, 26 Dec 2021 09:53:21 GMT"
    },
    "technique": "GitHub API"
  }
}