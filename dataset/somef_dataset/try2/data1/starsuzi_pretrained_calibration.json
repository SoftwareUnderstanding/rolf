{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2003.07892",
      "https://arxiv.org/abs/1810.04805",
      "https://arxiv.org/abs/1907.11692",
      "https://arxiv.org/abs/1606.01933",
      "https://arxiv.org/abs/1609.06038",
      "https://arxiv.org/abs/2003.07892",
      "https://arxiv.org/abs/1907.11692"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{desai-durrett-2020-calibration,\n  author={Desai, Shrey and Durrett, Greg},\n  title={{Calibration of Pre-trained Transformers}},\n  year={2020},\n  journal={arXiv preprint arXiv:1907.11692},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9999220558738565
      ],
      "excerpt": "Code and datasets for our preprint Calibration of Pre-trained Transformers. If you found this project helpful, please consider citing our paper: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8256326174517207
      ],
      "excerpt": "Please see our paper for the complete set of experiments and results! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9848179726092176
      ],
      "excerpt": "| SWAG    |  73,547 | 10,004 | 10,004 | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/starsuzi/pretrained_calibration",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-07-20T13:20:28Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-08-19T09:24:44Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8612066356603174
      ],
      "excerpt": "Code and datasets for our preprint Calibration of Pre-trained Transformers. If you found this project helpful, please consider citing our paper: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9970158522391628,
        0.880795358501019
      ],
      "excerpt": "Posterior calibration is a measure of how aligned a model's posterior probabilities are with empirical likelihoods. For example, a perfectly calibrated model that outputs 0.8 probability on 100 samples should get 80% of the samples correct. In this work, we analyze the calibration of two pre-trained Transformers (BERT and RoBERTa) on three tasks: natural language inference, paraphrase detection, and commonsense reasoning. \nFor natural language inference, we use Stanford Natural Language Inference (SNLI) and Multi-Genre Natural Language Inference (MNLI). For paraphrase detection, we use Quora Question Pairs (QQP) and TwitterPPDB (TPPDB). And, for commonsense reasoning, we use Situations with Adversarial Generations (SWAG) and HellaSWAG (HSWAG). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8169815468012596
      ],
      "excerpt": "| Model   | Accuracy |  ECE | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8728064954788735,
        0.860059181823877
      ],
      "excerpt": "To bring down calibration error, we experiment with two strategies. First, temperature scaling (TS; dividing non-normalized logits by scalar T) almost always brings ECE below 1. Below, we show in-domain results with and without temperature scaling: \n| Model         | SNLI | QQP  | SWAG | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9503268911906393,
        0.860059181823877
      ],
      "excerpt": "Second, deliberately inducing uncertainty via label smoothing (LS) helps calibrate posteriors out-of-domain. MLE training encourages models to be over-confident, which is typically unwarranted out-of-domain, where models should be uncertain. We show out-of-domain results with and without label smoothing: \n| Model       | MNLI | TPPDB |  HSWAG | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9808576268329918
      ],
      "excerpt": "Please see our paper for the complete set of experiments and results! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "    --model $MODEL \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9542841486403842
      ],
      "excerpt": "We evaluate calibration using the output files dumped in the previous step (when --do_evaluate is enabled). Below is an example script that evaluates the calibration of RoBERTa-MLE on QQP using temperature scaling. Note that we use QQP-dev to learn the temperature scaling hyperparameter T, then evaluate its performance on QQP-test.  \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/starsuzi/pretrained_calibration/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 29 Dec 2021 14:20:54 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/starsuzi/pretrained_calibration/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "starsuzi/pretrained_calibration",
    "technique": "GitHub API"
  },
  "invocation": [
    {
      "confidence": [
        0.8828665034782968
      ],
      "excerpt": "| ESIM    |   52.09  | 7.01 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.862233902125962
      ],
      "excerpt": "| Dataset |  Train  |   Dev  |  Test  | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.950563948951535
      ],
      "excerpt": "python3 train.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8914542725484403,
        0.8250064405112768
      ],
      "excerpt": "    --output_path \"output/${TASK}${MODEL}.json\" \\ \n    --train_path \"calibration_data/${TASK}/train.txt\" \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8267291959978095
      ],
      "excerpt": "    --test_path \"calibration_data/${TASK}/test.txt\" \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.81800761905586,
        0.9336801098518991
      ],
      "excerpt": "export TEST_PATH=\"output/test/QQP_QQP_roberta-base.json\" \npython3 calibrate.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8174540907975313
      ],
      "excerpt": " training  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8392035068526316
      ],
      "excerpt": "total error = 7.413702354436335 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/starsuzi/pretrained_calibration/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Calibration of Pre-trained Transformers",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "pretrained_calibration",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "starsuzi",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/starsuzi/pretrained_calibration/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This repository has the following requirements:\n\n- `numpy==1.18.1`\n- `scikit-learn==0.22.1`\n- `torch==1.2.0`\n- `tqdm==4.42.1`\n- `transformers==2.4.1`\n\nUse the following instructions to set up the dependencies:\n\n```bash\n$ virtualenv -p python3.6 venv\n$ pip install -r requirements.txt\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 29 Dec 2021 14:20:54 GMT"
    },
    "technique": "GitHub API"
  }
}