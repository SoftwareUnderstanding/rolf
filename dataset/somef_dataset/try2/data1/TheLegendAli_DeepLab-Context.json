{
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{KrahenbuhlK11,\n  title={Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials},\n  author={Philipp Kr{\\\"{a}}henb{\\\"{u}}hl and Vladlen Koltun},\n  booktitle={NIPS},      \n  year={2011}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{papandreou15weak,\n  title={Weakly- and Semi-Supervised Learning of a DCNN for Semantic Image Segmentation},\n  author={George Papandreou and Liang-Chieh Chen and Kevin Murphy and Alan L Yuille},\n  journal={arxiv:1502.02734},\n  year={2015}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{chen14semantic,\n  title={Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs},\n  author={Liang-Chieh Chen and George Papandreou and Iasonas Kokkinos and Kevin Murphy and Alan L Yuille},\n  booktitle={ICLR},\n  url={http://arxiv.org/abs/1412.7062},\n  year={2015}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/TheLegendAli/DeepLab-Context",
    "technique": "GitHub API"
  },
  "contributors": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contributors\nCaffe is developed by a core set of BVLC members and the open-source community.\nWe thank all of our contributors!\nFor the detailed history of contributions of a given file, try\ngit blame file\n\nto see line-by-line credits and\ngit log --follow file\n\nto see the change log even across renames and rewrites.\nPlease refer to the acknowledgements on the Caffe site for further details.\nCopyright is held by the original contributor according to the versioning history; see LICENSE.",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2015-08-15T06:55:45Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-22T11:21:45Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\nDeepLab is a state-of-art deep learning system for semantic image segmentation built on top of [Caffe](http://caffe.berkeleyvision.org).\r\n\r\nIt combines densely-computed deep convolutional neural network (CNN) responses with densely connected conditional random fields (CRF).\r\n\r\nThis distribution provides a publicly available implementation for the key model ingredients first reported in an [arXiv paper](http://arxiv.org/abs/1412.7062), accepted in revised form as conference publication to the ICLR-2015 conference. \r\nIt also contains implementations for methods supporting model learning using only weakly labeled examples, described in a second follow-up [arXiv paper](http://arxiv.org/abs/1502.02734).\r\nPlease consult and consider citing the following papers:\r\n\r\n    @inproceedings{chen14semantic,\r\n      title={Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs},\r\n      author={Liang-Chieh Chen and George Papandreou and Iasonas Kokkinos and Kevin Murphy and Alan L Yuille},\r\n      booktitle={ICLR},\r\n      url={http://arxiv.org/abs/1412.7062},\r\n      year={2015}\r\n    }\r\n\r\n    @article{papandreou15weak,\r\n      title={Weakly- and Semi-Supervised Learning of a DCNN for Semantic Image Segmentation},\r\n      author={George Papandreou and Liang-Chieh Chen and Kevin Murphy and Alan L Yuille},\r\n      journal={arxiv:1502.02734},\r\n      year={2015}\r\n    }\r\n\r\nNote that if you use the densecrf implementation, please consult and cite the following paper:\r\n\r\n    @inproceedings{KrahenbuhlK11,\r\n      title={Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials},\r\n      author={Philipp Kr{\\\"{a}}henb{\\\"{u}}hl and Vladlen Koltun},\r\n      booktitle={NIPS},      \r\n      year={2011}\r\n    }\r\n\r\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.943898624867368,
        0.9136713126584173
      ],
      "excerpt": "DeepLab currently achieves 73.9% on the challenging PASCAL VOC 2012 image segmentation task -- see the leaderboard. \nWe have released several trained models and corresponding prototxt files at here. Please check it for more model details. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/TheLegendAli/DeepLab-Context/releases",
    "technique": "GitHub API"
  },
  "faq": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\nCheck [FAQ](http://ccvl.stat.ucla.edu/deeplab_faq/) if you have some problems while using the code.\r\n",
      "technique": "Header extraction"
    }
  ],
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 94,
      "date": "Sat, 25 Dec 2021 07:25:15 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/TheLegendAli/DeepLab-Context/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "TheLegendAli/DeepLab-Context",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/TheLegendAli/DeepLab-Context/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/TheLegendAli/DeepLab-Context/master/examples/filter_visualization.ipynb",
      "https://raw.githubusercontent.com/TheLegendAli/DeepLab-Context/master/examples/net_surgery.ipynb",
      "https://raw.githubusercontent.com/TheLegendAli/DeepLab-Context/master/examples/detection.ipynb",
      "https://raw.githubusercontent.com/TheLegendAli/DeepLab-Context/master/examples/hdf5_classification.ipynb",
      "https://raw.githubusercontent.com/TheLegendAli/DeepLab-Context/master/examples/classification.ipynb",
      "https://raw.githubusercontent.com/TheLegendAli/DeepLab-Context/master/examples/siamese/mnist_siamese.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/TheLegendAli/DeepLab-Context/master/scripts/deploy_docs.sh",
      "https://raw.githubusercontent.com/TheLegendAli/DeepLab-Context/master/scripts/build_docs.sh",
      "https://raw.githubusercontent.com/TheLegendAli/DeepLab-Context/master/scripts/download_model_from_gist.sh",
      "https://raw.githubusercontent.com/TheLegendAli/DeepLab-Context/master/scripts/upload_model_to_gist.sh",
      "https://raw.githubusercontent.com/TheLegendAli/DeepLab-Context/master/scripts/gather_examples.sh",
      "https://raw.githubusercontent.com/TheLegendAli/DeepLab-Context/master/scripts/travis/travis_setup_makefile_config.sh",
      "https://raw.githubusercontent.com/TheLegendAli/DeepLab-Context/master/scripts/travis/travis_build_and_test.sh",
      "https://raw.githubusercontent.com/TheLegendAli/DeepLab-Context/master/scripts/travis/travis_install.sh",
      "https://raw.githubusercontent.com/TheLegendAli/DeepLab-Context/master/examples/imagenet/create_imagenet.sh",
      "https://raw.githubusercontent.com/TheLegendAli/DeepLab-Context/master/examples/imagenet/resume_training.sh",
      "https://raw.githubusercontent.com/TheLegendAli/DeepLab-Context/master/examples/imagenet/train_caffenet.sh",
      "https://raw.githubusercontent.com/TheLegendAli/DeepLab-Context/master/examples/imagenet/make_imagenet_mean.sh",
      "https://raw.githubusercontent.com/TheLegendAli/DeepLab-Context/master/examples/cifar10/create_cifar10.sh",
      "https://raw.githubusercontent.com/TheLegendAli/DeepLab-Context/master/examples/cifar10/train_quick.sh",
      "https://raw.githubusercontent.com/TheLegendAli/DeepLab-Context/master/examples/cifar10/train_full.sh",
      "https://raw.githubusercontent.com/TheLegendAli/DeepLab-Context/master/examples/mnist/train_lenet_consolidated.sh",
      "https://raw.githubusercontent.com/TheLegendAli/DeepLab-Context/master/examples/mnist/train_mnist_autoencoder_adagrad.sh",
      "https://raw.githubusercontent.com/TheLegendAli/DeepLab-Context/master/examples/mnist/create_mnist.sh",
      "https://raw.githubusercontent.com/TheLegendAli/DeepLab-Context/master/examples/mnist/train_mnist_autoencoder_nesterov.sh",
      "https://raw.githubusercontent.com/TheLegendAli/DeepLab-Context/master/examples/mnist/train_lenet.sh",
      "https://raw.githubusercontent.com/TheLegendAli/DeepLab-Context/master/examples/mnist/train_mnist_autoencoder.sh",
      "https://raw.githubusercontent.com/TheLegendAli/DeepLab-Context/master/examples/siamese/create_mnist_siamese.sh",
      "https://raw.githubusercontent.com/TheLegendAli/DeepLab-Context/master/examples/siamese/train_mnist_siamese.sh",
      "https://raw.githubusercontent.com/TheLegendAli/DeepLab-Context/master/tools/extra/parse_log.sh",
      "https://raw.githubusercontent.com/TheLegendAli/DeepLab-Context/master/tools/extra/launch_resize_and_crop_images.sh"
    ],
    "technique": "File Exploration"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/TheLegendAli/DeepLab-Context/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "C++",
      "Python",
      "Cuda",
      "MATLAB",
      "CMake",
      "Protocol Buffer",
      "Makefile",
      "Shell",
      "C",
      "Limbo"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other",
      "url": "https://raw.githubusercontent.com/TheLegendAli/DeepLab-Context/master/LICENSE"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'COPYRIGHT\\n\\nAll new contributions compared to the original Caffe branch:\\nCopyright (c) 2015, Liang-Chieh Chen (UCLA), George Papandreou (Google), Iasonas Kokkinos (CentraleSup\\xc3\\xa9lec /INRIA), Kevin Murphy (Google), Alan L. Yuille (UCLA)\\nAll rights reserved.\\n\\nAll contributions by the University of California:\\nCopyright (c) 2014, The Regents of the University of California (Regents)\\nAll rights reserved.\\n\\nAll other contributions:\\nCopyright (c) 2014, the respective contributors\\nAll rights reserved.\\n\\nCaffe uses a shared copyright model: each contributor holds copyright over\\ntheir contributions to Caffe. The project versioning records all such\\ncontribution and copyright details. If a contributor wants to further mark\\ntheir specific copyright on a particular contribution, they should indicate\\ntheir copyright solely in the commit message of the change when it is\\ncommitted.\\n\\nLICENSE\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met: \\n\\n1. Redistributions of source code must retain the above copyright notice, this\\n   list of conditions and the following disclaimer. \\n2. Redistributions in binary form must reproduce the above copyright notice,\\n   this list of conditions and the following disclaimer in the documentation\\n   and/or other materials provided with the distribution. \\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR\\nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\\nON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n\\nCONTRIBUTION AGREEMENT\\n\\nBy contributing to the BVLC/caffe repository through pull-request, comment,\\nor otherwise, the contributor releases their content to the\\nlicense and copyright terms herein.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# DeepLab",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "DeepLab-Context",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "TheLegendAli",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/TheLegendAli/DeepLab-Context/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\n1. Install wget library for python\r\n```\r\nsudo pip install wget\r\n```\r\n2. Change DATA_ROOT to point to the PASCAL images\r\n\r\n3. To use the mat_read_layer and mat_write_layer, please download and install [matio](http://sourceforge.net/projects/matio/files/matio/1.5.2/).\r\n\r\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\n```\r\npython run.py\r\n```\r\n\r\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 227,
      "date": "Sat, 25 Dec 2021 07:25:15 GMT"
    },
    "technique": "GitHub API"
  }
}