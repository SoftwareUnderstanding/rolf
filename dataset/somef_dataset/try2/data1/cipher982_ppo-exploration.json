{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1707.06347\n- Alternate through sampling data through interaction, and optimizing a _surrogate_ objective function using SGD.\n- **OLD** :\n  - Typically, policy methods perform one gradient update per data sample\n  - TYPES:\n    - DQN, policy gradients, trust regions\n    - DQN: bad on continuous spaces\n  - Room for improvement with less hyperparameter tuning\n- **NEW** :\n  - Multiple epochs of minibatch updates (PPO"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[image1]: https://raw.githubusercontent.com/cipher982/ppo-exploration/master/images/pong_screenshot_small.png \"Pong Screenshot\"\n\n[pimg1]: https://raw.githubusercontent.com/cipher982/ppo-exploration/master/images/pimg1.png\n[pimg2]: https://raw.githubusercontent.com/cipher982/ppo-exploration/master/images/pimg2.png\n[pimg3]: https://raw.githubusercontent.com/cipher982/ppo-exploration/master/images/pimg3.png\n[pimg4]: https://raw.githubusercontent.com/cipher982/ppo-exploration/master/images/pimg4.png\n[pimg5]: https://raw.githubusercontent.com/cipher982/ppo-exploration/master/images/pimg5.png\n[pimg6]: https://raw.githubusercontent.com/cipher982/ppo-exploration/master/images/pimg6.png\n[pimg7]: https://raw.githubusercontent.com/cipher982/ppo-exploration/master/images/pimg7.png\n[pimg8]: https://raw.githubusercontent.com/cipher982/ppo-exploration/master/images/pimg8.png\n[pimg9]: https://raw.githubusercontent.com/cipher982/ppo-exploration/master/images/pimg9.png\n[pimg10]: https://raw.githubusercontent.com/cipher982/ppo-exploration/master/images/pimg10.png\n[pimg11]: https://raw.githubusercontent.com/cipher982/ppo-exploration/master/images/pimg11.png\n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8490817347094297
      ],
      "excerpt": "And actually: \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/cipher982/ppo-exploration",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-12-27T20:14:48Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-12-27T23:29:41Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "![image1]\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9078841988302783
      ],
      "excerpt": "Collect trajectories based on PIE THETA, initialize theta&#39;=theta \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8563981702134636
      ],
      "excerpt": "Set new policies (theta=theta&#39;) and go back to step 1, repeat. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8372396012058242,
        0.9872591595398047
      ],
      "excerpt": "    - DQN: bad on continuous spaces \n  - Room for improvement with less hyperparameter tuning \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8804387540170268
      ],
      "excerpt": "  - Multiple epochs of minibatch updates (PPO) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8430437003346908
      ],
      "excerpt": "    - Forms a pessimistic estimate (lower-bound) of performance \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8706736040183308
      ],
      "excerpt": "Collect trajectory using these \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8014893987422467
      ],
      "excerpt": "Compute estimate of the reward gradient \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8333433829006016
      ],
      "excerpt": "Update the policy using gradient ascent with learning rate alpha. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8477953668091714
      ],
      "excerpt": "Multiple trajectories feature a distribution of rewards that we can use to normalize on. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9269669353081725
      ],
      "excerpt": "The re-weighting factor is just the product of all the policy across each step. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "sandbox for designing some PPO reinforcement learning algorithms",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/cipher982/ppo-exploration/releases",
    "technique": "GitHub API"
  },
  "faq": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- **Inefficient** - We run a policy, update once, then toss trajectory\n- **Noisy** - gradient estimate _g_, possibly the trajectory may not be representative of the policy.\n- **No Clear Credit Assignment** - A trajectory has good/bad actions, but all receive same reward.\n  - Must run more simulations to smooth out corrections\n  - Signals accumulate over time\n\n",
      "technique": "Header extraction"
    }
  ],
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 28 Dec 2021 14:30:10 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/cipher982/ppo-exploration/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "cipher982/ppo-exploration",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/cipher982/ppo-exploration/master/Untitled.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8327059449491132
      ],
      "excerpt": "All current rewards are from the past. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8122485341903903
      ],
      "excerpt": "Or else the approximations can become invalid \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/cipher982/ppo-exploration/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "(Image References)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "ppo-exploration",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "cipher982",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/cipher982/ppo-exploration/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 28 Dec 2021 14:30:10 GMT"
    },
    "technique": "GitHub API"
  }
}