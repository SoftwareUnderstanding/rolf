{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1604.00981"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8105456132409875
      ],
      "excerpt": "Rethinking the Inception Architecture for Computer Vision \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8715509475085618
      ],
      "excerpt": "bazel-bin/inception/imagenet_train --num_gpus=1 --batch_size=32 --train_dir=/tmp/imagenet_train --data_dir=/tmp/imagenet_data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9750807980404629,
        0.9941533744942845,
        0.9392395551057171,
        0.9611841524374782,
        0.9606712260438129,
        0.9817529343071055,
        0.9606712260438129
      ],
      "excerpt": "2016-03-07 12:24:59.922898: step 0, loss = 13.11 (5.3 examples/sec; 6.064 sec/batch) \n2016-03-07 12:25:55.206783: step 10, loss = 13.71 (9.4 examples/sec; 3.394 sec/batch) \n2016-03-07 12:26:28.905231: step 20, loss = 14.81 (9.5 examples/sec; 3.380 sec/batch) \n2016-03-07 12:27:02.699719: step 30, loss = 14.45 (9.5 examples/sec; 3.378 sec/batch) \n2016-03-07 12:27:36.515699: step 40, loss = 13.98 (9.5 examples/sec; 3.376 sec/batch) \n2016-03-07 12:28:10.220956: step 50, loss = 13.92 (9.6 examples/sec; 3.327 sec/batch) \n2016-03-07 12:28:43.658223: step 60, loss = 13.28 (9.6 examples/sec; 3.350 sec/batch) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8374695145293185
      ],
      "excerpt": "2016-02-17 22:32:50.391206: precision @ 1 = 0.735 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8792222142801163
      ],
      "excerpt": "In order to understand how --fine_tune works, please see the discussion on \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Robinatp/Tensorflow_Model_Inception",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-04-03T09:42:08Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-10-23T01:21:23Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9114530162536111
      ],
      "excerpt": "ImageNet is a common academic data set in machine \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9391770449019184
      ],
      "excerpt": "neural network (CNN) on this academic data set. In particular, we demonstrate \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8061891024678839
      ],
      "excerpt": "This network achieves 21.2% top-1 and 5.6% top-5 error for single frame \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9937456850105914
      ],
      "excerpt": "and with using less than 25 million parameters. Below is a visualization of the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9655816408766675
      ],
      "excerpt": "particular, focus on Training a Model Using Multiple GPU Cards. The model training method is nearly identical to that described in the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9658651476224179,
        0.9642819520451668,
        0.9555399988728857
      ],
      "excerpt": "Updates model parameters synchronously by waiting for all GPUs to finish \n    processing a batch of data. \nThe training procedure is encapsulated by this diagram of how operations and \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8896921714715442,
        0.8541580570067007,
        0.9215330219113497,
        0.930630567682268,
        0.8996602952637801,
        0.9150384606725178
      ],
      "excerpt": "Each tower computes the gradients for a portion of the batch and the gradients \nare combined and averaged across the multiple towers in order to provide a \nsingle update of the Variables stored on the CPU. \nA crucial aspect of training a network of this size is training speed in terms \nof wall-clock time. The training speed is dictated by many factors -- most \nimportantly the batch size and the learning rate schedule. Both of these \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.918592184557904,
        0.9493810104043043,
        0.8765109630095314
      ],
      "excerpt": "Generally speaking, a batch size is a difficult parameter to tune as it requires \nbalancing memory demands of the model, memory available on the GPU and speed of \ncomputation. Generally speaking, employing larger batch sizes leads to more \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8439545666962636
      ],
      "excerpt": "plans based on some selected hardware setups. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8319523076942907
      ],
      "excerpt": ": Build the model. Note that we need to make sure the TensorFlow is ready to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8189296280841785
      ],
      "excerpt": "--data_dir=\"${DATA_DIR}\". The script assumes that there exists a set of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8793051795730645
      ],
      "excerpt": "total loss (starts around 13.0-14.0) and the speed of processing in terms of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.89512165503302
      ],
      "excerpt": "The number of GPU devices is specified by --num_gpus (which defaults to 1). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8319523076942907
      ],
      "excerpt": ": Build the model. Note that we need to make sure the TensorFlow is ready to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.906906490242454
      ],
      "excerpt": "This model splits the batch of 64 images across 2 GPUs and calculates the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8778610053311022,
        0.8574142117721079,
        0.9001976016988245,
        0.9684107618027108
      ],
      "excerpt": "from their respective data (See diagram above). Generally speaking, using larger \nnumbers of GPUs leads to higher throughput as well as the opportunity to use \nlarger batch sizes. In turn, larger batch sizes imply better estimates of the \ngradient enabling the usage of higher learning rates. In summary, using more \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9163659010869007,
        0.9493810104043043,
        0.8765109630095314
      ],
      "excerpt": "Note that selecting a batch size is a difficult parameter to tune as it requires \nbalancing memory demands of the model, memory available on the GPU and speed of \ncomputation. Generally speaking, employing larger batch sizes leads to more \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8108814897931194,
        0.9609840311651009,
        0.9484330460375096
      ],
      "excerpt": "Note that there is considerable noise in the loss function on individual steps \nin the previous log. Because of this noise, it is difficult to discern how well \na model is learning. The solution to the last problem is to launch TensorBoard \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8545670964617713,
        0.9777156785535874
      ],
      "excerpt": "TensorBoard has access to the many Summaries produced by the model that describe \nmultitudes of statistics tracking the model behavior and the quality of the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9779388763261001,
        0.8367200135786905,
        0.9042560298153292
      ],
      "excerpt": "version of the loss. In practice, it is far easier to judge how well a model \nlearns by monitoring the smoothed version of the loss. \nEvaluating an Inception v3 model on the ImageNet 2012 validation data set \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9635971613112164
      ],
      "excerpt": "The evaluation procedure is nearly identical to Evaluating a Model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8319523076942907
      ],
      "excerpt": ": Build the model. Note that we need to make sure the TensorFlow is ready to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9476712420502965
      ],
      "excerpt": "Note that we point --checkpoint_dir to the location of the checkpoints saved \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8231926793024678,
        0.8490037945672047
      ],
      "excerpt": "The script calculates the precision @ 1 over the entire validation data \nperiodically. The precision @ 1 measures the how often the highest scoring \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9194942356599292
      ],
      "excerpt": "statistics of the model activations and weights during evaluation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8275219201229286
      ],
      "excerpt": "data set. This requires two distinct changes to our training procedure: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.877278385732807
      ],
      "excerpt": "The second flag --fine_tune is a boolean that indicates whether the last \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9221976892366528,
        0.8317341183564184
      ],
      "excerpt": "In order to understand how --fine_tune works, please see the discussion on \nVariables in the TensorFlow-Slim README.md. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8319523076942907
      ],
      "excerpt": ": Build the model. Note that we need to make sure the TensorFlow is ready to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8369877035495048
      ],
      "excerpt": "Fine-tuning a model a separate data set requires significantly lowering the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9819050521144246
      ],
      "excerpt": "The flowers data set is quite small so we shrink the size of the shuffling \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9589040312974123,
        0.8527219693197626
      ],
      "excerpt": "    for more details. \nThe training script will only reports the loss. To evaluate the quality of the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8319523076942907
      ],
      "excerpt": ": Build the model. Note that we need to make sure the TensorFlow is ready to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9818949425040004
      ],
      "excerpt": ": Evaluate the fine-tuned model on a hold-out of the flower data set. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8626514663010872
      ],
      "excerpt": "for training or fine-tuning. The main script to employ is \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8476690262375287
      ],
      "excerpt": "build_image_data.py on the data to generate the sharded TFRecord dataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9524974385330247
      ],
      "excerpt": "complete list of information contained in the tf.Example is described in the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9176076186077454
      ],
      "excerpt": ": location to where to save the TFRecord data. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.951232532257774
      ],
      "excerpt": ": convert the data. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9658298083831359
      ],
      "excerpt": "where the $OUTPUT_DIRECTORY is the location of the sharded TFRecords. The \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9865700264210283,
        0.8067922002638356
      ],
      "excerpt": "a list of all of the labels. For instance, in the case flowers data set, the \n$LABELS_FILE contained the following data: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9979977638319577
      ],
      "excerpt": "classifier in the model. That is, the daisy corresponds to the classifier for \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9968029537584643
      ],
      "excerpt": "  $VALIDATION_DIR/validation-00001-of-00024 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9968029537584643
      ],
      "excerpt": "  $VALIDATION_DIR/validation-00023-of-00024 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.905795675945359
      ],
      "excerpt": "respectively. Generally speaking, we aim for selecting the number of shards such \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8398464192622728
      ],
      "excerpt": "are ready to train or fine-tune an Inception model on this data set. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9860329920917615,
        0.9378070129472381
      ],
      "excerpt": "to correspond with your data. \nThe model architecture and training procedure is heavily dependent on the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8012530928922256,
        0.9394449182630016
      ],
      "excerpt": "of training hyper-parameters for your setup. What follows are some general \nconsiderations for novices. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8392007185461967,
        0.8052752431486249
      ],
      "excerpt": "addition to --batch_size and --num_gpus, there are several constants defined \nin inception_train.py which dictate the learning \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8420669633807765,
        0.9725081677090662
      ],
      "excerpt": "There are many papers that discuss the various tricks and trade-offs associated \nwith training a model with stochastic gradient descent. For those new to the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9487305379949895
      ],
      "excerpt": "Y Bengio, Practical recommendations for gradient-based training of deep \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9213941097585057
      ],
      "excerpt": "I Goodfellow, Y Bengio and A Courville, [Deep Learning] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9367561026008094,
        0.8232749828129919,
        0.9618802167906968
      ],
      "excerpt": "What follows is a summary of some general advice for identifying appropriate \nmodel hyper-parameters in the context of this particular model training setup. \nNamely, this library provides synchronous updates to model parameters based on \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8144120033031611,
        0.9239689812900337
      ],
      "excerpt": "Higher learning rates leads to faster training. Too high of learning rate \n    leads to instability and will cause model parameters to diverge to infinity \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9310240299559605,
        0.9372475213993242,
        0.8224122176909267
      ],
      "excerpt": "Larger batch sizes lead to higher quality estimates of the gradient and \n    permit training the model with higher learning rates. \nOften the GPU memory is a bottleneck that prevents employing larger batch \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8483441606128411
      ],
      "excerpt": "    this model splits the batch across the GPUs. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8896708435623395
      ],
      "excerpt": "be factored into hyperparameter tuning. See Large Scale Distributed Deep \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Train/Eval the inception network through the flowers and imagenet dataset",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Robinatp/Tensorflow_Model_Inception/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 29 Dec 2021 23:20:39 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Robinatp/Tensorflow_Model_Inception/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Robinatp/Tensorflow_Model_Inception",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Robinatp/Tensorflow_Model_Inception/master/flower_train_robin.sh",
      "https://raw.githubusercontent.com/Robinatp/Tensorflow_Model_Inception/master/process_bounding_boxes.sh",
      "https://raw.githubusercontent.com/Robinatp/Tensorflow_Model_Inception/master/preprocess_imagenet_validation_data.sh",
      "https://raw.githubusercontent.com/Robinatp/Tensorflow_Model_Inception/master/build_imagenet_data.sh",
      "https://raw.githubusercontent.com/Robinatp/Tensorflow_Model_Inception/master/download_flower.sh",
      "https://raw.githubusercontent.com/Robinatp/Tensorflow_Model_Inception/master/flower_eval_robin.sh",
      "https://raw.githubusercontent.com/Robinatp/Tensorflow_Model_Inception/master/inception/data/build_image_data_generate_validation.sh",
      "https://raw.githubusercontent.com/Robinatp/Tensorflow_Model_Inception/master/inception/data/build_image_data_run.sh",
      "https://raw.githubusercontent.com/Robinatp/Tensorflow_Model_Inception/master/inception/data/download_and_preprocess_flowers_mac.sh",
      "https://raw.githubusercontent.com/Robinatp/Tensorflow_Model_Inception/master/inception/data/download_and_preprocess_imagenet.sh",
      "https://raw.githubusercontent.com/Robinatp/Tensorflow_Model_Inception/master/inception/data/download_imagenet.sh",
      "https://raw.githubusercontent.com/Robinatp/Tensorflow_Model_Inception/master/inception/data/download_and_preprocess_flowers.sh",
      "https://raw.githubusercontent.com/Robinatp/Tensorflow_Model_Inception/master/inception/data/flowers/build_image_data_generate_validation.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "**NOTE** Distributed TensorFlow requires version 0.8 or later.\n\nDistributed TensorFlow lets us use multiple machines to train a model faster.\nThis is quite different from the training with multiple GPU towers on a single\nmachine where all parameters and gradients computation are in the same place. We\ncoordinate the computation across multiple machines by employing a centralized\nrepository for parameters that maintains a unified, single copy of model\nparameters. Each individual machine sends gradient updates to the centralized\nparameter repository which coordinates these updates and sends back updated\nparameters to the individual machines running the model training.\n\nWe term each machine that runs a copy of the training a `worker` or `replica`.\nWe term each machine that maintains model parameters a `ps`, short for\n`parameter server`. Note that we might have more than one machine acting as a\n`ps` as the model parameters may be sharded across multiple machines.\n\nVariables may be updated with synchronous or asynchronous gradient updates. One\nmay construct a an [`Optimizer`](https://www.tensorflow.org/api_docs/python/train.html#optimizers) in TensorFlow\nthat constructs the necessary graph for either case diagrammed below from the\nTensorFlow [Whitepaper](http://download.tensorflow.org/paper/whitepaper2015.pdf):\n\n<div style=\"width:40%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n  <img style=\"width:100%\"\n  src=\"https://www.tensorflow.org/images/tensorflow_figure7.png\">\n</div>\n\nIn [a recent paper](https://arxiv.org/abs/1604.00981), synchronous gradient\nupdates have demonstrated to reach higher accuracy in a shorter amount of time.\nIn this distributed Inception example we employ synchronous gradient updates.\n\nNote that in this example each replica has a single tower that uses one GPU.\n\nThe command-line flags `worker_hosts` and `ps_hosts` specify available servers.\nThe same binary will be used for both the `worker` jobs and the `ps` jobs.\nCommand line flag `job_name` will be used to specify what role a task will be\nplaying and `task_id` will be used to identify which one of the jobs it is\nrunning. Several things to note here:\n\n*   The numbers of `ps` and `worker` tasks are inferred from the lists of hosts\n    specified in the flags. The `task_id` should be within the range `[0,\n    num_ps_tasks)` for `ps` tasks and `[0, num_worker_tasks)` for `worker`\n    tasks.\n*   `ps` and `worker` tasks can run on the same machine, as long as that machine\n    has sufficient resources to handle both tasks. Note that the `ps` task does\n    not benefit from a GPU, so it should not attempt to use one (see below).\n*   Multiple `worker` tasks can run on the same machine with multiple GPUs so\n    machine_A with 2 GPUs may have 2 workers while machine_B with 1 GPU just has\n    1 worker.\n*   The default learning rate schedule works well for a wide range of number of\n    replicas [25, 50, 100] but feel free to tune it for even better results.\n*   The command line of both `ps` and `worker` tasks should include the complete\n    list of `ps_hosts` and `worker_hosts`.\n*   There is a chief `worker` among all workers which defaults to `worker` 0.\n    The chief will be in charge of initializing all the parameters, writing out\n    the summaries and the checkpoint. The checkpoint and summary will be in the\n    `train_dir` of the host for `worker` 0.\n*   Each worker processes a batch_size number of examples but each gradient\n    update is computed from all replicas. Hence, the effective batch size of\n    this model is batch_size * num_workers.\n\n```shell\n#: Build the model. Note that we need to make sure the TensorFlow is ready to\n#: use before this as this command will not build TensorFlow.\ncd tensorflow-models/inception\nbazel build //inception:imagenet_distributed_train\n\n#: To start worker 0, go to the worker0 host and run the following (Note that\n#: task_id should be in the range [0, num_worker_tasks):\nbazel-bin/inception/imagenet_distributed_train \\\n--batch_size=32 \\\n--data_dir=$HOME/imagenet-data \\\n--job_name='worker' \\\n--task_id=0 \\\n--ps_hosts='ps0.example.com:2222' \\\n--worker_hosts='worker0.example.com:2222,worker1.example.com:2222'\n\n#: To start worker 1, go to the worker1 host and run the following (Note that\n#: task_id should be in the range [0, num_worker_tasks):\nbazel-bin/inception/imagenet_distributed_train \\\n--batch_size=32 \\\n--data_dir=$HOME/imagenet-data \\\n--job_name='worker' \\\n--task_id=1 \\\n--ps_hosts='ps0.example.com:2222' \\\n--worker_hosts='worker0.example.com:2222,worker1.example.com:2222'\n\n#: To start the parameter server (ps), go to the ps host and run the following (Note\n#: that task_id should be in the range [0, num_ps_tasks):\nbazel-bin/inception/imagenet_distributed_train \\\n--job_name='ps' \\\n--task_id=0 \\\n--ps_hosts='ps0.example.com:2222' \\\n--worker_hosts='worker0.example.com:2222,worker1.example.com:2222'\n```\n\nIf you have installed a GPU-compatible version of TensorFlow, the `ps` will also\ntry to allocate GPU memory although it is not helpful. This could potentially\ncrash the worker on the same machine as it has little to no GPU memory to\nallocate. To avoid this, you can prepend the previous command to start `ps`\nwith: `CUDA_VISIBLE_DEVICES=''`\n\n```shell\nCUDA_VISIBLE_DEVICES='' bazel-bin/inception/imagenet_distributed_train \\\n--job_name='ps' \\\n--task_id=0 \\\n--ps_hosts='ps0.example.com:2222' \\\n--worker_hosts='worker0.example.com:2222,worker1.example.com:2222'\n```\n\nIf you have run everything correctly, you should see a log in each `worker` job\nthat looks like the following. Note the training speed varies depending on your\nhardware and the first several steps could take much longer.\n\n```shell\nINFO:tensorflow:PS hosts are: ['ps0.example.com:2222', 'ps1.example.com:2222']\nINFO:tensorflow:Worker hosts are: ['worker0.example.com:2222', 'worker1.example.com:2222']\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {ps0.example.com:2222, ps1.example.com:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {localhost:2222, worker1.example.com:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2222\nINFO:tensorflow:Created variable global_step:0 with shape () and init <function zeros_initializer at 0x7f6aa014b140>\n\n...\n\nINFO:tensorflow:Created variable logits/logits/biases:0 with shape (1001,) and init <function _initializer at 0x7f6a77f3cf50>\nINFO:tensorflow:SyncReplicas enabled: replicas_to_aggregate=2; total_num_replicas=2\nINFO:tensorflow:2016-04-13 01:56:26.405639 Supervisor\nINFO:tensorflow:Started 2 queues for processing input data.\nINFO:tensorflow:global_step/sec: 0\nINFO:tensorflow:Worker 0: 2016-04-13 01:58:40.342404: step 0, loss = 12.97(0.0 examples/sec; 65.428 \u00a0sec/batch)\nINFO:tensorflow:global_step/sec: 0.0172907\n...\n```\n\nand a log in each `ps` job that looks like the following:\n\n```shell\nINFO:tensorflow:PS hosts are: ['ps0.example.com:2222', 'ps1.example.com:2222']\nINFO:tensorflow:Worker hosts are: ['worker0.example.com:2222', 'worker1.example.com:2222']\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {localhost:2222, ps1.example.com:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {worker0.example.com:2222, worker1.example.com:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2222\n```\n\nIf you compiled TensorFlow (from v1.1-rc3) with VERBS support and you have the\nrequired device and IB verbs SW stack, you can specify --protocol='grpc+verbs'\nIn order to use Verbs RDMA for Tensor passing between workers and ps.\nNeed to add the the --protocol flag in all tasks (ps and workers).\nThe default protocol is the TensorFlow default protocol of grpc.\n\n\n[Congratulations!](https://www.youtube.com/watch?v=9bZkp7q19f0) You are now\ntraining Inception in a distributed manner.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9005268782370038
      ],
      "excerpt": "we emphasize that depending your hardware set up, you may need to adapt the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8046358283464713
      ],
      "excerpt": "To train this model, you simply need to specify the following: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8617517069739578
      ],
      "excerpt": ": Build the model. Note that we need to make sure the TensorFlow is ready to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9633959038491905,
        0.9096104964140866
      ],
      "excerpt": "cd tensorflow-models/inception \nbazel build //inception:imagenet_train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9748709027320682
      ],
      "excerpt": "GPU cards. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8617517069739578
      ],
      "excerpt": ": Build the model. Note that we need to make sure the TensorFlow is ready to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9633959038491905,
        0.9096104964140866
      ],
      "excerpt": "cd tensorflow-models/inception \nbazel build //inception:imagenet_train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8617517069739578
      ],
      "excerpt": ": Build the model. Note that we need to make sure the TensorFlow is ready to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9633959038491905,
        0.9096104964140866
      ],
      "excerpt": "cd tensorflow-models/inception \nbazel build //inception:imagenet_eval \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9086318404424183
      ],
      "excerpt": "following output: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8617517069739578
      ],
      "excerpt": ": Build the model. Note that we need to make sure the TensorFlow is ready to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9633959038491905,
        0.9096104964140866
      ],
      "excerpt": "cd tensorflow-models/inception \nbazel build //inception:flowers_train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8617517069739578
      ],
      "excerpt": ": Build the model. Note that we need to make sure the TensorFlow is ready to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9633959038491905,
        0.9096104964140866
      ],
      "excerpt": "cd tensorflow-models/inception \nbazel build //inception:flowers_eval \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8189948110380839
      ],
      "excerpt": "One can use the existing scripts supplied with this model to build a new dataset \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.859109834983323
      ],
      "excerpt": "your class labels will range from 0 to num_labels. Using the example above, the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8890531308279668
      ],
      "excerpt": "To run build_image_data.py, you can run the following command line: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9633959038491905,
        0.9096104964140866
      ],
      "excerpt": "cd tensorflow-models/inception \nbazel build //inception:build_image_data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8923959636823277
      ],
      "excerpt": "Note, if you are piggy backing on the flowers retraining scripts, be sure to \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8348366884328697
      ],
      "excerpt": "CIFAR-10 multi-GPU model training. Briefly, the model training \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8195561982512685
      ],
      "excerpt": "  <img style=\"width:100%\" src=\"https://www.tensorflow.org/images/Parallelism.png\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.822093365555733
      ],
      "excerpt": "Here is the output of the above command line when running on a Tesla K40c: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.802651658039177
      ],
      "excerpt": "bazel-bin/inception/imagenet_train --num_gpus=2 --batch_size=64 --train_dir=/tmp/imagenet_train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8000988887675343
      ],
      "excerpt": "Evaluating an Inception v3 model on the ImageNet 2012 validation data set \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8077319622579018
      ],
      "excerpt": "script begins training. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "  --fine_tune=True \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8135198168526546
      ],
      "excerpt": "2016-03-01 16:53:05.450419: [20 batches out of 20] (36.5 examples/sec; 0.684sec/batch) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639986685036579
      ],
      "excerpt": "  $TRAIN_DIR/dog/image1.jpg \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639986685036579
      ],
      "excerpt": "  $VALIDATION_DIR/dog/imageB.jpg \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639986685036579
      ],
      "excerpt": "  $VALIDATION_DIR/cat/cat.JPG \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8808876310242024,
        0.8003557346168833
      ],
      "excerpt": "build_image_data.py on the data to generate the sharded TFRecord dataset. \nEach entry of the TFRecord is a serialized tf.Example protocol buffer. A \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.869711104498328,
        0.8633752359811768
      ],
      "excerpt": "comments of build_image_data.py. \nTo run build_image_data.py, you can run the following command line: \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Robinatp/Tensorflow_Model_Inception/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Inception in TensorFlow",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Tensorflow_Model_Inception",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Robinatp",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Robinatp/Tensorflow_Model_Inception/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 29 Dec 2021 23:20:39 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Before you run the training script for the first time, you will need to download\nand convert the ImageNet data to native TFRecord format. The TFRecord format\nconsists of a set of sharded files where each entry is a serialized `tf.Example`\nproto. Each `tf.Example` proto contains the ImageNet image (JPEG encoded) as\nwell as metadata such as label and bounding box information. See\n[`parse_example_proto`](inception/image_processing.py) for details.\n\nWe provide a single [script](inception/data/download_and_preprocess_imagenet.sh) for\ndownloading and converting ImageNet data to TFRecord format. Downloading and\npreprocessing the data may take several hours (up to half a day) depending on\nyour network and computer speed. Please be patient.\n\nTo begin, you will need to sign up for an account with [ImageNet](http://image-net.org) to gain access to the data. Look for the sign up page,\ncreate an account and request an access key to download the data.\n\nAfter you have `USERNAME` and `PASSWORD`, you are ready to run our script. Make\nsure that your hard disk has at least 500 GB of free space for downloading and\nstoring the data. Here we select `DATA_DIR=$HOME/imagenet-data` as such a\nlocation but feel free to edit accordingly.\n\nWhen you run the below script, please enter *USERNAME* and *PASSWORD* when\nprompted. This will occur at the very beginning. Once these values are entered,\nyou will not need to interact with the script again.\n\n```shell\n#: location of where to place the ImageNet data\nDATA_DIR=$HOME/imagenet-data\n\n#: build the preprocessing script.\ncd tensorflow-models/inception\nbazel build //inception:download_and_preprocess_imagenet\n\n#: run it\nbazel-bin/inception/download_and_preprocess_imagenet \"${DATA_DIR}\"\n```\n\nThe final line of the output script should read:\n\n```shell\n2016-02-17 14:30:17.287989: Finished writing all 1281167 images in data set.\n```\n\nWhen the script finishes, you will find 1024 training files and 128 validation\nfiles in the `DATA_DIR`. The files will match the patterns\n`train-?????-of-01024` and `validation-?????-of-00128`, respectively.\n\n[Congratulations!](https://www.youtube.com/watch?v=9bZkp7q19f0) You are now\nready to train or evaluate with the ImageNet data set.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Much like training the ImageNet model we must first convert a new data set to\nthe sharded TFRecord format which each entry is a serialized `tf.Example` proto.\n\nWe have provided a script demonstrating how to do this for small data set of of\na few thousand flower images spread across 5 labels:\n\n```shell\ndaisy, dandelion, roses, sunflowers, tulips\n```\n\nThere is a single automated script that downloads the data set and converts it\nto the TFRecord format. Much like the ImageNet data set, each record in the\nTFRecord format is a serialized `tf.Example` proto whose entries include a\nJPEG-encoded string and an integer label. Please see [`parse_example_proto`](inception/image_processing.py) for details.\n\nThe script just takes a few minutes to run depending your network connection\nspeed for downloading and processing the images. Your hard disk requires 200MB\nof free storage. Here we select `DATA_DIR=/tmp/flowers-data/` as such a location\nbut feel free to edit accordingly.\n\n```shell\n#: location of where to place the flowers data\nFLOWERS_DATA_DIR=/tmp/flowers-data/\n\n#: build the preprocessing script.\ncd tensorflow-models/inception\nbazel build //inception:download_and_preprocess_flowers\n\n#: run it\nbazel-bin/inception/download_and_preprocess_flowers \"${FLOWERS_DATA_DIR}\"\n```\n\nIf the script runs successfully, the final line of the terminal output should\nlook like:\n\n```shell\n2016-02-24 20:42:25.067551: Finished writing all 3170 images in data set.\n```\n\nWhen the script finishes you will find 2 shards for the training and validation\nfiles in the `DATA_DIR`. The files will match the patterns `train-?????-of-00002`\nand `validation-?????-of-00002`, respectively.\n\n**NOTE** If you wish to prepare a custom image data set for transfer learning,\nyou will need to invoke [`build_image_data.py`](inception/data/build_image_data.py) on\nyour custom data set. Please see the associated options and assumptions behind\nthis script by reading the comments section of [`build_image_data.py`](inception/data/build_image_data.py). Also, if your custom data has a different\nnumber of examples or classes, you need to change the appropriate values in\n[`imagenet_data.py`](inception/imagenet_data.py).\n\nThe second piece you will need is a trained Inception v3 image model. You have\nthe option of either training one yourself (See [How to Train from Scratch](#how-to-train-from-scratch) for details) or you can download a pre-trained\nmodel like so:\n\n```shell\n#: location of where to place the Inception v3 model\nINCEPTION_MODEL_DIR=$HOME/inception-v3-model\nmkdir -p ${INCEPTION_MODEL_DIR}\ncd ${INCEPTION_MODEL_DIR}\n\n#: download the Inception v3 model\ncurl -O http://download.tensorflow.org/models/image/imagenet/inception-v3-2016-03-01.tar.gz\ntar xzf inception-v3-2016-03-01.tar.gz\n\n#: this will create a directory called inception-v3 which contains the following files.\n> ls inception-v3\nREADME.txt\ncheckpoint\nmodel.ckpt-157585\n```\n\n[Congratulations!](https://www.youtube.com/watch?v=9bZkp7q19f0) You are now\nready to fine-tune your pre-trained Inception v3 model with the flower data set.\n\n",
      "technique": "Header extraction"
    }
  ]
}