{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1810.04805"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9321448258727583
      ],
      "excerpt": "PyTorch implementation of BERT in \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" (https://arxiv.org/abs/1810.04805) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dreamgonfly/BERT-pytorch",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-10-19T17:00:33Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-23T05:26:59Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.910560416869953
      ],
      "excerpt": "PyTorch implementation of BERT in \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" (https://arxiv.org/abs/1810.04805) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8528293882180019,
        0.8420598185793791
      ],
      "excerpt": "This step trains BERT model with unsupervised objective. Also this step does: \n- logs the training procedure for every epoch \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8737588093243828
      ],
      "excerpt": "- reports the best checkpoint based on validation metric \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9836956661717811
      ],
      "excerpt": "Transformer-pytorch : My own implementation of Transformer. This BERT implementation is based on this repo. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "PyTorch implementation of BERT in \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\"",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dreamgonfly/BERT-pytorch/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 21,
      "date": "Wed, 29 Dec 2021 05:12:21 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/dreamgonfly/BERT-pytorch/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "dreamgonfly/BERT-pytorch",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/dreamgonfly/BERT-pytorch/master/run.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "First things first, you need to prepare your data in an appropriate format. \nYour corpus is assumed to follow the below constraints.\n\n- Each line is a *document*.\n- A *document* consists of *sentences*, seperated by vertical bar (|).\n- A *sentence* is assumed to be already tokenized. Tokens are seperated by space.\n- A *sentence* has no more than 256 tokens.\n- A *document* has at least 2 sentences. \n- You have two distinct data files, one for train data and the other for val data.\n\nThis repo comes with example data for pretraining in data/example directory.\nHere is the content of data/example/train.txt file.\n\n```\nOne, two, three, four, five,|Once I caught a fish alive,|Six, seven, eight, nine, ten,|Then I let go again.\nI\u2019m a little teapot|Short and stout|Here is my handle|Here is my spout.\nJack and Jill went up the hill|To fetch a pail of water.|Jack fell down and broke his crown,|And Jill came tumbling after.  \n```\n\nAlso, this repo includes SST-2 data in data/SST-2 directory for sentiment classification.\n\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9172043062016124,
        0.8275787489822739,
        0.9564512786668816
      ],
      "excerpt": "python bert.py preprocess-index data/example/train.txt --dictionary=dictionary.txt \nRunning the above command produces dictionary.txt file in your current directory. \npython bert.py pretrain --train_data data/example/train.txt --val_data data/example/val.txt --checkpoint_output model.pth \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9361192905542383
      ],
      "excerpt": "python bert.py finetune --pretrained_checkpoint model.pth --train_data data/SST-2/train.tsv --val_data data/SST-2/dev.tsv \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/dreamgonfly/BERT-pytorch/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "The Unlicense",
      "url": "https://api.github.com/licenses/unlicense"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'This is free and unencumbered software released into the public domain.\\n\\nAnyone is free to copy, modify, publish, use, compile, sell, or\\ndistribute this software, either in source code form or as a compiled\\nbinary, for any purpose, commercial or non-commercial, and by any\\nmeans.\\n\\nIn jurisdictions that recognize copyright laws, the author or authors\\nof this software dedicate any and all copyright interest in the\\nsoftware to the public domain. We make this dedication for the benefit\\nof the public at large and to the detriment of our heirs and\\nsuccessors. We intend this dedication to be an overt act of\\nrelinquishment in perpetuity of all present and future rights to this\\nsoftware under copyright law.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\\nIN NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY CLAIM, DAMAGES OR\\nOTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\\nARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\\nOTHER DEALINGS IN THE SOFTWARE.\\n\\nFor more information, please refer to http://unlicense.org\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "BERT-pytorch",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "BERT-pytorch",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "dreamgonfly",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dreamgonfly/BERT-pytorch/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Python 3.6+\n- [PyTorch 4.1+](http://pytorch.org/)\n- [tqdm](https://github.com/tqdm/tqdm)\n\nAll dependencies can be installed via:\n\n```\npip install -r requirements.txt\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 59,
      "date": "Wed, 29 Dec 2021 05:12:21 GMT"
    },
    "technique": "GitHub API"
  }
}