{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1911.05722"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find this project useful in your research, please consider citing:\n\n```\n   @article{ehsani2020learning,\n     title={Learning Visual Representation from Human Interactions},\n     author={Ehsani, Kiana and Gordon, Daniel and Nguyen, Thomas and Mottaghi, Roozbeh and Farhadi, Ali},\n     journal={International Conference on Learning Representations},\n     year={2021}\n   }\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9507374082549614,
        0.9992304604420942,
        0.9747466821846071,
        0.9909844968086463,
        0.8592871015078041
      ],
      "excerpt": "    <li> Scene classification (semantic) \n    <li> Action recognition (temporal) \n    <li> Depth estimation (geometric) \n    <li> Dynamics prediction (physics) \n    <li> Walkable surface estimation (affordance) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ehsanik/muscleTorch",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-10-13T21:27:43Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-16T09:21:43Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8247626860541872
      ],
      "excerpt": "(Project Page) (PDF) (Slides) (Video) (Presentation) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9942275745006779
      ],
      "excerpt": "Learning effective representations of visual data that generalize to a variety of downstream tasks has been a long quest for computer vision. Most representation learning approaches rely solely on visual data such as images or videos. In this paper, we explore a novel approach, where we use human interaction and attention cues to investigate whether we can learn better representations compared to visual-only representations. For this study, we collect a dataset of human interactions capturing <b>body part movements</b> and <b>gaze</b> in their daily lives. Our experiments show that our self-supervised representation that encodes interaction and attention cues outperforms a visual-only state-of-the-art method MoCo, on a variety of target tasks:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9829732992694064
      ],
      "excerpt": "We introduce a new dataset of human interactions for our representation learning framework. We record egocentric videos from a GoPro camera attached to the subjects' forehead. We simultaneously capture body movements, as well as the gaze. We use Tobii Pro2 eye-tracking to track the center of the gaze in the camera frame. We record the body part movements using BNO055 Inertial Measurement Units (IMUs) in 10 different locations (torso, neck, 2 triceps, 2 forearms, 2 thighs, and 2 legs).  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9207749915120322
      ],
      "excerpt": "The structure of the dataset is as follows: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "What Can You Learn from Your Muscles? Learning Visual Representation from Human Interactions (https://arxiv.org/pdf/2010.08539.pdf)",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ehsanik/muscleTorch/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 4,
      "date": "Sun, 26 Dec 2021 02:14:06 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ehsanik/muscleTorch/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "ehsanik/muscleTorch",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/ehsanik/muscleTorch/main/scripts/end_task_representation.sh",
      "https://raw.githubusercontent.com/ehsanik/muscleTorch/main/scripts/training_representation.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Clone the repository using the command:\n\n```\ngit clone https://github.com/ehsanik/muscleTorch\ncd muscleTorch\n```\n\n2. Install requirements:\n\n```\npip3 install -r requirements.txt\n```\n\n\n3. Download the images from [here](https://drive.google.com/file/d/1znyScxE50y9UpoVfKRHCBFyxYeoNd4rM/view?usp=sharing) and extract it to HumanDataset/images.\n4. Download the sensor data from [here](https://drive.google.com/file/d/1dLTpGhOlWJ6TctVe2iWLIGg2M7TkiBSq/view?usp=sharing) and extract it to HumanDataset/annotation_h5.\n5. Download pretrained weights from [here](https://drive.google.com/file/d/1IStOGhi8Qq_-5J2MH5MpCqeVyBTecNVs/view?usp=sharing) for reproducing the numbers in the paper, extract it to HumanDataset/saved_weights. \n\n\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8739371137238587
      ],
      "excerpt": "<div style=\"text-align: center;\"><img src=\"figs/human_teaser.png\" height=\"250px\" ></div> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8739371137238587
      ],
      "excerpt": "<div style=\"text-align: center;\"><img src=\"figs/human_dataset.png\" height=\"300px\" ></div> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8192612967105026
      ],
      "excerpt": "\u2502       \u2514\u2500\u2500 images_<video_stamp><INDEX>.jpg \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9213716497003656,
        0.9325294688271202,
        0.9325294688271202,
        0.9325294688271202
      ],
      "excerpt": "\u2502   \u251c\u2500\u2500 [test/train]<feature_name>.h5 \n\u2502   \u251c\u2500\u2500 [test/train]_image_name.json \n\u2502   \u251c\u2500\u2500 [test/train]_h5pyind_2_frameind.json \n\u2502   \u2514\u2500\u2500 [test/train]_timestamp.json \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9301230550246409
      ],
      "excerpt": "python3 main.py --gpu-ids 0 --arch MoCoGazeIMUModel --input_length 5 --sequence_length 5 --output_length 5 \\ \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ehsanik/muscleTorch/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "[What Can You Learn from Your Muscles?</br> Learning Visual Representation from Human Interactions](https://arxiv.org/pdf/2010.08539.pdf)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "muscleTorch",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "ehsanik",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ehsanik/muscleTorch/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 26,
      "date": "Sun, 26 Dec 2021 02:14:06 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "representation-learning",
      "human-activities",
      "gaze-tracking",
      "imu-data"
    ],
    "technique": "GitHub API"
  }
}