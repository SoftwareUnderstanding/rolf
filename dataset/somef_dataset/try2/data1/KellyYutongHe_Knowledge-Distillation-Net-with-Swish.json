{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1503.02531\nhttps://pytorch.org/tutorials/advanced/cpp_extension.html\nhttps://github.com/pytorch/examples/tree/master/mnist \nhttps://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\nhttps://github.com/peterliht/knowledge-distillation-pytorch "
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "https://arxiv.org/pdf/1710.05941.pdf\nhttps://arxiv.org/abs/1503.02531\nhttps://pytorch.org/tutorials/advanced/cpp_extension.html\nhttps://github.com/pytorch/examples/tree/master/mnist \nhttps://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\nhttps://github.com/peterliht/knowledge-distillation-pytorch \n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/KellyYutongHe/Knowledge-Distillation-Net-with-Swish",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-09-29T22:16:17Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-10-11T23:10:44Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8627175655858977,
        0.8081387512470747,
        0.8115145447425248,
        0.8535872418131173
      ],
      "excerpt": "- swish_function.cpp: C++ implementation of the Swish Activation Function \n- autograd_check.py: check the Swish implementation with torch.autograd.gradcheck \n- kd.py: student network, training and testing with Swish and KD (both on the fly and disk caching) \n- plots/: all the plots of the loss, top1 and top5 accuracy generated by plot.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8588966184349252
      ],
      "excerpt": "The student network has two conv layers with output channel number of 64 and 128 respectively. Each conv layer is also bundled with a max pooling layer with kernel size of 2 and stride of 2, a BN layer and an activation layer. After the conv layers two fc layers are applied. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8369142374719467
      ],
      "excerpt": "    Cross-entropy is used in all experiments. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9465781384006222,
        0.9911782218850568
      ],
      "excerpt": "    I implemented the Swish activation function in C++ based on the paper provided in the instruction. Since beta should be set to be trainable, I also calculated the derivative of beta, which is x^2sigmoid(betax)(1-sigmoid(betax)). \nComparison of Student Network with ReLu and Swish \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/KellyYutongHe/Knowledge-Distillation-Net-with-Swish/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 29 Dec 2021 14:33:26 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/KellyYutongHe/Knowledge-Distillation-Net-with-Swish/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "KellyYutongHe/Knowledge-Distillation-Net-with-Swish",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/KellyYutongHe/Knowledge-Distillation-Net-with-Swish/master/run.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8345631225383071
      ],
      "excerpt": "Main files you may be interested in are the following: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8489500089793839
      ],
      "excerpt": "- resnet.py: teacher network, training and testing with Swish \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/KellyYutongHe/Knowledge-Distillation-Net-with-Swish/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "C++",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Knowledge Distillation Net with Swish",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Knowledge-Distillation-Net-with-Swish",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "KellyYutongHe",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/KellyYutongHe/Knowledge-Distillation-Net-with-Swish/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 29 Dec 2021 14:33:26 GMT"
    },
    "technique": "GitHub API"
  }
}