{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2103.09474",
      "https://arxiv.org/abs/2103.09474",
      "https://arxiv.org/abs/2103.09474"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- [ming024's FastSpeech2](https://github.com/ming024/FastSpeech2)\n- [auspicious3000's SpeechSplit](https://github.com/auspicious3000/SpeechSplit)\n- [philipperemy's DeepSpeaker](https://github.com/philipperemy/deep-speaker)\n- [jik876's HiFi-GAN](https://github.com/jik876/hifi-gan)\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "If you would like to use or refer to this implementation, please cite our paper with the repo.\n```bash\n@article{lee2021styler,\n  title={STYLER: Style Modeling with Rapidity and Robustness via SpeechDecomposition for Expressive and Controllable Neural Text to Speech},\n  author={Lee, Keon and Park, Kyumin and Kim, Daeyoung},\n  journal={arXiv preprint arXiv:2103.09474},\n  year={2021}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Reference audio preparation has a similar process to training data preparation. There could be two kinds of references: clean and noisy.\n\nFirst, put clean audios with corresponding texts in a single directory and modify the `hp.ref_audio_dir` in `hparams.py` and process all the necessary features. Refer to the `Clean Data` section of `Train Preparation`.\n\n```bash\npython3 preprocess_refs.py\n```\n\nThen, get the noisy references.\n\n```bash\npython3 preprocess_noisy.py --refs\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{lee2021styler,\n  title={STYLER: Style Modeling with Rapidity and Robustness via SpeechDecomposition for Expressive and Controllable Neural Text to Speech},\n  author={Lee, Keon and Park, Kyumin and Kim, Daeyoung},\n  journal={arXiv preprint arXiv:2103.09474},\n  year={2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8854398367006624
      ],
      "excerpt": "wget http://www.openslr.org/resources/11/librispeech-lexicon.txt -O montreal-forced-aligner/pretrained_models/librispeech-lexicon.txt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8090016440670298
      ],
      "excerpt": "   ./montreal-forced-aligner/bin/mfa_align $YOUR_data_dir montreal-forced-aligner/pretrained_models/librispeech-lexicon.txt english $YOUR_PREPROCESSED_PATH -j 8 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/keonlee9420/STYLER",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-03-23T05:40:37Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-11T05:36:10Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9572715995108629
      ],
      "excerpt": "In our paper, we propose STYLER, a non-autoregressive TTS framework with style factor modeling that achieves rapidity, robustness, expressivity, and controllability at the same time. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9932783819022446
      ],
      "excerpt": "Abstract: Previous works on neural text-to-speech (TTS) have been addressed on limited speed in training and inference time, robustness for difficult synthesis conditions, expressiveness, and controllability. Although several approaches resolve some limitations, there has been no attempt to solve all weaknesses at once. In this paper, we propose STYLER, an expressive and controllable TTS framework with high-speed and robust synthesis. Our novel audio-text aligning method called Mel Calibrator and excluding autoregressive decoding enable rapid training and inference and robust synthesis on unseen data. Also, disentangled style factor modeling under supervision enlarges the controllability in synthesizing process leading to expressive TTS. On top of it, a novel noise modeling pipeline using domain adversarial training and Residual Decoding empowers noise-robust style transfer, decomposing the noise without any additional label. Various experiments demonstrate that STYLER is more effective in speed and robustness than expressive TTS with autoregressive decoding and more expressive and controllable than reading style non-autoregressive TTS. Synthesis samples and experiment results are provided via our demo page, and code is available publicly. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8154388029173811
      ],
      "excerpt": "We provide a bash script for the resampling. Refer to data/resample.sh for the detail. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8850477490239702
      ],
      "excerpt": "First, download ResCNN Softmax+Triplet pretrained model of philipperemy's DeepSpeaker for the speaker embedding as described in our paper and locate it in hp.speaker_embedder_dir. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8841538644653129
      ],
      "excerpt": "Then, process all the necessary features. You will get a stat.txt file in your hp.preprocessed_path/. You have to modify the f0 and energy parameters in the hparams.py according to the content of stat.txt. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8897429290909418
      ],
      "excerpt": "Finally, get the noisy data separately from the clean data by mixing each utterance with a randomly selected piece of background noise from WHAM! dataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8737629989794978
      ],
      "excerpt": "The following command will synthesize all combinations of texts in data/sentences.py and audios in hp.ref_audio_dir. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9539783099901358,
        0.871708115338645
      ],
      "excerpt": "--inspection will give you additional outputs that show the effects of each encoder of STYLER. The samples are the same as the Style Factor Modeling section on our demo page. \n--cont will generate the samples as the Style Factor Control section on our demo page. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9435296777510643
      ],
      "excerpt": "Here are some logging views of the model training on VCTK for 560k steps. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.857832752768104
      ],
      "excerpt": "There were too many noise data where extraction was not possible through pyworld as in clean data. To resolve this, pysptk was applied to extract log f0 for the noisy data's fundamental frequency. The --noisy_input option will automate this process during synthesizing. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9040724821518481
      ],
      "excerpt": "   #: Replace $data_dir and $PREPROCESSED_PATH with ./VCTK-Corpus-92/wav48_silence_trimmed and ./preprocessed/VCTK/TextGrid, for example \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "STYLER: Style Factor Modeling with Rapidity and Robustness via Speech Decomposition for Expressive and Controllable Neural Text to Speech, Interspeech 2021",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/keonlee9420/STYLER/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 21,
      "date": "Sun, 26 Dec 2021 02:20:10 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/keonlee9420/STYLER/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "keonlee9420/STYLER",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/keonlee9420/STYLER/deploy/data/resample.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Reference audio preparation has a similar process to training data preparation. There could be two kinds of references: clean and noisy.\n\nFirst, put clean audios with corresponding texts in a single directory and modify the `hp.ref_audio_dir` in `hparams.py` and process all the necessary features. Refer to the `Clean Data` section of `Train Preparation`.\n\n```bash\npython3 preprocess_refs.py\n```\n\nThen, get the noisy references.\n\n```bash\npython3 preprocess_noisy.py --refs\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Create `sentences.py` in `data/` which has a python list named `sentences` of texts to be synthesized. Note that `sentences` can contain more than one text.\n\n```bash\n#: In 'data/sentences.py',\nsentences = [\n    \"Nothing is lost, everything is recycled.\"\n]\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9359511701422157,
        0.8644500257064217
      ],
      "excerpt": "wget https://github.com/MontrealCorpusTools/Montreal-Forced-Aligner/releases/download/v1.1.0-beta.2/montreal-forced-aligner_linux.tar.gz \ntar -zxvf montreal-forced-aligner_linux.tar.gz \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.966621918999493
      ],
      "excerpt": "Now you have all the prerequisites! Train the model using the following command: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8560831683719954
      ],
      "excerpt": "    <img src=\"figs/model_wb.png\" width=\"70%\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8103751381110619
      ],
      "excerpt": "Put audio files and corresponding text (transcript) files in the same directory. Both audio and text files must have the same name, excluding the extension. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8436175713143576
      ],
      "excerpt": "Modify the hp.data_dir in hparams.py. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8436175713143576,
        0.8553295955760117
      ],
      "excerpt": "Modify the hp.noise_dir in hparams.py. \nUnzip hifigan/generator_universal.pth.tar.zip in the same directory. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991
      ],
      "excerpt": "python3 preprocess.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991
      ],
      "excerpt": "python3 preprocess_noisy.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.950563948951535,
        0.8173358712982418
      ],
      "excerpt": "python3 train.py \nThe following command will synthesize all combinations of texts in data/sentences.py and audios in hp.ref_audio_dir. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665509919451717
      ],
      "excerpt": "python3 synthesize.py --ckpt CHECKPOINT_PATH \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665509919451717
      ],
      "excerpt": "python3 synthesize.py --ckpt CHECKPOINT_PATH --ref_name AUDIO_FILENAME \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665509919451717
      ],
      "excerpt": "   python3 synthesize.py --ckpt CHECKPOINT_PATH --cont --r1 AUDIO_FILENAME_1 --r2 AUDIO_FILENAME_1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.909130101466226
      ],
      "excerpt": "    <img src=\"./figs/tensorboard_scalars_560k.png\" width=\"100%\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.909130101466226
      ],
      "excerpt": "    <img src=\"./figs/tensorboard_images_560k.png\" width=\"100%\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.909130101466226
      ],
      "excerpt": "    <img src=\"./figs/tensorboard_audio_560k.png\" width=\"100%\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8560831683719954
      ],
      "excerpt": "        <img src=\"./figs/spker_embed_tsne.png\" width=\"70%\"> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/keonlee9420/STYLER/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 Philippe R\\xc3\\xa9my\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "STYLER: Style Factor Modeling with Rapidity and Robustness via Speech Decomposition for Expressive and Controllable Neural Text to Speech",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "STYLER",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "keonlee9420",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/keonlee9420/STYLER/blob/deploy/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "keonlee9420",
        "body": "",
        "dateCreated": "2021-04-23T06:46:27Z",
        "datePublished": "2021-05-15T13:33:54Z",
        "html_url": "https://github.com/keonlee9420/STYLER/releases/tag/v0.1.0",
        "name": "First Release",
        "tag_name": "v0.1.0",
        "tarball_url": "https://api.github.com/repos/keonlee9420/STYLER/tarball/v0.1.0",
        "url": "https://api.github.com/repos/keonlee9420/STYLER/releases/42996903",
        "zipball_url": "https://api.github.com/repos/keonlee9420/STYLER/zipball/v0.1.0"
      }
    ],
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please install the python dependencies given in `requirements.txt`.\n\n```bash\npip3 install -r requirements.txt\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 68,
      "date": "Sun, 26 Dec 2021 02:20:10 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "neural-text-to-speech",
      "style-modeling",
      "style-transfer",
      "expressive-tts",
      "controllable-tts",
      "tts",
      "text-to-speech",
      "fast",
      "robust",
      "expressive-speech-synthesis",
      "fast-tts",
      "robust-tts",
      "prosody"
    ],
    "technique": "GitHub API"
  }
}