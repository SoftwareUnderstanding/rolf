{
  "citation": [
    {
      "confidence": [
        0.9734645542547574
      ],
      "excerpt": "  - Allow training style transfer networks following Cartoon-GAN paper \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9626356225854676,
        0.9449813035477502,
        0.9944484218006108
      ],
      "excerpt": "  - https://github.com/CompVis/adaptive-style-transfer \n  - https://compvis.github.io/adaptive-style-transfer/ \n  - https://arxiv.org/pdf/1807.10201.pdf \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Net-Mist/style-transfer-tf2",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-05-07T00:25:42Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-21T09:33:32Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9286484564893562,
        0.9442507826472414
      ],
      "excerpt": "This is an implementation of adaptive style transfer in tensorflow 2.   \nIt demonstrates how to: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9423219444984027,
        0.8278062941266466,
        0.8845448220521611
      ],
      "excerpt": "   The code is writen but I'm still looking for good parameters :) \n  - Demonstrate how to use style transfer on Android. I wrote models with the same kind of optimization presented in mobilenet v2 paper. The models train well, can be use \n  for inference on a computer and can be exported in tfLite format but I still have some bug in the android app.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9477164949813303
      ],
      "excerpt": "tensorRT. To compile and build a docker image with a more recent version of tensorflow 2 please see the readme inside trt_docker subdir \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8017313560845856
      ],
      "excerpt": "The research team that releases the adaptive style transfer paper also releases style image on their owncloud \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9966474696551649
      ],
      "excerpt": "part of the cartoon GAN. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8633351233620119,
        0.8594543401074821
      ],
      "excerpt": "  - The model architecture corresponding to the checkpoint (std or mobilenet) \nFor instance to export a model in TFLite format: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A Tensorflow2 adaptive style transfer implementation",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Net-Mist/style-transfer-tf2/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 4,
      "date": "Fri, 24 Dec 2021 13:57:18 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Net-Mist/style-transfer-tf2/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Net-Mist/style-transfer-tf2",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Net-Mist/style-transfer-tf2/master/Dockerfile",
      "https://raw.githubusercontent.com/Net-Mist/style-transfer-tf2/master/trt_docker/Dockerfile",
      "https://raw.githubusercontent.com/Net-Mist/style-transfer-tf2/master/webapp/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The easiest way to install this style transfer is by using docker. You need to install docker, docker-compose and nvidia-docker then run: `docker-compose build style-transfer` \n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.825148925474566
      ],
      "excerpt": "And you need to specified if your using tflite.   \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8612142158263317
      ],
      "excerpt": "  - Write the training loop of a GAN using tf.function and tf.GradientTape . \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8068512009076331
      ],
      "excerpt": "python3 app.py --action training \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8068512009076331
      ],
      "excerpt": "python3 app.py --action training \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8042981677884342,
        0.8033681690668286
      ],
      "excerpt": "  - The path of checkpoint to load (can be a file or a folder. In this case it will load the more recent one) \n  - The path to write the exported model \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Net-Mist/style-transfer-tf2/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Dockerfile",
      "Java",
      "Shell",
      "Vue",
      "HTML",
      "JavaScript",
      "Makefile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 S\\xc3\\xa9bastien Iooss\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Style Transfer",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "style-transfer-tf2",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Net-Mist",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Net-Mist/style-transfer-tf2/blob/master/Readme.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Go to webapp dir, create a folder \"models\" and put picasso model inside. You can download it [here](https://drive.google.com/open?id=16HS7a8h_n6ddv1LNgNHBQIxgBSsw1Xxu)\nThen run `docker-compose up`. \n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 14,
      "date": "Fri, 24 Dec 2021 13:57:18 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you want to use docker for the training, a solution is to run the docker-compose `style-transfer` app. For training you need to specified 3 environments variables : \n- PICTURE_DATASET: the path of the pictures\n- ART_DATASET : the path of the art images\n- EXPERIMENT : the folder where the program writes logs and checkpoints\n\nfor instance run :\n\n```bash\nexport PICTURE_DATASET=....\nexport ART_DATASET=....\nexport EXPERIMENT=....\ndocker-compose run style-transfer\n```\n\nAnd you will get bash prompt in a valid tensorflow 2 environment\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "As soon as you are in an environment running tensorflow 2, try :\n\n```bash\npython3 app.py --action training\n```\n\nTo train the model with default parameters. You can see the most important parameters with `python3 app.py --help` and all the parameters with \n`python3 app.py --helpfull`\n\n  - If you choose to not use docker, then you probably need to change the default paths where are stored the dataset and the tfrecords. The parameters to do this are : \n  `--picture_dataset_path`, `--picture_tfrecord_path`, `--style_dataset_path`, `--style_tfrecord_path` and `--training_dir`\n  - `--style_tfrecord_prefix` is also an option you should specified if you want to create several tfrecord files for several artists in the same folder.\n  - If you want to train a model with the same optimizations than the mobilenet-v2 then add the flag : `--mobilenet`\n \nFor instance, if you want to train a picasso style transfer with the same training parameters than the original paper, you can first run 200000 iterations with a learning\nrate of 0.0001 then 100000 iterations with a learning rate of 0.00002. Inside docker it will be:\n\n```bash\npython3 app.py --action training \\\n               --training_dir /opt/experiment/picasso \\\n               --style_tfrecord_prefix picasso\npython3 app.py --action training \\\n               --training_dir /opt/experiment/picasso_2 \\\n               --style_tfrecord_prefix picasso \\\n               --n_iterations 100000 \\\n               --lr 0.00002 \\\n               --pretrained_ckpt /opt/experiment/picasso/checkpoints/\n```\n\n\n",
      "technique": "Header extraction"
    }
  ]
}