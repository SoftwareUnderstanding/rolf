{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1312.5602",
      "https://arxiv.org/abs/1511.06581",
      "https://arxiv.org/abs/1507.06527"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- [DQN](https://arxiv.org/abs/1312.5602)\n- [Dueling DQN](https://arxiv.org/abs/1511.06581)\n- [Noisy layer](https://arxiv.org/pdf/1706.10295.pdf)\n- [C51](https://arxiv.org/pdf/1707.06887.pdf)\n- [PER](https://arxiv.org/pdf/1511.05952.pdf)\n- [Rainbow](https://arxiv.org/pdf/1710.02298.pdf)\n- [DRQN](https://arxiv.org/abs/1507.06527)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{DQN-Atari-Agents,\n  author = {Dittert, Sebastian},\n  title = {DQN-Atari-Agents:   Modularized PyTorch implementation of several DQN Agents, i.a. DDQN, Dueling DQN, Noisy DQN, C51, Rainbow and DRQN},\n  year = {2020},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/BY571/DQN-Atari-Agents}},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8997639958985995
      ],
      "excerpt": "For citation: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9927662050383874,
        0.8619400343366462,
        0.9664456561658856,
        0.9146894306581513,
        0.9851092049198196,
        0.989881025149432
      ],
      "excerpt": "  author = {Dittert, Sebastian}, \n  title = {DQN-Atari-Agents:   Modularized PyTorch implementation of several DQN Agents, i.a. DDQN, Dueling DQN, Noisy DQN, C51, Rainbow and DRQN}, \n  year = {2020}, \n  publisher = {GitHub}, \n  journal = {GitHub repository}, \n  howpublished = {\\url{https://github.com/BY571/DQN-Atari-Agents}}, \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/BY571/DQN-Atari-Agents",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-04-02T07:56:22Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-21T01:54:22Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9514761934698392
      ],
      "excerpt": "Both can be enhanced with Noisy layer, Per (Prioritized Experience Replay), Multistep Targets and be trained in a Categorical version (C51). Combining all these add-ons will lead to the state-of-the-art Algorithm of value-based methods called: Rainbow. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9222514248054086
      ],
      "excerpt": "Curiosity Exploration [X] currently only for DQN \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9063028531977918,
        0.8086146542067099
      ],
      "excerpt": "Since training for the Algorithms for Atari takes a lot of time I added a quick convergence prove for the CartPole-v0 environment. You can clearly see that Raibow outperformes the other two methods Dueling DQN and DDQN. \nTo reproduce the results following hyperparameter where used: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8942311412619048,
        0.8216775179419723
      ],
      "excerpt": "Its interesting to see that the add-ons have a negative impact for the super simple CartPole environment. Still the Dueling DDQN version performs clearly better than the standard DDQN version. \nTo reduce wall clock time while training parallel environments are implemented. Following diagrams show the speed improvement for the two envrionments CartPole-v0 and LunarLander-v2. Tested with 1,2,4,6,8,10,16 worker. Each number of worker was tested over 3 seeds.   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "DQN-Atari-Agents:   Modularized & Parallel PyTorch implementation of several DQN Agents, i.a. DDQN, Dueling DQN, Noisy DQN, C51, Rainbow, and DRQN",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/BY571/DQN-Atari-Agents/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 10,
      "date": "Tue, 21 Dec 2021 12:01:22 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/BY571/DQN-Atari-Agents/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "BY571/DQN-Atari-Agents",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/BY571/DQN-Atari-Agents/master/testing/CNN%20Testing%20and%20image%20stacking.ipynb",
      "https://raw.githubusercontent.com/BY571/DQN-Atari-Agents/master/testing/.ipynb_checkpoints/CNN%20Testing%20and%20image%20stacking-checkpoint.ipynb",
      "https://raw.githubusercontent.com/BY571/DQN-Atari-Agents/master/testing/.ipynb_checkpoints/Testing%20image%20stacking%20in%20the%20buffer-checkpoint.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8065206939415249
      ],
      "excerpt": "Following DQN versions are included: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8852782200166676
      ],
      "excerpt": "Its interesting to see that the add-ons have a negative impact for the super simple CartPole environment. Still the Dueling DDQN version performs clearly better than the standard DDQN version. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8828665034782968
      ],
      "excerpt": "- min_eps: 0.01 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8998118325065148,
        0.8998118325065148
      ],
      "excerpt": "  <img src=\"/imgs/CP_training_time_.png\" width=\"400\" /> \n  <img src=\"/imgs/CP_speed_test_.png\" width=\"400\" />  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8998118325065148,
        0.8998118325065148
      ],
      "excerpt": "  <img src=\"/imgs/worker_LL_t_.png\" width=\"400\" /> \n  <img src=\"/imgs/worker_LL_p_.png\" width=\"400\" />  \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/BY571/DQN-Atari-Agents/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 Sebastian Dittert\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# DQN-Atari-Agents",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "DQN-Atari-Agents",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "BY571",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/BY571/DQN-Atari-Agents/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Trained and tested on:\n<pre>\nPython 3.6 \nPyTorch 1.4.0  \nNumpy 1.15.2 \ngym 0.10.11 \n</pre>\n\nTo train the base DDQN simply run ``python run_atari_dqn.py``\nTo train and modify your own Atari Agent the following inputs are optional:\n\n*example:* ``python run_atari_dqn.py -env BreakoutNoFrameskip-v4 -agent dueling -u 1 -eps_frames 100000 -seed 42 -info Breakout_run1``\n- agent: Specify which type of DQN agent you want to train, default is DQN - baseline! **Following agent inputs are currently possible:** ``dqn``, ``dqn+per``, ``noisy_dqn``, ``noisy_dqn+per``, ``dueling``, ``dueling+per``, ``noisy_dueling``, ``noisy_dueling+per``, ``c51``, ``c51+per``, ``noisy_c51``, ``noisy_c51+per``, ``duelingc51``, ``duelingc51+per``, ``noisy_duelingc51``, ``noisy_duelingc51+per``, ``rainbow``\n- env: Name of the atari Environment, default = PongNoFrameskip-v4\n- frames: Number of frames to train, default = 5 mio\n- seed: Random seed to reproduce training runs, default = 1\n- bs: Batch size for updating the DQN, default = 32\n- layer_size: Size of the hidden layer, default=512\n- n_step: Number of steps for the multistep DQN Targets \n- eval_every, Evaluate every x frames, default = 50000\n- eval_runs, Number of evaluation runs, default = 5\n- m: Replay memory size, default = 1e5\n- lr: Learning rate, default = 0.00025\n- g: Discount factor gamma, default = 0.99\n- t: Soft update parameter tat, default = 1e-3\n- eps_frames: Linear annealed frames for Epsilon, default = 150000\n- min_eps: Epsilon greedy annealing crossing point. Fast annealing until this point, from there slowly to 0 until the last frame, default = 0.1\n- ic, --intrinsic_curiosity, Adding intrinsic curiosity to the extrinsic reward. 0 - only reward and no curiosity, 1 - reward and curiosity, 2 - only curiosity, default = 0.\n- info: Name of the training run.\n- fill_buffer: Adding samples to the replay buffer based on a random policy, before agent-env-interaction. Input numer of preadded frames to the buffer, default = 50000\n- save_model: Specify if the trained network shall be saved [1] or not [0], default is 1 - saved!\n- w, --worker: Number of parallel environments\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 59,
      "date": "Tue, 21 Dec 2021 12:01:22 GMT"
    },
    "technique": "GitHub API"
  },
  "support": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Im open for feedback, found bugs, improvements or anything. Just leave me a message or contact me.\n\n",
      "technique": "Header extraction"
    }
  ],
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "dqn-pytorch",
      "ddqn-pyotrch",
      "dueling-dqn-pytorch",
      "noisy-dqn",
      "c51",
      "reinforcement-learning-algorithms",
      "reinforcement-learning-agent",
      "atari",
      "openai",
      "deep-reinforcement-learning",
      "drqn",
      "rainbow",
      "prioritized-experience-replay",
      "multi-step-dqn",
      "n-step-dqn",
      "multi-environment",
      "multiprocessing",
      "parallel-computing"
    ],
    "technique": "GitHub API"
  }
}