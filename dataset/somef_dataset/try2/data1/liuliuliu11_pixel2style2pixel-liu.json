{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2008.00951\"><img src=\"https://img.shields.io/badge/arXiv-2008.00951-b31b1b.svg\"></a>\n  <a href=\"https://opensource.org/licenses/MIT\"><img src=\"https://img.shields.io/badge/License-MIT-yellow.svg\"></a>\n  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg",
      "https://arxiv.org/abs/2008.00951\">Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation</a>:\n\n```\n@article{richardson2020encoding,\n  title={Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation},\n  author={Richardson, Elad and Alaluf, Yuval and Patashnik, Or and Nitzan, Yotam and Azar, Yaniv and Shapiro, Stav and Cohen-Or, Daniel},\n  journal={arXiv preprint arXiv:2008.00951},\n  year={2020}\n}\n```",
      "https://arxiv.org/abs/2008.00951"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you use this code for your research, please cite our paper <a href=\"https://arxiv.org/abs/2008.00951\">Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation</a>:\n\n```\n@article{richardson2020encoding,\n  title={Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation},\n  author={Richardson, Elad and Alaluf, Yuval and Patashnik, Or and Nitzan, Yotam and Azar, Yaniv and Shapiro, Stav and Cohen-Or, Daniel},\n  journal={arXiv preprint arXiv:2008.00951},\n  year={2020}\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{richardson2020encoding,\n  title={Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation},\n  author={Richardson, Elad and Alaluf, Yuval and Patashnik, Or and Nitzan, Yotam and Azar, Yaniv and Shapiro, Stav and Cohen-Or, Daniel},\n  journal={arXiv preprint arXiv:2008.00951},\n  year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9999382753816727,
        0.9811934373974328
      ],
      "excerpt": "<a href=\"https://arxiv.org/abs/2008.00951\"><img src=\"https://img.shields.io/badge/arXiv-2008.00951-b31b1b.svg\"></a> \n  <a href=\"https://opensource.org/licenses/MIT\"><img src=\"https://img.shields.io/badge/License-MIT-yellow.svg\"></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8051836954530404
      ],
      "excerpt": "2020.10.04: Initial code release \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9380142677289868
      ],
      "excerpt": "--latent_mask=8,9,10,11,12,13,14,15,16,17  \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/liuliuliu11/pixel2style2pixel-liu",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-05-09T11:59:11Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-10T06:44:24Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Official Implementation of our pSp paper for both training and evaluation. The pSp method extends the StyleGAN model to \nallow solving different image-to-image translation problems using its encoder.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9943422771895676
      ],
      "excerpt": "We present a generic image-to-image translation framework, Pixel2Style2Pixel (pSp). Our pSp framework is based on a novel encoder network that directly generates a series of style vectors which are fed into a pretrained StyleGAN generator, forming the extended W+ latent space. We first show that our encoder can directly embed real images into W+, with no additional optimization. We further introduce a dedicated identity loss which is shown to achieve improved performance in the reconstruction of an input image. We demonstrate pSp to be a simple architecture that, by leveraging a well-trained, fixed generator network, can be easily applied on a wide-range of image-to-image translation tasks. Solving these tasks through the style representation results in a global approach that does not rely on a local pixel-to-pixel correspondence and further supports multi-modal synthesis via the resampling of styles. Notably, we demonstrate that pSp can be trained to align a face image to a frontal pose without any labeled data, generate multi-modal results for ambiguous tasks such as conditional face generation from segmentation maps, and construct high-resolution images from corresponding low-resolution images. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8651205655089669,
        0.8408590882161251
      ],
      "excerpt": "2020.10.06: Add pSp toonify model (Thanks to the great work from Doron Adler and Justin Pinkney)! \nHere, we use pSp to find the latent code of real images in the latent domain of a pretrained StyleGAN generator.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8636577253190403
      ],
      "excerpt": "Given a low-resolution input image, we generate a corresponding high-resolution image. As this too is an ambiguous task, we can use style-mixing to produce several plausible results. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.967274152896222
      ],
      "excerpt": "To help visualize the pSp framework on multiple tasks and to help you get started, we provide a Jupyter notebook found in notebooks/inference_playground.ipynb that allows one to visualize the various applications of pSp.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.928181889172213
      ],
      "excerpt": "For the tasks of conditional image synthesis and super resolution, the notebook also demonstrates pSp's ability to perform multi-modal synthesis using  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8784064601843594
      ],
      "excerpt": "In addition, we provide various auxiliary models needed for training your own pSp model from scratch as well as pretrained models needed for computing the ID metrics reported in the paper. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8149304801789785,
        0.8751423388718108,
        0.8388407567928262,
        0.887632503231055
      ],
      "excerpt": "|FFHQ StyleGAN | StyleGAN model pretrained on FFHQ taken from rosinality with 1024x1024 output resolution. \n|IR-SE50 Model | Pretrained IR-SE50 model taken from TreB1eN for use in our ID loss during pSp training. \n|CurricularFace Backbone  | Pretrained CurricularFace model taken from HuangYG123 for use in ID similarity metric computation. \n|MTCNN  | Weights for MTCNN model taken from TreB1eN for use in ID similarity metric computation. (Unpack the tar.gz to extract the 3 model weights.) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.912002370988404
      ],
      "excerpt": "is the number of semantic categories.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.801387479491222,
        0.8669928655133754
      ],
      "excerpt": "- During inference, the options used during training are loaded from the saved checkpoint and are then updated using the  \ntest options passed to the inference script. For example, there is no need to pass --dataset_type or --label_nc to the  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9708692445746252
      ],
      "excerpt": "- When running inference for segmentation-to-image or sketch-to-image, it is highly recommend to do so with a style-mixing, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8189310315705046
      ],
      "excerpt": "- When running inference for super-resolution, please provide a single down-sampling value using --resize_factors. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8641618141236352
      ],
      "excerpt": "style mixing on every image in the given data_path.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9856607083751786
      ],
      "excerpt": "To better show the flexibility of our pSp framework we present additional applications below. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8630454798830108,
        0.8274825430340824,
        0.9248236086058947
      ],
      "excerpt": "Using the toonify StyleGAN built by Doron Adler and Justin Pinkney, \nwe take a real face image and generate a toonified version of the given image. We train the pSp encoder to directly reconstruct real  \nface images inside the toons latent space resulting in a projection of each image to the closest toon. We do so without requiring any labeled pairs \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8649540361885054
      ],
      "excerpt": "This is trained exactly like the StyleGAN inversion task with several changes:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8284017457586235,
        0.8685632794602368
      ],
      "excerpt": "    - The toonify generator is taken from Doron Adler and Justin Pinkney  \n      and converted to Pytorch using rosinality's conversion script. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9531085564623333,
        0.8006263234279007
      ],
      "excerpt": "| &boxv;&nbsp; &boxur;&nbsp; psp.py | Implementation of our pSp framework \n| &boxvr;&nbsp; notebook | Folder with jupyter notebook containing pSp inference playground \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "refer https://github.com/rosinality/stylegan2-pytorch & https://github.com/eladrich/pixel2style2pixel",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/liuliuliu11/pixel2style2pixel-liu/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3,
      "date": "Mon, 27 Dec 2021 20:33:02 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/liuliuliu11/pixel2style2pixel-liu/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "liuliuliu11/pixel2style2pixel-liu",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/liuliuliu11/pixel2style2pixel-liu/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/liuliuliu11/pixel2style2pixel-liu/master/notebooks/inference_playground.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Currently, we provide support for numerous datasets and experiments (encoding, frontalization, etc.).\n    - Refer to `configs/paths_config.py` to define the necessary data paths and model paths for training and evaluation. \n    - Refer to `configs/transforms_config.py` for the transforms defined for each dataset/experiment. \n    - Finally, refer to `configs/data_configs.py` for the source/target data paths for the train and test sets\n      as well as the transforms.\n- If you wish to experiment with your own dataset, you can simply make the necessary adjustments in \n    1. `data_configs.py` to define your data paths.\n    2. `transforms_configs.py` to define your own data transforms.\n    \nAs an example, assume we wish to run encoding using ffhq (`dataset_type=ffhq_encode`). \nWe first go to `configs/paths_config.py` and define:\n``` \ndataset_paths = {\n    'ffhq': '/path/to/ffhq/images256x256'\n    'celeba_test': '/path/to/CelebAMask-HQ/test_img',\n}\n```\nThe transforms for the experiment are defined in the class `EncodeTransforms` in `configs/transforms_config.py`.   \nFinally, in `configs/data_configs.py`, we define:\n``` \nDATASETS = {\n   'ffhq_encode': {\n        'transforms': transforms_config.EncodeTransforms,\n        'train_source_root': dataset_paths['ffhq'],\n        'train_target_root': dataset_paths['ffhq'],\n        'test_source_root': dataset_paths['celeba_test'],\n        'test_target_root': dataset_paths['celeba_test'],\n    },\n}\n``` \nWhen defining our datasets, we will take the values in the above dictionary.\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "- Clone this repo:\n``` \ngit clone https://github.com/eladrich/pixel2style2pixel.git\ncd pixel2style2pixel\n```\n- Dependencies:  \nWe recommend running this repository using [Anaconda](https://docs.anaconda.com/anaconda/install/). \nAll dependencies for defining the environment are provided in `environment/psp_env.yaml`.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8312833552945635
      ],
      "excerpt": "By default, we assume that all auxiliary models are downloaded and saved to the directory pretrained_models. However, you may use your own paths by changing the necessary values in configs/path_configs.py. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8195283490507721
      ],
      "excerpt": "Additionally, if you have tensorboard installed, you can visualize tensorboard logs in opts.exp_dir/logs. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9042456532942793
      ],
      "excerpt": "<img src=\"docs/teaser.jpg\" width=\"800px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9042456532942793,
        0.9042456532942793
      ],
      "excerpt": "<img src=\"docs/encoding_inputs.jpg\" width=\"800px\"/> \n<img src=\"docs/encoding_outputs.jpg\" width=\"800px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9042456532942793,
        0.9042456532942793
      ],
      "excerpt": "<img src=\"docs/frontalization_inputs.jpg\" width=\"800px\"/> \n<img src=\"docs/frontalization_outputs.jpg\" width=\"800px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8739891000537701,
        0.8739891000537701
      ],
      "excerpt": "<img src=\"docs/seg2image.png\" width=\"800px\"/> \n<img src=\"docs/sketch2image.png\" width=\"800px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9042456532942793,
        0.9042456532942793
      ],
      "excerpt": "<img src=\"docs/super_res_32.jpg\" width=\"800px\"/> \n<img src=\"docs/super_res_style_mixing.jpg\" width=\"800px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8790923594498573
      ],
      "excerpt": "The main training script can be found in scripts/train.py.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8808400718595312
      ],
      "excerpt": "python scripts/train.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8808400718595312
      ],
      "excerpt": "python scripts/train.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8828665034782968
      ],
      "excerpt": "--l2_lambda_crop=0.01 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8808400718595312
      ],
      "excerpt": "python scripts/train.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8808400718595312
      ],
      "excerpt": "python scripts/train.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8808400718595312
      ],
      "excerpt": "python scripts/train.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8372399752020275,
        0.8501751782558576
      ],
      "excerpt": "See options/train_options.py for all training-specific flags.  \nSee options/test_options.py for all test-specific flags. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8178323041507995
      ],
      "excerpt": "Specifying --label_nc=0 (the default value), will directly use the RGB colors as input. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8216270093103228
      ],
      "excerpt": "For example,  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9042456532942793,
        0.9042456532942793
      ],
      "excerpt": "<img src=\"docs/toonify_input.jpg\" width=\"800px\"/> \n<img src=\"docs/toonify_output.jpg\" width=\"800px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8146715012507655
      ],
      "excerpt": "| &boxvr; models | Folder containting all the models and training objects \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9272716345557604
      ],
      "excerpt": "| &boxvr;&nbsp; options | Folder with training and test command-line options \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8328916229266603
      ],
      "excerpt": "| &boxvr;&nbsp; utils | Folder with various utility functions \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/liuliuliu11/pixel2style2pixel-liu/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python",
      "Cuda",
      "C++"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "pixel2style2pixel-liu",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "liuliuliu11",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/liuliuliu11/pixel2style2pixel-liu/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Linux or macOS\n- NVIDIA GPU + CUDA CuDNN (CPU may be possible with some modifications, but is not inherently supported)\n- Python 2 or 3\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Mon, 27 Dec 2021 20:33:02 GMT"
    },
    "technique": "GitHub API"
  }
}