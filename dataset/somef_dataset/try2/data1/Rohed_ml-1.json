{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1506.02640 \n\nA resource I have found useful was the demo by author of the paper itself: https://youtu.be/NM6lrxy0bxs \n\nThis README won't go into how YOLO itself works but instead focuses on how to prepare/explore dataset and train the network on a specific dataset.\n\n### ResNet ###\nResNet (https://arxiv.org/abs/1512.03385",
      "https://arxiv.org/abs/1512.03385"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9424622283716592
      ],
      "excerpt": "There have been many articles and videos describing this approach originally presented in the paper: https://arxiv.org/abs/1506.02640  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8456806903995955
      ],
      "excerpt": "if r &lt; 0.3: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8456806903995955
      ],
      "excerpt": "    if sc &lt; 1: \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Rohed/ml-1",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-08-05T19:23:47Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-08-05T19:42:07Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9764919290440711
      ],
      "excerpt": "This project was made only as a means to learn more about deep learning, training networks, transfer learning and implementing an actual paper (my first!). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9755265520943137
      ],
      "excerpt": "The dataset is uneven across different classes. It can additionally be noted that a certain view (rear) of cars dominates the rest (side and front). The lighting condition is constant throughout the capture and hence we will need data augumentation to help the network generalize better. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8562998640786891
      ],
      "excerpt": "The paper's author explains that they used GoogLeNet (inception) inspired architecture for their feature extractor, that was trained on PASCAL VOC dataset prior to making it part of the object detection network. We can skip this step and use a pre-trained network, that performs well on classification tasks. I've chosen ResNet for this purpose. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9259841584298183,
        0.9728588754224896
      ],
      "excerpt": "ResNet (https://arxiv.org/abs/1512.03385) has won several competitions and its architecture allows for better learning in deeper networks. I've used the Keras implementation with weights of ResNet50 from here https://github.com/fchollet/deep-learning-models.git and modified the code to have the YOLO classifier at the end. \nThe key part of this implementation was training the network, as it required defining the custom loss function in Tensorflow and image and frame data manipulation for better generalization. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8308006243378576,
        0.8747405440209827,
        0.9716170753315377,
        0.9838767186906732
      ],
      "excerpt": "The network is trained on 224x224x3 images and so our dataset images are resized with their corresponding label coordinates adjusted as well. \nKeras and TF have the standard loss defintions however, the YOLO paper uses a custom objective function that is fine tuned to improve stability (penalize loss from grid cells that do not have an object) and weigh dimension error in smaller boxes more than that in larger boxes: \nI've used Tensorflow's 'while_loop' to create the graph that calculates loss per each batch. All operations in the my loss function (see loop_body() in model_continue_train.py) are tensorflow operations, hence these will all be run only when the graph is computed, taking advantage of any hardware optimization. \nAs mentioned in the paper, I've also randomly scaled, translated and adjusted the saturation values of the data point while generating a batch for training and validation: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9164236212092216,
        0.8677989666257162
      ],
      "excerpt": "As described in the paper, I started to train with 1e-3 learning rate, then 1e-2 followed by 1e-3, 1e-4, 1e-5. All along saving model checkpoints using Keras' callback feature. \nCheck out the 'Vehicle Detection.ipynb' notebook to see the network in use. It performs well on the dataset and also the sample highway video that the network has never seen before. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9163660957601236
      ],
      "excerpt": "The network trained from scratch and was able to detect cars in a video that it had never seen before. It has some problems with far away objects and also the detections are not very smooth across frames.  \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Rohed/ml-1/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 21 Dec 2021 02:37:39 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Rohed/ml-1/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Rohed/ml-1",
    "technique": "GitHub API"
  },
  "invocation": [
    {
      "confidence": [
        0.803112688747487
      ],
      "excerpt": "I've chosen to use a 11x11 grid over the images and 2 bounding box predictions per grid cell, to keep sufficient resolution and at the same time have a smaller output prediction to train for. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8910452465161504,
        0.8737288687529231,
        0.8737288687529231,
        0.8737288687529231
      ],
      "excerpt": "    tr = np.random.random() * 0.2 + 0.01 \n    tr_y = np.random.randint(rows-tr, rowstr) \n    tr_x = np.random.randint(cols-tr, colstr) \n        r = np.random.rand() \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8997243352845468
      ],
      "excerpt": "    M = np.float32([[1,0,tr_x], [0,1,tr_y]]) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8400047791845093
      ],
      "excerpt": "    placeholder = np.zeros_like(img) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8380949510013975
      ],
      "excerpt": "        placeholder[:meta.shape[0], :meta.shape[1]] = meta \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8946413800109336
      ],
      "excerpt": "Sample output images: \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Rohed/ml-1/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Implementing YOLO using ResNet as feature extractor #",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "ml-1",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Rohed",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Rohed/ml-1/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 21 Dec 2021 02:37:39 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Training of this magnitude definitely needed some beefed up hardware and since I'm a console guy (PS4), I resorted to the EC instances Amazon provides (https://aws.amazon.com/ec2/instance-types/). Udacity's Amazon credits came in handy!\n\nAt first, I tried the g2.xlarge instance that Udacity's project on Traffic sign classifier had suggested (did that on my laptop back then) but the memory or the compute capability was nowhere near sufficient, since TF apparently drops to CPU and RAM after detecting that there isn't sufficient capacity on the GPU.\n\nIn the end, p2.xlarge EC2 instance were what I trained my network on. There was ~10GB GPU memory utilization and ~92% GPU at peak. My network trained pretty well on this setup.\n\nNOTE: I faced a lot of issues when getting setup on the remote instance due to issues with certain libraries being out of date and anaconda not having those updates. Luckily Amazon released its latest (v6 at time) deep learning Ubuntu AMI which worked just fine out of the box. So if you are using EC2, make sure to test sample code and library imports in python first to make sure the platform is ready for your code.\n\n\n\n\n",
      "technique": "Header extraction"
    }
  ]
}