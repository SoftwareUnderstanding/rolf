{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2007.13988"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\n@inproceedings{li2020monocular,\n  title={Monocular Real-Time Volumetric Performance Capture},\n  author={Li, Ruilong and Xiu, Yuliang and Saito, Shunsuke and Huang, Zeng and Olszewski, Kyle and Li, Hao},\n  booktitle={European Conference on Computer Vision},\n  pages={49--67},\n  year={2020},\n  organization={Springer}\n}\n  \n@inproceedings{10.1145/3407662.3407756,\n    author = {Li, Ruilong and Olszewski, Kyle and Xiu, Yuliang and Saito, Shunsuke and Huang, Zeng and Li, Hao},\n    title = {Volumetric Human Teleportation},\n    year = {2020},\n    isbn = {9781450380607},\n    publisher = {Association for Computing Machinery},\n    address = {New York, NY, USA},\n    url = {https://doi.org/10.1145/3407662.3407756},\n    doi = {10.1145/3407662.3407756},\n    booktitle = {ACM SIGGRAPH 2020 Real-Time Live!},\n    articleno = {9},\n    numpages = {1},\n    location = {Virtual Event, USA},\n    series = {SIGGRAPH 2020}\n  }\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{li2020monocular,\n  title={Monocular Real-Time Volumetric Performance Capture},\n  author={Li, Ruilong and Xiu, Yuliang and Saito, Shunsuke and Huang, Zeng and Olszewski, Kyle and Li, Hao},\n  booktitle={European Conference on Computer Vision},\n  pages={49--67},\n  year={2020},\n  organization={Springer}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9053637223741354,
        0.9997972154312528
      ],
      "excerpt": "PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization (ICCV 2019) \nShunsuke Saito*, Zeng Huang*, Ryota Natsume*, Shigeo Morishima, Angjoo Kanazawa, Hao Li \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8195977533405995,
        0.9954004602913301
      ],
      "excerpt": "ARCH: Animatable Reconstruction of Clothed Humans (CVPR 2020) \nZeng Huang, Yuanlu Xu, Christoph Lassner, Hao Li, Tony Tung \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999467610886296
      ],
      "excerpt": "Zhe Li, Tao Yu, Chuanyu Pan, Zerong Zheng, Yebin Liu \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Project-Splinter/MonoPort",
    "technique": "GitHub API"
  },
  "contributor": [
    {
      "confidence": [
        1
      ],
      "excerpt": "MonoPort is based on [Monocular Real-Time Volumetric Performance Capture(ECCV'20)](https://project-splinter.github.io/), authored by Ruilong Li*([@liruilong940607](https://github.com/liruilong940607)), Yuliang Xiu*([@yuliangxiu](https://github.com/YuliangXiu)), Shunsuke Saito([@shunsukesaito](https://github.com/shunsukesaito)), Zeng Huang([@ImaginationZ](https://github.com/ImaginationZ)) and Kyle Olszewski([@kyleolsz](https://github.com/kyleolsz)), [Hao Li](https://www.hao-li.com/) is the corresponding author.\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-08-07T08:19:34Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-23T12:12:47Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.963546131239328
      ],
      "excerpt": "Our volumetric capture system captures a completely clothed human body (including the back) using a single RGB webcam and in real time. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8857146501997127
      ],
      "excerpt": "ARCH: Animatable Reconstruction of Clothed Humans (CVPR 2020) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8156610749710342
      ],
      "excerpt": "Learning PIFu in canonical space for animatable avatar generation! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9054044531941458
      ],
      "excerpt": "They extend PIFu to RGBD + introduce \"PIFusion\" utilizing PIFu reconstruction for non-rigid fusion. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9198049677844805
      ],
      "excerpt": "Dr. Zeng Huang defensed his PhD virtually using our system. (Media in Chinese) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Volumetric Human Teleportation (SIGGRAPH 2020 Real-Time Live) Monocular Real-Time Volumetric Performance Capture(ECCV 2020)",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Project-Splinter/MonoPort/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 66,
      "date": "Fri, 24 Dec 2021 03:39:10 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Project-Splinter/MonoPort/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Project-Splinter/MonoPort",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Project-Splinter/MonoPort/master/scripts/download_model.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "First you need to download the model:\n```\nsh scripts/download_model.sh\n```\n\nThen install all the dependencies:\n```\npip install -r requirements.txt\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9202509175650415
      ],
      "excerpt": "    <img src='figs/rtl.jpg'/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8647354795712904
      ],
      "excerpt": "    <img src='figs/zeng.gif'/> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Project-Splinter/MonoPort/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "GLSL",
      "HTML",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other",
      "url": "https://raw.githubusercontent.com/Project-Splinter/MonoPort/master/LICENSE"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'This software is Copyright \\xc2\\xa9 2021 Ruilong Li, The University of Southern California. All Rights Reserved. \\n\\nPermission to use, copy, modify, and distribute this software and its documentation for educational, research and non-profit purposes, without fee, and without a written agreement is hereby granted, provided that the above copyright notice, this paragraph and the following three paragraphs appear in all copies. \\n\\nPermission to make commercial use of this software may be obtained by contacting: \\nUSC Stevens Center for Innovation\\nUniversity of Southern California\\n1150 S. Olive Street, Suite 2300 \\nLos Angeles, CA 90115, USA \\n\\nThis software program and documentation are copyrighted by The University of Southern California. The software program and documentation are supplied \"as is\", without any accompanying services from USC. USC does not warrant that the operation of the program will be uninterrupted or error-free. The end-user understands that the program was developed for research purposes and is advised not to rely exclusively on the program for any reason. \\n\\nIN NO EVENT SHALL THE UNIVERSITY OF SOUTHERN CALIFORNIA BE LIABLE TO ANY PARTY FOR DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES, INCLUDING LOST PROFITS, ARISING OUT OF THE USE OF THIS SOFTWARE AND ITS DOCUMENTATION, EVEN IF THE UNIVERSITY OF SOUTHERN CALIFORNIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. THE UNIVERSITY OF SOUTHERN CALIFORNIA SPECIFICALLY DISCLAIMS ANY WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE SOFTWARE PROVIDED HEREUNDER IS ON AN \"AS IS\" BASIS, AND THE UNIVERSITY OF SOUTHERN CALIFORNIA HAS NO OBLIGATIONS TO PROVIDE MAINTENANCE, SUPPORT, UPDATES, ENHANCEMENTS, OR MODIFICATIONS. \\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "[Monoport: Monocular Volumetric Human Teleportation (SIGGRAPH 2020 Real-Time Live)](http://xiuyuliang.cn/monoport/)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "MonoPort",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Project-Splinter",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Project-Splinter/MonoPort/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Python 3.7\n- PyOpenGL 3.1.5 (need X server in Ubuntu)\n- [PyTorch](https://pytorch.org/) tested on 1.4.0\n- [ImplicitSegCUDA](https://github.com/Project-Splinter/ImplicitSegCUDA)\n- [human_inst_seg](https://github.com/Project-Splinter/human_inst_seg)\n- [streamer_pytorch](https://github.com/Project-Splinter/streamer_pytorch)\n- [human_det](https://github.com/Project-Splinter/human_det)\n\nWe run the demo with 2 GeForce RTX 2080Ti GPUs, the memory usage is as follows (~3.4GB at GPU1, ~9.7GB at GPU2):\n\n<p align='center'>\n    <img src='figs/gpu.png'/>\n</p>\n\n**Note**: The last four dependencies are also developed by our team, and are all in active maintainess. If you meet any installation problems specificly regarding to those tools, we recommand you to file the issue in the corresponded repo. (You don't need to install them manally here as they are included in the requirements.txt)\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 393,
      "date": "Fri, 24 Dec 2021 03:39:10 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "deep-learning",
      "machine-learning",
      "volumetric",
      "3d-reconstruction",
      "3d-vision",
      "siggraph",
      "monocular",
      "performance-capture",
      "eccv2020",
      "virtual-reality",
      "pifu",
      "clothed-humans",
      "reconstruction",
      "pytorch",
      "pifuhd",
      "real-time",
      "teleportation",
      "virtual-conference",
      "teleportvr",
      "sdf"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\n#: if you want to use the input from a webcam:\npython RTL/main.py --use_server --ip <YOUR_IP_ADDRESS> --port 5555 --camera -- netG.ckpt_path ./data/PIFu/net_G netC.ckpt_path ./data/PIFu/net_C\n\n#: or if you want to use the input from a image folder:\npython RTL/main.py --use_server --ip <YOUR_IP_ADDRESS> --port 5555 --image_folder <IMAGE_FOLDER> -- netG.ckpt_path ./data/PIFu/net_G netC.ckpt_path ./data/PIFu/net_C\n\n#: or if you want to use the input from a video:\npython RTL/main.py --use_server --ip <YOUR_IP_ADDRESS> --port 5555 --videos <VIDEO_PATH> -- netG.ckpt_path ./data/PIFu/net_G netC.ckpt_path ./data/PIFu/net_C\n```\n\nIf everything goes well, you should be able to see those logs after waiting for a few seconds:\n\n    loading networkG from ./data/PIFu/net_G ...\n    loading networkC from ./data/PIFu/net_C ...\n    initialize data streamer ...\n    Using cache found in /home/rui/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n    Using cache found in /home/rui/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n    * Serving Flask app \"main\" (lazy loading)\n    * Environment: production\n    WARNING: This is a development server. Do not use it in a production deployment.\n    Use a production WSGI server instead.\n    * Debug mode: on\n    * Running on http://<YOUR_IP_ADDRESS>:5555/ (Press CTRL+C to quit)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Open the page `http://<YOUR_IP_ADDRESS>:5555/` on a web browser from any device (Desktop/IPad/IPhone), You should be able to see the **MonoPort VR Demo** page on that device, and at the same time you should be able to see the a screen poping up on your desktop, showing the reconstructed normal and texture image.\n\n<p align='center'>\n    <img src='figs/twoside.png'/>\n</p>\n\n",
      "technique": "Header extraction"
    }
  ]
}