{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1511.06581\n\n*The authors introduced a way to generalize learning across actions in a DQN by modifying the network architecture. The dueling network represents two separate estimators, one for the state function and the other one for the state-dependent action advantage function.*\n\nMy implementation: https://github.com/Brandon-Rozek/DeepRL/blob/master/DuelingPoleBalance.ipynb\n\n\n\n\n\n[3] Tom Schaul, John Quan, Ioannis Antonoglou, David Silver. **Prioritized Experience Replay**.\n\nhttps://arxiv.org/abs/1511.05952\n\n*Typically in the training of DQN networks, there exists an experience replay buffer where experiences are sampled after a certain time period to train the neural network.  Before this paper, the sampling was done uniformly across all experiences. The authors expand upon that, allowing for more priority or increasing the likeliness of an experience being sampled if the TD error is high.*\n\nMy implementation: https://github.com/Brandon-Rozek/DeepRL/blob/master/PrioReplayPoleBalanceKeras.ipyn",
      "https://arxiv.org/abs/1511.05952\n\n*Typically in the training of DQN networks, there exists an experience replay buffer where experiences are sampled after a certain time period to train the neural network.  Before this paper, the sampling was done uniformly across all experiences. The authors expand upon that, allowing for more priority or increasing the likeliness of an experience being sampled if the TD error is high.*\n\nMy implementation: https://github.com/Brandon-Rozek/DeepRL/blob/master/PrioReplayPoleBalanceKeras.ipyn"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8111036989382164,
        0.9959514615396599
      ],
      "excerpt": "My implementation: https://github.com/Brandon-Rozek/DeepRL/blob/master/PoleBalanceKeras.ipynb \n[2] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, Nando de Freitas. Dueling Network Architectures for Deep Reinforcement Learning. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8111036989382164
      ],
      "excerpt": "My implementation: https://github.com/Brandon-Rozek/DeepRL/blob/master/DuelingPoleBalance.ipynb \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Brandon-Rozek/DeepRL",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-01-22T15:26:49Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-01-22T15:31:50Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9204310639542577,
        0.8572548170144751
      ],
      "excerpt": "This is a walk through my journey of deep reinforcement learning. This will highlight the papers I've read and implemented. \n[1] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin Riedmiller. Playing Atari with Deep Reinforcement Learning. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9888596130306527
      ],
      "excerpt": "The authors expanded upon the concept of a Q-network in reinforcement learning, by introducing a non-linear appromixation with neural networks. They were able to apply a convolution neural network to parse the raw pixels of gameplay in order to outperform human experts. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8591297239418665
      ],
      "excerpt": "[2] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, Nando de Freitas. Dueling Network Architectures for Deep Reinforcement Learning. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9823962816971189
      ],
      "excerpt": "Typically in the training of DQN networks, there exists an experience replay buffer where experiences are sampled after a certain time period to train the neural network.  Before this paper, the sampling was done uniformly across all experiences. The authors expand upon that, allowing for more priority or increasing the likeliness of an experience being sampled if the TD error is high. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Further explorations into Reinforcement learning with Deep Learning Networks",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Brandon-Rozek/DeepRL/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 29 Dec 2021 20:26:17 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Brandon-Rozek/DeepRL/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Brandon-Rozek/DeepRL",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Brandon-Rozek/DeepRL/master/DuelingPoleBalance.ipynb",
      "https://raw.githubusercontent.com/Brandon-Rozek/DeepRL/master/PrioReplayPoleBalanceKeras.ipynb",
      "https://raw.githubusercontent.com/Brandon-Rozek/DeepRL/master/PoleBalanceKeras.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8399064745908871
      ],
      "excerpt": "My implementation: https://github.com/Brandon-Rozek/DeepRL/blob/master/PoleBalanceKeras.ipynb \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8399064745908871
      ],
      "excerpt": "My implementation: https://github.com/Brandon-Rozek/DeepRL/blob/master/DuelingPoleBalance.ipynb \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Brandon-Rozek/DeepRL/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Deep Reinforcement Learning",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "DeepRL",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Brandon-Rozek",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Brandon-Rozek/DeepRL/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 29 Dec 2021 20:26:17 GMT"
    },
    "technique": "GitHub API"
  }
}