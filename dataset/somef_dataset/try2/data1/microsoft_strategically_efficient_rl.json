{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1707.06347",
      "https://arxiv.org/abs/1705.05363"
    ],
    "technique": "Regular expression"
  },
  "codeOfConduct": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://raw.githubusercontent.com/microsoft/strategically_efficient_rl/main/CODE_OF_CONDUCT.md",
    "technique": "File Exploration"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/microsoft/strategically_efficient_rl",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-05-04T13:46:58Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-27T03:30:47Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9735068921361162,
        0.9856895902985763
      ],
      "excerpt": "This repository contains all code and hyperparameter configurations needed to replicate the results in Strategically Efficient Exploration in Competitive Multi-agent Reinforcement Learning (UAI 2021) \nIn addition to finite Markov games, this project also supports experiments with curiosity in deep multi-agent reinforcement learning. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.897524367489202
      ],
      "excerpt": "This will train PPO in self-play in a simple two-player matrix game.  This project currently supports two intrinsic reward mechansims with multi-agent PPO, Random Network Distillation and the Intrinsic Curiosity Module. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8199837720617734
      ],
      "excerpt": "a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8073737513954784,
        0.9783267603835809
      ],
      "excerpt": "This project has adopted the Microsoft Open Source Code of Conduct. \nFor more information see the Code of Conduct FAQ or \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8411378894176985,
        0.9214339495776546
      ],
      "excerpt": "This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft  \ntrademarks or logos is subject to and must follow  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "More efficient exploration for reinforcement learning in two-player, zero-sum game",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/microsoft/strategically_efficient_rl/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Wed, 29 Dec 2021 20:09:45 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/microsoft/strategically_efficient_rl/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "microsoft/strategically_efficient_rl",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.9906248903846466
      ],
      "excerpt": "cd finite_games \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8033059714433824
      ],
      "excerpt": "Deep RL experiments use RLLib 0.8.3 and Tensorflow 2.4.2, both installed by \"requirements.txt\".  Experiments with deep multi-agent RL can be run with the \"train_multiagent.py\" script. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8003352366805131
      ],
      "excerpt": "a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python learn_extensive_form.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python learn_extensive_form.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python plot_runs.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8098576282543919
      ],
      "excerpt": "    \"Optimistic ULCB\" results/debug/decoy_deep_sea_optimistic_ulcb/decoy_deep_sea_optimistic_ulcb_decoy_games=50,decoy_size=20,exploit=True \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9055065549455742
      ],
      "excerpt": "python3 train_multiagent.py -f experiment_configs/roshambo/ppo_hybrid_bandit.yaml --nash-conv \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/microsoft/strategically_efficient_rl/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'    MIT License\\n\\n    Copyright (c) Microsoft Corporation.\\n\\n    Permission is hereby granted, free of charge, to any person obtaining a copy\\n    of this software and associated documentation files (the \"Software\"), to deal\\n    in the Software without restriction, including without limitation the rights\\n    to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\n    copies of the Software, and to permit persons to whom the Software is\\n    furnished to do so, subject to the following conditions:\\n\\n    The above copyright notice and this permission notice shall be included in all\\n    copies or substantial portions of the Software.\\n\\n    THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\n    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\n    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\n    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\n    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\n    OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\n    SOFTWARE\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Strategically Efficient Exploration in Competitive Multi-agent Reinforcement Learning",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "strategically_efficient_rl",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "microsoft",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/microsoft/strategically_efficient_rl/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 9,
      "date": "Wed, 29 Dec 2021 20:09:45 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This code has only been tested with Python 3.7.11 on Ubuntu 18.04 and 20.04 in the Windows Subsystem for Linux (WSL).\n\nDependencies an be installed via PIP:\n\n```\npip install -r requirements.txt\n```\n\nThis will install all the dependencies needed to reproduce published results.  Some deep RL experiment configurations us environments implemented in the [OpenSpiel](https://github.com/deepmind/open_spiel) or [PettingZoo](https://github.com/PettingZoo-Team/PettingZoo) projects, which must be installed separately.  Please refer to these projects for complete installation instructions.\n\n",
      "technique": "Header extraction"
    }
  ]
}