{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1612.07837*, 2017\n\nWang, Yuxuan, et al. [\"Tacotron: Towards End-To-End Speech Synthesis.\"](https://arxiv.org/pdf/1703.10135.pdf) *arXiv preprint https://arxiv.org/abs/1703.10135*, 2017",
      "https://arxiv.org/abs/1703.10135*, 2017"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Mehri, Soroush, et al. [\"SampleRNN: An Unconditional End-To-End Neural Audio Generation Model.\"](https://arxiv.org/pdf/1612.07837.pdf) *arXiv preprint arXiv:1612.07837*, 2017\n\nWang, Yuxuan, et al. [\"Tacotron: Towards End-To-End Speech Synthesis.\"](https://arxiv.org/pdf/1703.10135.pdf) *arXiv preprint arXiv:1703.10135*, 2017\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/cchinchristopherj/Concert-of-Whales",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-12-01T21:29:22Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-12-09T14:03:52Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9863521953753038,
        0.9583224058857901
      ],
      "excerpt": "The goal of this project was to synthesize a 2-second-long audio waveform of a right whale upcall by harnessing the tremendous advances in speech synthesis deep learning research in recent years. As detailed in Part I, a model based on Tacotron was proposed as a viable option for tackling this problem, but ultimately yielded unsatisfactory results due to use of the Griffin-Lim algorithm to synthesize audio waveforms from magnitude spectrograms.  \nIn order to obtain more realistic-sounding results, the next model to be investigated was SampleRNN. A Three-Tier SampleRNN was implemented, in which each of the three modules in the hierarchy conditions the one below it so that the lowest module outputs sample-by-sample predictions.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9854243648839769
      ],
      "excerpt": "Concretely, the highest level module processes the previous 8 samples of the time series  using an RNN and passes a conditioning vector to the second-highest-level module, which in turn processes the previous 2 samples of the time series using an RNN and passes a conditioning vector to the lowest-level module. This module processes only the previous sample of the time series and outputs a q=256-way softmax over quantized values of the audio waveform. (As an additional design choice, which was proven to improve results, the lowest module passes the previous (quantized) value of the audio clip time series through an embedding layer, which maps each of the quantized values to a real-valued vector embedding. Further, to speed up training, a Multilayer Perceptron is used in the lowest-level module instead of an RNN to yield the final (quantized) prediction for the next sample).  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9940670597697172,
        0.9788545203954331
      ],
      "excerpt": "In order to demonstrate the high degree of realism attained by SampleRNN with only a few epochs of training, an application was developed in which four whales are drawn to a canvas, three of which make \u201creal\u201d right whale upcall sounds from the training set, and the fourth makes the \u201cfake\u201d upcall sound created by the neural network. The task of the user is to identify the whale that made the \"fake\" right whale upcall sound. \nThe application begins by displaying the four baby whales on the canvas, each of which is a different size and swims in a different direction.  \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/cchinchristopherj/Concert-of-Whales/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Thu, 23 Dec 2021 05:58:47 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/cchinchristopherj/Concert-of-Whales/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "cchinchristopherj/Concert-of-Whales",
    "technique": "GitHub API"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/cchinchristopherj/Concert-of-Whales/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "JavaScript",
      "HTML",
      "CSS"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Concert of Whales",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Concert-of-Whales",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "cchinchristopherj",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/cchinchristopherj/Concert-of-Whales/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Thu, 23 Dec 2021 05:58:47 GMT"
    },
    "technique": "GitHub API"
  }
}