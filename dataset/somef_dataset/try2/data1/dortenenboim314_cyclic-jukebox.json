{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2005.00341",
      "https://arxiv.org/abs/2005.00341"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please cite using the following bibtex entry:\n\n```\n@article{dhariwal2020jukebox,\n  title={Jukebox: A Generative Model for Music},\n  author={Dhariwal, Prafulla and Jun, Heewoo and Payne, Christine and Kim, Jong Wook and Radford, Alec and Sutskever, Ilya},\n  journal={arXiv preprint arXiv:2005.00341},\n  year={2020}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{dhariwal2020jukebox,\n  title={Jukebox: A Generative Model for Music},\n  author={Dhariwal, Prafulla and Jun, Heewoo and Payne, Christine and Kim, Jong Wook and Radford, Alec and Sutskever, Ilya},\n  journal={arXiv preprint arXiv:2005.00341},\n  year={2020}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dortenenboim314/cyclic-jukebox",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-08-03T14:13:19Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-08-11T18:57:38Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8440685037425066
      ],
      "excerpt": "Code for \"Jukebox: A Generative Model for Music\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.890974503199935
      ],
      "excerpt": "A summary of all sampling data including zs, x, labels and sampling_kwargs is stored in {name}/level_{level}/data.pth.tar. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9901307562899083
      ],
      "excerpt": "On a V100, it takes about 3 hrs to fully sample 20 seconds of music. Since this is a long time, it is recommended to use n_samples &gt; 1 so you can generate as many samples as possible in parallel. The 1B lyrics and upsamplers can process 16 samples at a time, while 5B can fit only up to 3. Since the vast majority of time is spent on upsampling, we recommend using a multiple of 3 less than 16 like --n_samples 15 for 5b_lyrics. This will make the top-level generate samples in groups of three while upsampling is done in one pass. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9749663451665092
      ],
      "excerpt": "The above trains a two-level VQ-VAE with downs_t = (5,3), and strides_t = (2, 2) meaning we downsample the audio by 2**5 = 32 to get the first level of codes, and 2**8 = 256 to get the second level codes. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8165483021527792
      ],
      "excerpt": "Once the VQ-VAE is trained, we can restore it from its saved checkpoint and train priors on the learnt codes.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9201993059651765,
        0.8110799252484653,
        0.8677688136406537
      ],
      "excerpt": "We pass sample_length = n_ctx * downsample_of_level so that after downsampling the tokens match the n_ctx of the prior hps.  \nHere, n_ctx = 8192 and downsamples = (32, 256), giving sample_lengths = (8192 * 32, 8192 * 256) = (65536, 2097152) respectively for the bottom and top level. \nTo get the best sample quality anneal the learning rate to 0 near the end of training. To do so, continue training from the latest  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8989285018843277
      ],
      "excerpt": "Our pre-trained VQ-VAE can produce compressed codes for a wide variety of genres of music, and the pre-trained upsamplers  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9682268668890546,
        0.9628543768373703,
        0.8480039990777805
      ],
      "excerpt": "Training the small_prior with a batch size of 2, 4, and 8 requires 6.7 GB, 9.3 GB, and 15.8 GB of GPU memory, respectively. A few days to a week of training typically yields reasonable samples when the dataset is homogeneous (e.g. all piano pieces, songs of the same style, etc). \nNear the end of training, follow this to anneal the learning rate to 0 \nYou can then run sample.py with the top-level of our models replaced by your new model. To do so, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9673938874412803
      ],
      "excerpt": "that make_models restores our checkpoint correctly. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9403451055190087
      ],
      "excerpt": "For example, let's say we trained small_vqvae, small_prior, and small_upsampler under /path/to/jukebox/logs. In make_models.py, we are going to declare a tuple of the new models as my_model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.928179067719255
      ],
      "excerpt": "Next, in hparams.py, we add them to the registry with the corresponding restore_paths and any other command line options used during training. Another important note is that for top-level priors with lyric conditioning, we have to locate a self-attention layer that shows alignment between the lyric and music tokens. Look for layers where prior.prior.transformer._attn_mods[layer].attn_func is either 6 or 7. If your model is starting to sing along lyrics, it means some layer, head pair has learned alignment. Congrats! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8684633100517684
      ],
      "excerpt": "    #: TODO For the two lines below, if --labels was used and the model is \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9382857743788479,
        0.9915320480952324,
        0.9117035202984511
      ],
      "excerpt": "  - For each file, we return an artist_id and a list of genre_ids. The reason we have a list and not a single genre_id  \n  is that in v2, we split genres like blues_rock into a bag of words [blues, rock], and we pass atmost  \n  max_bow_genre_size of those, in v3 we consider it as a single word and just set max_bow_genre_size=1. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8352905690648861
      ],
      "excerpt": "  - In small_labelled_prior, set the hps y_bins = (number_of_genres, number_of_artists) and max_bow_genre_size=1.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9469280256006246,
        0.9734980017498575,
        0.9439400014627934,
        0.9431762999934302
      ],
      "excerpt": "  - For each chunk of audio, we return the total_length of the song, the offset the current audio chunk is at and  \n  the sample_length of the audio chunk. We have three timing embeddings: total_length, our current position, and our  \n  current position as a fraction of the total length, and we divide the range of these values into t_bins discrete bins.  \n  - In small_labelled_prior, set the hps min_duration and max_duration to be the shortest/longest duration of audio  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8247287707784043
      ],
      "excerpt": "  min_duration * sr needs to be at least sample_length to have an audio chunk in it. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8066871290771077
      ],
      "excerpt": "To train in addition with lyrics, update get_metadata in data/files_dataset.py to return lyrics too. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8132761719271736,
        0.9847250817770113,
        0.8007480977818796
      ],
      "excerpt": "  - For each file, we linearly align the lyric characters to the audio, find the position in lyric that corresponds to  \n  the midpoint of our audio chunk, and pass a window of n_tokens lyric characters centred around that.  \n  - In small_single_enc_dec_prior, set the hps use_tokens=True and n_tokens to be the number of lyric characters  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9293175833376862
      ],
      "excerpt": "  the lyrics for an audio chunk are almost always found inside a window of that size. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9772161334393094
      ],
      "excerpt": "  and in v3 we missed + and so n_vocab=79 of characters.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9345693767294365,
        0.9722242063926577,
        0.9874156568168236
      ],
      "excerpt": "To simplify hps choices, here we used a single_enc_dec model like the 1b_lyrics model that combines both encoder and  \ndecoder of the transformer into a single model. We do so by merging the lyric vocab and vq-vae vocab into a single  \nlarger vocab, and flattening the lyric tokens and the vq-vae codes into a single sequence of length n_ctx + n_tokens.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8864295831067635
      ],
      "excerpt": "small_prior. To also get the alignment between lyrics and samples in the saved html, you'll need to set alignment_layer  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8380597021722954
      ],
      "excerpt": "save the attention weight tensors for all prime_attention layers, and pick the (layer, head) which has the best linear alignment  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8192814267398867
      ],
      "excerpt": "Previously, we showed how to train a small top-level prior from scratch. Assuming you have a GPU with at least 15 GB of memory and support for fp16, you could fine-tune from our pre-trained 1B top-level prior. Here are the steps: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9279754635852787
      ],
      "excerpt": "Add new entries in jukebox/data/ids. We recommend replacing existing mappings (e.g. rename \"unknown\", etc with styles of your choice). This uses the pre-trained style vectors as initialization and could potentially save some compute. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dortenenboim314/cyclic-jukebox/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Mon, 27 Dec 2021 18:42:39 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/dortenenboim314/cyclic-jukebox/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "dortenenboim314/cyclic-jukebox",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/dortenenboim314/cyclic-jukebox/master/apex/examples/docker/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/dortenenboim314/cyclic-jukebox/tree/master/tensorboardX/docs",
      "https://github.com/dortenenboim314/cyclic-jukebox/tree/master/apex/docs"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/dortenenboim314/cyclic-jukebox/master/jukebox/Interacting_with_Jukebox.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/dortenenboim314/cyclic-jukebox/master/tensorboardX/compile.sh",
      "https://raw.githubusercontent.com/dortenenboim314/cyclic-jukebox/master/apex/tests/docker_extension_builds/run.sh",
      "https://raw.githubusercontent.com/dortenenboim314/cyclic-jukebox/master/apex/tests/L1/common/run_test.sh",
      "https://raw.githubusercontent.com/dortenenboim314/cyclic-jukebox/master/apex/tests/L1/cross_product/run.sh",
      "https://raw.githubusercontent.com/dortenenboim314/cyclic-jukebox/master/apex/tests/L1/cross_product_distributed/run.sh",
      "https://raw.githubusercontent.com/dortenenboim314/cyclic-jukebox/master/apex/tests/distributed/synced_batchnorm/unit_test.sh",
      "https://raw.githubusercontent.com/dortenenboim314/cyclic-jukebox/master/apex/tests/distributed/DDP/run_race_test.sh",
      "https://raw.githubusercontent.com/dortenenboim314/cyclic-jukebox/master/apex/tests/distributed/amp_master_params/run.sh",
      "https://raw.githubusercontent.com/dortenenboim314/cyclic-jukebox/master/apex/examples/simple/distributed/run.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Install the conda package manager from https://docs.conda.io/en/latest/miniconda.html    \n    \n``` \n#: Required: Sampling\nconda create --name jukebox python=3.7.5\nconda activate jukebox\nconda install mpi4py=3.0.3 #: if this fails, try: pip install mpi4py==3.0.3\nconda install pytorch=1.4 torchvision=0.5 cudatoolkit=10.0 -c pytorch\ngit clone https://github.com/openai/jukebox.git\ncd jukebox\npip install -r requirements.txt\npip install -e .\n\n#: Required: Training\nconda install av=7.0.01 -c conda-forge \npip install ./tensorboardX\n \n#: Optional: Apex for faster training with fused_adam\nconda install pytorch=1.1 torchvision=0.3 cudatoolkit=10.0 -c pytorch\npip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9028246854296311
      ],
      "excerpt": "The hps are for a V100 GPU with 16 GB GPU memory. The 1b_lyrics, 5b, and 5b_lyrics top-level priors take up  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8181060374670783
      ],
      "excerpt": "mpiexec -n {ngpus} python jukebox/train.py --hps=small_vqvae --name=small_vqvae --sample_length=262144 --bs=4 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8680179930597692
      ],
      "excerpt": "Here, {audio_files_dir} is the directory in which you can put the audio files for your dataset, and {ngpus} is number of GPU's you want to use to train.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8181060374670783
      ],
      "excerpt": "mpiexec -n {ngpus} python jukebox/train.py --hps=small_vqvae,small_prior,all_fp16,cpu_ema --name=small_prior \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8779991937619007,
        0.8181060374670783
      ],
      "excerpt": "To train the upsampler, we can run \nmpiexec -n {ngpus} python jukebox/train.py --hps=small_vqvae,small_upsampler,all_fp16,cpu_ema --name=small_upsampler \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8181060374670783
      ],
      "excerpt": "mpiexec -n {ngpus} python jukebox/train.py --hps=vqvae,small_prior,all_fp16,cpu_ema --name=pretrained_vqvae_small_prior \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8181060374670783
      ],
      "excerpt": "mpiexec -n {ngpus} python jukebox/train.py --hps=vqvae,small_labelled_prior,all_fp16,cpu_ema --name=pretrained_vqvae_small_prior_labels \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8181060374670783
      ],
      "excerpt": "mpiexec -n {ngpus} python jukebox/train.py --hps=vqvae,small_single_enc_dec_prior,all_fp16,cpu_ema --name=pretrained_vqvae_small_single_enc_dec_prior_labels \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8181060374670783
      ],
      "excerpt": "mpiexec -n {ngpus} python jukebox/train.py --hps=vqvae,prior_1b_lyrics,all_fp16,cpu_ema --name=finetuned \\ \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9332503225450935
      ],
      "excerpt": "python jukebox/sample.py --model=5b_lyrics --name=sample_5b --levels=3 --sample_length_in_seconds=20 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9332503225450935
      ],
      "excerpt": "python jukebox/sample.py --model=1b_lyrics --name=sample_1b --levels=3 --sample_length_in_seconds=20 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.934833687695145
      ],
      "excerpt": "python jukebox/sample.py --model=5b_lyrics --name=sample_5b --levels=3 --mode=continue \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8456000022144375
      ],
      "excerpt": "Here, we take the 20 seconds samples saved from the first sampling run at sample_5b/level_0/data.pth.tar and continue by adding 20 more seconds.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.934833687695145,
        0.8070708654926224
      ],
      "excerpt": "python jukebox/sample.py --model=5b_lyrics --name=sample_5b --levels=3 --mode=upsample \\ \n--codes_file=sample_5b/level_2/data.pth.tar --sample_length_in_seconds=20 --total_sample_length_in_seconds=180 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8404281248416692
      ],
      "excerpt": "Here, we take the 20 seconds samples saved from the first sampling run at sample_5b/level_2/data.pth.tar and upsample the lower two levels. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.934833687695145
      ],
      "excerpt": "python jukebox/sample.py --model=5b_lyrics --name=sample_5b_prompted --levels=3 --mode=primed \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8577607811218887,
        0.8193100115666223,
        0.9551225911376596,
        0.8014823144909957
      ],
      "excerpt": "This will load the four files, tile them to fill up to n_samples batch size, and prime the model with the first prompt_length_in_seconds seconds. \nTo train a small vqvae, run \nmpiexec -n {ngpus} python jukebox/train.py --hps=small_vqvae --name=small_vqvae --sample_length=262144 --bs=4 \\ \n--audio_files_dir={audio_files_dir} --labels=False --train --aug_shift --aug_blend \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8111257662924631
      ],
      "excerpt": "Checkpoints are stored in the logs folder. You can monitor the training by running Tensorboard \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9628507133691993,
        0.8859862112207763,
        0.8360691585558456
      ],
      "excerpt": "mpiexec -n {ngpus} python jukebox/train.py --hps=small_vqvae,small_prior,all_fp16,cpu_ema --name=small_prior \\ \n--sample_length=2097152 --bs=4 --audio_files_dir={audio_files_dir} --labels=False --train --test --aug_shift --aug_blend \\ \n--restore_vqvae=logs/small_vqvae/checkpoint_latest.pth.tar --prior --levels=2 --level=1 --weight_decay=0.01 --save_iters=1000 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9628507133691993,
        0.8859862112207763,
        0.8360691585558456
      ],
      "excerpt": "mpiexec -n {ngpus} python jukebox/train.py --hps=small_vqvae,small_upsampler,all_fp16,cpu_ema --name=small_upsampler \\ \n--sample_length=262144 --bs=4 --audio_files_dir={audio_files_dir} --labels=False --train --test --aug_shift --aug_blend \\ \n--restore_vqvae=logs/small_vqvae/checkpoint_latest.pth.tar --prior --levels=2 --level=0 --weight_decay=0.01 --save_iters=1000 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9628507133691993
      ],
      "excerpt": "mpiexec -n {ngpus} python jukebox/train.py --hps=vqvae,small_prior,all_fp16,cpu_ema --name=pretrained_vqvae_small_prior \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8949789654244626
      ],
      "excerpt": "--labels=False --train --test --prior --levels=3 --level=2 --weight_decay=0.01 --save_iters=1000 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9168665188481279
      ],
      "excerpt": "- Run sample.py as outlined in the sampling section, but now with --model=my_model  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8362296145341152
      ],
      "excerpt": "To train with you own metadata for your audio files, implement get_metadata in data/files_dataset.py to return the  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9628507133691993
      ],
      "excerpt": "mpiexec -n {ngpus} python jukebox/train.py --hps=vqvae,small_labelled_prior,all_fp16,cpu_ema --name=pretrained_vqvae_small_prior_labels \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9133289776370042
      ],
      "excerpt": "--labels=True --train --test --prior --levels=3 --level=2 --weight_decay=0.01 --save_iters=1000 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8169424950400236
      ],
      "excerpt": "To train in addition with lyrics, update get_metadata in data/files_dataset.py to return lyrics too. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9628507133691993
      ],
      "excerpt": "mpiexec -n {ngpus} python jukebox/train.py --hps=vqvae,small_single_enc_dec_prior,all_fp16,cpu_ema --name=pretrained_vqvae_small_single_enc_dec_prior_labels \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9133289776370042
      ],
      "excerpt": "--labels=True --train --test --prior --levels=3 --level=2 --weight_decay=0.01 --save_iters=1000 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8086778739716873
      ],
      "excerpt": "Support --labels=True by implementing get_metadata in jukebox/data/files_dataset.py for your dataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9628507133691993
      ],
      "excerpt": "mpiexec -n {ngpus} python jukebox/train.py --hps=vqvae,prior_1b_lyrics,all_fp16,cpu_ema --name=finetuned \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9133289776370042
      ],
      "excerpt": "--labels=True --train --test --prior --levels=3 --level=2 --weight_decay=0.01 --save_iters=1000 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/dortenenboim314/cyclic-jukebox/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Cuda",
      "Jupyter Notebook",
      "C++",
      "Shell",
      "CSS",
      "Makefile",
      "HTML",
      "Dockerfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other",
      "url": "https://raw.githubusercontent.com/dortenenboim314/cyclic-jukebox/master/LICENSE"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'All rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\\n\\n1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\\n\\n2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\\n\\n3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Jukebox",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "cyclic-jukebox",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "dortenenboim314",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dortenenboim314/cyclic-jukebox/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Mon, 27 Dec 2021 18:42:39 GMT"
    },
    "technique": "GitHub API"
  }
}