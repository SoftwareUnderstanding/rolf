{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1509.06461"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{hasselt2015doubledqn,\n  abstract = {The popular Q-learning algorithm is known to overestimate action values under\ncertain conditions. It was not previously known whether, in practice, such\noverestimations are common, whether they harm performance, and whether they can\ngenerally be prevented. In this paper, we answer all these questions\naffirmatively. In particular, we first show that the recent DQN algorithm,\nwhich combines Q-learning with a deep neural network, suffers from substantial\noverestimations in some games in the Atari 2600 domain. We then show that the\nidea behind the Double Q-learning algorithm, which was introduced in a tabular\nsetting, can be generalized to work with large-scale function approximation. We\npropose a specific adaptation to the DQN algorithm and show that the resulting\nalgorithm not only reduces the observed overestimations, as hypothesized, but\nthat this also leads to much better performance on several games.},\n  added-at = {2019-11-18T11:40:13.000+0100},\n  author = {van Hasselt, Hado and Guez, Arthur and Silver, David},\n  biburl = {https://www.bibsonomy.org/bibtex/2c2bad4b4c5a34cb31a3f569c71e851ab/jan.hofmann1},\n  description = {[1509.06461] Deep Reinforcement Learning with Double Q-learning},\n  interhash = {d3061c37961afb78096e314854dd90bc},\n  intrahash = {c2bad4b4c5a34cb31a3f569c71e851ab},\n  keywords = {dqn q-learning reinforcement_learning},\n  note = {cite arxiv:1509.06461Comment: AAAI 2016},\n  timestamp = {2019-11-18T11:40:13.000+0100},\n  title = {Deep Reinforcement Learning with Double Q-learning},\n  url = {http://arxiv.org/abs/1509.06461},\n  year = 2015\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "$ source venv/bin/activate \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jihoonerd/Deep-Reinforcement-Learning-with-Double-Q-learning",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-11-19T10:29:59Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-19T16:22:54Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9746549635264323,
        0.9971824327999126,
        0.8688860557143415,
        0.9724320839001409,
        0.9142602881044163
      ],
      "excerpt": "This repository implements the paper: Deep Reinforcement Learning with Double Q-learning. \nThe authors of the paper applied Double Q-learning concept on their DQN algorithm. This paper proposed Double DQN, which is similar to DQN but more robust to overestimation of Q-values. \nThe major difference between those two algorithms is the way to calculate Q-value from target network. Compared to the DQN, directly using Q-value from target network, DDQN chooses an action that maximizes the Q-value of main network at the next state. \nMost of the implementation is almost the same as the implementation of DQN. \nEmployed TensorFlow 2 with performance optimization \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8110494781359243
      ],
      "excerpt": "Easy to reproduce \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8619825136856103
      ],
      "excerpt": "This implementation is guaranteed to work well for Atlantis, Boxing, Breakout and Pong. Tensorboard summary is located at ./archive. Tensorboard will show following information: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9771082459967502
      ],
      "excerpt": "Single RTX 2080 Ti is used for the results below. (Thanks to @JKeun for allowing his computation resources) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9668938514257235
      ],
      "excerpt": "We can see that DDQN's average Q-value is suppressed compared to that of DQN. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "\ud83d\udcd6 Paper: Deep Reinforcement Learning with Double Q-learning \ud83d\udd79\ufe0f",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jihoonerd/Deep-Reinforcement-Learning-with-Double-Q-learning/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 7,
      "date": "Sun, 26 Dec 2021 08:34:23 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jihoonerd/Deep-Reinforcement-Learning-with-Double-Q-learning/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "jihoonerd/Deep-Reinforcement-Learning-with-Double-Q-learning",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8211232956927162,
        0.9979947896609701
      ],
      "excerpt": "$ source venv/bin/activate \n$ pip install -r requirements.txt \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8003261166765615,
        0.853492186136904
      ],
      "excerpt": "Test score \nTotal frames \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8038662968326917
      ],
      "excerpt": "$ tensorboard --logdir=./archive/ \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jihoonerd/Deep-Reinforcement-Learning-with-Double-Q-learning/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 Papers with Code\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Deep Reinforcement Learning with Double Q-learning",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Deep-Reinforcement-Learning-with-Double-Q-learning",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "jihoonerd",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jihoonerd/Deep-Reinforcement-Learning-with-Double-Q-learning/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "***Default running environment is assumed to be CPU-ONLY. If you want to run this repo on GPU machine, just replace `tensorflow` to `tensorflow-gpu` in package lists.***\n\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You can run Atari 2600 game with `main.py`. Running environment needs to be `NoFrameskip` from `gym` package.\n\n```bash\n$ python main.py --help\nusage: main.py [-h] [--env ENV] [--train] [--play PLAY]\n               [--log_interval LOG_INTERVAL]\n               [--save_weight_interval SAVE_WEIGHT_INTERVAL]\n\nAtari: DQN\noptional arguments:\n  -h, --help            show this help message and exit\n  --env ENV             Should be NoFrameskip environment\n  --train               Train agent with given environment\n  --play PLAY           Play with a given weight directory\n  --log_interval LOG_INTERVAL\n                        Interval of logging stdout\n  --save_weight_interval SAVE_WEIGHT_INTERVAL\n                        Interval of saving weights\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 14,
      "date": "Sun, 26 Dec 2021 08:34:23 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "dqn",
      "ddqn",
      "atari",
      "reinforcement-learning",
      "deep-learning",
      "deepmind",
      "tensorflow2"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "``` bash\n$ python main.py --env BreakoutNoFrameskip-v4 --train\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```bash\n$ python main.py --env PongNoFrameskip-v4 --play ./log/[LOGDIR]/weights\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```bash\n$ python main.py --env BreakoutNoFrameskip-v4 --train --log_interval 100 --save_weight_interval 1000\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}