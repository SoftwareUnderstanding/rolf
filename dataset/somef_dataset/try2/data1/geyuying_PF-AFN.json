{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Our code is based on the implementation of \"Clothflow: A flow-based model for clothed person generation\" (See the citation below), including the implementation of the feature pyramid networks (FPN) and the ResUnetGenerator, and the adaptation of the cascaded structure to predict the appearance flows. If you use our code, please also cite their work as below.\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2103.04559"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If our code is helpful to your work, please cite:\n```\n@article{ge2021parser,\n  title={Parser-Free Virtual Try-on via Distilling Appearance Flows},\n  author={Ge, Yuying and Song, Yibing and Zhang, Ruimao and Ge, Chongjian and Liu, Wei and Luo, Ping},\n  journal={arXiv preprint arXiv:2103.04559},\n  year={2021}\n}\n```\n```\n@inproceedings{han2019clothflow,\n  title={Clothflow: A flow-based model for clothed person generation},\n  author={Han, Xintong and Hu, Xiaojun and Huang, Weilin and Scott, Matthew R},\n  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},\n  pages={10471--10480},\n  year={2019}\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{han2019clothflow,\n  title={Clothflow: A flow-based model for clothed person generation},\n  author={Han, Xintong and Hu, Xiaojun and Huang, Weilin and Scott, Matthew R},\n  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},\n  pages={10471--10480},\n  year={2019}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{ge2021parser,\n  title={Parser-Free Virtual Try-on via Distilling Appearance Flows},\n  author={Ge, Yuying and Song, Yibing and Zhang, Ruimao and Ge, Chongjian and Liu, Wei and Luo, Ping},\n  journal={arXiv preprint arXiv:2103.04559},\n  year={2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8684006851719224
      ],
      "excerpt": "Official code for CVPR 2021 paper 'Parser-Free Virtual Try-on via Distilling Appearance Flows' \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/geyuying/PF-AFN",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-03-07T04:08:01Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-30T04:18:55Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.804816325873611
      ],
      "excerpt": "Official code for CVPR 2021 paper 'Parser-Free Virtual Try-on via Distilling Appearance Flows' \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9240554093869691
      ],
      "excerpt": "VITON contains a training set of 14,221 image pairs and a test set of 2,032 image pairs, each of which has a front-view woman photo and a top clothing image with the resolution 256 x 192. Our saved model is trained on the VITON training set and tested on the VITON test set. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Official code for \"Parser-Free Virtual Try-on via Distilling Appearance Flows\", CVPR 2021. ",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/geyuying/PF-AFN/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 86,
      "date": "Thu, 30 Dec 2021 07:44:54 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/geyuying/PF-AFN/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "geyuying/PF-AFN",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/geyuying/PF-AFN/main/PF-AFN_test/test.sh",
      "https://raw.githubusercontent.com/geyuying/PF-AFN/main/PF-AFN_train/scripts/train_PBAFN_e2e.sh",
      "https://raw.githubusercontent.com/geyuying/PF-AFN/main/PF-AFN_train/scripts/train_PBAFN_stage1.sh",
      "https://raw.githubusercontent.com/geyuying/PF-AFN/main/PF-AFN_train/scripts/train_PFAFN_stage1.sh",
      "https://raw.githubusercontent.com/geyuying/PF-AFN/main/PF-AFN_train/scripts/train_PFAFN_e2e.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "conda create -n tryon python=3.6\n\nsource activate tryon     or     conda activate tryon\n\nconda install pytorch=1.1.0 torchvision=0.3.0 cudatoolkit=9.0 -c pytorch\n\nconda install cupy     or     pip install cupy==6.0.0\n\npip install opencv-python\n\ngit clone https://github.com/geyuying/PF-AFN.git\n\ncd PF-AFN\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9322609392449874
      ],
      "excerpt": "pytorch 1.1.0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8411004553040458
      ],
      "excerpt": "cuda 9.0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9143054177750861,
        0.8788932952524015,
        0.8837680365796365,
        0.9906248903846466
      ],
      "excerpt": "opencv-python 4.5.1 \n8 GTX1080 GPU for training; 1 GTX1080 GPU for test \npython 3.6 \ncd PF-AFN_train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8809328484340773
      ],
      "excerpt": "Following the above insructions with the provided training code, the [trained PF-AFN] achieves FID 9.92 on VITON test set with the test_pairs.txt (You can find it in https://github.com/minar09/cp-vton-plus/blob/master/data/test_pairs.txt). \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8459720401470828
      ],
      "excerpt": "8 GTX1080 GPU for training; 1 GTX1080 GPU for test \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8841518271465613,
        0.8620438972843202
      ],
      "excerpt": "Download the VITON training set from VITON_train and put the folder \"VITON_traindata\" under the folder \"dataset\". \nDowload the VGG_19 model from VGG_Model and put \"vgg19-dcbb9e9d.pth\" under the folder \"models\". \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8367551977664247
      ],
      "excerpt": "To train from scratch on VITON training set, you can download VITON_train. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/geyuying/PF-AFN/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Parser-Free Virtual Try-on via Distilling Appearance Flows, CVPR 2021",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "PF-AFN",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "geyuying",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/geyuying/PF-AFN/blob/main/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. cd PF-AFN_test\n2. First, you need to download the checkpoints from [checkpoints](https://drive.google.com/file/d/1_a0AiN8Y_d_9TNDhHIcRlERz3zptyYWV/view?usp=sharing) and put the folder \"PFAFN\" under the folder \"checkpoints\". The folder \"checkpoints/PFAFN\" shold contain \"warp_model_final.pth\" and \"gen_model_final.pth\". \n3. The \"dataset\" folder contains the demo images for test, where the \"test_img\" folder contains the person images, the \"test_clothes\" folder contains the clothes images, and the \"test_edge\" folder contains edges extracted from the clothes images with the built-in function in python (We saved the extracted edges from the clothes images for convenience). 'demo.txt' records the test pairs. \n4. During test, a person image, a clothes image and its extracted edge are fed into the network to generate the try-on image. **No human parsing results or human pose estimation results are needed for test.**\n5. To test with the saved model, run **test.sh** and the results will be saved in the folder \"results\".\n6. **To reproduce our results from the saved model, your test environment should be the same as our test environment, especifically for the version of cupy.** \n\n![image](https://github.com/geyuying/PF-AFN/blob/main/show/compare.jpg?raw=true)\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 312,
      "date": "Thu, 30 Dec 2021 07:44:54 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. cd PF-AFN_test\n2. First, you need to download the checkpoints from [checkpoints](https://drive.google.com/file/d/1_a0AiN8Y_d_9TNDhHIcRlERz3zptyYWV/view?usp=sharing) and put the folder \"PFAFN\" under the folder \"checkpoints\". The folder \"checkpoints/PFAFN\" shold contain \"warp_model_final.pth\" and \"gen_model_final.pth\". \n3. The \"dataset\" folder contains the demo images for test, where the \"test_img\" folder contains the person images, the \"test_clothes\" folder contains the clothes images, and the \"test_edge\" folder contains edges extracted from the clothes images with the built-in function in python (We saved the extracted edges from the clothes images for convenience). 'demo.txt' records the test pairs. \n4. During test, a person image, a clothes image and its extracted edge are fed into the network to generate the try-on image. **No human parsing results or human pose estimation results are needed for test.**\n5. To test with the saved model, run **test.sh** and the results will be saved in the folder \"results\".\n6. **To reproduce our results from the saved model, your test environment should be the same as our test environment, especifically for the version of cupy.** \n\n![image](https://github.com/geyuying/PF-AFN/blob/main/show/compare.jpg?raw=true)\n",
      "technique": "Header extraction"
    }
  ]
}