{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1605.07146",
      "https://arxiv.org/abs/1706.02677 for details.\n    hvd.callbacks.LearningRateWarmupCallback(warmup_epochs=args.warmup_epochs, verbose=verbose"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "verbose = 1 if hvd.rank() == 0 else 0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "if resume_from_epoch > 0: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9151345947539621
      ],
      "excerpt": "if resume_from_epoch > 0 and hvd.rank() == 0: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "    if epoch < 15: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "    if epoch < 25: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9426612454569065
      ],
      "excerpt": "    if epoch < 35: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8485228147960261
      ],
      "excerpt": "#: the first five epochs. See https://arxiv.org/abs/1706.02677 for details. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "if hvd.rank() == 0: \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/darkreapyre/HaaS",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-02-15T17:19:11Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-12-13T00:32:13Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9303529621579317
      ],
      "excerpt": "Why Keras?  We chose Keras due to its simplicity, and the fact that it will be the way to define models in TensorFlow 2.0. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8711998401653926
      ],
      "excerpt": "Let's dive into the modifications! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9016313242247573
      ],
      "excerpt": "This allows you to greatly simplify the model, since it does not have to deal with the manual placement of tensors.  Instead, you just specify which GPU you'd like to use in the beginning of your script. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8431277632761064
      ],
      "excerpt": "Replace verbose = 1 with the following code: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8978831476425502
      ],
      "excerpt": "For the same reason as above, we read the checkpoint only on the first worker and broadcast the initial state to other workers. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9041503085010439
      ],
      "excerpt": ": to other workers. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8115421272935746
      ],
      "excerpt": "Horovod uses an operation that averages gradients across workers.  Gradient averaging typically requires a corresponding increase in learning rate to make bigger steps in the direction of a higher-quality gradient. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8496517314104323
      ],
      "excerpt": ": Horovod: adjust learning rate based on number of GPUs. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8340767353728552
      ],
      "excerpt": "In the previous section, we mentioned that the first worker would broadcast parameters to the rest of the workers.  We will use horovod.keras.BroadcastGlobalVariablesCallback to make this happen. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9868852485702382,
        0.892774383639521
      ],
      "excerpt": "    #: This is necessary to ensure consistent initialization of all workers when \n    #: training is started with random weights or restored from a checkpoint. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9363656567737587,
        0.9827869219489868
      ],
      "excerpt": "Many models are sensitive to using a large learning rate (LR) immediately after initialization and can benefit from learning rate warmup.  The idea is to start training with lower LR and gradually raise it to a target LR over a few epochs.  Horovod has the convenient LearningRateWarmupCallback for the Keras API that implements that logic. \nSince we're already using LearningRateScheduler in this code, and it modifies learning rate along with LearningRateWarmupCallback, there is a possibility of a conflict.  In order to avoid such conflict, we will swap out LearningRateScheduler with Horovod LearningRateScheduleCallback. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8033709849988189
      ],
      "excerpt": "Since we're not validating full dataset on each worker anymore, each worker will have different validation results.  To improve validation metric quality and reduce variance, we will average validation results among all workers. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9564251332842473
      ],
      "excerpt": "#: Horovod: average metrics among workers at the end of every epoch. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Proof of Concept for Horovod-as-a-Service",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/darkreapyre/HaaS-dev/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Mon, 20 Dec 2021 19:10:15 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/darkreapyre/HaaS/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "darkreapyre/HaaS",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8083273017237101
      ],
      "excerpt": "This allows you to greatly simplify the model, since it does not have to deal with the manual placement of tensors.  Instead, you just specify which GPU you'd like to use in the beginning of your script. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8983557191619819
      ],
      "excerpt": ": Horovod: pin GPU to be used to process local rank (one GPU per process) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.916138144178887
      ],
      "excerpt": "Congratulations!  If you made it this far, your fashion_mnist.py should now be fully distributed.  To verify, you can run the following command in the terminal, which should produce no output: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8207897870290474
      ],
      "excerpt": "On the left hand side, you will see a number of Python files: fashion_mnist.py, fashion_mnist_solution.py, and a few intermediate files fashion_mnist_after_step_N.py. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8428941077784315
      ],
      "excerpt": "Add the following code after import tensorflow as tf: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9012248701992861
      ],
      "excerpt": "import horovod.keras as hvd \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8132145333878185,
        0.8415328298313424
      ],
      "excerpt": "config = tf.ConfigProto() \nconfig.gpu_options.allow_growth = True \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8000368641765965
      ],
      "excerpt": "Horovod uses MPI to run model training workers.  By default, MPI aggregates output from all workers.  To reduce clutter, we recommended that you write logs only on the first worker. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9134011868819332
      ],
      "excerpt": ": Horovod: print logs on the first worker. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8732941140719962
      ],
      "excerpt": "    model = keras.models.load_model(args.checkpoint_format.format(epoch=resume_from_epoch)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8520142343078304
      ],
      "excerpt": "    model = hvd.load_model(args.checkpoint_format.format(epoch=resume_from_epoch)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.811854372964597,
        0.8179248174790049
      ],
      "excerpt": "hvd.callbacks.LearningRateScheduleCallback(start_epoch=args.warmup_epochs, end_epoch=15, multiplier=1.), \nhvd.callbacks.LearningRateScheduleCallback(start_epoch=15, end_epoch=25, multiplier=1e-1), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991
      ],
      "excerpt": "$ diff fashion_mnist.py fashion_mnist_solution.py \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/darkreapyre/HaaS/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Fashion MNIST Tutorial",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "HaaS",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "darkreapyre",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/darkreapyre/HaaS/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Before we go into modifications required to scale our WideResNet model, let's run a single-GPU version of the model.\n\nIn the Launcher, click the Terminal button:\n\n<img src=\"https://user-images.githubusercontent.com/16640218/53534695-d135d080-3ab4-11e9-830b-ea5a9e8581d1.png\" width=\"300\"></img>\n\nIn the terminal, type:\n\n```\n$ cp fashion_mnist.py fashion_mnist_backup.py\n$ python fashion_mnist_backup.py --log-dir baseline\n```\n\n![image](https://user-images.githubusercontent.com/16640218/53534844-5620ea00-3ab5-11e9-9307-332db459da66.png)\n\nAfter a few minutes, it will train a few epochs:\n\n![image](https://user-images.githubusercontent.com/16640218/54184767-a4929900-4464-11e9-8a6a-e2fed3f4cd00.png)\n\nOpen the browser and load `http://<ip-address-of-vm>:6006/`:\n\n![image](https://user-images.githubusercontent.com/16640218/54184664-69906580-4464-11e9-8a8f-3a0b4028b379.png)\n\nYou will see training curves in the TensorBoard.  Let it run.  We will get back to the results later.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "To speed up training, we will execute fewer steps of distributed training.  To keep the total number of examples processed during the training the same, we will do `num_steps / N` steps, where `num_steps` is the original number of steps, and `N` is the total number of workers.\n\nWe will also speed up validation by validating `3 * num_validation_steps / N` steps on each worker.  The multiplier **3** provides over-sampling of validation data helps to increase probability that every validation example will be evaluated.\n\nReplace `model.fit_generator(...)` with:\n\n```python\n#: Train the model. The training will randomly sample 1 / N batches of training data and\n#: 3 / N batches of validation data on every worker, where N is the number of workers.\n#: Over-sampling of validation data, which helps to increase the probability that every\n#: validation example will be evaluated.\nmodel.fit_generator(train_iter,\n                    steps_per_epoch=len(train_iter) // hvd.size(),\n                    callbacks=callbacks,\n                    epochs=args.epochs,\n                    verbose=verbose,\n                    workers=4,\n                    initial_epoch=resume_from_epoch,\n                    validation_data=test_iter,\n                    validation_steps=3 * len(test_iter) // hvd.size())\n```\n\n![image](https://user-images.githubusercontent.com/16640218/53536410-283ea400-3abb-11e9-8742-05921b0795de.png)\n(see line 152-164)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "It's time to run your distributed `fashion_mnist.py`.  First, let's check if the single-GPU version completed.  Open the terminal, and verify that it did complete, and interrupt it using Ctrl-C if it did not.\n\n![image](https://user-images.githubusercontent.com/16640218/53536718-448f1080-3abc-11e9-9e22-021dc3ba5de9.png)\n\nNow, run distributed `fashion_mnist.py` using:\n\n```\n$ horovodrun -np 4 python fashion_mnist.py --log-dir distributed\n```\n\n![image](https://user-images.githubusercontent.com/16640218/53536888-da2aa000-3abc-11e9-9083-43060634433c.png)\n\nAfter a few minutes, you should see training progress.  It will be faster compared to the single-GPU model:\n\n![image](https://user-images.githubusercontent.com/16640218/53536956-270e7680-3abd-11e9-8f3b-acbe9bbfd085.png)\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Mon, 20 Dec 2021 19:10:15 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "In this tutorial, you will learn how to apply Horovod to a [WideResNet](https://arxiv.org/abs/1605.07146) model, trained on the [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "In `fashion_mnist.py`, we're using the filename of the last checkpoint to determine the epoch to resume training from in case of a failure:\n\n![image](https://user-images.githubusercontent.com/16640218/54185268-d35d3f00-4465-11e9-99eb-96d4b99f1d38.png)\n\nAs you scale your workload to multi-node, some of your workers may not have access to the filesystem containing the checkpoint.  For that reason, we make the first worker to determine the epoch to restart from, and *broadcast* that information to the rest of the workers.\n\nTo broadcast the starting epoch from the first worker, add the following code:\n\n```python\n#: Horovod: broadcast resume_from_epoch from rank 0 (which will have\n#: checkpoints) to other ranks.\nresume_from_epoch = hvd.broadcast(resume_from_epoch, 0, name='resume_from_epoch')\n```\n\n![image](https://user-images.githubusercontent.com/16640218/53534072-2de3bc00-3ab2-11e9-8cf1-7531542e3202.png)\n(see line 52-54)\n\n",
      "technique": "Header extraction"
    }
  ]
}