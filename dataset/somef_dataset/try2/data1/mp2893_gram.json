{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1409.0473",
      "https://arxiv.org/abs/1611.07012",
      "https://arxiv.org/abs/1406.1078"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mp2893/gram",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2016-11-04T01:01:38Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-24T06:58:44Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The current code trains an RNN ([Gated Recurrent Units](https://arxiv.org/abs/1406.1078)) to predict, at each timestep (i.e. visit), the diagnosis codes occurring in the next visit.\nThis is denoted as *Sequential Diagnoses Prediction* in the paper. \nIn the future, we will relases another version for making a single prediction for the entire visit sequence. (e.g. Predict the onset of heart failure given the visit record)\n\nNote that the current code uses [Multi-level Clinical Classification Software for ICD-9-CM](https://www.hcup-us.ahrq.gov/toolssoftware/ccs/ccs.jsp) as the domain knowledge.\nWe will release the one that uses ICD9 Diagnosis Hierarchy in the future.\n\t\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8142115067903609,
        0.9581067433267693
      ],
      "excerpt": "GRAM implements the algorithm introduced in the following paper: \nGRAM: Graph-based Attention Model for Healthcare Representation Learning \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Graph-based Attention Model",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mp2893/gram/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 66,
      "date": "Mon, 27 Dec 2021 10:10:19 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/mp2893/gram/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "mp2893/gram",
    "technique": "GitHub API"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/mp2893/gram/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "BSD 3-Clause \"New\" or \"Revised\" License",
      "url": "https://api.github.com/licenses/bsd-3-clause"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'Copyright (c) 2017, mp2893\\nAll rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met:\\n\\n Redistributions of source code must retain the above copyright notice, this\\n  list of conditions and the following disclaimer.\\n\\n Redistributions in binary form must reproduce the above copyright notice,\\n  this list of conditions and the following disclaimer in the documentation\\n  and/or other materials provided with the distribution.\\n\\n* Neither the name of GRAM nor the names of its\\n  contributors may be used to endorse or promote products derived from\\n  this software without specific prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "GRAM",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "gram",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "mp2893",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mp2893/gram/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "**STEP 1: Installation**  \n\n1. Install [python](https://www.python.org/), [Theano](http://deeplearning.net/software/theano/index.html). We use Python 2.7, Theano 0.8.2. Theano can be easily installed in Ubuntu as suggested [here](http://deeplearning.net/software/theano/install_ubuntu.html#install-ubuntu)\n\n2. If you plan to use GPU computation, install [CUDA](https://developer.nvidia.com/cuda-downloads)\n\n3. Download/clone the GRAM code  \n\n**STEP 2: Fastest way to test GRAM with MIMIC-III**  \n\nThis step describes how to run, with minimum number of steps, GRAM for predicting future diagnosis codes using MIMIC-III. \n\n0. You will first need to request access for [MIMIC-III](https://mimic.physionet.org/gettingstarted/access/), a publicly avaiable electronic health records collected from ICU patients over 11 years. \n\n1. You can use \"process_mimic.py\" to process MIMIC-III dataset and generate a suitable training dataset for GRAM. \nPlace the script to the same location where the MIMIC-III CSV files are located, and run the script. \nInstructions are described inside the script. \n\n2. Use \"build_trees.py\" to build files that contain the ancestor information of each medical code. \nThis requires \"ccs_multi_dx_tool_2015.csv\" (Multi-level CCS for ICD9), which can be downloaded from \n[here](https://www.hcup-us.ahrq.gov/toolssoftware/ccs/Multi_Level_CCS_2015.zip).\nRunning this script will re-map integer codes assigned to all medical codes.\nTherefore you also need the \".seqs\" file and the \".types\" file created by process_mimc.py.\nThe execution command is `python build_trees.py ccs_multi_dx_tool_2015.csv <seqs file> <types file> <output path>`. \nThis will build five files that have \".level#.pk\" as the suffix.\nThis will replace the old \".seqs\" and \".types\" files with the correct ones.\n(Tian Bai, a PhD student from Temple University found out there was a problem with the re-mapping issue, which is now fixed. Thanks Tian!)\n\n3. Run GRAM using the \".seqs\" file generated by build_trees.py. \nThe \".seqs\" file contains the sequence of visits for each patient. Each visit consists of multiple diagnosis codes.\nInstead of using the same \".seqs\" file as both the training feature and the training label, \nwe recommend using \".3digitICD9.seqs\" file, which is also generated by process_mimic.py, as the training label for better performance and eaiser analysis.\nThe command is `python gram.py <seqs file> <3digitICD9.seqs file> <tree file prefix> <output path>`. \n\n**STEP 3: How to pretrain the code embedding**\n\nFor sequential diagnoses prediction, it is very effective to pretrain the code embeddings with some co-occurrence based algorithm such as [word2vec](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality) or [GloVe](http://nlp.stanford.edu/projects/glove/)\nIn the paper, we use GloVe for its speed, but either algorithm should be fine.\nHere we release codes to pretrain the code embeddings with GloVe.\n\n1. Use \"create_glove_comap.py\" with \".seqs\" file, which is generated by build_trees.py. (Note that you must run build_trees.py first before training the code embedding)\nThe execution command is `python create_glove_comap.py <seqs file> <tree file prefix> <output path>`.\nThis will create a file that contains the co-occurrence information of codes and ancestors.\n\n2. Use \"glove.py\" on the co-occurrence file generated by create_glove_comap.py.\nThe execution command is `python glovepy <co-occurrence file> <tree file prefix> <output path>`.\nThe embedding dimension is set to 128. If you change this, be careful to use the same value when training GRAM.\n\n3. Use the pretrained embeddings when you train GRAM.\nThe command is `python gram.py <seqs file> <3digitICD9.seqs file> <tree file prefix> <output path> --embed_file <embedding path> --embed_size <embedding dimension>`.\nAs mentioned above, be sure to set the correct embedding dimension.\n\n**STEP 4: How to prepare your own dataset**\n\n1. GRAM's training dataset needs to be a Python Pickled list of list of list. Each list corresponds to patients, visits, and medical codes (e.g. diagnosis codes, medication codes, procedure codes, etc.)\nFirst, medical codes need to be converted to an integer. Then a single visit can be seen as a list of integers. Then a patient can be seen as a list of visits.\nFor example, [5,8,15] means the patient was assigned with code 5, 8, and 15 at a certain visit.\nIf a patient made two visits [1,2,3] and [4,5,6,7], it can be converted to a list of list [[1,2,3], [4,5,6,7]].\nMultiple patients can be represented as [[[1,2,3], [4,5,6,7]], [[2,4], [8,3,1], [3]]], which means there are two patients where the first patient made two visits and the second patient made three visits.\nThis list of list of list needs to be pickled using cPickle. We will refer to this file as the \"visit file\".\n\n2. The label dataset (let us call this \"label file\") needs to have the same format as the \"visit file\".\nThe important thing is, time steps of both \"label file\" and \"visit file\" need to match. DO NOT train GRAM with labels that is one time step ahead of the visits. It is tempting since GRAM predicts the labels of the next visit. But it is internally taken care of.\nYou can use the \"visit file\" as the \"label file\" if you want GRAM to predict the exact codes. \nOr you can use a grouped codes as the \"label file\" if you are okay with reasonable predictions and want to save time. \nFor example, ICD9 diagnosis codes can be grouped into 283 categories by using [CCS](https://www.hcup-us.ahrq.gov/toolssoftware/ccs/ccs.jsp) groupers. \nWe STRONGLY recommend that you do this, because the number of medical codes can be as high as tens of thousands, \nwhich can cause not only low predictive performance but also memory issues. (The high-end GPUs typically have only 12GB of VRAM)\n\n3. Use the \"build_trees.py\" to create ancestor information, using the \"visit file\". You will also need a mapping file between the actual medical code names (e.g. \"419.10\") and the integer codes. Please refer to Step 2 to learn how to use \"build_trees.py\" script.\n\n**STEP 5: Hyper-parameter tuning used in the paper**\n\nThis [document](http://www.cc.gatech.edu/~echoi48/docs/gram_hyperparamters.pdf) provides the details regarding how we conducted the hyper-parameter tuning for all models used in the paper.\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 203,
      "date": "Mon, 27 Dec 2021 10:10:19 GMT"
    },
    "technique": "GitHub API"
  }
}