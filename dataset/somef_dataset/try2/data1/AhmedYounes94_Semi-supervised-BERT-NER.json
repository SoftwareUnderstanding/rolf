{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "An exploration in using the pre-trained BERT model to perform Named Entity Recognition \n(NER) where labelled training data is limited but there is a considerable amount of unlabelled data.\nTwo different regularisation terms using Kullback\u2013Leibler (KL) divergence are proposed that aim to \nleverage the unlabelled data to help the model generalise to unseen data.\n\n",
      "technique": "Header extraction"
    }
  ],
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[1] Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean. Efficient Estimation of Word Representations in Vector Space. \n2013, https://arxiv.org/pdf/1301.3781.pdf\n\n[2] Jeffrey Pennington and Richard Socher and Christopher D. Manning. Glove: Global vectors for word representation. 2014,\nhttps://nlp.stanford.edu/pubs/glove.pdf\n\n[3] Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. 2018,\nhttps://arxiv.org/pdf/1810.04805.pdf\n\n[4] Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut. ALBERT: A Lite BERT for Self-supervised Learning of Language Representations. 2019, \nhttps://arxiv.org/pdf/1909.11942.pdf \n\n[5] Junyuan Xie and Ross Girshick and Ali Farhadi. Unsupervised Deep Embedding for Clustering Analysis. 2015,\nhttps://arxiv.org/pdf/1511.06335.pdf\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9278824608274014
      ],
      "excerpt": "| org  | Organization |  3.52% | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/AhmedYounes94/Semi-supervised-BERT-NER",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-03-21T19:20:58Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-07-19T09:12:36Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8437922015614984,
        0.9033141337843558,
        0.9519333245038335,
        0.807842534542584,
        0.9731879619262273
      ],
      "excerpt": "BERT NER Data Distribution KL: Same architecture as BERT NER but in the final epochs of \ntraining a KL term is introduced to encourage predicted labels for the unlabelled \ndata to match the expected probability distribution of the data. \nBERT NER Confidence KL: Same architecture as BERT NER but in the final epochs of \ntraining a KL term is introduced to encourage the model to have high confidence when  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9106792678704133,
        0.9262387025906282,
        0.8434742554342463,
        0.8107138000874607
      ],
      "excerpt": "Identifying named entities in a sentence is a common task in NLP pipelines. There are \nan extensive set of datasets available online with marked entities such as famous person  \nor geographic location but often more bespoke categories are needed for particular application. \nAn example of this could be a chatbot application which may begin processing a message \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8403335748074173,
        0.9769384334079243,
        0.9851615691220739,
        0.9700580627324741,
        0.938993740148753,
        0.9540831720183962,
        0.8976859417212442
      ],
      "excerpt": "of labelling this data must be done laboriously internally especially in a PoC phase.  \nThis results in there being only a small amount of labelled data with the potential addition  \nof some unlabelled data. \nThe aim behind this project is to design a solution to learn as much as possible from  \nthe small amount of labelled data without over-fitting as well as leveraging  \nunlabelled data to improve generalisation to unseen data. \nLearning representation for words on large corpora which are applicable across many NLP application has  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8474472783258864
      ],
      "excerpt": "in using deep neural networks to produce the pre-train word embeddings Word2Vec and GloVe respectively. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9315887248783316,
        0.8325996623417724,
        0.9792262831184888,
        0.931899834642575,
        0.9878331526727019
      ],
      "excerpt": "performance of the naive one-hot encoding approach especially when training data is limited. \nMore recently, there was a huge breakthrough in learned representation from Devlin et al. with the  \ndesign of the BERT model [3]. The model comprises of 12 transformer layers and learns a  \nrepresentation for the context of a word in a given sentence. For different downstream tasks,  \nminimal additional parameter are added and the whole model is fine tuned to the data. This \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8778650132533907,
        0.8482779360405724,
        0.9194644543858819
      ],
      "excerpt": "The pre-trained BERT model has achieved state-of-the-art performance on a number of NLP \ntasks and seems like the most appropriate architecture for the NER problem especially when data  \nis limited. However, fine tuning hundreds of millions of parameters requires considerable \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9406822897914536
      ],
      "excerpt": "version of BERT called ALBERT by utilising factorisation of embedding parameters and  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.95229141168783,
        0.8964307996408328,
        0.9942291969410803,
        0.9565127870078991,
        0.9918325093013189,
        0.9917506722664582,
        0.9082455010224336
      ],
      "excerpt": "The Kullback\u2013Leibler (KL) divergence is a measure of how one probability distribution is different  \nfrom another. Often in our data we can estimate a prior distribution for the categorical labels  \nby observing our labelled data or from knowledge of an industry e.g. roughly knowing the \npercentage of credit transactions which are fraudulent. The proposed Data Distribution KL Regularizer \nis designed to leverage this prior knowledge in combination with the predictions on the unlabelled \ndata to improve the generalisation of the model.  \nWe aim for the distribution of our model assigned labels, <img src=\"https://render.githubusercontent.com/render/math?math=q(l)\">, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9738234441363873,
        0.9932104543827596,
        0.9728691881595594,
        0.8536114271647917,
        0.928985912240785,
        0.9599770246529157,
        0.8339332621447076
      ],
      "excerpt": "the word in the flattened batch of 128 sentences. The KL loss is defined by: \nThis loss is optimised on batches of the unlabelled data on alternating steps with the optimisation \nfor the cross entropy loss on the labelled data in the later stages of training. \nThe second KL regularizer explored was designed to reward a model that had high confidence  \nin the predicted labels made on the unlabelled data. The model will use the unlabelled data to \nproduce better representations of the words that are more generalisable. Xie et al. proposed the \nfollowing prior to encourage confidence in unsupervised cluster assignment [5]:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8593929076344762,
        0.9418074318761674
      ],
      "excerpt": "distance metrics to cluster centroids whereas we continue to use the probabilities produced by \nthe softmax layer of the network. The KL loss is defined by: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9932104543827596,
        0.9728691881595594,
        0.9607815792972235
      ],
      "excerpt": "This loss is optimised on batches of the unlabelled data on alternating steps with the optimisation \nfor the cross entropy loss on the labelled data in the later stages of training. \nThe data consists of 48,000 sentences from newspaper articles with the following  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9819838052301368,
        0.9968184447179308
      ],
      "excerpt": "All the data is labelled but to simulate a case where we have unlabelled data, we ignore the labels \non the remaining 36,000 sentences. The data is placed into batches of size 128. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8770704890838043,
        0.9901095894706375,
        0.9666917796616403,
        0.9072667250264492,
        0.9023613381864626,
        0.9635706491177682,
        0.8600939424781916,
        0.9913452141580898,
        0.9419818583242676,
        0.983492717208683,
        0.8115558521679143,
        0.9772961037120373
      ],
      "excerpt": "The multi-layered perceptron on top has 3 layers of size 256, 128 and 64 with relu activation. \nThere is then a dense layer of size 32 with no activation which represents the latent space to visualise the data \nclusters that are forming. There is then a final dense layer with a softmax activation to assign the probabilities \nof the labels. \nThe BERT NER is trained over 20 epochs on the training dataset (400 batches); after this point the model began to over-fit. \nFor the models with KL divergence, the BERT NER is then fine tuned by doing a gradient descent on the KL divergence  \nusing an unlabelled batch followed by a gradient descent on the cross entropy loss on a labelled batch. This \nprocess is repeated for 40 labelled and unlabelled batches for the data distribution KL model and \n80 labelled and unlabelled batches for the confidence KL model respectively. \nThe tokenisation of the sentences is done using the tokenizer from the bert-for-tf2 library. \nAll words are lower cased. \nSentences are padded such that they all have a length of 50. These padded token are ignored when  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9769601623261703
      ],
      "excerpt": "to denominators of calculations on the probability because the probability on labels of  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9396716530581588
      ],
      "excerpt": "A baseline model that uses an embedding layer of size 784 instead of  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9078015086643081,
        0.9794910035903476
      ],
      "excerpt": "are the same as described in Section 4.1. This is a simple baseline model where each  \nword is seen to be independent in a sentence. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9295575498708772
      ],
      "excerpt": "| Validation Accuracy no Other | Overall accuracy on the full test set when words with ground truth tag of O are removed | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.942024604802275,
        0.9615664747420756,
        0.9899024167678087,
        0.9633748325862275,
        0.991833056272072
      ],
      "excerpt": "The addition of the KL optimisation steps has improved the overall performance of the  \nmodel when the hugely dominant O category is removed from consideration and when assessing  \nall categories equally with a mean F1. The overall accuracy in the model using the data \ndistribution KL is lower than the BERT NER but it has a significant boost in the accuracy of the  \nother categories; this is because the KL term is encouraging the model to categorise less  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9685091665393498
      ],
      "excerpt": "of 32 (the layer before the dense softmax layer). These representations are reduced to 2D using \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8757349394000123,
        0.9814784102776215,
        0.9765130218273969,
        0.9782881184592916,
        0.9106421856750776,
        0.9723615907894175,
        0.9067202499255832
      ],
      "excerpt": "These provide thoughtful insight into how the KL optimisation steps are affecting the  \nrepresentations the model is learning. The representation for the NER baseline is very  \ncompact and the large gaps within the latent space highlight that it is not learning \na very strong representation. This is because the model is encoding each work independantly \nand so there is no information encoded about the overall sentence. \nThe model with the data distribution KL can be seen to be classifying more samples into the  \ntim class opposed to O to match the prior distribution. To improve the precision of  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9650611808933718,
        0.8095968985772745,
        0.9857623297372295,
        0.9340109094370764,
        0.9491759790976343,
        0.941769012779417,
        0.8627256928323785
      ],
      "excerpt": "O. Furthermore, the addition of the unlabelled seems to have results in much better  \nclusters forming for the geo, gpe and per categories. \nThe model with the confidence KL has a very similar encoding space to the BERT NER model  \nbut to increase the confidence in predictions, it has dispersed the cluster centroids.  \nThis has resulted in encodings that are more spread over the latent space. \nCompare with bias labelled data: The labelled data was just a random sample of the set  \nof training data available. It would be interesting to compare the approaches if the labelled data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.836521486854109
      ],
      "excerpt": "the KL term more effectively. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9615582684010383
      ],
      "excerpt": "to more traditiomal methods such as L2 norm or dropout. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9642940045619782
      ],
      "excerpt": "models: Tensorflow model code, instances of tf.keras.Model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9788692938675431
      ],
      "excerpt": "model_evaluator: Functions to evaluate the performance of the model and produce visualisations \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/AhmedYounes94/Semi-supervised-BERT-NER/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 22 Dec 2021 12:38:11 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/AhmedYounes94/Semi-supervised-BERT-NER/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "AhmedYounes94/Semi-supervised-BERT-NER",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/AhmedYounes94/Semi-supervised-BERT-NER/master/install.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Create Environment: `conda create --name nerbert python=3.6`\n\nActivate Environment: `conda activate nerbert`\n\nMake Install Executable: `chmod +x install.sh`\n\nInstall Requirements: `./install.sh`\n\nSave All Models To `saved_models` Directory From: https://drive.google.com/drive/folders/1HgHJtuW1fOuO8bWxSAxTZZQL48FW-rRI?usp=sharing\n\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9047428462581225
      ],
      "excerpt": "predicting the unlabelled training examples. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8119281531593652
      ],
      "excerpt": "The labelled training dataset consists of 2,560 random sentences, there are 9,600 sentences in the test set. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.899851888624789,
        0.9079684128105863,
        0.9014120586771117,
        0.9155634854569387
      ],
      "excerpt": "NER Baseline| <img src=\"results/NER_baseline/NER_baseline_true_labels_2d.png\" height=\"450\" width=\"450\"> | <img src=\"results/NER_baseline/NER_baseline_predicted_labels_2d.png\" height=\"450\" width=\"450\"> \nBERT NER | <img src=\"results/BERT/BERT_true_labels_2d.png\" height=\"450\" width=\"450\"> | <img src=\"results/BERT/BERT_predicted_labels_2d.png\" height=\"450\" width=\"450\"> \nBERT NER Data Distribution KL | <img src=\"results/BERT_data_dist_kl/BERT_data_dist_kl_true_labels_2d.png\" height=\"450\" width=\"450\"> | <img src=\"results/BERT_data_dist_kl/BERT_data_dist_kl_predicted_labels_2d.png\" height=\"450\" width=\"450\"> \nBERT NER Confidence KL | <img src=\"results/BERT_confidence_kl/BERT_confidence_kl_true_labels_2d.png\" height=\"450\" width=\"450\"> | <img src=\"results/BERT_confidence_kl/BERT_confidence_kl_predicted_labels_2d.png\" height=\"450\" width=\"450\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.846686679665857,
        0.8626716304516803
      ],
      "excerpt": "config: Yaml files with configurations for models and input data \ndata: CSV file of raw data and preprocessed \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8303656063226849
      ],
      "excerpt": "models: Tensorflow model code, instances of tf.keras.Model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.894096643940258
      ],
      "excerpt": "preprocessor: Preprocesses training and test data \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/AhmedYounes94/Semi-supervised-BERT-NER/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Semi-Supervised Named Entity Recognition with BERT and KL Regularizers",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Semi-supervised-BERT-NER",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "AhmedYounes94",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/AhmedYounes94/Semi-supervised-BERT-NER/blob/master/readme.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Train: python -m examples.example_train_<model_name>\n\nEvaluate: python -m examples.example_evaluate_model\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Wed, 22 Dec 2021 12:38:11 GMT"
    },
    "technique": "GitHub API"
  }
}