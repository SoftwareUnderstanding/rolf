{
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{bansal2020sam,\n    title={SAM: The Sensitivity of Attribution Methods to Hyperparameters},\n    author={Naman Bansal, Chirag Agarwal, Anh Nguyen},\n    booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},\n    pages={},\n    year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9999041615054083,
        0.9999301459967642
      ],
      "excerpt": "Bansal, Agarwal, Nguyen (2020). SAM: The sensitivity of attribution methods to hyperparameters. Computer Vision and Pattern Recognition (CVPR). Oral* presentation. paper | code \nIf you use this software, please consider citing: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9818894004866677,
        0.9999999984528145,
        0.8944178096468923,
        0.9664456561658856
      ],
      "excerpt": "    author={Naman Bansal, Chirag Agarwal, Anh Nguyen}, \n    booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition}, \n    pages={}, \n    year={2020} \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/anguyen8/sam",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-02-18T17:47:23Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-09-21T15:23:41Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9853630224053341
      ],
      "excerpt": "This repository contains source code necessary to reproduce some of the main results in our paper: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9881160850368393,
        0.9330498097137375
      ],
      "excerpt": "All of our experiments were conducted on two groups of classifiers: (a) GoogLeNet and ResNet-50 pre-trained on the 1000-class 2012 ImageNet dataset; and (b) the robust versions of them, i.e. GoogLeNet-R and ResNet-R that were trained to also be invariant to small adversarial changes in the input image. The two regular models are obtained from the PyTorch model zoo, the ResNet-R from Engstrom et al., and we trained GoogLeNet-R by ourselves using the code released by the author. While the two robust classifiers are more invariant to pixel-wise noise they have lower ImageNet validation-set accuracy scores (50.94% and 56.25%) than those of the original GoogLeNet and ResNet (68.86% and 75.59%). \nEngstrom et al. adversarially trained a ResNet-50 model using Projected Gradient Descent (PGD) attack with a normalized step size. We followed the author and trained robust GoogLeNet model, denoted as GoogLeNet-R, for our sensitivity experiments. We used adversarial perturbation in <a href=\"https://www.codecogs.com/eqnedit.php?latex=L_2\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?L_2\"/></a>-norm for generating adversarial samples during training. Additionally, we used <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\epsilon\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\epsilon\"/></a>=3, a step size of 0.5 and the number of steps as 7 for PGD. The model was trained end-to-end for 90 epochs using a batch-size of 256 on 4 Tesla-V100 GPU's. We used SGD optimizer with a learning rate (lr) scheduler starting with lr=0.1 and dropping the learning rate by 10 after every 30 epochs. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9048517892580188,
        0.9475106704974864,
        0.8203643589677717
      ],
      "excerpt": "- By default, the robust model returns a tuple (logits, input). We changed it to output only logits. \n- By default, it requires a different input normalization as mentioned in the original repo. We have modified it to allow both the normal pytorch pre-processing and the one used originally. Please refer to the function load_madry_model(), in ./utils.py for more details.  \n- If you are wrtiting your own explanation method or want to rewrite one of the methods in our repo for robust models, we would recommend setting my_attacker=True while calling load_madry_model(). This greatly simplifies the implementation. You should only set it to False if you want to adversarially perturb the image. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9740049912241094,
        0.9778895810250738
      ],
      "excerpt": "<p align=\"center\"><i> The real image followed by the different attribution maps generated using (top-->bottom) LIME, Sliding-Patch, Meaningful Perturbation and SmoothGrad algorithms. We show the sensitivity (left-->right) of each explanation algorithm with respect to its respective hyperparameter.</i></p> \nThe shell script for generating Figure 2 of our paper is in gradient.sh. Given an image, the script generates the gradient of four models (GoogLeNet, GoogLeNet-R, ResNet-50, and ResNet-50-R) for a clean and noisy image respectively. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9880831906245425,
        0.9574791727070722
      ],
      "excerpt": "<p align=\"center\"><i> The clean image followed by the gradient attribution maps generated using (left-->right) GoogLeNet, GoogLeNet-R, ResNet-50, and ResNet-50-R models. We show the sensitivity of the gradients on adding a small Gaussian noise to the clean image for all the models respectively.</i></p> \nThe shell script for evaluating the sensitivity of different explanation methods is in sensitivity.sh. The sensitivity is calculated for five sample images in this folder across all four models (GoogLeNet, GoogLeNet-R, ResNet-50, and ResNet-50-R). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8530617246339036
      ],
      "excerpt": "from the sensitivity.sh would produce the sensitivity results of the Sliding-Patch explanation algorithm to its different patch sizes (52, 53, 54). It generates a report (shown in figure below) which lists down the mean and standard deviation of all evaluation metric scores. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Code for the CVPR 2020 [ORAL] paper \"SAM: The Sensitivity of Attribution Methods to Hyperparameters\"",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/anguyen8/sam/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 4,
      "date": "Tue, 21 Dec 2021 16:01:45 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/anguyen8/sam/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "anguyen8/sam",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/anguyen8/sam/master/train.sh",
      "https://raw.githubusercontent.com/anguyen8/sam/master/gradient.sh",
      "https://raw.githubusercontent.com/anguyen8/sam/master/teaser.sh",
      "https://raw.githubusercontent.com/anguyen8/sam/master/sensitivity.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This repository is built using PyTorch. You can install the necessary libraries by pip installing the requirements text file `pip install -r ./requirements.txt`\nThe code was set up using **python=3.6.7**\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8182151000557811,
        0.851961143522265,
        0.8144052703603745,
        0.9400582906312459
      ],
      "excerpt": "All the pre-trained models are available here. The user has to download the weight files and store them under the ./models/ directory. \nGoogLeNet-R can be trained using the script provided in train.sh. The user has to install the robustness repo and provide the input directory for the ImageNet training images under data_path argument.  \nNote: Before running the train.sh scripts, replace the files under robustness/imagenet_models/ (e.g. ~/anaconda3/envs/your_env/lib/python3.6/site-packages/robustness/imagenet_models/) in the robustness library folder with the files in the folder madry_files. \nThese are the following modifations that we made to the robustness directory (now ./naman_robustness).  \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8103181893783247
      ],
      "excerpt": "All the pre-trained models are available here. The user has to download the weight files and store them under the ./models/ directory. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9301057239271869
      ],
      "excerpt": "    <img src=\"./results/formal_teaser.jpg\" height=300px width=300px> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9220383272703306
      ],
      "excerpt": "    <img src=\"./results/formal_gradient.jpg\" width=500px> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9101288176117525
      ],
      "excerpt": "  CUDA_VISIBLE_DEVICES=0 python Occlusion_Madry.py -idp ./Images/images_sensitivity/ -ops ${patch} -op ./results/Sensitivity/Occlusion/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8503180584784705
      ],
      "excerpt": "python Sensitivity_Analysis_Basic_Occlusion_Comp_With_Default_Settings.py -idp ./results/Sensitivity/Occlusion/ -mn occlusion --metric_name hog -op ./results/evaluation_result_text_files/Occlusion --exp_num a03 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9277163160832309
      ],
      "excerpt": "    <img src=\"./formal_sensitivity_occlusion.jpg\" height=300px width=300px> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/anguyen8/sam/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# SAM: The Sensitivity of Attribution Methods to Hyperparameters",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "sam",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "anguyen8",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/anguyen8/sam/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 24,
      "date": "Tue, 21 Dec 2021 16:01:45 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- The shell script for generating Figure 1 of our paper is in [teaser.sh](teaser.sh). Given an [image](./Images/teaser/ILSVRC2012_val_00002056.JPEG), the script runs SmoothGrad, Sliding-Patch, LIME, and Meaningful Perturbation algorithm for their different hyperparameters and produces a montage image of their respective [attribution maps](./results/formal_teaser.jpg)\n\n",
      "technique": "Header extraction"
    }
  ]
}