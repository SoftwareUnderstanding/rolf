{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1707.02275\n\n##### Update\nThe code-docstring-corpus version 2, with class declarations, class methods, module docstrings and commit SHAs is now available in the directory V2\n\n#### Installation\nThe dependencies can be installed using `pip`:\n```\npip install -r requirements.txt\n```\n\nExtraction scripts require AST Unparser ( https://github.com/simonpercivall/astunparse ",
      "https://arxiv.org/abs/1611.08307 - code: https://github.com/uclmr/pycodesuggest ",
      "https://arxiv.org/abs/1508.07909 - code: https://github.com/rsennrich/subword-nmt ",
      "https://arxiv.org/abs/1703.04357 - code: https://github.com/rsennrich/nematus ",
      "https://arxiv.org/abs/1707.02275",
      "https://arxiv.org/abs/1707.02275 https://arxiv.org/abs/1707.02275"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you use this corpus for a scientific publication, please cite: Miceli Barone, A. V. and Sennrich, R., 2017 \"A parallel corpus of Python functions and documentation strings for automated code documentation and code generation\" arXiv:1707.02275 https://arxiv.org/abs/1707.02275\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9977994744046882
      ],
      "excerpt": "Paper: https://arxiv.org/abs/1707.02275 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.998689381378076
      ],
      "excerpt": "The corpora were assembled by scraping from open source GitHub repository with the GitHub scraper used by Bhoopchand et al. (2016) \"Learning Python Code Suggestion with a Sparse Pointer Network\" (paper: https://arxiv.org/abs/1611.08307 - code: https://github.com/uclmr/pycodesuggest ) . \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999829606249414
      ],
      "excerpt": "In order to compute baseline results, the data from the canonical split (parallel-corpus directory) was further sub-tokenized using Sennrich et al. (2016) \"Byte Pair Encoding\" (paper: https://arxiv.org/abs/1508.07909 - code: https://github.com/rsennrich/subword-nmt ). Finally, we trained baseline Neural Machine Translation models for both the code2doc and the doc2code tasks using Nematus (Sennrich et al. 2017, paper: https://arxiv.org/abs/1703.04357 - code: https://github.com/rsennrich/nematus ). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9924539268528326,
        0.9848179726092176
      ],
      "excerpt": "| decldesc2bodies.baseline   | 10.32           | 10.24     | \n| decldesc2bodies.backtransl | 10.85           | 10.90     | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/EdinburghNLP/code-docstring-corpus",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-04-07T16:15:37Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-10T06:47:03Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9160119012655003
      ],
      "excerpt": "We release a parallel corpus of 150370 triples of function declarations, function docstrings and function bodies. We include multiple corpus splits, and an additional \"monolingual\" code-only corpus with corresponding synthetically generated docstrings. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9300600156680126,
        0.9673806575183753
      ],
      "excerpt": "| code-only-corpus | A code-only corpus of 161630 pairs of function declarations and function bodies, annotated with metadata. | \n| backtranslations-corpus | A corpus of docstrings automatically generated from the code-only corpus using Neural Machine Translation, to enable data augmentation by \"backtranslation\" | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8504579524409855,
        0.9892091104899764
      ],
      "excerpt": "| repo_split.parallel-corpus | An alternate train/validation/test split of the parallel corpus which is \"repository-consistent\": no repository is split between training, validation or test sets. | \n| repo_split.code-only-corpus | A \"repository-consistent\" filtered version of the code-only corpus: it only contains fragments which appear in the training set of the above repository. | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9671054552914629
      ],
      "excerpt": "We also used the code2doc model to generate the docstring corpus from the code-only corpus which is available in the backtranslations-corpus directory. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Preprocessed Python functions and docstrings for automated code documentation (code2doc) and automated code generation (doc2code) tasks.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Avmb/code-docstring-corpus/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 44,
      "date": "Sun, 26 Dec 2021 08:22:58 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/EdinburghNLP/code-docstring-corpus/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "EdinburghNLP/code-docstring-corpus",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Avmb/code-docstring-corpus/master/scripts/extract_funcdefs_without_docstrings_recur.sh",
      "https://raw.githubusercontent.com/Avmb/code-docstring-corpus/master/scripts/prepare_data_ps.sh",
      "https://raw.githubusercontent.com/Avmb/code-docstring-corpus/master/scripts/extract_modules_and_classes_recur.sh",
      "https://raw.githubusercontent.com/Avmb/code-docstring-corpus/master/scripts/prepare_data_mono_ps.sh",
      "https://raw.githubusercontent.com/Avmb/code-docstring-corpus/master/scripts/extract_methoddefs_and_meta_without_docstrings_properspacing_recur.sh",
      "https://raw.githubusercontent.com/Avmb/code-docstring-corpus/master/scripts/prepare_mono_filtered.sh",
      "https://raw.githubusercontent.com/Avmb/code-docstring-corpus/master/scripts/extract_funcdefs_and_meta_without_docstrings_properspacing_recur.sh",
      "https://raw.githubusercontent.com/Avmb/code-docstring-corpus/master/scripts/extract_methoddefs_and_docstrings_and_meta_properspacing_recur.sh",
      "https://raw.githubusercontent.com/Avmb/code-docstring-corpus/master/scripts/prepare_meta.sh",
      "https://raw.githubusercontent.com/Avmb/code-docstring-corpus/master/scripts/extract_funcdefs_and_docstrings_and_meta_properspacing_recur.sh",
      "https://raw.githubusercontent.com/Avmb/code-docstring-corpus/master/scripts/prepare_v2.sh",
      "https://raw.githubusercontent.com/Avmb/code-docstring-corpus/master/scripts/extract_funcdefs_and_docstring_recur.sh",
      "https://raw.githubusercontent.com/Avmb/code-docstring-corpus/master/scripts/prepare_repo_split.sh",
      "https://raw.githubusercontent.com/Avmb/code-docstring-corpus/master/scripts/clear_linefeeds.sh",
      "https://raw.githubusercontent.com/Avmb/code-docstring-corpus/master/scripts/nmt/prepare_data_mono_declbodies2desc.sh",
      "https://raw.githubusercontent.com/Avmb/code-docstring-corpus/master/scripts/nmt/train_decldesc2bodies.sh",
      "https://raw.githubusercontent.com/Avmb/code-docstring-corpus/master/scripts/nmt/postprocess.sh",
      "https://raw.githubusercontent.com/Avmb/code-docstring-corpus/master/scripts/nmt/prepare_data_declbodies2desc.sh",
      "https://raw.githubusercontent.com/Avmb/code-docstring-corpus/master/scripts/nmt/prepare_data_decldesc2bodies.sh",
      "https://raw.githubusercontent.com/Avmb/code-docstring-corpus/master/scripts/nmt/train_declbodies2desc.sh",
      "https://raw.githubusercontent.com/Avmb/code-docstring-corpus/master/scripts/nmt/train_decldesc2bodies_backtransl.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The dependencies can be installed using `pip`:\n```\npip install -r requirements.txt\n```\n\nExtraction scripts require AST Unparser ( https://github.com/simonpercivall/astunparse ), NMT tokenization requires the Moses tokenizer scripts ( https://github.com/moses-smt/mosesdecoder )\n\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8684349721286629
      ],
      "excerpt": "| Model                  | Validation BLEU | Test BLEU | \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/EdinburghNLP/code-docstring-corpus/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other"
    },
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "code-docstring-corpus",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "code-docstring-corpus",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "EdinburghNLP",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/EdinburghNLP/code-docstring-corpus/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 151,
      "date": "Sun, 26 Dec 2021 08:22:58 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "corpus",
      "code-generation",
      "documentation-generator",
      "docstrings",
      "neural-machine-translation"
    ],
    "technique": "GitHub API"
  }
}