{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1301.3781. http://arxiv.org/pdf/1301.3781.pdf"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013a). Distributed representations of words and phrases and their compositionality. Advances in Neural Information Processing Systems. http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\n\nMikolov, T., Chen, K., Corrado, G., & Dean, J. (2013b). Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781. http://arxiv.org/pdf/1301.3781.pdf\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8550101043698384
      ],
      "excerpt": "| Negative sampling 5 |     0.01298     |     0.01594      |      0.01444    | 10.69 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/chikalabouka/INF8225-TP4",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-04-28T01:59:12Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-08-29T16:14:57Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9867005316454678,
        0.9877136712066913
      ],
      "excerpt": "A word2vec implementation in python of the Continuous Bag of Words (CBOW) and Skip-gram neural network architectures using Hierarchical Softmax and Negative Sampling for efficient learning of word vectors (Mikolov, et al., 2013a, b, c; http://code.google.com/p/word2vec/). \nWe used this implementation for testing the approch of the two research papers on smaller training sets and with the minumum of epochs possible to obtain an acceptable result. We decided to train on 16 millions wikipedia words rather than 15 billions and we stopped in 3 epochs for the algorithms. Here are our results : \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "word2vec with cbow and skipgrams models using hierarchical softmax and negative sampling",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/chikalabouka/INF8225-TP4/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 30 Dec 2021 04:39:51 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/chikalabouka/INF8225-TP4/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "chikalabouka/INF8225-TP4",
    "technique": "GitHub API"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/chikalabouka/INF8225-TP4/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "INF8225-TP4",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "INF8225-TP4",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "chikalabouka",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/chikalabouka/INF8225-TP4/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Thu, 30 Dec 2021 04:39:51 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Install python 3.6 or higher and install dependencies from requirements.txt\n```\n    pip3 install -r requirements.txt\n```\n\n- Train and test word vectors:\n```\nword2vec.py [-h] [-test TEST] -model FO [-train FI] [-cbow CBOW]\n                   [-negative NEG] [-dim DIM] [-alpha ALPHA] [-window WIN]\n                   [-min-count MIN_COUNT] [-processes NUM_PROCESSES]\n                   [-epochs EPOCHS]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -test TEST            Load trained model and test it\n  -model FO             Output model file -> if test is enabled, then provide\n                        the path for the model to test\n  -train FI             Training file\n  -cbow CBOW            1 for CBOW, 0 for skip-gram\n  -negative NEG         Number of negative examples (>0) for negative\n                        sampling, 0 for hierarchical softmax\n  -dim DIM              Dimensionality of word embeddings\n  -alpha ALPHA          Starting alpha\n  -window WIN           Max window length\n  -min-count MIN_COUNT  Min count for words used to learn <unk>\n  -processes NUM_PROCESSES\n                        Number of processes\n  -epochs EPOCHS        Number of training epochs\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}