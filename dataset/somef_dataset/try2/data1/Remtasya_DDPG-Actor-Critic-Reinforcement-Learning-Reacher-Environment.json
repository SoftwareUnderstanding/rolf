{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1511.05952\n2.  Distributional DQN - https://arxiv.org/abs/1707.06887\n3.  Learning from multi-step bootstrap targets -  https://arxiv.org/abs/1602.01783\n",
      "https://arxiv.org/abs/1707.06887\n3.  Learning from multi-step bootstrap targets -  https://arxiv.org/abs/1602.01783\n",
      "https://arxiv.org/abs/1602.01783\n"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8870876002041912
      ],
      "excerpt": "The algorithm used is based on the Deep Reinforcement Learning DDPG algorithm described in this paper: https://arxiv.org/pdf/1509.02971.pdf \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Remtasya/DDPG-Actor-Critic-Reinforcement-Learning-Reacher-Environment",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-04-27T08:29:53Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-09-11T06:34:46Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This is the repository for my trained Deep Deterministic Policy Gradient based agent on the Unity Reacher Enviroment from the Deep Reinforcement Learning nanodegree program. To 'solve' the environment the agent must navigate the Envirnoment with an average score of greater than 30 over the last 100 episodes. This repository provides the code to achieve this in 110 episodes. \n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9411570896163356,
        0.9677616881622276,
        0.9910933096140653,
        0.8634337889812324,
        0.8218365064146637
      ],
      "excerpt": "The task is episodic with termination after 1000 timesteps. \nA state is represented by a vector of 33 dimensions, which contains information about the agent and environment. \nAn action consists of a 4 diminsional vector with values between -1 and 1, which corresponds to forces applied to the joints of the robotic arm. \nPositioning the arm inside the moving target provides a reward of 0.1 per timestep. \nTo confirm the environment is set up correctly I recommend running the random_agent.ipynb notebook to observe a randomly-acting agent. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9932483016382682,
        0.9864878907840375,
        0.9796917475255589
      ],
      "excerpt": "This describes additional details of the implementation such as the empirical results, hyperparameter selections and other specifics of the DDPG algorithm. \nThis is a simple python script that specifies the pytorch model architectures used for the Actor Network and Critic Network. For this project the architecture is quite straightforward, simple feed-forward neural networks with linear layers. \nThis file contains all of the functions required for the agent to store experience, sample and learn from it, and select actions in the enviroment. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9502286686055287,
        0.9257475837510101
      ],
      "excerpt": "Details of the agent design can also be found in the Report.md, but a summary is provided here: \nThe algorithm used is based on the Deep Reinforcement Learning DDPG algorithm described in this paper: https://arxiv.org/pdf/1509.02971.pdf \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9213888113314533,
        0.9984194744726991,
        0.9930743068117677
      ],
      "excerpt": "Deep Reinforcement Learning is an innovative approach that effectively combines two seperate fields: \nIn Reinforcement learning, the goal is to have an agent learn how to navigate a new enviroment with the goal of maximising cummulative rewards. One approach to this end is Q-learning, where the agent tries to learn the dynamics of the enviroment indirectly by focusing on estimating the value of each state-action pair in the enviroment. This is acheived over the course of training, using it's experiences to produce and improve these estimates - as the agent encounters state-action pairs more often it becomes more confident in its estimate of their value. \nFamous in computer vision and natural language processing, deep learning uses machine learning to make predictions by leveraging vast amounts of training data and a flexible architecture that is able to generalise to previously unseen examples. In Deep Reinforcement Learning we leverage this power to learn which actions to take, and use the agents experiences within the enviroment as a reusable form of training data. This proves to be a powerful combination thanks to Deep learning's ability to generalise given sufficent data and flexibility. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9842348841515004,
        0.9546107412373906,
        0.9690549244230195,
        0.977865108221387,
        0.9833184443236701,
        0.8994744588010579
      ],
      "excerpt": "The Q network is designed to map state-action combinations to values. Thus we can feed it our current state and then determine the best action as the one that has the largest estimated state-action value. In practice we typically adopt a somewhat random action early on to encourage initial exporation. After we've collected enough state-action-reward-state experiences we start updating the model. This is acheived by sampling some of our experiences and then computing the empirically observed estimates of the state-action values compared to those estimated from the model. The difference between these two is coined the TD-error and we then make a small modification to the model weights to reduce this error, via neural network backpropagation of the TD-error. We simply iterate this process over many timesteps per episode, and many episodes, until convergence of the model weights is acheived. \nDeep Q-Learning is designed for environments with discreet action spaces, but struggles to generalise to continuous action spaces. This is because Q-Learning works by computing all possible state-actions values and then choosing the highest one. In continuous action spaces Q-value iteration is not possible and instead would require a continuous optimisation to select best actions. Actor-Critic methods rememdy this problem by using two networks instead of just the Q-network: \nThe Critic Network is a modified Q-network estimator that is designed to output the value for any given state-action value rather than iterating through all of them. As in Q-Learning, it uses the Bellman equations for learning the Q-values. \nThe Actor network attempts to estimate the optimal action directly given a state. It is learned by making use of the Critic Network as a baseline, and is used for selecting which actions to take. \nIn addition to using an actor-critic setup, the Deep Deterministic Policy Gradient algorithm (https://arxiv.org/pdf/1509.02971.pdf) additionally makes use of the success of Deep Q-networks to incorporate off-policy learning by use of a replay buffer and target networks for both the Critic and Actor which are updated periodically, and these modification enable more stable learning. \nAdditions that might improve the algorithm further are those of the D4PG algorithm, which inculdes prioritised experience replay, distributional value-learning, and n-step bootstrapping. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "This repo demonstrates the usage of an actor-critic setup via the deep-deterministic-policy-gradients algorithm. The environment to be solved is the Unity Reacher Environment provided in the Udacity Deep Reinforcement Learning nanodegree",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Remtasya/DDPG-Actor-Critic-Reinforcement-Learning-Reacher-Environment/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Wed, 29 Dec 2021 10:32:36 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Remtasya/DDPG-Actor-Critic-Reinforcement-Learning-Reacher-Environment/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Remtasya/DDPG-Actor-Critic-Reinforcement-Learning-Reacher-Environment",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Remtasya/DDPG-Actor-Critic-Reinforcement-Learning-Reacher-Environment/master/random_agent.ipynb",
      "https://raw.githubusercontent.com/Remtasya/DDPG-Actor-Critic-Reinforcement-Learning-Reacher-Environment/master/test_agent.ipynb",
      "https://raw.githubusercontent.com/Remtasya/DDPG-Actor-Critic-Reinforcement-Learning-Reacher-Environment/master/train_agent.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9560699572354264
      ],
      "excerpt": "<img src=\"https://github.com/Remtasya/DDPG-Actor-Critic-Reinforcement-Learning-Reacher-Environment/blob/master/project_images/reacher environment.gif\" alt=\"Environment\" width=\"700\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8007072958878088
      ],
      "excerpt": "As stated above train_agent.ipynb and test_agent.ipynb are intuitive files that are all that's required to walk you through training or testing this agent. If however you would like to change the code (such as to specify a different model architecture, or hyperparameter selection) then you may find the following descriptions useful: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8011994768851338
      ],
      "excerpt": "This file contains the trained weights of the most recently trained agent. You can use this file to straight away test an agent without having to train one yourself. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Remtasya/DDPG-Actor-Critic-Reinforcement-Learning-Reacher-Environment/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "DDPG Actor-Critic Reinforcement Learning Reacher Environment",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "DDPG-Actor-Critic-Reinforcement-Learning-Reacher-Environment",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Remtasya",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Remtasya/DDPG-Actor-Critic-Reinforcement-Learning-Reacher-Environment/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "In order to run this code you will require:\n\n1.  Python 3 with the packages in the following repository: https://github.com/udacity/deep-reinforcement-learning, including pytorch.\n\n2.  The ml-agents package, which can be the installed following the following instructions: https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md\n\n3.  The Reacher Unity environment specific to your operating system, which can be found here: https://github.com/udacity/deep-reinforcement-learning/tree/master/p2_continuous-control. After cloning this environment download the Reacher environment appropriate to your operating system, place the Reacher Folder with the root directory, and change it's path when loaded at the beginning of the notebooks.\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Wed, 29 Dec 2021 10:32:36 GMT"
    },
    "technique": "GitHub API"
  }
}