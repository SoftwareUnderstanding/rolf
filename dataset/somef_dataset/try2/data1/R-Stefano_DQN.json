{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1312.5602"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8761830525946069
      ],
      "excerpt": "arXiv by Deepmind \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/R-Stefano/DQN",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-10-27T10:54:55Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-18T07:24:08Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9593445337538805
      ],
      "excerpt": "Replicating Google Deepmind's paper \"Playing Atari with Deep Reinforcement Learning\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9362210646750732,
        0.8193628383873692,
        0.919858575715821
      ],
      "excerpt": "I implemented DQN on the games Pong and Breakout. I first used the hyperparameters given on the Nature paper but the agent was not able to learn any policy better than a random one.  \nThe agent was outputting the same q(s,a) for different states maybe due to neurons died problem. \nThis can happens when a big gradient value changes the weights linked to the neuron in such a way that the neuron will always output a very negative logit. So, even if the learning rate was given by the paper and the architecture is the same, could happen that a minimum difference in the settings of the architecture such as using a different weights initializer or a different frame preprocessing could make the learning rate 0.00025 not the optimal one.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9754805192907067
      ],
      "excerpt": "Moreover, I used Adam Optimizer instead of RMSProp Optimizer which experimentally has given me better results in a shorter period of time. My DQN required 700 episoded which are around 5 hours and 20 minutes to master Pong and more than 3000 episodes which are around 13 hours and 30 minutes to play decently Breakout. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Implementation of Deep Q-Network (DQN) on OpenAI games: Pong and Breakout using Tensorflow and Numpy",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/R-Stefano/DQN/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Tue, 28 Dec 2021 08:00:27 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/R-Stefano/DQN/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "R-Stefano/DQN",
    "technique": "GitHub API"
  },
  "invocation": [
    {
      "confidence": [
        0.8215604651798153
      ],
      "excerpt": "Initialize Replay Buffer: 10000 transactions \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8828665034782968
      ],
      "excerpt": "Final Epsilon: 0.01 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/R-Stefano/DQN/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Playing Atari with Deep Reinforcement Learning",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "DQN",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "R-Stefano",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/R-Stefano/DQN/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* Numpy\n* Tensorflow\n* Matplotlib\n* OpenAI Gym\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Tue, 28 Dec 2021 08:00:27 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "paper",
      "dqn",
      "deep-reinforcement-learning",
      "deepmind",
      "nature-paper",
      "breakout",
      "pong",
      "architecture",
      "gym-environment",
      "cnn"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The network architecture is in `DQN.py`. The class `replayMemory.py` stores and manages the transitions created during training. The file `main.py` is used to run the whole program callable using\n\n`python3 main.py`\n\nThe network is saved in the folder **myModel** while the tensorboard's file in **results**\n\n",
      "technique": "Header extraction"
    }
  ]
}