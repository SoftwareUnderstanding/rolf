{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Devlin et al. BERT: Pre-training of Deep Bidirectional Trasnsformers for Language Understanding (2018) [[paper]](https://arxiv.org/pdf/1810.04805.pdf)\n- google-research/bert [[github]](https://github.com/google-research/bert)\n- huggingface/pytorch-pretrained-BERT [[github]](https://github.com/huggingface/pytorch-pretrained-BERT)\n- NVIDIA/apex [[github]](https://github.com/NVIDIA/apex)\n- chakki-works/seqeval [[github]](https://github.com/chakki-works/seqeval)\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lemonhu/NER-BERT-pytorch",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-01-04T08:13:41Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-24T05:48:18Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9531524903106904
      ],
      "excerpt": "PyTorch solution of Named Entity Recognition task with Google AI's BERT model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9648538385548653,
        0.9410208779944643
      ],
      "excerpt": "Here, we take the Chinese NER data MSRA as an example. Of course, the English NER data is also fully applicable. \nNamed entity recognition task is one of the tasks of the Third SIGHAN Chinese Language Processing Bakeoff, we take the simplified Chinese version of the Microsoft NER dataset as the research object. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8599650577781451,
        0.9787623123673225
      ],
      "excerpt": "The dataset contains three types of entities: Person, Organization, Location and Other, the corresponding abbreviated tags are PER, ORG and LOC and O.  \nThe format is similar to that of the Co-NLL NER task 2002, adapted for Chinese. The data is presented in two-column format, where the first column consists of the character and the second is a tag. The tag is specified as follows: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9574700261299118
      ],
      "excerpt": "|   O   | Not part of a named entity                       | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8402085913971046,
        0.8386874171991067
      ],
      "excerpt": "| B-GPE | Beginning character of a geopolitical entity     | \n| I-GPE | Non-beginning character of a geopolitical entity | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.992428224666096
      ],
      "excerpt": "Based on the best performance of the model on the validation set, the overall effect of the model is as follows: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "PyTorch solution of named entity recognition task Using Google AI's pre-trained BERT model.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lemonhu/ner-bert-pytorch/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 80,
      "date": "Mon, 27 Dec 2021 07:52:54 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/lemonhu/NER-BERT-pytorch/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "lemonhu/NER-BERT-pytorch",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Based on the best model on the validation set, we can get the recognition effect of each entity type on the test set.\n\n| NE Types | Precison | Recall | F1_score |\n| :------: | :------: | :----: | :------: |\n|   PER    |  96.36   | 96.43  |  96.39   |\n|   ORG    |  89.64   | 92.07  |  90.84   |\n|   LOC    |  95.92   | 95.13  |  95.52   |\n\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.846429966952856
      ],
      "excerpt": "<div align=center><img src=\"./img/model.png\" width=\"500px\"></div> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.834376774111963
      ],
      "excerpt": "The NER dataset of MSRA consists of training set data/msra_train_bio and test set data/msra_test_bio, and no validation set is provided. There are 45000 training samples and 3442 test samples, and we will divide them appropriately later. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8152164435253624
      ],
      "excerpt": "|    test set    |  3442  | \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/lemonhu/NER-BERT-pytorch/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 \\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "NER-BERT-pytorch",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "NER-BERT-pytorch",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "lemonhu",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lemonhu/NER-BERT-pytorch/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This repo was tested on Python 3.5+ and PyTorch 0.4.1/1.0.0. The requirements are:\n\n- tensorflow >= 1.11.0\n- torch >= 0.4.1\n- pytorch-pretrained-bert == 0.4.0\n- tqdm\n- apex\n\n**Note**: The tensorflow library is only used for the conversion of pre-trained models from TensorFlow to PyTorch. apex is a tool for easy mixed precision and distributed training in Pytorch, please see https://github.com/NVIDIA/apex.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 303,
      "date": "Mon, 27 Dec 2021 07:52:54 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "ner",
      "named-entity-recognition",
      "entity-extraction",
      "chinese-ner",
      "google-bert",
      "transformer",
      "msra",
      "information-extraction",
      "pytorch"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. **Get BERT model for PyTorch**\n\n   There are two ways to get the pre-trained BERT model in a PyTorch dump for your experiments :\n\n   - **Direct download of the converted pytorch version of the BERT model**\n\n     You can download the pytorch dump I converted from the tensorflow checkpont from my Google Cloud Drive folder [`bert-base-chinese-pytorch`](https://drive.google.com/drive/folders/1K_xCYMCEfjpPjedSnMyL9zMVzqbanQX9), including the BERT parameters file `bert_config.json`, the model file `pytorch_model.bin` and the vocabulary file `vocab.txt`.\n\n   - **Convert the TensorFlow checkpoint to a PyTorch dump by yourself**\n\n     - Download the Google's BERT base model for Chinese from **[`BERT-Base, Chinese`](https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip)** (Chinese Simplified and Traditional, 12-layer, 768-hidden, 12-heads, 110M parameters), and decompress it.\n\n     - Execute the following command,  convert the TensorFlow checkpoint to a PyTorch dump.\n\n       ```shell\n       export TF_BERT_BASE_DIR=/path/to/chinese_L-12_H-768_A-12\n       export PT_BERT_BASE_DIR=/path/to/NER-BERT-pytorch/bert-base-chinese-pytorch\n       \n       pytorch_pretrained_bert convert_tf_checkpoint_to_pytorch \\\n       \t$TF_BERT_BASE_DIR/bert_model.ckpt \\\n       \t$TF_BERT_BASE_DIR/bert_config.json \\\n       \t$PT_BERT_BASE_DIR/pytorch_model.bin\n       ```\n\n     - Copy the BERT parameters file `bert_config.json` and dictionary file `vocab.txt` to the directory `$PT_BERT_BASE_DIR`.\n\n       ```shell\n       cp $TF_BERT_BASE_DIR/bert_config.json $PT_BERT_BASE_DIR/bert_config.json\n       cp $TF_BERT_BASE_DIR/vocab.txt $PT_BERT_BASE_DIR/vocab.txt\n       ```\n\n2. **Build dataset and tags**\n\n   ```shell\n   python build_msra_dataset_tags.py\n   ```\n\n   It will extract the sentences and tags from the dataset `data/msra_train_bio` and `data/msra_test_bio`, split them into train/val/test and save them in a convenient format for our model, and create a file `tags.txt` containing a collection of tags.\n\n3. **Set experimental hyperparameters**\n\n   We created a `base_model` directory for you under the `experiments` directory. It contains a file `params.json` which sets the hyperparameters for the experiment. It looks like\n\n   ```json\n   {\n       \"full_finetuning\": true,\n       \"max_len\": 180,\n   \n       \"learning_rate\": 3e-5,\n       \"weight_decay\": 0.01,\n       \"clip_grad\": 5,\n   }\n   ```\n\n   For every new experiment, you will need to create a new directory under `experiments` with a `params.json` file.\n\n4. **Train and evaluate your experiment**\n\n   if you use default parameters, just run\n\n   ```python\n   python train.py\n   ```\n\n   Or specify parameters on the command line\n\n   ```shell\n   python train.py --data_dir data/msra --bert_model_dir bert-base-chinese-pytorch --model_dir experiments/base_model --multi_gpu\n   ```\n\n   It will instantiate a model and train it on the training set following the hyperparameters specified in `params.json`. It will also evaluate some metrics on the development set.\n\n5. **Evaluation on the test set**\n\n   Once you've run many experiments and selected your best model and hyperparameters based on the performance on the development set, you can finally evaluate the performance of your model on the test set.\n\n   if you use default parameters, just run\n\n   ```shell\n   python evaluate.py\n   ```\n\n   Or specify parameters on the command line\n\n   ```shell\n   python evaluate.py --data_dir data/msra --bert_model_dir bert-base-chinese-pytorch --model_dir experiments/base_model\n   ```\n\n",
      "technique": "Header extraction"
    }
  ]
}