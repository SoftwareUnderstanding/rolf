{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* Dr. Gue Jun Jung, Speech Recognition Tech, SK Telecom\n* Dr. Woo-Jin Han, Netmarble IGS\n* Google Mentors\n* Tensorflow Korea\n* Google\n\nThis was supported by [Deep Learning Camp Jeju 2018](http://jeju.dlcamp.org/2018/) which was organized by [TensorFlow Korea User Group](https://facebook.com/groups/TensorFlowKR/).\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1809.10636",
      "https://arxiv.org/abs/1802.04208",
      "https://arxiv.org/abs/1710.11385",
      "https://arxiv.org/abs/1705.04058v6",
      "https://arxiv.org/abs/1609.03499",
      "https://arxiv.org/abs/1406.5298",
      "https://arxiv.org/abs/1609.03499",
      "https://arxiv.org/abs/1802.04208 (2018). [paper](https://arxiv.org/abs/1802.04208)\n* Shen, Jonathan, et al. \"Natural TTS synthesis by conditioning wavenet on mel spectrogram predictions.\" arXiv preprint https://arxiv.org/abs/1712.05884 (2017). [paper](https://arxiv.org/pdf/1712.05884.pdf)\n* Perez, Anthony, Chris Proctor, and Archa Jain. Style transfer for prosodic speech. Tech. Rep., Stanford University, 2017. [paper](http://web.stanford.edu/class/cs224s/reports/Anthony_Perez.pdf)\n* Goodfellow, Ian, et al. \"Generative adversarial nets.\" Advances in neural information processing systems. 2014. [paper](https://arxiv.org/pdf/1406.2661.pdf)\n* Salimans, Tim, et al. \"Improved techniques for training gans.\" Advances in Neural Information Processing Systems. 2016. [paper](https://arxiv.org/pdf/1606.03498.pdf)\n* Grinstein, Eric, et al. \"Audio style transfer.\" arXiv preprint https://arxiv.org/abs/1710.11385 (2017). [paper](https://arxiv.org/abs/1710.11385)\n* Pascual, Santiago, Antonio Bonafonte, and Joan Serra. \"SEGAN: Speech enhancement generative adversarial network.\" arXiv preprint https://arxiv.org/abs/1703.09452 (2017). [paper](https://arxiv.org/pdf/1703.09452.pdf)\n* Yongcheng Jing, Yezhou Yang, Zunlei Feng, Jingwen Ye, Yizhou Yu, Mingli Song. \"Neural Style Transfer: A Review\" \thttps://arxiv.org/abs/1705.04058 (2017) [paper](https://arxiv.org/abs/1705.04058v6)\n* Van Den Oord, A\u00e4ron, et al. \"Wavenet: A generative model for raw audio.\" CoRR abs/1609.03499 (2016). [paper](https://arxiv.org/abs/1609.03499)\n* Glow: Generative Flow with Invertible 1\u00d71 Convolutions [paper](https://d4mucfpksywv.cloudfront.net/research-covers/glow/paper/glow.pdf)\n* Kingma, Diederik P., et al. \"Semi-supervised learning with deep generative models.\" Advances in Neural Information Processing Systems. 2014. [paper](https://arxiv.org/abs/1406.5298)\n* Van Den Oord, A\u00e4ron, et al. \"Wavenet: A generative model for raw audio.\" CoRR abs/1609.03499 (2016). [paper](https://arxiv.org/abs/1609.03499)\n\n\n## Authors\n\n* **Anoop Toffy** - *IIIT Bangalore* - [Personal Website](https://www.anooptoffy.com)\n* **Chae Young Lee** - *Hankuk Academy of Foreign Studies* - [Homepage](https://github.com/acheketa)\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details\n\n## Credits\n\n* We used our baseline mode from waveGAN paper by Chris Donahue et al. (2018)\n\n```\n@article{donahue2018synthesizing,\n  title={Synthesizing Audio with Generative Adversarial Networks",
      "https://arxiv.org/abs/1712.05884 (2017). [paper](https://arxiv.org/pdf/1712.05884.pdf)\n* Perez, Anthony, Chris Proctor, and Archa Jain. Style transfer for prosodic speech. Tech. Rep., Stanford University, 2017. [paper](http://web.stanford.edu/class/cs224s/reports/Anthony_Perez.pdf)\n* Goodfellow, Ian, et al. \"Generative adversarial nets.\" Advances in neural information processing systems. 2014. [paper](https://arxiv.org/pdf/1406.2661.pdf)\n* Salimans, Tim, et al. \"Improved techniques for training gans.\" Advances in Neural Information Processing Systems. 2016. [paper](https://arxiv.org/pdf/1606.03498.pdf)\n* Grinstein, Eric, et al. \"Audio style transfer.\" arXiv preprint https://arxiv.org/abs/1710.11385 (2017). [paper](https://arxiv.org/abs/1710.11385)\n* Pascual, Santiago, Antonio Bonafonte, and Joan Serra. \"SEGAN: Speech enhancement generative adversarial network.\" arXiv preprint https://arxiv.org/abs/1703.09452 (2017). [paper](https://arxiv.org/pdf/1703.09452.pdf)\n* Yongcheng Jing, Yezhou Yang, Zunlei Feng, Jingwen Ye, Yizhou Yu, Mingli Song. \"Neural Style Transfer: A Review\" \thttps://arxiv.org/abs/1705.04058 (2017) [paper](https://arxiv.org/abs/1705.04058v6)\n* Van Den Oord, A\u00e4ron, et al. \"Wavenet: A generative model for raw audio.\" CoRR abs/1609.03499 (2016). [paper](https://arxiv.org/abs/1609.03499)\n* Glow: Generative Flow with Invertible 1\u00d71 Convolutions [paper](https://d4mucfpksywv.cloudfront.net/research-covers/glow/paper/glow.pdf)\n* Kingma, Diederik P., et al. \"Semi-supervised learning with deep generative models.\" Advances in Neural Information Processing Systems. 2014. [paper](https://arxiv.org/abs/1406.5298)\n* Van Den Oord, A\u00e4ron, et al. \"Wavenet: A generative model for raw audio.\" CoRR abs/1609.03499 (2016). [paper](https://arxiv.org/abs/1609.03499)\n\n\n## Authors\n\n* **Anoop Toffy** - *IIIT Bangalore* - [Personal Website](https://www.anooptoffy.com)\n* **Chae Young Lee** - *Hankuk Academy of Foreign Studies* - [Homepage](https://github.com/acheketa)\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details\n\n## Credits\n\n* We used our baseline mode from waveGAN paper by Chris Donahue et al. (2018)\n\n```\n@article{donahue2018synthesizing,\n  title={Synthesizing Audio with Generative Adversarial Networks",
      "https://arxiv.org/abs/1710.11385 (2017). [paper](https://arxiv.org/abs/1710.11385)\n* Pascual, Santiago, Antonio Bonafonte, and Joan Serra. \"SEGAN: Speech enhancement generative adversarial network.\" arXiv preprint https://arxiv.org/abs/1703.09452 (2017). [paper](https://arxiv.org/pdf/1703.09452.pdf)\n* Yongcheng Jing, Yezhou Yang, Zunlei Feng, Jingwen Ye, Yizhou Yu, Mingli Song. \"Neural Style Transfer: A Review\" \thttps://arxiv.org/abs/1705.04058 (2017) [paper](https://arxiv.org/abs/1705.04058v6)\n* Van Den Oord, A\u00e4ron, et al. \"Wavenet: A generative model for raw audio.\" CoRR abs/1609.03499 (2016). [paper](https://arxiv.org/abs/1609.03499)\n* Glow: Generative Flow with Invertible 1\u00d71 Convolutions [paper](https://d4mucfpksywv.cloudfront.net/research-covers/glow/paper/glow.pdf)\n* Kingma, Diederik P., et al. \"Semi-supervised learning with deep generative models.\" Advances in Neural Information Processing Systems. 2014. [paper](https://arxiv.org/abs/1406.5298)\n* Van Den Oord, A\u00e4ron, et al. \"Wavenet: A generative model for raw audio.\" CoRR abs/1609.03499 (2016). [paper](https://arxiv.org/abs/1609.03499)\n\n\n## Authors\n\n* **Anoop Toffy** - *IIIT Bangalore* - [Personal Website](https://www.anooptoffy.com)\n* **Chae Young Lee** - *Hankuk Academy of Foreign Studies* - [Homepage](https://github.com/acheketa)\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details\n\n## Credits\n\n* We used our baseline mode from waveGAN paper by Chris Donahue et al. (2018)\n\n```\n@article{donahue2018synthesizing,\n  title={Synthesizing Audio with Generative Adversarial Networks",
      "https://arxiv.org/abs/1703.09452 (2017). [paper](https://arxiv.org/pdf/1703.09452.pdf)\n* Yongcheng Jing, Yezhou Yang, Zunlei Feng, Jingwen Ye, Yizhou Yu, Mingli Song. \"Neural Style Transfer: A Review\" \thttps://arxiv.org/abs/1705.04058 (2017) [paper](https://arxiv.org/abs/1705.04058v6)\n* Van Den Oord, A\u00e4ron, et al. \"Wavenet: A generative model for raw audio.\" CoRR abs/1609.03499 (2016). [paper](https://arxiv.org/abs/1609.03499)\n* Glow: Generative Flow with Invertible 1\u00d71 Convolutions [paper](https://d4mucfpksywv.cloudfront.net/research-covers/glow/paper/glow.pdf)\n* Kingma, Diederik P., et al. \"Semi-supervised learning with deep generative models.\" Advances in Neural Information Processing Systems. 2014. [paper](https://arxiv.org/abs/1406.5298)\n* Van Den Oord, A\u00e4ron, et al. \"Wavenet: A generative model for raw audio.\" CoRR abs/1609.03499 (2016). [paper](https://arxiv.org/abs/1609.03499)\n\n\n## Authors\n\n* **Anoop Toffy** - *IIIT Bangalore* - [Personal Website](https://www.anooptoffy.com)\n* **Chae Young Lee** - *Hankuk Academy of Foreign Studies* - [Homepage](https://github.com/acheketa)\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details\n\n## Credits\n\n* We used our baseline mode from waveGAN paper by Chris Donahue et al. (2018)\n\n```\n@article{donahue2018synthesizing,\n  title={Synthesizing Audio with Generative Adversarial Networks",
      "https://arxiv.org/abs/1705.04058 (2017) [paper](https://arxiv.org/abs/1705.04058v6)\n* Van Den Oord, A\u00e4ron, et al. \"Wavenet: A generative model for raw audio.\" CoRR abs/1609.03499 (2016). [paper](https://arxiv.org/abs/1609.03499)\n* Glow: Generative Flow with Invertible 1\u00d71 Convolutions [paper](https://d4mucfpksywv.cloudfront.net/research-covers/glow/paper/glow.pdf)\n* Kingma, Diederik P., et al. \"Semi-supervised learning with deep generative models.\" Advances in Neural Information Processing Systems. 2014. [paper](https://arxiv.org/abs/1406.5298)\n* Van Den Oord, A\u00e4ron, et al. \"Wavenet: A generative model for raw audio.\" CoRR abs/1609.03499 (2016). [paper](https://arxiv.org/abs/1609.03499)\n\n\n## Authors\n\n* **Anoop Toffy** - *IIIT Bangalore* - [Personal Website](https://www.anooptoffy.com)\n* **Chae Young Lee** - *Hankuk Academy of Foreign Studies* - [Homepage](https://github.com/acheketa)\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details\n\n## Credits\n\n* We used our baseline mode from waveGAN paper by Chris Donahue et al. (2018)\n\n```\n@article{donahue2018synthesizing,\n  title={Synthesizing Audio with Generative Adversarial Networks",
      "https://arxiv.org/abs/1802.04208"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* We used our baseline mode from waveGAN paper by Chris Donahue et al. (2018)\n\n```\n@article{donahue2018synthesizing,\n  title={Synthesizing Audio with Generative Adversarial Networks},\n  author={Donahue, Chris and McAuley, Julian and Puckette, Miller},\n  journal={arXiv preprint arXiv:1802.04208},\n  year={2018}\n}\n```\n\n* TPU Implementations are based on the [DCGAN](https://github.com/tensorflow/tpu/tree/master/models/experimental/dcgan) implemenatation released by Tensorflow Hub. [link](https://github.com/tensorflow/tpu)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "* Donahue, Chris, Julian McAuley, and Miller Puckette. \"Synthesizing Audio with Generative Adversarial Networks.\" arXiv preprint arXiv:1802.04208 (2018). [paper](https://arxiv.org/abs/1802.04208)\n* Shen, Jonathan, et al. \"Natural TTS synthesis by conditioning wavenet on mel spectrogram predictions.\" arXiv preprint arXiv:1712.05884 (2017). [paper](https://arxiv.org/pdf/1712.05884.pdf)\n* Perez, Anthony, Chris Proctor, and Archa Jain. Style transfer for prosodic speech. Tech. Rep., Stanford University, 2017. [paper](http://web.stanford.edu/class/cs224s/reports/Anthony_Perez.pdf)\n* Goodfellow, Ian, et al. \"Generative adversarial nets.\" Advances in neural information processing systems. 2014. [paper](https://arxiv.org/pdf/1406.2661.pdf)\n* Salimans, Tim, et al. \"Improved techniques for training gans.\" Advances in Neural Information Processing Systems. 2016. [paper](https://arxiv.org/pdf/1606.03498.pdf)\n* Grinstein, Eric, et al. \"Audio style transfer.\" arXiv preprint arXiv:1710.11385 (2017). [paper](https://arxiv.org/abs/1710.11385)\n* Pascual, Santiago, Antonio Bonafonte, and Joan Serra. \"SEGAN: Speech enhancement generative adversarial network.\" arXiv preprint arXiv:1703.09452 (2017). [paper](https://arxiv.org/pdf/1703.09452.pdf)\n* Yongcheng Jing, Yezhou Yang, Zunlei Feng, Jingwen Ye, Yizhou Yu, Mingli Song. \"Neural Style Transfer: A Review\" \tarXiv:1705.04058 (2017) [paper](https://arxiv.org/abs/1705.04058v6)\n* Van Den Oord, A\u00e4ron, et al. \"Wavenet: A generative model for raw audio.\" CoRR abs/1609.03499 (2016). [paper](https://arxiv.org/abs/1609.03499)\n* Glow: Generative Flow with Invertible 1\u00d71 Convolutions [paper](https://d4mucfpksywv.cloudfront.net/research-covers/glow/paper/glow.pdf)\n* Kingma, Diederik P., et al. \"Semi-supervised learning with deep generative models.\" Advances in Neural Information Processing Systems. 2014. [paper](https://arxiv.org/abs/1406.5298)\n* Van Den Oord, A\u00e4ron, et al. \"Wavenet: A generative model for raw audio.\" CoRR abs/1609.03499 (2016). [paper](https://arxiv.org/abs/1609.03499)\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{donahue2018synthesizing,\n  title={Synthesizing Audio with Generative Adversarial Networks},\n  author={Donahue, Chris and McAuley, Julian and Puckette, Miller},\n  journal={arXiv preprint arXiv:1802.04208},\n  year={2018}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/chaeyoung-lee/cwavegan",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-07-25T13:39:03Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-08T14:45:19Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9662398014195178
      ],
      "excerpt": "Official implementation of CWaveGAN | paper | slides \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.856524759092126,
        0.9049262909817788
      ],
      "excerpt": "In this paper, we developed Conditional WaveGAN to synthesize speech / audio samples that are conditioned on class labels. The thus synthesized raw audio is used for improving the baseline ASR system. \nGenerative models are successfully used for image synthesis in the recent years. But when it comes to other modalities like audio, text, and etc, little progress has been made. Recent works focus on generating audio from a generative model in an unsupervised setting. We explore the possibility of using generative models conditioned on class labels. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9653795202744395
      ],
      "excerpt": "Data must assume the form of tf.Data.TFRecord. The label data must be in one hot encoded for concatenation based conditioning, whereas it must be simple integers for bias based conditioning. Thus, the code to make the TFRecord differs by the type of conditioning. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "    --data_dir ./data/customdataset \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Conditional WaveGAN: Generating audio samples conditioned on class labels",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/acheketa/cwavegan/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 19,
      "date": "Tue, 28 Dec 2021 17:39:08 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/chaeyoung-lee/cwavegan/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "chaeyoung-lee/cwavegan",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/acheketa/cwavegan/master/examples/samples.ipynb",
      "https://raw.githubusercontent.com/acheketa/cwavegan/master/examples/.ipynb_checkpoints/samples-checkpoint.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/acheketa/cwavegan/master/dependencies/install_vm.sh",
      "https://raw.githubusercontent.com/acheketa/cwavegan/master/data/make_tfrecord.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9040184827228761
      ],
      "excerpt": "    --name train --labels \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9542230725140624
      ],
      "excerpt": "python gpu/train_wavegan.py train ./gpu/train \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9622908520427337
      ],
      "excerpt": "python gpu/backup.py ./gpu/train 60 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9622908520427337
      ],
      "excerpt": "python gpu/train_wavegan.py preview ./gpu/train \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.902359343077145,
        0.902359343077145,
        0.8711951672720759
      ],
      "excerpt": "<img src=\"examples/concat.jpeg\"/> \n<img src=\"examples/bias.jpeg\"/> \n<img src=\"examples/generation.jpeg\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991,
        0.8589534893990137
      ],
      "excerpt": "python3 make_tfrecord_int.py \\ \n    ../sc09/train \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.83323858688002
      ],
      "excerpt": "    --name train --labels \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9285790912751759
      ],
      "excerpt": "python gpu/train_wavegan.py train ./gpu/train \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8848019944995117
      ],
      "excerpt": "python gpu/backup.py ./gpu/train 60 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9199730593696799
      ],
      "excerpt": "python gpu/train_wavegan.py preview ./gpu/train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python tpu/tpu_main.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8487432749701096,
        0.9246227682586091
      ],
      "excerpt": "To generate 20 preview audio samples with two per class \npython tpu/preview.py \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/chaeyoung-lee/cwavegan/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2018 Anoop Toffy, Chae Young Lee\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Conditional WaveGAN",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "cwavegan",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "chaeyoung-lee",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/chaeyoung-lee/cwavegan/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* Tensorflow 1.x.x\n* Python 2.x, 3.x\n* tqdm\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 121,
      "date": "Tue, 28 Dec 2021 17:39:08 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Training can be done in both GPU and TPU settings. Both versions of code implement bias scaling method.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "https://colab.research.google.com/drive/1VRyNJQBgiFF-Gi9qlZkOhiBE-KkUaHjw\n\n",
      "technique": "Header extraction"
    }
  ]
}