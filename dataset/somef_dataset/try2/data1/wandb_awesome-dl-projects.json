{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2003.00152 by Frankle et al. | [Sayak Paul](https://app.wandb.ai/sayakpaul",
      "https://arxiv.org/abs/2003.08934"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.971679416046096
      ],
      "excerpt": "| Report  |      Description      |      Author      | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.920917280619799
      ],
      "excerpt": "| DeOldify  | Understand how the DeOldify model works, reproduce and visualize the results obtained by the author. | Boris Dayma | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8688128680014449
      ],
      "excerpt": "| Bounding Boxes for Object Detection | How to log and explore bounding boxes | Stacey Svetlichnaya | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9427537933810815
      ],
      "excerpt": "| Video to 3D: Depth Perception for Self-Driving Cars | Unsupervised learning of depth perception from dashboard cameras | Stacey Svetlichnaya | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9688293020709013
      ],
      "excerpt": "| Curriculum Learning in Nature | Applying human learning strategies to neural nets on iNaturalist 2017 | Stacey Svetlichnaya | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/wandb/awesome-dl-projects",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "We welcome community collaborations :tada:\nThis GitHub repository features curated machine learning reports by researchers exploring deep learning techniques, Kagglers showcasing winning models, and industry leaders sharing best practices. You can also check them out in our Gallery. \nWhat are we looking for? :eyes:\nWe are looking for interesting colab notebooks(Jupyter notebooks) covering intermediate to advance level ML/DL techniques, best practices, a minimal implementation of research papers, benchmark on a dataset, etc.\nThe colab notebook should come with Weights and Biases integration. To get started with W&B:\n\n:snowflake: Check out our examples repo. \n:zap: Check out our official documentation. \n:raised_hands: (optional) If you want to create a report to document your experiment and observations check our reports video.\n\nHow can you contribute? :relaxed:\nIf you are working on an exciting ML/DL project, we would love to feature your work in this repo. The rules are simple:\n\nCreate a new issue with a relevant title. Briefly describe the project you are working on in a meaningful manner. Provide links wherever necessary.\nWe will review the issue and approve the idea if it meets the intermediate or advanced level mark. \nFork this repo.\nCreate a PR with a notebook. We have provided a template for the PR that will be auto-generated. \n\nWhat more we have to offer? :star2:\nIf we like your notebook, we will reach out to you for our Authors Program. \nIf you are passionate to showcase your machine learning research in a transparent and collaborative environment, you can even reach out to us using this typeform.",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-06-24T19:18:01Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-18T07:40:58Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9244372017676508,
        0.9893896985405376,
        0.9815272415522952
      ],
      "excerpt": "The Gallery by Weights & Biases features curated machine learning reports by researchers exploring deep learning techniques, Kagglers showcasing winning models, and industry leaders sharing best practices. \n:collision: This hacktoberfest we are open for contributions. Check out our CONTRIBUTING.md to learn more. :collision: \nawesome-dl-projects by wandb is a collection of the code that accompanies the reports. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9959821262307156
      ],
      "excerpt": "| Survival of the Fittest CNN Model  | Optimize the hyperparameters of a CNN with Variable Length Genetic Algorithm. | Aritra Roy Gosthipaty | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9852129597458454,
        0.8536731538140583,
        0.8112304128156614,
        0.8112304128156614,
        0.9557613557200948,
        0.8382268940315487,
        0.8796728901854015
      ],
      "excerpt": "| Under the hood of LSTMs  | A NumPy implementation of LSTMs. Difference of architectures and the pros of LSTMs have been discussed. | Aritra Roy Gosthipaty | \n| Get Started with TensorFlow Lite Examples Using Android Studio  | A step-by-step guide towards running example apps on your phone using Android Studio, TensorFlow Lite, and USB debugging. | Ivan Goncharov | \n| Part 2: Deep Representations, a way towards neural style transfer  | A top down approach to conceiving neural style transfer. | Aritra Roy Gosthipaty and Devjyoti Chakraborty | \n| Part 1: Deep Representations, a way towards neural style transfer  | A top down approach to conceiving neural style transfer. | Aritra Roy Gosthipaty and Devjyoti Chakraborty | \n| Hyperparameter Optimization for Huggingface Transformers  | The report explains three strategies for hyperparameter optimization for Huggingface Transformers.| Ayush Chaurasia | \n| Ray Tune: Distributed Hyperparameter Optimization at Scale  | How to use Ray Tune with W&B to run an effective distributed hyperparameter optimization pipeline at scale. | Ayush Chaurasia and Lavanya Shukla | \n| Modern Scalable Hyperparameter Tuning Methods | A comparison of Random search, Bayesian search using HyperOpt, Bayesian search combined with Asynchronous Hyperband and Population Based Training. | Ayush Chaurasia |  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9451754765927316,
        0.8963227498384457,
        0.949844219565898,
        0.9821235551323366,
        0.9332943192607428,
        0.9598723847591499,
        0.951194050444374,
        0.9612372347313963,
        0.9384777758697197,
        0.9290213908140513,
        0.9185515103281933
      ],
      "excerpt": "| Distilling Knowledge in Neural Networks  | This report discusses the compelling model optimization technique - knowledge distillation with code walkthroughs in TensorFlow. | Sayak Paul | \n| Using SimpleTransformers for Common NLP Applications  | Explore Language Modeling, Named Entity Recognition, Question Answering with the SimpleTransformer library. | Ayush Chaurasia | \n| SimpleTransformers: Transformers Made Easy   | Simple Transformers removes complexity and lets you get down to what matters \u2013 model training and experimenting with the Transformer model architectures. | Ayush Thakur | \n| Sentence Classification With Huggingface BERT and W&B  | In this tutorial, we\u2019ll build a near state of the art sentence classifier leveraging the power of recent breakthroughs in the field of Natural Language Processing. | Ayush Chaurasia | \n| 3D Image Inpainting  | A novel way to convert a single RGB-D image into a 3D image. | Ayush Thakur | \n| Unsupervised Visual Representation Learning with SwAV  | In this report, we explore the SwAV framework, as presented in the paper \"Unsupervised Learning of Visual Features by Contrasting Cluster Assignments\" by Caron et al. SwAV is currently the SoTA in self-supervised learning for visual recognition. We also address common problems in existing self-supervised methods. | Ayush Thakur and Sayak Paul | \n| DeepFaceDrawing: An Overview  | This is an overview report to break down the key components of DeepFaceDrawing | Ayush Thakur | \n| Tips for Kagglers From Winning a Solo Silver Medal | How I got started with the competition, my struggles, journey to the final solution and everything I learned in between. | Aman Dalmia | \n| Part 2 \u2013 Comparing Message Passing Based GNN Architectures  | In this report, we shall review various message-passing based GNN architectures and compare them using Sweeps by Weights and Biases. | Yash Kotadia | \n| Part 1 \u2013 Introduction to Graph Neural Networks with GatedGCN  | This report summarizes the need for Graph Neural Networks and analyzes one particular architecture \u2013 the Gated Graph Convolutional Network. | Yash Kotadia | \n| An Introduction to Adversarial Examples in Deep Learning  | This report provides an intuitive introduction to adversarial examples, discusses a wide variety of different adversarial attacks and, most notably, provides advice on defending against them. | Sayak Paul | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9700480003231592
      ],
      "excerpt": "| Visualize Scikit Models  | Visualize your scikit-learn model's performance with just a few lines of code | Lavanya Shukla | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9216424317573381,
        0.9090201590627133,
        0.8596746717649721,
        0.9847158123962363,
        0.9777019348740974,
        0.9489517745751443,
        0.9511017809852521,
        0.9722299724538402
      ],
      "excerpt": "| HuggingTweets - Train a model to generate tweets  | In this project, we fine-tune a pre-trained transformer on tweets using HuggingFace Transformers \u2013 a popular library with pre-trained architectures and frameworks for NLP. | Boris Dayma |  \n| Organize Your Machine Learning Pipelines with Artifacts  | In this report, we will show you how to use W&B Artifacts to store and keep track of datasets, models, and evaluation results across machine learning pipelines. | Ayush Chaurasia | \n| Weights & Biases and Ray Tune: Distributed hyperparameter optimization at scale  | How to use Ray Tune with W&B to run an effective distributed hyperparameter optimization pipeline at scale. | Ayush Chaurasia and Lavanya Shukla \n| Towards Representation Learning for an Image Retrieval Task  | This report explains self-supervised and regularized supervised image retrieval with the help of the latent space of an autoencoder. | Aritra Roy Gosthipaty and Souradip Chakraborty \n| Build the World's Open Hedge Fund by Modeling the Stock Market  | In this report, we show you how to get started with Numerai, a crowdsourced AI hedge fund and compete on the hardest data science tournament on the planet using Weights & Biases. | Carlo Lepelaars | \n| Understanding the Effectivity of Ensembles in Deep Learning  | The report explores the ideas presented in Deep Ensembles: A Loss Landscape Perspective by Stanislav Fort, Huiyi Hu, and Balaji Lakshminarayanan. | Ayush Thakur and Sayak Paul| \n| Plunging into Model Pruning in Deep Learning | This report discusses pruning techniques in the context of deep learning.   |Sayak Paul | \n| Visualizing Confusion Matrices with W&B  | Using Keras with Weights & Biases, plot a confusion matrix at every step of model training and see where your algorithm is wrong. | Math\u00efs F\u00e9d\u00e9rico | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9486364703390434,
        0.9793041319519232
      ],
      "excerpt": "| The Power of Random Features of a CNN             |This report presents a number of experiments based on the ideas shown in https://arxiv.org/abs/2003.00152 by Frankle et al. | Sayak Paul | \n| The Al-Dente Neural Network: Part I                 | Much like making pasta, training a neural network is easy to learn but takes a lifetime to master. What follows is probably the best recipe to make your own Al-Dente Neural Net, courtesy of Andrej Karpathy. | Sairam Sundaresan | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.887198371911217,
        0.8671858079965327,
        0.8809301545462761,
        0.9875776529066826
      ],
      "excerpt": "| Generating Digital Painting Lighting Effects via RGB-space Geometry  | Exploring the paper \"Generating Digital Painting Lighting Effects via RGB-space Geometry\" in which the authors propose an image processing algorithm to generate digital painting lighting effects from a single image. | Ayush Thakur | \n| Two Shots to Green Screen: Collage with Deep Learning | Train a deep net to extract foreground and background in natural images and videos | Stacey Svetlichnaya| \n| A Step by Step Guide to Tracking Hugging Face Model Performance  | A quick tutorial for training NLP models with HuggingFace and & visualizing their performance with Weights & Biases | Jack Morris | \n| A Tale of Model Quantization in TF Lite  | Model optimization strategies and quantization techniques to help deploy machine learning models in resource constrained environments. | Sayak Paul | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9676561275701675
      ],
      "excerpt": "| Who is Them? Text Disambiguation with Transformers  |Using HuggingFace to explore models for natural language understanding | Stacey Svetlichnaya | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9515993666581213,
        0.8887922872482467
      ],
      "excerpt": "| Interpretability in Deep Learning with W&B - CAM and GradCAM   | This report will review how Grad-CAM counters the common criticism that neural networks are not interpretable. We'll review feature visualization, class activation maps and implement a custom callback that you can use in your own projects.  | Ayush Thakur| \n| Adversarial Policies in Multi-Agent Settings  | One way to win is not to play the game | Stacey Svetlichnaya| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9666301943303386,
        0.8713902047578646,
        0.9839163219282213,
        0.9949838014520856
      ],
      "excerpt": "| Deep Q Networks with the Cartpole Environment | A brief explanation of the DQN algorithm for reinforcement learning, focusing on the Cartpole-v1 environment from OpenAI gym. | Jari| \n| Using simpleTransformer on common NLP applications  | Explore Language Modeling, Named Entity Recognition, Question Answering with distilbert from the simpleTransformer library. | Ayush Chaurasia| \n| Transfer Learning with EfficientNet family of models  | Learn to use the EfficientNet family of models for transfer learning in TensorFlow using TFHub. | Sayak Paul | \n| Automate Kaggle model training with Skorch and W&B    | Skorch combines the simplicity of scikit, with the power of pytorch and makes for a great framework to use in Kaggle competitions | Ayush Chaurasia | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9083843841347383,
        0.9787598766768097
      ],
      "excerpt": "| When Inception-ResNet-V2 is too slow | Some versions of Inception parallelize better than others | Stacey Svetlichnaya | \n| Using W&B in a Kaggle Competition   | In this tutorial, we\u2019ll see how you can use W&B in a Kaggle  competition. We'll also see how W&B's Scikit-learn integration  enables you to visualize performance metrics for your model with a  single line of code. Finally, we'll run a hyperparameter sweep to pick  the best model. | Ayush Chaurasia | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8646745456627349,
        0.8713434334614233
      ],
      "excerpt": "| COVID-19 research using PyTorch and W&B  | How to train T5 on SQUAD with Transformers and Nlp | Ayush Chaurasia | \n| Video to 3D: Depth Perception for Self-Driving Cars | Unsupervised learning of depth perception from dashboard cameras | Stacey Svetlichnaya | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8939776460728429,
        0.9878117838677576,
        0.9623733138144365
      ],
      "excerpt": "| Sentence classification with Huggingface BERT and W&B   | How to train T5 on SQUAD with Transformers and Nlp | Ayush Chaurasia| \n| Visualizing and Debugging Neural Networks with PyTorch and W&B  | In this post, we\u2019ll see what makes a neural network underperform and  ways we can debug this by visualizing the gradients and other parameters associated with model training. We\u2019ll also discuss the problem of  vanishing and exploding gradients and methods to overcome them. | Ayush Thakur | \n| Track Model Performance  | In this report, I'll show you show you can visualize any model's performance with Weights & Biases. We'll see how to log metrics from vanilla for loops, boosting models (xgboost & lightgbm), sklearn and neural networks. | Lavanya Shukla| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8950039381038278,
        0.976249409371638,
        0.924450587043382,
        0.8011998940053009,
        0.956245609506283,
        0.9116442250176954,
        0.8070758955721374,
        0.9475641830569705,
        0.8411965226840672,
        0.9087533944654859
      ],
      "excerpt": "| Visualize models in TensorBoard with Weights and Biases  | In this article, we are going see how to spin up and host a TensorBoard instance online with Weights and Biases. We'll end with visualizing a confusion matrix in TensorBoard. | Lavanya Shukla | \n| Visualizing and Debugging Neural Networks with PyTorch and W&B |In this post I'll show you how to use wandb.Molecule, to visualize molecular data with Weights and Biases. | Nicholas Bardy | \n| Visualize Model Predictions | In this report, I'll show you how to visualize a model's predictions with Weights & Biases \u2013 images, videos, audio, tables, HTML, metrics, plots, 3d objects and point clouds. | Lavanya Shukla | \n| Use Pytorch Lightning with Weights & Biases  | PyTorch Lightning lets you decouple the science code from engineering code. Try this quick tutorial to visualize Lightning models and optimize hyperparameters with an easy Weights & Biases integration. | Ayush Chaurasia | \n| Evaluating the Impact of Sequence Convolutions and Embeddings on Protein Structure Prediction | A vignette on recent work using deep learning for protein structure prediction. March 26, 2020.| Jonathan King | \n| Towards Deep Generative Modeling with W&B   | In this report, we will learn about the evolution of generative modeling. We'll start with Autoencoders and Variational Autoencoders and then dive into Generative Adversarial Modeling.| Ayush Thakur | \n| Exploring ResNets With W&B   | Post by Ayush Chaurasia | Lavanya Shukla| \n| NeRF \u2013 Representing Scenes as Neural Radiance Fields for View Synthesis  | In the Representing Scenes as Neural Radiance Fields for View Synthesis paper, the authors present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying  continuous volumetric scene function using a sparse set of input views. | Lavanya Shukla | \n| How Efficient is EfficientNet? | Evaluating the EfficientNet Family on a Smaller ImageNet-like Dataset | Ajay Arasanipalai  | \n| Distributed training in tf.keras with W&B  | Explore the ways to distribute your training workloads with minimal code changes and analyze system metrics with Weights and Biases. | Sayak Paul  | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9600293628306,
        0.987269901902221
      ],
      "excerpt": "| Effects of Weight Initialization on Neural Networks  | In this article, we\u2019ll review and compare a plethora of weight  initialization methods for neural nets. We will also outline a simple  recipe for initializing the weights in a neural net. | Sayak Paul | \n| An Introduction to Image Inpainting using Deep Learning  | In this report, we are going to learn how to do \u201cimage inpainting\u201d, i.e. fill in missing parts of images precisely using deep learning. | Ayush Thakur and Sayak Paul | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8957057656608621,
        0.9279372486871467,
        0.8002861534346415
      ],
      "excerpt": "| Curriculum Learning in Nature | Applying human learning strategies to neural nets on iNaturalist 2017 | Stacey Svetlichnaya | \n| Fashion MNIST | Explore various hyperparameters of a CNN trained on Fashion MNIST to identify 10 types of clothing| Stacey Svetlichnaya | \n| Classify the Natural World | Training and fine-tuning convolutional networks to identify species beyond ImageNet | Stacey Svetlichnaya | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8650688872289026
      ],
      "excerpt": "| Text Generation with Sequence Models | Explore network architectures and training settings for character-based text generation. Compare RNNs, GRUs, and LSTMS, with different depths, layer widths, and dropout. Also consider the training data length, sequence length, and number of sequences per batch. | Carey | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9701332064206225,
        0.8952625828627804
      ],
      "excerpt": "| FastText Nanobot for the Transformer Age | Integrate FastText with W&B to visualize incredibly efficient natural language processing| Stacey Svetlichnaya | \n| Dropout in PyTorch \u2013 An Example  | Regularize your PyTorch model with Dropout| Ayush Thakur | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9218146120973694
      ],
      "excerpt": "| Generate Meaningful Captions for Images with Attention Models | Image captioning has many use cases that include generating captions for  Google image search and live video surveillance as well as helping  visually impaired people to get information about their surroundings. | Rajesh Shreedhar Bhat and Souradip Chakraborty | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "This is a collection of the code that accompanies the reports in The Gallery by Weights & Biases.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/wandb/awesome-dl-projects/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 26,
      "date": "Tue, 21 Dec 2021 11:48:36 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/wandb/awesome-dl-projects/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "wandb/awesome-dl-projects",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/wandb/awesome-dl-projects/master/ml-tutorial/Investigating_EMNIST.ipynb",
      "https://raw.githubusercontent.com/wandb/awesome-dl-projects/master/ml-tutorial/EMNIST_Dense_Classification.ipynb"
    ],
    "technique": "File Exploration"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/wandb/awesome-dl-projects/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Awesome Deep Learning Projects",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "awesome-dl-projects",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "wandb",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/wandb/awesome-dl-projects/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 218,
      "date": "Tue, 21 Dec 2021 11:48:36 GMT"
    },
    "technique": "GitHub API"
  }
}