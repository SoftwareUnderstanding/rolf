{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1808.08720",
      "https://arxiv.org/abs/1711.03953"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9576581443887728
      ],
      "excerpt": "presented at CoNLL 2018. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/tdmeeste/SparseSeqModels",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-08-23T08:34:46Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-02-20T20:16:02Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9676800691916324
      ],
      "excerpt": "This repository contains code to run the exeriments presented in our paper Predefined Sparseness in Recurrent Sequence Models, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9764543708823838,
        0.8965388532313648,
        0.9171874971950293,
        0.8306477232759839,
        0.8491373562339328
      ],
      "excerpt": "The package sparse_seq contains the implementation of predefined sparse LSTM's and embedding layers, as described in that paper. \n- rnn.py: contains SparseLSTM, a pytorch module that allows composing a sparse single-layer LSTM based on elementary dense LSTM's, \nfor a given parameter density, or given fractions in terms of input and hidden representation size \nFor example, with reduce_in=0.5 and reduce_out=0.5, the sparse LSTM would have the same number of trainable parameters as a \ndense LSTM with half the number of input and output dimensions. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9568770833021701
      ],
      "excerpt": "to gain in speed and memory capacity compared to the dense LSTM. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.890644816005041
      ],
      "excerpt": "as a composition of a user-specified number individual trainable embedding blocks with smaller dimensions. As shown in the paper, this only behaves as intended, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.872921038739763
      ],
      "excerpt": "Both embedding regularization mechanisms described in Merity's paper Regularizing and Optimizing LSTM Language Models \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9786159389255941,
        0.958096651004277
      ],
      "excerpt": "The folders language_modeling and sequence_labeling contain the code for the language modeling and part-of-speech tagging experiments \ndescribed in our paper. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8275372295246597
      ],
      "excerpt": "The language_modeling code is mostly based on https://github.com/salesforce/awd-lstm-lm, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8052727122943586,
        0.9377535620064633
      ],
      "excerpt": "Given the strong dependence on hyperparameters and corresponding computational cost, \nwe only presented results for the Merity's model AWD-LSTM and its sparse counterpart. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8810913058975822,
        0.9504508928686662
      ],
      "excerpt": "The code should work with a sparse embedding layer, but given the small relative number of embedding parameters in the setup, \nand to keep the analysis untangled, we only ran experiments for AWD-LSTM with a sparse LSTM layer. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8505590901672964,
        0.9497353380788643
      ],
      "excerpt": "The result, averaged over different seeds, is given in Table 1 in the paper. \nThe sparse model with wider middle LSTM layer (1725 dimensions instead of 1150) but predefined sparseness to maintain \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9749794749344746
      ],
      "excerpt": "The POS tagging baseline is based on code contributed by Frederic Godin, augmented with the SparseEmbedding's in the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9632848379910426
      ],
      "excerpt": "Dense model with reduced dimensions (Fig. 3), e.g., for embedding size 5, for one particular setting of the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8962311596117306
      ],
      "excerpt": "The counterpart with predefined sparse embedding layer (note that the vocab is sorted by default) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.919028788739184,
        0.9268490315210371
      ],
      "excerpt": "Simulating the effect of inversing the vocabulary order (such that predefined sparseness in the embedding layer \ncorresponds to shorter embeddings for more frequent terms, rather than the proposed ordering) can be done for instance as \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Code for the CoNLL 2018 paper \"Predefined Sparseness in Recurrent Sequence Models\"",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/tdmeeste/SparseSeqModels/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 28 Dec 2021 17:24:34 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/tdmeeste/SparseSeqModels/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "tdmeeste/SparseSeqModels",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/tdmeeste/SparseSeqModels/master/language_modeling/getdata.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9454188487244647
      ],
      "excerpt": "The code was developed on Python 3.6.4, with pytorch 0.4.0 (CUDA V8.0, CuDNN 6.0) and all experiments were run on a GeForce GTX 1080 core. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.90620898977763
      ],
      "excerpt": "python main.py --seed 0 --save logs/awd-lstm \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336709127548557,
        0.9121932794424014
      ],
      "excerpt": "python main.py --sparse_mode sparse_hidden --sparse_fract 0.66666 --nhid 1725 --save logs/awd-lstm-sparse \npython finetune.py --sparse_mode sparse_hidden --sparse_fract 0.66666 --nhid 1725 --save logs/awd-lstm-sparse \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8666613341492476,
        0.8666613341492476,
        0.87468602345667
      ],
      "excerpt": "python main_overfit.py --save logs/awd-lstm-overfit-dense_reduced --emsize 200 --nhid 575 --epochs 150 --lr 5 \npython main_overfit.py --save logs/awd-lstm-overfit-sparse1 --emsize 200 --sparse_mode sparse_hidden --sparse_fract 0.5 --epochs 150 --lr 5 \npython main_overfit.py --save logs/awd-lstm-overfit-sparse2 --emblocks 10 --emdensity 0.5 --sparse_mode sparse_all --sparse_fract 0.5 --epochs 150 --lr 5 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9201839050599392
      ],
      "excerpt": "python main.py --emsize 5 --nhid 10 --epochs 50 --dropouti 0.2 --wdrop 0.2 --save logs/pos_dense \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9182782398050071
      ],
      "excerpt": "python main.py --emsize 20 --emb_density 0.25 --emb_blocks 20 --nhid 10 --epochs 50 --dropouti 0.2 --wdrop 0.2 --save logs/pos_sparse \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/tdmeeste/SparseSeqModels/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2018 Thomas Demeester, Frederic Godin\\n\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Predefined Sparseness in Recurrent Sequence Models",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "SparseSeqModels",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "tdmeeste",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/tdmeeste/SparseSeqModels/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 9,
      "date": "Tue, 28 Dec 2021 17:24:34 GMT"
    },
    "technique": "GitHub API"
  }
}