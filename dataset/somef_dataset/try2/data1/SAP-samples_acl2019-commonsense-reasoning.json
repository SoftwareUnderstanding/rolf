{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2005.00669",
      "https://arxiv.org/abs/1810.04805.\n\n\n## License\nCopyright (c"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- [1] J. Devlin et al., BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, 2018, https://arxiv.org/abs/1810.04805.\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "If you use this code in your research,\nplease cite:\n\n```\n@inproceedings{klein-nabi-2019-attention,\n    title = \"Attention Is (not) All You Need for Commonsense Reasoning\",\n    author = \"Klein, Tassilo  and\n      Nabi, Moin\",\n    booktitle = \"Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics\",\n    month = jul,\n    year = \"2019\",\n    address = \"Florence, Italy\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/P19-1477\",\n    doi = \"10.18653/v1/P19-1477\",\n    pages = \"4831--4836\",\n    abstract = \"The recently introduced BERT model exhibits strong performance on several language understanding benchmarks. In this paper, we describe a simple re-implementation of BERT for commonsense reasoning. We show that the attentions produced by BERT can be directly utilized for tasks such as the Pronoun Disambiguation Problem and Winograd Schema Challenge. Our proposed attention-guided commonsense reasoning method is conceptually simple yet empirically powerful. Experimental analysis on multiple datasets demonstrates that our proposed system performs remarkably well on all cases while outperforming the previously reported state of the art by a margin. While results suggest that BERT seems to implicitly learn to establish complex relationships between entities, solving commonsense reasoning tasks might require more than unsupervised models learned from huge text corpora.\",\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{klein-nabi-2019-attention,\n    title = \"Attention Is (not) All You Need for Commonsense Reasoning\",\n    author = \"Klein, Tassilo  and\n      Nabi, Moin\",\n    booktitle = \"Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics\",\n    month = jul,\n    year = \"2019\",\n    address = \"Florence, Italy\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/P19-1477\",\n    doi = \"10.18653/v1/P19-1477\",\n    pages = \"4831--4836\",\n    abstract = \"The recently introduced BERT model exhibits strong performance on several language understanding benchmarks. In this paper, we describe a simple re-implementation of BERT for commonsense reasoning. We show that the attentions produced by BERT can be directly utilized for tasks such as the Pronoun Disambiguation Problem and Winograd Schema Challenge. Our proposed attention-guided commonsense reasoning method is conceptually simple yet empirically powerful. Experimental analysis on multiple datasets demonstrates that our proposed system performs remarkably well on all cases while outperforming the previously reported state of the art by a margin. While results suggest that BERT seems to implicitly learn to establish complex relationships between entities, solving commonsense reasoning tasks might require more than unsupervised models learned from huge text corpora.\",\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9766221915844664
      ],
      "excerpt": "See our latest work accepted at ACL'20 on commonsense reasoning using contrastive self-supervised learning. arXiv, GitHub \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/SAP-samples/acl2019-commonsense",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-07-18T15:08:11Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-07T12:09:04Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "![Schematic Illustration MAS](https://github.com/SAP-samples/acl2019-commonsense/blob/master/img/mas_illustration.png)\nThe recently introduced [BERT (Deep Bidirectional Transformers for Language Understanding)](https://github.com/google-research/bert) [1] model exhibits strong performance on several language understanding benchmarks. In this work, we describe a simple re-implementation of BERT for commonsense reasoning. We show that the attentions produced by BERT can be directly utilized for tasks such as the Pronoun Disambiguation Problem (PDP) and Winograd Schema Challenge (WSC). Our proposed attention-guided commonsense reasoning method is conceptually simple yet empirically powerful. Experimental analysis on multiple datasets demonstrates that our proposed system performs remarkably well on all cases while outperforming the previously reported state of the art by a margin. While results suggest that BERT seems to implicitly learn to establish complex relationships between entities, solving commonsense reasoning tasks might require more than unsupervised models learned from huge text corpora.\nThe sample code provided within this repository allows to replicate the results reported in the paper for PDP and WSC.\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9634113603241555
      ],
      "excerpt": "Updated to version 0.1.0: allows replication of our ACL'19 paper results \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8691137899820293
      ],
      "excerpt": "See our latest work accepted at ACL'20 on commonsense reasoning using contrastive self-supervised learning. arXiv, GitHub \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Source code for the paper \"Attention Is (not) All You Need for Commonsense Reasoning\" published at ACL 2019.",
      "technique": "GitHub API"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Install BertViz by cloning the repository and getting dependencies:\n```\ngit clone https://github.com/jessevig/bertviz.git\ncd bertviz\npip install -r requirements.txt\ncd ..\n```\n\n2. To replicate the results proceed to step 3). If you want to run the stand-alone version, you can just use [MAS.py](https://github.com/SAP-samples/acl2019-commonsense-reasoning/blob/master/MAS.py). Usage is showcased in the Jupyter Notebook example [MAS_Example.ipynb](https://github.com/SAP-samples/acl2019-commonsense-reasoning/blob/master/MAS_Example.ipynb).\n\n3. Add BertViz path to Python path:\n```\n  export PYTHONPATH=$PYTHONPATH:/home/ubuntu/bertviz/\n```\nalternatively, you can add the statement to commonsense.py after importing of sys, e.g.\n```\nsys.path.append(\"/home/ubuntu/bertviz/\")\n```\n\n4. Clone this repository and install dependencies:\n```\ngit clone https://github.com/SAP/acl2019-commonsense-reasoning\ncd acl2019-commonsense-reasoning\npip install -r requirements.txt\n```\n\n5. Create 'data' sub-directory and download files for PDP and WSC challenges:\n```\nmkdir data\nwget https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/PDPChallenge2016.xml\nwget https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WSCollection.xml\ncd ..\n```\n6. Run the scripts from the paper\n\nFor replicating the results on WSC:\n```\npython commonsense.py --data_dir=~/acl2019-commonsense-reasoning/data/ --bert_mode=bert-base-uncased --do_lower_case --task_name=MNLI\n```\n\nFor replicating the results on PDP:\n```\npython commonsense.py --data_dir=~/acl2019-commonsense-reasoning/data/ --bert_mode=bert-base-uncased --do_lower_case --task_name=pdp\n```\n\nFor more information on the individual functions, please refer to their doc strings.\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/SAP-samples/acl2019-commonsense-reasoning/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 6,
      "date": "Sun, 26 Dec 2021 12:06:16 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/SAP-samples/acl2019-commonsense/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "SAP-samples/acl2019-commonsense",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/SAP-samples/acl2019-commonsense-reasoning/main/MAS_Example.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Install BertViz by cloning the repository and getting dependencies:\n```\ngit clone https://github.com/jessevig/bertviz.git\ncd bertviz\npip install -r requirements.txt\ncd ..\n```\n\n2. To replicate the results proceed to step 3). If you want to run the stand-alone version, you can just use [MAS.py](https://github.com/SAP-samples/acl2019-commonsense-reasoning/blob/master/MAS.py). Usage is showcased in the Jupyter Notebook example [MAS_Example.ipynb](https://github.com/SAP-samples/acl2019-commonsense-reasoning/blob/master/MAS_Example.ipynb).\n\n3. Add BertViz path to Python path:\n```\n  export PYTHONPATH=$PYTHONPATH:/home/ubuntu/bertviz/\n```\nalternatively, you can add the statement to commonsense.py after importing of sys, e.g.\n```\nsys.path.append(\"/home/ubuntu/bertviz/\")\n```\n\n4. Clone this repository and install dependencies:\n```\ngit clone https://github.com/SAP/acl2019-commonsense-reasoning\ncd acl2019-commonsense-reasoning\npip install -r requirements.txt\n```\n\n5. Create 'data' sub-directory and download files for PDP and WSC challenges:\n```\nmkdir data\nwget https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/PDPChallenge2016.xml\nwget https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WSCollection.xml\ncd ..\n```\n6. Run the scripts from the paper\n\nFor replicating the results on WSC:\n```\npython commonsense.py --data_dir=~/acl2019-commonsense-reasoning/data/ --bert_mode=bert-base-uncased --do_lower_case --task_name=MNLI\n```\n\nFor replicating the results on PDP:\n```\npython commonsense.py --data_dir=~/acl2019-commonsense-reasoning/data/ --bert_mode=bert-base-uncased --do_lower_case --task_name=pdp\n```\n\nFor more information on the individual functions, please refer to their doc strings.\n\n",
      "technique": "Header extraction"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/SAP-samples/acl2019-commonsense/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Attention Is (not) All You Need for Commonsense Reasoning",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "acl2019-commonsense",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "SAP-samples",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/SAP-samples/acl2019-commonsense/blob/main/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- [Python](https://www.python.org/)\n- [PyTorch](https://pytorch.org/)\n- [Huggingface Tranformers](https://github.com/huggingface/transformers)\n- [BertViz](https://github.com/jessevig/bertviz)\n- [WSC,PDP data](https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/)\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 12,
      "date": "Sun, 26 Dec 2021 12:06:16 GMT"
    },
    "technique": "GitHub API"
  },
  "support": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This project is provided \"as-is\" and any bug reports are not guaranteed to be fixed.\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "sample",
      "machine-learning",
      "nlp",
      "commonsense-reasoning"
    ],
    "technique": "GitHub API"
  }
}