{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1411.4555v2 </li>\n        <li> https://en.wikipedia.org/wiki/Convolutional_neural_network </li>\n        <li> Deep Learning with Python by Fra\u00e7ois Chollet </li>\n        <li> https://towardsdatascience.com/understanding-and-coding-a-resnet-in-keras-446d7ff84d33 </li>\n        <li> https://arxiv.org/abs/1512.03385 </li>\n    </ul>",
      "https://arxiv.org/abs/1512.03385 </li>\n    </ul>"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8670498468297771,
        0.9737528548338414
      ],
      "excerpt": "<img src=\"https://img.shields.io/badge/made%20by%20-matakshay-blue\"> \n<img src=\"https://badges.frapsoft.com/os/v1/open-source.svg?v=103\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9909844968086463,
        0.9686679014285212,
        0.9175585083128629,
        0.9858279336460283,
        0.9821300997599387,
        0.9858279336460283,
        0.9942757182925156
      ],
      "excerpt": "    <li><a href=\"#intro\"> Introduction </a></li> \n    <li><a href=\"#dataset\"> Dataset </a></li> \n    <li><a href=\"#model\"> Model </a> </li> \n<!--     <li><a href=\"#examples\"> Examples </a></li> --> \n    <li><a href=\"#frameworks\"> Frameworks, Libraries & Languages </a></li> \n    <li><a href=\"#usage\"> Usage </a></li> \n    <li><a href=\"#acknowledgement\"> Acknowledgement </a></li> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9942757182925156,
        0.9821300997599387,
        0.9909844968086463,
        0.9734325286710462,
        0.9777619725330154,
        0.9942757182925156
      ],
      "excerpt": "    <li> Keras </li> \n    <li> Tensorflow </li> \n    <li> Python3 </li> \n    <li> Numpy </li> \n    <li> Matplotlib </li> \n    <li> pickle-mixin </li> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9156566588472104
      ],
      "excerpt": "    &lt;li&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9156566588472104,
        0.9156566588472104
      ],
      "excerpt": "    &lt;/li&gt; \n    &lt;li&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9156566588472104
      ],
      "excerpt": "    &lt;/li&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9156566588472104
      ],
      "excerpt": "    &lt;/li&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9156566588472104
      ],
      "excerpt": "    &lt;/li&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9997892827998496,
        0.999915130614252,
        0.9988197099192393
      ],
      "excerpt": "    &lt;li&gt; https://www.ijcai.org/Proceedings/15/Papers/593.pdf &lt;/li&gt; \n    &lt;li&gt;  https://arxiv.org/abs/1411.4555v2 &lt;/li&gt; \n    &lt;li&gt; https://en.wikipedia.org/wiki/Convolutional_neural_network &lt;/li&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9828029216366083,
        0.999915130614252
      ],
      "excerpt": "    &lt;li&gt; https://towardsdatascience.com/understanding-and-coding-a-resnet-in-keras-446d7ff84d33 &lt;/li&gt; \n    &lt;li&gt; https://arxiv.org/abs/1512.03385 &lt;/li&gt; \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/matakshay/Neural_Image_Caption_Generator",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-03-14T13:45:44Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-04-22T10:56:14Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9255695429280633
      ],
      "excerpt": "This is a Deep Learning Model for generating captions for images. It uses techniques from Computer Vision and Natural Language Processing. Some handpicked examples of images from test dataset and the captions generated by the model are shown below. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9968029537584643
      ],
      "excerpt": "<h3> TABLE OF CONTENTS </h3> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8595615247809957
      ],
      "excerpt": "<h2 id=\"intro\"> Introduction </h2> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9897330131759758
      ],
      "excerpt": "Deep Learning and Neural Networks have found profound applications in both NLP and Computer Vision. Before the Deep Learning era, statistical and Machine Learning techniques were commonly used for these tasks, especially in NLP. Neural Networks however have now proven to be powerful techniques, especially for more complex tasks. With the increase in size of available datasets and efficient computational tools, Deep Learning is being throughly researched on and applied in an increasing number of areas. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9883440785652303
      ],
      "excerpt": "In 2012 the Google <a href=\"http://www.image-net.org/\">ImageNet</a> Challenge (ILSVRC) results showed that Convolutional Neural Networks (CNNs) can be an excellent choice for tasks involving visual imagery. Being translation invariant, after learning a pattern in one region of an image, CNNs can very easily recognize it in another region - a task which was quite computationally inefficient in vanilla feed-forward networks. When many convolutional layers are stacked together, they can efficiently learn to recognize patterns in a hierarchical manner - the initial layers learn to detect edges, lines etc. while the later layers make  use of these to learn more complex features. In this project, we make use of a popular CNN architecture - the ResNet50 to process the input images and get the feature vectors. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9871377346641839
      ],
      "excerpt": "For generating the captions, we make use of Long Short-Term Memory (LSTM) networks. LSTMs are a variant of Recurrent Neural Networks which are widely used in Natural Language Processing. Unlike a Dense layer, an RNN layer does not process an input in one go. Instead, it processes a sequence element-by-element, at each step incorporating new data with the information processed so far. This property of an RNN makes it a natural yet powerful architecture for processing sequential inputs. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.973695254719637
      ],
      "excerpt": "This project uses the <a href=\"https://www.ijcai.org/Proceedings/15/Papers/593.pdf\">Flickr 8K</a> dataset for training the model. This can be downloaded from <a href=\"https://www.kaggle.com/shadabhussain/flickr8k?select=model_weights.h5\"> here</a>. It contains 8000 images, most of them featuring people and animals in a state of action. Each image is provided with five different captions describing the entities and events depicted in the image. Different captions of the same image tend to focus on different aspects of the scene, or use different linguistic constructions. This ensures that there is enough linguistic variety in the description of the images. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8534402249604769
      ],
      "excerpt": "<h2 id=\"model\"> Model </h2> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9326650843599966,
        0.8946110460309353,
        0.9920592860197059
      ],
      "excerpt": "    This project uses the ResNet50 architecture for obtaining the image features. ResNets (short for Residual Networks) have been classic approach for many Computer Vision tasks, after this network won the 2015 ImageNet Challenge. ResNets showed how even very Deep Neural Networks (the original ResNet was around 152 layers deep!) can be trained without worrying about the vanishing gradient problem. \nThe strength of a ResNet lies in the use of Skip Connections - these mitigate the vanishing gradient problem by providing a shorter alternate path for the gradient to flow through. <br> \nResNet50 which is used in this project is a smaller version of the original ResNet152. This architecture is so frequently used for Transfer Learning that it comes preloaded in the Keras framework, along with the weights (trained on the ImageNet dataset). Since we only need this network for getting the image feature vectors, so we remove the last layer (which in the original model was used to classify input image into one of the 1000 classes). The encoded features for training and test images are stored at \"encoded_train_features.pkl\" and \"encoded_test_features.pkl\" respectively. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.903279745780821,
        0.9717528873013275
      ],
      "excerpt": "<a href=\"https://nlp.stanford.edu/projects/glove/\">GloVe</a> vectors were used for creating the word embeddings for the captions. The version used in this project contains 50-dimensional embedding vectors for 6 Billion English words. It can be downloaded from <a href=\"https://www.kaggle.com/watts2/glove6b50dtxt\"> here</a>. These Embeddings are not processed (fine-tuned using the current data) further during training time. <br> \n    The neural network for generating the captions has been built using the Keras Functional API. The features vectors (obtained form the ResNet50 network) are processed and combined with the caption data (which after converting into Embeddings, have been passed through an LSTM layer). This combined information is passed through a Dense layer followed by a Softmax layer (over the vocabulary words). The model was trained for 20 epochs, and at the end of each epoch, the model was saved in the \"/model_checkpoints\" directory. This process took about half an hour. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8018859610966392
      ],
      "excerpt": "    <figcaption> <p align=\"justify\">A dog jumps over a red and white obstacle </p></figcaption> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8490037945672047
      ],
      "excerpt": "    <figcaption> <p align=\"justify\"> A tennis player hitting the ball </p> </figcaption> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9005755682289478
      ],
      "excerpt": "    <figcaption> <p align=\"justify\"> A man on a bike is jumping off a rocky ledge </p> </figcaption> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9868209214679863
      ],
      "excerpt": "    <figcaption> <p align=\"justify\"> A skateboarder is performing a trick on a railing </p> </figcaption> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.966350332591716
      ],
      "excerpt": "    <figcaption> <p align=\"justify\"> A dog is jumping into a pond </p> </figcaption> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9197183115059743
      ],
      "excerpt": "        Clone this repository on your system and head over to it \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8460677702829258
      ],
      "excerpt": "        To run the model and generate caption for a custom image, move the image (in JPEG format) to the current directory and rename it to \"input.jpg\". Then type the following on the terminal- \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9455092801488225
      ],
      "excerpt": "        This loads the model and runs it with the input image. The generated caption will be printed. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9382403923842905
      ],
      "excerpt": "    I referred many articles & research papers while working on this project. Some of them are listed below- \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Deep Learning model which uses Computer Vision and NLP to generate captions for images",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/matakshay/Neural_Image_Caption_Generator/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 4,
      "date": "Thu, 30 Dec 2021 04:18:29 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/matakshay/Neural_Image_Caption_Generator/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "matakshay/Neural_Image_Caption_Generator",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/matakshay/Neural_Image_Caption_Generator/master/build.ipynb",
      "https://raw.githubusercontent.com/matakshay/Neural_Image_Caption_Generator/master/text_preprocessing.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8833828031135371
      ],
      "excerpt": "<img src=\"https://badges.frapsoft.com/os/v1/open-source.svg?v=103\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8136338321887024
      ],
      "excerpt": "<img src=\"https://img.shields.io/badge/python-v3.7%2B-orange\" /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9818761167639474
      ],
      "excerpt": "<p align=\"justify\">On the terminal run the following commands-</p> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9880631962726351
      ],
      "excerpt": "        Install all dependencies \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8089725681194972
      ],
      "excerpt": "        &lt;code&gt; pip install numpy &lt;/code&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8423450427739586
      ],
      "excerpt": "        &lt;code&gt; pip install matplotlib &lt;/code&gt; \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9028351979428585
      ],
      "excerpt": "     src=\"demo/demo.gif\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8321310455553157,
        0.9286546183490366
      ],
      "excerpt": "<img alt=\"Dataset demo\" \n     src=\"demo/demo2.gif\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8836704212480256,
        0.9011250291579453
      ],
      "excerpt": "        <img src=\"model_plot.png\" \n             alt=\"Model Plot\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8694269052892559
      ],
      "excerpt": "<!-- <h2 id=\"examples\"> Examples </h2> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8355534092856827
      ],
      "excerpt": "The trained model was used to generate caption for images from the test dataset. A few chosen examples from this are given below. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9519501313984958,
        0.9219711465910898
      ],
      "excerpt": "    <img src=\"examples/eg1.jpg\" \n         alt=\"First example\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9519501313984958,
        0.8778487586960795
      ],
      "excerpt": "    <img src=\"examples/eg5.jpg\" \n         alt=\"Fifth example\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9519501313984958,
        0.8299721059753647
      ],
      "excerpt": "    <img src=\"examples/eg2.jpg\" \n         alt=\"Second example\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9519501313984958,
        0.8778487586960795
      ],
      "excerpt": "    <img src=\"examples/eg3.jpg\" \n         alt=\"Third example\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9519501313984958,
        0.8778487586960795
      ],
      "excerpt": "    <img src=\"examples/eg4.jpg\" \n         alt=\"Fourth example\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9519501313984958,
        0.8778487586960795
      ],
      "excerpt": "    <img src=\"examples/eg6.jpg\" \n         alt=\"Sixth example\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8492411425756007
      ],
      "excerpt": "<h2 id=\"usage\">Usage</h2> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/matakshay/Neural_Image_Caption_Generator/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "intro\"> Introduction </a></li>",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Neural_Image_Caption_Generator",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "matakshay",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/matakshay/Neural_Image_Caption_Generator/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 7,
      "date": "Thu, 30 Dec 2021 04:18:29 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "deep-learning",
      "machine-learning",
      "computer-vision",
      "natural-language-processing",
      "imagenet-dataset",
      "resnet-50",
      "cnn",
      "convolutional-neural-networks",
      "lstm-neural-networks",
      "recurrent-neural-networks",
      "keras",
      "numpy",
      "matplotlib",
      "glove",
      "glove-vectors",
      "tensorflow"
    ],
    "technique": "GitHub API"
  }
}