{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1810.04805"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[1] - [Jacob Devlin et all: *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding*, 2018](https://arxiv.org/abs/1810.04805)\n\n[2] - [Mozharova V., Loukachevitch N.: *Two-stage approach in Russian named entity recognition*, 2016](https://ieeexplore.ieee.org/document/7584769)\n\n[3] - [BSNLP-2019 Shared Task](http://bsnlp.cs.helsinki.fi/shared_task.html)\n\n[4] - [DeepPavlov: open-source library for dialog systems](https://github.com/deepmipt/deeppavlov)\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```\n@inproceedings{arkhipov-etal-2019-tuning,\n    title = \"Tuning Multilingual Transformers for Language-Specific Named Entity Recognition\",\n    author = \"Arkhipov, Mikhail  and\n      Trofimova, Maria  and\n      Kuratov, Yuri  and\n      Sorokin, Alexey\",\n    booktitle = \"Proceedings of the 7th Workshop on Balto-Slavic Natural Language Processing\",\n    month = aug,\n    year = \"2019\",\n    address = \"Florence, Italy\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/W19-3712\",\n    doi = \"10.18653/v1/W19-3712\",\n    pages = \"89--93\"\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{arkhipov-etal-2019-tuning,\n    title = \"Tuning Multilingual Transformers for Language-Specific Named Entity Recognition\",\n    author = \"Arkhipov, Mikhail  and\n      Trofimova, Maria  and\n      Kuratov, Yuri  and\n      Sorokin, Alexey\",\n    booktitle = \"Proceedings of the 7th Workshop on Balto-Slavic Natural Language Processing\",\n    month = aug,\n    year = \"2019\",\n    address = \"Florence, Italy\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/W19-3712\",\n    doi = \"10.18653/v1/W19-3712\",\n    pages = \"89--93\"\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "Organizations (ORG) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "|              | ORG  | 84.3          | 92.1         | 88.0                              | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/deepmipt/Slavic-BERT-NER",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-05-14T13:41:48Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-22T10:46:51Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9907584213707372,
        0.9865354878382386,
        0.9478175826323325,
        0.860059181823877
      ],
      "excerpt": "Notice: The repo is left as-is, the Slavic BERT model is now as part of DeepPavlov repo. \nBERT is a method of pre-training language representations, meaning that we train a general-purpose \"language understanding\" model on a large text corpus (like Wikipedia), and then use that model for downstream NLP tasks that we care about (like question answering). For details see original BERT github. \nThe repository contains Bulgarian+Czech+Polish+Russian specific: \n- shared BERT model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8023650088146606,
        0.994006755458974,
        0.9735639471265379
      ],
      "excerpt": "Our academic paper which describes tuning Transformers for NER task in detail can be found here: https://www.aclweb.org/anthology/W19-3712/. \nThe Slavic model is the result of transfer from 2018_11_23/multi_cased_L-12_H-768_A-12 Multilingual BERT model to languages of Bulgarian (bg), Czech (cs), Polish (pl) and Russian (ru). The fine-tuning was performed with a stratified dataset of bg, cs and pl Wikipedias and ru news. \nThe model format is the same as in the original repository. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8684383115809992,
        0.8931492050494106
      ],
      "excerpt": "Named Entity Recognition (further, NER) is a task of recognizing named entities in text, as well as detecting their type. \nWe used Slavic BERT model as a base to build NER system. First, we feed each input word into WordPiece case-sensitive tokenizer and extract the final hidden representation corresponding to the first subtoken in each word. These representations are fed into a classification dense layer over the NER label set. A token-level CRF layer is also added on top. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594320375865495
      ],
      "excerpt": "The metrics for all languages and entities on test set are: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9172124480859546
      ],
      "excerpt": "For detailed description of evaluation method see BSNLP-2019 Shared Task page. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Shared BERT model for 4 languages of Bulgarian, Czech, Polish and Russian. Slavic NER model.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/deepmipt/Slavic-BERT-NER/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 7,
      "date": "Sat, 25 Dec 2021 18:11:23 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/deepmipt/Slavic-BERT-NER/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "deepmipt/Slavic-BERT-NER",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The toolkit is implemented in Python 3.6 and requires a number of packages. To install all needed packages use:\n\n```bash\n$ pip3 install -r requirements.txt\n```\n\nCAUTION: Python3.5 and Python3.7 are not supported, see [DeepPavlov rep](https://github.com/deepmipt/deeppavlov) for details.\n\n",
      "technique": "Header extraction"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/deepmipt/Slavic-BERT-NER/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Slavic BERT NER",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Slavic-BERT-NER",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "deepmipt",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/deepmipt/Slavic-BERT-NER/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 60,
      "date": "Sat, 25 Dec 2021 18:11:23 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\nfrom deeppavlov import build_model\n\n#: Download and load model (set download=False to skip download phase)\nner = build_model(\"./ner_bert_slav.json\", download=True)\n\n#: Get predictions\nner([\"To Bert z ulicy Sezamkowej\"])\n#: [[['To', 'Bert', 'z', 'ulicy', 'Sezamkowej']], [['O', 'B-PER', 'O', 'B-LOC', 'I-LOC']]]\nner([\"\u042d\u0442\u043e\", \"\u0411\u0435\u0440\u0442\", \"\u0438\u0437\", \"\u0420\u043e\u0441\u0441\u0438\u0438\"])\n#: [[['\u042d\u0442\u043e'], ['\u0411\u0435\u0440\u0442'], ['\u0438\u0437'], ['\u0420\u043e\u0441\u0441\u0438\u0438']], [['O'], ['B-PER'], ['O'], ['B-LOC']]]\n```\n \n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "The Slavic Bert model can be used in any way proposed by the BERT developers.\n\nOne approach may be:\n\n```python\n\nimport tensorflow as tf\n  \nfrom bert_dp.modeling import BertConfig, BertModel\nfrom deeppavlov.models.preprocessors.bert_preprocessor import BertPreprocessor\n\n\nbert_config = BertConfig.from_json_file('./bg_cs_pl_ru_cased_L-12_H-768_A-12/bert_config.json')\n\ninput_ids = tf.placeholder(shape=(None, None), dtype=tf.int32)\ninput_mask = tf.placeholder(shape=(None, None), dtype=tf.int32)\ntoken_type_ids = tf.placeholder(shape=(None, None), dtype=tf.int32)\n\nbert = BertModel(config=bert_config,\n                 is_training=False,\n                 input_ids=input_ids,\n                 input_mask=input_mask,\n                 token_type_ids=token_type_ids,\n                 use_one_hot_embeddings=False)\n\npreprocessor = BertPreprocessor(vocab_file='./bg_cs_pl_ru_cased_L-12_H-768_A-12/vocab.txt',\n                                do_lower_case=False,\n                                max_seq_length=512)\n\nwith tf.Session() as sess:\n\n    #: Load model\n    tf.train.Saver().restore(sess, './bg_cs_pl_ru_cased_L-12_H-768_A-12/bert_model.ckpt')\n\n    #: Get predictions\n    features = preprocessor([\"Bert z ulicy Sezamkowej\"])[0]\n\n    print(sess.run(bert.sequence_output, feed_dict={input_ids: [features.input_ids],\n                                                    input_mask: [features.input_mask],\n                                                    token_type_ids: [features.input_type_ids]}))\n\n    features = preprocessor([\"\u0411\u0435\u0440\u0442\", \"\u0441\", \"\u0423\u043b\u0438\u0446\u044b\", \"\u0421\u0435\u0437\u0430\u043c\"])[0]\n\n    print(sess.run(bert.sequence_output, feed_dict={input_ids: [features.input_ids],\n                                                    input_mask: [features.input_mask],\n                                                    token_type_ids: [features.input_type_ids]}))\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}