{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1706.03762> Attention Is All You Need (official paper"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "<pre>\n<b>Coder un Transformer avec Tensorflow et Keras (LIVE)</b>\n<a href=\"https://www.youtube.com/watch?v=mWA-PmxMBDk\">https://www.youtube.com/watch?v=mWA-PmxMBDk</a>\n<i>Thibault Neveu</i>\n<a href=\"https://colab.research.google.com/drive/1akAsUAddF2-x57BJBA_gF-v4htyiR12y?usp=sharing\">https://colab.research.google.com/drive/1akAsUAddF2-x57BJBA_gF-v4htyiR12y?usp=sharing</a>\n</pre>\n\n<pre>\n<b>[TUTORIAL + C\u00d3DIGO] Machine Translation usando redes TRANSFORMER (Python + Keras)</b>\n<a href=\"https://www.youtube.com/watch?v=p2sTJYoIwj0\">https://www.youtube.com/watch?v=p2sTJYoIwj0</a>\n<i>codificandobits</i>\n<a href=\"https://github.com/codificandobits/Traductor_con_redes_Transformer/blob/master/machine-translation-transformers.ipynb\">https://github.com/codificandobits/Traductor_con_redes_Transformer/blob/master/machine-translation-transformers.ipynb</a>\n</pre>\n\n<pre>\n<a href=\"https://github.com/CyberZHG/keras-transformer\">https://github.com/CyberZHG/keras-transformer</a>\n<i>CyberZHG</i>\n</pre>\n\n<pre>\n<b>Dataset</b>\n<a href=\"https://www.manythings.org/anki/\">https://www.manythings.org/anki/</a>\n</pre>\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "Epoch 10/15 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9347624864997427
      ],
      "excerpt": "https://arxiv.org/abs/1706.03762 Attention Is All You Need (official paper) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ays-dev/keras-transformer",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-02-18T15:53:49Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-12T13:50:11Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "Model: \"Transformer-Model\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9415094332091255
      ],
      "excerpt": "Epoch 00001: val_accuracy improved from -inf to 0.84285, saving model to ./logs/transformer_ep-01_loss-1.05_acc-0.84.ckpt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9415094332091255
      ],
      "excerpt": "Epoch 00002: val_accuracy improved from 0.84285 to 0.87188, saving model to ./logs/transformer_ep-02_loss-0.46_acc-0.91.ckpt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9415094332091255
      ],
      "excerpt": "Epoch 00003: val_accuracy improved from 0.87188 to 0.88687, saving model to ./logs/transformer_ep-03_loss-0.32_acc-0.93.ckpt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9415094332091255
      ],
      "excerpt": "Epoch 00008: val_accuracy improved from 0.89448 to 0.89624, saving model to ./logs/transformer_ep-08_loss-0.15_acc-0.96.ckpt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9415094332091255
      ],
      "excerpt": "Epoch 00009: val_accuracy improved from 0.89624 to 0.89654, saving model to ./logs/transformer_ep-09_loss-0.14_acc-0.96.ckpt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9076675384837094
      ],
      "excerpt": "Traduction: i like to eat some cake . \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8613032781534842
      ],
      "excerpt": "Traduction: we have to feed the people . \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8527961369516825
      ],
      "excerpt": "with tf.summary.create_file_writer(\"logs/hparam_tuning\").as_default(): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9567588029116127
      ],
      "excerpt": "  with tf.summary.create_file_writer(run_dir).as_default(): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Transformer",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ays-dev/keras-transformer/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 21 Dec 2021 02:30:51 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ays-dev/keras-transformer/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "ays-dev/keras-transformer",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8661176197453521
      ],
      "excerpt": "name = \"transformer\" \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9503189345333785
      ],
      "excerpt": "python train.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.925671696398174,
        0.9457175861910134,
        0.8932227647655261,
        0.8934477375241742
      ],
      "excerpt": "import tensorflow as tf \nimport numpy as np \nfrom dataset import get_dataset, prepare_dataset \nfrom model import get_model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8954410619827247
      ],
      "excerpt": "print(\"Dataset loaded. Length:\", len(dataset), \"lines\") \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9316599398306985
      ],
      "excerpt": "print(\"Train data loaded. Length:\", len(train_dataset), \"lines\") \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "  lowercase = True, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8172342705654815,
        0.8172342705654815
      ],
      "excerpt": "  ENCODER_VOCAB_SIZE = len(encoder_vocab), \n  DECODER_VOCAB_SIZE = len(decoder_vocab), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9241022411340848,
        0.9241022411340848,
        0.8421074476017179
      ],
      "excerpt": "x = [np.array(encoder_input), np.array(decoder_input)] \ny = np.array(decoder_output) \nname = \"transformer\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8023167036065165,
        0.876848800860438
      ],
      "excerpt": "tensorboard_callback = tf.keras.callbacks.TensorBoard( \n  log_dir = \"logs/{}\".format(name) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123763140827432
      ],
      "excerpt": "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984,
        0.8594142235991984,
        0.8594142235991984
      ],
      "excerpt": "  save_weights_only = True, \n  save_best_only = True, \n  verbose = True \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123763140827432
      ],
      "excerpt": "early_stopping_callback = tf.keras.callbacks.EarlyStopping( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "  verbose = True \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8028773559313631
      ],
      "excerpt": "Train data loaded. Length: 100000 lines \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.856989540636594
      ],
      "excerpt": "Layer (type)                    Output Shape         Param #:     Connected to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8934745846565956
      ],
      "excerpt": "Total params: 2,445,406 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8423524686921765
      ],
      "excerpt": "Epoch 1/15 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8014763427996605
      ],
      "excerpt": "Epoch 00001: val_accuracy improved from -inf to 0.84285, saving model to ./logs/transformer_ep-01_loss-1.05_acc-0.84.ckpt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8423524686921765
      ],
      "excerpt": "Epoch 2/15 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.801532563374205
      ],
      "excerpt": "Epoch 00002: val_accuracy improved from 0.84285 to 0.87188, saving model to ./logs/transformer_ep-02_loss-0.46_acc-0.91.ckpt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8423524686921765
      ],
      "excerpt": "Epoch 3/15 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.801532563374205
      ],
      "excerpt": "Epoch 00003: val_accuracy improved from 0.87188 to 0.88687, saving model to ./logs/transformer_ep-03_loss-0.32_acc-0.93.ckpt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8423524686921765
      ],
      "excerpt": "Epoch 8/15 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8058539444190935
      ],
      "excerpt": "Epoch 00008: val_accuracy improved from 0.89448 to 0.89624, saving model to ./logs/transformer_ep-08_loss-0.15_acc-0.96.ckpt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8423524686921765
      ],
      "excerpt": "Epoch 9/15 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8058539444190935
      ],
      "excerpt": "Epoch 00009: val_accuracy improved from 0.89624 to 0.89654, saving model to ./logs/transformer_ep-09_loss-0.14_acc-0.96.ckpt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8734162220490913
      ],
      "excerpt": "Epoch 10/15 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9350350788181568
      ],
      "excerpt": "python predict.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.925671696398174,
        0.9457175861910134,
        0.8932227647655261,
        0.8934477375241742,
        0.9416522774131079
      ],
      "excerpt": "import tensorflow as tf \nimport numpy as np \nfrom dataset import get_dataset, prepare_dataset \nfrom model import get_model \nfrom utils.make_translate import make_translate \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8954410619827247
      ],
      "excerpt": "print(\"Dataset loaded. Length:\", len(dataset), \"lines\") \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9316599398306985
      ],
      "excerpt": "print(\"Train data loaded. Length:\", len(train_dataset), \"lines\") \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "  lowercase = True, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8172342705654815,
        0.8172342705654815
      ],
      "excerpt": "  ENCODER_VOCAB_SIZE = len(encoder_vocab), \n  DECODER_VOCAB_SIZE = len(decoder_vocab), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9278802297796007
      ],
      "excerpt": "python params.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.925671696398174,
        0.9457175861910134
      ],
      "excerpt": "import tensorflow as tf \nimport numpy as np \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8932227647655261,
        0.8934477375241742
      ],
      "excerpt": "from dataset import get_dataset, prepare_dataset \nfrom model import get_model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984,
        0.8594142235991984
      ],
      "excerpt": "  shuffle = True, \n  lowercase = True, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9286377707943565,
        0.9286377707943565,
        0.9001220658817062,
        0.9001220658817062
      ],
      "excerpt": "x_train = [np.array(encoder_input[0:100]), np.array(decoder_input[0:100])] \ny_train = np.array(decoder_output[0:100]) \nx_test = [np.array(encoder_input[100:150]), np.array(decoder_input[100:150])] \ny_test = np.array(decoder_output[100:150]) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8221077828106255
      ],
      "excerpt": "with tf.summary.create_file_writer(\"logs/hparam_tuning\").as_default(): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8172342705654815,
        0.8172342705654815
      ],
      "excerpt": "    ENCODER_VOCAB_SIZE = len(encoder_vocab), \n    DECODER_VOCAB_SIZE = len(decoder_vocab), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8008331685760428,
        0.936606094659785,
        0.919553842565818,
        0.8073231056506955
      ],
      "excerpt": "          run_name = \"run-%d\" % session_num \n      print(\"--- Starting trial: %s\" % run_name) \n      print({ h.name: hparams[h] for h in hparams }) \n      run(\"logs/hparam_tuning/\" + run_name, hparams) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8008331685760428
      ],
      "excerpt": "--- Starting trial: run-0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8008331685760428
      ],
      "excerpt": "--- Starting trial: run-1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8008331685760428
      ],
      "excerpt": "--- Starting trial: run-31 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8616551106466185
      ],
      "excerpt": " -> 60 millions params (model size : ~600 Mo) \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ays-dev/keras-transformer/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "### Requirements\n\n```\n$ cat requirements.txt && pip install -r requirements.txt\n```\n\n```\ntensorflow==2.3.0\nnumpy==1.18.5\nnltk==3.5\n```\n\n```\n$ tensorboard --version\n> 2.4.1\n\n$ pip --version\n> pip 21.0.1 from /Users/ays-dev/opt/anaconda3/envs/tf/lib/python3.7/site-packages/pip (python 3.7)\n\n$ python --version\n> Python 3.7.9\n```\n\n\n### Train\n``python train.py``\n\n```python\nimport tensorflow as tf\nimport numpy as np\n\nfrom dataset import get_dataset, prepare_dataset\nfrom model import get_model\n\n\ndataset = get_dataset(\"fr-en\")\n\nprint(\"Dataset loaded. Length:\", len(dataset), \"lines\")\n\ntrain_dataset = dataset[0:100000]\n\nprint(\"Train data loaded. Length:\", len(train_dataset), \"lines\")\n\n(encoder_input,\ndecoder_input,\ndecoder_output,\nencoder_vocab,\ndecoder_vocab,\nencoder_inverted_vocab,\ndecoder_inverted_vocab) = prepare_dataset(\n  train_dataset,\n  shuffle = False,\n  lowercase = True,\n  max_window_size = 20\n)\n\ntransformer_model = get_model(\n  EMBEDDING_SIZE = 64,\n  ENCODER_VOCAB_SIZE = len(encoder_vocab),\n  DECODER_VOCAB_SIZE = len(decoder_vocab),\n  ENCODER_LAYERS = 2,\n  DECODER_LAYERS = 2,\n  NUMBER_HEADS = 4,\n  DENSE_LAYER_SIZE = 128\n)\n\ntransformer_model.compile(\n  optimizer = \"adam\",\n  loss = [\n    \"sparse_categorical_crossentropy\"\n  ],\n  metrics = [\n    \"accuracy\"\n  ]\n)\n\ntransformer_model.summary()\n\nx = [np.array(encoder_input), np.array(decoder_input)]\ny = np.array(decoder_output)\n\nname = \"transformer\"\ncheckpoint_filepath = \"./logs/transformer_ep-{epoch:02d}_loss-{loss:.2f}_acc-{accuracy:.2f}.ckpt\"\n\ntensorboard_callback = tf.keras.callbacks.TensorBoard(\n  log_dir = \"logs/{}\".format(name)\n)\n\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n  filepath = checkpoint_filepath,\n  monitor = \"val_accuracy\",\n  mode = \"max\",\n  save_weights_only = True,\n  save_best_only = True,\n  verbose = True\n)\n\nearly_stopping_callback = tf.keras.callbacks.EarlyStopping(\n  monitor = \"val_accuracy\",\n  mode = \"max\",\n  patience = 2,\n  min_delta = 0.001,\n  verbose = True\n)\n\ntransformer_model.fit(\n  x,\n  y,\n  epochs = 15,\n  batch_size = 32,\n  validation_split = 0.1,\n  callbacks=[\n    model_checkpoint_callback,\n    tensorboard_callback,\n    early_stopping_callback\n  ]\n)\n```\n\n\n##### Output\n\n```\nDataset loaded. Length: 185583 lines\nTrain data loaded. Length: 100000 lines\nModel: \"Transformer-Model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "keras-transformer",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "ays-dev",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ays-dev/keras-transformer/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\n$ cat requirements.txt && pip install -r requirements.txt\n```\n\n```\ntensorflow==2.3.0\nnumpy==1.18.5\nnltk==3.5\n```\n\n```\n$ tensorboard --version\n> 2.4.1\n\n$ pip --version\n> pip 21.0.1 from /Users/ays-dev/opt/anaconda3/envs/tf/lib/python3.7/site-packages/pip (python 3.7)\n\n$ python --version\n> Python 3.7.9\n```\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 4,
      "date": "Tue, 21 Dec 2021 02:30:51 GMT"
    },
    "technique": "GitHub API"
  }
}