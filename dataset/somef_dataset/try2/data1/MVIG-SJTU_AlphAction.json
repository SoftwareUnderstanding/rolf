{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We thankfully acknowledge the computing resource support of Huawei Corporation\nfor this project. \n\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2004.07485"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If this project helps you in your research or project, please cite\nthis paper:\n\n```\n@inproceedings{tang2020asynchronous,\n  title={Asynchronous Interaction Aggregation for Action Detection},\n  author={Tang, Jiajun and Xia, Jin and Mu, Xinzhi and Pang, Bo and Lu, Cewu},\n  booktitle={Proceedings of the European conference on computer vision (ECCV)},\n  year={2020}\n}\n```",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{tang2020asynchronous,\n  title={Asynchronous Interaction Aggregation for Action Detection},\n  author={Tang, Jiajun and Xia, Jin and Mu, Xinzhi and Pang, Bo and Lu, Cewu},\n  booktitle={Proceedings of the European conference on computer vision (ECCV)},\n  year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.985735709622531,
        0.979147393314151,
        0.9927662050383874
      ],
      "excerpt": "Asynchronous Interaction Aggregation for Action Detection (ECCV 2020), authored \nby Jiajun Tang, Jin Xia (equal contribution), Xinzhi Mu, Bo Pang,  \nCewu Lu (corresponding author).  \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/MVIG-SJTU/AlphAction",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-03-17T13:41:21Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-20T19:07:24Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9763908115377498,
        0.8102052824664094
      ],
      "excerpt": "AlphAction aims to detect the actions of multiple persons in videos. It is  \nthe first open-source project that achieves 30+ mAP (32.4 mAP) with single  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9970049080330524
      ],
      "excerpt": "This project is the official implementation of paper  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Spatio-Temporal Action Localization System",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/MVIG-SJTU/AlphAction/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 65,
      "date": "Wed, 22 Dec 2021 22:18:59 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/MVIG-SJTU/AlphAction/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "MVIG-SJTU/AlphAction",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To do training or inference on AVA dataset, please check [DATA.md](DATA.md)\nfor data preparation instructions.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "You need first to install this project, please check [INSTALL.md](INSTALL.md)\n\n",
      "technique": "Header extraction"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/MVIG-SJTU/AlphAction/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Cuda",
      "C++",
      "C"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "AlphAction",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "AlphAction",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "MVIG-SJTU",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/MVIG-SJTU/AlphAction/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 293,
      "date": "Wed, 22 Dec 2021 22:18:59 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "action-detection",
      "spatio-temporal-action-localization",
      "alphaction",
      "alpha-action",
      "action-recognition",
      "state-of-the-art",
      "pytorch",
      "torch",
      "gpu",
      "ava",
      "demo-video",
      "ava-dataset",
      "model-zoo"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[![AlphAction demo video](https://user-images.githubusercontent.com/22748802/94115680-a83a1500-fe7c-11ea-878c-536db277fba7.jpg)](https://www.youtube.com/watch?v=TdGmbOJ9hoE \"AlphAction demo video\")\n[[YouTube]](https://www.youtube.com/watch?v=TdGmbOJ9hoE) [[BiliBili]](https://www.bilibili.com/video/BV14A411J7Xv)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "To run the demo program on video or webcam, please check the folder [demo](demo).\nWe select 15 common categories from the 80 action categories of AVA, and \nprovide a practical model which achieves high accuracy (about 70 mAP) on these categories. \n\n",
      "technique": "Header extraction"
    }
  ]
}