{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1707.04585>`__ running the code from\ntheir `GitHub <https://github.com/renmengye/revnet-public>`__.\n\nThe PyTorch results listed were recomputed on June 11th 2018, and differ\nfrom the results in the ICLR paper. The Tensorflow results are still the\nsame.\n\nPrediction accuracy\n^^^^^^^^^^^^^^^^^^^\n\n+------------+------------------------+--------------------------+----------------------+----------------------+\n|            |               Cifar-10                            |               Cifar-100                     |\n+------------+------------------------+--------------------------+----------------------+----------------------+\n| Model      |    Tensorflow          |      PyTorch             |      Tensorflow      |     PyTorch          |\n+============+========================+==========================+======================+======================+\n| resnet-32  |  92.74                 |    92.86                 |   69.10              |  69.81               |\n+------------+------------------------+--------------------------+----------------------+----------------------+\n| resnet-110 |  93.99                 |    93.55                 |   73.30              |  72.40               |\n+------------+------------------------+--------------------------+----------------------+----------------------+\n| resnet-164 |  94.57                 |    94.80                 |   76.79              |  76.47               |\n+------------+------------------------+--------------------------+----------------------+----------------------+\n| revnet-38  |  93.14                 |    92.80                 |   71.17              |  69.90               |\n+------------+------------------------+--------------------------+----------------------+----------------------+\n| revnet-110 |  94.02                 |    94.10                 |   74.00              |  73.30               |\n+------------+------------------------+--------------------------+----------------------+----------------------+\n| revnet-164 |  94.56                 |    94.90                 |   76.39              |  76.90               |\n+------------+------------------------+--------------------------+----------------------+----------------------+\n\nTraining time (hours : minutes",
      "https://arxiv.org/abs/1902.02729>`__ by Tycho van der Ouderaa et al.\n* `Chest CT Super-resolution and Domain-adaptation using Memory-efficient 3D Reversible GANs <https://openreview.net/forum?id=SkxueFsiFV>`__ by Tycho van der Ouderaa et al.\n* `iUNets: Fully invertible U-Nets with Learnable Up- and Downsampling <https://arxiv.org/abs/2005.05220>`__ by Christian Etmann et al.\n\nCitation\n--------\n\nSil C. van de Leemput, Jonas Teuwen, Bram van Ginneken, and Rashindra Manniesing.\nMemCNN: A Python/PyTorch package for creating memory-efficient invertible neural networks.\nJournal of Open Source Software, 4, 1576, http://dx.doi.org/10.21105/joss.01576, 2019.\n\nIf you use our code, please cite:\n\n.. code:: bibtex\n\n    @article{vandeLeemput2019MemCNN,\n      journal = {Journal of Open Source Software},\n      doi = {10.21105/joss.01576},\n      issn = {2475-9066},\n      number = {39},\n      publisher = {The Open Journal},\n      title = {MemCNN: A Python/PyTorch package for creating memory-efficient invertible neural networks},\n      url = {http://dx.doi.org/10.21105/joss.01576},\n      volume = {4},\n      author = {Sil C. {van de} Leemput and Jonas Teuwen and Bram {van} Ginneken and Rashindra Manniesing},\n      pages = {1576},\n      date = {2019-07-30},\n      year = {2019},\n      month = {7},\n      day = {30},\n    }",
      "https://arxiv.org/abs/2005.05220>`__ by Christian Etmann et al.\n\nCitation\n--------\n\nSil C. van de Leemput, Jonas Teuwen, Bram van Ginneken, and Rashindra Manniesing.\nMemCNN: A Python/PyTorch package for creating memory-efficient invertible neural networks.\nJournal of Open Source Software, 4, 1576, http://dx.doi.org/10.21105/joss.01576, 2019.\n\nIf you use our code, please cite:\n\n.. code:: bibtex\n\n    @article{vandeLeemput2019MemCNN,\n      journal = {Journal of Open Source Software},\n      doi = {10.21105/joss.01576},\n      issn = {2475-9066},\n      number = {39},\n      publisher = {The Open Journal},\n      title = {MemCNN: A Python/PyTorch package for creating memory-efficient invertible neural networks},\n      url = {http://dx.doi.org/10.21105/joss.01576},\n      volume = {4},\n      author = {Sil C. {van de} Leemput and Jonas Teuwen and Bram {van} Ginneken and Rashindra Manniesing},\n      pages = {1576},\n      date = {2019-07-30},\n      year = {2019},\n      month = {7},\n      day = {30},\n    }"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Sil C. van de Leemput, Jonas Teuwen, Bram van Ginneken, and Rashindra Manniesing.\nMemCNN: A Python/PyTorch package for creating memory-efficient invertible neural networks.\nJournal of Open Source Software, 4, 1576, http://dx.doi.org/10.21105/joss.01576, 2019.\n\nIf you use our code, please cite:\n\n.. code:: bibtex\n\n    @article{vandeLeemput2019MemCNN,\n      journal = {Journal of Open Source Software},\n      doi = {10.21105/joss.01576},\n      issn = {2475-9066},\n      number = {39},\n      publisher = {The Open Journal},\n      title = {MemCNN: A Python/PyTorch package for creating memory-efficient invertible neural networks},\n      url = {http://dx.doi.org/10.21105/joss.01576},\n      volume = {4},\n      author = {Sil C. {van de} Leemput and Jonas Teuwen and Bram {van} Ginneken and Rashindra Manniesing},\n      pages = {1576},\n      date = {2019-07-30},\n      year = {2019},\n      month = {7},\n      day = {30},\n    }\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{vandeLeemput2019MemCNN,\n  journal = {Journal of Open Source Software},\n  doi = {10.21105/joss.01576},\n  issn = {2475-9066},\n  number = {39},\n  publisher = {The Open Journal},\n  title = {MemCNN: A Python/PyTorch package for creating memory-efficient invertible neural networks},\n  url = {http://dx.doi.org/10.21105/joss.01576},\n  volume = {4},\n  author = {Sil C. {van de} Leemput and Jonas Teuwen and Bram {van} Ginneken and Rashindra Manniesing},\n  pages = {1576},\n  date = {2019-07-30},\n  year = {2019},\n  month = {7},\n  day = {30},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9201896952023112
      ],
      "excerpt": ".. image:: https://readthedocs.org/projects/memcnn/badge/?version=latest       \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8906174419333412
      ],
      "excerpt": "        :target: https://anaconda.org/silvandeleemput/memcnn \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8709273166115206,
        0.8008247398284739,
        0.8111036989382164,
        0.9998818801833417,
        0.9945436264284249,
        0.9999870429394447
      ],
      "excerpt": ".. image:: https://img.shields.io/github/license/silvandeleemput/memcnn.svg       \n        :alt: GitHub - Repository license \n        :target: https://github.com/silvandeleemput/memcnn/blob/master/LICENSE.txt \n.. image:: http://joss.theoj.org/papers/10.21105/joss.01576/status.svg \n        :alt: JOSS - DOI \n        :target: https://doi.org/10.21105/joss.01576 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9972645883837314
      ],
      "excerpt": "Free software: MIT license &lt;https://github.com/silvandeleemput/memcnn/blob/master/LICENSE.txt&gt;__ (please cite our work if you use it) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9562509331748128,
        0.9017912595277376,
        0.9176665473197341
      ],
      "excerpt": "network &lt;https://arxiv.org/abs/1707.04585&gt; running the code from \ntheir GitHub &lt;https://github.com/renmengye/revnet-public&gt;. \nThe PyTorch results listed were recomputed on June 11th 2018, and differ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "|            |               Cifar-10                            |               Cifar-100                     | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.881848456479086
      ],
      "excerpt": "| resnet-32  |  92.74                 |    92.86                 |   69.10              |  69.81               | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9572487334023723
      ],
      "excerpt": "| revnet-110 |  94.02                 |    94.10                 |   74.00              |  73.30               | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "|            |               Cifar-10                            |               Cifar-100                     | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeOfConduct": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://raw.githubusercontent.com/silvandeleemput/memcnn/master/CODE_OF_CONDUCT.md",
    "technique": "File Exploration"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/silvandeleemput/memcnn",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contributing\nContributions are welcome, and they are greatly appreciated! \nEvery little bit helps, and credit will always be given.\nThe latest information about how to contribute to MemCNN can be found here:\nhttps://memcnn.readthedocs.io/en/latest/contributing.html",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-02-26T14:11:31Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-24T05:33:19Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8480399540567647
      ],
      "excerpt": "        :alt: GitHub - Repository license \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9630342750306515,
        0.9194732032179647
      ],
      "excerpt": "Simple toggling of memory saving by setting the keep_input property of the InvertibleModuleWrapper. \nTurn arbitrary non-linear PyTorch functions into invertible versions using the AdditiveCoupling or the AffineCoupling classes. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8515242038188667
      ],
      "excerpt": "CI tests for Python v3.7 and torch v1.0, v1.1, v1.4 and v1.7 with good code coverage. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8842753837859835
      ],
      "excerpt": "from the results in the ICLR paper. The Tensorflow results are still the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9295537883113455
      ],
      "excerpt": "Memory consumption of model training in PyTorch \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9710338119228518,
        0.931942987427483
      ],
      "excerpt": "The ResNet model is the conventional Residual Network implementation in PyTorch, while \nthe RevNet model uses the memcnn.InvertibleModuleWrapper to achieve memory savings. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "PyTorch Framework for Developing Memory Efficient Deep Invertible Networks",
      "technique": "GitHub API"
    }
  ],
  "documentation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "https://memcnn.readthedocs.io/",
      "technique": "Regular expression"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/silvandeleemput/memcnn/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 21,
      "date": "Mon, 27 Dec 2021 15:54:10 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/silvandeleemput/memcnn/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "silvandeleemput/memcnn",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/silvandeleemput/memcnn/master/docker/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/silvandeleemput/memcnn/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8687985544794319
      ],
      "excerpt": ".. image:: https://img.shields.io/circleci/build/github/silvandeleemput/memcnn/master.svg       \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8328635853330358
      ],
      "excerpt": ".. image:: https://readthedocs.org/projects/memcnn/badge/?version=latest       \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8373490594817731,
        0.9354542608836819,
        0.9468523482201536,
        0.8135957893086111,
        0.9456144766519456
      ],
      "excerpt": ".. image:: https://img.shields.io/pypi/v/memcnn.svg \n        :alt: PyPI - Latest release \n        :target: https://pypi.python.org/pypi/memcnn \n.. image:: https://img.shields.io/conda/vn/silvandeleemput/memcnn?label=anaconda \n        :alt: Conda - Latest release \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9468523482201536,
        0.8373490594817731,
        0.9939147543179442,
        0.9468523482201536
      ],
      "excerpt": "        :target: https://pypi.python.org/pypi/memcnn \n.. image:: https://img.shields.io/pypi/pyversions/memcnn.svg       \n        :alt: PyPI - Python version \n        :target: https://pypi.python.org/pypi/memcnn \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9580111292682334
      ],
      "excerpt": "Installation: https://memcnn.readthedocs.io/en/latest/installation.html \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8178387813972405
      ],
      "excerpt": "| revnet-38  |             2:17       |    2:09                  |       2:20           |              2:16    | \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.808663601585936
      ],
      "excerpt": "| revnet-110 |  94.02                 |    94.10                 |   74.00              |  73.30               | \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/silvandeleemput/memcnn/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "TeX",
      "Dockerfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "memcnn",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "silvandeleemput",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/silvandeleemput/memcnn/blob/master/README.rst",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "After installing MemCNN run:\n\n.. code:: bash\n\n    python -m memcnn.train [MODEL] [DATASET] [--fresh] [--no-cuda]\n\n* Available values for ``DATASET`` are ``cifar10`` and ``cifar100``.\n* Available values for ``MODEL`` are ``resnet32``, ``resnet110``, ``resnet164``, ``revnet38``, ``revnet110``, ``revnet164``\n* Use the ``--fresh`` flag to remove earlier experiment results.\n* Use the ``--no-cuda`` flag to train on the CPU rather than the GPU through CUDA.\n\nDatasets are automatically downloaded if they are not available.\n\nWhen using Python 3.* replace the ``python`` directive with the appropriate Python 3 directive. For example when using the MemCNN docker image use ``python3.6``.\n\nWhen MemCNN was installed using `pip` or from sources you might need to setup a configuration file before running this command.\nRead the corresponding section about how to do this here: https://memcnn.readthedocs.io/en/latest/installation.html\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 218,
      "date": "Mon, 27 Dec 2021 15:54:10 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "pytorch",
      "machine-learning",
      "deep-learning",
      "python27",
      "python36"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Creating an AdditiveCoupling with memory savings\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n.. code:: python\n\n    import torch\n    import torch.nn as nn\n    import memcnn\n\n\n    # define a new torch Module with a sequence of operations: Relu o BatchNorm2d o Conv2d\n    class ExampleOperation(nn.Module):\n        def __init__(self, channels):\n            super(ExampleOperation, self).__init__()\n            self.seq = nn.Sequential(\n                                        nn.Conv2d(in_channels=channels, out_channels=channels,\n                                                  kernel_size=(3, 3), padding=1),\n                                        nn.BatchNorm2d(num_features=channels),\n                                        nn.ReLU(inplace=True)\n                                    )\n\n        def forward(self, x):\n            return self.seq(x)\n\n\n    # generate some random input data (batch_size, num_channels, y_elements, x_elements)\n    X = torch.rand(2, 10, 8, 8)\n\n    # application of the operation(s) the normal way\n    model_normal = ExampleOperation(channels=10)\n    model_normal.eval()\n\n    Y = model_normal(X)\n\n    # turn the ExampleOperation invertible using an additive coupling\n    invertible_module = memcnn.AdditiveCoupling(\n        Fm=ExampleOperation(channels=10 // 2),\n        Gm=ExampleOperation(channels=10 // 2)\n    )\n\n    # test that it is actually a valid invertible module (has a valid inverse method)\n    assert memcnn.is_invertible_module(invertible_module, test_input_shape=X.shape)\n\n    # wrap our invertible_module using the InvertibleModuleWrapper and benefit from memory savings during training\n    invertible_module_wrapper = memcnn.InvertibleModuleWrapper(fn=invertible_module, keep_input=True, keep_input_inverse=True)\n\n    # by default the module is set to training, the following sets this to evaluation\n    # note that this is required to pass input tensors to the model with requires_grad=False (inference only)\n    invertible_module_wrapper.eval()\n\n    # test that the wrapped module is also a valid invertible module\n    assert memcnn.is_invertible_module(invertible_module_wrapper, test_input_shape=X.shape)\n\n    # compute the forward pass using the wrapper\n    Y2 = invertible_module_wrapper.forward(X)\n\n    # the input (X) can be approximated (X2) by applying the inverse method of the wrapper on Y2\n    X2 = invertible_module_wrapper.inverse(Y2)\n\n    # test that the input and approximation are similar\n    assert torch.allclose(X, X2, atol=1e-06)\n\n",
      "technique": "Header extraction"
    }
  ]
}