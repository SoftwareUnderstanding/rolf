{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1904.07223",
      "https://arxiv.org/abs/1711.09349",
      "https://arxiv.org/abs/1711.10295",
      "https://arxiv.org/abs/2002.10857",
      "https://arxiv.org/abs/1904.07223",
      "https://arxiv.org/abs/1711.09349",
      "https://arxiv.org/abs/2002.10857",
      "https://arxiv.org/abs/1703.07737"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The following paper uses and reports the result of the baseline model. You may cite it in your paper.\n```bib\n@article{zheng2019joint,\n  title={Joint discriminative and generative learning for person re-identification},\n  author={Zheng, Zhedong and Yang, Xiaodong and Yu, Zhiding and Zheng, Liang and Yang, Yi and Kautz, Jan},\n  journal={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n  year={2019}\n}\n```\n\nThe following papers may be the first two to use the bottleneck baseline. You may cite them in your paper.\n```bib\n@article{DBLP:journals/corr/SunZDW17,\n  author    = {Yifan Sun and\n               Liang Zheng and\n               Weijian Deng and\n               Shengjin Wang},\n  title     = {SVDNet for Pedestrian Retrieval},\n  booktitle   = {ICCV},\n  year      = {2017},\n}\n\n@article{hermans2017defense,\n  title={In Defense of the Triplet Loss for Person Re-Identification},\n  author={Hermans, Alexander and Beyer, Lucas and Leibe, Bastian},\n  journal={arXiv preprint arXiv:1703.07737},\n  year={2017}\n}\n```\n\nBasic Model\n```bib\n@article{zheng2018discriminatively,\n  title={A discriminatively learned CNN embedding for person reidentification},\n  author={Zheng, Zhedong and Zheng, Liang and Yang, Yi},\n  journal={ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)},\n  volume={14},\n  number={1},\n  pages={13},\n  year={2018},\n  publisher={ACM}\n}\n\n@article{zheng2020vehiclenet,\n  title={VehicleNet: Learning Robust Visual Representation for Vehicle Re-identification},\n  author={Zheng, Zhedong and Ruan, Tao and Wei, Yunchao and Yang, Yi and Mei, Tao},\n  journal={IEEE Transaction on Multimedia (TMM)},\n  year={2020}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{zheng2020vehiclenet,\n  title={VehicleNet: Learning Robust Visual Representation for Vehicle Re-identification},\n  author={Zheng, Zhedong and Ruan, Tao and Wei, Yunchao and Yang, Yi and Mei, Tao},\n  journal={IEEE Transaction on Multimedia (TMM)},\n  year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{zheng2018discriminatively,\n  title={A discriminatively learned CNN embedding for person reidentification},\n  author={Zheng, Zhedong and Zheng, Liang and Yang, Yi},\n  journal={ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)},\n  volume={14},\n  number={1},\n  pages={13},\n  year={2018},\n  publisher={ACM}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{hermans2017defense,\n  title={In Defense of the Triplet Loss for Person Re-Identification},\n  author={Hermans, Alexander and Beyer, Lucas and Leibe, Bastian},\n  journal={arXiv preprint arXiv:1703.07737},\n  year={2017}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{DBLP:journals/corr/SunZDW17,\n  author    = {Yifan Sun and\n               Liang Zheng and\n               Weijian Deng and\n               Shengjin Wang},\n  title     = {SVDNet for Pedestrian Retrieval},\n  booktitle   = {ICCV},\n  year      = {2017},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{zheng2019joint,\n  title={Joint discriminative and generative learning for person re-identification},\n  author={Zheng, Zhedong and Yang, Xiaodong and Yu, Zhiding and Zheng, Liang and Yang, Yi and Kautz, Jan},\n  journal={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n  year={2019}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8942856639420041
      ],
      "excerpt": "How to Cite? \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.808684694647591
      ],
      "excerpt": "- DG-Market (10x Large Synethic Dataset from Market CVPR 2019 Oral) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8944178096468923
      ],
      "excerpt": "- Circle Loss (CVPR 2020 Oral) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9222383658450612
      ],
      "excerpt": "  2019 News \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9901059506189113
      ],
      "excerpt": "  2018 & 2017 News \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "2stream Person re-ID  \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/layumi/Person_reID_baseline_pytorch",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-12-29T10:22:41Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-27T06:57:10Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9128969197448303
      ],
      "excerpt": "Tips for training with other datasets \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9264656321735555
      ],
      "excerpt": "- Running the code on Google Colab with Free GPU. Check Here (Thanks to @ronghao233) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8055455732502326
      ],
      "excerpt": "- Float16 to save GPU memory based on apex \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8061940439843257,
        0.8721490872796789,
        0.8265085814971824
      ],
      "excerpt": "Here we provide hyperparameters and architectures, that were used to generate the result.  \nSome of them (i.e. learning rate) are far from optimal. Do not hesitate to change them and see the effect.  \nP.S. With similar structure, we arrived Rank@1=87.74% mAP=69.46% with Matconvnet. (batchsize=8, dropout=0.75)  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8120827509023274
      ],
      "excerpt": "**3 Dec 2021** We add supports for four losses, including triplet loss, contrastive loss, sphere loss and lifted loss. The hyper-parameters are still tunning. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8410880467568421
      ],
      "excerpt": "**17 Aug 2021** We support running code on Google Colab with free GPU. Please check it out at https://github.com/layumi/Person_reID_baseline_pytorch/tree/master/colab . \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9809097304807736
      ],
      "excerpt": "**12 Aug 2021** We have supported the transformer-based model `Swin` by `--use_swin`. The basic performance is 92.73% Rank@1 and 79.71%mAP. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8142607873277091
      ],
      "excerpt": "**11 January 2021** On the Market-1501 dataset, we accelerate the re-ranking processing from **89.2s** to **9.4ms** with one K40m GPU, facilitating the real-time post-processing. The pytorch implementation can be found in [GPU-Re-Ranking](GPU-Re-Ranking/). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9646591272197644
      ],
      "excerpt": "**11 June 2020** People live in the 3D world. We release one new person re-id code [Person Re-identification in the 3D Space](https://github.com/layumi/person-reid-3d), which conduct representation learning in the 3D space. You are welcomed to check out it. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8877710239870854
      ],
      "excerpt": "**01 July 2019:** [My CVPR19 Paper](https://arxiv.org/abs/1904.07223) is online. It is based on this baseline repo as teacher model to provide pseudo label for the generated images to train a better student model. You are welcomed to check out the opensource code at [here](https://github.com/NVlabs/DG-Net). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8285291397101184
      ],
      "excerpt": "**What's new:** Re-ranking is added to evaluation. The re-ranked result is about **Rank@1=90.20% mAP=84.76%**. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8497992345979094
      ],
      "excerpt": "**What's new:** I add some code to generate training curves. The figure will be saved into the model folder when training. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9291517321648396,
        0.8101817113138587
      ],
      "excerpt": "The hyper-parameter of DG-Market --DG is not tuned. Better hyper-parameter may lead to better results. \nI do not optimize the hyper-parameters. You are free to tune them for better performance. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9302287581602335
      ],
      "excerpt": "--which_epoch select the i-th model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9274048119302095
      ],
      "excerpt": "For mAP calculation, you also can refer to the C++ code for Oxford Building. We use the triangle mAP calculation (consistent with the Market1501 original code). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9439104594494018
      ],
      "excerpt": "Notes the format of the camera id and the number of cameras. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Pytorch ReID: A tiny, friendly, strong pytorch implement of object re-identification baseline. Tutorial \ud83d\udc49https://github.com/layumi/Person_reID_baseline_pytorch/tree/master/tutorial",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/layumi/Person_reID_baseline_pytorch/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 849,
      "date": "Mon, 27 Dec 2021 11:28:58 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/layumi/Person_reID_baseline_pytorch/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "layumi/Person_reID_baseline_pytorch",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/layumi/Person_reID_baseline_pytorch/master/GPU-Re-Ranking/extension/make.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Download [Market1501 Dataset](http://www.liangzheng.com.cn/Project/project_reid.html) [[Google]](https://drive.google.com/file/d/0B8-rUzbwVRk0c054eEozWG9COHM/view) [[Baidu]](https://pan.baidu.com/s/1ntIi2Op)\n\nPreparation: Put the images with the same id in one folder. You may use \n```bash\npython prepare.py\n```\nRemember to change the dataset path to your own path.\n\nFuthermore, you also can test our code on [DukeMTMC-reID Dataset]( [GoogleDriver](https://drive.google.com/open?id=1jjE85dRCMOgRtvJ5RQV9-Afs-2_5dY3O) or ([BaiduYun](https://pan.baidu.com/s/1jS0XM7Var5nQGcbf9xUztw) password: bhbh)).\nOur baseline code is not such high on DukeMTMC-reID **Rank@1=64.23%, mAP=43.92%**. Hyperparameters are need to be tuned.\n\n- [Optional] [DG-Market](https://github.com/NVlabs/DG-Net#dg-market) is a generated pedestrian dataset of 128,307 images for training a robust model.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "- Install Pytorch from http://pytorch.org/\n- Install Torchvision from the source\n```\ngit clone https://github.com/pytorch/vision\ncd vision\npython setup.py install\n```\n- [Optinal] You may skip it. Install apex from the source\n```\ngit clone https://github.com/NVIDIA/apex.git\ncd apex\npython setup.py install --cuda_ext --cpp_ext\n```\nBecause pytorch and torchvision are ongoing projects.\n\nHere we noted that our code is tested based on Pytorch 0.3.0/0.4.0/0.5.0/1.0.0 and Torchvision 0.2.0/0.2.1 .\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.920992687207367
      ],
      "excerpt": "- Float16 to save GPU memory based on apex \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9795606441012735
      ],
      "excerpt": "- Re-Ranking (GPU Version) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8271774125452266
      ],
      "excerpt": "**17 Aug 2021** We support running code on Google Colab with free GPU. Please check it out at https://github.com/layumi/Person_reID_baseline_pytorch/tree/master/colab . \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8175574300715083
      ],
      "excerpt": "**23 Jun 2021** Attack your re-ID model via Query! They are not robust as you expected! Check the code at [Here](https://github.com/layumi/A_reID). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9201079499915507
      ],
      "excerpt": "**11 January 2021** On the Market-1501 dataset, we accelerate the re-ranking processing from **89.2s** to **9.4ms** with one K40m GPU, facilitating the real-time post-processing. The pytorch implementation can be found in [GPU-Re-Ranking](GPU-Re-Ranking/). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9642665283620424
      ],
      "excerpt": "**What's new:** FP16 has been added. It can be used by simply added `--fp16`. You need to install [apex](https://github.com/NVIDIA/apex) and update your pytorch to 1.0.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8587956492739945,
        0.8352188776989673,
        0.8181060374670783,
        0.8257700426724666
      ],
      "excerpt": "| [ResNet-50-ibn] | 89.13% | 73.40% | python train.py --train_all --name res-ibn --ibn | \n| [DenseNet-121] | 90.17% | 74.02% | python train.py --name ft_net_dense --use_dense --train_all | \n| [DenseNet-121 (Circle)] | 91.00% | 76.54% | python train.py --name ft_net_dense_circle_w5 --circle --use_dense --train_all --warm_epoch 5 | \n| [HRNet-18] | 90.83% | 76.65% |  python train.py --use_hr --name hr18; python test.py --name hr18 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8211230948159045
      ],
      "excerpt": "| [ResNet-50 (all tricks+Circle+DG)] | 92.13% | 80.13% | python train.py --warm_epoch 5 --stride 1 --erasing_p 0.5 --batchsize 8 --lr 0.02 --name warm5_s1_b8_lr2_p0.5_circle_DG --circle --DG; python test.py --name warm5_s1_b8_lr2_p0.5_circle_DG | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8018624946757805,
        0.8195479637120197
      ],
      "excerpt": "| [Swin] | 92.73% | 79.71% | python train.py --use_swin --name swin; python test.py --name swin| \n| [Swin (all tricks+Circle)] | 93.65% | 83.65% | python train.py --use_swin --name swin_p0.5_circle_w5  --erasing_p 0.5 --circle --warm_epoch 5;  python test.py --name swin_p0.5_circle_w5| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9751071074704663
      ],
      "excerpt": "Swin costs more GPU memory (11G GPU is needed) to run.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9427820148741699
      ],
      "excerpt": "--gpu_ids which gpu to run. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9427820148741699
      ],
      "excerpt": "--gpu_ids which gpu to run. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8194842505323139
      ],
      "excerpt": "Dataset Preparation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9503189345333785
      ],
      "excerpt": "python train.py --fp16 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9503189345333785,
        0.9515752551715031,
        0.9333574343066754
      ],
      "excerpt": "python train.py \npython test.py \npython demo.py --query_index 777 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9503189345333785,
        0.8484693396717187,
        0.9246227682586091
      ],
      "excerpt": "python train.py \npython test.py --multi \npython evaluate_gpu.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.915295314721357,
        0.9499161596653152
      ],
      "excerpt": "python train.py --PCB --batchsize 64 --name PCB-64 \npython test.py --PCB --name PCB-64 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8447580893318155
      ],
      "excerpt": "**What's new:** I add some code to generate training curves. The figure will be saved into the model folder when training. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9712163970742236,
        0.9037758835109134,
        0.9184817193922838,
        0.892259828922456,
        0.9502883860429022,
        0.9390074103975046,
        0.9598068474694303,
        0.9419495422253252,
        0.9589269091038813,
        0.9230115967333467,
        0.9013715303279086,
        0.9273513803622011,
        0.9668306461421022,
        0.9709409135708438,
        0.9685597506178977,
        0.976978533305417,
        0.9754151138174026
      ],
      "excerpt": "| [EfficientNet-b4] | 85.78% | 66.80% |  python train.py --use_efficient --name eff; python test.py --name eff | \n| [ResNet-50 (fp16)] | 88.03% | 71.40% | python train.py --name fp16 --fp16 --train_all | \n| [ResNet-50] | 88.84% | 71.59% |  python train.py --train_all | \n| [ResNet-50-ibn] | 89.13% | 73.40% | python train.py --train_all --name res-ibn --ibn | \n| [DenseNet-121] | 90.17% | 74.02% | python train.py --name ft_net_dense --use_dense --train_all | \n| [DenseNet-121 (Circle)] | 91.00% | 76.54% | python train.py --name ft_net_dense_circle_w5 --circle --use_dense --train_all --warm_epoch 5 | \n| [HRNet-18] | 90.83% | 76.65% |  python train.py --use_hr --name hr18; python test.py --name hr18 | \n| [PCB] | 92.64% | 77.47% | python train.py --name PCB --PCB --train_all --lr 0.02 | \n| [PCB + DG] | 92.70% | 78.31% | python train.py --name PCB_DG --PCB --train_all --lr 0.02 --DG; python test.py --name PCB_DG | \n| [ResNet-50 (all tricks)] | 91.83% | 78.32% | python train.py --warm_epoch 5 --stride 1 --erasing_p 0.5 --batchsize 8 --lr 0.02 --name warm5_s1_b8_lr2_p0.5 | \n| [ResNet-50 (all tricks+Circle)] | 92.13% | 79.84% | python train.py --warm_epoch 5 --stride 1 --erasing_p 0.5 --batchsize 8 --lr 0.02 --name warm5_s1_b8_lr2_p0.5_circle  --circle | \n| [ResNet-50 (all tricks+Circle+DG)] | 92.13% | 80.13% | python train.py --warm_epoch 5 --stride 1 --erasing_p 0.5 --batchsize 8 --lr 0.02 --name warm5_s1_b8_lr2_p0.5_circle_DG --circle --DG; python test.py --name warm5_s1_b8_lr2_p0.5_circle_DG | \n| [HRNet-18 (all tricks+Circle+DG)]| 92.19% | 81.00% | python train.py --use_hr --name  hr18_p0.5_circle_w5_b16_lr0.01_DG --lr 0.01 --batch 16 --DG --erasing_p 0.5 --circle --warm_epoch 5; python test.py --name  hr18_p0.5_circle_w5_b16_lr0.01_DG | \n| [Swin] | 92.73% | 79.71% | python train.py --use_swin --name swin; python test.py --name swin| \n| [Swin (all tricks+Circle)] | 93.65% | 83.65% | python train.py --use_swin --name swin_p0.5_circle_w5  --erasing_p 0.5 --circle --warm_epoch 5;  python test.py --name swin_p0.5_circle_w5| \n| [Swin (all tricks+Circle+b16)] | 93.91% | 85.17% | python train.py --use_swin --name swin_p0.5_circle_w5_b16_lr0.01 --lr 0.01 --batch 16  --erasing_p 0.5 --circle --warm_epoch 5; python test.py --name swin_p0.5_circle_w5_b16_lr0.01| \n| [Swin (all tricks+Circle+b16+DG)] | 94.00% | 85.36% | python train.py --use_swin --name swin_p0.5_circle_w5_b16_lr0.01_DG --lr 0.01 --batch 16 --DG --erasing_p 0.5 --circle --warm_epoch 5; python test.py --name swin_p0.5_circle_w5_b16_lr0.01_DG| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9823655076888318,
        0.9709048166062237,
        0.9648594776647297,
        0.9648594776647297,
        0.9335177897216109,
        0.9578749187016199,
        0.9497141029096813,
        0.9503405391921057,
        0.9382269106891071,
        0.9342810660712952
      ],
      "excerpt": "| CE | 92.01% | 79.31% | python train.py --warm_epoch 5 --stride 1 --erasing_p 0.5 --batchsize 32 --lr 0.08 --name warm5_s1_b32_lr8_p0.5_100 --total 100 ; python test.py  --name  warm5_s1_b32_lr8_p0.5_100| \n| CE + Sphere [Paper] | 92.01% | 79.39% | python train.py --warm_epoch 5 --stride 1 --erasing_p 0.5 --batchsize 32 --lr 0.08 --name warm5_s1_b32_lr8_p0.5_sphere100 --sphere --total 100; python test.py --name warm5_s1_b32_lr8_p0.5_sphere100 | \n| CE + Triplet [Paper] | 92.40% | 79.71% | python train.py --warm_epoch 5 --stride 1 --erasing_p 0.5 --batchsize 32 --lr 0.08 --name warm5_s1_b32_lr8_p0.5_triplet100 --triplet --total 100; python test.py  --name warm5_s1_b32_lr8_p0.5_triplet100 | \n| CE + Lifted [Paper]|  91.78% | 79.77% | python train.py --warm_epoch 5 --stride 1 --erasing_p 0.5 --batchsize 32 --lr 0.08 --name warm5_s1_b32_lr8_p0.5_lifted100 --lifted --total 100; python test.py --name warm5_s1_b32_lr8_p0.5_lifted100 | \n| CE + Contrast [Paper] | 92.28% | 81.42% | python train.py --warm_epoch 5 --stride 1 --erasing_p 0.5 --batchsize 32 --lr 0.08 --name warm5_s1_b32_lr8_p0.5_contrast100 --contrast  --total 100; python test.py  --name warm5_s1_b32_lr8_p0.5_contrast100| \n| CE + Circle [Paper] | 92.46% | 81.70% | python train.py --warm_epoch 5 --stride 1 --erasing_p 0.5 --batchsize 32 --lr 0.08 --name warm5_s1_b32_lr8_p0.5_circle100 --circle --total 100 ; python test.py  --name  warm5_s1_b32_lr8_p0.5_circle100 | \n| CE + Contrast + Sphere | 92.79% | 82.02% | python train.py --warm_epoch 5 --stride 1 --erasing_p 0.5 --batchsize 32 --lr 0.08 --name warm5_s1_b32_lr8_p0.5_cs100 --contrast --sphere --total 100; python test.py --name warm5_s1_b32_lr8_p0.5_cs100| \n| CE + Contrast + Triplet (Long) | 92.61% | 82.01% | python train.py --warm_epoch 5 --stride 1 --erasing_p 0.5 --batchsize 24 --lr 0.062 --name warm5_s1_b24_lr6.2_p0.5_contrast_triplet_133 --contrast --triplet --total 133 ; python test.py  --name  warm5_s1_b24_lr6.2_p0.5_contrast_triplet_133 | \n| CE + Contrast + Circle (Long) | 92.19% | 82.07% | python train.py --warm_epoch 5 --stride 1 --erasing_p 0.5 --batchsize 24 --lr 0.08 --name warm5_s1_b24_lr8_p0.5_contrast_circle133 --contrast --circle --total 133 ; python test.py  --name  warm5_s1_b24_lr8_p0.5_contrast_circle133 | \n| CE + Contrast + Sphere (Long) | 92.84% | 82.37% | python train.py --warm_epoch 5 --stride 1 --erasing_p 0.5 --batchsize 24 --lr 0.06 --name warm5_s1_b24_lr6_p0.5_contrast_sphere133 --contrast --sphere --total 133 ; python test.py  --name  warm5_s1_b24_lr6_p0.5_contrast_sphere133 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8081624365215927
      ],
      "excerpt": "Train a model by \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9293006804235666
      ],
      "excerpt": "python train.py --gpu_ids 0 --name ft_ResNet50 --train_all --batchsize 32  --data_dir your_data_path \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8420446505342062
      ],
      "excerpt": "--name the name of model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.817700032297289
      ],
      "excerpt": "--train_all using all images to train.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9293006804235666
      ],
      "excerpt": "python train.py --gpu_ids 0 --name ft_ResNet50 --train_all --batchsize 32  --data_dir your_data_path --erasing_p 0.5 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9299984643403167
      ],
      "excerpt": "python test.py --gpu_ids 0 --name ft_ResNet50 --test_dir your_data_path  --batchsize 32 --which_epoch 59 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8645259509180295
      ],
      "excerpt": "--name the dir name of trained model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8708148446064828
      ],
      "excerpt": "python evaluate.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python evaluate_rerank.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8264859728702388
      ],
      "excerpt": "For some dataset, e.g., MSMT17, there are more than 10 cameras. You need to modify the prepare.py and test.py to read the double-digit camera ID. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/layumi/Person_reID_baseline_pytorch/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Cuda",
      "C++",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2018 Zhedong Zheng\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Tutorial",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Person_reID_baseline_pytorch",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "layumi",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/layumi/Person_reID_baseline_pytorch/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "layumi",
        "body": "We add supports for four losses, including triplet loss, contrastive loss, sphere loss and lifted loss. The hyper-parameters are still tunning.\r\n\r\n## What's Changed\r\n* Random grayscale erasure by @CynicalHeart in https://github.com/layumi/Person_reID_baseline_pytorch/pull/301\r\n\r\n## New Contributors\r\n* @CynicalHeart made their first contribution in https://github.com/layumi/Person_reID_baseline_pytorch/pull/301\r\n\r\n**Full Changelog**: https://github.com/layumi/Person_reID_baseline_pytorch/compare/v1.0.1...v1.0.2",
        "dateCreated": "2021-12-03T15:47:36Z",
        "datePublished": "2021-12-03T15:49:31Z",
        "html_url": "https://github.com/layumi/Person_reID_baseline_pytorch/releases/tag/v1.0.2",
        "name": "We add supports for four losses",
        "tag_name": "v1.0.2",
        "tarball_url": "https://api.github.com/repos/layumi/Person_reID_baseline_pytorch/tarball/v1.0.2",
        "url": "https://api.github.com/repos/layumi/Person_reID_baseline_pytorch/releases/54588840",
        "zipball_url": "https://api.github.com/repos/layumi/Person_reID_baseline_pytorch/zipball/v1.0.2"
      },
      {
        "authorType": "User",
        "author_name": "layumi",
        "body": "Add supports for Swin Transformer / EfficientNet / HRNet\r\n\r\n## What's Changed\r\n* Create LICENSE by @layumi in https://github.com/layumi/Person_reID_baseline_pytorch/pull/1\r\n* Create random_erasing.py by @zhunzhong07 in https://github.com/layumi/Person_reID_baseline_pytorch/pull/6\r\n* add random erasing by @zhunzhong07 in https://github.com/layumi/Person_reID_baseline_pytorch/pull/8\r\n* add random erasing by @zhunzhong07 in https://github.com/layumi/Person_reID_baseline_pytorch/pull/7\r\n* Update train.py by @zhangchuangnankai in https://github.com/layumi/Person_reID_baseline_pytorch/pull/63\r\n* The pytorch implementation of GNN-Re-Ranking. by @Xuanmeng-Zhang in https://github.com/layumi/Person_reID_baseline_pytorch/pull/250\r\n* Update the code for GNN-Re-Ranking. by @Xuanmeng-Zhang in https://github.com/layumi/Person_reID_baseline_pytorch/pull/251\r\n* Add the code for GPU-Re-Ranking by @Xuanmeng-Zhang in https://github.com/layumi/Person_reID_baseline_pytorch/pull/252\r\n* update filename by @Xuanmeng-Zhang in https://github.com/layumi/Person_reID_baseline_pytorch/pull/255\r\n* Change application by @ronghao233 in https://github.com/layumi/Person_reID_baseline_pytorch/pull/278\r\n\r\n## New Contributors\r\n* @layumi made their first contribution in https://github.com/layumi/Person_reID_baseline_pytorch/pull/1\r\n* @zhunzhong07 made their first contribution in https://github.com/layumi/Person_reID_baseline_pytorch/pull/6\r\n* @zhangchuangnankai made their first contribution in https://github.com/layumi/Person_reID_baseline_pytorch/pull/63\r\n* @Xuanmeng-Zhang made their first contribution in https://github.com/layumi/Person_reID_baseline_pytorch/pull/250\r\n* @ronghao233 made their first contribution in https://github.com/layumi/Person_reID_baseline_pytorch/pull/278\r\n\r\n**Full Changelog**: https://github.com/layumi/Person_reID_baseline_pytorch/commits/v1.0.1",
        "dateCreated": "2021-12-01T11:01:16Z",
        "datePublished": "2021-12-01T11:06:26Z",
        "html_url": "https://github.com/layumi/Person_reID_baseline_pytorch/releases/tag/v1.0.1",
        "name": "V1.0.1 Add supports for Swin Transformer / EfficientNet / HRNet",
        "tag_name": "v1.0.1",
        "tarball_url": "https://api.github.com/repos/layumi/Person_reID_baseline_pytorch/tarball/v1.0.1",
        "url": "https://api.github.com/repos/layumi/Person_reID_baseline_pytorch/releases/54398976",
        "zipball_url": "https://api.github.com/repos/layumi/Person_reID_baseline_pytorch/zipball/v1.0.1"
      }
    ],
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Python 3.6\n- GPU Memory >= 6G\n- Numpy\n- Pytorch 0.3+\n- timm `pip install timm` for Swin-Transformer with Pytorch >1.7.0\n- pretrainedmodels via `pip install pretrainedmodels`\n- [Optional] apex (for float16) \n- [Optional] [pretrainedmodels](https://github.com/Cadene/pretrained-models.pytorch)\n\n**(Some reports found that updating numpy can arrive the right accuracy. If you only get 50~80 Top1 Accuracy, just try it.)**\nWe have successfully run the code based on numpy 1.12.1 and 1.13.1 .\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2980,
      "date": "Mon, 27 Dec 2021 11:28:58 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "open-reid",
      "pytorch",
      "person-reidentification",
      "image-retrieval",
      "person-reid",
      "re-ranking",
      "random-erasing",
      "image-search",
      "market-1501",
      "tutorial",
      "apex",
      "baseline",
      "msmt17",
      "cuhk-np",
      "vehicle-reid",
      "awesome-list",
      "gait-recognition",
      "circle-loss",
      "metric-learning",
      "object-reid"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* [8 min Tutorial](https://github.com/layumi/Person_reID_baseline_pytorch/blob/master/tutorial/README.md)\uff0c[8\u5206\u949f\u6559\u7a0b](https://zhuanlan.zhihu.com/p/50387521)\n* [\u4e2d\u6587\u89c6\u9891\u7b80\u4ecb](https://www.bilibili.com/video/BV11K4y1f7eQ)\n* [Answer to Tutorial](https://github.com/layumi/Person_reID_baseline_pytorch/blob/master/tutorial/Answers_to_Quick_Questions.md)\n\n",
      "technique": "Header extraction"
    }
  ]
}