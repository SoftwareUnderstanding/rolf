{
  "citation": [
    {
      "confidence": [
        0.9903619484840082
      ],
      "excerpt": "Gonz\u00e1lez-Rivero M, Beijbom O, Rodriguez-Ramirez A, Bryant DEP, Ganase A, Gonzalez-Marrero Y, Herrera-Reveles A, Kennedy EV, Kim C, Lopez-Marcano S, Markey K, Neal BP, Osborne K, Reyes-Nivia C, Sampayo EM, Stolberg K, Taylor A, Vercelloni J, Wyatt M and Hoegh-Guldberg O. (2019) Monitoring of coral reefs using artificial intelligence: a feasible and cost-effective approach Remote Sensing in review \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8334316807263773
      ],
      "excerpt": "|2|38001004402|2017-06-20 01:33:56+00:00|Obs 2|235|450|Tabular Acropora|TB_Acr|Hard Coral|38001004402.jpg|random \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mgonzalezrivero/reef_learning",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We hope this repo can be useful for your work. Feel free to contact us if you need any help or want to discuss the applications of this code to your specific work. \n\n*Manuel Gonzalez-Rivero*   \nAustralian Institute of Marine Science.  \n[m.gonzalezrivero@aims.gov.au](mailto:m.gonzalezrivero@aims.gov.au)\n",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-08-02T05:49:58Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-13T20:20:27Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.998549134661695,
        0.9990360313971856,
        0.9803664240597485
      ],
      "excerpt": "This repository contains a series of tools and wrappers for implementing Deep Learning in ecology as a tool to automate the annotation of images and estimate the abundance of organisms, sensu the point-count method.  \nMachine learning is increasingly being implemented in a number of applications in ecology. The intention of this work is to facilitate the uptake of this technology for ecological studies that rely on the analysis of images (i.e., photoquadrats) to extract abundance of organism (e.g., cover). While this work is particularly centred on benthic ecology of coral reefs, the same approach can be applied to plant ecology, temperate marine ecology, and other similar fields.  \nThe repository derives from the publication below, a study designed to evaluate the applications of automated image annotation for coral reef monitoring. This README file will guide you through the configuration and code execution, using our data as examples, to: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8232489773308188,
        0.8044097364259412,
        0.9909386657476604,
        0.9938836958941188,
        0.8960679309297217
      ],
      "excerpt": "Test and select CNN models  \nDeploy selected CCN model to classify new images and estimate percent cover \nThe repository provides a series of scripts easy to implement and designed to automate estimations of cover from images and to reproduce the results from the publication below. Should you prefer to avoid configuring and executing this application, deep learning has also been implemented in two online and open access annotation platforms, designed by  \nOscar Beijbom and Mat Wyatt, co-authors of this work. The platforms, CoralNet and BenthoBox, offer automated image annotation capabilities for extracting benthic cover data, as well as, a series of helper tools to facilitate the annotations, evaluation and data synthesis. As such, CoralNet and BenthoBox are stand-alone and user-friendly applications readily available for automated image annotations of benthic images. \nImportantly, while CoralNet and BenthoBox use Deep Learning for automated image classification, there are technical differences in the approach (e.g., Net architecture, initialization). Therefore, we do not expect that the classification from these platforms will reproduce exactly the same results from this publication, although similar. Some advantages from using the source code provided here are:   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8468301057030031
      ],
      "excerpt": "Adaptability to specific needs.   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8836638585827621
      ],
      "excerpt": "Gonz\u00e1lez-Rivero M, Beijbom O, Rodriguez-Ramirez A, Bryant DEP, Ganase A, Gonzalez-Marrero Y, Herrera-Reveles A, Kennedy EV, Kim C, Lopez-Marcano S, Markey K, Neal BP, Osborne K, Reyes-Nivia C, Sampayo EM, Stolberg K, Taylor A, Vercelloni J, Wyatt M and Hoegh-Guldberg O. (2019) Monitoring of coral reefs using artificial intelligence: a feasible and cost-effective approach Remote Sensing in review \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9920328204834512,
        0.9909165530674615,
        0.8993318410824075,
        0.9936910495587364,
        0.9805655143277993
      ],
      "excerpt": "This work is based on Caffe, a deep learning architecture designed for automated image classification, developed by the Berkeley AI Research group. Based on this architecture, we have written a series of functions and wrappers, written in Python, that execute caffe for training and predictions.  For more information about Caffe and Deep Learning networks, have a look a this comprenhensive overview from Caffe.   \nCaffe architecture is designed for scene classification, i.e. assigning a class to the whole image. Here, however, we are interested in learning to automate random point annotation, i.e. the assignment of one class to a particular location in an image. This method, also referred as random point count, is commonly used in many population estimation applications using photographic records. In random point annotations, the relative cover or abundance of each class is defined by the number of points classified as such relative to the total number of observed points on the image. In this study, the relative cover of benthic groups or labels was estimated using 50 points per image.   \nIn order to achieve automated random point annotation, we converted each image to a set of patches (224 x 224 pixels) cropped out around each given point location. Each patch was classified independently and the relative abundance for each of the benthic classifications was estimated by the ratio between the number of patches classified for a given class by the total number of patches evaluated in an image (see figure below): \nThis work is based on two type of data: \nImages: these are conceived here as what we typically refer to photo-quadrats in ecology: scaled images of a substrate used to estimate the abundance and size of organisms and substrate types. Here is an example from our data:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9903386077988771
      ],
      "excerpt": "Annotations: For this work, we are using point-based annotations from each image to train the CNN to recognise organisms and substrate type. For this to work within this framework, each annotated image that we use either for training or testing the CNNs need to have information about the location of each annotation point (Row and Columns in pixels) and the label assigned to it. Here is a sample from the annotation data used here: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9608930666261878
      ],
      "excerpt": "This work is based on using a VGG-16 network architecture. Instead of having many hyper parameters, VGG-16 use a much simpler network, just having convoluted layers that are just 3 x 3 filters with stride, using the same padding. While VGG-16 is a very deep network, with about 138 million parameters, VGG is a very simplified architecture. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9650852210503307,
        0.8682713099408841
      ],
      "excerpt": "Note: This repository is designed as a python module that contains sub-modules for executing caffe using pycaffe. \nconfig: Contains markdown files to guide you through the configuration of the AWS instance. More intructions for local installation here \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8341832476952334
      ],
      "excerpt": "experiments: Python module designed as a sweeper to train the NET under a range of values for base learning rate as well as image size. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "ReefLearning is a python module for automated estimations of coral reef benthic cover from images, using a deep learning framework",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mgonzalezrivero/reef_learning/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Tue, 21 Dec 2021 10:19:14 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/mgonzalezrivero/reef_learning/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "mgonzalezrivero/reef_learning",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/mgonzalezrivero/reef_learning/master/notebooks/compare_nets.ipynb",
      "https://raw.githubusercontent.com/mgonzalezrivero/reef_learning/master/notebooks/train_nets.ipynb",
      "https://raw.githubusercontent.com/mgonzalezrivero/reef_learning/master/notebooks/deploy_net.ipynb",
      "https://raw.githubusercontent.com/mgonzalezrivero/reef_learning/master/notebooks/.ipynb_checkpoints/deploy_net-checkpoint.ipynb",
      "https://raw.githubusercontent.com/mgonzalezrivero/reef_learning/master/notebooks/.ipynb_checkpoints/train_nets-checkpoint.ipynb",
      "https://raw.githubusercontent.com/mgonzalezrivero/reef_learning/master/notebooks/.ipynb_checkpoints/compare_nets-checkpoint.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/mgonzalezrivero/reef_learning/master/toolbox/parse_log.sh",
      "https://raw.githubusercontent.com/mgonzalezrivero/reef_learning/master/toolbox/launch_resize_and_crop_images.sh",
      "https://raw.githubusercontent.com/mgonzalezrivero/reef_learning/master/toolbox/color_balance.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8481820176875737
      ],
      "excerpt": "Note: Make sure your dataset follows the same structure as shown above, as these scripts are hard-coded to this structure. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8744853561147636
      ],
      "excerpt": "Below you can see a VGG diagram representing the 16 layers (more details here): \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8499345457184646
      ],
      "excerpt": "<div style=\"text-align:center\"><img src =\"http://grr-images-au.s3.amazonaws.com/15025/0150250068-quadrat2.jpg\" /></div> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/mgonzalezrivero/reef_learning/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Reef Learning:",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "reef_learning",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "mgonzalezrivero",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mgonzalezrivero/reef_learning/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "GPU capabilties are highly recommended for this work. However, caffe can be configured to run for only using CPU. \n\nAs a reference, we have processed all images for this work in Amazon Web Services, using [AWS Cloud Computing p2-large instances](https://aws.amazon.com/ec2/instance-types/p2/). Also, the same work was replicated locally using NVIDIA Titan X graphic cards. \n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 8,
      "date": "Tue, 21 Dec 2021 10:19:14 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "deep-learning",
      "coralreefs",
      "monitoring"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Here is an overview on how to do automated image analysis of photoquadrats to estimate abundance (i.e., cover). More detailed steps will be detailed in documents within the repository and referred to in this summary. \n\n1. __Configure the machine:__ Prepare either your local machine or the AWS instance by installing Caffe and other dependencies needed to execute this code\u0010. Please refer to Caffe installation instructions [here](http://caffe.berkeleyvision.org/installation.html). We have also created a configurations guide for the AWS instance using a Bitfusion imafe. (Details in  `config.md` )\n2. __Download data:__ Here we are using the data from the publication above as an example. However, you can use your own dataset. If this is the case, follow the data structure of our data repository. You can download our dataset from [here](https://nextcloud.qriscloud.org.au/index.php/s/YMgU7ZpdxSjPwpu).\n\n>__Note:__ The data provided includes the entire dataset for all the global regions analysed in this paper. For each region, or country within a region, we trained and tested a different Deep Learning network. \n\nOnce the AWS instance and data have been configured, the Jupyter Notebooks, in the `notebooks` folder, will provide a guide and an example to run the source code from this repository. Here is a brief description of each notebook to automatically extract cover data from your images:\n\n1. __Train Deep Learning Nets (`train_nets.pynb`):__  \nUsing the training images and annotations, this step will perform a number of iterations to fine-tune the base Net `model_zoo/VGG_ILSVRC_16_layers.caffemodel`. This base Net is a VGG-16 network initialized or trained with the ImageNet dataset (1M images and 1K classification units or labels). For more details about this Net, please refer to the [Caffe Model Zoo documentation](https://github.com/BVLC/caffe/wiki/Model-Zoo) and the developers [arxiv paper](http://arxiv.org/pdf/1409.1556). This code will fine-tune the base Net using the training images and annotations provided. In the process of training the net, the last layer of the base net gets replaces by the classifciation units of our data (labels) and the hyperparameters of the network will be adjusted by back-propagation. During the training, two more hyperparameters are recommended calibrate (e.g., number of iterations, base Learning Rate, Image Scale). to calibrate such parameters, the code will train  multiple nets using a range of parameter values. This step is refered here as \"Experiments\", and the resulting nets from these experiments will then be screened to select the best network.  \n\n2. __Screen and select best Net configuration on their performance (`compare_nets.pynb`):__  \nAll Nets produced in the step above, using different configuration parameters, will be contrasted in this step. Different performance metrics will be produced based on the net classification on manually classified images. \nOnce the best Net is selected, the final step is to evaluate the performance of the Net by using the error metric described in the publication. This error is a measure of the absolute difference between automated and manual estimations of cover by the machine and expert human, respectively. For this work, we are no longer looking at the accuracy of the machine at the point level (as determined by the confusion matrix), but rather the precision of the automated estimations of benthic cover for each label within a sample unit. In our case, a sample unit is a 50m transect containing a number of images. To evaluate the performance error, a set of transects are set aside, called \"test-cells\". The images and manual annotations from these test-cells are stored in the test folder, as downloaded from our dataset. The predicted cover estimations will then be contrasted with the manual cover estimations on the test images.\n3. __Classify new images (`deploy_net.pynb`):__ \nThe selected Net will be used to predict labels on a given number of random points from a specified set of images that are contained in a folder called `data`. After running the classifier on the new images, the scripts will produce a folder called `coverages` that will contain a text file for every new image with the location and classification of each point. Use these files to estimate benthic coverage as described in the manuscript associated to this repository\n\n",
      "technique": "Header extraction"
    }
  ]
}