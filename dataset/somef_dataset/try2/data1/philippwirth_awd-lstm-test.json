{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1708.02182",
      "https://arxiv.org/abs/1803.08240",
      "https://arxiv.org/abs/1612.04426",
      "https://arxiv.org/abs/1708.02182",
      "https://arxiv.org/abs/1803.08240"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{merityAnalysis,\n  title={{An Analysis of Neural Language Modeling at Multiple Scales}},\n  author={Merity, Stephen and Keskar, Nitish Shirish and Socher, Richard},\n  journal={arXiv preprint arXiv:1803.08240},\n  year={2018}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{merityRegOpt,\n  title={{Regularizing and Optimizing LSTM Language Models}},\n  author={Merity, Stephen and Keskar, Nitish Shirish and Socher, Richard},\n  journal={arXiv preprint arXiv:1708.02182},\n  year={2017}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9977354257352328,
        0.8785122982810534,
        0.9973638085240776,
        0.9999537354568557,
        0.9959021299438506
      ],
      "excerpt": "If you use this code or our results in your research, please cite as appropriate: \n  title={{An Analysis of Neural Language Modeling at Multiple Scales}}, \n  author={Merity, Stephen and Keskar, Nitish Shirish and Socher, Richard}, \n  journal={arXiv preprint arXiv:1803.08240}, \n  year={2018} \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/philippwirth/awd-lstm-test",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-04-25T19:06:30Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-04-25T19:17:02Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9326727486428142,
        0.8964846297883073,
        0.9290885857716861
      ],
      "excerpt": "This repository contains the code used for two Salesforce Research papers: \n+ Regularizing and Optimizing LSTM Language Models \n+ An Analysis of Neural Language Modeling at Multiple Scales \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.825758870484835,
        0.9345552078579791,
        0.8438133895703962,
        0.9622729589931469
      ],
      "excerpt": "The model comes with instructions to train: \n+ word level language models over the Penn Treebank (PTB), WikiText-2 (WT2), and WikiText-103 (WT103) datasets \ncharacter level language models over the Penn Treebank (PTBC) and Hutter Prize dataset (enwik8) \nThe model can be composed of an LSTM or a Quasi-Recurrent Neural Network (QRNN) which is two or more times faster than the cuDNN LSTM in this setup while achieving equivalent or better accuracy. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8111396861526387,
        0.9417042596013939
      ],
      "excerpt": "(Optionally) Finetune the model using finetune.py \n(Optionally) Apply the continuous cache pointer to the finetuned model using pointer.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9068394917744174,
        0.9378103597262917
      ],
      "excerpt": "The codebase is now PyTorch 0.4 compatible for most use cases (a big shoutout to https://github.com/shawntan for a fairly comprehensive PR https://github.com/salesforce/awd-lstm-lm/pull/43). Mild readjustments to hyperparameters may be necessary to obtain quoted performance. If you desire exact reproducibility (or wish to run on PyTorch 0.3 or lower), we suggest using an older commit of this repository. We are still working on pointer, finetune and generate functionalities. \nThe codebase was modified during the writing of the paper, preventing exact reproduction due to minor differences in random seeds or similar. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8363225255712923,
        0.9198436445666125,
        0.9598413814546597,
        0.9128763495250674
      ],
      "excerpt": "Next, decide whether to use the QRNN or the LSTM as the underlying recurrent neural network model. \nThe QRNN is many times faster than even Nvidia's cuDNN optimized LSTM (and dozens of times faster than a naive LSTM implementation) yet achieves similar or better results than the LSTM for many word level datasets. \nAt the time of writing, the QRNN models use the same number of parameters and are slightly deeper networks but are two to four times faster per epoch and require less epochs to converge. \nThe QRNN model uses a QRNN with convolutional size 2 for the first layer, allowing the model to view discrete natural language inputs (i.e. \"New York\"), while all other layers use a convolutional size of 1. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9870690753066664
      ],
      "excerpt": "The instruction below trains a PTB model that without finetuning achieves perplexities of approximately 61.2 / 58.8 (validation / testing), with finetuning achieves perplexities of approximately 58.8 / 56.5, and with the continuous cache pointer augmentation achieves perplexities of approximately 53.2 / 52.5. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9870690753066664
      ],
      "excerpt": "The instruction below trains a QRNN model that without finetuning achieves perplexities of approximately 60.6 / 58.3 (validation / testing), with finetuning achieves perplexities of approximately 59.1 / 56.7, and with the continuous cache pointer augmentation achieves perplexities of approximately 53.4 / 52.6. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9891164644311351
      ],
      "excerpt": "The instruction below trains a PTB model that without finetuning achieves perplexities of approximately 68.7 / 65.6 (validation / testing), with finetuning achieves perplexities of approximately 67.4 / 64.7, and with the continuous cache pointer augmentation achieves perplexities of approximately 52.2 / 50.6. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9819711850863249
      ],
      "excerpt": "The instruction below will a QRNN model that without finetuning achieves perplexities of approximately 69.3 / 66.8 (validation / testing), with finetuning achieves perplexities of approximately 68.5 / 65.9, and with the continuous cache pointer augmentation achieves perplexities of approximately 53.6 / 52.1. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8948029292351094,
        0.8235360156362893
      ],
      "excerpt": "For speed regarding character-level PTB and enwik8 or word-level WikiText-103, refer to the relevant paper. \nThe default speeds for the models during training on an NVIDIA Quadro GP100: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9927727818939408,
        0.9540256510762326,
        0.9175356766244863,
        0.9867411433783557,
        0.9700412311787676
      ],
      "excerpt": "The default QRNN models can be far faster than the cuDNN LSTM model, with the speed-ups depending on how much of a bottleneck the RNN is. The majority of the model time above is now spent in softmax or optimization overhead (see PyTorch QRNN discussion on speed). \nSpeeds are approximately three times slower on a K80. On a K80 or other memory cards with less memory you may wish to enable the cap on the maximum sampled sequence length to prevent out-of-memory (OOM) errors, especially for WikiText-2. \nIf speed is a major issue, SGD converges more quickly than our non-monotonically triggered variant of ASGD though achieves a worse overall perplexity. \nFor full details, refer to the PyTorch QRNN repository. \nAll the augmentations to the LSTM, including our variant of DropConnect (Wan et al. 2013) termed weight dropping which adds recurrent dropout, allow for the use of NVIDIA's cuDNN LSTM implementation. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/philippwirth/awd-lstm-test/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sat, 25 Dec 2021 19:14:21 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/philippwirth/awd-lstm-test/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "philippwirth/awd-lstm-test",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/philippwirth/awd-lstm-test/master/getdata.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9986563398845941
      ],
      "excerpt": "Install PyTorch 0.4 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.803678577979847
      ],
      "excerpt": "The codebase is now PyTorch 0.4 compatible for most use cases (a big shoutout to https://github.com/shawntan for a fairly comprehensive PR https://github.com/salesforce/awd-lstm-lm/pull/43). Mild readjustments to hyperparameters may be necessary to obtain quoted performance. If you desire exact reproducibility (or wish to run on PyTorch 0.3 or lower), we suggest using an older commit of this repository. We are still working on pointer, finetune and generate functionalities. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8671526817437554
      ],
      "excerpt": "For data setup, run ./getdata.sh. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559614939205502
      ],
      "excerpt": "PyTorch will automatically use the cuDNN backend if run on CUDA with cuDNN installed. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9030504587953663,
        0.8717294828563956
      ],
      "excerpt": "Train the base model using main.py \n(Optionally) Finetune the model using finetune.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9237414278586189,
        0.9196231404064746,
        0.864195087727808
      ],
      "excerpt": "python -u main.py --epochs 50 --nlayers 3 --emsize 400 --nhid 1840 --alpha 0 --beta 0 --dropoute 0 --dropouth 0.1 --dropouti 0.1 --dropout 0.4 --wdrop 0.2 --wdecay 1.2e-6 --bptt 200 --batch_size 128 --optimizer adam --lr 1e-3 --data data/enwik8 --save ENWIK8.pt --when 25 35 \npython -u main.py --epochs 500 --nlayers 3 --emsize 200 --nhid 1000 --alpha 0 --beta 0 --dropoute 0 --dropouth 0.25 --dropouti 0.1 --dropout 0.1 --wdrop 0.5 --wdecay 1.2e-6 --bptt 150 --batch_size 128 --optimizer adam --lr 2e-3 --data data/pennchar --save PTBC.pt --when 300 400 \npython -u main.py --epochs 14 --nlayers 4 --emsize 400 --nhid 2500 --alpha 0 --beta 0 --dropoute 0 --dropouth 0.1 --dropouti 0.1 --dropout 0.1 --wdrop 0 --wdecay 0 --bptt 140 --batch_size 60 --optimizer adam --lr 1e-3 --data data/wikitext-103 --save WT103.12hr.QRNN.pt --when 12 --model QRNN \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9504400517758302,
        0.9367314112780453,
        0.9022285019093377
      ],
      "excerpt": "python main.py --batch_size 20 --data data/penn --dropouti 0.4 --dropouth 0.25 --seed 141 --epoch 500 --save PTB.pt \npython finetune.py --batch_size 20 --data data/penn --dropouti 0.4 --dropouth 0.25 --seed 141 --epoch 500 --save PTB.pt \npython pointer.py --data data/penn --save PTB.pt --lambdasm 0.1 --theta 1.0 --window 500 --bptt 5000 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9306836061768479,
        0.9100158255801801,
        0.8905714189606874
      ],
      "excerpt": "python -u main.py --model QRNN --batch_size 20 --clip 0.2 --wdrop 0.1 --nhid 1550 --nlayers 4 --emsize 400 --dropouth 0.3 --seed 9001 --dropouti 0.4 --epochs 550 --save PTB.pt \npython -u finetune.py --model QRNN --batch_size 20 --clip 0.2 --wdrop 0.1 --nhid 1550 --nlayers 4 --emsize 400 --dropouth 0.3 --seed 404 --dropouti 0.4 --epochs 300 --save PTB.pt \npython pointer.py --model QRNN --lambdasm 0.1 --theta 1.0 --window 500 --bptt 5000 --save PTB.pt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9232916405646634,
        0.896422779876524,
        0.9022285019093377
      ],
      "excerpt": "python main.py --epochs 750 --data data/wikitext-2 --save WT2.pt --dropouth 0.2 --seed 1882 \npython finetune.py --epochs 750 --data data/wikitext-2 --save WT2.pt --dropouth 0.2 --seed 1882 \npython pointer.py --save WT2.pt --lambdasm 0.1279 --theta 0.662 --window 3785 --bptt 2000 --data data/wikitext-2 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9425436928895772,
        0.9259110775846796,
        0.9150892914480145
      ],
      "excerpt": "python -u main.py --epochs 500 --data data/wikitext-2 --clip 0.25 --dropouti 0.4 --dropouth 0.2 --nhid 1550  --nlayers 4 --seed 4002 --model QRNN --wdrop 0.1 --batch_size 40 --save WT2.pt \npython finetune.py --epochs 500 --data data/wikitext-2 --clip 0.25 --dropouti 0.4 --dropouth 0.2 --nhid 1550 --nlayers 4 --seed 4002 --model QRNN --wdrop 0.1 --batch_size 40 --save WT2.pt \npython -u pointer.py --save WT2.pt --model QRNN --lambdasm 0.1279 --theta 0.662 --window 3785 --bptt 2000 --data data/wikitext-2 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8257593487728601,
        0.8548058306153065
      ],
      "excerpt": "Penn Treebank (batch size 20): LSTM takes 65 seconds per epoch, QRNN takes 28 seconds per epoch \nWikiText-2 (batch size 20): LSTM takes 180 seconds per epoch, QRNN takes 90 seconds per epoch \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/philippwirth/awd-lstm-test/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "BSD 3-Clause \"New\" or \"Revised\" License",
      "url": "https://api.github.com/licenses/bsd-3-clause"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'BSD 3-Clause License\\n\\nCopyright (c) 2017, \\nAll rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met:\\n\\n Redistributions of source code must retain the above copyright notice, this\\n  list of conditions and the following disclaimer.\\n\\n Redistributions in binary form must reproduce the above copyright notice,\\n  this list of conditions and the following disclaimer in the documentation\\n  and/or other materials provided with the distribution.\\n\\n* Neither the name of the copyright holder nor the names of its\\n  contributors may be used to endorse or promote products derived from\\n  this software without specific prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "LSTM and QRNN Language Model Toolkit",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "awd-lstm-test",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "philippwirth",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/philippwirth/awd-lstm-test/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Python 3 and PyTorch 0.4 are required for the current codebase.\n\nIncluded below are hyper parameters to get equivalent or better results to those included in the original paper.\n\nIf you need to use an earlier version of the codebase, the original code and hyper parameters accessible at the [PyTorch==0.1.12](https://github.com/salesforce/awd-lstm-lm/tree/PyTorch%3D%3D0.1.12) release, with Python 3 and PyTorch 0.1.12 are required.\nIf you are using Anaconda, installation of PyTorch 0.1.12 can be achieved via:\n`conda install pytorch=0.1.12 -c soumith`.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sat, 25 Dec 2021 19:14:21 GMT"
    },
    "technique": "GitHub API"
  }
}