{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1509.06461."
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "***\n\nJi He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, and MariOstendorf.  Deep Reinforcement Learning with an Unbounded Action Space.CoRR, 2015. URL: http://arxiv.org/abs/1511.04636.\n\nKarthik Narasimhan, Tejas D. Kulkarni, and Regina Barzilay.  Language Under-standing for Text-based Games Using Deep Reinforcement Learning, 2015. URL: http://arxiv.org/abs/1506.08941.\n\nZelinka Mikula\u02c7s. \u201cUsing Reinforcement Learning to Learnhow to Play Text-Based Games.\u201d Charles University, 2017. URL: https://arxiv.org/pdf/1801.01999.\n\nMatan Haroush, Tom Zahavy, Daniel J. Mankowitz, Shie Mannor. Learning How Not to Act in Text-based Games, 2018. URL: https://openreview.net/forum?id=B1-tVX1Pz.\n\nHado van Hasselt, Arthur Guez, David Silver. Deep Reinforcement Learning with Double Q-learning, 2015. URL: https://arxiv.org/abs/1509.06461.\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9534439571730225
      ],
      "excerpt": "        - Take key, open the door \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8090016440670298
      ],
      "excerpt": ":#: Verb-object commands \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8840467439138965
      ],
      "excerpt": "['open OBJ', \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8232969498734352
      ],
      "excerpt": " 'find OBJ', \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8232969498734352
      ],
      "excerpt": " 'find OBJ', \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9305668685311609
      ],
      "excerpt": ":#: Verb-object-prep-object commands \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8840467439138965
      ],
      "excerpt": "f = open('tutorials_2.txt', 'r') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8840467439138965
      ],
      "excerpt": "w2v.wv.similarity('open', 'table') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8840467439138965
      ],
      "excerpt": "w2v.wv.similarity('open', 'chest') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8064486660563246
      ],
      "excerpt": "3b) If predicted, find action with max Q-value \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693,
        0.8043073075947367
      ],
      "excerpt": "7) If round % batch_size == 0, experience replay \n8) If last round, restart game \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9876339741899863
      ],
      "excerpt": "plt.title('End Game Scores') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9301242705116426
      ],
      "excerpt": "plt.title('Per Turn Scores') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9301242705116426
      ],
      "excerpt": "plt.title('Per Turn Scores') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9956966093208229
      ],
      "excerpt": "plt.title('Top 10 Actions Taken') \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/matthewsparr/Deep-Zork",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-03-20T00:10:12Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-17T09:36:02Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8888900147142754,
        0.9996151225269915
      ],
      "excerpt": "Can a machine learn to play a text-based game? \nNeural networks such DQNs have proven to be successful in applications involve visual data but do not perform as well processing language data. Teaching an AI to play text-based games successfully and consistently would be a significant milestone in the field. This project seeks to explore the application of reinforcement learning on text-based games in hopes of advancing research on the topic. This project focuses solely on a single game with hopes of future application across many games. The results of the experimentation done in this project are a ways off from the ultimate end goal of a text-based game playing AI,  but show promising results and offer new insights. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9973317435014526,
        0.8145191933020692,
        0.9887307782667759,
        0.9974207734642692
      ],
      "excerpt": "Advancement in machine learning involving reinforcement learning has seen the rise of AIs capable of playing video games and outperforming human players. These include all sorts of games from chess and Go to Atari games. All of these instances involve games in which gameplay is based on visuals. Using convolutional neural networks, machines become quite capable of recognizing patterns in changing pixels on a screen and can react in real-time in order to win.  \nEnter text-based games. \nAlso known as interactive fiction games, text-based games were some of the first ever computer games due to relying solely on text for gameplay and not needing to process graphics. They rely on language parsing text commands given by the player in response to changing descriptions of the player's environment.  \nThis presents a whole different type of problem for AIs attempting to play one of these games. In text-based games, reaction time is not a factor. Instead, the player must have an understanding of the language used and to be able to make smart decisions based off of subtle hints in the game's descriptions. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9518206482628719,
        0.9649317473025427,
        0.9896563792503893,
        0.9832680455951054,
        0.9875415467756963
      ],
      "excerpt": "There have been some attempts at teaching machines to play text-based games with a key example being TextWorld by Microsoft with variable success. There is still, however, a long way to go before an interactive fiction AI will perform at the level of AlphaGo or AlphaZero. \nThis project will aide in tackling the issue by focusing on teaching a machine to play one single game - Zork - in hopes that the techniques used can be applied to other games in the future. \nOne of the most classic and well-known text-based game, Zork I (also known as Zork: The Great Underground Empire), was written in the late 1970s. At the time it was well-revered for its compelling story-telling and also its language parsing engines which allowed players to use not only simple commands ('Attack monster') but more complex ones with prepositions ('Attack the monster with the sword'). It was one of the most successful works of interactive fiction and its legacy is still alive today with many elements still being used in newer works \nZork is first and foremost an adventure game. The ultimate goal of the game is to collect all 19 different treasures and install them in a trophy case. To do so, the player must explore all areas of the game, including a sprawling underground dungeon. \nBelow are two maps of the Zork world - above and below ground. As you can see it's a very large map with many different rooms and paths between them. Note that the player always starts at West of House on the above ground map. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8913970901810967
      ],
      "excerpt": "There are three types of commands used in Zork. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9681564941746964
      ],
      "excerpt": "        - Attack the monster with the sword, unlock the chest with the key \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8147272563953775
      ],
      "excerpt": "The text was processed using Matcher and POS from the <b>spacy</b> library to find all the possible verbs and verb phrases. This can be seen in the ZorkVerbs notebook. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9909181957086941
      ],
      "excerpt": "Also, for the 2nd type of commands, the noun phrase was replaced with 'OBJ' and for the 3rd type of commands the first noun phrases were replaced with 'OBJ' and the second with 'DCT' to allow for easy substitution of nouns. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8372396012058242
      ],
      "excerpt": "['pour OBJ on DCT', \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8372396012058242,
        0.9394449182630016,
        0.9567588029116127,
        0.8372396012058242,
        0.9567588029116127,
        0.9567588029116127,
        0.9567588029116127,
        0.8372396012058242
      ],
      "excerpt": " 'hide OBJ on DCT', \n 'flip OBJ for DCT', \n 'fix OBJ with DCT', \n 'spray OBJ on DCT', \n 'dig OBJ with DCT', \n 'cut OBJ with DCT', \n 'pick OBJ with DCT', \n 'squeeze OBJ on DCT', \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9567588029116127,
        0.9567588029116127
      ],
      "excerpt": " 'burn OBJ with DCT', \n 'flip OBJ with DCT', \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9567588029116127,
        0.9567588029116127,
        0.8646352312000413,
        0.9693233393948762,
        0.9567588029116127,
        0.8320114477852841,
        0.9804357965733066,
        0.9056171868732211
      ],
      "excerpt": " 'inflate OBJ with DCT', \n 'unlock OBJ with DCT', \n 'give OBJ to DCT', \n 'carry OBJ to DCT', \n 'spray OBJ with DCT'] \nWhen the nouns from the game's descriptions are substituted into the above commands, certain combinations might not make much sense. For example, if there is a door and a table in the environment, the command \"cut the door with the key\" would not make any sense. \nTo help with this problem, walkthroughs and tutorials from other text-based games can be studied to have a way to determine the relevance and likelihood of different commands. \nThis can all be found in the scrape_tutorials notebook but the basic idea is that tutorials and walkthroughs from two databases - https://www.ifarchive.org/indexes/if-archive/solutions/ and http://www.textfiles.com/adventure/ were scraped using urllib and BeautifulSoup to get text files containing relevant commands for these games. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8642531378916758
      ],
      "excerpt": "To determine likelihood of command phrases, a Word2Vec model was used to determine the similarity score. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8008505729813025
      ],
      "excerpt": "w2v.wv.similarity(word_tokenize('unlock tree with'), 'key').mean() \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9636063435359383
      ],
      "excerpt": "To give an idea of the size and complexity of this corpus of walkthroughs and tuturials, a TSNE plot can be seen below. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877,
        0.9644365593724885
      ],
      "excerpt": "def tsne_plot(model): \n    \"Creates and TSNE model and plots it\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9736528848850865,
        0.860059181823877
      ],
      "excerpt": "for word in model.wv.vocab: \n    tokens.append(model[word]) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9465591128692896
      ],
      "excerpt": "The majority of text-based games can be played using Z-Machine data files (.z3, .z4, .z5, .z8). To play games with this file format there are several emulators for different systems. For Windows one of the best emulators is Frotz (https://github.com/DavidGriffith/frotz). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9152314447248392,
        0.9561313005339442,
        0.990777077147624,
        0.9031281107487571,
        0.9549921924740016,
        0.9900872298233592,
        0.9507577092828318,
        0.8592960981691239,
        0.8212901462844536
      ],
      "excerpt": "For any general reinforcement learning network, there must be an agent which, given a game state,  chooses an action which then interacts with the enviornment. The new state along with a corresponding reward are then passed back to the agent which then adjusts its weights for future decisions. \nFor this project, the environment is the Zork 1 game which connects to Python via Popen in order to be able to write new commands and read the lines given by the game. \nThe state consists of both the surroundings in the game as well as the inventory of the player. The combination of these two will provide all necessary information including relevant objects to interact with. \nThe action is chosen either randomly or by the agent. A value - epsilon - is decayed by a small amount each turn. As its value gets closer to 0, a predicted action is more likely to be chosen. Thus, over time the agent will eventually always predict actions. \nThe reward should influence \"good\" behaviour while penalizing \"bad\" behaviour by the agent. There is already an in-game score given to the player when reaching certain areas or performing certain actions. However additional rewards can help to direct the agent more successfully.  \nOne of the most important parts of playing a text-based game is exploration. Any human player playing Zork would need to take part in exploration of the various areas and rooms to learn more about the game world and to find essential items. \nAnother important part is finding, taking, and using items in the game. For Zork especially, the main goal is to find and collect the various treasures so interacting with items should be rewarded.  \nAs far as negative rewards go, taking too many turns in a game usually is a bad thing as the goal should be to get to the end as quick as possible. Thus, a small negative reward can be given per each turn to help dissuade the agent from taking more turns than neccesary.  \nBelow are the different rewards. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8882458919995989
      ],
      "excerpt": "    - Discovering a new area or room in the game should be rewarded in order to encourage exploration. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9775582396816749
      ],
      "excerpt": "    - In order to prevent the agent from staying in one room too long, a small positive reward is given each time the agent moves from one area to another that has already been visited. This also helps to encourage the agent to return to an area it previously left when it had not yet completed all objectives in that area. Providing this reward instead of the new_area_reward when the agent revisits an area will prevent it from exploiting the reward system and hopping back and forth between rooms. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9407432680332813
      ],
      "excerpt": "    - If the agent picks up an item or uses an item, the in-game inventory will change. This will be rewarded with a large number of points as finding the treasures of the game ultimately is how the agent can win.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8632887189811679
      ],
      "excerpt": "    - In order to prevent the agent taking advantage of the large inventory_reward, a much smaller reward will be given when an inventory change occurs that has already taken place. This will prevent exploitable situations in which the agent repeatedly picks up and then drops an item. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9752515888340887,
        0.9758490547559063
      ],
      "excerpt": "    - Scoring points in the game is the best type of action the agent could take so it should be rewarded the greatest. Each time an in-game score is granted, a weight (initially set at 10) will be applied to it to show its significance to the agent. \nThe agent consists of a neural network - more specifically a Deep-Q Network (referred to as DQN). The architecture of the model can be seen below. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9910614021319292,
        0.9387275228662175,
        0.9957596013409621,
        0.8690534601311413,
        0.9575387940560408,
        0.9757626176134667,
        0.8996720365854141
      ],
      "excerpt": "The model takes in the state and performed action as seperate inputs. The inputs are preprocessed (removing special characters, converting to lower-case) and tokenized using a keras tokenizer with a vocabulary size of 1200. The tokens are then padded to ensure a consistent length of 50. \nThe state and action inputs are fed separately into a shared embedding layer with a dimension of 16. This layer is shared so that the vector for a word found in a state will be the same vector if the word is in an action. \nThe state and action embeddings are then sent to a long short-term memory or LSTM layer. This layer hopes to capture some of the time based features in the states and actions. For example, if the state is \"To the north lies a treasure chest and to the west there is a monster.\", the model should be able to capture the signficance of the order of the description. The LSTM layer uses a dimension of 32. \nThe model passes the output from the LSTM layers to separate dense layers of dimension 8. The activation function used at this layer is 'tanh'. \nLastly, to combine the dense state and action layers, a Dot layer is used to represent the condensed information of both inputs into a single value. \nQ-learning allows an agent to make decisions based on future rewards. Instead of just picking the action that gives the highest reward for the current turn, Q-learning takes into account the future reward of the next state following an action as well. \nBelow is the formula for the Q-function. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9268895224666965
      ],
      "excerpt": "The Q-value for each state, chosen action pair is calculated by taking the sum of the reward (rt) given and the maximum reward for the next state (maxQ(st+1,at+1)) multiplied by a discount rate (gamma). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9619444395258292,
        0.9723480018081624
      ],
      "excerpt": "This tactic more closely mimics biological learning. It also decouples the temporal relationship between subsequent turns. \nTurns with more positive outcomes are less common so to ensure that they are trained upon, a prioritize queue of positive experiences (score > 0) can be stored and selected from based on a prioritized fraction when selecting a batch for training during experience replay. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9601716361353405,
        0.9838604759782025,
        0.9881557811382805
      ],
      "excerpt": "The main idea behind a DDQN is that instead of training and predicting on the same model, a secondary model is used for prediction while training on the main model. After so many rounds of training, the weights are then transferred from the main model to the secondary model.  \nSince in the case of a text-based game, the state and action space is so large, using a DDQN can help slow down and stabilize training. \nThe following shows different checkpoints of performance of the AI in this project. These can be used to measure and judge the success of the AI. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9398260739364008,
        0.8740546117170004
      ],
      "excerpt": "    - The AI can complete simple tasks such as taking the egg from up in a tree in the forest and opening and entering the window into the house. \nLevel 3: Able to access the underground. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8404978931173511
      ],
      "excerpt": "Level 4: Able to collect treasures. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8529708908086883
      ],
      "excerpt": "Level 5: Able to beat the game. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9339641300059558,
        0.9431083997657214
      ],
      "excerpt": "Note that these levels grow exponentially in difficulty and the biggest jump is from Level 4 to Level 5. \nBelow is the simplified flow for each game that is played. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8265440619270267
      ],
      "excerpt": "5) Get response, reward, and next state \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9592952785921689
      ],
      "excerpt": "After training for 210 games consisting of 256 turns each, the results of both the AI as well as a dummy model which only chosen random actions can be seen below. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.985946468990181
      ],
      "excerpt": "It is important to keep in mind though, that the score referred to in this chart is not the in-game score but instead the rewarded score. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9634380207959217
      ],
      "excerpt": "The above shows the score at the end of each turn when the score was above 0. With additional training, more scores over 0 occur. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9190525888709683,
        0.9855049436949371
      ],
      "excerpt": "The above shows the score at the end of each turn when the score was below 0. With additional training, less scores below 0 occur. \nThe above is a graph of the reward for each turn taken by the AI.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9886781109376693,
        0.9851934015936661,
        0.98111680148208,
        0.8680855672760659,
        0.9865053227677216,
        0.9945777535762643
      ],
      "excerpt": "Above shows the top 10 most common actions taken. Nearly all of them are directional commands which is not surprising as moving around in the game is vital to scoring points. The only command in the top 10 which is not a directional command is the command 'leave nest'. This is equivalent to dropping the 'nest' which is a synonym for the 'egg' in the game. Dropping the egg or nest early on is actually an important step in scoring points for the item later on in the game. \nIn terms of scoring the performance of the AI against the previously mentioned goals, the AI is just shy of Level 3. \nThe AI is able to consistently complete the mini-quests of obtaining the egg from the tree in the forest as well as entering into the house. However, it still has not discovered one of the three passages to the underground. \nFrom the training so far, it would seem that an AI can be somewhat capable of playing a text-based game, albeit poorly in its current state. I believe that with additional training, the AI will certainly be able to progress to Level 3. \nHowever, it\u2019s hard to say how easily it would then be able to jump to Level 4 and begin obtaining the various treasures in the game. It is not too farfetched to believe it to be capable of that though, considering the fact that it was able to learn how to interact with both the egg in the tree and the window into the house. \nGoing forward, there is much that can be done to improve the performance of Deep Zork. Below is a list of all planned future work and investigation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9094108506158889,
        0.9649923032644063,
        0.8814341312843967
      ],
      "excerpt": "Training on the game data was the most computationally expensive part of this project. Calculating the max possible Q value for each state involves a large amount of iterations. \nFinding a way to cut down the action space size could be one way to improve the performance. Using a better method of predicting which commands are likely to be successful could speed up training by a large factor. \nAnother option would be to parallelize the training and separate it completely from playing the game. A large amount of game data could be collected and then all the training could take place across multiple threads or machines. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9959406815002121,
        0.918269493262286,
        0.8823850585946348
      ],
      "excerpt": "Because the state consists of the surroundings and the inventory and there are many items in the game, many combinations of surroundings and inventories can occur resulting in a very large state size. Currently the game stores all of these as separate states which leads to a rather large number of states. \nFinding a way to combine similar states could reduce overall state space size. \nAnother option would be to feed the inventory and surroundings into the agent separately and create separate pipes for them within the model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8143926324211646,
        0.9611677385610282
      ],
      "excerpt": "Different values for the rewards could be tested as well as removing or adding additional conditions. \nThe logic behind adding rewards for exploration and item interaction was to help the AI more quickly learn what it needed to do to beat the game. Although it might involve a longer training period, using only the in-game rewards could result in a better performance in the long run. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9803457910510207,
        0.9653799481323215,
        0.9279467000342432
      ],
      "excerpt": "Using a DDQN might not be the optimal form of reinforcement learning for the task of playing text-based games. \nMore experimental networks could be tested and compared to the performance of the DDQN. \nAn ensemble of networks could also be tested and might provide a more robust solution. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Using NLP and reinforcement learning to build an AI capable of playing text-based games",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/matthewsparr/Deep-Zork/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3,
      "date": "Tue, 21 Dec 2021 11:58:33 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/matthewsparr/Deep-Zork/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "matthewsparr/Deep-Zork",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/matthewsparr/Deep-Zork/master/main.ipynb",
      "https://raw.githubusercontent.com/matthewsparr/Deep-Zork/master/index.ipynb",
      "https://raw.githubusercontent.com/matthewsparr/Deep-Zork/master/ZorkVerbs.ipynb",
      "https://raw.githubusercontent.com/matthewsparr/Deep-Zork/master/game.ipynb",
      "https://raw.githubusercontent.com/matthewsparr/Deep-Zork/master/Untitled2.ipynb",
      "https://raw.githubusercontent.com/matthewsparr/Deep-Zork/master/agent.ipynb",
      "https://raw.githubusercontent.com/matthewsparr/Deep-Zork/master/scrape_tutorials.ipynb",
      "https://raw.githubusercontent.com/matthewsparr/Deep-Zork/master/process_tutorials.ipynb",
      "https://raw.githubusercontent.com/matthewsparr/Deep-Zork/master/Untitled1.ipynb",
      "https://raw.githubusercontent.com/matthewsparr/Deep-Zork/master/TechnicalWriteUp.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "***\n__Epsilon decay:__ 0.9995\n\n__Batch-size:__ 64\n\n__Gamma:__ 0.75\n\n__Prioritized fraction:__ 0.25\n\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9202509175650415
      ],
      "excerpt": "<img src='zork_box_art.jpg'/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8647354795712904
      ],
      "excerpt": "<img src='zork_map_1.gif'/> <img src='zork_map_2.gif'/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516,
        0.8900486270063179
      ],
      "excerpt": "from nltk.tokenize import word_tokenize \nfrom gensim.models import Word2Vec \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.802731610469789
      ],
      "excerpt": "tutorials = f.read() \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8180033470670719
      ],
      "excerpt": "plt.figure(figsize=(16, 16))  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8261366541058346
      ],
      "excerpt": "    plt.scatter(x[i],y[i]) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8836704212480256
      ],
      "excerpt": "<img src='DDQN_model.png'/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8836704212480256
      ],
      "excerpt": "<img src='q_formula.png'/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8347306400897082
      ],
      "excerpt": "import matplotlib \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9613290558991899,
        0.9544364649257886,
        0.8562167907848781
      ],
      "excerpt": "ax = agent_scores.plot.line(y='Score', use_index=True) \nrandom_scores.plot(ax=ax, y='Score', use_index=True, legend=True) \nax.legend([\"DDQN Agent\", \"Random Agent\"]) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8163988632746755
      ],
      "excerpt": "plt.ylabel('Number of occurences') \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/matthewsparr/Deep-Zork/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "**Deep Zork**",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Deep-Zork",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "matthewsparr",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/matthewsparr/Deep-Zork/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 7,
      "date": "Tue, 21 Dec 2021 11:58:33 GMT"
    },
    "technique": "GitHub API"
  }
}