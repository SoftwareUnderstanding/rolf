{
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rahulmadanahalli/manifold_mixup",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-05-06T02:30:54Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-17T17:44:19Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8821935203933446
      ],
      "excerpt": "Tensorflow implementation of the following research paper: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9344547718644276,
        0.8150673229653901,
        0.967223889335144,
        0.9157290743543943,
        0.932333749139759
      ],
      "excerpt": "confidence. In manifold mixup, you also give the network data that\u2019s on/near the decision boundary between different label \nclasses, and train it to output a label with low confidence because it should be uncertain about that data. \nManifold Mixup is a simple regularizer that encourages neural networks to: \npredict less confidently on interpolations of hidden representations \nlimit the directions of variance for hidden representations of the same class, so hidden representations with the same \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8243339629512935,
        0.8715778076134463,
        0.8797038524777574,
        0.9881748461927921
      ],
      "excerpt": "In training, during each forward pass of a mini batch, we choose a layer k among a set of eligible mixup layers. \nDuring the forward pass for that mini batch, we perform \"manifold mixup\" at layer k: \nWe process the normal forward pass of the mini batch until we have the output of layer k - 1. We now have the layer  \n(k-1)th hidden representation of all p points in our mini batch. Define the hidden representations as {x<sub>1</sub>, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9578885542183285,
        0.8971075851354913
      ],
      "excerpt": "For each point p, we choose another point p' at random and perform mixup on the features and labels to get a new hidden  \nrepresentation x<sub>mp</sub> and label y<sub>mp</sub> for every point in the mini-batch \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8913872031330089
      ],
      "excerpt": "We then input these new mixup hidden representations into layer k and continue the forward pass. We define the loss \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9661562109322477
      ],
      "excerpt": "With my Tensorflow implementation, I was able to reproduce some of the research paper's findings: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9096664970905389,
        0.8614615680855839,
        0.9787263218714142,
        0.8710100502527698
      ],
      "excerpt": "The below plots show the model's confidence for a given label \npoints in the input space. Yellow means it predicts class 0 with high probability, purple means it predicts class 1 \nwith high probability, and green/blue means a low confidence for either label. The plots below show that there is a smoother \ndecision boundary and a bigger band of lower confidence predictions. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9024892905322865,
        0.8792397245318301,
        0.9714447996937707,
        0.9729439669416221,
        0.9394449182630016
      ],
      "excerpt": "mixup and a baseline). Then I plotted the hidden representation of the dataset at the 2D bottleneck layer. Points of the same color belong to the  \nsame class (ie. yellow refers to points with the label '3'). Looking at the difference between the baseline and the manifold \nmixup models, we can see that manifold mixup makes the hidden representations much tighter with the real data occupying \nsmaller regions in the hidden space and with a bigger separating margin between the classes. \n|         | Hidden Rep for #'s 0-4     |  Hidden Rep for #'s 0-9  | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9091815022070672,
        0.9190683409212107,
        0.9560119943137507,
        0.8286148492609496
      ],
      "excerpt": "the dataset by label class and found the matrix defining the 12D hidden representation for all data points of a particular class. \nFor each class, I plotted the singular values for the SVD of that hidden representation matrix. As expected, the singular \nvalues for the manifold mixup model are much less than the respective singular values of the other models, which means \nthat manifold mixup leads to flatter representations. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9651687850258774,
        0.9450950701310766,
        0.9139433077983308
      ],
      "excerpt": "deep neural network. Follow the below directions to add manifold mixup to your model: \nPass in your desired deep neural network as a list of tuples. Each tuple is a layer  \n(subclass of tf.keras.layers.Layer) and a boolean that dictates whether that layer is eligible for mixup or not. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9421864558579321
      ],
      "excerpt": "Pass in mixup_alpha. It is the parameter used to initialize a beta distribution. We then draw &#955;  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9908745497382384
      ],
      "excerpt": "The below example creates a deep neural network with manifold mixup using an alpha of 1.0. Only the 2nd to last and 3rd to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Tensorflow implementation of the Manifold Mixup machine learning research paper",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rahulmadanahalli/manifold_mixup/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 4,
      "date": "Tue, 21 Dec 2021 00:05:01 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/rahulmadanahalli/manifold_mixup/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "rahulmadanahalli/manifold_mixup",
    "technique": "GitHub API"
  },
  "invocation": [
    {
      "confidence": [
        0.8437325226858129
      ],
      "excerpt": "Example: (tf.keras.layers.Dense(1024), True) \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/rahulmadanahalli/manifold_mixup/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Makefile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Manifold Mixup ML Implementation",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "manifold_mixup",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "rahulmadanahalli",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rahulmadanahalli/manifold_mixup/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Install Python 3\n2. `pip install virtualenv`\n3. `make all`\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 17,
      "date": "Tue, 21 Dec 2021 00:05:01 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "machine-learning",
      "research",
      "tensorflow",
      "deep-neural-networks",
      "regularization",
      "data-augmentation",
      "deep-learning"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Install Python 3\n2. `pip install virtualenv`\n3. `make all`\n\n",
      "technique": "Header extraction"
    }
  ]
}