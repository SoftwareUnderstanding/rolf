{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To cite this repository:\n\n```\n@software{haiku2020github,\n  author = {Tom Hennigan and Trevor Cai and Tamara Norman and Igor Babuschkin},\n  title = {{H}aiku: {S}onnet for {JAX}},\n  url = {http://github.com/deepmind/dm-haiku},\n  version = {0.0.3},\n  year = {2020},\n}\n```\n\nIn this bibtex entry, the version number is intended to be from\n[`haiku/__init__.py`](https://github.com/deepmind/dm-haiku/blob/main/haiku/__init__.py),\nand the year corresponds to the project's open-source release.\n\n[JAX]: https://github.com/google/jax\n[Sonnet]: https://github.com/deepmind/sonnet\n[Tensorflow]: https://github.com/tensorflow/tensorflow\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@software{haiku2020github,\n  author = {Tom Hennigan and Trevor Cai and Tamara Norman and Igor Babuschkin},\n  title = {{H}aiku: {S}onnet for {JAX}},\n  url = {http://github.com/deepmind/dm-haiku},\n  version = {0.0.3},\n  year = {2020},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9565696357172301
      ],
      "excerpt": "| Citing Haiku \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9784220887606534
      ],
      "excerpt": "please see https://haiku-os.org/. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8786044417622563
      ],
      "excerpt": "  transformed function, hk.next_rng_key() returns a unique rng key. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8714162992508173,
        0.8109194328925066
      ],
      "excerpt": "key = hk.PRNGSequence(42) \nparams = forward.init(next(key), x) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8714162992508173
      ],
      "excerpt": "    key = hk.next_rng_key() \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9507374082549614
      ],
      "excerpt": "  net = hk.nets.ResNet50(1000) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "  logits = hk.nets.MLP([8, 4, 2])(x) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8088275851153562
      ],
      "excerpt": "for _ in range(10): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9550653423319696
      ],
      "excerpt": "For a more complete look at distributed Haiku training, take a look at our \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/deepmind/dm-haiku",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contributing guidelines\nHow to become a contributor and submit your own code\nContributor License Agreements\nWe'd love to accept your patches! Before we can take them, we have to jump a\ncouple of legal hurdles.\nPlease fill out either the individual or corporate Contributor License Agreement\n(CLA).\n\nIf you are an individual writing original source code and you're sure you\n    own the intellectual property, then you'll need to sign an individual\n    CLA.\nIf you work for a company that wants to allow you to contribute your work,\n    then you'll need to sign a corporate\n    CLA.\n\nFollow either of the two links above to access the appropriate CLA and\ninstructions for how to sign and return it. Once we receive it, we'll be able to\naccept your pull requests.\nNOTE: Only original source code from you and other people that have signed\nthe CLA can be accepted into the main repository.\nContributing code\nIf you have improvements to Haiku, send us your pull requests! For those just\ngetting started, Github has a\nhowto.\nIf you want to contribute but you're not sure where to start, take a look at the\nissues with the \"contributions welcome\"\nlabel.\nThese are issues that we believe are particularly well suited for outside\ncontributions, often because we probably won't get to them right now. If you\ndecide to start on an issue, leave a comment so that other people know that\nyou're working on it. If you want to help out, but not alone, use the issue\ncomment thread to coordinate.",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-02-18T07:14:02Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-22T13:55:04Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8622850624089683,
        0.8021180679133698
      ],
      "excerpt": "Haiku is a tool<br> \nFor building neural networks<br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.968211988130584,
        0.9285519957784122
      ],
      "excerpt": "Haiku is a simple neural network library for [JAX] developed by some of the \nauthors of [Sonnet], a neural network library for [TensorFlow]. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9339616295178594,
        0.8114383855720281
      ],
      "excerpt": "Haiku is a simple neural network library for JAX that enables users to use \nfamiliar object-oriented programming models while allowing full access to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8849451059737864
      ],
      "excerpt": "Haiku provides two core tools: a module abstraction, hk.Module, and a simple \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8775320646732764
      ],
      "excerpt": "other modules, and methods that apply functions on user inputs. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9152390297979888,
        0.816639542609693,
        0.893866743841471,
        0.91802949948129,
        0.9625248257875143,
        0.9968029537584643,
        0.9734731095394207
      ],
      "excerpt": "  ease. These include large-scale results in image and language processing, \n  generative models, and reinforcement learning. \nHaiku is designed to make specific things simpler: managing model parameters \n  and other model state. \nHaiku can be expected to compose with other libraries and work well with the \n  rest of JAX. \nHaiku otherwise is designed to get out of your way - it does not define custom \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9691764066678248,
        0.9097954642101077,
        0.9359468719241807
      ],
      "excerpt": "Haiku builds on the programming model and APIs of Sonnet, a neural network \n  library with near universal adoption at DeepMind. It preserves Sonnet's \n  Module-based programming model for state management while retaining access \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8787915914544683,
        0.908139467052905
      ],
      "excerpt": "Haiku APIs and abstractions are as close as reasonable to Sonnet. Many users \n  have found Sonnet to be a productive programming model in TensorFlow; Haiku \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.968363281687895,
        0.984556899870914
      ],
      "excerpt": "By design, transitioning from TensorFlow and Sonnet to JAX and Haiku is easy. \nOutside of new features (e.g. hk.transform), Haiku aims to match the API of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8109642664297034
      ],
      "excerpt": "  passed into the top-level transformed function, and are thus safe to use with \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9368430932239062
      ],
      "excerpt": "loop. (For more examples, see our \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9356862196366765,
        0.8342773478244947
      ],
      "excerpt": "The core of Haiku is hk.transform. The transform function allows you to \nwrite neural network functions that rely on parameters (here the weights of the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8133536455322983,
        0.8813436288537122,
        0.908925214220865
      ],
      "excerpt": "for initialising those parameters. transform does this by transforming the \nfunction into a pair of functions that are pure (as required by JAX) init \nand apply. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8312388047515367
      ],
      "excerpt": "the arguments to the untransformed function), allows you to collect the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9819239724804907,
        0.927995913359719
      ],
      "excerpt": "The params object returned is a nested data structure of all the \nparameters in your network, designed for your to inspect and manipulate.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8143547341811779
      ],
      "excerpt": "parameter is a mapping of parameter name to parameter value. For example: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8923277451217193
      ],
      "excerpt": "we could also pass in None for the rng argument. (Note that if your \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9438952777084814
      ],
      "excerpt": "an error to be raised.) In our example above, we ask Haiku to do this for us \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9698257676085882
      ],
      "excerpt": "Since apply is a pure function we can pass it to jax.grad (or any of JAX's \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8033042699762487,
        0.9212214102814563
      ],
      "excerpt": "The training loop in this example is very simple. One detail to note is the use \nof jax.tree_multimap to apply the sgd function across all matching \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8810234349743848
      ],
      "excerpt": "Haiku is written in pure Python, but depends on C++ code via JAX. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8496099617378557,
        0.876082339015969,
        0.908925214220865
      ],
      "excerpt": "In Haiku, all modules are a subclass of hk.Module. You can implement any \nmethod you like (nothing is special-cased), but typically modules implement \n__init__ and __call__. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.988658021540912,
        0.8983463215891823
      ],
      "excerpt": "of pure functions using hk.transform. See our quickstart for \nmore details about the functions returned from transform: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877,
        0.8838112794725558
      ],
      "excerpt": "  return model(x) \n: Turn forward_fn into an object with init and apply methods. By default, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8149565463488971
      ],
      "excerpt": ": models transformed using hk.transform(f) must be called with an additional \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9660078265226816
      ],
      "excerpt": ": hk.without_apply_rng(hk.transform(f)) is this is undesirable. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8252446162663314,
        0.9165970008865274
      ],
      "excerpt": "Some models may require random sampling as part of the computation. \nFor example, in variational autoencoders with the reparametrization trick, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9981096729371396,
        0.8278635245971334,
        0.895652971440148
      ],
      "excerpt": "work with JAX is in management of PRNG keys. \nIn Haiku we provide a simple API for maintaining a PRNG key sequence associated \nwith modules: hk.next_rng_key() (or next_rng_keys() for multiple keys): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9550054138142337
      ],
      "excerpt": "For a more complete look at working with stochastic models, please see our \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8389852937755019,
        0.8675969600500755
      ],
      "excerpt": "avoid using it alongside JAX transformations which are inside hk.transform. \nFor more information and possible workarounds, please consult the docs on \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8633664697953736
      ],
      "excerpt": "batch normalization a moving average of values encountered during training is \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9630016473400655,
        0.9682115883511281
      ],
      "excerpt": "In Haiku we provide a simple API for maintaining mutable state that is \nassociated with modules: hk.set_state and hk.get_state. When using these \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9873949485871762
      ],
      "excerpt": "since the signature of the returned pair of functions is different: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8910401881641888,
        0.906766481531172
      ],
      "excerpt": ": anything that was created using hk.set_state. The structure is the same as \n: params (e.g. it is a per-module mapping of named values). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9873771933545118
      ],
      "excerpt": "are fully compatible with jax.pmap. For more details on SPMD programming with \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9570957378005935
      ],
      "excerpt": "One common use of jax.pmap with Haiku is for data-parallel training on many \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.955928469303965
      ],
      "excerpt": ": Initialize the model on a single device. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8078388039582814,
        0.8493296599559059
      ],
      "excerpt": "  \"\"\"Constructs a superbatch, i.e. one batch of data per device.\"\"\" \n  #: Get N batches, then split into list-of-images and list-of-labels. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8797067155729023
      ],
      "excerpt": "  \"\"\"Updates params based on performance on inputs and labels.\"\"\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9106827083499437
      ],
      "excerpt": "  #: Take the mean of the gradients across all data-parallel replicas. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "JAX-based neural network library",
      "technique": "GitHub API"
    }
  ],
  "documentation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "https://jax.readthedocs.io/",
      "technique": "Regular expression"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/deepmind/dm-haiku/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 120,
      "date": "Wed, 22 Dec 2021 15:24:44 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/deepmind/dm-haiku/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "deepmind/dm-haiku",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/deepmind/dm-haiku/tree/main/docs"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/deepmind/dm-haiku/main/examples/vqvae_example.ipynb",
      "https://raw.githubusercontent.com/deepmind/dm-haiku/main/examples/haiku_lstms.ipynb",
      "https://raw.githubusercontent.com/deepmind/dm-haiku/main/examples/mnist_gan.ipynb",
      "https://raw.githubusercontent.com/deepmind/dm-haiku/main/docs/notebooks/visualization.ipynb",
      "https://raw.githubusercontent.com/deepmind/dm-haiku/main/docs/notebooks/basics.ipynb",
      "https://raw.githubusercontent.com/deepmind/dm-haiku/main/docs/notebooks/jax2tf.ipynb",
      "https://raw.githubusercontent.com/deepmind/dm-haiku/main/docs/notebooks/build_your_own_haiku.ipynb",
      "https://raw.githubusercontent.com/deepmind/dm-haiku/main/docs/notebooks/non_trainable.ipynb",
      "https://raw.githubusercontent.com/deepmind/dm-haiku/main/docs/notebooks/transforms.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/deepmind/dm-haiku/main/test.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9769420979004304
      ],
      "excerpt": "| Installation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8215387902050032
      ],
      "excerpt": "Disambiguation: if you are looking for Haiku the operating system then \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8230948934347108
      ],
      "excerpt": "\"impure\" modules into pure functions that can be used with jax.jit, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.965330649568616
      ],
      "excerpt": "Because JAX installation is different depending on your CUDA version, Haiku does \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9098226956519594,
        0.9971581277210553,
        0.9989505157564216,
        0.9948657602012146,
        0.999746712887969,
        0.8197007514060154,
        0.989546273021627
      ],
      "excerpt": "to install JAX with the relevant accelerator support. \nThen, install Haiku using pip: \n$ pip install git+https://github.com/deepmind/dm-haiku \nAlternatively, you can install via PyPI: \n$ pip install -U dm-haiku \nOur examples rely on additional libraries (e.g. bsuite). You can install the full set of additional requirements using pip: \n$ pip install -r examples/requirements.txt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8374603619732941
      ],
      "excerpt": "becomes my_linear). Modules can have named parameters that are accessed \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8029055727087168
      ],
      "excerpt": "  #: Update parameters using SGD or Adam or ... \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8838148168639296
      ],
      "excerpt": "| Examples \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8914359206178266
      ],
      "excerpt": "examples directory. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8778487586960795
      ],
      "excerpt": "MNIST example \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9012248701992861,
        0.8896045222112412
      ],
      "excerpt": "import haiku as hk \nimport jax.numpy as jnp \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8353955972082883
      ],
      "excerpt": "  return param - 0.01 * update \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8060372108626078
      ],
      "excerpt": "  params = jax.tree_multimap(update_rule, params, grads) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.857002578889979,
        0.857002578889979,
        0.8506067880442849,
        0.8811301508201074
      ],
      "excerpt": " 'linear_1': {'b': ndarray(..., shape=(100,), dtype=float32), \n              'w': ndarray(..., shape=(1000, 100), dtype=float32)}, \n 'linear_2': {'b': ndarray(..., shape=(10,), dtype=float32), \n              'w': ndarray(..., shape=(100, 10), dtype=float32)}} \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8997243352845468
      ],
      "excerpt": "    w_init = hk.initializers.TruncatedNormal(1. / np.sqrt(j)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8849413366909309
      ],
      "excerpt": "name is inferred from the name of the Python class (for example MyLinear \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8150832584268567
      ],
      "excerpt": "  model = MyLinear(10) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8778487586960795
      ],
      "excerpt": "VAE example. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8894288787719958
      ],
      "excerpt": "params = jax.tree_util.tree_map(lambda x: np.stack([x] * num_devices), params) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8252319709417002
      ],
      "excerpt": "  \"\"\"Constructs a superbatch, i.e. one batch of data per device.\"\"\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8017285540429218
      ],
      "excerpt": "  superbatch_images, superbatch_labels = zip(*superbatch) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8628114170533585,
        0.8628114170533585
      ],
      "excerpt": "  superbatch_images = np.stack(superbatch_images) \n  superbatch_labels = np.stack(superbatch_labels) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8060372108626078
      ],
      "excerpt": "  new_params = my_update_rule(params, grads) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8139090233653156
      ],
      "excerpt": ": Run several training updates. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/deepmind/dm-haiku/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Starlark",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Apache License 2.0",
      "url": "https://api.github.com/licenses/apache-2.0"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'\\n                                 Apache License\\n                           Version 2.0, January 2004\\n                        http://www.apache.org/licenses/\\n\\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\\n\\n   1. Definitions.\\n\\n      \"License\" shall mean the terms and conditions for use, reproduction,\\n      and distribution as defined by Sections 1 through 9 of this document.\\n\\n      \"Licensor\" shall mean the copyright owner or entity authorized by\\n      the copyright owner that is granting the License.\\n\\n      \"Legal Entity\" shall mean the union of the acting entity and all\\n      other entities that control, are controlled by, or are under common\\n      control with that entity. For the purposes of this definition,\\n      \"control\" means (i) the power, direct or indirect, to cause the\\n      direction or management of such entity, whether by contract or\\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\\n      outstanding shares, or (iii) beneficial ownership of such entity.\\n\\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\\n      exercising permissions granted by this License.\\n\\n      \"Source\" form shall mean the preferred form for making modifications,\\n      including but not limited to software source code, documentation\\n      source, and configuration files.\\n\\n      \"Object\" form shall mean any form resulting from mechanical\\n      transformation or translation of a Source form, including but\\n      not limited to compiled object code, generated documentation,\\n      and conversions to other media types.\\n\\n      \"Work\" shall mean the work of authorship, whether in Source or\\n      Object form, made available under the License, as indicated by a\\n      copyright notice that is included in or attached to the work\\n      (an example is provided in the Appendix below).\\n\\n      \"Derivative Works\" shall mean any work, whether in Source or Object\\n      form, that is based on (or derived from) the Work and for which the\\n      editorial revisions, annotations, elaborations, or other modifications\\n      represent, as a whole, an original work of authorship. For the purposes\\n      of this License, Derivative Works shall not include works that remain\\n      separable from, or merely link (or bind by name) to the interfaces of,\\n      the Work and Derivative Works thereof.\\n\\n      \"Contribution\" shall mean any work of authorship, including\\n      the original version of the Work and any modifications or additions\\n      to that Work or Derivative Works thereof, that is intentionally\\n      submitted to Licensor for inclusion in the Work by the copyright owner\\n      or by an individual or Legal Entity authorized to submit on behalf of\\n      the copyright owner. For the purposes of this definition, \"submitted\"\\n      means any form of electronic, verbal, or written communication sent\\n      to the Licensor or its representatives, including but not limited to\\n      communication on electronic mailing lists, source code control systems,\\n      and issue tracking systems that are managed by, or on behalf of, the\\n      Licensor for the purpose of discussing and improving the Work, but\\n      excluding communication that is conspicuously marked or otherwise\\n      designated in writing by the copyright owner as \"Not a Contribution.\"\\n\\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\\n      on behalf of whom a Contribution has been received by Licensor and\\n      subsequently incorporated within the Work.\\n\\n   2. Grant of Copyright License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      copyright license to reproduce, prepare Derivative Works of,\\n      publicly display, publicly perform, sublicense, and distribute the\\n      Work and such Derivative Works in Source or Object form.\\n\\n   3. Grant of Patent License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      (except as stated in this section) patent license to make, have made,\\n      use, offer to sell, sell, import, and otherwise transfer the Work,\\n      where such license applies only to those patent claims licensable\\n      by such Contributor that are necessarily infringed by their\\n      Contribution(s) alone or by combination of their Contribution(s)\\n      with the Work to which such Contribution(s) was submitted. If You\\n      institute patent litigation against any entity (including a\\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\\n      or a Contribution incorporated within the Work constitutes direct\\n      or contributory patent infringement, then any patent licenses\\n      granted to You under this License for that Work shall terminate\\n      as of the date such litigation is filed.\\n\\n   4. Redistribution. You may reproduce and distribute copies of the\\n      Work or Derivative Works thereof in any medium, with or without\\n      modifications, and in Source or Object form, provided that You\\n      meet the following conditions:\\n\\n      (a) You must give any other recipients of the Work or\\n          Derivative Works a copy of this License; and\\n\\n      (b) You must cause any modified files to carry prominent notices\\n          stating that You changed the files; and\\n\\n      (c) You must retain, in the Source form of any Derivative Works\\n          that You distribute, all copyright, patent, trademark, and\\n          attribution notices from the Source form of the Work,\\n          excluding those notices that do not pertain to any part of\\n          the Derivative Works; and\\n\\n      (d) If the Work includes a \"NOTICE\" text file as part of its\\n          distribution, then any Derivative Works that You distribute must\\n          include a readable copy of the attribution notices contained\\n          within such NOTICE file, excluding those notices that do not\\n          pertain to any part of the Derivative Works, in at least one\\n          of the following places: within a NOTICE text file distributed\\n          as part of the Derivative Works; within the Source form or\\n          documentation, if provided along with the Derivative Works; or,\\n          within a display generated by the Derivative Works, if and\\n          wherever such third-party notices normally appear. The contents\\n          of the NOTICE file are for informational purposes only and\\n          do not modify the License. You may add Your own attribution\\n          notices within Derivative Works that You distribute, alongside\\n          or as an addendum to the NOTICE text from the Work, provided\\n          that such additional attribution notices cannot be construed\\n          as modifying the License.\\n\\n      You may add Your own copyright statement to Your modifications and\\n      may provide additional or different license terms and conditions\\n      for use, reproduction, or distribution of Your modifications, or\\n      for any such Derivative Works as a whole, provided Your use,\\n      reproduction, and distribution of the Work otherwise complies with\\n      the conditions stated in this License.\\n\\n   5. Submission of Contributions. Unless You explicitly state otherwise,\\n      any Contribution intentionally submitted for inclusion in the Work\\n      by You to the Licensor shall be under the terms and conditions of\\n      this License, without any additional terms or conditions.\\n      Notwithstanding the above, nothing herein shall supersede or modify\\n      the terms of any separate license agreement you may have executed\\n      with Licensor regarding such Contributions.\\n\\n   6. Trademarks. This License does not grant permission to use the trade\\n      names, trademarks, service marks, or product names of the Licensor,\\n      except as required for reasonable and customary use in describing the\\n      origin of the Work and reproducing the content of the NOTICE file.\\n\\n   7. Disclaimer of Warranty. Unless required by applicable law or\\n      agreed to in writing, Licensor provides the Work (and each\\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\\n      implied, including, without limitation, any warranties or conditions\\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\\n      PARTICULAR PURPOSE. You are solely responsible for determining the\\n      appropriateness of using or redistributing the Work and assume any\\n      risks associated with Your exercise of permissions under this License.\\n\\n   8. Limitation of Liability. In no event and under no legal theory,\\n      whether in tort (including negligence), contract, or otherwise,\\n      unless required by applicable law (such as deliberate and grossly\\n      negligent acts) or agreed to in writing, shall any Contributor be\\n      liable to You for damages, including any direct, indirect, special,\\n      incidental, or consequential damages of any character arising as a\\n      result of this License or out of the use or inability to use the\\n      Work (including but not limited to damages for loss of goodwill,\\n      work stoppage, computer failure or malfunction, or any and all\\n      other commercial damages or losses), even if such Contributor\\n      has been advised of the possibility of such damages.\\n\\n   9. Accepting Warranty or Additional Liability. While redistributing\\n      the Work or Derivative Works thereof, You may choose to offer,\\n      and charge a fee for, acceptance of support, warranty, indemnity,\\n      or other liability obligations and/or rights consistent with this\\n      License. However, in accepting such obligations, You may act only\\n      on Your own behalf and on Your sole responsibility, not on behalf\\n      of any other Contributor, and only if You agree to indemnify,\\n      defend, and hold each Contributor harmless for any liability\\n      incurred by, or claims asserted against, such Contributor by reason\\n      of your accepting any such warranty or additional liability.\\n\\n   END OF TERMS AND CONDITIONS\\n\\n   APPENDIX: How to apply the Apache License to your work.\\n\\n      To apply the Apache License to your work, attach the following\\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\\n      replaced with your own identifying information. (Don\\'t include\\n      the brackets!)  The text should be enclosed in the appropriate\\n      comment syntax for the file format. We also recommend that a\\n      file or class name and description of purpose be included on the\\n      same \"printed page\" as the copyright notice for easier\\n      identification within third-party archives.\\n\\n   Copyright [yyyy] [name of copyright owner]\\n\\n   Licensed under the Apache License, Version 2.0 (the \"License\");\\n   you may not use this file except in compliance with the License.\\n   You may obtain a copy of the License at\\n\\n       http://www.apache.org/licenses/LICENSE-2.0\\n\\n   Unless required by applicable law or agreed to in writing, software\\n   distributed under the License is distributed on an \"AS IS\" BASIS,\\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n   See the License for the specific language governing permissions and\\n   limitations under the License.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Haiku: [Sonnet] for [JAX]",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "dm-haiku",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "deepmind",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/deepmind/dm-haiku/blob/main/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "tomhennigan",
        "body": "- Added support for mixed precision training (dba1fd9) via [jmp](https://github.com/deepmind/jmp)\r\n- Added `hk.with_empty_state(..)`.\r\n- Added `hk.multi_transform(..)` (#137), supporting transforming multiple functions that share parameters.\r\n- Added `hk.data_structures.is_subset(..)` to test whether parameters are a subset of another.\r\n- Minimum Python version is now 3.7.\r\n- Multiple changes in preparation for a future version of Haiku changing to plain `dict`s.\r\n- `hk.next_rng_keys(..)` now returns a stacked array rather than a collection.\r\n- `hk.MultiHeadAttention` now supports distinct sequence lengths in query and key/value.\r\n- `hk.LayerNorm` now optionally supports faster (but less stable) variance computation.\r\n- `hk.nets.MLP` now has an output_shape property.\r\n- `hk.nets.ResNet` now supports changing strides.\r\n- `UnexpectedTracerError` inside a Haiku transform now has a more useful error message.\r\n- `hk.{lift,custom_creator,custom_getter}` are no longer experimental.\r\n- Haiku now supports JAX's pluggable RNGs.\r\n- We have made multiple improvements to our docs an error messages.\r\n\r\nAny many other small fixes and improvements.",
        "dateCreated": "2021-11-01T10:33:45Z",
        "datePublished": "2021-11-01T13:18:04Z",
        "html_url": "https://github.com/deepmind/dm-haiku/releases/tag/v0.0.5",
        "name": "Haiku 0.0.5",
        "tag_name": "v0.0.5",
        "tarball_url": "https://api.github.com/repos/deepmind/dm-haiku/tarball/v0.0.5",
        "url": "https://api.github.com/repos/deepmind/dm-haiku/releases/52446620",
        "zipball_url": "https://api.github.com/repos/deepmind/dm-haiku/zipball/v0.0.5"
      },
      {
        "authorType": "User",
        "author_name": "tomhennigan",
        "body": "**Changelog:**\r\n\r\n- **(Important Fix) Fixed strides in basic block (300e6a40be3).**\r\n- Added map, partition_n and traverse to data_structures.\r\n- Added \"build your own Haiku\" to the docs.\r\n- Added summarise utility to Haiku.\r\n- Added visualisation section to docs.\r\n- Added precision arg to Linear, Conv and ConvTranspose.\r\n- Added RMSNorm.\r\n- Added module_name and name to GetterContext.\r\n- Added hk.eval_shape.\r\n- Improved performance of non cross-replica BN variance.\r\n- Haiku branch functions are only traced once (mirroring JAX).\r\n- Attention logits are rescaled before the softmax now.\r\n- ModuleMetaclass now inherits from Protocol.\r\n- Removed \"dot access\" to FlatMapping.\r\n- Removed query_size from MultiHeadAttention constructor.\r\n\r\nAny many other small fixes and improvements.\r\n",
        "dateCreated": "2021-04-12T09:32:51Z",
        "datePublished": "2021-04-12T13:12:13Z",
        "html_url": "https://github.com/deepmind/dm-haiku/releases/tag/v0.0.4",
        "name": "Haiku 0.0.4",
        "tag_name": "v0.0.4",
        "tarball_url": "https://api.github.com/repos/deepmind/dm-haiku/tarball/v0.0.4",
        "url": "https://api.github.com/repos/deepmind/dm-haiku/releases/41278404",
        "zipball_url": "https://api.github.com/repos/deepmind/dm-haiku/zipball/v0.0.4"
      },
      {
        "authorType": "User",
        "author_name": "tomhennigan",
        "body": "**Changelog:**\r\n\r\n- Added `hk.experimental.intercept_methods `.\r\n- Added `hk.running_init`.\r\n- Added `hk.experimental.name_scope`.\r\n- Added optional support for state in `custom_creator` and `custom_getter`.\r\n- Added index groups to `BatchNorm`.\r\n- Added interactive notebooks to documentation, including [basics guide](https://dm-haiku.readthedocs.io/en/latest/notebooks/basics.html).\r\n- Added support for batch major unrolls in `static_unroll` and `dynamic_unroll`.\r\n- Added `hk.experimental.abstract_to_dot`.\r\n- Added step markers in imagenet example.\r\n- Added `hk.MultiHeadAttention`.\r\n- Added option to remove double bias from `VanillaRNN`.\r\n- Added support for `feature_group_count` in `ConvND`.\r\n- Added logits config to resnet models.\r\n- Added various control flow primitives (`fori_loop`, `switch`, `while_loop`).\r\n- Added `cross_replica_axis` to `VectorQuantizerEMA`.\r\n- Added `original_shape` to `ParamContext`.\r\n- Added `hk.SeparableDepthwiseConv2D `.\r\n- Added support for `unroll` kwarg to `hk.scan`.\r\n- Added `output_shape` argument to `ConvTranspose` modules.\r\n- Replaced `frozendict` with `FlatMapping`, significantly reduces overheads calling jitted computations.\r\n- Misc changes to ensure parameter dtype follows input dtype.\r\n- Multiple changes to support JAX omnistaging.\r\n- `ExponentialMovingAverage.initialize` now takes shape/dtype not value.\r\n- Replaced optix with optax in examples.\r\n- `hk.Embed` embeddings now created lazily.\r\n- Re-indexed documentation for easier navigation.",
        "dateCreated": "2020-11-24T11:02:39Z",
        "datePublished": "2020-11-24T11:25:05Z",
        "html_url": "https://github.com/deepmind/dm-haiku/releases/tag/v0.0.3",
        "name": "Haiku 0.0.3",
        "tag_name": "v0.0.3",
        "tarball_url": "https://api.github.com/repos/deepmind/dm-haiku/tarball/v0.0.3",
        "url": "https://api.github.com/repos/deepmind/dm-haiku/releases/34356571",
        "zipball_url": "https://api.github.com/repos/deepmind/dm-haiku/zipball/v0.0.3"
      },
      {
        "authorType": "User",
        "author_name": "LenaMartens",
        "body": "**Changelog:**\r\n\r\n  - Changed the default value of `apply_rng` to `True` in `hk.transform` to simplify the `apply_fn` signature. \r\n  - Made `ConvND`, `ConvNDTranspose`, `ResetCore` and pooling modules optionally batched. \r\n  - Added `hk.GroupNorm`.\r\n  - Added `hk.scan`.\r\n  - Changed `hk.BatchNorm` to always create state for moving averages. \r\n  - Changed `use_projection` in `hk.nets.ResNet` to take a sequence of bools.\r\n  - Exposed `hk.net.ResNet.{BlockGroup, BlockV1, BlockV2}`.\r\n  - Added `original_dtype` to `ParamContext` to expose the original parameter dtype to custom_getters.\r\n  - Added `GAN` example notebook.\r\n",
        "dateCreated": "2020-07-29T14:01:33Z",
        "datePublished": "2020-07-29T14:49:50Z",
        "html_url": "https://github.com/deepmind/dm-haiku/releases/tag/v0.0.2",
        "name": "Haiku 0.0.2",
        "tag_name": "v0.0.2",
        "tarball_url": "https://api.github.com/repos/deepmind/dm-haiku/tarball/v0.0.2",
        "url": "https://api.github.com/repos/deepmind/dm-haiku/releases/29076441",
        "zipball_url": "https://api.github.com/repos/deepmind/dm-haiku/zipball/v0.0.2"
      },
      {
        "authorType": "User",
        "author_name": "tomhennigan",
        "body": "Haiku is a simple neural network library for [JAX] developed by some of the\r\nauthors of [Sonnet], a neural network library for [TensorFlow].\r\n\r\n**Changelog:**\r\n\r\nFeatures:\r\n\r\n  - Exposed `hk.nets.ResNet` and addeed `hk.nets.ResNet{18,34,101,152,200}`\r\n  - Added `IdentityCore`.\r\n  - Added `custom_getter` API for advanced parameter manipulation.\r\n  - Added `ConvND` and lifted `N<=3` restriction.\r\n  - Added `tree_size` and `tree_bytes` to easily compute parameter counts. \r\n  - `hk.remat` now only threads changed values (faster compilation).\r\n  - Added support for `@dataclass` to define modules.\r\n  - Added support for splitting >1 key at a time `k1, k2 = hk.next_rng_keys(2)`.\r\n  - *Experimental:* Added `profiler_name_scopes` API to add Haiku names to XProf.\r\n  - *Experimental:* Added `optimize_rng_use` to improve compilation time for models with lots of RNG keys.\r\n\r\nExamples:\r\n\r\n  - Added language model example.\r\n  - Added `VQVAE` example.\r\n\r\nBug fixes:\r\n\r\n  - `LayerNorm` now correctly handles bf16 inputs.\r\n  - `TruncatedNormal` initializer now respects dtype.\r\n\r\nUsability:\r\n\r\n  - Improved error messages for `get_parameter`, `to_module` and others.\r\n  - Reimplemented core modules with \"public\" API (easier to read and fork).\r\n  - Added tests that ensure all public symbols are included in documentation.\r\n  - Added type annotations to more internal code.\r\n\r\n[JAX]: https://github.com/google/jax\r\n[Sonnet]: https://github.com/deepmind/sonnet\r\n[Tensorflow]: https://github.com/tensorflow/tensorflow",
        "dateCreated": "2020-06-04T10:48:16Z",
        "datePublished": "2020-06-04T11:18:34Z",
        "html_url": "https://github.com/deepmind/dm-haiku/releases/tag/v0.0.1",
        "name": "Haiku 0.0.1",
        "tag_name": "v0.0.1",
        "tarball_url": "https://api.github.com/repos/deepmind/dm-haiku/tarball/v0.0.1",
        "url": "https://api.github.com/repos/deepmind/dm-haiku/releases/27217006",
        "zipball_url": "https://api.github.com/repos/deepmind/dm-haiku/zipball/v0.0.1"
      },
      {
        "authorType": "User",
        "author_name": "tomhennigan",
        "body": "# Changes\r\n\r\n## Examples\r\n\r\n- Added VAE example.\r\n- Added pruning example (https://arxiv.org/abs/1710.01878).\r\n- MNIST example uses 300-100-10 MLP.\r\n- Updated imagenet dataset to return correctly scaled examples.\r\n\r\n## Breaking changes\r\n\r\n- State arg to `hk.transform` dropped in favor of `transform_with_state`.\r\n- Decay argument is now required in `BatchNorm`.\r\n\r\n## Features\r\n\r\n- Added `hk.maybe_next_rng_key()`.\r\n- BatchNorm and LayerNorm speed improvements.\r\n- Added support for partition/filter/merge params.\r\n- Haiku now allows running with `jax_numpy_rank_promotion`.\r\n\r\n## Experimental features\r\n\r\n- `hk.experimental.to_dot` - experimental visualisation support.\r\n- `hk.experimental.lift` - experimental purification support.\r\n\r\n## Usability\r\n\r\n- Improved error message when RNG arg is not and RNG.\r\n- Improved documentation.\r\n- Improved test coverage.",
        "dateCreated": "2020-03-26T15:07:25Z",
        "datePublished": "2020-03-26T19:17:29Z",
        "html_url": "https://github.com/deepmind/dm-haiku/releases/tag/v0.0.1-beta",
        "name": "Haiku Beta Release",
        "tag_name": "v0.0.1-beta",
        "tarball_url": "https://api.github.com/repos/deepmind/dm-haiku/tarball/v0.0.1-beta",
        "url": "https://api.github.com/repos/deepmind/dm-haiku/releases/24905714",
        "zipball_url": "https://api.github.com/repos/deepmind/dm-haiku/zipball/v0.0.1-beta"
      },
      {
        "authorType": "User",
        "author_name": "tomhennigan",
        "body": "Haiku is a neural network library for JAX.",
        "dateCreated": "2020-02-20T10:50:55Z",
        "datePublished": "2020-02-20T11:20:44Z",
        "html_url": "https://github.com/deepmind/dm-haiku/releases/tag/v0.0.1-alpha",
        "name": "Haiku Alpha Release",
        "tag_name": "v0.0.1-alpha",
        "tarball_url": "https://api.github.com/repos/deepmind/dm-haiku/tarball/v0.0.1-alpha",
        "url": "https://api.github.com/repos/deepmind/dm-haiku/releases/23836925",
        "zipball_url": "https://api.github.com/repos/deepmind/dm-haiku/zipball/v0.0.1-alpha"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1596,
      "date": "Wed, 22 Dec 2021 15:24:44 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "machine-learning",
      "neural-networks",
      "jax",
      "deep-learning",
      "deep-neural-networks"
    ],
    "technique": "GitHub API"
  }
}