{
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/yaringal/DropoutUncertaintyExps",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2016-09-14T14:55:06Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-18T12:47:13Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9958524530189579
      ],
      "excerpt": "This is the code used for the uncertainty experiments in the paper \"Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning\" (2015), with a few adaptions following recent (2018) feedback from the community (many thanks to @capybaralet for spotting some bugs, and @omegafragger for restructuring the code). This code is based on the code by Jos\u00e9 Miguel Hern\u00e1ndez-Lobato used for his paper \"Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks\". The datasets supplied here are taken from the UCI machine learning repository. Note the data splits used in these experiments (which are identical to the ones used in Hern\u00e1ndez-Lobato's code). Because of the small size of the data, if you split the data yourself you will most likely get different and non-comparable results to the ones here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9539343573711775
      ],
      "excerpt": "Below we report the new results using grid-search (new, with code in this updated repo) vs. results obtained from a re-run of the original code used in the paper which used Bayesian optimisation (paper, code in previous commit). Note that we report slightly different numbers for paper than in the previous commit, due to differences in package versions and hardware from 3 years ago. Further note the improved results in new on some datasets (mostly LL) due to proper grid-search (cases where BayesOpt failed). The other results agree with paper within standard error. If you used the code from the previous commits we advise you evaluate your method again following the stream-lined implementation here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9518000434199677
      ],
      "excerpt": "A summary of the results is reported below (lower RMSE is better, higher test log likelihood (LL) is better; note the \u00b1X reported is standard error and not standard deviation). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Experiments used in \"Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning\"",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/yaringal/DropoutUncertaintyExps/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 105,
      "date": "Fri, 24 Dec 2021 15:23:31 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/yaringal/DropoutUncertaintyExps/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "yaringal/DropoutUncertaintyExps",
    "technique": "GitHub API"
  },
  "invocation": [
    {
      "confidence": [
        0.8542665991533146
      ],
      "excerpt": "The new code (new) uses 10x training epochs and one layer as well, and trains models on the same 20 randomly generated train-test splits of the data. Each training set is further divided into an 80-20 train-validation split to find best hyperparameters, dropout rate and tau value through grid search. Finally, a network is trained on the whole training set using the best hyperparameters and is then tested on the test set.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8290972901990252
      ],
      "excerpt": "Energy Efficiency   | 1.08 \u00b1 0.03 | 0.54 \u00b1 0.06 | -1.72 \u00b1 0.01 | -1.21 \u00b1 0.01 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/yaringal/DropoutUncertaintyExps/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other",
      "url": "https://raw.githubusercontent.com/yaringal/DropoutUncertaintyExps/master/LICENSE"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'COPYRIGHT\\n\\nCopyright (c) 2016, the respective contributors\\nAll rights reserved.\\n\\nLICENSE\\n\\nAll contributions by Yarin Gal are licensed under a\\nCreative Commons Attribution-NonCommercial 4.0 International License.\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met:\\n\\n1. Redistributions of source code must retain the above copyright notice, this\\n   list of conditions and the following disclaimer.\\n2. Redistributions in binary form must reproduce the above copyright notice,\\n   this list of conditions and the following disclaimer in the documentation\\n   and/or other materials provided with the distribution.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR\\nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\\nON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Gal2015Dropout), with a few adaptions following recent (2018) feedback from the community (many thanks to @capybaralet for spotting some bugs, and @omegafragger for restructuring the code). This code is based on the code by Jos\u00e9 Miguel Hern\u00e1ndez-Lobato used for his paper \"Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks\". The datasets supplied here are taken from the UCI machine learning repository. Note the data splits used in these experiments (which are identical to the ones used in Hern\u00e1ndez-Lobato's code). Because of the small size of the data, if you split the data yourself you will most likely get different and non-comparable results to the ones here.",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "DropoutUncertaintyExps",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "yaringal",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/yaringal/DropoutUncertaintyExps/blob/master/readme.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 403,
      "date": "Fri, 24 Dec 2021 15:23:31 GMT"
    },
    "technique": "GitHub API"
  }
}