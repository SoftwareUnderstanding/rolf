{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1409.3215",
      "https://arxiv.org/abs/1406.1078"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9421358370175823,
        0.9984822083747434
      ],
      "excerpt": "* [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215) \n* [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dewanderelex/LanguageTranslation",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-06-23T08:41:44Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-06-23T08:46:01Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9716183057066872,
        0.8587006201929553,
        0.8854045648177777
      ],
      "excerpt": "For this project we build a RNN sequence-to-sequence learning in Keras to translate a language A to a language B. \nSince I am french, I choose to translate english to french. However our system is pretty general and accepts any other language pair (e.g. english/french). By defauft, we use ANKI dataset which can be easy download there \nSequence-to-sequence learning (Seq2Seq) is about training models to convert sequences from one domain to sequences in another domain. It works as following: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8838308389916254,
        0.9260818127083772,
        0.8083073328158914
      ],
      "excerpt": "An encoder LSTM turns input sequences to 2 state vectors (we keep the last LSTM state and discard the outputs). \nA decoder LSTM is trained to turn the target sequences into the same sequence but offset by one timestep in the future,     a training process called \"teacher forcing\" in this context.  Is uses as initial state the state vectors from the encoder.     Effectively, the decoder learns to generate targets[t+1...] given targets[...t], conditioned on the input sequence. \nIn inference mode, when we want to decode unknown input sequences, we: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9415900033645721
      ],
      "excerpt": "Feed the state vectors and 1-char target sequence to the decoder to produce predictions for the next character \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8413098615109286,
        0.956231488580479,
        0.9013085280915597
      ],
      "excerpt": "Append the sampled character to the target sequence \nRepeat until we generate the end-of-sequence character or we hit the character limit. \nFor more information, please check these papers: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9788519490684758,
        0.9495201740910509
      ],
      "excerpt": "By default, the model runs with LSTM cell (long short term memory), but we also provide the user the opportunity to use instead GRU cell. (GRU cell only include 1 gate which is meke the training faster) \nFor sure, our system is far from being as accurate as Google Transle. But after 20 epoch only, it reconnizes accurately short sentences. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.848292773197377
      ],
      "excerpt": "It is accurate. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.848292773197377
      ],
      "excerpt": "It is accurate. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8178640310880632,
        0.9480067311007069
      ],
      "excerpt": "The translation is not correct. \nTo conclude, our network learnt the basic concept of english/french, but it still requires two things: \n",
      "technique": "Supervised classification"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We trained this model on the complete English/French dataset. The all training takes weeks. But we got promising results after 18 h of training (20 epoch). You can download our weights [there](https://drive.google.com/open?id=12s5KVDXex1Icy5FeFMLtQ2ADuWupzG_u)\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dewanderelex/LanguageTranslation/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sat, 25 Dec 2021 10:01:12 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/dewanderelex/LanguageTranslation/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "dewanderelex/LanguageTranslation",
    "technique": "GitHub API"
  },
  "invocation": [
    {
      "confidence": [
        0.8850063701822648
      ],
      "excerpt": "Example of output: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8103548402846317
      ],
      "excerpt": "Input sentence: I slept well. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/dewanderelex/LanguageTranslation/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Language Translation with deep learning",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "LanguageTranslation",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "dewanderelex",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dewanderelex/LanguageTranslation/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sat, 25 Dec 2021 10:01:12 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Downlow you training dataset\n2. Update path and the number of training example\n3. Run ```python3 training.py ```\t\n4. Prediction with ```python3 predictionTranslation.py```\n\t\n",
      "technique": "Header extraction"
    }
  ]
}