{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2002.07016\"><strong>Paper</strong></a>\n<p>\n  \n\n  \n<br>\n\nWe propose a hierarchical meta-learning-inspired model for music source separation in which **a generator model is used to predict the weights of individual extractor models**.  This enables efficient parameter-sharing, while still allowing for instrument-specific parameterization.  The resulting models are shown to be more effective than those trained independently or in a multi-task setting, and achieve performance comparable with state-of-the-art methods.\n\n<br>\n\n## Brief Introduction to Music Source Separation\n\nGiven a mixed source signal, the task of source separation algorithm is to divide the signal into its original components. We test our method on music separation and specifically on the [MUSDB18 dataset](https://zenodo.org/record/1117372#.XiSY9Bco9QJ",
      "https://arxiv.org/abs/1809.07454"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\n@inproceedings={meta-tasnet:2020,\n    title={Meta-learning Extractors for Music Source Separation},\n    author={David Samuel and Aditya Ganeshan and Jason Naradowsky},\n    booktitle={IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\n    pages={816-820},\n    year={2020},\n}\n```\n<br>\n\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/pfnet-research/meta-tasnet",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-02-18T12:52:49Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-10T19:46:01Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Given a mixed source signal, the task of source separation algorithm is to divide the signal into its original components. We test our method on music separation and specifically on the [MUSDB18 dataset](https://zenodo.org/record/1117372#.XiSY9Bco9QJ) where the sources consist of contemporary songs and the goal is to divide them into four stems: \n\n   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:drum::shark:&nbsp;&nbsp; **drums**  \n   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:studio_microphone::rabbit2:&nbsp;&nbsp; **vocals**  \n   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:guitar::eagle:&nbsp;&nbsp; **bass**  \n   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:saxophone::snake:&nbsp;&nbsp; **other accompaniments**\n \n \nMusic source separation can not only be used as a preprocessing step to other MIR problems (like sound source identification), but it can also be used more creatively: we can create backing tracks to any song for musical practice or just for fun (karaoke), we can create \"smart\" equilizers that are able to make a new remix, or we can separate a single instrument to better study its intricacies (guitar players can more easily determine the exact chords for example). \n\n<br>\n\n<p align=\"center\">\n  <img src=\"img/spectrogram.png\" alt=\"Spectrogram illustration.\" width=\"600\"/>  \n</p>\n\n<p align=\"center\">\n  <sub><em>Illustration of a separated audio signal (projected on log-scaled spectrograms). The top spectrogram shows the mixed audio that is transformed into the four separated components at the bottom. Note that we use the spectrograms just to illustrate the task \u2014 our model operates directly on the audio waveforms.</em></sub>\n</p>\n\n<br>\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9924839964025884,
        0.9872145737725959,
        0.940553007036228
      ],
      "excerpt": "The key idea is to utilize a tiered architecture where a generator network \"supervises\" the training of the individual extractors by generating some of their parameters directly.  This allows the generator to develop a dense representation of how instruments relate to each other as it pertains to the task, and to utilize their commonalities when generating each extractor. \nOur model is based on Conv-TasNet, a time domain-based approach to speech separation comprising three parts:  \n1. an encoder which applies a 1-D convolutional transform to a segment of the mixture waveform to produce a high-dimensional representation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9728224248665899
      ],
      "excerpt": "The masking network is of particular interest, as it contains the source-specific masking information; the encoder and decoder are source-agnostic and remain fixed for separation of all sources. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8828104997419665
      ],
      "excerpt": "Despite the data's higher sampling rate (44kHz), we find that models trained using lower sampling rates are more effective despite the loss in resolution.  We therefore propose a multi-stage architecture to leverage this strength while still fundamentally predicting high resolution audio and use three stages with 8, 16 and 32kHz sampling rates. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9562917726081601
      ],
      "excerpt": "  <sub><em>Illustration of the multi-stage architecture. The resolution of the estimated signal is progressively enhanced by utilizing information from previous stages. The encoders increase the stride <strong>s</strong> to preserve the same time dimension <strong>T'</strong>. Note that the masking TCN is still generated (not included in the illustration).</em></sub> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8216715361118995,
        0.9962618318055937
      ],
      "excerpt": "signal-to-distortion ratio (SDR) evaluated with BSSEval v4 \nresults are in dB, higher is better (median of frames, median of tracks) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8739262087986751
      ],
      "excerpt": "A pretrained model on the MUSDB18 dataset can be downloaded from here. After downloading, load the model by the following Python lines. An example usage of the pretrained model for separation can be seen in the aforementioned Google Colab notebook. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.945705337351011
      ],
      "excerpt": "network = MultiTasNet(state[\"args\"]).to(device)  #: initialize the model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A PyTorch implementation of Meta-TasNet from \"Meta-learning Extractors for Music Source Separation",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/pfnet-research/meta-tasnet/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 14,
      "date": "Wed, 22 Dec 2021 18:15:38 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/pfnet-research/meta-tasnet/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "pfnet-research/meta-tasnet",
    "technique": "GitHub API"
  },
  "invocation": [
    {
      "confidence": [
        0.8748458625402364
      ],
      "excerpt": "  <img src=\"/img/results.png\" width=\"400\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8353891623213743
      ],
      "excerpt": "A pretrained model on the MUSDB18 dataset can be downloaded from here. After downloading, load the model by the following Python lines. An example usage of the pretrained model for separation can be seen in the aforementioned Google Colab notebook. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/pfnet-research/meta-tasnet/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 Preferred Networks, Inc.\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Brief Introduction to Music Source Separation",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "meta-tasnet",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "pfnet-research",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/pfnet-research/meta-tasnet/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1) First, you have to download the [MUSDB18 dataset](https://zenodo.org/record/1117372#.XiB9Cxco9QI) and run the data generator to resample to music stems and save them as numpy arrays: `python3 data_generator.py --musdb_path path/to/the/downloaded/dataset`.\n\n2) After creating the dataset, you can start the training by running `python3 train.py`. Please note that this configuration was trained on 2 Nvidia V100 GPUs so you need ~64 GB of GPU memory to train with the default batch size.\n\n3) Finally, you can evaluate the model by running `python3 evaluate.py --model_dir directory --musdb_path path/to/the/downloaded/dataset`.\n\n<br>\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 111,
      "date": "Wed, 22 Dec 2021 18:15:38 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You can try an interactive demo of the pretrained model in [**Google Colab notebook**](https://colab.research.google.com/drive/1iVFGlRuhdpjtnO3a7ATzgd-lnPxtd4oT).\n\n<br>\n\n",
      "technique": "Header extraction"
    }
  ]
}