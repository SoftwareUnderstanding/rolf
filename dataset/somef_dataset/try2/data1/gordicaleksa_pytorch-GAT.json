{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "I found these repos useful (while developing this one):\n\n* [official GAT](https://github.com/PetarV-/GAT) and [GCN](https://github.com/tkipf/gcn)\n* [PyTorch Geometric](https://github.com/rusty1s/pytorch_geometric)\n* [DeepInf](https://github.com/xptree/DeepInf) and [pyGAT](https://github.com/Diego999/pyGAT)\n\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1710.10903",
      "https://arxiv.org/abs/1802.00543",
      "https://arxiv.org/abs/1902.06673"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find this code useful, please cite the following:\n\n```\n@misc{Gordi\u01072020PyTorchGAT,\n  author = {Gordi\u0107, Aleksa},\n  title = {pytorch-GAT},\n  year = {2020},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/gordicaleksa/pytorch-GAT}},\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{Gordi\u01072020PyTorchGAT,\n  author = {Gordi\u0107, Aleksa},\n  title = {pytorch-GAT},\n  year = {2020},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/gordicaleksa/pytorch-GAT}},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8854398367006624
      ],
      "excerpt": "* Recommendation systems (used at Pintrest, Uber, Twitter, etc.)  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8502644800268612
      ],
      "excerpt": "and pasting the http://localhost:6006/ URL into your browser: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8302556419090275
      ],
      "excerpt": "<a href=\"https://www.youtube.com/watch?v=uFLeKkXWq2c\" target=\"_blank\"><img src=\"https://img.youtube.com/vi/uFLeKkXWq2c/0.jpg\"  \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/gordicaleksa/pytorch-GAT",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-12-26T09:54:52Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-21T07:48:50Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9535332760059066,
        0.9378563697939369,
        0.87965460472167,
        0.908925214220865
      ],
      "excerpt": "This repo contains a PyTorch implementation of the original GAT paper (:link: Veli\u010dkovi\u0107 et al.). <br/> \nIt's aimed at making it easy to start playing and learning about GAT and GNNs in general. <br/> \nWhat are graph neural networks and GAT? \nVisualizations (Cora and PPI, attention, t-SNE embeddings, entropy histograms) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8310863300303505
      ],
      "excerpt": "Tips for understanding the code \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9277312214835278
      ],
      "excerpt": "Visualization tools \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9428419916454098
      ],
      "excerpt": "Graph neural networks are a family of neural networks that are dealing with signals defined over graphs! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8997028565306401
      ],
      "excerpt": "* Traffic forecasting - e.g. it's used in Google Maps \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9610737207190698,
        0.8939313556127261,
        0.938143053980041,
        0.9425644138492615
      ],
      "excerpt": "and all the way to particle physics at Large Hedron Collider (LHC), fake news detection and the list goes on and on! \nGAT is a representative of spatial (convolutional) GNNs. Since CNNs had a tremendous success in the field of computer vision, \nresearchers decided to generalize it to graphs and so here we are! :nerd_face: \nHere is a schematic of GAT's structure: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.871075985072619
      ],
      "excerpt": "I've added a utility for visualizing Cora and doing basic network analysis. Here is how Cora looks like: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8537704422542036,
        0.9565652460921427
      ],
      "excerpt": "Node size corresponds to its degree (i.e. the number of in/outgoing edges). Edge thickness roughly corresponds \nto how \"popular\" or \"connected\" that edge is (edge betweennesses is the nerdy term check out the code.) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9433514852353956
      ],
      "excerpt": "In and out degree plots are the same since we're dealing with an undirected graph.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9330182293911007
      ],
      "excerpt": "This means that the majority of nodes have a small number of edges but there is 1 node that has 169 edges! (the big green node) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8654938876850102
      ],
      "excerpt": "This is one of Cora's nodes that has the most edges (citations). The colors represent the nodes of the same class. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8546115004132413,
        0.9887721303552122,
        0.8479625256082068
      ],
      "excerpt": "* The graph is homophilic meaning similar nodes (nodes with same class) tend to cluster together. \n* Edge thickness on this chart is a function of attention, and since they are all of the same thickness, GAT basically learned to do something similar to GCN! \nSimilar rules hold for smaller neighborhoods. Also notice the self edges: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9586525869362296
      ],
      "excerpt": "On the other hand PPI is learning much more interesting attention patterns: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9712796445992807,
        0.9924722890830705,
        0.957949223492152,
        0.9948867593449984
      ],
      "excerpt": "On the left we can see that 6 neighbors are receiving a non-negligible amount of attention and on the right we can \nsee that all of the attention is focused onto a single neighbor. \nFinally 2 more interesting patterns - a strong self edge on the left and on the right we can see that a single neighbor \nis receiving a bulk of attention whereas the rest is equally distributed across the rest of the neighborhood: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9045652643801507,
        0.8979348073755933,
        0.9319938138813597
      ],
      "excerpt": "For some reason the attention coefficients for the second and third layers are almost all 0s (even though I achieved the published results). \nAnother way to understand that GAT isn't learning interesting attention patterns on Cora (i.e. that it's learning const attention) \nis by treating the node neighborhood's attention weights as a probability distribution, calculating the entropy, and \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9586525869362296
      ],
      "excerpt": "On the other hand PPI is learning much more interesting attention patterns: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8724219914539044,
        0.9278497819236843,
        0.9846973277589354,
        0.9730394690873914
      ],
      "excerpt": "As expected, the uniform distribution entropy histogram lies to the right (orange) since uniform distributions have the highest entropy. \nOk, we've seen attention! What else is there to visualize? Well, let's visualize the learned embeddings from GAT's \nlast layer. The output of GAT is a tensor of shape = (2708, 7) where 2708 is the number of nodes in Cora and 7 is \nthe number of classes. Once we project those 7-dim vectors into 2D, using t-SNE, we get this: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9394215321510485
      ],
      "excerpt": "We can see that the nodes with the same label/class are roughly clustered together - with these representations it's easy \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9652246122545198,
        0.9742881978157546
      ],
      "excerpt": "* add the --should_visualize - to visualize your graph data \n* add the --should_test - to evaluate GAT on the test portion of the data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8467404579499141
      ],
      "excerpt": "and pasting the http://localhost:6006/ URL into your browser: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9018013844378416,
        0.838370823837343,
        0.8665612739486779,
        0.9122822155916397
      ],
      "excerpt": "I've added 3 GAT implementations - some are conceptually easier to understand some are more efficient. \nThe most interesting and hardest one to understand is implementation 3. \nImplementation 1 and implementation 2 differ in subtle details but basically do the same thing. \nAdvice on how to approach the code: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8264316900801212
      ],
      "excerpt": "* Check out the differences it has compared to implementation #1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8144119335767697
      ],
      "excerpt": "Note: implementation #3 is by far the most optimized one - you can see the details in the code. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8300652037031135
      ],
      "excerpt": "set the visualization_type to: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8869453376865275
      ],
      "excerpt": "On the left you can see the node with the highest degree in the whole Cora dataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.921416625079373,
        0.9891785691729262
      ],
      "excerpt": "which is particularly well suited for tree like graphs (since we're visualizing a node and its neighbors this \nsubgraph is effectively a m-ary tree). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.931244552033037
      ],
      "excerpt": "tend to perform the best on small-world, homophilic graph datasets. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8542809360610284
      ],
      "excerpt": "Figure out why are the attention coefficients equal to 0 (for the PPI dataset, second and third layer) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8452954376551373,
        0.8015564130015924,
        0.8956570879548531,
        0.8710660995841388,
        0.819264983674404
      ],
      "excerpt": "If you have an idea of how to implement GAT using PyTorch's sparse API please feel free to submit a PR. \nI personally had difficulties with their API, it's in beta, and it's questionable whether it's at all possible \nto make an implementation as efficient as my implementation 3 using it. \nSecondly, I'm still not sure why is GAT achieving reported results on PPI while there are some obvious numeric \nproblems in deeper layers as manifested by all attention coefficients being equal to 0. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "My implementation of the original GAT paper (Veli\u010dkovi\u0107 et al.). I've additionally included the playground.py file for visualizing the Cora dataset, GAT embeddings, an attention mechanism, and entropy histograms. I've supported both Cora (transductive) and PPI (inductive) examples!",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/gordicaleksa/pytorch-GAT/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 169,
      "date": "Tue, 21 Dec 2021 16:02:59 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/gordicaleksa/pytorch-GAT/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "gordicaleksa/pytorch-GAT",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/gordicaleksa/pytorch-GAT/main/The%20Annotated%20GAT%20%28Cora%29.ipynb",
      "https://raw.githubusercontent.com/gordicaleksa/pytorch-GAT/main/The%20Annotated%20GAT%20%28PPI%29.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "So we talked about what GNNs are, and what they can do for you (among other things). <br/>\nLet's get this thing running! Follow the next steps:\n\n1. `git clone https://github.com/gordicaleksa/pytorch-GAT`\n2. Open Anaconda console and navigate into project directory `cd path_to_repo`\n3. Run `conda env create` from project directory (this will create a brand new conda environment).\n4. Run `activate pytorch-gat` (for running scripts from your console or setup the interpreter in your IDE)\n\nThat's it! It should work out-of-the-box executing environment.yml file which deals with dependencies. <br/>\n\n-----\n\nPyTorch pip package will come bundled with some version of CUDA/cuDNN with it,\nbut it is highly recommended that you install a system-wide CUDA beforehand, mostly because of the GPU drivers. \nI also recommend using Miniconda installer as a way to get conda on your system.\nFollow through points 1 and 2 of [this setup](https://github.com/Petlja/PSIML/blob/master/docs/MachineSetup.md)\nand use the most up-to-date versions of Miniconda and CUDA/cuDNN for your system.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8864574716181884
      ],
      "excerpt": "Hardware requirements \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9048276856622764
      ],
      "excerpt": "We'd love GAT's attention distributions to be skewed. You can see in orange how the histogram looks like for ideal uniform distributions, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9927186821877624
      ],
      "excerpt": "Just do pip uninstall pywin32 and then either pip install pywin32 or conda install pywin32 should fix it! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.888406430009006
      ],
      "excerpt": "Everything needed to train GAT on Cora is already setup. To run it (from console) just call: <br/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8090130929852599
      ],
      "excerpt": "* Save metrics into runs/, just run tensorboard --logdir=runs from your Anaconda to visualize it \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9692937281531144
      ],
      "excerpt": "you don't have a strong GPU with at least 8 GBs you'll need to add the --force_cpu flag to train GAT on CPU. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8148829034643754
      ],
      "excerpt": "On the left you can see the node with the highest degree in the whole Cora dataset. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8174540907975313
      ],
      "excerpt": "Training GAT \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9078564040380478
      ],
      "excerpt": "<img src=\"data/readme_pics/GAT_schematic.PNG\" width=\"600\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9052924254802468
      ],
      "excerpt": "<img src=\"data/readme_pics/cora_graph_jupyter.PNG\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9078564040380478
      ],
      "excerpt": "<img src=\"data/readme_pics/cora_degree_statistics.PNG\" width=\"850\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9330184127168011
      ],
      "excerpt": "<img src=\"data/readme_pics/attention1.jpg\" width=\"600\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9361727459014461,
        0.9361727459014461
      ],
      "excerpt": "<img src=\"data/readme_pics/attention2.jpg\" width=\"400\"/> \n<img src=\"data/readme_pics/attention4.jpg\" width=\"400\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9361727459014461,
        0.9361727459014461
      ],
      "excerpt": "<img src=\"data/readme_pics/neighborhood_attention_ppi/3.jpg\" width=\"400\"/> \n<img src=\"data/readme_pics/neighborhood_attention_ppi/2.jpg\" width=\"400\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9361727459014461,
        0.9361727459014461
      ],
      "excerpt": "<img src=\"data/readme_pics/neighborhood_attention_ppi/4.jpg\" width=\"400\"/> \n<img src=\"data/readme_pics/neighborhood_attention_ppi/1.jpg\" width=\"400\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9361727459014461,
        0.9361727459014461
      ],
      "excerpt": "<img src=\"data/readme_pics/entropy_histograms/layer_0_head_0.jpg\" width=\"400\"/> \n<img src=\"data/readme_pics/entropy_histograms/layer_1_head_0.jpg\" width=\"400\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9361727459014461
      ],
      "excerpt": "<img src=\"data/readme_pics/entropy_histograms_ppi/layer_0_head_0.jpg\" width=\"400\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9078564040380478
      ],
      "excerpt": "<img src=\"data/readme_pics/t-sne.PNG\" width=\"600\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python training_script_cora.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8210266215504783
      ],
      "excerpt": "* Dump checkpoint .pth models into models/checkpoints/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8160731142741525,
        0.8427994671642617
      ],
      "excerpt": "* Periodically write some training metadata to the console \nSame goes for training on PPI, just run python training_script_ppi.py. PPI is much more GPU-hungry so if \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9014794620903522,
        0.9014794620903522
      ],
      "excerpt": "<img src=\"data/readme_pics/val_loss.PNG\" height=\"290\"/> \n<img src=\"data/readme_pics/val_acc.PNG\" height=\"290\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9330184127168011,
        0.9330184127168011
      ],
      "excerpt": "<img src=\"data/readme_pics/attention3.jpg\" width=\"410\"/> \n<img src=\"data/readme_pics/kk_layout.jpg\" width=\"410\"/> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/gordicaleksa/pytorch-GAT/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# GAT - Graph Attention Network (PyTorch) :computer: + graphs + :mega: = :heart:",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "pytorch-GAT",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "gordicaleksa",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/gordicaleksa/pytorch-GAT/blob/main/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "HW requirements are highly dependent on the graph data you'll use. If you just want to play with `Cora`, you're good to go with a **2+ GBs** GPU.\n\nIt takes (on Cora citation network):\n* ~10 seconds to train GAT on my RTX 2080 GPU\n* 1.5 GBs of VRAM memory is *reserved* (PyTorch's caching overhead - far less is allocated for the actual tensors)\n* The model itself has only 365 KBs!\n\nCompare this to hardware needed even for the smallest of [transformers](https://github.com/gordicaleksa/pytorch-original-transformer#hardware-requirements)!\n\nOn the other hand the `PPI` dataset is much more GPU-hungry. You'll need a GPU with **8+ GBs** of VRAM, or you\ncan reduce the batch size to 1 and make the model \"slimmer\" and thus try to reduce the VRAM consumption.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1438,
      "date": "Tue, 21 Dec 2021 16:02:59 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "gat",
      "graph-attention-networks",
      "attention-mechanism",
      "self-attention",
      "pytorch",
      "python",
      "attention",
      "pytorch-gat",
      "gat-tutorial",
      "deep-learning",
      "graph-attention-network",
      "pytorch-implementation",
      "jupyter"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You just need to link the Python environment you created in the [setup](#setup) section.\n\n",
      "technique": "Header extraction"
    }
  ]
}