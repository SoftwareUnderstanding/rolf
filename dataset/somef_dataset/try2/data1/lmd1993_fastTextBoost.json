{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1607.04606",
      "https://arxiv.org/abs/1607.01759",
      "https://arxiv.org/abs/1612.03651",
      "https://arxiv.org/abs/1607.04606",
      "https://arxiv.org/abs/1607.01759",
      "https://arxiv.org/abs/1612.03651"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please cite [1](#enriching-word-vectors-with-subword-information) if using this code for learning word representations or [2](#bag-of-tricks-for-efficient-text-classification) if using for text classification.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{joulin2016fasttext,\n  title={FastText.zip: Compressing text classification models},\n  author={Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Douze, Matthijs and J{\\'e}gou, H{\\'e}rve and Mikolov, Tomas},\n  journal={arXiv preprint arXiv:1612.03651},\n  year={2016}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{joulin2016bag,\n  title={Bag of Tricks for Efficient Text Classification},\n  author={Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},\n  journal={arXiv preprint arXiv:1607.01759},\n  year={2016}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{bojanowski2016enriching,\n  title={Enriching Word Vectors with Subword Information},\n  author={Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},\n  journal={arXiv preprint arXiv:1607.04606},\n  year={2016}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lmd1993/fastTextBoost",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contributing to fastText\nWe want to make contributing to this project as easy and transparent as\npossible.\nPull Requests\nWe actively welcome your pull requests.\n\nFork the repo and create your branch from master.\nIf you've added code that should be tested, add tests.\nIf you've changed APIs, update the documentation.\nEnsure the test suite passes.\nMake sure your code lints.\nIf you haven't already, complete the Contributor License Agreement (\"CLA\").\n\nContributor License Agreement (\"CLA\")\nIn order to accept your pull request, we need you to submit a CLA. You only need\nto do this once to work on any of Facebook's open source projects.\nComplete your CLA here: https://code.facebook.com/cla\nIssues\nWe use GitHub issues to track public bugs. Please ensure your description is\nclear and has sufficient instructions to be able to reproduce the issue.\nLicense\nBy contributing to fastText, you agree that your contributions will be licensed\nunder its BSD license.",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-04-29T02:54:01Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-04-29T03:30:17Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9769698115976808
      ],
      "excerpt": "fastText is a library for efficient learning of word representations and sentence classification. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8273099514127742,
        0.8445442627931573,
        0.9156813420755983
      ],
      "excerpt": "This will produce object files for all the classes as well as the main binary fasttext. \nIf you do not plan on using the default system-wide compiler, update the two macros defined at the beginning of the Makefile (CC and INCLUDES). \nIn order to learn word vectors, as described in 1, do: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9483541808483614
      ],
      "excerpt": "At the end of optimization the program will save two files: model.bin and model.vec. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9781224015614493,
        0.894958727550187,
        0.8852187335675773
      ],
      "excerpt": "model.bin is a binary file containing the parameters of the model along with the dictionary and all hyper parameters. \nThe binary file can be used later to compute word vectors or to restart the optimization. \nThe previously trained model can be used to compute word vectors for out-of-vocabulary words. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8846431569672244
      ],
      "excerpt": "By default, we assume that labels are words that are prefixed by the string __label__. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9614742823684236,
        0.8272065922480798
      ],
      "excerpt": "The argument k is optional, and is equal to 1 by default. \nIn order to obtain the k most likely labels for a piece of text, use: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9186981690125441
      ],
      "excerpt": "The argument k is optional, and equal to 1 by default. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8230558135694459
      ],
      "excerpt": "This assumes that the text.txt file contains the paragraphs that you want to get vectors for. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.838895537599196
      ],
      "excerpt": "This will create a .ftz file with a smaller memory footprint. All the standard functionality, like test or predict work the same way on the quantized models: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8965799723048201
      ],
      "excerpt": "[2] A. Joulin, E. Grave, P. Bojanowski, T. Mikolov, Bag of Tricks for Efficient Text Classification \n",
      "technique": "Supervised classification"
    }
  ],
  "documentation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Invoke a command without arguments to list available arguments and their default values:\n\n```\n$ ./fasttext supervised\nEmpty input or output path.\n\nThe following arguments are mandatory:\n  -input              training file path\n  -output             output file path\n\n  The following arguments are optional:\n  -verbose            verbosity level [2]\n\n  The following arguments for the dictionary are optional:\n  -minCount           minimal number of word occurences [5]\n  -minCountLabel      minimal number of label occurences [0]\n  -wordNgrams         max length of word ngram [1]\n  -bucket             number of buckets [2000000]\n  -minn               min length of char ngram [3]\n  -maxn               max length of char ngram [6]\n  -t                  sampling threshold [0.0001]\n  -label              labels prefix [__label__]\n\n  The following arguments for training are optional:\n  -lr                 learning rate [0.05]\n  -lrUpdateRate       change the rate of updates for the learning rate [100]\n  -dim                size of word vectors [100]\n  -ws                 size of the context window [5]\n  -epoch              number of epochs [5]\n  -neg                number of negatives sampled [5]\n  -loss               loss function {ns, hs, softmax} [ns]\n  -thread             number of threads [12]\n  -pretrainedVectors  pretrained word vectors for supervised learning []\n  -saveOutput         whether output params should be saved [0]\n\n  The following arguments for quantization are optional:\n  -cutoff             number of words and ngrams to retain [0]\n  -retrain            finetune embeddings if a cutoff is applied [0]\n  -qnorm              quantizing the norm separately [0]\n  -qout               quantizing the classifier [0]\n  -dsub               size of each sub-vector [2]\n```\n\nDefaults may vary by mode. (Word-representation modes `skipgram` and `cbow` use a default `-minCount` of 5.)\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lmd1993/fastTextBoost/releases",
    "technique": "GitHub API"
  },
  "faq": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You can find [answers to frequently asked questions](https://fasttext.cc/docs/en/faqs.html#content) on our [website](https://fasttext.cc/).\n\nWe also provide a [cheatsheet](https://fasttext.cc/docs/en/cheatsheet.html#content) full of useful one-liners.\n\n",
      "technique": "Header extraction"
    }
  ],
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 28 Dec 2021 05:26:45 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/lmd1993/fastTextBoost/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "lmd1993/fastTextBoost",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/lmd1993/fastTextBoost/master/classification-results.sh",
      "https://raw.githubusercontent.com/lmd1993/fastTextBoost/master/wordJustExample.sh",
      "https://raw.githubusercontent.com/lmd1993/fastTextBoost/master/classification-example.sh",
      "https://raw.githubusercontent.com/lmd1993/fastTextBoost/master/word-vector-example.sh",
      "https://raw.githubusercontent.com/lmd1993/fastTextBoost/master/get-wikimedia.sh",
      "https://raw.githubusercontent.com/lmd1993/fastTextBoost/master/quantization-example.sh",
      "https://raw.githubusercontent.com/lmd1993/fastTextBoost/master/quantization-results.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8632430548072154,
        0.982061580294172,
        0.9906248903846466,
        0.8474895321345809
      ],
      "excerpt": "In order to build fastText, use the following: \n$ git clone https://github.com/facebookresearch/fastText.git \n$ cd fastText \n$ make \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8147438040199454
      ],
      "excerpt": "You can also quantize a supervised model to reduce its memory usage with the following command: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.901453818582758,
        0.9103756700490212
      ],
      "excerpt": "$ ./fasttext skipgram -input data.txt -output model \nwhere data.txt is a training file containing utf-8 encoded text. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9070631331869214
      ],
      "excerpt": "model.vec is a text file containing the word vectors, one per line. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8553814334179051
      ],
      "excerpt": "This will output word vectors to the standard output, one vector per line. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8643354341371444
      ],
      "excerpt": "$ cat queries.txt | ./fasttext print-word-vectors model.bin \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8839158252682724,
        0.9437964632054884
      ],
      "excerpt": "$ ./fasttext supervised -input train.txt -output model \nwhere train.txt is a text file containing a training sentence per line along with the labels. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8271340404619316
      ],
      "excerpt": "This will output two files: model.bin and model.vec. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8951381859294457
      ],
      "excerpt": "$ ./fasttext test model.bin test.txt k \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9118148969249582,
        0.8919829784350816,
        0.855185380383668
      ],
      "excerpt": "$ ./fasttext predict model.bin test.txt k \nwhere test.txt contains a piece of text to classify per line. \nDoing so will print to the standard output the k most likely labels for each line. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8530417207217321
      ],
      "excerpt": "$ ./fasttext print-sentence-vectors model.bin &lt; text.txt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8906678155209428
      ],
      "excerpt": "The program will output one vector representation per line in the file. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.857129346874625
      ],
      "excerpt": "$ ./fasttext quantize -output model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8910782061406535
      ],
      "excerpt": "$ ./fasttext test model.ftz test.txt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.861059532256563
      ],
      "excerpt": "run the script quantization-example.sh for an example. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8514016617147449
      ],
      "excerpt": "[3] A. Joulin, E. Grave, P. Bojanowski, M. Douze, H. J\u00e9gou, T. Mikolov, FastText.zip: Compressing text classification models \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/lmd1993/fastTextBoost/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "C++",
      "Shell",
      "Python",
      "Perl",
      "Makefile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other",
      "url": "https://raw.githubusercontent.com/lmd1993/fastTextBoost/master/LICENSE"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'BSD License\\n\\nFor fastText software\\n\\nCopyright (c) 2016-present, Facebook, Inc. All rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without modification,\\nare permitted provided that the following conditions are met:\\n\\n * Redistributions of source code must retain the above copyright notice, this\\n   list of conditions and the following disclaimer.\\n\\n * Redistributions in binary form must reproduce the above copyright notice,\\n   this list of conditions and the following disclaimer in the documentation\\n   and/or other materials provided with the distribution.\\n\\n * Neither the name Facebook nor the names of its contributors may be used to\\n   endorse or promote products derived from this software without specific\\n   prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR\\nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\\nANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "fastText",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "fastTextBoost",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "lmd1993",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lmd1993/fastTextBoost/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "**fastText** builds on modern Mac OS and Linux distributions.\nSince it uses C++11 features, it requires a compiler with good C++11 support.\nThese include :\n\n* (gcc-4.6.3 or newer) or (clang-3.3 or newer)\n\nCompilation is carried out using a Makefile, so you will need to have a working **make**.\nFor the word-similarity evaluation script you will need:\n\n* python 2.6 or newer\n* numpy & scipy\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 28 Dec 2021 05:26:45 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This library has two main use cases: word representation learning and text classification.\nThese were described in the two papers [1](#enriching-word-vectors-with-subword-information) and [2](#bag-of-tricks-for-efficient-text-classification).\n\n",
      "technique": "Header extraction"
    }
  ]
}