{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1511.05952\n        tau_initial (float"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9325182562707431
      ],
      "excerpt": "    beta_rate (float): Rate (0 to 1) for increasing beta to 1 as per Schauel et al.         https://arxiv.org/abs/1511.05952 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/snhwang/p1_navigation_SNH",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-10-29T22:06:16Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-11-21T03:46:02Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9556000753789495,
        0.9942441194039251,
        0.9505210557771488,
        0.9510090643128051
      ],
      "excerpt": "This project was one of the requirements for completing the Deep Reinforcement Learning Nanodegree (DRLND) course at Udacity.com. The preceding lessons focused on deep Q networks. \nA learning agent is trained to navigate and collect bananas in a finite square world shown in the clip below. Collecting a yellow banana results in a reward of +1 while collecting a blue banana results in a negative reward of -1. The environment was pre-built for the project using the Unity ML-agents toolkit. \n\u200b                   (From the Udacity course project introduction) \nThe state space has 37 dimensions. Parameters characterize the agent's velocity, along with ray-based perception of objects around the agent's forward direction.  Given this information, the agent ideally  learns how to select actions that increase the score \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9928550618637545
      ],
      "excerpt": "The environment is episodic. The stated goal of the project is to have the learning agent achieve a score of at least +13 averaged over 100 consecutive episodes. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8562532675265718,
        0.9450115656020787,
        0.9489398834763088
      ],
      "excerpt": "To train the the deep Q network with the provided parameters, just \"run all\" under the Cell drop down menu of the jupyter notebook.  \nThe parameters of the learning agent can be changed in Section 4 of the notebook. The parameters for running the simulation and training the agent can be modified in Section 5. The notebook can then be run again. The parameters are described below. During training, a checkpoint named checkpoint13.pth is saved after it achieves a score of greater than 13 averaged over 100 episodes. After all the training is completed (currently set at 5000 episodes), a checkpoint named checkpoint_final.pth is saved. \nRun the notebook named Navigation_run_saved.ipynb to read in the save checkpoint for the trained agent to watch it play the game without further learning. The name of the notebook can be changed in section 3 of the notebook. It is currently set up to run the agent through 100 episodes end provide scores and the final average score. The final parameter is the number of episodes to run and can also be changed: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.955413554779079
      ],
      "excerpt": "These parameters and the implementation are discussed more in the file Report.md. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8932873952735602,
        0.899974325079821,
        0.9410430100884215,
        0.9754666528057057
      ],
      "excerpt": "    batch_normalize (boolean): Flag for using batch normalization in the neural network \n    error_clipping (boolean): Flag for limiting the TD error to between -1 and 1  \n    reward_clipping (boolean): Flag for limiting the reward to between -1 and 1 \n    gradient_clipping (boolean): Flag for clipping the norm of the gradient to 1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8322650946891409,
        0.9381857757076263,
        0.9166246494820449
      ],
      "excerpt": "        learning steps between updating the neural network for fixed Q targets.  \n    double_dqn (boolean): Flag for using double Q learning \n    dueling_dqn (boolean): Flag for using dueling Q networks \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8966822276720685
      ],
      "excerpt": "    epsilon_initial (float): Initial value of epsilon for epsilon-greedy selection of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8730153461966612
      ],
      "excerpt": "        Higher is faster decay. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8726031645600245
      ],
      "excerpt": "    beta_initial (float): For prioritized replay. Corrects bias induced by weighted \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.891455462061632
      ],
      "excerpt": "        unless  prioritized experience replay is used. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8481044516665399
      ],
      "excerpt": "        targets instead of soft updating. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Project 1 for the Udacity Deep Reinforcement Learning Nanodegree",
      "technique": "GitHub API"
    }
  ],
  "documentation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "http://ipython.readthedocs.io/",
      "technique": "Regular expression"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Use one of the following links:\n\n- Linux: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Linux.zip)\n- Mac OSX: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana.app.zip)\n- Windows (32-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Windows_x86.zip)\n- Windows (64-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Windows_x86_64.zip)\n\nPlace the file in the DRLND GitHub repository, in the `p1_navigation_SNH/` folder, and unzip (or decompress) the file.  Copy the file into the folder p1_navigation_SNH\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/snhwang/p1_navigation_SNH/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 23 Dec 2021 22:02:38 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/snhwang/p1_navigation_SNH/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "snhwang/p1_navigation_SNH",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/snhwang/p1_navigation_SNH/master/Navigation_SNH.ipynb",
      "https://raw.githubusercontent.com/snhwang/p1_navigation_SNH/master/Navigation_SNH_run_saved.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Create an [IPython kernel](http://ipython.readthedocs.io/en/stable/install/kernel_install.html) for the `drlnd` environment:\n\n```\npython -m ipykernel install --user --name drlnd --display-name \"drlnd\"\n```\n\nThese steps only need to be performed once. \n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "The instructions at https://github.com/udacity/deep-reinforcement-learning indicate that you should enter the following on the command line to clone the the repository and install the dependencies:\n\n```\ngit clone https://github.com/udacity/deep-reinforcement-learning.git\ncd deep-reinforcement-learning/python\npip install .\n```\n\nHowever, for Windows 10, this did not work for me. The pip command fails when it tries to install torch 0.4.0. This version may no longer be available. I edited the dependencies shown in the requirements.txt file in the directory and changed the line for torch from\n\n `torch==0.4.0` to `torch==0.4.1`. \n\nThe pip command worked after the change. Otherwise you can install the required packages in the requirements folder manually. Sometimes these software packages change and you may need to refer to the specific instructions for an individual package. For example, https://pytorch.org/get-started/locally/ may be helpful for installing PyTorch. \n\nIf you clone the DRLND repository, the original files from the project can be found in the folder deep-reinforcement-learning\\p1_navigation\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "- **Linux** or **Mac**:\n\n  In a terminal window, perform the following commands:\n\n```\nconda create --name drlnd python=3.6\nsource activate drlnd\n```\n\n- **Windows**:\n\n  Make sure you are using the anaconda command line rather than the usual windows cmd.exe. \n\n```\nconda create --name drlnd python=3.6 \nactivate drlnd\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "The installation of the software is accomplished with the package manager, conda. Installing Anaconda (https://www.anaconda.com/) will include conda as well as facilitate the installation of other data science software packages. The Jupyter Notebook App is also required for running this project and is installed automatically with Anaconda.\n\nThe dependencies for this project can be installed by following the instructions at https://github.com/udacity/deep-reinforcement-learning#dependencies.  Required components include but are are not limited to Python 3.6 (I specifically used 3.6.6), and PyTorch v0.4, and a version of the Unity ML-Agents toolkit. Note that ML-Agents are only supported on Microsoft Windows 10. I only used Windows 10, so cannot vouch for the accuracy of the instructions for other operating systems.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9890348583315558,
        0.869220493838264,
        0.9554169436001461,
        0.9172361032193227,
        0.8712279230539444,
        0.9116099320754565,
        0.8820234121644812
      ],
      "excerpt": "In a terminal window, specifically an Anaconda terminal window for Microsoft Windows, activate the conda environment if not already done: \nLinux or Mac: \nsource activate drlnd \nMake sure you are using the anaconda command line rather than the usual windows cmd.exe.  \nactivate drlnd \nChange directory to the p1_navigate_SNH folder. Run Jupyter Notebook: \njupyter notebook \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8416647351223449
      ],
      "excerpt": "Change directory to the p1_navigate_SNH folder. Run Jupyter Notebook: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8200660649657823
      ],
      "excerpt": "The parameters of the learning agent can be changed in Section 4 of the notebook. The parameters for running the simulation and training the agent can be modified in Section 5. The notebook can then be run again. The parameters are described below. During training, a checkpoint named checkpoint13.pth is saved after it achieves a score of greater than 13 averaged over 100 episodes. After all the training is completed (currently set at 5000 episodes), a checkpoint named checkpoint_final.pth is saved. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.826447801518247,
        0.8620066811537707
      ],
      "excerpt": "load_and_run_agent(agent, env, 'checkpoint_5000_not_prioritized.pth', 100) \nNavigation_SNH.ipynb: Jupyter notebook to train the agent and to save the trained agent as a checkpoint. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8102192919040947
      ],
      "excerpt": "    tau_initial (float): Initial value for tau, the weighting factor for soft updating \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/snhwang/p1_navigation_SNH/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Project 1: Navigation",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "p1_navigation_SNH",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "snhwang",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/snhwang/p1_navigation_SNH/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The instructions at https://github.com/udacity/deep-reinforcement-learning indicate that you should enter the following on the command line to clone the the repository and install the dependencies:\n\n```\ngit clone https://github.com/udacity/deep-reinforcement-learning.git\ncd deep-reinforcement-learning/python\npip install .\n```\n\nHowever, for Windows 10, this did not work for me. The pip command fails when it tries to install torch 0.4.0. This version may no longer be available. I edited the dependencies shown in the requirements.txt file in the directory and changed the line for torch from\n\n `torch==0.4.0` to `torch==0.4.1`. \n\nThe pip command worked after the change. Otherwise you can install the required packages in the requirements folder manually. Sometimes these software packages change and you may need to refer to the specific instructions for an individual package. For example, https://pytorch.org/get-started/locally/ may be helpful for installing PyTorch. \n\nIf you clone the DRLND repository, the original files from the project can be found in the folder deep-reinforcement-learning\\p1_navigation\n\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Create an [IPython kernel](http://ipython.readthedocs.io/en/stable/install/kernel_install.html) for the `drlnd` environment:\n\n```\npython -m ipykernel install --user --name drlnd --display-name \"drlnd\"\n```\n\nThese steps only need to be performed once. \n\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 23 Dec 2021 22:02:38 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The installation of the software is accomplished with the package manager, conda. Installing Anaconda (https://www.anaconda.com/) will include conda as well as facilitate the installation of other data science software packages. The Jupyter Notebook App is also required for running this project and is installed automatically with Anaconda.\n\nThe dependencies for this project can be installed by following the instructions at https://github.com/udacity/deep-reinforcement-learning#dependencies.  Required components include but are are not limited to Python 3.6 (I specifically used 3.6.6), and PyTorch v0.4, and a version of the Unity ML-Agents toolkit. Note that ML-Agents are only supported on Microsoft Windows 10. I only used Windows 10, so cannot vouch for the accuracy of the instructions for other operating systems.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Create an [IPython kernel](http://ipython.readthedocs.io/en/stable/install/kernel_install.html) for the `drlnd` environment:\n\n```\npython -m ipykernel install --user --name drlnd --display-name \"drlnd\"\n```\n\nThese steps only need to be performed once. \n\n\n\n",
      "technique": "Header extraction"
    }
  ]
}