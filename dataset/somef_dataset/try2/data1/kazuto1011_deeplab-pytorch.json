{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1606.00915",
      "https://arxiv.org/abs/1612.03716"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, A. L. Yuille. DeepLab: Semantic Image\nSegmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs. *IEEE TPAMI*,\n2018.<br>\n[Project](http://liangchiehchen.com/projects/DeepLab.html) /\n[Code](https://bitbucket.org/aquariusjay/deeplab-public-ver2) / [arXiv\npaper](https://arxiv.org/abs/1606.00915)\n\n2. H. Caesar, J. Uijlings, V. Ferrari. COCO-Stuff: Thing and Stuff Classes in Context. In *CVPR*, 2018.<br>\n[Project](https://github.com/nightrome/cocostuff) / [arXiv paper](https://arxiv.org/abs/1612.03716)\n\n1. M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, A. Zisserman. The PASCAL Visual Object\nClasses (VOC) Challenge. *IJCV*, 2010.<br>\n[Project](http://host.robots.ox.ac.uk/pascal/VOC) /\n[Paper](http://host.robots.ox.ac.uk/pascal/VOC/pubs/everingham10.pdf)\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kazuto1011/deeplab-pytorch",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-11-01T18:27:51Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-23T17:15:29Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9945300912356602
      ],
      "excerpt": "This is an unofficial PyTorch implementation of DeepLab v2 [1] with a ResNet-101 backbone.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9091901469152961,
        0.848219402735098
      ],
      "excerpt": "* DeepLab v3/v3+ models with the identical backbone are also included (not tested). \n* torch.hub is supported. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8919655262169967
      ],
      "excerpt": "To evaluate the performance on a validation set: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9151412650174399
      ],
      "excerpt": "To re-evaluate with a CRF post-processing:<br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8638847637765537
      ],
      "excerpt": "Execution of a series of the above scripts is equivalent to bash scripts/train_eval.sh. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8136176939062639
      ],
      "excerpt": "Please specify the appropriate configuration files for the other datasets. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9757860827597588,
        0.8405927093880907
      ],
      "excerpt": "Model: DeepLab v2 with ResNet-101 backbone. Dilated rates of ASPP are (6, 12, 18, 24). Output stride is 8. \nGPU: All the GPUs visible to the process are used. Please specify the scope with \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8654350672199908,
        0.9819653229771419,
        0.8513996639891802
      ],
      "excerpt": "Multi-scale loss: Loss is defined as a sum of responses from multi-scale inputs (1x, 0.75x, 0.5x) and element-wise max across the scales. The unlabeled class is ignored in the loss computation. \nGradient accumulation: The mini-batch of 10 samples is not processed at once due to the high occupancy of GPU memories. Instead, gradients of small batches of 5 samples are accumulated for 2 iterations, and weight updating is performed at the end (batch_size * iter_size = 10). GPU memory usage is approx. 11.2 GB with the default setting (tested on the single Titan X). You can reduce it with a small batch_size. \nLearning rate: Stochastic gradient descent (SGD) is used with momentum of 0.9 and initial learning rate of 2.5e-4. Polynomial learning rate decay is employed; the learning rate is multiplied by (1-iter/iter_max)**power at every 10 iterations. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.897731661996665,
        0.9411777006022575
      ],
      "excerpt": "While the official code employs 1/16 bilinear interpolation (Interp layer) for downsampling a label for only 0.5x input, this codebase does for both 0.5x and 0.75x inputs with nearest interpolation (PIL.Image.resize, related issue). \nBilinear interpolation on images and logits is performed with the align_corners=False. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "PyTorch implementation of DeepLab v2 on COCO-Stuff / PASCAL VOC",
      "technique": "GitHub API"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* [COCO-Stuff 10k/164k](data/datasets/cocostuff/README.md)\n* [PASCAL VOC 2012](data/datasets/voc12/README.md)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Caffemodels pre-trained on COCO and PASCAL VOC datasets are released by the DeepLab authors.\nIn accordance with the papers [[1](##references),[2](##references)], this repository uses the COCO-trained parameters as initial weights.\n\n1. Run the follwing script to download the pre-trained caffemodels (1GB+).\n\n```sh\n$ bash scripts/setup_caffemodels.sh\n```\n\n2. Convert the caffemodels to pytorch compatibles. No need to build the Caffe API!\n\n```sh\n#: Generate \"deeplabv1_resnet101-coco.pth\" from \"init.caffemodel\"\n$ python convert.py --dataset coco\n#: Generate \"deeplabv2_resnet101_msc-vocaug.pth\" from \"train2_iter_20000.caffemodel\"\n$ python convert.py --dataset voc12\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kazuto1011/deeplab-pytorch/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 241,
      "date": "Thu, 23 Dec 2021 20:36:45 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/kazuto1011/deeplab-pytorch/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "kazuto1011/deeplab-pytorch",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/kazuto1011/deeplab-pytorch/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/kazuto1011/deeplab-pytorch/master/scripts/setup_voc12.sh",
      "https://raw.githubusercontent.com/kazuto1011/deeplab-pytorch/master/scripts/setup_caffemodels.sh",
      "https://raw.githubusercontent.com/kazuto1011/deeplab-pytorch/master/scripts/train_eval.sh",
      "https://raw.githubusercontent.com/kazuto1011/deeplab-pytorch/master/scripts/setup_cocostuff164k.sh",
      "https://raw.githubusercontent.com/kazuto1011/deeplab-pytorch/master/scripts/setup_cocostuff10k.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.826874516690105
      ],
      "excerpt": "* DeepLab v3/v3+ models with the identical backbone are also included (not tested). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9164579050522337
      ],
      "excerpt": "        <td rowspan=\"2\"><a href=\"https://github.com/kazuto1011/deeplab-pytorch/releases/download/v1.0/deeplabv2_resnet101_msc-cocostuff10k-20000.pth\">Download</a></td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9164579050522337
      ],
      "excerpt": "        <td rowspan=\"2\"><a href=\"https://github.com/kazuto1011/deeplab-pytorch/releases/download/v1.0/deeplabv2_resnet101_msc-cocostuff164k-100000.pth\">Download</a> &Dagger;</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8301255409144193
      ],
      "excerpt": "&Dagger; Note for SPADE followers: The provided COCO-Stuff 164k weight has been kept intact since 2019/02/23. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9164579050522337
      ],
      "excerpt": "        <td rowspan=\"2\"><a href=\"https://github.com/kazuto1011/deeplab-pytorch/releases/download/v1.0/deeplabv2_resnet101_msc-vocaug-20000.pth\">Download</a></td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9406143402308655
      ],
      "excerpt": "To monitor a loss, run the following command in a separate terminal. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8028684719450871
      ],
      "excerpt": "| COCO-Stuff 10k  | configs/cocostuff10k.yaml  | 20,000      | 182 thing/stuff              | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9224681492906003,
        0.9853723880054749
      ],
      "excerpt": "install the extra library below. \npip install torch-encoding \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8015647395177325
      ],
      "excerpt": "        <th>Train set</th> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": "            10k <i>train</i> &dagger; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8702815774991463
      ],
      "excerpt": "        <td><strong>45.5</strong></td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8702815774991463
      ],
      "excerpt": "        <td><strong>45.7</strong></td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": "            164k <i>train</i> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8015647395177325
      ],
      "excerpt": "        <th>Train set</th> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8281109550300468
      ],
      "excerpt": "To train DeepLab v2 on PASCAL VOC 2012: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.956243229936475
      ],
      "excerpt": "python main.py train \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.956999482560778
      ],
      "excerpt": "python main.py test \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8812579898184352
      ],
      "excerpt": "    --model-path data/models/voc12/deeplabv2_resnet101_msc/train_aug/checkpoint_final.pth \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9333384803827206
      ],
      "excerpt": "python main.py crf \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8033768870223226
      ],
      "excerpt": "tensorboard --logdir data/logs \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8194074619888245
      ],
      "excerpt": "| PASCAL VOC 2012 | configs/voc12.yaml         | 20,000      | 20 foreground + 1 background | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8018910732275644
      ],
      "excerpt": "Batch normalization layers in a model are automatically switched in libs/models/resnet.py. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "    from encoding.nn import SyncBatchNorm \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/kazuto1011/deeplab-pytorch/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2017 Kazuto Nakashima\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "DeepLab with PyTorch",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "deeplab-pytorch",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "kazuto1011",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kazuto1011/deeplab-pytorch/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "kazuto1011",
        "body": "",
        "dateCreated": "2020-06-29T08:55:13Z",
        "datePublished": "2020-06-29T08:57:51Z",
        "html_url": "https://github.com/kazuto1011/deeplab-pytorch/releases/tag/v1.0",
        "name": "Pretrained Weights",
        "tag_name": "v1.0",
        "tarball_url": "https://api.github.com/repos/kazuto1011/deeplab-pytorch/tarball/v1.0",
        "url": "https://api.github.com/repos/kazuto1011/deeplab-pytorch/releases/28016276",
        "zipball_url": "https://api.github.com/repos/kazuto1011/deeplab-pytorch/zipball/v1.0"
      }
    ],
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Required Python packages are listed in the Anaconda configuration file `configs/conda_env.yaml`.\nPlease modify the listed `cudatoolkit=10.2` and `python=3.6` as needed and run the following commands.\n\n```sh\n#: Set up with Anaconda\nconda env create -f configs/conda_env.yaml\nconda activate deeplab-pytorch\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 898,
      "date": "Thu, 23 Dec 2021 20:36:45 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "pytorch",
      "deeplab",
      "semantic-segmentation",
      "cocostuff",
      "coco",
      "voc"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You can use [the pre-trained models](#performance), [the converted models](#download-pre-trained-caffemodels), or your models.\n\nTo process a single image:\n\n```bash\npython demo.py single \\\n    --config-path configs/voc12.yaml \\\n    --model-path deeplabv2_resnet101_msc-vocaug-20000.pth \\\n    --image-path image.jpg\n```\n\nTo run on a webcam:\n\n```bash\npython demo.py live \\\n    --config-path configs/voc12.yaml \\\n    --model-path deeplabv2_resnet101_msc-vocaug-20000.pth\n```\n\nTo run a CRF post-processing, add `--crf`. To run on a CPU, add `--cpu`.\n\n",
      "technique": "Header extraction"
    }
  ]
}