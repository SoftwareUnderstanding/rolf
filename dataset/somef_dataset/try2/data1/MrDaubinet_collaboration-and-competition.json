{
  "citation": [
    {
      "confidence": [
        0.8109194328925066
      ],
      "excerpt": "Implement a reinforcement learning algorithm. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "        if i_episode % 100 == 0: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8456806903995955
      ],
      "excerpt": "        if average_score &gt;= 0.5: \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/MrDaubinet/Collaboration-and-Competition",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-05-31T08:45:58Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-06-03T07:21:09Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9888590239055506,
        0.9883188079925541,
        0.9887586092487681,
        0.9874232373116522,
        0.9476680339258932,
        0.8990420647077751
      ],
      "excerpt": "In the field of reinforcement learning, new algorithms are tested in simulated game environments. These allow agents to learn at accelerated speeds, not possible in the real world. While some agents may not have real world aplicability, like the one in this project, others can be initially trained in a simulated environment and continue to learn in a real world environment. The purpose of this project is to train an agent to play tennis by competitivly learning against itself. \nThis project is based off of the Tennis Environment. \nIn this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1. If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01. Thus, the goal of each agent is to keep the ball in play. \nThe observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Each agent receives its own, local observation. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \nThe task is episodic, and in order to solve the environment, your agents must get an average score of +0.5 (over 100 consecutive episodes, after taking the maximum over both agents). Specifically, \nAfter each episode, we add up the rewards that each agent received (without discounting), to get a score for each agent. This yields 2 (potentially different) scores. We then take the maximum of these 2 scores. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9437894947513777
      ],
      "excerpt": "The environment is considered solved, when the average (over 100 episodes) of those scores is at least +0.5. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8754665232329328,
        0.9783876772583103
      ],
      "excerpt": "Ideas for future work. \nTo evaluate the difficulty of the environment. A random walk was scored before any algorithmic implementation of the reinforcement learning agents was made. This was done by randomly selecting actions to interact with the evironment for a set number of steps.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8537671885927693
      ],
      "excerpt": "    actions = np.clip(actions, -1, 1)                       #: all actions between -1 and 1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8766415299897331
      ],
      "excerpt": "    states = next_states                                    #: roll over states to next time step \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9959282507829978,
        0.9418466541225337
      ],
      "excerpt": "Similar to the reasoning pointed out in the previous Continuous Control Project. Due to the nature of the environment being a continuous control problem. The reinforcement learning agorithm needs to be able to work in a continuous space. This hard requirement means we have to use a deep learning approach where neural networks are used for continuous function approximation. When considering between Policy-based vs Value-based Methods. Policy-based methods are better suited for continuous action spaces. I selected the https://arxiv.org/pdf/1706.02275.pdf algorithm, since I had already implemented the ddpg algorithm for the previous project and it could be adapted to the maddpg algorithm with some minor changes. \nI copied the Actor and Critic models, as found here, but I removed batch normalization from the actor model and changed the critics input shape to accept states and actions from both agents. I copied the agent code, found here, then changed it to accomidate a single environment, where both agents share the critic model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9673557859681435
      ],
      "excerpt": "The model was not very consistent in achieving the required score (0.5) and often when it did, it would proceed to decrease in score drastically if left to train more. In one one of the better runs, it was able to achieve the result in around 1000 epochs, in others it could take longer. Worst of all, in allot of approached, the average moving score would gradually increase and then suddenly drop and become terrible \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Udactity - Deep Reinforcement Learning - Project 3",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/MrDaubinet/collaboration-and-competition/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Mon, 27 Dec 2021 19:12:33 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/MrDaubinet/Collaboration-and-Competition/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "MrDaubinet/Collaboration-and-Competition",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/MrDaubinet/collaboration-and-competition/master/Soccer.ipynb",
      "https://raw.githubusercontent.com/MrDaubinet/collaboration-and-competition/master/Tennis.ipynb",
      "https://raw.githubusercontent.com/MrDaubinet/collaboration-and-competition/master/Continuous_Control.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Download the environment from one of the links below. You need only select the environment that matches your operating system:\n  * Linux: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Linux.zip)\n  * Mac OSX: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis.app.zip)\n  * Windows (32-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Windows_x86.zip)\n  * Windows (64-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Windows_x86_64.zip)\n\nImport the Unity environment and create an env object\n```python\nfrom unityagents import UnityEnvironment\nenv = UnityEnvironment(file_name='location of tennis.exe')\n```\n\nInfo about the environment is printed out through the ```Info()``` class found [here](https://github.com/MrDaubinet/collaboration-and-competition/blob/master/info.py)  as seen below:\n```\nUnity Academy name: Academy\nNumber of Brains: 1\nNumber of External Brains : 1\nLesson number : 0\nReset Parameters :\n\nUnity brain name: TennisBrain\nNumber of Visual Observations (per agent): 0\nVector Observation space type: continuous\nVector Observation space size (per agent): 8\nNumber of stacked Vector Observation: 3\nVector Action space type: continuous\nVector Action space size (per agent): 2\nVector Action descriptions: ,\ncreated Info\nNumber of agents: 2\nNumber of actions: 2\n\nStates look like: \n[ 0.          0.          0.          0.          0.          0.\n  0.          0.          0.          0.          0.          0.\n  0.          0.          0.          0.         -6.65278625 -1.5\n -0.          0.          6.83172083  6.         -0.          0.        ]\nStates have length: 24\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.983546516207076
      ],
      "excerpt": "Setup the Environment. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8137769027046937
      ],
      "excerpt": "  env_info = self.env.reset(train_mode=False)[brain_name]   #: reset the environment   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8361047648469796
      ],
      "excerpt": "As you can see below, these guys are pretty retarded. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8172342705654815
      ],
      "excerpt": "  num_agents = len(env_info.agents) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8568247828337854
      ],
      "excerpt": "  scores = np.zeros(num_agents)                             #: initialize the score (for each agent) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8380583209086311
      ],
      "excerpt": "    actions = np.random.randn(num_agents, self.action_size) #: select an action (for each agent) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.920537572092247
      ],
      "excerpt": "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores))) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8382624905131323
      ],
      "excerpt": "        score = np.zeros(self.num_agents) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8452966759815301
      ],
      "excerpt": "        score_max = np.max(score) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8813516794690208
      ],
      "excerpt": "        average_score = np.mean(scores_deque) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8882898251703746
      ],
      "excerpt": "        print('\\rEpisode {}\\tAverage Score: {:.3f}'.format(i_episode, np.mean(scores_deque)), end=\"\") \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.888672232906171
      ],
      "excerpt": "            print('\\rEpisode {}\\tAverage score: {:.3f}'.format(i_episode , average_score)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8812387743637728
      ],
      "excerpt": "            print(\"\\rSolved in episode: {} \\tAverage score: {:.3f}\".format(i_episode , average_score)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8808185061092532,
        0.8755350789916541,
        0.8817680669831361,
        0.8987625515297122
      ],
      "excerpt": "fig = plt.figure() \nax = fig.add_subplot(111) \nplt.plot(np.arange(len(scores)), scores, label='DDPG') \nplt.plot(np.arange(len(scores)), avgs, c='r', label='moving avg') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8310159060815993,
        0.8054663883997478
      ],
      "excerpt": "plt.xlabel('Episode #:') \nplt.legend(loc='upper left'); \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/MrDaubinet/Collaboration-and-Competition/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Project 3: Collaboration and Competition",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Collaboration-and-Competition",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "MrDaubinet",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/MrDaubinet/Collaboration-and-Competition/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Mon, 27 Dec 2021 19:12:33 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Install Anaconda from [here](https://www.anaconda.com/). \n2. Create a new evironment from the environment file in this repository with the command \n    ```\n    conda env create -f environment.yml\n    ```\n3. Run ```python main.py```\n\n    Remove the comments in main to train and run the baseline.\n\n4. Watch the agents vs one another.",
      "technique": "Header extraction"
    }
  ]
}