{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "I would like to thank:\n* [Ngoc Quan Pham](https://scholar.google.com/citations?hl=en&user=AzzJssIAAAAJ)\nfor his advice and support throughout this project. \n* [Parnia Bahar](https://scholar.google.com/citations?user=eyc24McAAAAJ&hl=en)\nfor her thorough response to my email questions about the details of [her paper](https://arxiv.org/abs/1810.03975).\n* [Timo Denk](https://timodenk.com) for our inspiring paper discussions around the topic,\nhis ideas and last but not least his awesome\n[TeX2Img API](https://tools.timodenk.com/tex-math-to-image-conversion) used in this README! \n\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1810.03975",
      "https://arxiv.org/abs/1810.03975",
      "https://arxiv.org/abs/1810.03975",
      "https://arxiv.org/abs/1810.03975",
      "https://arxiv.org/abs/1706.03762",
      "https://arxiv.org/abs/1810.03975",
      "https://arxiv.org/abs/1810.03975\n\n[3] Vaswani et al., 2017, \"Attention Is All You Need\", https://arxiv.org/abs/1706.03762",
      "https://arxiv.org/abs/1706.03762"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[1] Voigtlander et al., 2016, \"Handwriting Recognition with Large Multidimensional Long Short-Term Memory\nRecurrent Neural Networks\", https://www.vision.rwth-aachen.de/media/papers/MDLSTM_final.pdf\n\n[2] Bahar et al., 2018, \"Towards Two-Dimensional Sequence to Sequence Model in Neural Machine Translation\", \nhttps://arxiv.org/abs/1810.03975\n\n[3] Vaswani et al., 2017, \"Attention Is All You Need\", https://arxiv.org/abs/1706.03762\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/FlorianPfisterer/2D-LSTM-Seq2Seq",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-12-12T10:27:41Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-29T11:16:01Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9888455924364734,
        0.9843282631104194
      ],
      "excerpt": "This repository contains a PyTorch implementation of a 2D-LSTM model for sequence-to-sequence learning. \nIn addition, it contains code to apply the 2D-LSTM to neural machine translation (NMT) based on the paper \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8192505441185383,
        0.8454739280633873,
        0.933208251965635
      ],
      "excerpt": "by Parnia Bahar, Christopher Brix and Hermann Ney. \n2D recurrent neural networks are widely used in many applications manipulating 2D objects such as like images. \nFor instance, 2D-LSTMs have become the state-of-the-art in Handwritten Text Recognition \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9920816896745686
      ],
      "excerpt": "The method described in [2] is an approach to apply such 2D-LSTMs to  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8728179595044352
      ],
      "excerpt": "(concatenating both directions) are then used as the inputs in the horizontal dimension of the 2D-LSTM. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9068338610752144,
        0.8604279134157903,
        0.9518473622841243
      ],
      "excerpt": " of the respective previous row  \nare given to the 2D cell. In training mode, teacher forcing is used (i.e. the correct tokens are used). \nThe hidden state of the cell in the last column is then fed into a fully-connected softmax layer which forms \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9245761171096145
      ],
      "excerpt": "The basic idea is that the 2D-LSTM re-reads the input sentence for each new output token, conditioned on the  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9810706751001248,
        0.8586216421666817
      ],
      "excerpt": " concatenated to the (embedded) token \n as well as the hidden and cell states from the  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9190155503546601
      ],
      "excerpt": "See lstm2d_cell.py or the paper for details. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8127370207738734,
        0.9075142831949181
      ],
      "excerpt": "are not known in advance. Thus, only the naive  \nimplementation of going through each row after the other is feasible. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9544155460683174
      ],
      "excerpt": "the 2D-LSTM code contains two different implementations of the forward propagation: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8728622595252752
      ],
      "excerpt": "These tests make sure the input and output dimensions of a single 2D-LSTM cell are as expected and  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9086259336135439,
        0.9905183876681982
      ],
      "excerpt": "and inference forward propagation code by comparing the predictions in both modes to each other when the same  \ntarget tokens are used. This includes the handling of padding for batches that contain sequences of different lengths. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.866272050428004
      ],
      "excerpt": "in favor of a Transformer-like self-attention mechanism [3]. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "PyTorch implementation of a 2D-LSTM Seq2Seq Model for NMT.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/FlorianPfisterer/2D-LSTM-Seq2Seq/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 6,
      "date": "Thu, 23 Dec 2021 08:52:19 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/FlorianPfisterer/2D-LSTM-Seq2Seq/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "FlorianPfisterer/2D-LSTM-Seq2Seq",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.9362210455169797,
        0.8837680365796365
      ],
      "excerpt": "test-requirements.txt, you can run all of them using  \npython -m unittest \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9210577262080067
      ],
      "excerpt": "* The tests in test_lstm2d_training.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9266726782845583
      ],
      "excerpt": "* The tests in test_lstm2d_train_vs_inference.py validate the training \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/FlorianPfisterer/2D-LSTM-Seq2Seq/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'Copyright (c) 2019 Florian Pfisterer &#102;&#108;&#111;&#114;&#105;&#97;&#110;&#46;&#112;&#102;&#105;&#115;&#116;&#101;&#114;&#101;&#114;&#49;&#64;&#103;&#109;&#97;&#105;&#108;&#46;&#99;&#111;&#109;\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in\\nall copies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\\nTHE SOFTWARE.'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "2D-LSTM Seq2Seq Model",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "2D-LSTM-Seq2Seq",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "FlorianPfisterer",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/FlorianPfisterer/2D-LSTM-Seq2Seq/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Clone the project and make sure to install the dependencies listed in [`requirements.txt`](./requirements.txt).\n\nIf you use the included dataset helper functions for the small IWSLT14 deu-eng NMT dataset (taken from\n[harvardnlp/var-attn/data](https://github.com/harvardnlp/var-attn/tree/master/data)), it will automatically \npreprocess the data into `.csv` files before the first run.\n\nI've successfully run all tests using:\n* [PyTorch](http://pytorch.org) `1.0.1.post2`\n* [torchtext](https://github.com/pytorch/text) `0.3.1` \n* [NumPy](http://www.numpy.org) `1.16.2`\n* [pandas](https://pandas.pydata.org) `0.24.1`\n* [tensorboardX](https://github.com/lanpa/tensorboardX) `1.6`\n\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "With the dependencies installed, you can run the scripts in the [`main/`](./main) folder. \nTo run the IWSLT14 training script for example, just run\n```\npython -m main.train_iwslt14_small\n```\nThe available command line arguments for this script can be found below.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "The [train_iwslt14_small.py](./main/train_iwslt14_small.py) script contains code to train a 2D-LSTM model on \nthe small IWSLT14 deu-eng NMT dataset\n(taken from [harvardnlp/var-attn/data](https://github.com/harvardnlp/var-attn/tree/master/data)).\n\nThe following command line arguments are supported, with the given default values:\n* `--batch_size=32`: The batch size to use for training and inference.\n* `--epochs=20`: The number of epochs to train.\n* `--shuffle=True`: Whether or not to shuffle the training examples.\n* `--lr=0.0005`: The learning rate to use.\n* `--embed_dim=128`: The dimension of the embedding vectors for both the source and target language.\n* `--encoder_state_dim=64`: The dimension of the bidirectional encoder LSTM states.\n* `--state_2d_dim=128`: The dimension of the 2D-LSTM hidden & cell states.\n* `--disable_cuda=False`: Disable CUDA (i.e. use the CPU for all computations).\n* `--dropout_p=0.2`: The dropout probability, used after the embeddings and before the final softmax layer.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 25,
      "date": "Thu, 23 Dec 2021 08:52:19 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "pytorch",
      "2d-lstm",
      "lstm",
      "nmt-model",
      "nlp",
      "seq2seq"
    ],
    "technique": "GitHub API"
  }
}