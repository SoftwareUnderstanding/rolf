{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1903.12261"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find the code, models, or data useful, please cite this paper:\n\n```\n@article{yeo2021robustness,\n  title={Robustness via Cross-Domain Ensembles},\n  author={Yeo, Teresa and Kar, O\\u{g}uzhan Fatih and Zamir, Amir},\n  journal={ICCV},\n  year={2021}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{yeo2021robustness,\n  title={Robustness via Cross-Domain Ensembles},\n  author={Yeo, Teresa and Kar, O\\u{g}uzhan Fatih and Zamir, Amir},\n  journal={ICCV},\n  year={2021}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/EPFL-VILAB/XDEnsembles",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-01-19T12:24:49Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-27T15:03:40Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "**Robustness problem in neural networks:** Neural networks deployed in the real-world will encounter data with naturally occurring distortions, e.g. motion blur, brightness changes, etc. Such changes make up shifts from the training data distribution. While neural networks are able to learn complex functions in-distribution, their predictions are deemed unreliable under such shifts, i.e. they are not robust. This presents a core challenge that needs to be solved for these models to be useful in the real-world.\n\n**Why do we need robust predictions?** Suppose we want to learn a mapping from an input domain, e.g. RGB images, to a target domain, e.g. surface normals (see above figure). A common approach is to learn this mapping with a `direct` path, i.e. `RGB \u2192 surface normals`. Since this path directly operates on the input domain, it is prone to being affected by any slight alterations in the RGB image, e.g. brightness changes. \n\n**How do we obtain robust predictions?** An alternative can be to go through a **middle domain** that is invariant to that change. For example, the surface normals predicted via the `RGB \u2192 2D edges \u2192 surface normals` path will be resilient to brightness distortions in the input as the 2D edges domain abstracts that away. However, the distortions that a model may encounter are broad and unknown ahead of time, and some middle domains can be too lossy for certain downstream predictions. These issues can be mitigated by employing an **ensemble** of predictions made via a **diverse set of middle domains** and merging their (relatively weaker) predictions into one (stronger) output on-the-fly.\n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9855974708305714
      ],
      "excerpt": "This repository contains tools for training and evaluating: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9915926262568138
      ],
      "excerpt": "      <tr><td><em>Above: An overview of the proposed method for creating a robust and diverse ensemble of predictions. A set of networks predict a target domain (surface normals) given an input image that has undergone an unknown distribution shift (JPEG compression degradation), via K middle domains (e.g. 2D texture edges, low-pass filtering, greyscale image, emboss filtering, etc). The prediction by each of the K paths are then merged into one final strong prediction using weights that are based on the uncertainty associated with each prediction. This method is shown to be significantly robust against adversarial and non-adversarial distribution shifts. In the figure above, solid and dashed arrows represent learned and analytical functions, respectively.</em></td></tr> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9968029537584643
      ],
      "excerpt": "Table of Contents \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8293569137449673
      ],
      "excerpt": "For the paper, we programmatically extracted the following middle domains from the RGB input (no learning, implementations are provided here): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9239082859727699
      ],
      "excerpt": "A visualization of these middle domains for an RGB image can be seen below.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8879565917095231
      ],
      "excerpt": "A visualization of target domains for an RGB image can be seen below. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8883464970069879
      ],
      "excerpt": "You can also apply distortion to your input from Common Corruptions to see how well the models handle the distribution shift. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8060744995067841
      ],
      "excerpt": "The argument --distortion can be set to one of the followings:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.904338167666328
      ],
      "excerpt": "The argument --severity can be set from 1 to 5 to change the severity of the applied distortion. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.983703690097919
      ],
      "excerpt": "All networks are based on the UNet architecture. They take in an input size of 256x256, upsampling is done via bilinear interpolations instead of deconvolutions. All models were trained with an NLL loss. Architectural hyperparameters for the models are detailed in transfers.py. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8274909547102883
      ],
      "excerpt": "distortions.py      #: Implemementations of common corruptions \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9582476195960963,
        0.8979411005071259
      ],
      "excerpt": "        ood_standard_set/   #: OOD data for visualization (OOD_DIR) \n    data_dir/               #: taskonomy data (DATA_DIRS) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9776287632874007
      ],
      "excerpt": "This trains the model with negative log likelihood (NLL) loss. In addition to the prediction, the uncertainty is also predicted. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9228853015027889
      ],
      "excerpt": "The above training step returns predicted uncertainties that are overconfident when given out of distribution data. In this step, train the uncertainties to generalize while holding the predictions fixed. The command is given by  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9411385298935643
      ],
      "excerpt": "again, for a emboss to normal network, the command is \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9362685893948133
      ],
      "excerpt": "This trains the target model with 4 perceptual losses: reshading, curvature, depth, imagenet. Note that for the paper, for the depth target models, we use 7 perceptual losses, similar to cross-task consistency. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9069161966404874
      ],
      "excerpt": "After steps 1-3 have been done for each path, we propose to merge the predictions either by the inverse of their predicted variance or training a network to do the merging. Both options gives similar performance, thus the latter is optional. The command to train a network that takes in as input all predictions and output a single one for e.g. reshading, is given by, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9005681824007048
      ],
      "excerpt": "The expected folder structure for the data is, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9602178531027012
      ],
      "excerpt": "Pytorch's dataloader __getitem__ method has been overwritten to return a tuple of all tasks for a given building and view point. This is done in datasets.py. Thus, for other folder structures, a function to get the corresponding file paths for different domains should be defined. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8242838896683323
      ],
      "excerpt": "For example, to train a CIFAR-100 classifier with emboss as a middle domain (i.e. RGB\u2192emboss\u2192classes) and ResNet18 model, the command is \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Robustness via Cross-Domain Ensembles",
      "technique": "GitHub API"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you haven't yet, then download the [pretrained models](#pretrained-models). Models used for the demo can be downloaded with the following command:\n```bash\nsh ./tools/download_models.sh\n```\n\nThis downloads the `single UNet baseline`, `deep ensembles`, and `cross-domain ensembles` models for `normal`, `reshading` and `depth_zbuffer` targets (14GB) to a folder called `./models/`. Individual models can be downloaded [here](https://drive.switch.ch/index.php/s/6dFgYwR8dGj07jF).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "The following command downloads the final pretrained models.\n```bash\nsh ./tools/download_models.sh\n```\n\nThis downloads the `single UNet baseline`, `deep ensembles`, and `cross-domain ensembles` models for `normal`, `reshading` and `depth_zbuffer` targets (14GB) to a folder called `./models/`. Individual models can be downloaded [here](https://drive.switch.ch/index.php/s/6dFgYwR8dGj07jF).\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Our method includes three training stages: baseline training, consistency training (optional), and sigma training. The pretrained models for the first two stages can be downloaded [here](https://drive.switch.ch/index.php/s/WVH91pZfgG2VnhB) (9.3GB). These models were used for the ablation studies in the paper. \n\n```bash\nsh ./tools/download_baselines.sh\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "The pretrained perceptual models for the consistency training are the same as those used in [Robust Learning Through Cross-Task Consistency](https://github.com/EPFL-VILAB/XTConsistency). They can be downloaded with the following command.\n\n```bash\nsh ./tools/download_percep_models.sh\n```\n\nThis downloads the perceptual models for the `normal`, `reshading` and `depth_zbuffer` targets (1.6GB). Each target has 7 pretrained models (from the other sources below).\n\n```\nCurvature         Edge-3D            Reshading\nDepth-ZBuffer     Keypoint-2D        RGB       \nEdge-2D           Keypoint-3D        Surface-Normal \n```\n\nPerceptual model architectural hyperparameters are detailed in [transfers.py](./transfers.py), and some of the pretrained models were trained using L2 loss. For using these models with the provided training code, the pretrained models should be placed in the file path defined by `MODELS_DIR` in [utils.py](./utils.py#L25).\n\nIndividual perceptual models can be downloaded [here](https://drive.switch.ch/index.php/s/aXu4EFaznqtNzsE).\n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "We also provide the models for other baselines used in the paper. The pretrained baselines can be downloaded [here](https://drive.switch.ch/index.php/s/ep2j3s8nC7QoqWV) and the architectural details can be reached from [transfers.py](./transfers.py). Note that we will not be providing support for them. \n- A full list of baselines is in the table below:\n   |                     Baseline Method                     |                                                       Description                                                              |   \n   |---------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------|\n   | Baseline UNet [[PDF](https://arxiv.org/pdf/1505.04597.pdf)]   | UNets trained on the Taskonomy dataset.                                                                                  | \n   | Baseline UNet + Adversarial training [[PDF](https://arxiv.org/pdf/1706.06083.pdf)]   | Baseline UNets (above) were finetuned with adversarial examples using I-FGSM (details can be reached from the paper).                                                                                  | \n   | Multi-Task [[PDF](http://arxiv.org/pdf/1609.02132.pdf)] | A multi-task model we trained using UNets, using a shared encoder (similar to [here](http://arxiv.org/pdf/1609.02132.pdf)) to predict normal, reshading, and depth from the RGB input.     |\n   | Multi-Domain |  Instead of a single input domain, we use all the [middle domains](#middle-and-target-domains) and RGB as inputs, and output the prediction for a single task.  |\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/EPFL-VILAB/XDEnsembles/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 5,
      "date": "Wed, 29 Dec 2021 14:41:00 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/EPFL-VILAB/XDEnsembles/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "EPFL-VILAB/XDEnsembles",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/EPFL-VILAB/XDEnsembles/master/tools/download_percep_models.sh",
      "https://raw.githubusercontent.com/EPFL-VILAB/XDEnsembles/master/tools/download_models.sh",
      "https://raw.githubusercontent.com/EPFL-VILAB/XDEnsembles/master/tools/download_otherbaselines.sh",
      "https://raw.githubusercontent.com/EPFL-VILAB/XDEnsembles/master/tools/download_baselines.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The code can also be run using a Python environment manager such as Conda. See [requirements.txt](./requirements.txt) for complete list of packages. We recommend doing a clean installation of requirements using virtualenv:\n1.  Clone the repo:\n```bash\ngit clone https://github.com/EPFL-VILAB/XDEnsembles.git\ncd XDEnsembles\n```\n\n2. Create a new environment and install the libraries:\n```bash\nconda create -n testenv -y python=3.6\nsource activate testenv\npip install -r tools/requirements.txt\napt-get update && apt-get -y install libmagickwand-dev && apt-get -y install libgl1-mesa-glx\ngit clone https://github.com/fbcotter/pytorch_wavelets && cd pytorch_wavelets && pip install .\n```\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "We provide a docker that contains the code and all the necessary libraries. It's simple to install and run.\n1. Simply run:\n\n```bash\ndocker run --runtime=nvidia -ti --rm ofkar/xdensembles:latest\n```\nThe code is now available in the docker under your home directory (`/XDEnsembles`), and all the necessary libraries should already be installed in the docker.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "There are two convenient ways to run the code. Either using Docker (recommended) or using a Python-specific tool such as pip, conda, or virtualenv.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8206206603909694
      ],
      "excerpt": "Download all pretrained models \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8276813573418753
      ],
      "excerpt": "Assuming that you want to train on the full dataset or on your own dataset, read on. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8416850107156709,
        0.8028136210837094
      ],
      "excerpt": "    jobinfo.txt         #: Defines job name, base_dir \nmodules/            #: Network definitions \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.86417087769169
      ],
      "excerpt": "Accuracy on the test data can also be evaluated using the following command \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8245539886860519
      ],
      "excerpt": "Pretrained models \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8661284161560743,
        0.8737873112808705
      ],
      "excerpt": "Download all pretrained models \nTrain a model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8826155258294552
      ],
      "excerpt": "python demo.py --task $TASK --img_path $PATH_TO_IMAGE_OR_FOLDER --output_path $PATH_TO_SAVE_OUTPUT --distortion $DISTORTION_NAME --severity $SEVERITY_LEVEL \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9466601337613157
      ],
      "excerpt": "python demo.py --task normal --img_path assets/test.png --output_path assets/ --distortion 'pixelate' --severity 2 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8314176816942925
      ],
      "excerpt": "Note that there are 3 stages of training: 1. baseline training, 2. consistency training (optional), and 3. sigma training. The pretrained models for the first two stages are provided here and the final models here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.827774293983635
      ],
      "excerpt": "    split.txt           #: Train, val split \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8395176900917038,
        0.897271397005078,
        0.8757732209701289,
        0.897271397005078,
        0.9152750849795331,
        0.8416607718463419,
        0.859261433173912,
        0.8612059329335424,
        0.859261433173912
      ],
      "excerpt": "train_baseline.py   #: Script for baseline training  \ntrain_cons.py       #: Script for consistency training \ntrain_sig.py        #: Script for sigma training \ntrain_merging.py    #: Script for merging training \ndataset.py          #: Creates dataloader \nenergy_baseline.py  #: Defines path config, computes total loss, plots for baseline training \nenergy_cons.py      #: Defines path config, computes total loss, logging for consistency training \nenergy_sig.py       #: Defines path config, computes total loss, logging for sigma training \nenergy_merging.py       #: Defines path config, computes total loss, logging for merging training \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8692162072079107
      ],
      "excerpt": "transfers.py        #: Loads models \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8963521101614512
      ],
      "excerpt": "demo.py             #: Demo script \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8153028628306336
      ],
      "excerpt": "base_dir/                   #: The following paths are defined in utils.py (BASE_DIR) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8136969277951048
      ],
      "excerpt": "        models/             #: Pretrained models (MODELS_DIR) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python train_baseline.py baseline_{input_domain}2{output_domain} \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python train_baseline.py baseline_emboss2normal \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8813532070210278
      ],
      "excerpt": "Add its definition in utils.py. The expected input and output is of size 1xCxHxW. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python train_sig.py trainsig_{input_domain}{output_domain} \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python train_sig.py trainsig_embossnormal \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python train_cons.py consistency_{input_domain}{output_domain} \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python train_cons.py consistency_embossnormal \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python train_merging.py merge_reshading \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8649806327946665,
        0.811482855069483
      ],
      "excerpt": "python train.py -domain $DOMAIN_NAME -net $ARCH_NAME \nFor example, to train a CIFAR-100 classifier with emboss as a middle domain (i.e. RGB\u2192emboss\u2192classes) and ResNet18 model, the command is \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8649806327946665
      ],
      "excerpt": "python train.py -domain emboss -net resnet18 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/EPFL-VILAB/XDEnsembles/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Robustness via Cross-Domain Ensembles [ICCV 2021, Oral]\n\nThis repository contains tools for training and evaluating:\n\n- [Pretrained models](#pretrained-models)\n- [Demo code](#quickstart)\n- [Training scripts](#training)\n- [Docker and installation instructions](#installation)\n\nfor the paper: [**Robustness via Cross-Domain Ensembles**](https://crossdomain-ensembles.epfl.ch).",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "XDEnsembles",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "EPFL-VILAB",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/EPFL-VILAB/XDEnsembles/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To run the trained model of a task on a specific image:\n\n```bash\npython demo.py --task $TASK --img_path $PATH_TO_IMAGE_OR_FOLDER --output_path $PATH_TO_SAVE_OUTPUT\n```\n\nThe `--task` flag specifies the target task for the input image, which should be either `normal`, `reshading` or `depth_zbuffer`.\n\nTo run the script for a `normal` target on the [example image](./assets/test.png):\n\n```bash\npython demo.py --task normal --img_path assets/test.png --output_path assets/\n```\n\nIt returns the output prediction (`test_normal_ours_mean.png`) and the associated uncertainty (`test_normal_ours_sig.png`) for the proposed method as well as the single UNet baseline and deep ensembles.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 21,
      "date": "Wed, 29 Dec 2021 14:41:00 GMT"
    },
    "technique": "GitHub API"
  }
}