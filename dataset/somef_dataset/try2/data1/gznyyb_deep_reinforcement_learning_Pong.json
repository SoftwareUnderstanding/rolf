{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py \n* https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/tree/master/Chapter06/lib\n* https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter06/02_dqn_pong.py\n* https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf\n* https://github.com/ageron/handson-ml/blob/master/16_reinforcement_learning.ipynb\n* https://arxiv.org/pdf/1509.06461.pdf\n\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/gznyyb/deep_reinforcement_learning_Pong",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-02-16T21:54:35Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-07-24T23:04:57Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "In the Pong environment, the agent has three related elements: action, reward and state.\n\nActions: agent takes the action at time t; there are six actions including going up, down, staying put, fire the ball, etc.\nRewards: agent/environment receives/produces reward, when the opponent fails to hit the ball back towards the agent or the agent get 21 points and win.\nState: environment updates state St, which is defined by four game frames\u2019 interfaces stacking together - the Pong involves motion of two paddles and one ball, and background features that the agent need to learn at the game.\nThe network, suggested by https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf, is used to approximate the action values, which consists of three convolutional neural networks followed by two dense layers. In addition to a network used for training, the other network, which is architecture identical with the first one, gets its weights by copying them from the train network periodically during training and is used to compute the action value label. The other network (called the target network by the paper) is set up to avoid instability in training.\n\nThe model is trained using the following three frameworks. \n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9564761056765143
      ],
      "excerpt": "This project is intended to build up an intelligent agent able to play and win Pong game (https://gym.openai.com/envs/Pong-v0/). This agent was trained under the methods of Neutral Network and Deep Learning. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8290155887746604,
        0.9371297130353483
      ],
      "excerpt": "Then a simple deep-Q network model has been established as a baseline model. This time a target network, whose parameter values are periodically copied from the train network, is utilized to compute the labels. \nLastly a double q network model has been tried to compare its performance with that of the baseline model. This time a best action is chosen by using the train network to compute the action values of the next state and finding the maximum action value. Then the target network is used to compute the action value of this \u201cbest action,\u201d which is used as the label. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/gznyyb/deep_reinforcement_learning_Pong/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Sun, 26 Dec 2021 17:25:06 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/gznyyb/deep_reinforcement_learning_Pong/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "gznyyb/deep_reinforcement_learning_Pong",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/gznyyb/deep_reinforcement_learning_Pong/master/code/reinforcement-learning-code.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.926772778348527
      ],
      "excerpt": "Matplotlib (https://matplotlib.org/) - The python package used to visualize the final result  \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/gznyyb/deep_reinforcement_learning_Pong/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Deep reinforcement learning with pixel features in Atari Pong Game",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "deep_reinforcement_learning_Pong",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "gznyyb",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/gznyyb/deep_reinforcement_learning_Pong/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Sun, 26 Dec 2021 17:25:06 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The whole project is done in Google's colaboratory environment (https://colab.research.google.com/). \n\n* run the \"Set Up Google Cloud GPU\" section first to set up the GPU for faster computation\n* run the sequent chunks of code to start training \n\n",
      "technique": "Header extraction"
    }
  ]
}