{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1607.04606",
      "https://arxiv.org/abs/1904.13264",
      "https://arxiv.org/abs/1607.04606",
      "https://arxiv.org/abs/1904.13264",
      "https://arxiv.org/abs/1607.04606.](https://arxiv.org/abs/1607.04606)\n- [Adriaan M. J. Schakel and Benjamin J Wilson.  Measuring Word Significance using Distributed Representations of Words. aug 2015. http://arxiv.org/abs/1508.02297](http://arxiv.org/abs/1508.02297).\n- [Vitalii  Zhelezniak,  Aleksandar  Savkov,  April  Shen,Francesco  Moramarco,   Jack  Flann,   and  Nils  Y.Hammerla. 2019.   Don\u2019t settle for average,  go for the max:  Fuzzy sets and max-pooled word vectors. In International Conference on Learning Representations.](https://arxiv.org/abs/1904.13264"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- [Piotr  Bojanowski,   Edouard  Grave,   Armand  Joulin,and  Tomas  Mikolov.  2016.    Enriching  word  vec-tors  with  subword  information. arXiv preprint arXiv:1607.04606.](https://arxiv.org/abs/1607.04606)\n- [Adriaan M. J. Schakel and Benjamin J Wilson.  Measuring Word Significance using Distributed Representations of Words. aug 2015. http://arxiv.org/abs/1508.02297](http://arxiv.org/abs/1508.02297).\n- [Vitalii  Zhelezniak,  Aleksandar  Savkov,  April  Shen,Francesco  Moramarco,   Jack  Flann,   and  Nils  Y.Hammerla. 2019.   Don\u2019t settle for average,  go for the max:  Fuzzy sets and max-pooled word vectors. In International Conference on Learning Representations.](https://arxiv.org/abs/1904.13264)",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9702105236373249,
        0.8906174419333412
      ],
      "excerpt": "    <tr> <th>0</th> <td>month</td> <td>January</td> <td>-22.559890</td> <td>12.179197</td> <td>29.066855</td> <td>76.994548</td> </tr> \n    <tr> <th>1</th> <td>month</td> <td>February</td> <td>-34.532857</td> <td>13.354693</td> <td>30.934289</td> <td>57.404248</td> </tr> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9593299683604384,
        0.8109194328925066,
        0.8444342525991423,
        0.9278824608274014,
        0.8955886365383559,
        0.9278824608274014,
        0.9840846195802034,
        0.9934273497841208,
        0.987383198710525
      ],
      "excerpt": "    <tr> <th>3</th> <td>month</td> <td>April</td> <td>25.993046</td> <td>10.371281</td> <td>25.993049</td> <td>86.943093</td> </tr> \n    <tr> <th>4</th> <td>month</td> <td>May</td> <td>247.451639</td> <td>6.607942</td> <td>15.817219</td> <td>219.577850</td> </tr> \n    <tr> <th>5</th> <td>month</td> <td>June</td> <td>86.636376</td> <td>9.665938</td> <td>24.424255</td> <td>80.363607</td> </tr> \n    <tr> <th>6</th> <td>month</td> <td>July</td> <td>93.219042</td> <td>12.777551</td> <td>28.812698</td> <td>71.600872</td> </tr> \n    <tr> <th>7</th> <td>month</td> <td>August</td> <td>-4.813989</td> <td>11.601140</td> <td>26.914686</td> <td>56.870358</td> </tr> \n    <tr> <th>8</th> <td>month</td> <td>September</td> <td>-44.985682</td> <td>12.394985</td> <td>28.366747</td> <td>61.352990</td> </tr> \n    <tr> <th>9</th> <td>month</td> <td>October</td> <td>-21.949212</td> <td>12.073578</td> <td>30.084649</td> <td>64.158556</td> </tr> \n    <tr> <th>10</th> <td>month</td> <td>November</td> <td>-35.144106</td> <td>13.222669</td> <td>29.711792</td> <td>55.423498</td> </tr> \n    <tr> <th>11</th> <td>month</td> <td>December</td> <td>-34.639645</td> <td>12.905714</td> <td>30.720711</td> <td>59.169547</td> </tr> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "    <tr> <th>18</th> <td>color</td> <td>black</td> <td>-5.428290</td> <td>-3.726623</td> <td>-5.428305</td> <td>26.119314</td> </tr> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "    <tr> <th>22</th> <td>color</td> <td>violet</td> <td>-28.965518</td> <td>10.081180</td> <td>-5.287368</td> <td>-98.384650</td> </tr> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488,
        0.8550101043698384
      ],
      "excerpt": "    <tr> <th>30</th> <td>animal</td> <td>insect</td> <td>-7.264797</td> <td>7.887063</td> <td>-7.264797</td> <td>-90.300180</td> </tr> \n    <tr> <th>31</th> <td>animal</td> <td>fly</td> <td>259.201598</td> <td>-6.115292</td> <td>-10.199600</td> <td>-24.141623</td> </tr> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8763134177696574,
        0.9421445313892791
      ],
      "excerpt": "    <tr> <th>34</th> <td>tool</td> <td>screwdriver</td> <td>-75.439298</td> <td>33.551341</td> <td>10.523150</td> <td>-97.639422</td> </tr> \n    <tr> <th>35</th> <td>tool</td> <td>drill</td> <td>-43.531817</td> <td>11.923173</td> <td>-15.297724</td> <td>-85.132555</td> </tr> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9586556701111782
      ],
      "excerpt": "    <tr> <th>37</th> <td>tool</td> <td>knife</td> <td>-37.435886</td> <td>20.666681</td> <td>-6.153829</td> <td>-75.100349</td> </tr> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "    <tr> <th>47</th> <td>fruit</td> <td>pomegranate</td> <td>-63.004678</td> <td>3.623020</td> <td>10.985970</td> <td>-97.139377</td> </tr> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488,
        0.9848179726092176
      ],
      "excerpt": "    <tr> <th>55</th> <td>flower</td> <td>marigold</td> <td>-25.294849</td> <td>9.122744</td> <td>12.057728</td> <td>-99.084527</td> </tr> \n    <tr> <th>56</th> <td>flower</td> <td>orchid</td> <td>10.703952</td> <td>7.983661</td> <td>10.703952</td> <td>-93.583253</td> </tr> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "    <tr> <th>59</th> <td>tree</td> <td>maple</td> <td>-12.181924</td> <td>19.677509</td> <td>31.727102</td> <td>-90.921096</td> </tr> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "    <tr> <th>62</th> <td>tree</td> <td>spruce</td> <td>-32.479379</td> <td>5.101566</td> <td>35.041240</td> <td>-97.131057</td> </tr> \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/vackosar/fasttext-vector-norms-and-oov-words",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-06-29T13:35:34Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-06-27T14:13:50Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "FastText embeds words by adding word's n-grams to the word embedding and then normalizes by total token count i.e. <b>fastText(word)<sub></sub> = (v<sub>word</sub> + &Sigma;<sub>g &isin; ngrams(word)</sub>v<sub>g</sub>) / (1 + |ngrams(word)|)</b>. However if the word is not present in the dictionary (OOV) only n-grams are used i.e. <b>(&Sigma;<sub>g &isin; ngrams(word)</sub>v<sub>g</sub>) / |ngrams(word)|</b>. For purpose of studying OOV words this asymmetry between vocabulary and out of vocabulary words is removed by only utilizing word's n-grams regardless if the word is OOV or not.\n\nIn order to study contrast between common english words e.g. \"apple\" and noise-words (usually some parsing artifacts or unusual tokens with very specific meaning) e.g. \"wales-2708\" or \"G705\" [MIT 10K Common words dataset is used](https://www.mit.edu/~ecprice/wordlist.10000).\n\nEntire code for this post in available in [this repository in file \"main.py\"](https://github.com/vackosar/fasttext-vector-norms-and-oov-words/blob/master/main.py). FastText model used is [5-gram English 2M \"cc.en.300.bin\"](https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz).\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9791567197447173,
        0.9748680496341656,
        0.9837168430280938,
        0.9138663496585593
      ],
      "excerpt": "Word embeddings, trained on large unlabeled corpora are useful for many natural language processing tasks. FastText (Bojanowski et al., 2016) in contrast to Word2vec model accounts for sub-word information by also embedding sub-word n-grams. FastText word representation is the word embedding vector plus sum of n-grams contained in it. \nWord2vec vector norms have been shown (Schakel & Wilson, 2015) to be correlated to word significance. This blog post visualize vector norms of FastText embedding and evaluates use of FastText word vector norm multiplied with number of word n-grams for detecting non-english OOV words. \nStandard vector norm as defined in Gensim implementation is used in this section. Common words are located mostly on the right in the term-frequency spectrum and clustered in three different areas in the norm spectrum. On both axis common words are clustered approximatelly in 4 areas. \nFrom below samples it is not clear what clusters correspond to: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9511107698438289,
        0.9798005835429359,
        0.8971076253839219,
        0.8901667166381566,
        0.9949583486055346
      ],
      "excerpt": "As mentioned above each FastText vocab word has its vector representation regardless its size. Norms of those vectors are plotted in this section. The shape of the distribution seems to match closely the shape of the same plot for Word2Vec (Schakel & Wilson, 2015). The vector norm as measure of word significance seems to hold even for FastText in terms of this norm as can be seen from labeled samples in the scatter plot (same frequency bin with increasing vector norm: authors, Alfine, numbertel). \nAs mentioned above FastText uses average of word vectors used. However for detection of noise-words number of ngrams seems to useful. For that purpose NG_Norm is defined <b>ng_norm(word)= || &Sigma;<sub>g &isin; ngrams(word)</sub>v<sub>g</sub> ||</b>. Using this norm common words are clustered in narrower band on ng_norm axis. \nExplicitly aggregated distribution on ng_norm axis is plotted in histogram below. \nProbability distribution of given FastText vocabulary word being common word is plotted below. The distribution is well approximated by t-distribution. \nTo evaluate thesis of (Shakel 2015) that word specificity in given term-frequency norm is correlated with vector norm for FastText 67 pairs of hyponyms and hypernyms are used. From just these few examples we see that No-NGram norm with 77% accuracy predicts which word is hyponym and which hypernym disregarding their term-frequencies. Below is the data used. The norm colums contain relative percent differences i.e. (hypo-hyper) / hyper * 100. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9182263868689131
      ],
      "excerpt": "Ability to detect noisy-words is evaluated on simple task of splitting two concatenated words back apart below. For example let's split back concatenation 'inflationlithium': \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.969591113909074,
        0.9033652454037365
      ],
      "excerpt": "Above approach yielded around 48% accuracy on 3000 random two-word samples from MIT 10k common words. A more efficient method in this specific case would be to search vocabulary instead of calculating vector norms. More appropriate comparison however would be for more general task involving OOV words e.g. using Edit Distance performed also on OOV words and words with typos. \nFastText vector norms and their term-frequency were visualized and investigated in this post. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8931964066943698
      ],
      "excerpt": "No-N-Gram Norm has very similar Norm-TF distribution as Word2Vec shown in (Schakel & Wilson, 2015). The word significance correlation does seem to hold even for FastText embeddings in terms of No-N-Gram Norm. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "This blog post visualize vector norms of FastText embedding and evaluates use of FastText word vector norm multiplied with number of word n-grams for detecting non-english OOV words.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/vackosar/fasttext-vector-norms-and-oov-words/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 23 Dec 2021 08:43:33 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/vackosar/fasttext-vector-norms-and-oov-words/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "vackosar/fasttext-vector-norms-and-oov-words",
    "technique": "GitHub API"
  },
  "invocation": [
    {
      "confidence": [
        0.8443882495421419,
        0.8196523740495366,
        0.8093096911817319
      ],
      "excerpt": "    <tr> <th>15</th> <td>color</td> <td>white</td> <td>-3.950346</td> <td>-4.100787</td> <td>-3.950355</td> <td>24.457167</td> </tr> \n    <tr> <th>16</th> <td>color</td> <td>orange</td> <td>-19.920026</td> <td>1.365383</td> <td>6.773289</td> <td>-80.102688</td> </tr> \n    <tr> <th>17</th> <td>color</td> <td>purple</td> <td>-25.538990</td> <td>-3.007207</td> <td>-0.718664</td> <td>-87.577665</td> </tr> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8353551019427649
      ],
      "excerpt": "    <tr> <th>20</th> <td>color</td> <td>yellow</td> <td>-25.193438</td> <td>1.552819</td> <td>-0.257928</td> <td>-71.494422</td> </tr> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8221367214016831
      ],
      "excerpt": "    <tr> <th>23</th> <td>color</td> <td>grey</td> <td>39.793408</td> <td>-0.529619</td> <td>-6.804404</td> <td>-86.867216</td> </tr> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8196882203917155
      ],
      "excerpt": "    <tr> <th>26</th> <td>animal</td> <td>bird</td> <td>85.873812</td> <td>2.556551</td> <td>-7.063093</td> <td>-45.238106</td> </tr> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8186223153780717
      ],
      "excerpt": "    <tr> <th>34</th> <td>tool</td> <td>screwdriver</td> <td>-75.439298</td> <td>33.551341</td> <td>10.523150</td> <td>-97.639422</td> </tr> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8132994439434427
      ],
      "excerpt": "    <tr> <th>39</th> <td>tool</td> <td>pliers</td> <td>-45.382544</td> <td>50.950378</td> <td>9.234910</td> <td>-98.404390</td> </tr> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8153058269406914
      ],
      "excerpt": "    <tr> <th>51</th> <td>flower</td> <td>rose</td> <td>146.126568</td> <td>-2.189412</td> <td>23.063286</td> <td>16.749135</td> </tr> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8088385932598596
      ],
      "excerpt": "    <tr> <th>67</th> <td>tree</td> <td>elm</td> <td>196.460629</td> <td>20.488973</td> <td>48.230311</td> <td>-98.977328</td> </tr> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/vackosar/fasttext-vector-norms-and-oov-words/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "FastText Vector Norms And OOV Words",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "fasttext-vector-norms-and-oov-words",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "vackosar",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/vackosar/fasttext-vector-norms-and-oov-words/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 19,
      "date": "Thu, 23 Dec 2021 08:43:33 GMT"
    },
    "technique": "GitHub API"
  }
}