{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2103.07941",
      "https://arxiv.org/abs/1911.01911",
      "https://arxiv.org/abs/1512.03012"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please cite our paper if you find this repo useful!\n\n```bibtex\n@inproceedings{cheng2021mivos,\n  title={Modular Interactive Video Object Segmentation: Interaction-to-Mask, Propagation and Difference-Aware Fusion},\n  author={Cheng, Ho Kei and Tai, Yu-Wing and Tang, Chi-Keung},\n  booktitle={CVPR},\n  year={2021}\n}\n```\n\nAnd if you want to cite the datasets:\n\n<details> \n<summary>\n\nbibtex\n\n</summary>\n\n```bibtex\n@inproceedings{shi2015hierarchicalECSSD,\n  title={Hierarchical image saliency detection on extended CSSD},\n  author={Shi, Jianping and Yan, Qiong and Xu, Li and Jia, Jiaya},\n  booktitle={TPAMI},\n  year={2015},\n}\n\n@inproceedings{wang2017DUTS,\n  title={Learning to Detect Salient Objects with Image-level Supervision},\n  author={Wang, Lijun and Lu, Huchuan and Wang, Yifan and Feng, Mengyang \n  and Wang, Dong, and Yin, Baocai and Ruan, Xiang}, \n  booktitle={CVPR},\n  year={2017}\n}\n\n@inproceedings{FSS1000,\n  title = {FSS-1000: A 1000-Class Dataset for Few-Shot Segmentation},\n  author = {Li, Xiang and Wei, Tianhan and Chen, Yau Pun and Tai, Yu-Wing and Tang, Chi-Keung},\n  booktitle={CVPR},\n  year={2020}\n}\n\n@inproceedings{zeng2019towardsHRSOD,\n  title = {Towards High-Resolution Salient Object Detection},\n  author = {Zeng, Yi and Zhang, Pingping and Zhang, Jianming and Lin, Zhe and Lu, Huchuan},\n  booktitle = {ICCV},\n  year = {2019}\n}\n\n@inproceedings{cheng2020cascadepsp,\n  title={{CascadePSP}: Toward Class-Agnostic and Very High-Resolution Segmentation via Global and Local Refinement},\n  author={Cheng, Ho Kei and Chung, Jihoon and Tai, Yu-Wing and Tang, Chi-Keung},\n  booktitle={CVPR},\n  year={2020}\n}\n\n@inproceedings{xu2018youtubeVOS,\n  title={Youtube-vos: A large-scale video object segmentation benchmark},\n  author={Xu, Ning and Yang, Linjie and Fan, Yuchen and Yue, Dingcheng and Liang, Yuchen and Yang, Jianchao and Huang, Thomas},\n  booktitle = {ECCV},\n  year={2018}\n}\n\n@inproceedings{perazzi2016benchmark,\n  title={A benchmark dataset and evaluation methodology for video object segmentation},\n  author={Perazzi, Federico and Pont-Tuset, Jordi and McWilliams, Brian and Van Gool, Luc and Gross, Markus and Sorkine-Hornung, Alexander},\n  booktitle={CVPR},\n  year={2016}\n}\n\n@inproceedings{denninger2019blenderproc,\n  title={BlenderProc},\n  author={Denninger, Maximilian and Sundermeyer, Martin and Winkelbauer, Dominik and Zidan, Youssef and Olefir, Dmitry and Elbadrawy, Mohamad and Lodhi, Ahsan and Katam, Harinandan},\n  booktitle={arXiv:1911.01911},\n  year={2019}\n}\n\n@inproceedings{shapenet2015,\n  title       = {{ShapeNet: An Information-Rich 3D Model Repository}},\n  author      = {Chang, Angel Xuan and Funkhouser, Thomas and Guibas, Leonidas and Hanrahan, Pat and Huang, Qixing and Li, Zimo and Savarese, Silvio and Savva, Manolis and Song, Shuran and Su, Hao and Xiao, Jianxiong and Yi, Li and Yu, Fisher},\n  booktitle   = {arXiv:1512.03012},\n  year        = {2015}\n}\n```\n\n</details>\n\nContact: <hkchengrex@gmail.com>\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "f-BRS: <https://github.com/saic-vul/fbrs_interactive_segmentation>\n\nivs-demo: <https://github.com/seoungwugoh/ivs-demo>\n\ndeeplab: <https://github.com/VainF/DeepLabV3Plus-Pytorch>\n\nSTM: <https://github.com/seoungwugoh/STM>\n\nBlenderProc: <https://github.com/DLR-RM/BlenderProc>\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{shapenet2015,\n  title       = {{ShapeNet: An Information-Rich 3D Model Repository}},\n  author      = {Chang, Angel Xuan and Funkhouser, Thomas and Guibas, Leonidas and Hanrahan, Pat and Huang, Qixing and Li, Zimo and Savarese, Silvio and Savva, Manolis and Song, Shuran and Su, Hao and Xiao, Jianxiong and Yi, Li and Yu, Fisher},\n  booktitle   = {arXiv:1512.03012},\n  year        = {2015}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{denninger2019blenderproc,\n  title={BlenderProc},\n  author={Denninger, Maximilian and Sundermeyer, Martin and Winkelbauer, Dominik and Zidan, Youssef and Olefir, Dmitry and Elbadrawy, Mohamad and Lodhi, Ahsan and Katam, Harinandan},\n  booktitle={arXiv:1911.01911},\n  year={2019}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{perazzi2016benchmark,\n  title={A benchmark dataset and evaluation methodology for video object segmentation},\n  author={Perazzi, Federico and Pont-Tuset, Jordi and McWilliams, Brian and Van Gool, Luc and Gross, Markus and Sorkine-Hornung, Alexander},\n  booktitle={CVPR},\n  year={2016}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{xu2018youtubeVOS,\n  title={Youtube-vos: A large-scale video object segmentation benchmark},\n  author={Xu, Ning and Yang, Linjie and Fan, Yuchen and Yue, Dingcheng and Liang, Yuchen and Yang, Jianchao and Huang, Thomas},\n  booktitle = {ECCV},\n  year={2018}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{cheng2020cascadepsp,\n  title={{CascadePSP}: Toward Class-Agnostic and Very High-Resolution Segmentation via Global and Local Refinement},\n  author={Cheng, Ho Kei and Chung, Jihoon and Tai, Yu-Wing and Tang, Chi-Keung},\n  booktitle={CVPR},\n  year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{zeng2019towardsHRSOD,\n  title = {Towards High-Resolution Salient Object Detection},\n  author = {Zeng, Yi and Zhang, Pingping and Zhang, Jianming and Lin, Zhe and Lu, Huchuan},\n  booktitle = {ICCV},\n  year = {2019}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{FSS1000,\n  title = {FSS-1000: A 1000-Class Dataset for Few-Shot Segmentation},\n  author = {Li, Xiang and Wei, Tianhan and Chen, Yau Pun and Tai, Yu-Wing and Tang, Chi-Keung},\n  booktitle={CVPR},\n  year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{wang2017DUTS,\n  title={Learning to Detect Salient Objects with Image-level Supervision},\n  author={Wang, Lijun and Lu, Huchuan and Wang, Yifan and Feng, Mengyang \n  and Wang, Dong, and Yin, Baocai and Ruan, Xiang}, \n  booktitle={CVPR},\n  year={2017}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{shi2015hierarchicalECSSD,\n  title={Hierarchical image saliency detection on extended CSSD},\n  author={Shi, Jianping and Yan, Qiong and Xu, Li and Jia, Jiaya},\n  booktitle={TPAMI},\n  year={2015},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{cheng2021mivos,\n  title={Modular Interactive Video Object Segmentation: Interaction-to-Mask, Propagation and Difference-Aware Fusion},\n  author={Cheng, Ho Kei and Tai, Yu-Wing and Tang, Chi-Keung},\n  booktitle={CVPR},\n  year={2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9747466821846071,
        0.8944178096468923
      ],
      "excerpt": "Ho Kei Cheng, Yu-Wing Tai, Chi-Keung Tang \nCVPR 2021 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9105121297654528
      ],
      "excerpt": "<sub><sup>Credit (left to right): DAVIS 2017, Academy of Historical Fencing, Modern History TV</sup></sub> \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/hkchengrex/MiVOS",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-11-17T14:10:00Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-29T08:14:02Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8398909567036016,
        0.8391727531812238,
        0.9679737657103962
      ],
      "excerpt": "[arXiv] [Paper PDF] [Project Page] [Demo] [Papers with Code] [Supplementary Material] \n<sub><sup>Credit (left to right): DAVIS 2017, Academy of Historical Fencing, Modern History TV</sup></sub> \nWe manage the project using three different repositories (which are actually in the paper title). This is the main repo, see also Mask-Propagation and Scribble-to-Mask. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9082502686363026
      ],
      "excerpt": "| Generate more synthetic data | :heavy_check_mark: | :x: | :x: | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8659064144074647
      ],
      "excerpt": "There are instructions in the GUI. You can also watch the demo videos for some ideas. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9751143044371664
      ],
      "excerpt": "All results are generated using the unmodified official DAVIS interactive bot without saving masks (--save_mask not specified) and with an RTX 2080Ti. We follow the official protocol. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8821370063564236,
        0.9382238428957063
      ],
      "excerpt": "| Full model, without BL30K for propagation/fusion | 87.4 | 88.0 | \n| Full model, STCN backbone | 88.4 | 88.8 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9459329358707803,
        0.9177453079532147,
        0.9941863999320971
      ],
      "excerpt": "BL30K is a synthetic dataset rendered using Blender with ShapeNet's data. We break the dataset into six segments, each with approximately 5K videos. \nThe videos are organized in a similar format as DAVIS and YouTubeVOS, so dataloaders for those datasets can be used directly. Each video is 160 frames long, and each frame has a resolution of 768*512. There are 3-5 objects per video, and each object has a random smooth trajectory -- we tried to optimize the trajectories greedily to minimize object intersection (not guaranteed), with occlusions still possible (happen a lot in reality). See generation/blender/generate_yaml.py for details. \nWe noted that using probably half of the data is sufficient to reach full performance (although we still used all), but using less than one-sixth (5K) is insufficient. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9273556005439357,
        0.834973343728097
      ],
      "excerpt": "We implemented training with Distributed Data Parallel (DDP) with two 11GB GPUs. Replace a, b with the GPU ids, cccc with an unused port number,  defg with a unique experiment identifier, and h with the training stage (0/1). \nThe model is trained progressively with different stages (0: BL30K; 1: DAVIS). After each stage finishes, we start the next stage by loading the trained weight. A pretrained propagation model is required to train the fusion module. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "[CVPR 2021] Modular Interactive Video Object Segmentation: Interaction-to-Mask, Propagation and Difference-Aware Fusion. Semi-supervised VOS as well!",
      "technique": "GitHub API"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You can either use the automatic script `download_bl30k.py` or download it manually below. Note that each segment is about 115GB in size -- 700GB in total. You are going to need ~1TB of free disk space to run the script (including extraction buffer).\n\nGoogle Drive is much faster in my experience. Your mileage might vary.\n\nManual download: [[Google Drive]](https://drive.google.com/drive/folders/1KxriFZM8Y_-KbiA3D0PaMv6LQaatKFH-?usp=sharing) [[OneDrive]](https://uillinoisedu-my.sharepoint.com/:f:/g/personal/hokeikc2_illinois_edu/ElEqJXQqaqZAqG8QROa0VesBAw4FiOl5wleP2iq_KXDPyw?e=eKMSbx)\n\n[UST Mirror] (Reliability not guaranteed, speed throttled, do not use if others are available): <ckcpu1.cse.ust.hk:8080/MiVOS/BL30K_{a-f}.tar> (Replace {a-f} with the part that you need).\n\nMD5 Checksum:\n\n```bash\n35312550b9a75467b60e3b2be2ceac81  BL30K_a.tar\n269e2f9ad34766b5f73fa117166c1731  BL30K_b.tar\na3f7c2a62028d0cda555f484200127b9  BL30K_c.tar\ne659ed7c4e51f4c06326855f4aba8109  BL30K_d.tar\nd704e86c5a6a9e920e5e84996c2e0858  BL30K_e.tar\nbf73914d2888ad642bc01be60523caf6  BL30K_f.tar\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/hkchengrex/MiVOS/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 28,
      "date": "Wed, 29 Dec 2021 17:48:42 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/hkchengrex/MiVOS/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "hkchengrex/MiVOS",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/hkchengrex/MiVOS/tree/main/docs"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Datasets should be arranged as the following layout. You can use `download_datasets.py` (same as the one Mask-Propagation) to get the DAVIS dataset and manually download and extract fusion_data ([[OneDrive]](https://hkustconnect-my.sharepoint.com/:u:/g/personal/hkchengad_connect_ust_hk/ESGj7FihDUpNjpygP8u1NGkBc-9YFSMFCDDpxKA87aTJ4w?e=SPXheO)) and [BL30K](#bl30k).\n\n```bash\n\u251c\u2500\u2500 BL30K\n\u251c\u2500\u2500 DAVIS\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 2017\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 test-dev\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 Annotations\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 ...\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 trainval\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 Annotations\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 fusion_data\n\u2514\u2500\u2500 MiVOS\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9324927161336041
      ],
      "excerpt": "python download_model.py to get all the required models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.849035382938485
      ],
      "excerpt": "See eval_interactive_davis.py. If you have downloaded the datasets and pretrained models using our script, you only need to specify the output path, i.e., python eval_interactive_davis.py --output [somewhere]. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9983526696848647
      ],
      "excerpt": "python download_model.py should get you all the models that you need. (pip install gdown required.) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8096422022825122,
        0.8438079615626676
      ],
      "excerpt": "Download ShapeNet. \nInstall Blender. (We used 2.82) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8187236509318547
      ],
      "excerpt": "One concrete example is: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8468319551963799
      ],
      "excerpt": "See eval_interactive_davis.py. If you have downloaded the datasets and pretrained models using our script, you only need to specify the output path, i.e., python eval_interactive_davis.py --output [somewhere]. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8072230334103736
      ],
      "excerpt": "Precomputed result, with the json summary: [Google Drive] [OneDrive] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.810870454768102
      ],
      "excerpt": "Download ShapeNet. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/hkchengrex/MiVOS/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Cuda",
      "C++",
      "Cython"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "GNU General Public License v3.0",
      "url": "https://api.github.com/licenses/gpl-3.0"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2018 Tamaki Kojima\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Modular Interactive Video Object Segmentation: Interaction-to-Mask, Propagation and Difference-Aware Fusion (MiVOS)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "MiVOS",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "hkchengrex",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/hkchengrex/MiVOS/blob/main/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "hkchengrex",
        "body": "Pretrained models and precomputed results",
        "dateCreated": "2021-03-14T14:59:09Z",
        "datePublished": "2021-03-14T15:05:04Z",
        "html_url": "https://github.com/hkchengrex/MiVOS/releases/tag/1.0",
        "name": "v1.0",
        "tag_name": "1.0",
        "tarball_url": "https://api.github.com/repos/hkchengrex/MiVOS/tarball/1.0",
        "url": "https://api.github.com/repos/hkchengrex/MiVOS/releases/39783082",
        "zipball_url": "https://api.github.com/repos/hkchengrex/MiVOS/zipball/1.0"
      }
    ],
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We used these packages/versions in the development of this project. It is likely that higher versions of the same package will also work. This is not an exhaustive list -- other common python packages (e.g. pillow) are expected and not listed.\n\n- PyTorch `1.7.1`\n- torchvision `0.8.2`\n- OpenCV `4.2.0`\n- Cython\n- progressbar\n- davis-interactive (<https://github.com/albertomontesg/davis-interactive>)\n- PyQt5 for GUI\n- networkx `2.4` for DAVIS\n- gitpython for training\n- gdown for downloading pretrained models\n\nRefer to the official [PyTorch guide]((<https://pytorch.org/>)) for installing PyTorch/torchvision. The rest can be installed by:\n\n`pip install PyQt5 davisinteractive progressbar2 opencv-python networkx gitpython gdown Cython`\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 243,
      "date": "Wed, 29 Dec 2021 17:48:42 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "computer-vision",
      "segmentation",
      "deep-learning",
      "pytorch",
      "cvpr2021",
      "interactive-segmentation",
      "video-object-segmentation",
      "video-segmentation"
    ],
    "technique": "GitHub API"
  }
}