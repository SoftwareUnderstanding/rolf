{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. M. Welling, Y. Teh. [Bayesian Learning via Stochastic Gradient Langevin Dynamics](https://pdfs.semanticscholar.org/aeed/631d6a84100b5e9a021ec1914095c66de415.pdf). ICML'11\n\n2. W. Deng, Q. Feng, L. Gao, F. Liang, G. Lin. [Non-convex Learning via Replica Exchange Stochastic Gradient MCMC](https://arxiv.org/pdf/2008.05367.pdf). ICML'20.\n\n4. W. Deng, Q. Feng, G. Karagiannis, G. Lin, F. Liang. [Accelerating Convergence of Replica Exchange Stochastic Gradient MCMC via Variance Reduction](https://openreview.net/forum?id=iOnhIy-a-0n). ICLR'21.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{VR-reSGLD,\n  title={Accelerating Convergence of Replica Exchange Stochastic Gradient MCMC via Variance Reduction},\n  author={Wei Deng and Qi Feng and Georgios P. Karagiannis and Guang Lin and Faming Liang},\n  booktitle={International Conference on Learning Representations},\n  year={2021}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/WayneDW/Variance_Reduced_Replica_Exchange_SGMCMC",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-03-04T21:16:10Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-08-23T11:57:56Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.976195200402725,
        0.8565312602018578
      ],
      "excerpt": "Despite the advantages of gradient variance reduction in near-convex problems, a natural discrepancy between theory and practice is that whether we should avoid the gradient noise in non-convex problems. To fill in the gap, we only focus on the variance reduction of noisy energy estimators to exploit the theoretical accelerations but no longer consider the variance reduction of the noisy gradients so that the empirical experience from stochastic gradient descents with momentum (M-SGD) can be naturally imported. \nMomentum stochastic gradient descent (M-SGD) with 500 epochs, batch size 256 and decreasing learning rates \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8843522518176079
      ],
      "excerpt": "Standard SGHMC with cylic learning rates and 1000 epochs \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8060440728039628
      ],
      "excerpt": "Standard Replica Exchange SGHMC (reSGHMC) with annealing temperatures in warm-up period and fixed temperature afterward \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8980029059017026
      ],
      "excerpt": "Variance-reduced Replica Exchange SGLD with control variates updated every 2 epochs and fixed temperature after the warm-up period (Algorithm 1) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9425092181598763
      ],
      "excerpt": "Apply a temperature scaling of 2 for uncertainty calibration \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Variance reduction in energy estimators accelerates the exponential convergence in deep learning (ICLR'21)",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/WayneDW/Variance_Reduced_Replica_Exchange_SGMCMC/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Tue, 28 Dec 2021 11:41:09 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/WayneDW/Variance_Reduced_Replica_Exchange_SGMCMC/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "WayneDW/Variance_Reduced_Replica_Exchange_SGMCMC",
    "technique": "GitHub API"
  },
  "invocation": [
    {
      "confidence": [
        0.8560919189083024
      ],
      "excerpt": "$ python bayes_cnn.py -sn 500 -chains 1 -lr 2e-6 -LRanneal 0.984 -T 1e-300  -burn 0.6 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9384608127052568
      ],
      "excerpt": "$ python bayes_cnn.py -sn 500 -chains 1 -lr 2e-6 -LRanneal 0.984 -T 0.01 -Tanneal 1.02 -burn 0.6 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8863851840750523
      ],
      "excerpt": "$ python bayes_cnn.py -sn 500 -chains 2 -lr 2e-6 -LRanneal 0.984 -T 0.01 -var_reduce 0 -period 2 -bias_F 1.5e5 -burn 0.6 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8863851840750523
      ],
      "excerpt": "$ python bayes_cnn.py -sn 500 -chains 2 -lr 2e-6 -LRanneal 0.984 -T 0.01 -var_reduce 1 -period 2 -bias_F 1.5e5 -burn 0.6 -seed 85674 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8863851840750523
      ],
      "excerpt": "$ python bayes_cnn.py -sn 500 -chains 2 -lr 2e-6 -LRanneal 0.984 -T 0.01 -var_reduce 1 -period 2 -bias_F 1.5e5 -burn 0.6 -adapt_c 1 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/WayneDW/Variance_Reduced_Replica_Exchange_SGMCMC/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "C",
      "Python",
      "R"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Variance Reduced Replica Exchange Stochastic Gradient MCMC",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Variance_Reduced_Replica_Exchange_SGMCMC",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "WayneDW",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/WayneDW/Variance_Reduced_Replica_Exchange_SGMCMC/blob/main/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* Python 2.7\n* [PyTorch = 1.1](https://pytorch.org/) or similar\n* numpy\n* CUDA \n\n\n<p float=\"left\">\n  <img src=\"output/SGLD.gif\" width=\"270\" title=\"SGLD\"/>\n  <img src=\"output/reSGLD_vs_VR_reSGLD.gif\" width=\"500\" alt=\"Made with Angular\" title=\"Angular\" /> \n</p>\n\n\nPlease cite our paper ([link](https://openreview.net/forum?id=iOnhIy-a-0n)) if you find it useful in uncertainty estimations\n\n```\n@inproceedings{VR-reSGLD,\n  title={Accelerating Convergence of Replica Exchange Stochastic Gradient MCMC via Variance Reduction},\n  author={Wei Deng and Qi Feng and Georgios P. Karagiannis and Guang Lin and Faming Liang},\n  booktitle={International Conference on Learning Representations},\n  year={2021}\n}\n```\n\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 4,
      "date": "Tue, 28 Dec 2021 11:41:09 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "variance-reduction",
      "sghmc",
      "sgmcmc",
      "replica-exchange",
      "parallel-tempering"
    ],
    "technique": "GitHub API"
  }
}