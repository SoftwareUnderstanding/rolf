{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1506.01497>Faster R-CNN</a>. However, the background class is ill-defined and has a changing definition based on the foreground object categories considered. Establishing an mapping for the background class in the attribute space is ambiguous and unclear.\n- Thus, existing deep convolutional recognition pipelines need to be modified to allow bypassing an attribute mapping for the background. We propose one such modification in the current work. The proposed architecture affords incorporation of attribute-based embedding space over the non-background category labels in classification models; while bypassing the background label by the use of a learned threshold that is supposed to preemptively filter out non-object samples.\n- More details here: https://github.com/saumya-jetley/pp_ICVSS15_TacklingBkgndDifferently/blob/master/poster/poster.pdf\n\n\n\n<a href=https://github.com/saumya-jetley/TacklingBkgndDifferently_ICVSS15/blob/master/License>License</a>"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/saumya-jetley/pp_ICVSS15_TacklingBkgndDifferently",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-05-01T12:24:58Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-02-18T16:03:23Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Classifying background using a learned threshold. This affords smooth incorporation of attribute-based embedding space over the non-background category labels in classification models.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/saumya-jetley/pp_ICVSS15_TacklingBkgndDifferently/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sun, 26 Dec 2021 09:53:36 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/saumya-jetley/pp_ICVSS15_TacklingBkgndDifferently/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "saumya-jetley/pp_ICVSS15_TacklingBkgndDifferently",
    "technique": "GitHub API"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/saumya-jetley/pp_ICVSS15_TacklingBkgndDifferently/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "TeX"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'TacklingBkgndDifferently \\xc2\\xa9 2015, Torr Vision Group, The University of Oxford (the \"Software\") \\n\\nThe Software remains the property of the University of Oxford (\"the University\"). \\nThe Software is distributed \"AS IS\" under this Licence solely for non-commercial\\nuse in the hope that it will be useful, but in order that the University as a \\ncharitable foundation protects its assets for the benefit of its educational \\nand research purposes, the University makes clear that no condition is made \\nor to be implied, nor is any warranty given or to be implied, as to the accuracy \\nof the Software, or that it will be suitable for any particular purpose or for \\nuse under any specific conditions. Furthermore, the University disclaims all \\nresponsibility for the use which is made of the Software. It further disclaims \\nany liability for the outcomes arising from using the Software.\\n\\nThe Licensee agrees to indemnify the University and hold the University harmless \\nfrom and against any and all claims, damages and liabilities asserted by third \\nparties (including claims for negligence), which arise directly or indirectly \\nfrom the use of the Software or the sale of any products based on the Software.\\n\\nNo part of the Software may be reproduced, modified, transmitted or transferred \\nin any form or by any means, electronic or mechanical, without the express \\npermission of the University. The permission of the University is not required \\nif the said reproduction, modification, transmission or transference is done \\nwithout financial return, the conditions of this Licence are imposed upon the \\nreceiver of the product, and all original and amended source code is included \\nin any transmitted product. You may be held legally responsible for any copyright \\ninfringement that is caused or encouraged by your failure to abide by \\nthese terms and conditions.\\n\\nYou are not permitted under this Licence to use this Software commercially. \\nUse for which any financial return is received shall be defined as commercial \\nuse, and includes: integration of all or part of the source code or the Software \\ninto a product for sale or license by or on behalf of Licensee to third parties \\nor use of the Software or any derivative of it for research with the final aim \\nof developing software products for sale or license to a third party or use of \\nthe Software or any derivative of it for research with the final aim of \\ndeveloping non-software products for sale or license to a third party, or \\nuse of the Software to provide any service to an external organisation for \\nwhich payment is received. \\n\\nIf you are interested in using the Software commercially, \\nplease contact Torr Vision Group directly to negotiate a licence. \\nContact details are: philip.torr@eng.ox.ac.uk.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Tackling Background Differently - presented at ICVSS-2015",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "pp_ICVSS15_TacklingBkgndDifferently",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "saumya-jetley",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/saumya-jetley/pp_ICVSS15_TacklingBkgndDifferently/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sun, 26 Dec 2021 09:53:36 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- This work proposes classifying background using a learned threshold. \n- It branches off from the research presented in <a href=http://www.robots.ox.ac.uk/~tvg/publications/2015/bmvc_383_cr.pdf>Prototypical priors, Jetley et.al</a>. \n- The sub-field of zero-shot recognition relies heavily upon the definition of an attribute-based continuous embedding space that maps through to the category labels to allow test-time classification of images to unseen categories with known attributes. In particular for the case above, the attributes are the prototypical templates of objects such as traffic lights, logos, etc.\n- For recognition in real-world, soa CNN-based detection models trivially introduce a background class which subsumes all real-world visuals that do not belong to any of the pre-determined object categories of interest, see <a href=https://arxiv.org/pdf/1504.08083.pdf>Fast-RCNN</a>, <a href=https://arxiv.org/abs/1506.01497>Faster R-CNN</a>. However, the background class is ill-defined and has a changing definition based on the foreground object categories considered. Establishing an mapping for the background class in the attribute space is ambiguous and unclear.\n- Thus, existing deep convolutional recognition pipelines need to be modified to allow bypassing an attribute mapping for the background. We propose one such modification in the current work. The proposed architecture affords incorporation of attribute-based embedding space over the non-background category labels in classification models; while bypassing the background label by the use of a learned threshold that is supposed to preemptively filter out non-object samples.\n- More details here: https://github.com/saumya-jetley/pp_ICVSS15_TacklingBkgndDifferently/blob/master/poster/poster.pdf\n\n\n\n<a href=https://github.com/saumya-jetley/TacklingBkgndDifferently_ICVSS15/blob/master/License>License</a>\n",
      "technique": "Header extraction"
    }
  ]
}