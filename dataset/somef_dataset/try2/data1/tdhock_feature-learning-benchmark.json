{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1703.03352][arXiv:1703.03352]]\n(it computes the most likely peak positions for a given penalty\nparameter",
      "https://arxiv.org/abs/1406.4729\n- Typical neural network implementations, e.g. [[http://scikit-learn.org/stable/modules/neural_networks_supervised.html][scikit-learn]], support\n  regression but not censored outputs. We could either (1",
      "https://arxiv.org/abs/1703.03352]]\n(it computes the most likely peak positions for a given penalty\nparameter). Typical unsupervised methods (which do not use the labels)\nfor choosing the penalty parameter are theoretically-motivated\npenalties (AIC/BIC) or cross-validation. The current methods for\nlearning the penalty function use pre-defined features as inputs\n(number of data points, mean, variance estimates, quantiles, etc) and\nthe predicted penalty is the output. This data set may be useful to\nexplore feature learning methods such as deep convolutional neural\nnetworks, which could be used to relax the assumption of pre-defined\nfeatures. The interest of this data set is that it has a large number\nof labels (e.g. 15961 labels in the H3K27ac-H3K4me3_TDHAM_BP data\nset), which is an order of magnitude more labels than other benchmark\ndata sets of this type (e.g. 3418 labels in the [[https://cran.r-project.org/package%3Dneuroblastoma][neuroblastoma]] data\nset). For more info about supervised changepoint detection see [[https://tdhock.github.io/change-tutorial/Supervised.html][My\nuseR2017 tutorial]].\n\nTo compare against this baseline, the following files are included in\nthis repository:\n- [[file:labeled_problems_features.csv]]: pre-defined features to be used\n  as inputs in the machine learning problem.\n- [[file:labeled_problems_targets.csv]]: outputs for the machine learning\n  problem: target interval of log(penalty) values which achieves min\n  incorrect labels. The goal is to learn a function that inputs the\n  vector of features and outputs a value in this interval. These\n  outputs can be used to compute a simple prediction error metric\n  (number of incorrectly predicted intervals).\n- [[file:labeled_problems_errors.csv]]: used to compute a more relevant\n  prediction error metric, the number of incorrectly predicted labels\n  (and ROC-AUC). For each problem the table gives the number of false\n  positives (fp), false negatives (fn), and total incorrect labels\n  (errors) for intervals of log(penalty) values (min.log.penalty,\n  max.log.penalty). For example the first row is\n\n#+BEGIN_SRC \nATAC_JV_adipose/samples/AC1/MSC77/problems/chr10:18024675-38818835,0,-Inf,2.05953368773019,6,0,6\n#+END_SRC\n\nand should be interpreted in the following way:\n- for the problem ATAC_JV_adipose/samples/AC1/MSC77/problems/chr10:18024675-38818835\n- if your function predicts a value between -Inf and 2.05953368773019\n- then the predicted peaks are given by running PeakSegFPOP with penalty=0\n- which yields 6 fp, 0 fn, and 6 errors.\n\n- [[file:labeled_problems_possible_errors.csv]] contains the total number\n  of labels, for computing test error/accuracy rates and Receiver Operating\n  Characteristic (ROC) curves. If there is no model/parameter with\n  peaks in all positive labels, then the ROC curve does not have a\n  point at FPR=TPR=1, so it is suggested to use linear extrapolation\n  between that point and the point in the ROC space which corresponds\n  to the model with the most predicted peaks. If there is a model with\n  non-hierarchical peaks (e.g. there is a parameter lambda_1 which\n  yields 1 peak, and there is a parameter lambda_2 which yields 2\n  peaks which are both different from the lambda_1 peak) then the ROC\n  curve may be non-monotonic. In that case it is suggested to compute\n  AUC using an algorithm that works for general polygons, for example\n  geometry::polyarea in R.\n- [[file:labeled_problems_AUC.R]] contains R code for computing K-fold CV\n  test error/accuracy/AUC, given the folds defined in the data set\n  (e.g. [[file:data/ATAC_JV_adipose/folds.csv]]). To use this script,\n  first save predicted log(penalty) values in the\n  [[file:labeled_problems_pred]] directory. Each model should be saved as\n  a separate csv file, with two columns: prob.dir and\n  pred.log.lambda. For example this repo has two unsupervised\n  baselines: (1) [[file:labeled_problems_pred/AIC.csv]] is the constant\n  AIC baseline, and (2) [[file:labeled_problems_pred/BIC.csv]] is the\n  penalty=log(number of data points) baseline. Running the R script\n  will create a labeled_problems_pred_error/MODEL.csv file with one\n  line for every (data set, fold) combination. Columns include test\n  error/accuracy/FPR/TPR and test AUC.\n- [[file:labeled_problems_plot_test_accuracy.R]] is an R script that can\n  be used to plot such test error metrics.\n- [[file:labeled_problems_folds.csv]] is a copy of the folds defined in\n  the data set. (included to be able to compute K-fold CV test\n  error/accuracy/AUC metrics for baselines, even if the full data set\n  is not available)\n- [[file:labeled_problems_pred_IntervalRegressionCV.R]] computes\n  L1-regularized linear model predictions, saving them to\n  [[file:labeled_problems_pred/IntervalRegressionCV.csv]]\n- [[file:labeled_problems_pred_BestConstant.R]] computes model predictions\n  for the best constant penalty, based on the penalty error functions\n  in the training data. Predictions saved to\n  [[file:labeled_problems_pred/BestConstant.csv]]\n- [[file:labeled_problems_pred_MultiTaskIRCV.R]] computes a linear\n  multi-task learning model, by creating indicator variables for each\n  task. Predictions saved to [[file:labeled_problems_pred/MultiTaskIRCV.csv]]\n\nThe figure below shows prediction accuracy and AUC, using the\ndesignated four-fold cross-validation scheme. It shows room for\nimprovement in penalty function learning algorithms:\n- learned linear penalty functions (IntervalRegressionCV,\n  MultiTaskIRCV) have similar test accuracy as the learned constant\n  penalty (BestConstant).\n- All models have similar test AUC.\n\n[[file:labeled_problems_plot_test_accuracy.png]]\n\n** Ideas/links\n\n- Deep Convnet with spatial pyramid pooling, in order to train the\n  model using inputs of variable sizes (different chrom\n  subsets). https://arxiv.org/abs/1406.4729\n- Typical neural network implementations, e.g. [[http://scikit-learn.org/stable/modules/neural_networks_supervised.html][scikit-learn]], support\n  regression but not censored outputs. We could either (1) convert\n  target intervals into a real-valued output and use existing code, or\n  (2) implement code for a loss function that exploits the structure\n  of the censored outputs.\n\n** 18 Nov 2019 AUC improvement\n\n[[file:figure-auc-improved-train-predictions.R]] makes\n\n[[file:figure-auc-improved-train-predictions.png]]\n\n** 20 Aug 2019 Min Area Under Min(FP,FN)\n\n[[file:auc.improved.R]] performs the computation using a gradient descent\nalgorithm, and [[file:figure-auc-improved-interactive.R]] visualizes the\nresults:\n- [[http://jan.ucc.nau.edu/~th798/viz/2019-08-20-min-area-under-min-fp-fn/][improvement starting from best predicted when labels get equal weight]].\n- [[http://jan.ucc.nau.edu/~th798/viz/2019-10-03-auc-improved-weighted/][improvement starting from best predicted when classes get equal weight]].\n\n** 15 Feb 2018 fold assignment code\n\n[[file:folds.R]] implements a randomized heuristic for assigning problems\nto folds such that there are approximately equal numbers of labels in\neach fold.\n\n** 30 Jan 2018 data set sizes\n\n[[file:download.R]] used to download count data bedGraph.gz files, along\nwith labels.bed files (38337 labels total in 5581 problems), for a\ntotal of almost 40GB of data. Maybe distribute one file per data set?\n\n#+BEGIN_SRC \n> mb[per.set, on=list(set)][order(labels)]\n    megabytes                      set labels\n 1:       554       H3K36me3_TDH_other    200\n 2:       377      H3K36me3_TDH_ENCODE    338\n 3:       375       H3K4me3_TDH_ENCODE    525\n 4:       592       H3K27me3_RL_cancer    570\n 5:       798         H3K27ac_TDH_some    627\n 6:       906      H3K36me3_TDH_immune    630\n 7:       296        H3K27me3_TDH_some    696\n 8:      2407          CTCF_TDH_ENCODE   1378\n 9:      3223           H3K4me1_TDH_BP   1584\n10:      5871       H3K36me3_AM_immune   1743\n11:      6407          ATAC_JV_adipose   3241\n12:      3017       H3K4me3_PGP_immune   3780\n13:      2902       H3K4me3_TDH_immune   3807\n14:      5421 H3K27ac-H3K4me3_TDHAM_BP  15961\n> \n#+END_SRC"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8848622769763193
      ],
      "excerpt": "wget https://archive.ics.uci.edu/ml/machine-learning-databases/00439/peak-detection-data.tar.xz \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9104388306336967
      ],
      "excerpt": "McGill University \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9105368110547479
      ],
      "excerpt": "PeakError, https://github.com/tdhock/PeakError \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.983192530216809
      ],
      "excerpt": "Optimizing ChIP-seq peak detectors using visual labels and supervised machine learning. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999963437873496,
        0.8654671031158477,
        0.996505004364323
      ],
      "excerpt": "Bioinformatics, Volume 33, Issue 4, 15 February 2017, Pages 491\u2013499, https://doi.org/10.1093/bioinformatics/btw672 \n** Citation requests \nPlease cite the Bioinformatics paper above. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9805831925890827
      ],
      "excerpt": "described in our [[http://proceedings.mlr.press/v37/hocking15.html][ICML'15]] paper. The changepoint detection method \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9646392902913777
      ],
      "excerpt": "implements the log-linear time algorithm described in [[https://arxiv.org/abs/1703.03352][arXiv:1703.03352]] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9944484218006108
      ],
      "excerpt": "  subsets). https://arxiv.org/abs/1406.4729 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9677640385174676
      ],
      "excerpt": "** 18 Nov 2019 AUC improvement \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8636272261581446
      ],
      "excerpt": "** 20 Aug 2019 Min Area Under Min(FP,FN) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8431563035065749,
        0.9174848289839764
      ],
      "excerpt": "- [[http://jan.ucc.nau.edu/~th798/viz/2019-10-03-auc-improved-weighted/][improvement starting from best predicted when classes get equal weight]]. \n** 15 Feb 2018 fold assignment code \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8570793244268878
      ],
      "excerpt": "** 30 Jan 2018 data set sizes \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "10:      5871       H3K36me3_AM_immune   1743 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "12:      3017       H3K4me3_PGP_immune   3780 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/tdhock/feature-learning-benchmark",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-01-30T20:48:31Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-10-16T16:52:54Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.930037505740478,
        0.8644127145205314,
        0.9038181113820394
      ],
      "excerpt": "This repo contains code for creating a benchmark data set for \npredicting peaks in epigenomic data. It has been submitted to the UCI \nMachine Learning Repository and is now online as the [[https://archive.ics.uci.edu/ml/datasets/chipseq#][chipseq]] data set. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8266651887487488,
        0.824245017004274
      ],
      "excerpt": "designing a loss function and algorithm that exploits the structure of \nthe weak labels. For more details about the weak labels please read \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9176120608006595
      ],
      "excerpt": "problem in these data is structured binary classification / changepoint detection. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8022555699001245
      ],
      "excerpt": "These data are significant because they are among the first to provide \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9562461263253277
      ],
      "excerpt": "the H3K9me3_TDH_BP data set). To save disk space the vectors are saved \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.891766188959302
      ],
      "excerpt": "which mean that the first 26 entries of the vector are 0, the next \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8564529473438748
      ],
      "excerpt": "start positions are 0-based but end positions are 1-based, so the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9421040411890913,
        0.8688526749490445
      ],
      "excerpt": "The goal is to learn a function that takes the coverage.bedGraph.gz \nfile as input, and outputs a binary classification for every genomic \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9438471833251811
      ],
      "excerpt": "several regions of the genome with or without peaks. For example the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8951988559380281
      ],
      "excerpt": "noPeaks: all of the predictions in this region should be negative / \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9936703053915232
      ],
      "excerpt": "that for a vector x_i of count data from i=30028083 to i=103863906, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9703311400278745
      ],
      "excerpt": "predicted f(x_i)=1 for any i in this region, that is counted as a \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9036936265727625
      ],
      "excerpt": "f(x_{i-1})=0. The exact position is unspecified; any position is fine, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9962509660234871
      ],
      "excerpt": "negative). More starts is a false positive, and fewer starts is a \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8900184767570718
      ],
      "excerpt": "region. A peak end is defined as a position i such that a peak is \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9186131617159489,
        0.8089380109456028
      ],
      "excerpt": "The exact position is unspecified; any position is fine, as long as \nthere is only one end in the region. Predicting exactly one peak end \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9976308234752119
      ],
      "excerpt": "ends is a false positive, and fewer ends is a false negative. For \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8710532237756113
      ],
      "excerpt": "region (anywhere is fine). Zero predicted peaks in this region is a \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.83812558725477
      ],
      "excerpt": "as an evaluation metric (smaller is better). Typically the peak \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9833474982371181
      ],
      "excerpt": "of predicted peaks f_lambda(x), where lambda is some significance \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9346288651932394,
        0.8248555985991057
      ],
      "excerpt": "number of peakStart/End labels with two or more predicted starts/end +  \nnumber of noPeaks labels with overlapping predicted peaks \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9480612762441701,
        0.9563937089051595
      ],
      "excerpt": "means that for data set H3K36me3_TDH_other, the fold ID 2 consists of \nall data in \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9527297911282095
      ],
      "excerpt": "There are several types of learning settings that could be used with \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8002053094750391
      ],
      "excerpt": "for training model parameters). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8844938904561023,
        0.8462023909087415
      ],
      "excerpt": "between data sets. However there is something common across data sets \nin that in each data set, the peak / positive class is large values, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.924548895692912
      ],
      "excerpt": "learning model to a single-task learning model, use the suggested \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8517803341378728,
        0.9405144924073533
      ],
      "excerpt": "multi-task and single-task learning models using all other folds, then \nmake predictions on all data with fold ID 1.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8321653375720374,
        0.8629699149267673,
        0.8196457794190638
      ],
      "excerpt": "sequence reads that has aligned at that particular region of the \ngenome. Larger values are more likely to be peaks / positive, smaller \nvalues are more likely to be noise / negative. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.990651183597077
      ],
      "excerpt": "The labeling method and details on how to compute the number of incorrect labels is described in: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8781986123308833,
        0.9781229523754219
      ],
      "excerpt": "** Current state-of-the-art \nThe current state-of-the-art on these type of problems is constrained \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8896503711243335,
        0.9110016860433029,
        0.9762450730784787
      ],
      "excerpt": "described in our [[http://proceedings.mlr.press/v37/hocking15.html][ICML'15]] paper. The changepoint detection method \ndescribed in that paper is the Constrained Dynamic Programming \nAlgorithm which is quadratic time so is too slow for these large data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9466905244082854
      ],
      "excerpt": "for choosing the penalty parameter are theoretically-motivated \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8747172019276778
      ],
      "excerpt": "(number of data points, mean, variance estimates, quantiles, etc) and \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8152070389397013,
        0.9450212474075962,
        0.9680728948514551,
        0.87036245978177,
        0.9630816936724903,
        0.9535272582052062
      ],
      "excerpt": "explore feature learning methods such as deep convolutional neural \nnetworks, which could be used to relax the assumption of pre-defined \nfeatures. The interest of this data set is that it has a large number \nof labels (e.g. 15961 labels in the H3K27ac-H3K4me3_TDHAM_BP data \nset), which is an order of magnitude more labels than other benchmark \ndata sets of this type (e.g. 3418 labels in the [[https://cran.r-project.org/package%3Dneuroblastoma][neuroblastoma]] data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9181371878687146
      ],
      "excerpt": "this repository: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9137223509059043,
        0.9605089446892978
      ],
      "excerpt": "  incorrect labels. The goal is to learn a function that inputs the \n  vector of features and outputs a value in this interval. These \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8136463374728882
      ],
      "excerpt": "  (and ROC-AUC). For each problem the table gives the number of false \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9067427224018578
      ],
      "excerpt": "  Characteristic (ROC) curves. If there is no model/parameter with \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8342711996122834,
        0.9395398267972326,
        0.9736519934725134,
        0.9150871283320826,
        0.9385734457379418
      ],
      "excerpt": "  point at FPR=TPR=1, so it is suggested to use linear extrapolation \n  between that point and the point in the ROC space which corresponds \n  to the model with the most predicted peaks. If there is a model with \n  non-hierarchical peaks (e.g. there is a parameter lambda_1 which \n  yields 1 peak, and there is a parameter lambda_2 which yields 2 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8772188191972413,
        0.8337681590337828
      ],
      "excerpt": "  curve may be non-monotonic. In that case it is suggested to compute \n  AUC using an algorithm that works for general polygons, for example \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8614647946302754
      ],
      "excerpt": "  is not available) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8711915846982756
      ],
      "excerpt": "  L1-regularized linear model predictions, saving them to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8894413954292694,
        0.9446145419644002
      ],
      "excerpt": "  for the best constant penalty, based on the penalty error functions \n  in the training data. Predictions saved to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8193339787440598
      ],
      "excerpt": "The figure below shows prediction accuracy and AUC, using the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9277896756199592,
        0.9222450357099372
      ],
      "excerpt": "Deep Convnet with spatial pyramid pooling, in order to train the \n  model using inputs of variable sizes (different chrom \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8929030487287943
      ],
      "excerpt": "Typical neural network implementations, e.g. [[http://scikit-learn.org/stable/modules/neural_networks_supervised.html][scikit-learn]], support \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9426641146295002
      ],
      "excerpt": "  of the censored outputs. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9444260628806272
      ],
      "excerpt": "to folds such that there are approximately equal numbers of labels in \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/tdhock/feature-learning-benchmark/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Sun, 26 Dec 2021 09:53:50 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/tdhock/feature-learning-benchmark/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "tdhock/feature-learning-benchmark",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.9373630990827815
      ],
      "excerpt": "** Download instructions \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8539362522617827
      ],
      "excerpt": "above), you should have a =data= directory. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9043929270298233
      ],
      "excerpt": "** Source \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8918974083095406
      ],
      "excerpt": "PeakError, https://github.com/tdhock/PeakError \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8408618437514348
      ],
      "excerpt": "number of peaks labels with at least one overlapping predicted peak + \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8561982175950494
      ],
      "excerpt": "  peaks which are both different from the lambda_1 peak) then the ROC \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8292143772677144
      ],
      "excerpt": "       0%       25%       50%       75%      100%  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8051580412496055
      ],
      "excerpt": "first line means a 0 from all positions from 48135600 to 48135625 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8830955029367317
      ],
      "excerpt": "background noise. For example the first line in the file above means \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8475732701789596
      ],
      "excerpt": "the desired function should predict negative / background noise \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.85460757077341
      ],
      "excerpt": "false negative. For example, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8571458130165743
      ],
      "excerpt": "TPR = (total number of true positives)/(total number of labels that could have a true positive) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.81879501189808
      ],
      "excerpt": "FPR = (total number of false positives)/(total number of labels that could have a false positive) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8322798213235338
      ],
      "excerpt": "can be found in data/*/folds.csv files. For example \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8004691776089373
      ],
      "excerpt": "these data. Here are four examples. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8954974937413815
      ],
      "excerpt": "[[file:labeled_problems_possible_errors.csv]] contains the total number \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8192543902034155,
        0.8489333611923237,
        0.8294808921247606,
        0.8223966361040922
      ],
      "excerpt": "  (e.g. [[file:data/ATAC_JV_adipose/folds.csv]]). To use this script, \n  first save predicted log(penalty) values in the \n  [[file:labeled_problems_pred]] directory. Each model should be saved as \n  a separate csv file, with two columns: prob.dir and \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8171726996213133
      ],
      "excerpt": "  baselines: (1) [[file:labeled_problems_pred/AIC.csv]] is the constant \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9004778765188841
      ],
      "excerpt": "  will create a labeled_problems_pred_error/MODEL.csv file with one \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8621882743635256
      ],
      "excerpt": "[[file:labeled_problems_pred_IntervalRegressionCV.R]] computes \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8875221763003002,
        0.8425683558877639
      ],
      "excerpt": "  [[file:labeled_problems_pred/IntervalRegressionCV.csv]] \n[[file:labeled_problems_pred_BestConstant.R]] computes model predictions \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8875221763003002
      ],
      "excerpt": "  [[file:labeled_problems_pred/BestConstant.csv]] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8157786861461312
      ],
      "excerpt": "  task. Predictions saved to [[file:labeled_problems_pred/MultiTaskIRCV.csv]] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8222811556731915
      ],
      "excerpt": "[[file:figure-auc-improved-train-predictions.R]] makes \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8844327367730577
      ],
      "excerpt": "[[file:download.R]] used to download count data bedGraph.gz files, along \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8709160041305285
      ],
      "excerpt": "total of almost 40GB of data. Maybe distribute one file per data set? \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/tdhock/feature-learning-benchmark/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "R",
      "Makefile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "][chipseq]] data set.",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "feature-learning-benchmark",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "tdhock",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/tdhock/feature-learning-benchmark/blob/master/README.org",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Sun, 26 Dec 2021 09:53:50 GMT"
    },
    "technique": "GitHub API"
  }
}