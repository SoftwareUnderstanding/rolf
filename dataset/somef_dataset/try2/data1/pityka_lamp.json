{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1711.05101",
      "https://arxiv.org/abs/1802.03426"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/pityka/lamp",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-05-25T20:26:16Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-19T11:31:16Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9240229980848905,
        0.8422807822338669
      ],
      "excerpt": "Lamp is a Scala library for deep learning and scientific computing.  \nIt features a native CPU and GPU backend and operates on off-heap memory.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9913106347869235
      ],
      "excerpt": "The foundation of lamp is a JNI binding to ATen, the C++ tensor backend of pytorch (see here). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9411370772930987,
        0.8863209401278563,
        0.9549072035236543
      ],
      "excerpt": "This repository also hosts some other loosely related libraries.  \na fast GPU compatible implementation of UMAP (see) \nan implementation of extratrees (see). This is a JVM implementation with no further dependencies. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8822441991861624
      ],
      "excerpt": "On linux, see the following Dockerfile. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9137619370432198,
        0.9882071585809019
      ],
      "excerpt": "On top of those tensors lamp provides autograd for the operations needed to build neural networks. \nThere is substantial test coverage in terms of unit tests and a suite of end to end tests which compares lamp to PyTorch on 50 datasets. All gradient operations and neural network modules are tested for correctness using numeric differentiation, both on CPU and GPU. Nevertheless, advance with caution. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8842006874514572,
        0.954084993917104,
        0.9499261707070127,
        0.9617662338400117
      ],
      "excerpt": "The JNI binding is hosted in the pityka/aten-scala git repository. \nRefer to the readme in that repository on how to build the JNI sources and publish them as a scala library. \nLamp itself is a pure Scala library and builds like any other Scala project.  \nOnce aten-scala is published to a local repository invoking sbt compile will work. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "deep learning and scientific computing library with native CPU and GPU backend for the Scala programming language",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/pityka/lamp/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Wed, 29 Dec 2021 16:47:10 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/pityka/lamp/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "pityka/lamp",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/pityka/lamp/master/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/pityka/lamp/tree/master/docs",
      "https://github.com/pityka/lamp/tree/master/docs/docs"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/pityka/lamp/master/run_timemachine.sh",
      "https://raw.githubusercontent.com/pityka/lamp/master/run_translation.sh",
      "https://raw.githubusercontent.com/pityka/lamp/master/run_cifar.sh",
      "https://raw.githubusercontent.com/pityka/lamp/master/test_cuda.sh",
      "https://raw.githubusercontent.com/pityka/lamp/master/run_arxiv.sh",
      "https://raw.githubusercontent.com/pityka/lamp/master/test_slow.sh",
      "https://raw.githubusercontent.com/pityka/lamp/master/test_custom.sh",
      "https://raw.githubusercontent.com/pityka/lamp/master/run_gan.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8864463604240109,
        0.9777411130091576,
        0.9647572626166336
      ],
      "excerpt": "Lamp depends on the JNI bindings in aten-scala which has cross compiled artifacts for Mac and Linux. Mac has no GPU support. Your system has to have the libtorch 1.9.0 shared libraries in its linker path. \nOn mac it suffices to copy the shared libraries from https://download.pytorch.org/libtorch/cpu/libtorch-macos-1.9.0.zip to e.g. /usr/local/lib/. \nOn linux, see the following Dockerfile. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9191343620993191
      ],
      "excerpt": "First, one has to build the JNI binding to libtorch, then build lamp itself. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/pityka/lamp/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Scala",
      "SCSS",
      "HTML",
      "Python",
      "Shell",
      "JavaScript",
      "Dockerfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other",
      "url": "https://raw.githubusercontent.com/pityka/lamp/master/LICENSE"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) ONNX Project Contributors\\nAll rights reserved.\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Lamp",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "lamp",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "pityka",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/pityka/lamp/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "In addition to the libtorch shared libraries:\n- `lamp-core` depends on [saddle-core](https://github.com/pityka/saddle), [cats-effect](https://github.com/typelevel/cats-effect) and [aten-scala](https://github.com/pityka/aten-scala)\n- `lamp-data` further depends on [scribe](https://github.com/outr/scribe) and [jsoniter-scala](https://github.com/plokhotnyuk/jsoniter-scala)\n\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "`sbt test` will run a short test suite of unit tests.\n\nCuda tests are run separately with `sbt cuda:test`. See `test_cuda.sh` in the source tree about how to run this in a remote docker context. Some additional tests are run from `test_slow.sh`.\n\nAll tests are executed with `sbt alltest:test`. This runs all unit tests, all cuda tests, additional tests marked as slow, and a more extensive end-to-end benchmark against PyTorch itself on 50 datasets.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 10,
      "date": "Wed, 29 Dec 2021 16:47:10 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "scala",
      "libtorch",
      "deep-learning",
      "neural-networks",
      "tensor",
      "umap",
      "extratrees",
      "machine-learning",
      "scala-library",
      "gpu"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Add to build.sbt:\n\n```scala\nlibraryDependencies += \"io.github.pityka\" %% \"lamp-data\" % \"VERSION\" // look at the github page for version\n```\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Examples for various tasks:\n\n- Image classification: `bash run_cifar.sh` runs the code in `example-cifar100/`.\n- Text generation: `bash run_timemachine.sh` runs the code in `example-timemachine/`.\n- Machine translation: `bash run_translation.sh` runs the code in `example-translation/`.\n- Graph node property prediction: `bash run_arxiv.sh` runs the code in `example-arxiv/`.\n\n",
      "technique": "Header extraction"
    }
  ]
}