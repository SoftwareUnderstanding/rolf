{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1706.03762](https://arxiv.org/pdf/1706.03762.pdf)\n\n[3] [Michel Claveau. 2004. \"Traduction nombre => texte\" from mclaveau.com](http://mclaveau.com/ress/python/trad.py)\n\n[4] [E Malmi, P Takala, H Toivonen, T Raiko, A. Gionis. 2016. DopeLearning: A Computational Approach to Rap Lyrics Generation. arXiv preprint https://arxiv.org/abs/1505.04771](https://arxiv.org/pdf/1505.04771.pdf)\n\n[5] [Tomas Mikolov, Edouard Grave, Piotr Bojanowski, Christian Puhrsch, and Armand Joulin.   Advances in pre-training distributed word representations.  InProceedings of the Eleventh International Conference on LanguageResources and Evaluation (LREC-2018), 2018.](https://arxiv.org/pdf/1712.09405.pdf)\n\n[6] [Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. 2003. Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network. In Proceedings of HLT-NAACL 2003, pp. 252-259.](https://nlp.stanford.edu/~manning/papers/tagging.pdf)\n\n[7] [Jason Brownlee. January 5 2018. \"How to Implement a Beam Search Decoder for Natural Language Processing\" from machinelearningmastery.com](https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/)\n\n[8] [Th\u00e9\u00e2tre classique](http://theatre-classique.fr/pages/programmes/PageEdition.php)\n\n## License\n\n\u00a9 R\u00e9mi Desmarescaux, Ryan Ugolini\n\nLicensed under the [MIT License](LICENSE).",
      "https://arxiv.org/abs/1505.04771](https://arxiv.org/pdf/1505.04771.pdf)\n\n[5] [Tomas Mikolov, Edouard Grave, Piotr Bojanowski, Christian Puhrsch, and Armand Joulin.   Advances in pre-training distributed word representations.  InProceedings of the Eleventh International Conference on LanguageResources and Evaluation (LREC-2018), 2018.](https://arxiv.org/pdf/1712.09405.pdf)\n\n[6] [Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. 2003. Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network. In Proceedings of HLT-NAACL 2003, pp. 252-259.](https://nlp.stanford.edu/~manning/papers/tagging.pdf)\n\n[7] [Jason Brownlee. January 5 2018. \"How to Implement a Beam Search Decoder for Natural Language Processing\" from machinelearningmastery.com](https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/)\n\n[8] [Th\u00e9\u00e2tre classique](http://theatre-classique.fr/pages/programmes/PageEdition.php)\n\n## License\n\n\u00a9 R\u00e9mi Desmarescaux, Ryan Ugolini\n\nLicensed under the [MIT License](LICENSE)."
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[1] [New, Boris, Christophe Pallier, Ludovic Ferrand, and Rafael Matos. 2001. \"Une Base de Donn\u00e9es Lexicales Du Fran\u00e7ais Contemporain Sur Internet: LEXIQUE\" L'Ann\u00e9e Psychologique 101 (3): 447-462](https://chrplr.github.io/openlexicon/datasets-info/Lexique382/New%20et%20al.%20-%202001%20-%20Une%20base%20de%20donn%C3%A9es%20lexicales%20du%20fran%C3%A7ais%20contempo.pdf)\n\n[2] [Vaswani, A., et al.: Attention is all you need. arXiv (2017). arXiv:1706.03762](https://arxiv.org/pdf/1706.03762.pdf)\n\n[3] [Michel Claveau. 2004. \"Traduction nombre => texte\" from mclaveau.com](http://mclaveau.com/ress/python/trad.py)\n\n[4] [E Malmi, P Takala, H Toivonen, T Raiko, A. Gionis. 2016. DopeLearning: A Computational Approach to Rap Lyrics Generation. arXiv preprint arXiv:1505.04771](https://arxiv.org/pdf/1505.04771.pdf)\n\n[5] [Tomas Mikolov, Edouard Grave, Piotr Bojanowski, Christian Puhrsch, and Armand Joulin.   Advances in pre-training distributed word representations.  InProceedings of the Eleventh International Conference on LanguageResources and Evaluation (LREC-2018), 2018.](https://arxiv.org/pdf/1712.09405.pdf)\n\n[6] [Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. 2003. Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network. In Proceedings of HLT-NAACL 2003, pp. 252-259.](https://nlp.stanford.edu/~manning/papers/tagging.pdf)\n\n[7] [Jason Brownlee. January 5 2018. \"How to Implement a Beam Search Decoder for Natural Language Processing\" from machinelearningmastery.com](https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/)\n\n[8] [Th\u00e9\u00e2tre classique](http://theatre-classique.fr/pages/programmes/PageEdition.php)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "| Que les parfums l\u00e9gers de ton air embaum\u00e9, | k\u00b0lepaRf1leZed\u00b0t\u00a7nER@bome | 23 | (0.001, ..., 0.03) | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Remiphilius/PoemesProfonds",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-04-06T11:48:54Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-10T22:13:30Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9820205608399798
      ],
      "excerpt": "PoemesProfonds is a project created to write automatically French poetry with neural networks. Two main neural networks are used to do so : a text-to-phoneme converter and a predictor that creates the best sequence of verses from a given single verse or a given sequence of verses. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9693233393948762
      ],
      "excerpt": "Text-to-phenomes converter \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9807124273484472,
        0.9931656697673439
      ],
      "excerpt": "In order to get the phonemes out of the verses from the poetry, a text-to-phonemes converter for French needed to be developped. Some words, especially proper nouns, may appear in poetry but may not in any database. Thus, a model based on neural networks was created so every verse has its phonetic representation. \nThe data to train the model on was found in [1]. Instead of using the International Phonetic Alphabet (IPA), a substitution alphabet is used as in [1]. It is the one described below: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9065132921552418,
        0.9575499162218557,
        0.9596480044667005,
        0.9161139393910281,
        0.9390241601157305,
        0.9502157539109016,
        0.9966010197812288,
        0.9907400250115341
      ],
      "excerpt": "The model takes as input a word up to 25 letters long and returns a phonetic transcription up to 21 phonemes long. \nThe architecture of the model features an attention mechanism [2]. \nThe model has a 99.76% accuracy on words it was not trained on. It seems to be the best (as of August 2020) in French. \nA class Lecteur was developped to read the texts. The algorithm uses the phonemes in the data [1] thanks to a mapping in the dictionnary dico_u. \nHowever, some words can have several pronounciations (i.e. \"est\" can be read /e/ or /&#603;st/). The algorithm uses a dictionnary mapping the word and its part-of-speech (POS) to the phonemes. This mapping is stored in the dictionnary dico_m. The keys are a tuple (word, POS) (i.e. {(\"est\", \"AUX\"): /e/} and {(\"est\", \"NOUN\"): /&#603;st/}). \nTherefore, only words absent of these dictionnaries are read by the model. \nThis project was inspired by [4]. The aim of this project is to get a realistic sequence of verses with a neural network. This model scans a sequence of previous verses to get the most likely verse to continue this sequence. Some verses are candidates to be chosen as the best sequel. The model predicts a score for each candidate. Here, it is the probability of the verse to be the sequence's sequel. \nUnlike in [4], the neural network reckons a verse as a tuple reprensenting its phonemes (got thanks to the text-to-phonemes converter) and its FastText embedding. FastText [5] is a word embedding representation which can derive a unique vector for a sentence. It also considers the punctuation and it is case-sensitive. This allows the model to make more realistic predictions as it considers these elements. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9966474696551649,
        0.9164382356199913,
        0.9032269142314998,
        0.9857087383187852
      ],
      "excerpt": "phonemes: phonemes of the verse \nid: identifying number of the poem from which the verse is from. (Only needed for training) \nvect: FastText representation of the verse. The models are built to use 300-dimension vectors. \nThis is an example of the data: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8372396012058242
      ],
      "excerpt": "| Que tout ce qu'on entend, l'on voit ou l'on respire, | k\u00b0tus2k\u00a7n@t@l\u00a7vwaul\u00a7REspiR | 23 | (0.2, ..., 0.004) | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.94446992504075,
        0.9661554007024954,
        0.8199101460944039,
        0.9097424353749203
      ],
      "excerpt": "In order to have a huge amount of verses, the idea was to get French classical plays. They were got from the website Th\u00e9\u00e2tre classique [8] through XML files format. \nThe aim of the neural network is to compute, for a set of <img src=\"https://render.githubusercontent.com/render/math?math=s\"> verses, the probability that a verse is the real one following this set. \nThese verses are modeled by two different matrixes: \n<img src=\"https://render.githubusercontent.com/render/math?math=P\">: one hot encoding representation of the phonemes of the verses. Its shape is <img src=\"https://render.githubusercontent.com/render/math?math=(s %2B 1, l, n_p %2B 1)\">. <img src=\"https://render.githubusercontent.com/render/math?math=l\"> is the size of the longest string of phonemes among the data. <img src=\"https://render.githubusercontent.com/render/math?math=n_p\"> is the number of characters of the phonemes alphabet (38 with the alphabet used here). One is added to <img src=\"https://render.githubusercontent.com/render/math?math=s\"> as the matrix holds the representations of the <img src=\"https://render.githubusercontent.com/render/math?math=s\"> verses of the set and the one of the verse for which we want to compute the probability. One is also added to <img src=\"https://render.githubusercontent.com/render/math?math=n_p\"> as there is always a symbol for a blank. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8870639282423742,
        0.9002139311404341,
        0.9760228698708939
      ],
      "excerpt": "A LSTM layer is applied on the matrix <img src=\"https://render.githubusercontent.com/render/math?math=P_e\">. It creates an embedding of the set of <img src=\"https://render.githubusercontent.com/render/math?math=s\"> verses for which the model derives the best sequel. This embedding is a <img src=\"https://render.githubusercontent.com/render/math?math=n_s\">-dimension vector. LSTM was chosen over GRU as it has an additionnal cell state vector which can be useful while creating an embedding considering better enclosing rhymes (rimes crois\u00e9es and rimes embrass\u00e9es). Once the set of <img src=\"https://render.githubusercontent.com/render/math?math=s\"> verses is embedded, it has to be concatenated to the embedding of the verse which is a candidate for the sequel. This concatenation returns a <img src=\"https://render.githubusercontent.com/render/math?math=n_s %2B n_e\">-dimension vector which represents the set of <img src=\"https://render.githubusercontent.com/render/math?math=s\"> verses and the candidate verse. \nA fully-connected layer with <img src=\"https://render.githubusercontent.com/render/math?math=n_{d1}\"> units is applied to the output of the previous concatenation. Another fully-connected layer with <img src=\"https://render.githubusercontent.com/render/math?math=n_{d2}\"> units derives the ultimate embedding of the phonemes of the <img src=\"https://render.githubusercontent.com/render/math?math=s\"> verses and the candidate verse. A leaky rectified linear unit with &alpha; = 0.2 is used as the activation function for these fully-connected layers. These layers are regularized with a 10% dropout. A batch normalization is also applied on these fully-connected layers to avoid vanishing or exploding gradients. \nThe only architectures producing realistic results, were the symetric ones. That is to say that for both the phonemes and the FastText representation sides, the output of each layer needs to be of the same size. Thus, each verse's FastText representation needs to be turned into a <img src=\"https://render.githubusercontent.com/render/math?math=n_e\">-dimension vector like the phoneme embedding. Therefore, the matrix <img src=\"https://render.githubusercontent.com/render/math?math=V\"> is turned into a <img src=\"https://render.githubusercontent.com/render/math?math=(s %2B 1, n_e)\"> shaped matrix. A fully-connected layer with <img src=\"https://render.githubusercontent.com/render/math?math=n_e\"> units derives the embedding of the same size as the one of the phonemes. The output of the layer is normalized in order not to have some FastText representations to activate more the next layers' units and thus to be more likely to be picked as a sequel, even though they are not the most realistic ones. Indeed, shorter verses seem to have FastText representations with a norm closer to 1 than the longer verses. Thus, shorter verses were more likely to be picked up by the model. This problem is thus fixed by this procedure. This normalization can be considered as the activation function. A 10% dropout regularization and a batch normalization are applied to the output of this layer. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.927870765628171,
        0.8770882658434693,
        0.9540692584655689,
        0.9505539290029197
      ],
      "excerpt": "The embeddings from both the phonemes and the FastText sides are concatenated. This derives a <img src=\"https://render.githubusercontent.com/render/math?math=2 n_{d2}\">-dimension vector. This vector is input to a fully connected layer with <img src=\"https://render.githubusercontent.com/render/math?math=n_f\"> units. Its activation function is again a leaky rectifier linear unit with &alpha; = 0.2. Again, it is regularized with a 10% dropout. A batch normalization is applied to the output of this layer. Adding more layers here was tried, but it was not as efficient as adding a layer to the phoneme and FastText sides. \nThis <img src=\"https://render.githubusercontent.com/render/math?math=n_f\">-dimension vector feeds the last layer which has a single unit and is fully-connected. Its activation function is the sigmoid. Thus the number is the probability of the candidate verse to be the sequel of the <img src=\"https://render.githubusercontent.com/render/math?math=s\"> verses. \nBellow, the diagram of the architecture generated with netron: \nIn order to generate enclosing rhymes, the poem written will not keep only the most likely verse after having applied the neural network model (greedy search). Similary to machine translation, a beam search algorithm is used here to keep the <img src=\"https://render.githubusercontent.com/render/math?math=k\"> best sequences of verses which were derived by the model. Then, the model uses these <img src=\"https://render.githubusercontent.com/render/math?math=k\"> sequences as the set of <img src=\"https://render.githubusercontent.com/render/math?math=s\"> verses and compute the best sequel for each of them. The code is inspired from [7]. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8045980332233235,
        0.8045953270015981
      ],
      "excerpt": "A third neural network which can generate its own verses can be developped so this algorithm could write poetry all by itself. It can be done with a generative adversarial network or with reinforcement learning. \nThe text-to-phonemes converter can be used in various applications like text-to-speech applications. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Text-to-phoneme converter in French and poetry generator with neural networks",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Remiphilius/PoemesProfonds/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Sun, 26 Dec 2021 08:20:06 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Remiphilius/PoemesProfonds/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Remiphilius/PoemesProfonds",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\nimport preprocessing as pp\nfrom lecture import *\nfrom keras.models import load_model\n\ndico_u, dico_m, df_w2p = pd.read_pickle(os.path.join(\".\", \"data\", \"dicos.pickle\"))\nltr2idx, phon2idx, Tx, Ty = pp.chars2idx(df_w2p)\nmodel_lire = load_model(os.path.join(\".\", \"models\", \"lecteur\", \"CE1_T12_l10.h5\"))\nlecteur = Lecteur(Tx, Ty, ltr2idx, phon2idx, dico_u, dico_m, n_brnn1=90, n_h1=80, net=model_lire, blank=\"_\")\n```\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8190869369466598
      ],
      "excerpt": "Text-to-phenomes converter \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Remiphilius/PoemesProfonds/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 R\\xc3\\xa9mi Desmarescaux, Ryan Ugolini\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "PoemesProfonds",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "PoemesProfonds",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Remiphilius",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Remiphilius/PoemesProfonds/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Python >= 3.5\n- Tensorflow 2\n- Keras >= 2.3.1\n- Java 1.8\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 8,
      "date": "Sun, 26 Dec 2021 08:20:06 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "poetry-generator",
      "poetry-generation",
      "text-to-phoneme",
      "phoneme-conversion",
      "keras",
      "tensorflow2",
      "machine-learning",
      "phonetic",
      "poem-generator",
      "french",
      "natural-language-processing",
      "nlp"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "There are three main methods in the class *Lecteur*.\n\n- Method **lire_nn** returns a dictionnary mapping words to phonemes using only the neural network model.\n\n```python\n>>> lecteur.lire_nn([\"cheval\", \"abistouque\"])\n{'cheval': 'S\u00b0val', 'abistouque': 'abistuk'}\n```\n- Method **lire_mots** uses words' POS and the dictionnaries *dico_u* and *dico_m* besides the model to read words. It returns a list containing the phonetic transcriptions of the words.\n\n```python\n>>> lecteur.lire_mots([\"cheval\", \"abistouque\"])\n['S\u00b0val', 'abistuk']\n```\n\n- Method **lire_vers** features the [French *liaisons*](https://en.wikipedia.org/wiki/Liaison_(French)). The POS is considered while applying the *liaison* or not. For instance, with *les enfants ouvrent* there is no *liaison* between *enfants* (noun) and *ouvrent* (verb). The POS-tagger used is [*StanfordPOSTagger*](https://nlp.stanford.edu/software/tagger.shtml) [6]. Because of it, only a single sentence should be input to the method.\n\nNumbers can also be read thanks to a script broadly inspired by [3].\n\nAs this text-to-phonemes converter was developped to read French poetry, the phoneme /&#601;/ is added when a word ends with a consonant sound followed by a mute *e* (except at the end of a verse). This was added thanks to the functions *e_final* and *e_final_tokens* used in the method *lire_vers*. These /&#601;/ are neither present in the dictionnaries *dico_u* and *dico_m* nor in the model.\n\n```python\n>>> lecteur.lire_vers(\"Les trains arrivent en gare de Jarlitude, voies 14 et 97.\")\n'letR5aRiv\u00b0t@gaR\u00b0d\u00b0ZaRlityd\u00b0vwakatORzekatR\u00b0v5disEt'\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "The user has to write at least one verse by oneself. The model writes a poem from it.\n\n```python\nimport preprocessing as pp\nfrom lecture import *\nfrom chercheur2vers import *\nfrom keras.models import load_model\nimport fasttext.util\n\ndico_u, dico_m, df_w2p = pd.read_pickle(os.path.join(\".\", \"data\", \"dicos.pickle\"))\nltr2idx, phon2idx, Tx, Ty = pp.chars2idx(df_w2p)\nmodel_lire = load_model(os.path.join(\".\", \"models\", \"lecteur\", \"lecteur_mdl.h5\"))\nlecteur = Lecteur(Tx, Ty, ltr2idx, phon2idx, dico_u, dico_m, n_brnn1=90, n_h1=80, net=model_lire, blank=\"_\")\n\nft = fasttext.load_model(os.path.join(\"models\", \"FastText\", 'cc.fr.300.bin'))\n\nvers = pd.read_pickle(os.path.join(\"data\", \"vers.pkl\"))\n_, vers_test = split_train_dev(vers, test_size=0.03)\n\nmodel_chercher = load_model(os.path.join(\".\", \"models\", \"chercher_vers\", \"fast.h5\"))\nchecheur = Chercheur2Vers(t_p=50, p2idx=phon2idx, net=model_chercher, n_antecedant_vers=8)\n\npoem = checheur.beam_search_write([\"Apr\u00e8s cette peine, emprisonn\u00e9 par l'ennui,\",\n                                   \"Je suis mon cher ami tr\u00e8s heureux de te voir.\"],\n                                  df=vers_test, vers_suivants=5, k=5, batch_size=512, split=100\n                                  lecteur=lecteur, ft=ft)\n```\nTwo parameters are important for the quality of the generated poem and the speed of execution:\n\n- **test_size**: amount of the poems in the data to pick the verses from to write the sequel\n- **k**: beam width\n\nIf there is any memory error, it is possible to divide the size of the input matrixes thanks to the parameter **split**. The higher it is, the less memory is required.\n\nIt is also possible to export formated inputs from verses to run the neural networks on a GPU or TPU (like colab) elsewhere:\n\n1. Exporting the inputs\n\n```python\nphoneme_input, fasttext_input = checheur.vers2matrixes([\"Apr\u00e8s cette peine emprisonn\u00e9 par l'ennui,\",\n                                                        \"Je suis mon cher ami tr\u00e8s heureux de te voir.\"],\n                                                        lecteur, ft, len_output=8)\n```\n\n2. Using them\n\n```python\npoem = checheur.beam_search_write(liste_vers=[\"Apr\u00e8s cette peine emprisonn\u00e9 par l'ennui,\",\n                                              \"Je suis mon cher ami tr\u00e8s heureux de te voir.\"],\n                                              df=vers_test, vers_suivants=7, k=3, split=100, batch_size=512,\n                                              mphon_prec=phoneme_input, mvect_prec=fasttext_input)\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "*Harry* : <br/>\nApr\u00e8s cette peine, emprisonn\u00e9 par l'ennui,<br/>\nJe suis mon cher ami tr\u00e8s heureux de te voir.<br/>\nJ'en attends peu de chose, et br\u00fble de le voir.<br/>\nPass\u00e9s aupr\u00e8s de toi, blonde soeur d'Oph\u00e9lie,<br/>\nQue se borne en effet le roman de ma vie.<br/>\nVous aviez d\u00e9sol\u00e9 les pays d'alentour,<br/>\n\u00c9gare votre coeur, vous s\u00e9duit en ce jour.<br/>\n\n*Dauphin* :<br/>\nApr\u00e8s cette peine, emprisonn\u00e9 par l'ennui,<br/>\nJe suis mon cher ami tr\u00e8s heureux de te voir.<br/>\nJ'en attends peu de chose, et br\u00fble de le voir.<br/>\nPass\u00e9s aupr\u00e8s de toi, blonde soeur d'Oph\u00e9lie,<br/>\nQue se borne en effet le roman de ma vie.<br/>\nVous aviez d\u00e9sol\u00e9 les pays d'alentour,<br/>\nQue tous les Citoyens vous rendent tour \u00e0 tour.<br/>\n\n*\u00c9lu* :<br/>\nApr\u00e8s cette peine, emprisonn\u00e9 par l'ennui,<br/>\nJe suis mon cher ami tr\u00e8s heureux de te voir.<br/>\nJ'en attends peu de chose, et br\u00fble de le voir.<br/>\nPass\u00e9s aupr\u00e8s de toi, blonde soeur d'Oph\u00e9lie,<br/>\nQue se borne en effet le roman de ma vie.<br/>\nVous aviez d\u00e9sol\u00e9 les pays d'alentour,<br/>\nQu'un peu de vanit\u00e9 se sent dans vos atours.<br/>\n\n*Harry* is the third most likely, *Dauphin* the second and *\u00c9lu* is the most likely. There are three results displayed thanks to the beam seach (when k = 2, *Harry* is not displayed as it is not computed). These poems was got with k = 5, vers_suivants = 5 and the model fast.\n\n",
      "technique": "Header extraction"
    }
  ]
}