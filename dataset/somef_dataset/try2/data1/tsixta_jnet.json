{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1505.04597",
      "https://arxiv.org/abs/1411.4038",
      "https://arxiv.org/abs/1505.04597](https://arxiv.org/abs/1505.04597).\n\n[2] Long J., Shelhamer E., Trevor Darrell T.: Fully convolutional networks for semantic segmentation.\nIn 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 3431-3440.\nAvailable at [https://arxiv.org/abs/1411.4038](https://arxiv.org/abs/1411.4038).\n\n[3] [http://www.celltrackingchallenge.net](http://www.celltrackingchallenge.net)\n\n",
      "https://arxiv.org/abs/1411.4038](https://arxiv.org/abs/1411.4038).\n\n[3] [http://www.celltrackingchallenge.net](http://www.celltrackingchallenge.net)\n\n"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[1] Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedical image segmentation. \nIn Medical Image Computing and Computer-Assisted Intervention \u2013 MICCAI 2015. pp. 234\u2013241. Springer International Publishing, Cham (2015). \nAvailable at [arXiv:1505.04597](https://arxiv.org/abs/1505.04597).\n\n[2] Long J., Shelhamer E., Trevor Darrell T.: Fully convolutional networks for semantic segmentation.\nIn 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 3431-3440.\nAvailable at [arXiv:1411.4038](https://arxiv.org/abs/1411.4038).\n\n[3] [http://www.celltrackingchallenge.net](http://www.celltrackingchallenge.net)\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8656070203791273
      ],
      "excerpt": "(see https://gist.github.com/chsasank/4d8f68caf01f041a6453e67fb30f8f5a \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/tsixta/jnet",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-07-14T15:16:13Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-09T19:20:46Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8088790981769469
      ],
      "excerpt": "[1]. Since it consists of the expansive path only, it resembles the letter J  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.882328501556182
      ],
      "excerpt": "neural network (CNN) followed either by a deconvolution layer [2], which  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8210432779353586,
        0.9934475737224874,
        0.8505660248006254,
        0.8251031252850894,
        0.854643543525675,
        0.9162681292565104,
        0.9761789057079799,
        0.8214147669553994,
        0.8247300354648958
      ],
      "excerpt": "outputs the segmentation. The input of the first segment is the image  \ndownsampled to the lowest resolution and the input of the other segments is the \n(upsampled) output of the previous segment concatenated with the image  \ndownsampled to the corresponding resolution level. \nThe figure shows an example of a J-net architecture. It consists of three  \nsegments, each being a CNN with 3&times;3 convolution filters and leaky ReLU  \nactivations. In order to maintain the spatial dimensions of the input  \nthroughout the segment the convolutions are preceded with a padding layer,  \nwhich extends the tensor by reflecting the boundary pixels. The convolutions  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8472167145663694
      ],
      "excerpt": "binary segmentation (sigmoid activations) and the other predicts for each pixel \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9082485484962953
      ],
      "excerpt": "Size of the batch. Default value is 1. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8331412989332522
      ],
      "excerpt": "allows to use bigger batch size and makes the training more stable. Default  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.953017875916078,
        0.8622217339867614
      ],
      "excerpt": "Dictionary of lists of three digit ids of the input images from the Cell Tracking  \nChallenge. The keys of the dictionary are the sequence numbers (including the  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9467610705110525
      ],
      "excerpt": "on demand. This saves some time when the training set is small and the images are  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9681130884544634,
        0.9191481808222448
      ],
      "excerpt": "is used for parameter learning, the eval mode is used for calculating the loss and  \naccuracies for input images and the vis mode is used for visualization (similar to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.93057208207515,
        0.9162720690885278,
        0.9478647333963861
      ],
      "excerpt": "List of resolution levels. Every level is a negative integer (or 0), such \nthat the corresponding input is downscaled by factor 2<sup>-lvl</sup>. For  \nexample 0 is the original resolution, for -1 the image is downscaled by factor 2, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8810300982280636,
        0.9305846519464763
      ],
      "excerpt": "mainly for debugging). For example \"[-3,-2,-1]\" means, that the input to the  \ninitial segment is downscaled by factor 8 and the output of the network is half \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8957951953299219
      ],
      "excerpt": "Structure of the network in the format  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8309129226019573,
        0.9921975007407211,
        0.8599364962746294,
        0.8412555754941297
      ],
      "excerpt": "where numof_layers is the number of convolution layers, numof_channels is the number  \nof channels and rf_size is the size of the convolution filters in pixels. \nNumber of channels is additive, i.e., number of channels in the second segment \nis given by numof_channels1+additional_numof_channels2. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "Data augmentation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9326239474232613,
        0.9924726130629027,
        0.9879087053179981
      ],
      "excerpt": "List of parameters for elastic transforms: [[alpha1, sigma1, weight1], [alpha2, sigma2, weight2],...],  \nwhere alpha and sigma are parameters of the elastic transform and weight is the unnormalized probability of this  \ncombination. Parameter alpha is related to the scale of the transform and sigma to its smoothness  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9462626166957867,
        0.9117349225024961,
        0.8285782778761503
      ],
      "excerpt": "for more details). For example [[50, 5, 7], [4, 1, 2]] means, that the image will be with probability \n7/9 distorted with parameters alpha=50, sigma=5, and with probability 2/9 with parameters alpha=4, sigma=1. \nIf either alpha or sigma is smaller than 0, this parameter combination means no distortion is done. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9702474706386232,
        0.9918473395289535
      ],
      "excerpt": "List of parameters for intensity transform of the foreground pixels: \"[shift_lbound, shift_ubound, mult_lbound, mult_ubound]\". \nThe intensity i of every foreground pixel is changed to i+(mi)+sforeground_mean,  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.97634138315645
      ],
      "excerpt": "part of the intensity transform is not used. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9506903665609651,
        0.9132695501886666
      ],
      "excerpt": "the boundary pixels. The advantage of this procedure is that pixels in the image  \ncorners are equally likely to be in the rotated image as the pixels that were  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "J-Net: Multiresolution Neural Network for Semantic Segmentation",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/tsixta/jnet/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Mon, 27 Dec 2021 11:38:09 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/tsixta/jnet/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "tsixta/jnet",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/tsixta/jnet/master/toyexample/train.sh",
      "https://raw.githubusercontent.com/tsixta/jnet/master/toyexample/eval.sh",
      "https://raw.githubusercontent.com/tsixta/jnet/master/toyexample/vis.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8845979122165692
      ],
      "excerpt": "(see https://gist.github.com/chsasank/4d8f68caf01f041a6453e67fb30f8f5a \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9748709027320682,
        0.8995958160653864
      ],
      "excerpt": "GPU related \nUse GPU if available. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8560554971487745
      ],
      "excerpt": "is done on GPU (if available). Default value is 0 (PyTorch default). \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8293567317434903
      ],
      "excerpt": "Main arguments \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8316562388834466
      ],
      "excerpt": "Size of the batch. Default value is 1. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8100119633034264
      ],
      "excerpt": "Truncation threshold for the boundary-distance output. Default value is 9. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8268031776603483,
        0.8473779329112442
      ],
      "excerpt": "leading zero). For example '{\"01\":[\"002\",\"005\",\"021\"]'} means images 002, 005 and 021 \nfrom sequence 01 and '{\"01\":[\"002\",\"005\",\"021\"],\"02\":[\"006\",\"007\"]}' means images \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8191247245047376
      ],
      "excerpt": "replicated. Default value is 0. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8050167024574432
      ],
      "excerpt": "Mode of the script. Possible values are \"train\", \"eval\" and \"vis\". The train mode \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8997967770448587
      ],
      "excerpt": "File name of the model loaded in the eval or vis mode. Default value is ''. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8559176813856197
      ],
      "excerpt": "Number of training epochs. Default value is 5000. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8765697558269363
      ],
      "excerpt": "Output directory, where the script saves the learned models (train mode) and  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8366136256968489
      ],
      "excerpt": "Save model every save_model_frequency-th epoch. -1 means never. Default value is 200. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8370088812761115
      ],
      "excerpt": "validation during training. Default value is 0 (no validation is done). \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/tsixta/jnet/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "J-Net: Multiresolution Neural Network for Semantic Segmentation",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "jnet",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "tsixta",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/tsixta/jnet/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The code is known to work with\n\n* Python 3.5, 3.6\n* numpy 1.13, 1.14\n* PyTorch 0.2.0, 0.3.1\n\nOther versions may work too (and probably will) but they were not tested.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 7,
      "date": "Mon, 27 Dec 2021 11:38:09 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This example demonstrates the J-net on segmentation of images from the \nDIC-C2DH-HeLa from the Cell Tracking Challenge [3]. The structure of the \nnetwork was the same as described above. Since there are only 17 annotated\nimages, data augmentation was used extensively when training the network\n(random flips and elastic transforms). The truncation threshold for the \nboundary-distance was set to 6 pixels. The optimizer was Adam, the initial \nlearning rate was 0.00003 (decreased my multiplicative factor 0.75 if the \ntraining loss did not decrease for 30 epochs) and the training was\nstopped after 1850 epochs. The batch size was 8 and to make the size of the\ntraining set was artificially increased by replicating each training image 16 \ntimes (note that due to random augmentation the network never see the same \nimage multiple times). The loss was sum of the BCE loss of the segmentation\noutput and the MSE loss of the boundary-distance layer.\nThe full command used to start the learning was as follows:\n\n`python3 main.py --cuda --images_idx '{\"01\":[\"002\",\"005\",\"021\",\"031\",\"033\",\"034\",\"039\",\"054\"],\"02\":[\"006\",\"007\",\"014\",\"027\",\"034\",\"038\",\"042\",\"061\",\"067\"]}' --load_dataset_to_ram 1 --num_workers -1 --dataset_len_multiplier 16 --batch_size 8 --resolution_levels \"[-2,-1,0]\" --aug_rotation_flip --aug_elastic_params \"[(50,5,5),(-1,-1,1)]\" --structure \"[[16,64,3],[2,8,3],[2,8,3]]\" --dt_bound ${DT_BOUND} --validation_percentage 0.17 --learning_rate 0.00003 --mode train --dataset_root \"/path/to/DIC-C2DH-HeLa_training\" --output_dir results/DIC-C2DH-HeLa6`\n\nThis command needs about 14 GB of GPU memory and one learning epoch takes about \n30 seconds. Most of the time is spent on elastic transforms since they have to \nbe done on the original resolution.\n\nSegmentation results for all images in the challenge sequences can be obtained by\n\n`python3 main.py --cuda --resolution_levels '[-2,-1,0]' --dt_bound 6 --images_idx '{\"01\":[],\"02\":[]}' --mode vis --dataset_root /path/to/DIC-C2DH-HeLa_test --model_file results/DIC-C2DH-HeLa6/model_best_train_train --output_dir results/DIC-C2DH-HeLa6`\n\nOne image takes about 0.12 seconds. \n\n**Results**  \n\n\nSegmentation results on a training image from the DIC-C2DH-HeLa dataset (image 038 from sequence 02, same as in the U-net paper).\nFrom left to right: original image, segmentation, truncated distance to the cell boundary. \nThe upper row are the images generated by the network and the lower row is the ground truth:\n\n\n![Segmentation results on a training image (image 038 from sequence 02, same as in the U-net paper). From left to right: original image, segmentation, truncated distance to the cell boundary. The upper row are the images generated by the network and the lower row is the ground truth.](images/train.png) \n\n\nSegmentation results on an image from a challenge sequence (image 024 from sequence 01, \nno image from the challenge sequence was used during the training). From left to right: \noriginal image, segmentation, truncated distance to the cell boundary:\n\n![Segmentation results on an image from a challenge sequence (image 024 from sequence 01). From left to right: original image, segmentation, truncated distance to the cell boundary.](images/test.png)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "The previous example needs a GPU and substantial amount of memory. If you do not \nhave proper HW, it is more convenient to experiment with smaller network, less \nimages, etc. Directory `toyexample` contains scripts for training, evaluation\nand visualization and furthermore a subset of the DIC-C2DH-HeLa dataset. The \nnetwork has two segments with three and two layers respectively, 16 and 24 \nchannels, batch size is decreased to 2 and the training finishes after 100 \nepochs. All scripts run on CPU and do not need more than 1.5 GB RAM. \nDo not expect good results though, DIC-C2DH-HeLa is quite difficult dataset.\n\n\n\n",
      "technique": "Header extraction"
    }
  ]
}