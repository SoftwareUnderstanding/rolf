{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1706.03762",
      "https://arxiv.org/abs/1607.06450",
      "https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762) \n\n# WARNING:\n\nThis is an old code. I have an updated version of Transformers over here: https://github.com/JRC1995/Transformers\n\n# Preprocessing Translation Data\n(from Translation_preprocess.py)\n\n### Function for expanding English contractions\n\nsource: https://gist.github.com/nealrs/96342d8231b75cf4bb82\n\n\n```python\nimport numpy as np\nfrom __future__ import division\nimport io\nimport unicodedata\nimport nltk\nfrom nltk import word_tokenize\nimport string\nimport re\nimport random\n\n\n#source: https://gist.github.com/nealrs/96342d8231b75cf4bb82\ncList = {\n  \"ain't\": \"am not\",\n  \"aren't\": \"are not\",\n  \"can't\": \"cannot\",\n  \"can't've\": \"cannot have\",\n  \"'cause\": \"because\",\n  \"could've\": \"could have\",\n  \"couldn't\": \"could not\",\n  \"couldn't've\": \"could not have\",\n  \"didn't\": \"did not\",\n  \"doesn't\": \"does not\",\n  \"don't\": \"do not\",\n  \"hadn't\": \"had not\",\n  \"hadn't've\": \"had not have\",\n  \"hasn't\": \"has not\",\n  \"haven't\": \"have not\",\n  \"he'd\": \"he would\",\n  \"he'd've\": \"he would have\",\n  \"he'll\": \"he will\",\n  \"he'll've\": \"he will have\",\n  \"he's\": \"he is\",\n  \"how'd\": \"how did\",\n  \"how'd'y\": \"how do you\",\n  \"how'll\": \"how will\",\n  \"how's\": \"how is\",\n  \"I'd\": \"I would\",\n  \"I'd've\": \"I would have\",\n  \"I'll\": \"I will\",\n  \"I'll've\": \"I will have\",\n  \"I'm\": \"I am\",\n  \"I've\": \"I have\",\n  \"isn't\": \"is not\",\n  \"it'd\": \"it had\",\n  \"it'd've\": \"it would have\",\n  \"it'll\": \"it will\",\n  \"it'll've\": \"it will have\",\n  \"it's\": \"it is\",\n  \"let's\": \"let us\",\n  \"ma'am\": \"madam\",\n  \"mayn't\": \"may not\",\n  \"might've\": \"might have\",\n  \"mightn't\": \"might not\",\n  \"mightn't've\": \"might not have\",\n  \"must've\": \"must have\",\n  \"mustn't\": \"must not\",\n  \"mustn't've\": \"must not have\",\n  \"needn't\": \"need not\",\n  \"needn't've\": \"need not have\",\n  \"o'clock\": \"of the clock\",\n  \"oughtn't\": \"ought not\",\n  \"oughtn't've\": \"ought not have\",\n  \"shan't\": \"shall not\",\n  \"sha'n't\": \"shall not\",\n  \"shan't've\": \"shall not have\",\n  \"she'd\": \"she would\",\n  \"she'd've\": \"she would have\",\n  \"she'll\": \"she will\",\n  \"she'll've\": \"she will have\",\n  \"she's\": \"she is\",\n  \"should've\": \"should have\",\n  \"shouldn't\": \"should not\",\n  \"shouldn't've\": \"should not have\",\n  \"so've\": \"so have\",\n  \"so's\": \"so is\",\n  \"that'd\": \"that would\",\n  \"that'd've\": \"that would have\",\n  \"that's\": \"that is\",\n  \"there'd\": \"there had\",\n  \"there'd've\": \"there would have\",\n  \"there's\": \"there is\",\n  \"they'd\": \"they would\",\n  \"they'd've\": \"they would have\",\n  \"they'll\": \"they will\",\n  \"they'll've\": \"they will have\",\n  \"they're\": \"they are\",\n  \"they've\": \"they have\",\n  \"to've\": \"to have\",\n  \"wasn't\": \"was not\",\n  \"we'd\": \"we had\",\n  \"we'd've\": \"we would have\",\n  \"we'll\": \"we will\",\n  \"we'll've\": \"we will have\",\n  \"we're\": \"we are\",\n  \"we've\": \"we have\",\n  \"weren't\": \"were not\",\n  \"what'll\": \"what will\",\n  \"what'll've\": \"what will have\",\n  \"what're\": \"what are\",\n  \"what's\": \"what is\",\n  \"what've\": \"what have\",\n  \"when's\": \"when is\",\n  \"when've\": \"when have\",\n  \"where'd\": \"where did\",\n  \"where's\": \"where is\",\n  \"where've\": \"where have\",\n  \"who'll\": \"who will\",\n  \"who'll've\": \"who will have\",\n  \"who's\": \"who is\",\n  \"who've\": \"who have\",\n  \"why's\": \"why is\",\n  \"why've\": \"why have\",\n  \"will've\": \"will have\",\n  \"won't\": \"will not\",\n  \"won't've\": \"will not have\",\n  \"would've\": \"would have\",\n  \"wouldn't\": \"would not\",\n  \"wouldn't've\": \"would not have\",\n  \"y'all\": \"you all\",\n  \"y'alls\": \"you alls\",\n  \"y'all'd\": \"you all would\",\n  \"y'all'd've\": \"you all would have\",\n  \"y'all're\": \"you all are\",\n  \"y'all've\": \"you all have\",\n  \"you'd\": \"you had\",\n  \"you'd've\": \"you would have\",\n  \"you'll\": \"you you will\",\n  \"you'll've\": \"you you will have\",\n  \"you're\": \"you are\",\n  \"you've\": \"you have\"\n"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9537267708315463
      ],
      "excerpt": "source: https://gist.github.com/nealrs/96342d8231b75cf4bb82 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9537267708315463
      ],
      "excerpt": ":source: https://gist.github.com/nealrs/96342d8231b75cf4bb82 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9979572389801111,
        0.9951715937282142,
        0.9654173109432447,
        0.9217379280481715,
        0.913943657476169,
        0.8444342525991423,
        0.9333774530385558
      ],
      "excerpt": "  \"he'd\": \"he would\", \n  \"he'd've\": \"he would have\", \n  \"he'll\": \"he will\", \n  \"he'll've\": \"he will have\", \n  \"he's\": \"he is\", \n  \"how'd\": \"how did\", \n  \"how'd'y\": \"how do you\", \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8944178096468923
      ],
      "excerpt": "  \"I'd\": \"I would\", \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8944178096468923
      ],
      "excerpt": "  \"she'd\": \"she would\", \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8944178096468923
      ],
      "excerpt": "  \"they'd\": \"they would\", \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9576581443887728
      ],
      "excerpt": "  \"would've\": \"would have\", \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.833774087897174,
        0.9305162210093608
      ],
      "excerpt": "  \"you'd\": \"you had\", \n  \"you'd've\": \"you would have\", \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "        if unicodedata.category(c) != 'Mn' \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8190350429114063
      ],
      "excerpt": "PAD represents empty and EOS represents end of sentence. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "    if j==0: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "        if j!=len(vectorized_eng[i])-1: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "    if j==len(vectorized_eng[i])-1: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "        if j!=0: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8847580360151676
      ],
      "excerpt": "    if j&gt;0 and j&lt;len(vectorized_eng[i])-1: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "    if j==0: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "        if j!=len(vectorized_beng[i])-1: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "    if j==len(vectorized_beng[i])-1: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "        if j!=0: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8847580360151676
      ],
      "excerpt": "    if j&gt;0 and j&lt;len(vectorized_beng[i])-1: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9534283659480219
      ],
      "excerpt": "If word = \"am\" and context = [\"I\",\"alright\"], \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8654671031158477
      ],
      "excerpt": "        embd_labels_eng.append(context) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8654671031158477
      ],
      "excerpt": "        embd_labels_beng.append(context) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "                 num_sampled=10,  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8456806903995955
      ],
      "excerpt": "    if step&gt;=n: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "                 num_sampled=10,  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8456806903995955
      ],
      "excerpt": "    if step&gt;=n: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8456806903995955
      ],
      "excerpt": "    if i+batch_size&gt;=len(x): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "            if k1==len(sorted_x[j]): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8456806903995955
      ],
      "excerpt": "            if k2&gt;len(sorted_y[j]): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9869405348645832
      ],
      "excerpt": "Details: https://arxiv.org/pdf/1706.03762.pdf \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8714162992508173
      ],
      "excerpt": "K = Key \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9630811906933039
      ],
      "excerpt": "More details: https://arxiv.org/pdf/1706.03762.pdf \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8109194328925066
      ],
      "excerpt": "#: SUBLAYER 1 (MASKED MULTI HEADED SELF ATTENTION) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9630811906933039
      ],
      "excerpt": "More details: https://arxiv.org/pdf/1706.03762.pdf \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8109194328925066
      ],
      "excerpt": "#: SUBLAYER 1 (MASKED MULTI HEADED SELF ATTENTION) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8353898032420385
      ],
      "excerpt": "Details about the model: https://arxiv.org/pdf/1706.03762.pdf \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "            if output_batch[i,j]==pad_index: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "        if i%display_step==0: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "                print(vec2word_eng(vec),end=\" \") \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "        if rand==1: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "        if i%display_step==0: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "                    print(vocab_beng[prediction_int],end='') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "                    print(\" \"+vocab_beng[prediction_int],end='') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8456806903995955
      ],
      "excerpt": "        if(loss&lt;best_loss): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "Epoch: 129 Iteration: 10 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "Epoch: 129 Iteration: 12 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9278824608274014
      ],
      "excerpt": "RESTORATION COMPLETE \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/JRC1995/Machine-Translation-Transformers",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-11-12T03:34:05Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-17T10:26:30Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9945796062363349
      ],
      "excerpt": "The model is based on: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8446078536897716
      ],
      "excerpt": "This is an old code. I have an updated version of Transformers over here: https://github.com/JRC1995/Transformers \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9891785691729262
      ],
      "excerpt": "  \"how's\": \"how is\", \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9294906031627522
      ],
      "excerpt": "  \"isn't\": \"is not\", \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9234422227091826
      ],
      "excerpt": "  \"it's\": \"it is\", \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9966474696551649
      ],
      "excerpt": "  \"o'clock\": \"of the clock\", \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9891785691729262
      ],
      "excerpt": "  \"she's\": \"she is\", \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.945686167384973
      ],
      "excerpt": "  \"that's\": \"that is\", \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8143851061486383
      ],
      "excerpt": "  \"there's\": \"there is\", \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8793275092527069
      ],
      "excerpt": "  \"to've\": \"to have\", \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9253323886103801
      ],
      "excerpt": "  \"when's\": \"when is\", \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9891785691729262
      ],
      "excerpt": "  \"who's\": \"who is\", \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9758246017092218,
        0.8030115413318057
      ],
      "excerpt": "Splitting the data into eng and beng. \neng will contain the list of English lines, and beng will contain the corresponding list of Bengali lines. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "        c for c in unicodedata.normalize('NFD', s) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8844068521458947
      ],
      "excerpt": "The index of vocabulary will represent the numerical representation of the word which is stored at that index.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8104544510237184
      ],
      "excerpt": ":The index of vocab will serve as an integer representation of the word \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "for i in xrange(len(eng)): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "for word in eng[i]: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "for word in beng[i]: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9318834749517372,
        0.9084855604138458,
        0.8287050936261598
      ],
      "excerpt": "Later lots of pads may be applied after the end of sentence to fit sequence length. \nSo I also added the word PAD with context words being PADs, and PAD and EOS for embedding. \n(Doing what I wrote directly above, was actually unnecessary but I already did it. We don't need to consider these cases. With masking I will ignore the effect of PADs on the cost, anyway, and the model doesn't need to predict pads correctly. Predicting the EOS properly will be enough. So PAD embedding doesn't need to be taken so seriously.) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076,
        0.9560187895509076
      ],
      "excerpt": "for i in xrange(len(vectorized_eng)): \nfor j in xrange(0,len(vectorized_eng[i])): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "for j in xrange(0,len(vectorized_beng[i])): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.908925214220865
      ],
      "excerpt": "and  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "for i in xrange(len(contexts_eng)): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "for i in xrange(len(contexts_beng)): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9800365338067607,
        0.8636232919859399
      ],
      "excerpt": "for details of word2vec and code description.  \nMost of the word2vec code used here are from the Tensorflow tutorial.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8490037945672047
      ],
      "excerpt": ": Initializing the variables \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8108535585266006
      ],
      "excerpt": ": Compute the NCE loss, using a sample of the negative labels each time. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8790448652594542
      ],
      "excerpt": "                 num_classes=vocabulary_size_eng)) #:num_sampled = no. of negative samples \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9319708861235405
      ],
      "excerpt": "with tf.Session() as sess: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8490037945672047
      ],
      "excerpt": ": Initializing the variables \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8108535585266006
      ],
      "excerpt": ": Compute the NCE loss, using a sample of the negative labels each time. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8790448652594542
      ],
      "excerpt": "                 num_classes=vocabulary_size_beng)) #:num_sampled = no. of negative samples \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9319708861235405
      ],
      "excerpt": "with tf.Session() as sess: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8612507612005332,
        0.8934437175487189,
        0.9748347621340957
      ],
      "excerpt": "Mini-batch training requires all lines in a batch to be of equal length. \nWe have different lines of different lengths.  \nA solution is to fill shorter sentences with PADs so that length of all sentences become equal. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9752675108862559,
        0.8616105308478602
      ],
      "excerpt": "The solution to that is bucketing. First the sentences in the total list are sorted. After that sentences of similar lengths will be closer to each other. Batches are then formed with sentences of similar lengths. Much less padding will be required to turn sentences of similar lengths into sentences of equal lengths.  \nAlso while creating the batch, the input samples (the Engish lines) will have their words embedded using the recently trained embedding matrix for English. The output samples (the labels) will simply contain the index of the target Bengali word in the Bengali vocabulary list. The labels being in this format will be easier to train with sparse_softmax_cross_entropy cost function of Tensorflow.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "for i in xrange(len(x)): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "for i in xrange(len(x)): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "    for j in xrange(i,i+batch_size): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "    for j in xrange(i,i+batch_size): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.895608950120991
      ],
      "excerpt": "with open('translationPICKLE', 'wb') as fp: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.895608950120991
      ],
      "excerpt": "with open ('translationPICKLE', 'rb') as fp: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8790448652594542
      ],
      "excerpt": "h=8 #:no. of heads \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8436939121540402
      ],
      "excerpt": ":Parameters for attention sub-layers for all n encoders \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8436939121540402
      ],
      "excerpt": ":Parameters for 2 attention sub-layers for all n decoders \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076,
        0.9560187895509076
      ],
      "excerpt": "    for pos in xrange(0,seq_len): \n        for i in xrange(0,model_dimensions): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8246810759623834,
        0.8975803730869162,
        0.9151701844587377
      ],
      "excerpt": "These masks are to be used to fill illegal positions with -infinity (or a very low value eg. -2^30). \nIllegal positions are positions of the decoder input tokens that aren't predicted at a given timestep. \n{ In a transformer, the decoder input is of the same shape as the WHOLE decoder output sequence. One word for the sequence is predicted at each timestep (from left to right). So in most timesteps, the left side of the decoder input sequence will contain valid previously predicted output words, but the right side -the yet to be predicted side should contain some values that should be ignored and never attended. We make sure that they're ignored by masking it } \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9362019351784698
      ],
      "excerpt": "The masks are used to assign the value -2^30 to all positions in the tensor influenced by the illegal ones. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.87469858221377
      ],
      "excerpt": "Dynamically creating masks depending on the current position\\timestep (depending on which the program can know which positions are legal and which aren't) is, however, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8350302359724742
      ],
      "excerpt": "I will be pre-generating all the masks with Python native code and feed the list of all required masks to the network at each training step (output length can be different at different training steps).  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "for i in xrange(1,out_len): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9680657473229143
      ],
      "excerpt": "d is the dimension for Q, K and V.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "for i in xrange(0,h): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "for i in xrange(1,h): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "for i in xrange(0,layer_num): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "for j in xrange(0,layer_num): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8820486585752553,
        0.8719340870036753
      ],
      "excerpt": "Given a probability distribution and an embedding matrix, this function returns the embedding of the word with the maximum probability in the given distribution. \nSOS signifies the start of sentence for the decoder. Also often represented as 'GO'. I am using an all ones vector as the first decoder input token.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8072156834656299
      ],
      "excerpt": "It follows the encoder-decoder paradigm. The main exception from standard encoder-decoder paradigm, is that it uses 'transformers' instead of Reccurrent networks. The decoder undergoes a sequential processing, though.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8240816418480229
      ],
      "excerpt": "#: filled value is used to retrieve appropriate mask for illegal positions. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8916646711895847
      ],
      "excerpt": "    #: summation over all the word_vec_dim dimensional vectors in the sequence to transform dimensions \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.831480420459527
      ],
      "excerpt": "    #: for the next iteration. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9390060319454073,
        0.969819075286202
      ],
      "excerpt": "The mask will have the same shape as the batch of labels but with the value 0 wherever there is a PAD. \nThe mask will be element-wise multipled to the cost (before its averaged), so that any position in the cost tensor that is effected by the PAD will be multiplied by 0. This way, the effect of PADs (which we don't need to care about) on the cost (and therefore on the gradients) can be nullified.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8469129675572135,
        0.8469129675572135
      ],
      "excerpt": "    for i in xrange(len(mask)): \n        for j in xrange(len(mask[i])): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.908802251028482
      ],
      "excerpt": "The input batch is positionally encoded before its fed to the network. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8643122958031353
      ],
      "excerpt": "#: Prepares variable for saving the model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "    for i in xrange(0,batch_len): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "            for vec in train_batch_x[shuffled_indices[i]][sample_no]: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9863562797130322
      ],
      "excerpt": "        rand = random.randint(0,2) #:determines chance of using Teacher Forcing \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9394449182630016
      ],
      "excerpt": "                                         #: feed random bool for randomized teacher forcing.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9319708861235405
      ],
      "excerpt": "with tf.Session() as sess: #: Begin session \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9803414538289263,
        0.9537668385057334,
        0.8901431986026075
      ],
      "excerpt": "The model seems to fit well on the training data even using only 1 layer of encoder and decoder. \nIn fact, it seems to be fitting better when I am training with 1 layer of encoder and decoder. \nHowever, I suppose the model is most likely 'memorizing' and overfitting. I tried some predictions below, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9696123194845622,
        0.8186887770366839
      ],
      "excerpt": "Also the model may not really learn to generalize very well, given that there are only 4378 data samples. \nMost deep learning model will probably overfit on this.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8907525025097308
      ],
      "excerpt": "A linear layer may be usable to achieve this goal too, but a summation seems to work - at least in fitting the data. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Machine Translation using Transfromers",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/JRC1995/Machine-Translation-Transformers/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 5,
      "date": "Fri, 24 Dec 2021 16:40:01 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/JRC1995/Machine-Translation-Transformers/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "JRC1995/Machine-Translation-Transformers",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/JRC1995/Machine-Translation-Transformers/master/Machine%20Translation.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\n#: Construct Model\noutput = model(x,y,teacher_forcing)\n\n#:OPTIMIZER\n\ncost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=output, labels=y)\ncost = tf.multiply(cost,tf_pad_mask) #:mask used to remove loss effect due to PADS\ncost = tf.reduce_mean(cost)\n\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,beta1=0.9,beta2=0.98,epsilon=1e-9).minimize(cost)\n\n#:wanna add some temperature?\n\n\"\"\"temperature = 0.7\nscaled_output = tf.log(output)/temperature\nsoftmax_output = tf.nn.softmax(scaled_output)\"\"\"\n\n#:(^Use it with \"#:prediction_int = np.random.choice(range(vocab_len), p=array.ravel())\")\n\nsoftmax_output = tf.nn.softmax(output)\n\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Randomly shuffling the complete dataset (not yet embedded with word2vec embeddings which was learned just now), \nand then splitting it into train, validation and test set\n\n\n```python\nshuffled_indices = np.arange(len(eng))\nnp.random.shuffle(shuffled_indices)\n\nshuffled_vectorized_eng = []\nshuffled_vectorized_beng = []\n\nfor i in xrange(len(eng)):\n    shuffled_vectorized_eng.append(vectorized_eng[shuffled_indices[i]])\n    shuffled_vectorized_beng.append(vectorized_beng[shuffled_indices[i]])\n\ntrain_len = int(.75*len(eng))\nval_len = int(.15*len(eng))\n\ntrain_eng = shuffled_vectorized_eng[0:train_len]\ntrain_beng = shuffled_vectorized_beng[0:train_len]\n\nval_eng = shuffled_vectorized_eng[train_len:val_len]\nval_beng = shuffled_vectorized_beng[train_len:val_len]\n\ntest_eng = shuffled_vectorized_eng[train_len+val_len:]\ntest_beng = shuffled_vectorized_beng[train_len+val_len:]\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\nimport tensorflow as tf\nimport math\n\n#:https://www.tensorflow.org/tutorials/word2vec\nembedding_size = 256\nvocabulary_size_eng = len(vocab_eng)\nvocabulary_size_beng = len(vocab_beng)\n\n#: Placeholders for inputs\ntrain_inputs = tf.placeholder(tf.int32, shape=[batch_size])\ntrain_labels = tf.placeholder(tf.int32, shape=[batch_size,1])\n\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\nbatch_size = 128\n\ndef generate_batch(inputs,labels,batch_size):\n    rand = random.sample((np.arange(len(inputs))),batch_size)\n    batch_inputs=[]\n    batch_labels=[]\n    for i in xrange(batch_size):\n        batch_inputs.append(inputs[int(rand[i])])\n        batch_labels.append(labels[int(rand[i])])\n    batch_inputs = np.asarray(batch_inputs,np.int32)\n    batch_labels = np.asarray(batch_labels,np.int32)\n    return batch_inputs,batch_labels\n    \n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9351984389199732
      ],
      "excerpt": "source: https://gist.github.com/nealrs/96342d8231b75cf4bb82 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9351984389199732
      ],
      "excerpt": ":source: https://gist.github.com/nealrs/96342d8231b75cf4bb82 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9088063058459555
      ],
      "excerpt": "  \"y'alls\": \"you alls\", \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8658170669534125
      ],
      "excerpt": "  \"you'd\": \"you had\", \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8372225046083971,
        0.8380249253110347,
        0.8667954789731419,
        0.8779344856148122
      ],
      "excerpt": "  \"you'll\": \"you you will\", \n  \"you'll've\": \"you you will have\", \n  \"you're\": \"you are\", \n  \"you've\": \"you have\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8548830877787832
      ],
      "excerpt": "test = ['who','are','you'] #: Enter tokenized text here \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8895199484851994
      ],
      "excerpt": "(from Translation_preprocess.py) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9457175861910134
      ],
      "excerpt": "import numpy as np \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8596521807794544,
        0.9133368656218674,
        0.9133368656218674,
        0.8801854956928516,
        0.863830493850468
      ],
      "excerpt": "import io \nimport unicodedata \nimport nltk \nfrom nltk import word_tokenize \nimport string \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8722253495125046
      ],
      "excerpt": "import random \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8550335416452869
      ],
      "excerpt": "filename = 'ben.txt' \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9048509741854245
      ],
      "excerpt": "    file = io.open(filename,'r') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.864344793584503,
        0.8440293382790072
      ],
      "excerpt": "    for line in file.readlines(): \n        lang_pair = line.split('\\t') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8575435024620189,
        0.8730620083729311,
        0.9414583265472072
      ],
      "excerpt": "eng,beng = loaddata(filename) \nsample = random.randint(0,len(eng)) \nprint \"Example Sample #:\"+str(sample)+\":\\n\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8362875859751352,
        0.8004330343179692,
        0.8899092210513045
      ],
      "excerpt": "for i in xrange(0,len(eng[sample])): \n    string+=\" \"+eng[sample][i] \nprint string \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8362875859751352,
        0.8004330343179692,
        0.8899092210513045
      ],
      "excerpt": "for i in xrange(0,len(beng[sample])): \n    string+=\" \"+beng[sample][i] \nprint string \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.89757866040602
      ],
      "excerpt": "Example Sample #646: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9457175861910134
      ],
      "excerpt": "import numpy as np \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8172342705654815
      ],
      "excerpt": "    context1 = vectorized_eng[i][len(vectorized_eng[i])-1] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8172342705654815
      ],
      "excerpt": "    context1 = vectorized_beng[i][len(vectorized_beng[i])-1] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8289669050403863
      ],
      "excerpt": "output = \"I\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9095848236805116,
        0.9095848236805116
      ],
      "excerpt": "embd_inputs_eng = np.asarray(embd_inputs_eng,np.int32) \nembd_labels_eng = np.asarray(embd_labels_eng,np.int32) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9095848236805116,
        0.9095848236805116
      ],
      "excerpt": "embd_inputs_beng = np.asarray(embd_inputs_beng,np.int32) \nembd_labels_beng = np.asarray(embd_labels_beng,np.int32) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123763140827432
      ],
      "excerpt": "    tf.random_uniform([vocabulary_size_eng, embedding_size], -1.0, 1.0)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123763140827432
      ],
      "excerpt": "  tf.truncated_normal([vocabulary_size_eng, embedding_size], \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123763140827432
      ],
      "excerpt": "embed_eng = tf.nn.embedding_lookup(embeddings_eng, train_inputs) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8446434232000626
      ],
      "excerpt": "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0).minimize(loss) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8135654125968134
      ],
      "excerpt": "with tf.Session() as sess: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8369238415880416
      ],
      "excerpt": "    training_iters = 500*(int((len(embd_inputs_eng))/batch_size)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.906421761377065
      ],
      "excerpt": "    last_n_losses = np.zeros((n),np.float32) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8192175632204302
      ],
      "excerpt": "    print \"Iter \"+str(step)+\", loss = \"+str(cur_loss) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.815752593700219
      ],
      "excerpt": "print \"\\nOptimization Finished\\n\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123763140827432
      ],
      "excerpt": "    tf.random_uniform([vocabulary_size_beng, embedding_size], -1.0, 1.0)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123763140827432
      ],
      "excerpt": "  tf.truncated_normal([vocabulary_size_beng, embedding_size], \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123763140827432
      ],
      "excerpt": "embed_beng = tf.nn.embedding_lookup(embeddings_beng, train_inputs) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8446434232000626
      ],
      "excerpt": "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0).minimize(loss) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8135654125968134
      ],
      "excerpt": "with tf.Session() as sess: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8369238415880416
      ],
      "excerpt": "    training_iters = 500*(int((len(embd_inputs_beng))/batch_size)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.906421761377065
      ],
      "excerpt": "    last_n_losses = np.zeros((n),np.float32) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8192175632204302
      ],
      "excerpt": "    print \"Iter \"+str(step)+\", loss = \"+str(cur_loss) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.815752593700219
      ],
      "excerpt": "print \"\\nOptimization Finished\\n\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9306194445993059
      ],
      "excerpt": "len_x= np.zeros((len(x)),np.int32) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8172342705654815,
        0.8920832651232377
      ],
      "excerpt": "    len_x[i] = len(x[i]) \nsorted_by_len_indices = np.flip(np.argsort(len_x),0) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8172342705654815,
        0.9306194445993059
      ],
      "excerpt": "    max_len_x = len(sorted_x[i]) \n    len_y= np.zeros((len(y)),np.int32) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8172342705654815,
        0.8997243352845468
      ],
      "excerpt": "        len_y[j] = len(sorted_y[j]) \n    max_len_y = np.amax(len_y) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8440293382790072
      ],
      "excerpt": "        line=[] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8440293382790072
      ],
      "excerpt": "        line=[] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8172342705654815
      ],
      "excerpt": "            elif k2==len(sorted_y[j]): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9095848236805116,
        0.9095848236805116
      ],
      "excerpt": "    batch_x = np.asarray(batch_x,np.float32) \n    batch_y = np.asarray(batch_y,np.int32) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8214055523315815
      ],
      "excerpt": "import pickle \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8214055523315815,
        0.9133368656218674,
        0.9457175861910134
      ],
      "excerpt": "import pickle \nimport math \nimport numpy as np \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8172342705654815
      ],
      "excerpt": "vocab_len = len(vocab_beng) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9095848236805116,
        0.9095848236805116
      ],
      "excerpt": "np_embedding_eng = np.asarray(np_embedding_eng,np.float32) \nnp_embedding_beng = np.asarray(np_embedding_beng,np.float32) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.925671696398174
      ],
      "excerpt": "import tensorflow as tf \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8128131768734507
      ],
      "excerpt": "keep_prob = tf.placeholder(tf.float32) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8188774533332797,
        0.8188774533332797,
        0.8128131768734507,
        0.8128131768734507,
        0.8188774533332797,
        0.8022035983138374,
        0.8168640499537292
      ],
      "excerpt": "x = tf.placeholder(tf.float32, [None,None,word_vec_dim]) \ny = tf.placeholder(tf.int32, [None,None]) \noutput_len = tf.placeholder(tf.int32) \nteacher_forcing = tf.placeholder(tf.bool) \ntf_pad_mask = tf.placeholder(tf.float32,[None,None]) \ntf_illegal_position_masks = tf.placeholder(tf.float32,[None,None,None]) \ntf_pe_out = tf.placeholder(tf.float32,[None,None,None]) #:positional codes for output \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8682740923588441,
        0.8682740923588441,
        0.8682740923588441,
        0.8682740923588441
      ],
      "excerpt": "Wq_enc = tf.Variable(tf.truncated_normal(shape=[N,h,word_vec_dim,dqkv],stddev=0.01)) \nWk_enc = tf.Variable(tf.truncated_normal(shape=[N,h,word_vec_dim,dqkv],stddev=0.01)) \nWv_enc = tf.Variable(tf.truncated_normal(shape=[N,h,word_vec_dim,dqkv],stddev=0.01)) \nWo_enc = tf.Variable(tf.truncated_normal(shape=[N,h*dqkv,word_vec_dim],stddev=0.01)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8682740923588441,
        0.8105470765664503,
        0.8682740923588441,
        0.8105470765664503
      ],
      "excerpt": "W1_enc = tf.Variable(tf.truncated_normal(shape=[N,1,1,word_vec_dim,d],stddev=0.01)) \nb1_enc = tf.Variable(tf.constant(0,tf.float32,shape=[N,d])) \nW2_enc = tf.Variable(tf.truncated_normal(shape=[N,1,1,d,word_vec_dim],stddev=0.01)) \nb2_enc = tf.Variable(tf.constant(0,tf.float32,shape=[N,word_vec_dim])) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8682740923588441,
        0.8682740923588441,
        0.8682740923588441,
        0.8682740923588441,
        0.8682740923588441,
        0.8682740923588441,
        0.8682740923588441,
        0.8682740923588441
      ],
      "excerpt": "Wq_dec_1 = tf.Variable(tf.truncated_normal(shape=[N,h,word_vec_dim,dqkv],stddev=0.01)) \nWk_dec_1 = tf.Variable(tf.truncated_normal(shape=[N,h,word_vec_dim,dqkv],stddev=0.01)) \nWv_dec_1 = tf.Variable(tf.truncated_normal(shape=[N,h,word_vec_dim,dqkv],stddev=0.01)) \nWo_dec_1 = tf.Variable(tf.truncated_normal(shape=[N,hdqkv,word_vec_dim],stddev=0.01)) \nWq_dec_2 = tf.Variable(tf.truncated_normal(shape=[N,h,word_vec_dim,dqkv],stddev=0.01)) \nWk_dec_2 = tf.Variable(tf.truncated_normal(shape=[N,h,word_vec_dim,dqkv],stddev=0.01)) \nWv_dec_2 = tf.Variable(tf.truncated_normal(shape=[N,h,word_vec_dim,dqkv],stddev=0.01)) \nWo_dec_2 = tf.Variable(tf.truncated_normal(shape=[N,hdqkv,word_vec_dim],stddev=0.01)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8682740923588441,
        0.8105470765664503,
        0.8682740923588441,
        0.8105470765664503
      ],
      "excerpt": "W1_dec = tf.Variable(tf.truncated_normal(shape=[N,1,1,word_vec_dim,d],stddev=0.01)) \nb1_dec = tf.Variable(tf.constant(0,tf.float32,shape=[N,d])) \nW2_dec = tf.Variable(tf.truncated_normal(shape=[N,1,1,d,word_vec_dim],stddev=0.01)) \nb2_dec = tf.Variable(tf.constant(0,tf.float32,shape=[N,word_vec_dim])) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8150074486009266
      ],
      "excerpt": "shift_enc_1 = tf.Variable(tf.zeros([N,1,1,word_vec_dim]),dtype=tf.float32) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8150074486009266
      ],
      "excerpt": "shift_enc_2 = tf.Variable(tf.zeros([N,1,1,word_vec_dim]),dtype=tf.float32) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8150074486009266
      ],
      "excerpt": "shift_dec_1 = tf.Variable(tf.zeros([N,1,1,word_vec_dim]),dtype=tf.float32) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8150074486009266
      ],
      "excerpt": "shift_dec_2 = tf.Variable(tf.zeros([N,1,1,word_vec_dim]),dtype=tf.float32) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8150074486009266
      ],
      "excerpt": "shift_dec_3 = tf.Variable(tf.zeros([N,1,1,word_vec_dim]),dtype=tf.float32) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.906421761377065
      ],
      "excerpt": "    pe = np.zeros((seq_len,model_dimensions,),np.float32) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8932801566009344,
        0.867528081255321
      ],
      "excerpt": "mean, var = tf.nn.moments(inputs, [1,2], keep_dims=True) \nLN = tf.multiply((scale / tf.sqrt(var + epsilon)),(inputs - mean)) + shift \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8049565196810243,
        0.8623073846125879
      ],
      "excerpt": "So, the illegal positions depends on the total output length and the no. of predicted output tokens. \nThe appropriate mask when i output tokens are predicted can be retrieved from mask[i-1] where mask is the return value from this function. The argument out_len that function takes, signifies the total length of the output.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9198044800134126
      ],
      "excerpt": "    mask = np.zeros((out_len,out_len),dtype=np.float32) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8731562459058029
      ],
      "excerpt": "V = Value \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123763140827432,
        0.8123763140827432,
        0.8308103703211265,
        0.8217229926984767
      ],
      "excerpt": "K = tf.transpose(K,[0,2,1]) \nd = tf.cast(d,tf.float32) \nsoftmax_component = tf.div(tf.matmul(Q,K),tf.sqrt(d)) \nif mask == True: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8510471952245053
      ],
      "excerpt": "result = tf.matmul(tf.nn.dropout(tf.nn.softmax(softmax_component),keep_prob),V) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8511698700032063,
        0.8511698700032063,
        0.8511698700032063,
        0.8530158888138687
      ],
      "excerpt": "Q_ = tf.reshape(Q,[-1,tf.shape(Q)[2]]) \nK_ = tf.reshape(K,[-1,tf.shape(Q)[2]]) \nV_ = tf.reshape(V,[-1,tf.shape(Q)[2]]) \nheads = tf.TensorArray(size=h,dtype=tf.float32) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123763140827432,
        0.8605899444349602,
        0.8123763140827432,
        0.8605899444349602,
        0.8123763140827432,
        0.8605899444349602
      ],
      "excerpt": "   Q_w = tf.matmul(Q_,Wq[i]) \n   Q_w = tf.reshape(Q_w,[tf.shape(Q)[0],tf.shape(Q)[1],d]) \n   K_w = tf.matmul(K_,Wk[i]) \n   K_w = tf.reshape(K_w,[tf.shape(K)[0],tf.shape(K)[1],d]) \n   V_w = tf.matmul(V_,Wv[i]) \n   V_w = tf.reshape(V_w,[tf.shape(V)[0],tf.shape(V)[1],d]) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123763140827432
      ],
      "excerpt": "    concated = tf.concat([concated,heads[i]],2) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8099463784856867,
        0.8639868044634743
      ],
      "excerpt": "out = tf.matmul(concated,Wo) \nout = tf.reshape(out,[tf.shape(heads)[1],tf.shape(heads)[2],word_vec_dim]) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123763140827432
      ],
      "excerpt": "sublayer1 = tf.nn.dropout(sublayer1,keep_prob) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8605899444349602
      ],
      "excerpt": "sublayer1_ = tf.reshape(sublayer1,[tf.shape(sublayer1)[0],1,tf.shape(sublayer1)[1],word_vec_dim]) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8170396574231907,
        0.8123763140827432,
        0.8123763140827432,
        0.8170396574231907,
        0.8123763140827432,
        0.8605899444349602,
        0.8123763140827432
      ],
      "excerpt": "sublayer2 = tf.nn.conv2d(sublayer1_, W1, strides=[1,1,1,1], padding='SAME') \nsublayer2 = tf.nn.bias_add(sublayer2,b1) \nsublayer2 = tf.nn.relu(sublayer2) \nsublayer2 = tf.nn.conv2d(sublayer2, W2, strides=[1,1,1,1], padding='SAME') \nsublayer2 = tf.nn.bias_add(sublayer2,b2) \nsublayer2 = tf.reshape(sublayer2,[tf.shape(sublayer2)[0],tf.shape(sublayer2)[2],word_vec_dim]) \nsublayer2 = tf.nn.dropout(sublayer2,keep_prob) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123763140827432
      ],
      "excerpt": "sublayer1 = tf.nn.dropout(sublayer1,keep_prob) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123763140827432
      ],
      "excerpt": "sublayer2 = tf.nn.dropout(sublayer2,keep_prob) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8605899444349602,
        0.8170396574231907,
        0.8123763140827432,
        0.8123763140827432,
        0.8170396574231907,
        0.8123763140827432,
        0.8605899444349602,
        0.8123763140827432
      ],
      "excerpt": "sublayer2_ = tf.reshape(sublayer2,[tf.shape(sublayer2)[0],1,tf.shape(sublayer2)[1],word_vec_dim]) \nsublayer3 = tf.nn.conv2d(sublayer2_, W1, strides=[1,1,1,1], padding='SAME') \nsublayer3 = tf.nn.bias_add(sublayer3,b1) \nsublayer3 = tf.nn.relu(sublayer3) \nsublayer3 = tf.nn.conv2d(sublayer3, W2, strides=[1,1,1,1], padding='SAME') \nsublayer3 = tf.nn.bias_add(sublayer3,b2) \nsublayer3 = tf.reshape(sublayer3,[tf.shape(sublayer3)[0],tf.shape(sublayer3)[2],word_vec_dim]) \nsublayer3 = tf.nn.dropout(sublayer3,keep_prob) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8085013333509747
      ],
      "excerpt": "This function will concatenate the last predicted output into a tensor of concatenated sequence of output tokens.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8507729892858653
      ],
      "excerpt": "    return tf.gather(tf_embd,out_index) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8881765785744405
      ],
      "excerpt": "    return output,tf.constant(1),tf.reshape(out_prob_dist,[tf.shape(x)[0],1,vocab_len]) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8830191258262278
      ],
      "excerpt": "    decoderin_part_1 = tf.concat([decoderin_part_1,output],1) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8479761483086884
      ],
      "excerpt": "    out_probs = tf.concat([out_probs,tf.reshape(out_prob_dist,[tf.shape(x)[0],1,vocab_len])],1) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8297589695877159,
        0.8565797490471122
      ],
      "excerpt": "def model(x,y,teacher_forcing=True): \n#: NOTE: tf.shape(x)[0] == batch_size \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123763140827432
      ],
      "excerpt": "encoderin = tf.nn.dropout(encoderin,keep_prob) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8331239779015759
      ],
      "excerpt": "decoderin_part_1 = tf.ones([tf.shape(x)[0],1,word_vec_dim],dtype=tf.float32) #:represents SOS \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123763140827432,
        0.8123763140827432
      ],
      "excerpt": "tf_embd = tf.convert_to_tensor(np_embedding_beng) \nWpd = tf.transpose(tf_embd) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8621328389932146
      ],
      "excerpt": "out_probs = tf.zeros([tf.shape(x)[0],output_len,vocab_len],tf.float32) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8598968491824687,
        0.8123763140827432,
        0.8123763140827432
      ],
      "excerpt": "    decoderin_part_2 = tf.zeros([tf.shape(x)[0],(output_len-filled),word_vec_dim],dtype=tf.float32) \n    decoderin = tf.concat([decoderin_part_1,decoderin_part_2],1) \n    decoderin = tf.nn.dropout(decoderin,keep_prob) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8509415152289815,
        0.8123763140827432
      ],
      "excerpt": "    #: decoderout shape (now) = batch_size x seq_len x word_vec_dim \n    decoderout = tf.reduce_sum(decoderout,1) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8509415152289815
      ],
      "excerpt": "    #: decoderout shape (now) = batch_size x word_vec_dim \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123763140827432
      ],
      "excerpt": "    out_prob_dist = tf.matmul(decoderout,Wpd) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8778669994397222
      ],
      "excerpt": "    output = tf.cond(tf.equal(teacher_forcing,tf.convert_to_tensor(False)), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.851674374655141
      ],
      "excerpt": "                     lambda: tf.gather(tf_embd,y[:,i])) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8289669050403863,
        0.9020247773548667
      ],
      "excerpt": "    output = output + tf_pe_out[i] \n    output = tf.reshape(output,[tf.shape(x)[0],1,word_vec_dim]) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8289669050403863
      ],
      "excerpt": "                                    lambda:replaceSOS(output,out_prob_dist), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8126471378252474
      ],
      "excerpt": "The mask will have the same shape as the batch of labels but with the value 0 wherever there is a PAD. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9108656470019463,
        0.8027796940770363,
        0.8027796940770363
      ],
      "excerpt": "    mask = np.ones_like((output_batch),np.float32) \n    for i in xrange(len(mask)): \n        for j in xrange(len(mask[i])): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.863830493850468,
        0.8722253495125046
      ],
      "excerpt": "import string \nimport random \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8383163063168253,
        0.8936954105699045
      ],
      "excerpt": "with tf.Session() as sess: #: Start Tensorflow Session \nsaver = tf.train.Saver()  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8172342705654815,
        0.8997243352845468,
        0.8737288687529231
      ],
      "excerpt": "    batch_len = len(train_batch_x) \n    shuffled_indices = np.arange(batch_len) \n    np.random.shuffle(shuffled_indices) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9112927358002096,
        0.9140294926549425
      ],
      "excerpt": "        sample_no = np.random.randint(0,len(train_batch_x[0])) \n        print(\"\\nCHOSEN SAMPLE NO.: \"+str(sample_no)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8056528458468546,
        0.9442417739250801
      ],
      "excerpt": "            print(\"\\nEpoch: \"+str(step+1)+\" Iteration: \"+str(i+1)) \n            print(\"\\nSAMPLE TEXT:\") \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.936606094659785,
        0.8172342705654815
      ],
      "excerpt": "            print(\"\\n\") \n        input_seq_len = len(train_batch_x[shuffled_indices[i]][0]) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8172342705654815
      ],
      "excerpt": "        output_seq_len = len(train_batch_y[shuffled_indices[i]][0]) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "            random_bool = True \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8172342705654815
      ],
      "excerpt": "                                         output_len: len(train_batch_y[shuffled_indices[i]][0]), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8280399876448089
      ],
      "excerpt": "            print(\"\\nPREDICTED TRANSLATION OF THE SAMPLE:\\n\") \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9103606878579814
      ],
      "excerpt": "                #:prediction_int = np.random.choice(range(vocab_len), p=array.ravel())  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8495594847589982
      ],
      "excerpt": "                prediction_int = np.argmax(array) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.936606094659785,
        0.8280399876448089
      ],
      "excerpt": "            print(\"\\n\") \n            print(\"ACTUAL TRANSLATION OF THE SAMPLE:\\n\") \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.936606094659785,
        0.8014341352115534
      ],
      "excerpt": "            print(\"\\n\") \n        print(\"loss=\"+str(loss)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8823867403514597
      ],
      "excerpt": "SAMPLE TEXT: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8823867403514597
      ],
      "excerpt": "SAMPLE TEXT: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8823867403514597
      ],
      "excerpt": "SAMPLE TEXT: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8823867403514597
      ],
      "excerpt": "SAMPLE TEXT: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8531307586624955,
        0.8823867403514597
      ],
      "excerpt": "Epoch: 128 Iteration: 45 \nSAMPLE TEXT: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8823867403514597
      ],
      "excerpt": "SAMPLE TEXT: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8823867403514597
      ],
      "excerpt": "SAMPLE TEXT: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8823867403514597
      ],
      "excerpt": "SAMPLE TEXT: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8823867403514597
      ],
      "excerpt": "SAMPLE TEXT: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8823867403514597
      ],
      "excerpt": "SAMPLE TEXT: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8823867403514597
      ],
      "excerpt": "SAMPLE TEXT: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8823867403514597
      ],
      "excerpt": "SAMPLE TEXT: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8408270126567471
      ],
      "excerpt": "CHOSEN SAMPLE NO.: 15 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8823867403514597
      ],
      "excerpt": "SAMPLE TEXT: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8823867403514597
      ],
      "excerpt": "SAMPLE TEXT: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8823867403514597
      ],
      "excerpt": "SAMPLE TEXT: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8823867403514597
      ],
      "excerpt": "SAMPLE TEXT: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8823867403514597
      ],
      "excerpt": "SAMPLE TEXT: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8823867403514597
      ],
      "excerpt": "SAMPLE TEXT: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8823867403514597
      ],
      "excerpt": "SAMPLE TEXT: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8823867403514597
      ],
      "excerpt": "SAMPLE TEXT: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8823867403514597
      ],
      "excerpt": "SAMPLE TEXT: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8823867403514597
      ],
      "excerpt": "SAMPLE TEXT: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8823867403514597
      ],
      "excerpt": "SAMPLE TEXT: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8823867403514597
      ],
      "excerpt": "SAMPLE TEXT: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.858539997398074,
        0.8936954105699045
      ],
      "excerpt": "print('Loading pre-trained weights for the model...') \nsaver = tf.train.Saver() \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589185899045798,
        0.8684199455215529
      ],
      "excerpt": "print('\\nRESTORATION COMPLETE\\n') \ntest = ['who','are','you'] #: Enter tokenized text here \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.944559296033787,
        0.8937603816784562,
        0.8826707990150668
      ],
      "excerpt": "test = np.asarray(test,np.float32) \ntest = test.reshape((1,test.shape[0],test.shape[1])) \ninput_seq_len = test.shape[0] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8633989807152664
      ],
      "excerpt": "test_pe = test+pe_in \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.906421761377065,
        0.8274383408349155
      ],
      "excerpt": "                                 y: np.zeros((1,1),np.int32),  \n                                 #: y value doesn't matter here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8504063645071395
      ],
      "excerpt": "        print(vocab_beng[np.argmax(array)],end=' ') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8346103235920521
      ],
      "excerpt": "I am then adding the decoder output along the second axis, to transform the shape into batch_size x word_vector_dimensions. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/JRC1995/Machine-Translation-Transformers/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2017 Jishnu Ray Chowdhury\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Machine Translation using Transformers",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Machine-Translation-Transformers",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "JRC1995",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/JRC1995/Machine-Translation-Transformers/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 26,
      "date": "Fri, 24 Dec 2021 16:40:01 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "machine-translation",
      "word2vec",
      "transformer",
      "attention-is-all-you-need",
      "english-bengali"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\ndef most_similar_eucli_eng(x):\n    xminusy = np.subtract(np_embedding_eng,x)\n    sq_xminusy = np.square(xminusy)\n    sum_sq_xminusy = np.sum(sq_xminusy,1)\n    eucli_dists = np.sqrt(sum_sq_xminusy)\n    return np.argsort(eucli_dists)\n    \ndef vec2word_eng(vec):   #: converts a given vector representation into the represented word \n    most_similars = most_similar_eucli_eng(np.asarray(vec,np.float32))\n    return vocab_eng[most_similars[0]]\n    \n```\n\n",
      "technique": "Header extraction"
    }
  ]
}