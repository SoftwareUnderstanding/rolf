{
  "citation": [
    {
      "confidence": [
        0.9205090845819072
      ],
      "excerpt": "Multi-Source Neural Translation \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/isi-nlp/Zoph_RNN",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2015-08-17T22:29:22Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-23T07:21:14Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9713889075157536
      ],
      "excerpt": "This is Barret Zoph's code for Zoph_RNN \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9401222374347393
      ],
      "excerpt": "This toolkit can successfully replicate the results from the following papers (the multi-gpu parallelism, which is explained in the tutorial, is similar to 6) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8279908210748592,
        0.8163282605194595
      ],
      "excerpt": "Transfer Learning for Low-Resource Neural Machine Translation \nEffective Approaches to Attention-based Neural Machine Translation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.863494423365924
      ],
      "excerpt": "Sequence to Sequence Learning with Neural Networks \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9383681012905883
      ],
      "excerpt": "The code for Zoph_RNN is provided in the src/ directory. Additionally, a precompiled binary (named ZOPH_RNN) is provided that will work on 64 bit linux for cuda 7.5, so it is not necessary to compile the code.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8547682927155172,
        0.8276599817750626
      ],
      "excerpt": "Any version of Eigen \nLets step through an example that trains a basic sequence-to-sequence model. The following code will train a sequence-to-sequence model with the source training data /path/to/source_train_data.txt and the target training data /path/to/target_train_data.txt. These are placeholder names that will be replaced with your data files when you are training your own model. The resulting model will be saved to model.nn, but this can be named whatever the user wants. Training data always needs to consist of one training example per line, with tokens separated by spaces. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8473195412183848,
        0.9323351281825853
      ],
      "excerpt": "By default the source sentences will always be fed in the reversed direction as in Sequence to Sequence Learning with Neural Networks. If you want to feed in the source sentences in the forward direction then simply preprocess your source data, so that it is in the reversed direction. \nThere are many flags that can be used to train more specific architectures. Lets say we want to train a model with 3 layers (default is 1), 500 hiddenstates (default is 100), and a minibatch of size 64 (default is 8). The following command does this: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9744673321165425
      ],
      "excerpt": "Lets also make the model have 20,000 source vocabulary and 10,000 target vocabulary (by default the code makes the source vocabulary equal to the number of unique tokens in the source training data, and the target vocab does the same). Also lets apply dropout with a keep probability of 0.8 to the model, where dropout is applied as specified in Recurrent Neural Network Regularization.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9836792386832913
      ],
      "excerpt": "Additionally, lets change the learning rate to 0.5 (default is 0.7), add the local-p attention model with feed input as in Effective Approaches to Attention-based Neural Machine Translation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9837669650124857
      ],
      "excerpt": "To monitor the training we also want to be able to monitor the performance of the model during training on some held out set of data (developement/validation). Lets do this in the code and also add the option that if perplexity (better is lower) on the held out set of data increased since it was previously checked, then we multiply the current learning rate by 0.5. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9243782721002471
      ],
      "excerpt": "During training the code needs to produce temporary files. By default these will be put in the directory where the code is launched from, but we can change this to whatever we want. Additionally, we can make all of the output that is typically printed to standard out (the screen) also be printed to a file. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.907254951107253
      ],
      "excerpt": "Typically during training only one model will be output at the end of training. To make the code output the best model during training according to the perplexity on your heldout data specificed by the -a flag we can add the -B flag.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8159927161194711
      ],
      "excerpt": "Or if you want to save all models every half epoch we can do that with the --save-all-models flag. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9344607002956742
      ],
      "excerpt": "By default the code will throw away any sentences in training and in the held out data longer than some fixed length which is 100 by default. We can change this to whatever we want, but be careful as it will greatly increase memory usage. Lets change it to 500. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9592947120385762
      ],
      "excerpt": "By default the code uses an MLE objective function, which can be very computationally expensive if the target vocabulary is big. To alleviate this issue we can train with NCE instead of MLE by using the --NCE flag. This is the same NCE as in Simple, Fast Noise Contrastive Estimation for Large RNN Vocabularies. A good number of noise samples is usually around 100. Note that the --NCE flag only has to be specified during training and not during force-decode or decode.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9714110994643375
      ],
      "excerpt": "One feature of this code is that is supports model parallelism across multiple gpus. To see the number of available gpu's on your node you can type nvidia-smi. The -M flag allows our model to put each layer on a gpu of our choosing along with the softmax. -M 0 1 2 3 means put layer 1 on GPU 0, layer 2 on GPU 1, layer 3 on GPU 2 and the softmax on GPU 3. By default the code does -M 0 0 0 0, putting everything on the default GPU 0. We can also change up the specification depending how many gpus we have on the node, so we could do -M 0 0 1 1 if we only have 2 gpus on our node. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8167906409840081
      ],
      "excerpt": "The --source-vocab-size N and the --target-vocab-size N flags create a vocabulary mapping file that will replace all words not in the top N most frequent words with <unks>'s. The code will create an integer mapping that is stored in the top of the model file. If you want to supply your own mapping file you can do this using the  --vocab-mapping-file /path/to/my_mapping.nn. The my_mapping.nn can be a previously trained model, in that case it will use the exact same vocabulary mapping as that model. This is useful because if you want to ensemble models using the --decode flag, then the models must have exactly the same target vocabulary mapping file for it to work. In the scripts/ directory there is a python script called create_vocab_mapping_file.py. We can use this to create a mapping file, which then gets fed into the training using the following command: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8640091790009444
      ],
      "excerpt": "Instead of using the create_vocab_mapping_file.py script, we can also use an existing model as the input for the --vocab-mapping-file flag \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8673732936978085,
        0.8705440319828534
      ],
      "excerpt": "If we trained the model using NCE then we can use the --NCE-score flag, which will make the model get the per line log probabilities using an unnormalized softmax. This greatly speeds up force-decode as now a normalization over the softmax does not have to be done, but now it does not represent a distribution that sums to 1. The reason we can do this is because the NCE training objective makes the normalization constant close to 1, so we can get a reasonably good approximation. \nLets have the model output the most likely target translation given the source using beam decoding. This can be done with the --decode (-k) flag. The model.nn file will be the trained neural network, kbest.txt is where we want the output to be put to and and source_data.txt is the file containing the source sentences that we want to be decoded. Once again short sentences are thrown out, so we can change that using the -L flag.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.970302856811558
      ],
      "excerpt": "By default the model uses beam decoding with a beam size of 12. We can change this using the -b flag. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8892164012465223
      ],
      "excerpt": "In the above example we only decoded a single model. In this code you have the option of ensembling multiple outputs using the --decode flag. All of the models you want to ensemble must have the same target vocabulary mappings, so you must use the --vocab-mapping-file flag as specified above. We can ensemble together 8 models below, but any number of models can be specified by the user.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9050073274198938,
        0.9628141526486153
      ],
      "excerpt": "Note that now we pass in 8 different model files and 8 different source data files. The reason for the 8 different source files is that the source vocabularies could be different for all 8 models, so different types of data can be passed in. If you want the same data passed in for all 8 model, then simply copy /path/to/source_data.txt 8 times as the input to --decode-main-data-files. \nTraining a sequence model is much like training a sequence-to-sequence model. Now we must employ the -s flag to denote that we want to train a sequence model. Lets train a model with slighly different parameters from the sequence-to-sequence model above. This model will have a hiddenstate size of 1000, minibatch size of 32, 2 layers, dropout rate of 0.3 and a target vocabulary size of 15K. Also note that now we only need to pass in one data file for training and for dev since it is only a sequence model and not a sequence-to-sequence model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8982816946704629
      ],
      "excerpt": "To force decode the model it is almost the same as force-decoding a sequence-to-sequence model. In the seq model you can also use the -m flag to speedup the batching process, but it will no longer output the per line log probability if -m is not set to 1. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9367526942280229
      ],
      "excerpt": "This is not a feature in the code. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8419586185530694
      ],
      "excerpt": "By default the model combines the two source encoders using the \"Basic\" method as specified in Multi-Source Neural Translation. To use the \"Child-Sum\" method we can add the following flag --lstm-combine 1. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9108154712778633,
        0.8578342328219601
      ],
      "excerpt": "Lets train a model using tranfer learning as specified in Transfer Learning for Low-Resource Neural Machine Translation. First we need to have parent data (source and target) and child data (source and target) where the parent and child models must have the same target language. In the paper the shared target language was English.  \nAlso note that this can only be done with seq-to-seq models and not seq models or multi-source models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8117668210028185
      ],
      "excerpt": "Once the above arguements are supplied other normal parameter flags can be added just like in the ZOPH_RNN executable. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8033938229482596
      ],
      "excerpt": "For the paper Simple, Fast Noise Contrastive Estimation for Large RNN Vocabularies the command below will train the billion word language model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9561999158084077
      ],
      "excerpt": "For the paper Transfer Learning for Low-Resource Neural Machine Translation the following command wil train the parent model and the child model (with the child language being Uzbek). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8060036769842819
      ],
      "excerpt": "For the paper Sequence to Sequence Learning with Neural Networks the following command will train the \"Single reversed LSTM\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8272297637424705
      ],
      "excerpt": "The flag (--random-seed) now takes in an integer to use as the fixed random seed, or by default now seeds with the time \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "C++/CUDA toolkit for training sequence and sequence-to-sequence models across multiple GPUs",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/isi-nlp/Zoph_RNN/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 64,
      "date": "Tue, 28 Dec 2021 02:11:08 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/isi-nlp/Zoph_RNN/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "isi-nlp/Zoph_RNN",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/isi-nlp/Zoph_RNN/master/scripts/compile.xing.sh",
      "https://raw.githubusercontent.com/isi-nlp/Zoph_RNN/master/scripts/compile.sh",
      "https://raw.githubusercontent.com/isi-nlp/Zoph_RNN/master/scripts/translate/f2e_decode.sh",
      "https://raw.githubusercontent.com/isi-nlp/Zoph_RNN/master/scripts/translate/f2e_train.sh",
      "https://raw.githubusercontent.com/isi-nlp/Zoph_RNN/master/scripts/berk_aligner/run_aligner.sh",
      "https://raw.githubusercontent.com/isi-nlp/Zoph_RNN/master/scripts/fsa/demo.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9283801625750232,
        0.9594435211765192
      ],
      "excerpt": "If you just want to use the executable, then run the following command cat executable/ZOPH_RNN_1 executable/ZOPH_RNN_2 executable/ZOPH_RNN_3 executable/ZOPH_RNN_4 &gt; ZOPH_RNN. Then ZOPH_RNN will be the executable that you can use.  To run the executable you need to be sure your path variable includes the location to CUDA. This is a sample command of putting cuda into your PATH variable export PATH=/usr/cuda/7.5/bin:$PATH \nIf you want to compile the Zoph_RNN code run bash scripts/compile.sh, which will compile the code given you set a few environmental variables. The variables that need to be set are below: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9681837496597198,
        0.8773691901597198
      ],
      "excerpt": "Note that cuda version greater than 7.0 is required to run the code, while the rest are required to compile the code \ncuda version greater than 7.0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9460142429167908,
        0.9663404984868463,
        0.8073446175808562
      ],
      "excerpt": "CuDNN version = 4 \nBoost version = 1.51.0 or 1.55.0  \nAny version of Eigen \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.835270419259897
      ],
      "excerpt": "Or if you want to save all models every half epoch we can do that with the --save-all-models flag. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8326821358600776
      ],
      "excerpt": "The unks.txt file will be generated during decoding, so save it somewhere that it can be accessed later. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8776390102360891
      ],
      "excerpt": "Here are sample commands that can be run to create models in the papers above: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8123725722464379
      ],
      "excerpt": "PATH_TO_BOOST_INCLUDE (example value: /usr/boost/1.55.0/include/ ) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9329659569523088,
        0.9329659569523088,
        0.875264760508601
      ],
      "excerpt": "PATH_TO_CUDNN_V4_64 (example value: /usr/cudnn_v4/lib64/ ) \nPATH_TO_EIGEN (example value: /usr/eigen/ ) \nPATH_TO_CUDNN_INCLUDE (example value: /usr/cudnn_v4/include/ ) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8698360316544693
      ],
      "excerpt": "Lets step through an example that trains a basic sequence-to-sequence model. The following code will train a sequence-to-sequence model with the source training data /path/to/source_train_data.txt and the target training data /path/to/target_train_data.txt. These are placeholder names that will be replaced with your data files when you are training your own model. The resulting model will be saved to model.nn, but this can be named whatever the user wants. Training data always needs to consist of one training example per line, with tokens separated by spaces. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.876778726248035
      ],
      "excerpt": "./ZOPH_RNN -t /path/to/source_train_data.txt /path/to/target_train_data.txt model.nn -N 3 -H 500 -m 64 --source-vocab-size 20000 --target-vocab-size 10000 -d 0.8 -l 0.5 --attention-model true --feed-input true \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8477340999060077
      ],
      "excerpt": "./ZOPH_RNN -t /path/to/source_train_data.txt /path/to/target_train_data.txt model.nn -N 3 -H 500 -m 64 --source-vocab-size 20000 --target-vocab-size 10000 -d 0.8 -l 0.5 --attention-model true --feed-input true -a /path/to/source_dev_data.txt /path/to/target_dev_data.txt -A 0.5 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8549126437044228
      ],
      "excerpt": "./ZOPH_RNN -t /path/to/source_train_data.txt /path/to/target_train_data.txt model.nn -N 3 -H 500 -m 64 --source-vocab-size 20000 --target-vocab-size 10000 -d 0.8 -l 0.5 --attention-model true --feed-input true -a /path/to/source_dev_data.txt /path/to/target_dev_data.txt -A 0.5 --tmp-dir-location /path/to/tmp/ --logfile /path/to/log/logfile.txt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8500635748847946
      ],
      "excerpt": "./ZOPH_RNN -t /path/to/source_train_data.txt /path/to/target_train_data.txt model.nn -N 3 -H 500 -m 64 --source-vocab-size 20000 --target-vocab-size 10000 -d 0.8 -l 0.5 --attention-model true --feed-input true -a /path/to/source_dev_data.txt /path/to/target_dev_data.txt -A 0.5 --tmp-dir-location /path/to/tmp/ --logfile /path/to/log/logfile.txt -B best.nn \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8807777443589326
      ],
      "excerpt": "./ZOPH_RNN -t /path/to/source_train_data.txt /path/to/target_train_data.txt model.nn -N 3 -H 500 -m 64 --source-vocab-size 20000 --target-vocab-size 10000 -d 0.8 -l 0.5 --attention-model true --feed-input true -a /path/to/source_dev_data.txt /path/to/target_dev_data.txt -A 0.5 --tmp-dir-location /path/to/tmp/ --logfile /path/to/log/logfile.txt --save-all-models true \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8807777443589326
      ],
      "excerpt": "./ZOPH_RNN -t /path/to/source_train_data.txt /path/to/target_train_data.txt model.nn -N 3 -H 500 -m 64 --source-vocab-size 20000 --target-vocab-size 10000 -d 0.8 -l 0.5 --attention-model true --feed-input true -a /path/to/source_dev_data.txt /path/to/target_dev_data.txt -A 0.5 --tmp-dir-location /path/to/tmp/ --logfile /path/to/log/logfile.txt --save-all-models true -L 500 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8904256572361867
      ],
      "excerpt": "./ZOPH_RNN -t /path/to/source_train_data.txt /path/to/target_train_data.txt model.nn -N 3 -H 500 -m 64 --source-vocab-size 20000 --target-vocab-size 10000 -d 0.8 -l 0.5 --attention-model true --feed-input true -a /path/to/source_dev_data.txt /path/to/target_dev_data.txt -A 0.5 --tmp-dir-location /path/to/tmp/ --logfile /path/to/log/logfile.txt --save-all-models true -L 500 --NCE 100 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8904256572361867
      ],
      "excerpt": "./ZOPH_RNN -t /path/to/source_train_data.txt /path/to/target_train_data.txt model.nn -N 3 -H 500 -m 64 --source-vocab-size 20000 --target-vocab-size 10000 -d 0.8 -l 0.5 --attention-model true --feed-input true -a /path/to/source_dev_data.txt /path/to/target_dev_data.txt -A 0.5 --tmp-dir-location /path/to/tmp/ --logfile /path/to/log/logfile.txt --save-all-models true -L 500 --NCE 100 -M 0 1 2 3 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8273998787501732
      ],
      "excerpt": "Once the model finished training we can use the model file (model.nn, best.nn or any of the models output from --save-all-best in the previous training example) for getting the perplexity for a set of source/target pairs or do beam decoding to get the best target outputs given some source sentences. Lets do the former first. We will specify the source and target data we want to get the perplexity for along with the per line log probabilities of each sentece. The output file we specify (/path/to/output/perp_output.txt) will contain the per line log probabilities and the total perplexity will be output to standard out. Additionally, we can use the --logfile flag as before if we also want standard out to be put to a file too and the -L flag to change what the longest sentence the code will accept. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8662795763903198
      ],
      "excerpt": "./ZOPH_RNN -k 1 model.nn kbest.txt --decode-main-data-files /path/to/source_data.txt -L 500 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.883567292872002
      ],
      "excerpt": "./ZOPH_RNN -k 1 model.nn kbest.txt --decode-main-data-files /path/to/source_data.txt -L 500 -b 25 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9475125476682471
      ],
      "excerpt": "./ZOPH_RNN -k 1 model.nn kbest.txt --decode-main-data-files /path/to/source_data.txt -L 500 -b 25 --print-score true \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9433796745353741
      ],
      "excerpt": "./ZOPH_RNN -k 1 model.nn kbest.txt --decode-main-data-files /path/to/source_data.txt -L 500 -b 25 --print-score true --dec-ratio 0.2 1.8 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8173971761960321
      ],
      "excerpt": "./ZOPH_RNN -k 1 model1.nn model2.nn model3.nn model4.nn model5.nn model6.nn model7.nn model8.nn kbest.txt --decode-main-data-files /path/to/source_data1.txt /path/to/source_data2.txt /path/to/source_data3.txt /path/to/source_data4.txt /path/to/source_data5.txt /path/to/source_data6.txt /path/to/source_data7.txt /path/to/source_data8.txt -L 500 -b 25 --print-score true --dec-ratio 0.2 1.8 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8576417967062178
      ],
      "excerpt": "./ZOPH_RNN -s -t /path/to/training_data.txt model.nn -H 1000 -m 32 -l 0.2 -N 2 -M 0 1 2 -d 0.7 --target-vocab-size 15000 -a /path/to/dev_data.txt -A 0.5 --tmp-dir-location /path/to/tmp/ --logfile /path/to/log/logfile.txt --save-all-models true -L 500 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8014109757201173
      ],
      "excerpt": "./ZOPH_RNN -s -f /path/to/dev_data.txt model.nn /path/to/output/perp_output.txt -L 500 --logfile /path/to/log/logfile.txt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8110944934351865
      ],
      "excerpt": "Now we have created a mapping file mapping.nn, which can now be used for training. Now lets train the parent model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8857563453298561
      ],
      "excerpt": "python scripts/pretrain.py --parent parent_model.nn --trainsource /path/to/source_child_data.txt --traintarget /path/to/target_child_data.txt --devsource /path/to/source_child_dev_data.txt --devtarget /path/to/target_child_dev_data.txt --rnnbinary ./ZOPH_RNN --child child.nn -d 0.8 -l 0.5 -A 0.5 -P 0.01 -w 5 -L 200 -m 32 -n 15 --attention_model True --feed_input True \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.894036606553709
      ],
      "excerpt": "ls data/train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137,
        0.896858500678198
      ],
      "excerpt": "train.e  train.u \nls data/test \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8633989807152664
      ],
      "excerpt": "test.e test.u \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9224771835495269,
        0.8479682975221484
      ],
      "excerpt": "./ZOPH_RNN -k 1 model.nn kbest.txt --decode-main-data-files /path/to/source_data.txt -L 500 -b 25 --print-score true --dec-ration 0.2 1.8 --UNK-decode /path/to/unks.txt \nNext we will run the scripts/unk_format.py script to convert the output of the ZOPH_RNN code into correct format for the scripts/att_unk_rep.py script.   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8222183613754221
      ],
      "excerpt": "Next we will run the final scripts/att_unk_rep.py script. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8722450665447117
      ],
      "excerpt": "Now the kbest.txt.formatted.unkrep will contain the decoded sentences with the rare words replaced. The format is 1 output per line. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8411886831980538
      ],
      "excerpt": "./ZOPH_RNN -t german_train_data.txt train_english_data.txt model.nn -n 15 -B best.nn -m 128 -H 1000 -l 0.7 -w 5 -a german_dev_data.txt english_dev_data.txt french_dev_data.txt -A 1 -v 50000 -V 50000 --clip-cell 50 1000 -N 4 -M 0 1 1 2 3 --multi-source french_train_data.txt src.nn -d 0.8 -L 65 --logfile log.txt --screen-print-rate 15 --fixed-halve-lr-full 11 -P -0.08 0.08 --lstm-combine 1 --attention-model 1 --feed-input 1 --multi-attention 1  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8824980238661093
      ],
      "excerpt": "./ZOPH_RNN --logfile log.txt -a english_dev_data.txt -s -t english_train_data.txt model.nn -B best.nn --NCE 100 --screen-print-rate 300 -N 4 -M 0 1 2 3 3 -l 0.7 -P -0.08 0.08 -A 0.5 -d 0.8 -n 20  -c 5 -H 2048 --vocab-mapping-file my_mapping.nn -L 205 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8880025188514927
      ],
      "excerpt": "./ZOPH_RNN -t french_train_data.txt english_parent_train_data.txt -H 750 -N 2 -d 0.8 -m 128 -l 0.5 -P -0.08 0.08 -w 5 --attention-model 1 --feed-input 1 --screen-print-rate 30 --logfile log.txt -B best.nn -n 10 -L 100 -A 0.5 -a french_dev_data.txt english_parent_dev_data.txt --vocab-mapping-file my_mapping.nn  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8772454161246976,
        0.8766287388764757
      ],
      "excerpt": "Once the parent model finishes training then run: \npython scripts/pretrain.py --parent best.nn --trainsource uzbek_train_data.txt --traintarget english_child_train_data.txt --devsource uzbek_dev_data.txt --devtarget english_child_dev_data.txt --rnnbinary ZOPH_RNN --child child.nn -d 0.5 -l 0.5 -A 0.9 -P -0.05 0.05 -w 5 -L 100 -m 128 -n 100 --attention_model True --feed_input True \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8608142967780361
      ],
      "excerpt": "./ZOPH_RNN --logfile log.txt -a english_dev_data.txt german_dev_data.txt -t english_train_data.txt german_train_data.txt model.nn -B best.nn --screen-print-rate 300 -N 4 -M 0 1 2 2 3 -L 50 -l 1 -P -0.1 0.1 --fixed-halve-lr-full 9 -A 1 -d 0.8 -n 12 -w 5 --attention-model 1 --feed-input 1 --attention-width 10 -v 50000 -V 50000 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8580339164651533
      ],
      "excerpt": "./ZOPH_RNN -t source_train_data.txt target_train_data.txt model.nn  -H 1000 -N 4 -v 160000 -V 80000 -P -0.08 0.08 -l 0.7 -n 8 --fixed-halve-lr 6 -m 128 -w 5 -L 100 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/isi-nlp/Zoph_RNN/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "C++",
      "Cuda",
      "Python",
      "Shell",
      "Perl",
      "C"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Zoph\\_RNN: A C++/CUDA toolkit for training sequence and sequence-to-sequence models across multiple GPUs",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Zoph_RNN",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "isi-nlp",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/isi-nlp/Zoph_RNN/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 169,
      "date": "Tue, 28 Dec 2021 02:11:08 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "For this tutorial `ZOPH_RNN` represents the executable to run the code. Also all the scripts in the `scripts` folder require python 3 to run.\n\nThis command will bring up the program's help menu showing all the flags that the program can be run with:\n\n```\n./ZOPH_RNN -h\n```\n\nThere are two different kinds of models this code can train\n\n1. Sequence models (Ex: Language Modeling)\n2. Sequence-to-Sequence models (Ex: Machine Translation)\n\nThe commands for these two different architectures are almost the same, all that needs to change is adding a `-s` flag if you want to use the sequence model. The sequence-to-sequence model is used by default.\n\nIn the `sample_data` directory there is sample data provided that shows the proper formatting for files.\n\n\n",
      "technique": "Header extraction"
    }
  ]
}