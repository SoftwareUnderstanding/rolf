{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "I express my sincere gratitude to my director, Roc\u00edo Vilar Cortabitarte, and co-director, Alicia Calder\u00f3n Taz\u00f3n, for providing their expertise and guidance throughout the course of this project. I also thank the rest of my advisors, Agust\u00edn Lantero Barreda and N\u00faria Castell\u00f3-Mor, who contributed so thoroughly through their assistance and dedicated involvement.\n\n",
      "technique": "Header extraction"
    }
  ],
  "citation": [
    {
      "confidence": [
        0.8906174419333412
      ],
      "excerpt": ":mortar_board: Publication: https://repositorio.unican.es/xmlui/handle/10902/20627 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9788836068726089
      ],
      "excerpt": "1. :computer: [Getting Started](https://github.com/aritzLizoain/CNN-Image-Segmentation#1-computer-getting-started)  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.934429703673198
      ],
      "excerpt": "4. :dart: [Results](https://github.com/aritzLizoain/CNN-Image-Segmentation#4-dart-results) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8940583111730483
      ],
      "excerpt": "augmentation_sequence_Invert and augmentation_Invert apply color channel inversion, dropout, logContrast, hue, gammaContrast. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9146482686409504,
        0.9446351942225217
      ],
      "excerpt": "Implemented U-Net architecture. Adapted from Ronneberger, O. et al. \"Convolutional Networks for Biomedical Image Segmentation\". (2015). Link. \n:information_source: For more information regarding ML (CNNs, layers, image segmentation) please read Theoretical Concepts: Machine Learning \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/aritzLizoain/CNN-Image-Segmentation",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contributing\nWhen contributing to this repository, please first discuss the change you wish to make via issue,\nemail, or any other method with the owner of this repository before making a change. \nPull Request Process\n\nUpdate the README.md with details of changes to the interface, this includes new environment \n   variables, exposed ports, useful file locations and container parameters.\nIncrease the version numbers in any examples files and the README.md to the new version that this\n   Pull Request would represent.\n\nCode of Conduct\nPledge\nIn the interest of fostering an open and welcoming environment,\ncontributors and maintainers pledge to making participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, gender identity and expression, level of experience,\nnationality, personal appearance, race, religion, or sexual identity and\norientation.\nStandards\nExamples of behavior that contributes to creating a positive environment\ninclude:\n\nUsing welcoming and inclusive language\nBeing respectful of differing viewpoints and experiences\nGracefully accepting constructive criticism\nFocusing on what is best for the community\nShowing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\nThe use of sexualized language or imagery and unwelcome sexual attention or\nadvances\nTrolling, insulting/derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others' private information, such as a physical or electronic\n  address, without explicit permission\nOther conduct which could reasonably be considered inappropriate in a\n  professional setting\n\nOwn Responsibilities\nProject maintainers are responsible for clarifying the standards of acceptable\nbehavior and are expected to take appropriate and fair corrective action in\nresponse to any instances of unacceptable behavior.\nProject maintainers have the right and responsibility to remove, edit, or\nreject comments, commits, code, wiki edits, issues, and other contributions\nthat are not aligned to this Code of Conduct, or to ban temporarily or\npermanently any contributor for other behaviors that they deem inappropriate,\nthreatening, offensive, or harmful.\nScope\nThis Code of Conduct applies both within project spaces and in public spaces\nwhen an individual is representing the project or its community. Examples of\nrepresenting a project or community include using an official project e-mail\naddress, posting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event. Representation of a project may be\nfurther defined and clarified by project maintainers.\nEnforcement\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting the project team. All\ncomplaints will be reviewed and investigated and will result in a response that\nis deemed necessary and appropriate to the circumstances. The project team is\nobligated to maintain confidentiality with regard to the reporter of an incident.\nFurther details of specific enforcement policies may be posted separately.\nProject maintainers who do not follow or enforce the Code of Conduct in good\nfaith may face temporary or permanent repercussions as determined by other\nmembers of the project's leadership.\nAttribution\nThis Code of Conduct is adapted from the Contributor Covenant, version 1.4,\navailable at http://contributor-covenant.org/version/1/4",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-05-08T10:59:45Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-01T05:37:09Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The Python application consists on the 8 files previously explained. Only the last two ([train.py](https://github.com/aritzLizoain/CNN-Image-Segmentation/blob/master/Code/train.py) and [load_model.py](https://github.com/aritzLizoain/CNN-Image-Segmentation/blob/master/Code/load_model.py)) are executed.\n\n<p align=\"center\">\n<img src=\"https://github.com/aritzLizoain/CNN-Image-Segmentation/blob/master/Images/Example_Images/Summary.png\" width=\"1000\"/>\n</p>\n\n*Python implementation summary.*\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8741675528565206,
        0.9541917279631339
      ],
      "excerpt": "The Standard Model of particle physics, while being able to make accurate predictions, has been proved to fail to explain various phenomena, such as astronomical dark matter observations. \nIn this work, a machine learning application has been implemented with the goal of studying dark matter candidates. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8214414708704628
      ],
      "excerpt": "A U-Net model is trained with Python. The model performs multi-class image segmentation in order to detect dark matter particle signals among background noise. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.99954021722941,
        0.8219979848617904
      ],
      "excerpt": "In  this  section  the  core  of  the  project  is  dissected.   Every  employed method is explained;  from the origination of an image,  to the training of the model, every necessary step in the creation of the deep learning application is analyzed. \n:chart_with_upwards_trend: Function: sets details of the simulated images that are created in image_simulation.py. The pixel intensity value of each element in the image can be defined. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9866650400195975
      ],
      "excerpt": ":warning: Caution: it is important to be aware of the importance of the predefined pixel intensity values. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.886601577351111
      ],
      "excerpt": "In order to do this, image pixel intensity values are taken as reference to label different classes (please read mask.py for more information). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955824344927777
      ],
      "excerpt": "    In a case where >95% of the pixels are labeled as background, and <1% as clusters, the model can give a 95% accuracy prediction, but all pixels might be predicted as background, giving a meaningless output. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9621792735978854,
        0.8225214403453015,
        0.8448540896436174
      ],
      "excerpt": "    The weight of each class is obtained as the inverse of its frequency in the training samples. \n    The weights are then normalized to the number of classes. \n    These weights are used by the model in the weighted_categorical_crossentropy loss function. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8894009546744185,
        0.9809180057513175,
        0.9831239046744444,
        0.9203150401487744
      ],
      "excerpt": "    These files are read and saved as arrays that contain the collected charge by each CCD pixel. Since 256X256 pixel images are used for training the model, the DAMIC image is divided into sections of the same size, so they can be individually passed through the trained model, obtaining their respective predicted labels. \n    The possibility to normalize, cut and resize the image is given. \nimages_small2big is used to reconstruct the predictions of all sections into a full segmentation map. \ncheck_one_object is used to analyze the final output. It looks for a chosen category (i.e. 'Clusters') section by section. It returns a modified predicted label; it only shows background and the pixels classified as a chosen class. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9092650936357377,
        0.9168411090271567,
        0.9285108166449447,
        0.882867015702688
      ],
      "excerpt": "statistics shows the number of classes and their frequency on the dataset. \nget_percentages simply returns the percentages of each class. This is used to calculate the weights for the loss function by get_weights. \nvisualize_label is used to visualize the created label. \ncreate_masks takes the images as input and returns the labels. These labels are used by the model to train and evaluate the model while training. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8488581083567232,
        0.8051553005741086,
        0.8791826222485442
      ],
      "excerpt": "create_labels_noStat_noPrint is the same as create_labels but it does not print the information in the console. Created in order to avoid the repeated information shown by the console. \noutput_to_label takes the model outputs and converts them into labels that can be visualized.  \noutput_to_label_one_object takes the model outputs and converts them into labels where a chosen class is visualized.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8021396384276817
      ],
      "excerpt": "contrast augments the contrast of the image (unused).  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9782828460052252
      ],
      "excerpt": ":warning: Caution: it is important to be aware of the importance of the predefined pixel intensity values in image_details.py. The way this model is implemented, image lables do not need to be provided. Image labels are directly obtained from the images. In order to do this, image pixel intensity values are taken as reference to label different classes. Therefore elements with overlapping pixel intensity values will not be correctly labeled.<br/><br/>Labels can perfectly be created using a labeling software. However, for the purpose of this project, automatic pixel-wise labeling is a practical solution. Remember that, in case of using your own labels, image and label names must match. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9543050905646222
      ],
      "excerpt": ":chart_with_upwards_trend: Function: applies data augmentation techniques to both images and corresponding labels. Please read the imgaug documentation for more information on augmentation techniques. This is an optional step; it is applied when only a few training samples are available, or when the desired property is not present in the dataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.810160808003608,
        0.8423072346441415
      ],
      "excerpt": ":information_source: For more information regarding data augmentation please read Theoretical Concepts: Data Augmentation \n:chart_with_upwards_trend: Function: defines the model architecture and layer features. The model has U-Net architecture. The code is already prepared to add or remove layers in the model. Additionally, pretrained weights from an already trained model can be used. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9946197108310845
      ],
      "excerpt": "    is used to calculate the categorical crossentropy loss of the model with the modification of taking into account the weight of each class. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8089758561846111,
        0.8123827505379958
      ],
      "excerpt": ":chart_with_upwards_trend: Function: trains a model with the defined parameters. Process: \n  * Loads the datasets. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9451098309881482,
        0.840133458691946
      ],
      "excerpt": "  * Trains the model with the defined hyperparameters and callbacks. For more information regarding callbacks please read the keras callbacks documentation. \n  * Plots and saves the accuracy and loss over the training process. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.864261431168893,
        0.9255973742934459
      ],
      "excerpt": "  * Gives a classification report that analyzes the performance of each class. For more information regarding the classification reports please read the scikit-learn classification report documentation. \nThe trained model and all the figures are saved in the defined paths. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.849132851242027
      ],
      "excerpt": ":warning: Caution: Depending on the used device, training a model can be rather slow, particularly when large datasets and number of epochs are being used. If the model name is not changed, the model will be overwritten. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9723638607894121,
        0.9440353785259179
      ],
      "excerpt": "An imbalanced dataset entails further problems. A good solution to this issue is creating balanced images, with approximately the same percentage of presence of each class. The classes are not mixed in order to avoid confusion to the model when labeling the images. \nAdditionally, only 60% of  the  images  contain  glowing,  and  it  does  not  always  start  from  the first pixel.  This way, the model does not learn that all predictions should have a glowing column, nor where should it be. Here is an example of an image used for training the model:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9330302823769132
      ],
      "excerpt": "For this project 180 training and 42 test images are created. The model is trained with the following hyperparameters:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9414453136403702,
        0.9104147315316966
      ],
      "excerpt": "Please note that these parameters work well for this particular dataset, but do not assure reliable results for other datasets. \nThe loss and accuracy of the training and evaluation set verify that the model does not suffer from overfitting. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8644892112772746
      ],
      "excerpt": "Training process of the model, that reaches its optimum state at epoch 40. <br/> LEFT: Training and validation accuracy. <br/> RIGHT: Training and validation loss. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9700705577428183,
        0.9573124674437806
      ],
      "excerpt": "The accuracy of the model on the test set is: 99.21%<br/> \nThe loss of the model on the test set is: 0.304<br/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9101200158252316
      ],
      "excerpt": "Recall: percentage of correctly classified pixels among all pixels that truly are of the given class.<br/>  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9652716300295163
      ],
      "excerpt": "Best score is 1.00 and worst score is 0.00.<br/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9674443884322537
      ],
      "excerpt": "The model also gives a seemingly correct prediction of a real image. Due to the small size of the objects, these cannot be seen when the whole image was displayed. If the 256\u00d7256 sections are individually observed instead, the segmented clusters can be analyzed. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8729360876074153,
        0.9714745952427226
      ],
      "excerpt": "<br/> LEFT: Real image 256X256 section to be passed through the trained model. The pixel intensity values are given in ADCs.<br/> RIGHT: Predicted label with segmented clusters. \nThe  application\u2019s  shortcomings  are  revealed  when  a  more complex image is passed through the model, obtaining a prediction that is  not  entirely  correct. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8628534398227111,
        0.8983256471876205,
        0.8512154410459907,
        0.9924623777155758
      ],
      "excerpt": "<br/> LEFT: DAMIC image (T=240K). The pixel intensity values are given in ADCs.<br/> RIGHT: Predicted label with unusual hot pixel and cluster segmentation on the bottom right side.. \nIn  order  to  deal  with  this  issue, defining geometrical restrictions is a possible solution.  For instance, imposing a hot pixel prediction to have its expected shape,  which is avertical or horizontal line. Likewise a maximum cluster size can be defined.  \nAdditionally, glowing predictions are likely to improve if different, more realistic shapes are generated in the simulated dataset. \nThe results might suggest that a model is limited to the range of intensities to which it is trained. However, applying different normalization techniques, such as linear scaling or clipping, to the DAMIC images is a seemingly good approach to solve this limitation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8412784439671993,
        0.9625687032852568
      ],
      "excerpt": "Feel free to submit pull requests. \nPlease read CONTRIBUTING.md for details on the code of conduct, and the process for submitting pull requests. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Deep learning project focused on dark matter searches",
      "technique": "GitHub API"
    }
  ],
  "documentation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "https://imgaug.readthedocs.io/",
      "technique": "Regular expression"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/aritzLizoain/Image-segmentation/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 5,
      "date": "Sat, 25 Dec 2021 08:38:18 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/aritzLizoain/CNN-Image-Segmentation/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "aritzLizoain/CNN-Image-Segmentation",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The project can be either cloned or downloaded to your own device. The source code of the application is implemented in Python, with the requirement of the following open-source libraries (version numbers are up to date: 21.06.2020):\n\n* TensorFlow (2.1.0)\n* Keras (2.3.1)\n* Numpy (1.18.1)\n* scikit-learn (0.22.1)\n* scikit-image (0.9.3)\n* OpenCV-Python (4.0.1)\n* imgaug (0.4.0)\n* Matplotlib (3.2.1)\n* Pillow (7.1.2)\n\nThe libraries can manually be installed from the anaconda prompt with the command ``` pip install 'library_name' ```. Make sure the correct working environment is activated.\nIf a module cannot be properly installed (installing tensorflow might sometimes be troublesome), doing it through the anaconda navigator is a good option.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9422116738500697
      ],
      "excerpt": "  * 1.1 [Installation](https://github.com/aritzLizoain/CNN-Image-Segmentation#11-installation) \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8412967315061691
      ],
      "excerpt": "Testing and training images can be created. There is no need to create a validation dataset. The model automatically shuffles all images and creates a validation split in the training process. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8128380405055851
      ],
      "excerpt": ":chart_with_upwards_trend: Function: calculates the weights for the loss function and processes FITS files (DAMIC images). Originaly created to load the datasets as PNG files into arrays (unused in version 2.0). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8654252828055706,
        0.8012955509272581,
        0.8533960893366934,
        0.884688243283717,
        0.8795754641087481,
        0.85538810979067
      ],
      "excerpt": "get_monochrome converts the input image into a monochrome image. Input shape = (number of images, height, width, 3(RGB)) --> Output shape = (number of images, height, width, 1). \nget_class classifies each pixel as a certain class depending on its intensity value. \nget_mask creates labels from input images. Input shape = (number of images, height, width, 3(RGB)) --> Output shape = (number of images, height, width, number of classes), where number of classes = 4 in this project. \nget_max_in_mask takes the position of the maximum  value, i.e., the class. Input shape = (number of images, height, width, number of classes) --> Output shape = (number of images, height, width, 1). \nmask_to_label creates the segmentatin map that can be visualized. It applies a color multiplier to each class. Input shape = (number of images, height, width, 1) --> Output shape = (number of images, height, width, 3(RGB)). \nmask_to_label_one_object creates the segmentatin map to visualize a chosen class. It is used by check_one_object. It applies a color multiplier to that one class. Input shape = (number of images, height, width, 1) --> Output shape = (number of images, height, width, 3(RGB)). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8458388101513388
      ],
      "excerpt": "Example of   the console display while training \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8982950676727861
      ],
      "excerpt": ":cop: Requirements: working directory path must contain models.py, mask.py and augmentation.py(if used). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9143431334106238
      ],
      "excerpt": ":cop: Requirements: working directory path must contain load_dataset.py, models.py and mask.py. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/aritzLizoain/CNN-Image-Segmentation/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "CNN Image Segmentation",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "CNN-Image-Segmentation",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "aritzLizoain",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/aritzLizoain/CNN-Image-Segmentation/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "aritzLizoain",
        "body": "Image multi-class segmentation on simulated images.\r\nImage multi-class segmentation on real detector images. :new: ",
        "dateCreated": "2020-10-13T22:15:17Z",
        "datePublished": "2020-10-13T21:22:23Z",
        "html_url": "https://github.com/aritzLizoain/CNN-Image-Segmentation/releases/tag/v2.0",
        "name": "CNN Image Segmentation",
        "tag_name": "v2.0",
        "tarball_url": "https://api.github.com/repos/aritzLizoain/CNN-Image-Segmentation/tarball/v2.0",
        "url": "https://api.github.com/repos/aritzLizoain/CNN-Image-Segmentation/releases/32531495",
        "zipball_url": "https://api.github.com/repos/aritzLizoain/CNN-Image-Segmentation/zipball/v2.0"
      },
      {
        "authorType": "User",
        "author_name": "aritzLizoain",
        "body": "Image multi-class segmentation on simulated images.",
        "dateCreated": "2020-06-24T19:24:21Z",
        "datePublished": "2020-06-25T14:54:42Z",
        "html_url": "https://github.com/aritzLizoain/CNN-Image-Segmentation/releases/tag/v1.0",
        "name": "CNN on simulated images",
        "tag_name": "v1.0",
        "tarball_url": "https://api.github.com/repos/aritzLizoain/CNN-Image-Segmentation/tarball/v1.0",
        "url": "https://api.github.com/repos/aritzLizoain/CNN-Image-Segmentation/releases/27906380",
        "zipball_url": "https://api.github.com/repos/aritzLizoain/CNN-Image-Segmentation/zipball/v1.0"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 4,
      "date": "Sat, 25 Dec 2021 08:38:18 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "dark-matter",
      "deep-learning",
      "python",
      "cnn",
      "keras-tensoflow",
      "image-segmentation"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "These instructions explain how to get a copy of the project to run it on your local machine for development and testing purposes.\n\n",
      "technique": "Header extraction"
    }
  ]
}