{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1505.04597 \"Paper pdf\"\n  [2922dbe6]: https://github.com/LeeJunHyun/Image_Segmentation/blob/master/network.py \"Models class\"\n\n  [3aad73d3]: https://arxiv.org/pdf/1711.10684.pdf \"Paper pdf\"\n  [714b6f76]: https://github.com/galprz/brain-tumor-segmentation/blob/master/src/models.py \"Model\"\n  [6dbd0345]: https://github.com/nikhilroxtomar/Deep-Residual-Unet/blob/master/Deep%20Residual%20UNet.ipynb \"Keras notebook\"\n  [da501ee2]: https://towardsdatascience.com/u-net-b229b32b4a71 \"Medium article\"\n\n#### 4. Metrics, Results and observations\n\n  Pixel wise accuracy was initially used to as a metric but since we have lots of black pixels it is easy to fool the metric to get high values. Other options are using - iou or dice coefficient. IOU  again for the same reason of most matching\n\n  Chosen dice coefficient for final runs\n\n   ```\n   def dice_coeff(pred, target"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8101227425953824
      ],
      "excerpt": "   1. Object inside the above background \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "     if zipfile.is_zipfile(fname): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8275296219550469
      ],
      "excerpt": "    img = Image.open(io.BytesIO(imgdata)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8043073075947367
      ],
      "excerpt": "    if array: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8239070269453883
      ],
      "excerpt": "  <!-- - [ ] **Copy Code from video and understand** \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/hemanth346/mde_bs",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-05-24T16:33:59Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-06-10T01:51:41Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.804135972886659
      ],
      "excerpt": "2. Depth map for the image \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8960344518462658
      ],
      "excerpt": "the mask can be used to check if a person is in the frame and the movement of the person \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.829304305483755,
        0.9978174128189354
      ],
      "excerpt": "To be made real-time the inference time has to be very less and be able to run on the high fps. \nThe objective of this project is to get a working model with less number of parameters (and less size). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8860458519148934
      ],
      "excerpt": "Generated using pretrained weights of a SOTA model [pretrained weights][8ead022d] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8403737639291592,
        0.9760221138891886
      ],
      "excerpt": "Calculated mean, std for each of the image set... \nTo calculate the standard deviation by averaging samples of the std from mini batches. While very close to the true std, it\u2019s not calculated exactly and can be leveraged if time/computation limitations. But in production settings where new data is added on daily basis this will work. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9813385508298789
      ],
      "excerpt": "    for data in loader: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "        std += data.std(2).sum(0) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8103472664000531,
        0.8823823324406304
      ],
      "excerpt": "Each of the folders in fg_bg, depth_maps, fg_bg_masks -  has separate zip file for every bg file. Images are read directly from the zip file to save on disk space and extraction time. \nList of all paths are stored obtained from fg_bg and when dataset is called by dataloader, corresponding images from other folders/zip files are obtained \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8349720522036919
      ],
      "excerpt": "A helper function is added to the dataset class to convert the zipfile into PIL file for regular transforms and numpy array for albumentation transformations \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9263396006834609,
        0.9383121596227979,
        0.9296797680133856
      ],
      "excerpt": "Since the model is trying to predict dense ouput, the architecture should have de-Convolvution(Transpose Convolvution). --> \nAll above models were trained with BCE loss for both mask and depth_maps. While the results looks promising masks are not clear and depth maps still misses out the details. \nAfter learning about few losses and experimenting soft Dice loss was chosen for masks, and pixel wise MSE loss was chosen for Depth maps over ssim and msssim, since we have to decide the window width for ssim. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9928327883500082
      ],
      "excerpt": "Pixel wise accuracy was initially used to as a metric but since we have lots of black pixels it is easy to fool the metric to get high values. Other options are using - iou or dice coefficient. IOU  again for the same reason of most matching \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8586468645709918
      ],
      "excerpt": "    - [ ] Collect own data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9247815232396366
      ],
      "excerpt": "  1. [ ] **Make API for data to be consumed by model** \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8638019956482662,
        0.9050742510592596
      ],
      "excerpt": "  1. [ ] **Design Network architecture and model the data** \n    - [ ] model for background subtraction only \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.908870030594422
      ],
      "excerpt": "    - [ ] model for Monocular Depth Estimation only \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Monocular Depth Estimation and Background substraction on custom dataset of 400k images created. Both objectives are achieved using single custom deep learning model using transpose convolutions",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/hemanth346/mde_bs/releases",
    "technique": "GitHub API"
  },
  "faq": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Worked with minimal data of 12k images to test the waters before diving in.\n\n- [x] ",
      "technique": "Header extraction"
    }
  ],
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Fri, 24 Dec 2021 19:25:02 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/hemanth346/mde_bs/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "hemanth346/mde_bs",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/hemanth346/mde_bs/master/notebooks/Create_compressed_dataset.ipynb",
      "https://raw.githubusercontent.com/hemanth346/mde_bs/master/notebooks/Cal_DataStats.ipynb",
      "https://raw.githubusercontent.com/hemanth346/mde_bs/master/notebooks/mask_depth_11.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Used below extension and magic function to access TensorBoard. This will open the tensorboard in the cell output\n```\n#:Load extension\n%load_ext tensorboard\n\nlogs_base_dir = 'logs'\n%tensorboard --logdir {logs_base_dir}\n```\nTo write to tensorboard we can use summary writer from torch.utils\n```\n#: TensorBoard support\nfrom torch.utils.tensorboard import SummaryWriter\n```\n\n- [x] ",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "> Model is expected to give us a mask and depth map for foreground given 2 images.\n\n```\n Can we think of using a single fg_bg image and predict it as well.?\n\n   Will that change the problem scope, from trying to identify foreground object in given background image the model will be trying to find the mask and depth in any general setting..?\n\n   Will experiment with that as well if time permits\n ```\n\n- using inputs as\n    - [ ] only fg_bg\n    - [x] both bg and fg_bg\n\n- Predicting\n    - [x] Only mask\n    - [x] Only depth map\n    - [x] both mask and depth_maps  \n\nAfter looking at the 64x64 images, the images became pixelted and share edges are not available. RF for gradients was set at 5 pixels.\n\n> Since the output is also to be as the same size of input, we have to either do a Transpose conv or maintiain the size without doing any maxpool/stride 2 and no padding. Without any stride/maxpool - to get the receptive field of image size, in final layer, requires lot of convolutional layers.\n\n\nUsed Group convolutions with 2 groups of 3 channels each for first few convolutions. The idea being for the network to be able to learn for low level features from both the images intially.\n\nInitial network is created without using  transpose conv/de-conv. And accounting for RF, below is the brief network summary. Heavy model with more params\n\n```\nFinal layer RF - 70\n\nTotal params: 3,769,664\nTrainable params: 3,769,664\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.09\nForward/backward pass size (MB): 462.03\nParams size (MB): 14.38\nEstimated Total Size (MB): 476.51\n```\n- Training took around 40 mins per epoch on 16G Tesla P100\n\n\n<!-- Model such that the ground truth image size output is achieved in final layer. We can use **view** on tensor in final layer to get required output size or do tranpose/deConv convolvution.\n\nAs a baseline, we are using a ResNet 18 model with final layer. **Training Notebook** -->\n\n<!-- If transpose conv/upsampling is not used, no. of parameters required to reach the  receptive field of image is very high.\nIf the image sizes are reduced drastically it'll not serve as a baseline model. But since the objective is to get the setup correct, images have been resized to 64x64 using  transforms\n\nArchitecture diagram :\n**Todo add image**\n\n -->\n\n- [x] ",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "- Transformations used are : _RandomCrop, HorizontalFlip, Resize(64x64), Normalization_\n\nUsed [albumentation library][07ffb173] for transformations. Read about its advantages [here][8a687b30] and [here][3dd4fa4a]\n\nAlbumentations doesn't support loading PIL images directly and works with numpy arrays. _Hence we have to modify the dataset class accordingly_. I found this [notebook][33e23d49] very useful as a guide.\n\nAlbumentations also have support to pass masks/weights along with original image and get the same transforms applied on them [example here][49e22e45] or we can create our own [custom targets][f98e539c] to be sent for the compose, which are useful if we have multiple images not linked with each other but need same transforms.\n\nExamples of some of the transforms applied for segmentation problem can be found in this [notebook][8a740691]\n\n  [07ffb173]: https://github.com/albumentations-team/albumentations/ \"GitHub link\"\n  [8a687b30]: https://arxiv.org/pdf/1809.06839 \"arxiv paper\"\n  [3dd4fa4a]: https://medium.com/@ArjunThoughts/albumentations-package-is-a-fast-and-%EF%AC%82exible-library-for-image-augmentations-with-many-various-207422f55a24 \"Medium article\"\n\n  [33e23d49]: https://github.com/albumentations-team/albumentations_examples/blob/master/notebooks/migrating_from_torchvision_to_albumentations.ipynb \"albumentation transform notebook\"\n\n  [49e22e45]: http://www.andrewjanowczyk.com/employing-the-albumentation-library-in-pytorch-workflows-bonus-helper-for-selecting-appropriate-values \"transforms for mask article\"\n\n  [f98e539c]: https://github.com/albumentations-team/albumentations_examples/blob/master/notebooks/example_multi_target.ipynb \"Multi Target Notebook\"\n\n  [8a740691]: https://github.com/albumentations-team/albumentations_examples/blob/master/notebooks/example_kaggle_salt.ipynb \"examples for segmentation - Notebook\"\n\n- [x] ",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Worked with minimal data of 12k images to test the waters before diving in.\n\n- [x] ",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9035546680869465
      ],
      "excerpt": "Train a model to predict, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8059270639693213,
        0.8141010752038044
      ],
      "excerpt": "Using 100 background images and 100 transparent foreground images(with alphachannel) created a dataset, which contains \n100 images of empty - office spaces, office reception lounges and office kitchen as background(bg) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8284613964305637
      ],
      "excerpt": "400k respective depth images \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8017285540429218,
        0.8017285540429218
      ],
      "excerpt": "    -- bg1.zip \n    -- bg2.zip \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8017285540429218,
        0.8017285540429218
      ],
      "excerpt": "    -- bg1.zip \n    -- bg2.zip \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8017285540429218,
        0.8017285540429218
      ],
      "excerpt": "    -- bg1.zip \n    -- bg2.zip \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9429703609925499
      ],
      "excerpt": "    print(len(dataset)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "                      shuffle=True) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8075430529612975,
        0.8131421743930939
      ],
      "excerpt": "        batch_samples = data.size(0) \n        data = data.view(batch_samples, data.size(1), -1) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9040797615468465
      ],
      "excerpt": "print(mean, std) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9097706680386619
      ],
      "excerpt": "bg_img = np.array(Image.open(str(bg_file))) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8334882644577463,
        0.8329015283702058
      ],
      "excerpt": "def read_img_from_zip(self, zip_name, file_name, array=True): \n    imgdata = zipfile.ZipFile(zip_name).read(file_name) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8456634692429335
      ],
      "excerpt": "        img = np.array(img) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8676042772388266
      ],
      "excerpt": "Data is split into train and test set with 80:20 split \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.845468170383905,
        0.8520186037847441,
        0.8287625358364002
      ],
      "excerpt": "tr_size = int(0.8 * len(dataset)) \ntst_size = len(dataset) - train_size \ntrain_dl, test_dl = torch.utils.data.random_split(dataset, [tr_size, tst_size]) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123204755090391
      ],
      "excerpt": "    num = pred.size(0) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8135309044556482
      ],
      "excerpt": "    num = targets.size(0)  #: Number of batches \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123204755090391
      ],
      "excerpt": "       num = pred.size(0) \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/hemanth346/mde_bs/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "## Objective :\n\nTrain a model to predict,\n1. mask for the fg object **(Background Subtraction)**\n2. Depth map for the image\n **(Monocular Depth Estimation)**\n\n  given 2 images\n   1. a background(bg) image and\n   1. Object inside the above background\n   > i.e. foreground*(fg)* overlaid on background (fg_bg)\n\n\n## Usecases\nCan be used mostly for security checks and in CC TV footage videos\n\n1. the mask can be used to check if a person is in the frame and the movement of the person\n1. the depth map can be used to determine if the person has entered any restricted zone\n\n> The same can be extended to parking lot scenario to assist with/monitor parking.\n\nTo be made real-time the inference time has to be very less and be able to run on the high fps.\n\nThe objective of this project is to get a working model with less number of parameters (and less size).\n\n\n## Dataset Overview\n  Own dataset has been curated/created for the task at hand.\n\nUsing 100 background images and 100 transparent foreground images(with alphachannel) created a dataset, which contains\n\n  - 100 images of empty - office spaces, office reception lounges and office kitchen as background(bg)\n\n![bg_images_thumbnail](https://github.com/hemanth346/mde_bs/blob/master/images/bg.png)\n\n  - 100 images of people as foreground(fg)\n\n![fg_images_thumbnail](https://github.com/hemanth346/mde_bs/blob/master/images/fg.png)\n\n  - 100 respective masks for fg\n\n  ![fg_masks_thumbnail](https://github.com/hemanth346/mde_bs/blob/master/images/fg_masks.png)\n\n  - 400k foreground with background(fg_bg) images\n    - Generated by overlaying foreground images(and their flips) top of background\n\n  ![fg_bg_thumbnail](https://github.com/hemanth346/mde_bs/blob/master/images/fg_bg.png)\n\n  - 400k respective masks for fg_bg images\n    - by overlaying corresponding foreground mask at the same postion on black canvas of bg shape\n\n  ![fg_bg_masks_thumbnail](https://github.com/hemanth346/mde_bs/blob/master/images/fg_bg_masks.png)\n\n  - 400k respective depth images\n  - Generated using pretrained weights of a SOTA model [pretrained weights][8ead022d]\n\n  ![depth_img_thumbnail](https://github.com/hemanth346/mde_bs/blob/master/images/depth_maps.png)\n\nComplete details can be found [here][0a34b972]\n\nDataset is stored google drive and can be accessed by this [link ][4e348988]\n\n  [4e348988]: https://drive.google.com \"Compressed Depth Data - 1.8Gb\"\n\nThe link redirects to a google drive folder which has 2 folders\n1. Dataset(Incomplete, due to i/o timeout in colab)\n1. Compressed_Dataset (will be using this. Size: 1.8G)\n\nWith below tree structure\n\n```\n- Dataset\n  -- inp_bg\n  -- inp_fg\n  -- inp_fg_masks\n  -- depth_maps\n    -- bg1\n      -- image1\n      -- image2\n      ...\n    -- bg2\n      -- image1\n      -- image2\n      ...\n    -- bg3\n    ...\n  -- fg_bg\n    -- bg1\n      -- image1\n      -- image2\n      ...\n    -- bg2\n      -- image1\n      -- image2\n      ...\n    -- bg3\n    ...\n  -- fg_bg_masks\n    -- bg1\n      -- image1\n      -- image2\n      ...\n    -- bg2\n      -- image1\n      -- image2\n      ...\n    -- bg3\n    ...\n```\n\n```\n- Compressed_Dataset\n  -- depth_maps\n    -- bg1.zip\n    -- bg2.zip\n    ...\n  -- fg_bg\n    -- bg1.zip\n    -- bg2.zip\n    ...\n  -- fg_bg_masks\n    -- bg1.zip\n    -- bg2.zip\n    ...\n```\n\n\n\nCalculated mean, std for each of the image set...\n\n> To calculate the standard deviation by averaging samples of the std from mini batches. While very close to the true std, it\u2019s not calculated exactly and can be leveraged if time/computation limitations. But in production settings where new data is added on daily basis this will work.\n\n    def get_batchwise_avg_mean_std(dataset, batch_size=50):\n        print(len(dataset))\n        loader = DataLoader(dataset,\n                          batch_size=batch_size,\n                          shuffle=True)\n        mean = 0.\n        std = 0.\n        nb_samples = 0.\n        for data in loader:\n            batch_samples = data.size(0)\n            data = data.view(batch_samples, data.size(1), -1)\n            mean += data.mean(2).sum(0)\n            std += data.std(2).sum(0)\n            nb_samples += batch_samples\n\n        mean /= nb_samples\n        std /= nb_samples\n        # return mean, std\n        print(mean, std)\n\n\nComplete details can be found at [dataset creation][bc7410ba]\n\n\n[8ead022d]: https://github.com/ialhashim/DenseDepth \"DenseDepth Pretrained weights\"\n\n[0a34b972]: https://github.com/hemanth346/eva4/tree/master/S15A \"Github Repo\"\n\n[bc7410ba]: https://github.com/hemanth346/eva4/tree/master/S15A \"Dataset Creation\"\n\n## Iterations\n\n#### 1.Get the setup right (for 2 problems)\n\nWorked with minimal data of 12k images to test the waters before diving in.\n\n- [x] ##### Dataloader\n\nEach of the folders in fg_bg, depth_maps, fg_bg_masks -  has separate zip file for every bg file. Images are read directly from the zip file to save on disk space and extraction time.\n\nList of all paths are stored obtained from fg_bg and when dataset is called by dataloader, corresponding images from other folders/zip files are obtained\n```\n  for file in os.listdir(fg_bg_dir):\n     fname = os.path.join(fg_bg_dir, file)\n     if zipfile.is_zipfile(fname):\n         self.fg_bg+=[x.filename for x in zipfile.ZipFile(fname).infolist()]\n\ndef __getitem__(self, index):\n   bg = self.fg_bg[index].split('_')[0]\n   bg_file = Path(self.data_root).joinpath('bg' ,bg+'.jpg')\n\n   bg_img = np.array(Image.open(str(bg_file)))\n   fg_bg_img = self.read_img_from_zip(f'{self.data_root}/fg_bg/{bg}.zip', self.fg_bg[index])\n   mask_img = self.read_img_from_zip(f'{self.data_root}/fg_bg_masks/{bg}.zip', self.fg_bg[index])\n   depth_img = self.read_img_from_zip(f'{self.data_root}/depth_maps/{bg}.zip', self.fg_bg[index])\n\n```\n\n_A helper function is added to the dataset class to convert the zipfile into PIL file for regular transforms and numpy array for albumentation transformations_\n```\ndef read_img_from_zip(self, zip_name, file_name, array=True):\n    imgdata = zipfile.ZipFile(zip_name).read(file_name)\n    img = Image.open(io.BytesIO(imgdata))\n    # img = img.convert(\"RGB\")\n    if array:\n        img = np.array(img)\n        return img\n    # PIL image\n    return img\n\n```\n\nData is split into train and test set with 80:20 split\n```\ntr_size = int(0.8 * len(dataset))\ntst_size = len(dataset) - train_size\n\ntrain_dl, test_dl = torch.utils.data.random_split(dataset, [tr_size, tst_size])\n```\n\n- [x] ##### Basic transforms is set for each of the image set.\n\n- Transformations used are : _RandomCrop, HorizontalFlip, Resize(64x64), Normalization_\n\nUsed [albumentation library][07ffb173] for transformations. Read about its advantages [here][8a687b30] and [here][3dd4fa4a]\n\nAlbumentations doesn't support loading PIL images directly and works with numpy arrays. _Hence we have to modify the dataset class accordingly_. I found this [notebook][33e23d49] very useful as a guide.\n\nAlbumentations also have support to pass masks/weights along with original image and get the same transforms applied on them [example here][49e22e45] or we can create our own [custom targets][f98e539c] to be sent for the compose, which are useful if we have multiple images not linked with each other but need same transforms.\n\nExamples of some of the transforms applied for segmentation problem can be found in this [notebook][8a740691]\n\n  [07ffb173]: https://github.com/albumentations-team/albumentations/ \"GitHub link\"\n  [8a687b30]: https://arxiv.org/pdf/1809.06839 \"arxiv paper\"\n  [3dd4fa4a]: https://medium.com/@ArjunThoughts/albumentations-package-is-a-fast-and-%EF%AC%82exible-library-for-image-augmentations-with-many-various-207422f55a24 \"Medium article\"\n\n  [33e23d49]: https://github.com/albumentations-team/albumentations_examples/blob/master/notebooks/migrating_from_torchvision_to_albumentations.ipynb \"albumentation transform notebook\"\n\n  [49e22e45]: http://www.andrewjanowczyk.com/employing-the-albumentation-library-in-pytorch-workflows-bonus-helper-for-selecting-appropriate-values \"transforms for mask article\"\n\n  [f98e539c]: https://github.com/albumentations-team/albumentations_examples/blob/master/notebooks/example_multi_target.ipynb \"Multi Target Notebook\"\n\n  [8a740691]: https://github.com/albumentations-team/albumentations_examples/blob/master/notebooks/example_kaggle_salt.ipynb \"examples for segmentation - Notebook\"\n\n- [x] ##### Setup basic model\n\n> Model is expected to give us a mask and depth map for foreground given 2 images.\n\n```\n Can we think of using a single fg_bg image and predict it as well.?\n\n   Will that change the problem scope, from trying to identify foreground object in given background image the model will be trying to find the mask and depth in any general setting..?\n\n   Will experiment with that as well if time permits\n ```\n\n- using inputs as\n    - [ ] only fg_bg\n    - [x] both bg and fg_bg\n\n- Predicting\n    - [x] Only mask\n    - [x] Only depth map\n    - [x] both mask and depth_maps  \n\nAfter looking at the 64x64 images, the images became pixelted and share edges are not available. RF for gradients was set at 5 pixels.\n\n> Since the output is also to be as the same size of input, we have to either do a Transpose conv or maintiain the size without doing any maxpool/stride 2 and no padding. Without any stride/maxpool - to get the receptive field of image size, in final layer, requires lot of convolutional layers.\n\n\nUsed Group convolutions with 2 groups of 3 channels each for first few convolutions. The idea being for the network to be able to learn for low level features from both the images intially.\n\nInitial network is created without using  transpose conv/de-conv. And accounting for RF, below is the brief network summary. Heavy model with more params\n\n```\nFinal layer RF - 70\n\nTotal params: 3,769,664\nTrainable params: 3,769,664\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.09\nForward/backward pass size (MB): 462.03\nParams size (MB): 14.38\nEstimated Total Size (MB): 476.51\n```\n- Training took around 40 mins per epoch on 16G Tesla P100\n\n\n<!-- Model such that the ground truth image size output is achieved in final layer. We can use **view** on tensor in final layer to get required output size or do tranpose/deConv convolvution.\n\nAs a baseline, we are using a ResNet 18 model with final layer. **Training Notebook** -->\n\n<!-- If transpose conv/upsampling is not used, no. of parameters required to reach the  receptive field of image is very high.\nIf the image sizes are reduced drastically it'll not serve as a baseline model. But since the objective is to get the setup correct, images have been resized to 64x64 using  transforms\n\nArchitecture diagram :\n**Todo add image**\n\n -->\n\n- [x] ##### Setup tensorboard on colab\n\nUsed below extension and magic function to access TensorBoard. This will open the tensorboard in the cell output\n```\n#Load extension\n%load_ext tensorboard\n\nlogs_base_dir = 'logs'\n%tensorboard --logdir {logs_base_dir}\n```\nTo write to tensorboard we can use summary writer from torch.utils\n```\n# TensorBoard support\nfrom torch.utils.tensorboard import SummaryWriter\n```\n\n- [x] ##### Make sure model is training\n\nModel was run for 2 epochs without any issue..\n\n\n<!-- ## Preprocessing and DataPipeline\n\nImages are read from zip files and resized to 112x112. Enough spatial information is retained for good enough details.  \n\n\n\n## ToDo:\n\n  Since the model is trying to predict dense ouput, the architecture should have de-Convolvution(Transpose Convolvution). -->\n\n#### 2. Get the basic skeleton\n\nDifferent model architectures were tried out **involving transpose convolutions**.\n\nArchitecture search was done and found that  similar problems involve one form or another of U-Net. Taken below two architectures as starting point\n\n  - U-Net [arxiv pdf;][f010ac45] [Github Code;][2922dbe6] [article][da501ee2]\n\n\n  - Deep Residual U-Net [arxiv pdf;][3aad73d3] [Github Code(Pytorch);][714b6f76]\n  [Notebook(Keras)][6dbd0345]\n\n\nArchitecutures experimented are\n\n1. [x] transpose convolutions using width of 2\n> ConvTranspose is convolution and has trainable kernels while Upsample is a simple interpolation (bilinear, nearest etc.).. Transpose has learning parameter while Up-sampling has no-learning parameters. Using Upsampling can make inference or training faster as it does not require to update weight or compute gradient, but since the input image is already pixelated, using transpose conv with cost of additional parameters and model size\n\n  ```\n  Final layer RF - 124",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "mde_bs",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "hemanth346",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/hemanth346/mde_bs/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Fri, 24 Dec 2021 19:25:02 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Different model architectures were tried out **involving transpose convolutions**.\n\nArchitecture search was done and found that  similar problems involve one form or another of U-Net. Taken below two architectures as starting point\n\n  - U-Net [arxiv pdf;][f010ac45] [Github Code;][2922dbe6] [article][da501ee2]\n\n\n  - Deep Residual U-Net [arxiv pdf;][3aad73d3] [Github Code(Pytorch);][714b6f76]\n  [Notebook(Keras)][6dbd0345]\n\n\nArchitecutures experimented are\n\n1. [x] transpose convolutions using width of 2\n> ConvTranspose is convolution and has trainable kernels while Upsample is a simple interpolation (bilinear, nearest etc.).. Transpose has learning parameter while Up-sampling has no-learning parameters. Using Upsampling can make inference or training faster as it does not require to update weight or compute gradient, but since the input image is already pixelated, using transpose conv with cost of additional parameters and model size\n\n  ```\n  Final layer RF - 124\n\n  ================================================================\n  Total params: 1,190,272\n  Trainable params: 1,190,272\n  Non-trainable params: 0\n  ----------------------------------------------------------------\n  Input size (MB): 0.09\n  Forward/backward pass size (MB): 139.16\n  Params size (MB): 4.54\n  Estimated Total Size (MB): 143.80\n  ```\n  - Training took around 15 mins per epoch on 16G Tesla P100\n\n1. [ ] Modified DeepResUNet architecuture.\n> Have to revisit the architecture, to reduce training memory.\n\n```\n================================================================\nTotal params: 1,765,421\nTrainable params: 1,765,421\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.09\nForward/backward pass size (MB): 43093.52\nParams size (MB): 6.73\nEstimated Total Size (MB): 43100.34\n----------------------------------------------------------------\n\n```\n\nProceeding with ConvTranspose as the results seems to be good with limited training\n\n",
      "technique": "Header extraction"
    }
  ]
}