{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2109.00859\n\nBlog link: https://blog.einstein.ai/codet5/\n\nThe code currently includes two pre-trained checkpoints ([CodeT5-small](https://huggingface.co/Salesforce/codet5-small",
      "https://arxiv.org/abs/1810.03993",
      "https://arxiv.org/abs/1909.09436"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find this code to be useful for your research, please consider citing.\n\n```\n@inproceedings{\n    wang2021codet5,\n    title={CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation}, \n    author={Yue Wang, Weishi Wang, Shafiq Joty, Steven C.H. Hoi},\n    booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021},\n    year={2021},\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{\n    wang2021codet5,\n    title={CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation}, \n    author={Yue Wang, Weishi Wang, Shafiq Joty, Steven C.H. Hoi},\n    booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021},\n    year={2021},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.99813152003196,
        0.9713679853829249
      ],
      "excerpt": "Title: CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation \nAuthors: Yue Wang, Weishi Wang \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8568217992087697
      ],
      "excerpt": "checkpoint (Salesforce/codet5-base-multi-sum) for \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "if isinstance(string, unicode): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8387611005341266
      ],
      "excerpt": "if not renderer.isValid(): \n",
      "technique": "Supervised classification"
    }
  ],
  "codeOfConduct": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://raw.githubusercontent.com/salesforce/codet5/main/CODE_OF_CONDUCT.md",
    "technique": "File Exploration"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/salesforce/CodeT5",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-08-16T04:03:46Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-24T07:39:37Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This repo provides the code for reproducing the experiments\nin [CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation](https://arxiv.org/pdf/2109.00859.pdf)\n. CodeT5 is a new pre-trained encoder-decoder model for programming languages, which is pre-trained on **8.35M**\nfunctions in 8 programming languages (Python, Java, JavaScript, PHP, Ruby, Go, C, and C#). In total, it achieves\nstate-of-the-art results on **14 sub-tasks** in a code intelligence benchmark - [CodeXGLUE](https://github.com/microsoft/CodeXGLUE).\n\nPaper link: https://arxiv.org/abs/2109.00859\n\nBlog link: https://blog.einstein.ai/codet5/\n\nThe code currently includes two pre-trained checkpoints ([CodeT5-small](https://huggingface.co/Salesforce/codet5-small)\nand [CodeT5-base](https://huggingface.co/Salesforce/codet5-base)) and scripts to fine-tine them on 4 generation tasks (\ncode summarization, code generation, translation, and refinement) plus 2 understanding tasks (code defect detection and\nclone detection) in CodeXGLUE. We also provide their fine-tuned checkpoints to facilitate the easy replication\nof our paper.\n\nIn practice, CodeT5 can be deployed as an AI-powered coding assistant to boost the productivity of software developers.\nAt Salesforce, we build an [AI coding assistant demo](https://github.com/salesforce/CodeT5/raw/main/codet5.gif) using\nCodeT5 as a VS Code plugin to provide three capabilities for Apex developers:\n\n- **Text-to-code generation**: generate code based on the natural language description.\n- **Code autocompletion**: complete the whole function of code given the target function name.\n- **Code summarization**: generate the summary of a function in natural language description.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9529924647664111
      ],
      "excerpt": "This is the official PyTorch implementation for the following EMNLP 2021 paper from Salesforce Research: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.908925214220865
      ],
      "excerpt": ", Shafiq Joty, and Steven C.H. Hoi \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9555670855189247
      ],
      "excerpt": "for all the downstream tasks covered in the paper. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9172419332344609
      ],
      "excerpt": "multilingual code summarzation. Below is how to use this model: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.92094628433545
      ],
      "excerpt": "We add a model card for CodeT5! Please reach out \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8087966486082133
      ],
      "excerpt": "CodeT5 is now in hugginface! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9040242032542738
      ],
      "excerpt": "and CodeT5-base) and do the inference: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9820424695921963
      ],
      "excerpt": "the sub_task to specify which specific datasets to fine-tine on. Below is the full list: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8761557052263931
      ],
      "excerpt": "| summarize | ruby/javascript/go/python/java/php | code summarization task on CodeSearchNet data with six PLs                                   | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8021472882015624,
        0.8695194656960828
      ],
      "excerpt": "model_dir: where to save fine-tuning checkpoints \nres_dir: where to save the performance results  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9860873597738159
      ],
      "excerpt": "data_num: how many data instances to use, the default -1 is for using the full data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8517711537451514
      ],
      "excerpt": "Please refer to the argument flags in configs.py for the full \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Code for CodeT5: a new code-aware pre-trained encoder-decoder model.",
      "technique": "GitHub API"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* [Pre-trained checkpoints](https://console.cloud.google.com/storage/browser/sfr-codet5-data-research/pretrained_models)\n* [Fine-tuning data](https://console.cloud.google.com/storage/browser/sfr-codet5-data-research/data)\n* [Fine-tuned checkpoints](https://console.cloud.google.com/storage/browser/sfr-codet5-data-research/finetuned_models)\n\nInstructions to download:\n\n```\n#: pip install gsutil\ncd your-cloned-codet5-path\n\ngsutil -m cp -r \"gs://sfr-codet5-data-research/pretrained_models\" .\ngsutil -m cp -r \"gs://sfr-codet5-data-research/data\" .\ngsutil -m cp -r \"gs://sfr-codet5-data-research/finetuned_models\" .\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/salesforce/codet5/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 41,
      "date": "Tue, 28 Dec 2021 00:37:59 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/salesforce/CodeT5/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "salesforce/CodeT5",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/salesforce/codet5/main/sh/exp_with_args.sh",
      "https://raw.githubusercontent.com/salesforce/codet5/main/evaluator/CodeBLEU/parser/build.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8891401888405708,
        0.9072170530151445
      ],
      "excerpt": "Get Involved \nGo to sh folder, set the WORKDIR in exp_with_args.sh to be your cloned CodeT5 repository path. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8548148548999783
      ],
      "excerpt": "Besides, you can specify: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8411756090130755
      ],
      "excerpt": "You can also revise the suggested \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9760994213089562
      ],
      "excerpt": "Note that we employ one A100 GPU for all fine-tuning experiments. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8801854956928516,
        0.8458751354831934
      ],
      "excerpt": "from transformers import RobertaTokenizer, T5ForConditionalGeneration \nif name == 'main': \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8432403596107199
      ],
      "excerpt": "text = \"\"\"def svg_to_image(string, size=None): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8359299706379749,
        0.8162116095251141,
        0.9537071715201267
      ],
      "excerpt": "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids \ngenerated_ids = model.generate(input_ids, max_length=20) \nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "from transformers import RobertaTokenizer, T5ForConditionalGeneration \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9204371811838056,
        0.8359299706379749
      ],
      "excerpt": "text = \"def greet(user): print(f'hello <extra_id_0>!')\" \ninput_ids = tokenizer(text, return_tensors=\"pt\").input_ids \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8023622814229289,
        0.9537071715201267
      ],
      "excerpt": "generated_ids = model.generate(input_ids, max_length=8) \nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8611731696305316
      ],
      "excerpt": "python run_exp.py --model_tag codet5_base --task summarize --sub_task python \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8537928159300355
      ],
      "excerpt": "arguments here or directly customize the exp_with_args.sh bash file. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/salesforce/CodeT5/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "BSD 3-Clause \"New\" or \"Revised\" License",
      "url": "https://api.github.com/licenses/bsd-3-clause"
    },
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "CodeT5",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "salesforce",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/salesforce/CodeT5/blob/main/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Pytorch 1.7.1\n- tensorboard 2.4.1\n- transformers 4.6.1\n- tree-sitter 0.2.2\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 275,
      "date": "Tue, 28 Dec 2021 00:37:59 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "language-model",
      "code-intelligence",
      "programming-language",
      "representation-learning",
      "nlp"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please create a GitHub issue if you have any questions, suggestions, requests or bug-reports. We welcome PRs!\n\n",
      "technique": "Header extraction"
    }
  ]
}