{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Special thanks to AI Research Lab in Neowiz Play Studio (http://neowizplaystudio.com/ko/) that allowed me to use resources for the project. If you need a white-background dataset, please send an e-mail to the address given below contact information.\n\n\uc704 \ud504\ub85c\uc81d\ud2b8 \uc9c4\ud589\uc744 \uc704\ud55c \ub9ac\uc18c\uc2a4\ub97c \uc0ac\uc6a9\ud558\ub3c4\ub85d \ud5c8\ub77d\ud574\uc8fc\uc2e0 \ub124\uc624\uc704\uc988\ud50c\ub808\uc774\uc2a4\ud29c\ub514\uc624 \ub0b4\uc758 AI\uc5f0\uad6c\uc18c\uc5d0\uac8c \uac10\uc0ac\ub4dc\ub9bd\ub2c8\ub2e4. \ud639\uc2dc \uc81c\uac00 \uc0ac\uc6a9\ud55c \ud558\uc580\uc0c9 \ubc30\uacbd\uc758 \uce90\ub9ad\ud130 \uc774\ubbf8\uc9c0\uac00 \ud544\uc694\ud558\uc2e0 \ubd84\ub4e4\uc774 \uc788\ub2e4\uba74 \uc544\ub798 \uc774\uba54\uc77c \uc8fc\uc18c\ub85c \ubb38\uc758\uc8fc\uc2dc\uba74 \uac10\uc0ac\ud558\uaca0\uc2b5\ub2c8\ub2e4. \n\n\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1701.07875</p>\n<p>(2"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9350518741113235
      ],
      "excerpt": "For previous works, visit: https://github.com/dabsdamoon/Anime-Colorization \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9105368110547479,
        0.9278824608274014,
        0.9115465676107721
      ],
      "excerpt": "\uc774\uc804 \uacb0\uacfc\ubb3c\uacfc \uad00\ub828\ud574\uc11c\ub294 \uc774 \ub9c1\ud06c\ub97c \ucc38\uc870\ud574\uc8fc\uc2dc\uba74 \uac10\uc0ac\ud558\uaca0\uc2b5\ub2c8\ub2e4 (https://github.com/dabsdamoon/Anime-Colorization). \n<p> (1) https://en.wikipedia.org/wiki/CIELAB_color_space</p> \n<p> (2) https://www.aces.edu/dept/fisheries/education/pond_to_plate/documents/ExplanationoftheLABColorSpace.pdf</p> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9586556701111782
      ],
      "excerpt": "GAN\uc5d0 \uad00\ub828\ud574\uc11c\ub294 \uc774\uc804 repo\uc5d0 \uc124\uba85\ud558\uc600\uae30 \ub54c\ubb38\uc5d0 \uc0dd\ub7b5\ud558\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4. \uc774\uc804 repo\uc5d0\uc11c\ub294 GAN\uc744 \uc81c\ub300\ub85c \uc0ac\uc6a9\ud558\uc9c0 \ubabb\ud588\ub358 \uac83 \uac19\uc544\uc11c, \uc774\ubc88\uc5d0 \ub2e4\uc2dc \ud55c\ubc88 \uc801\uc6a9\ud558\uc5ec \ub098\uc740 \uacb0\uacfc\ub97c \ub3c4\ucd9c\ud558\uace0\uc790 \ud558\uc600\uc2b5\ub2c8\ub2e4. \uc81c\uac00 \ucc38\uace0\ud55c \ucf54\ub4dc\ub4e4\uc740 (1)\uacfc (2)\uc5d0\uc11c \ucc38\uc870\ud558\uc600\uc2b5\ub2c8\ub2e4. \ub610\ud55c, GAN\uc744 \uc774\uc6a9\ud55c \ub9ce\uc740 colorization project\ub4e4\uc774 ResNet \ud639\uc740 U-Net \uad6c\uc870\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \uba87\uba87 \uac04\ub2e8\ud55c \uc2e4\ud5d8\ub4e4\uc744 \ud1b5\ud574, \ubcf8 colorization\uc5d0\uc11c\ub294 U-Net \uad6c\uc870\uac00 \uc880 \ub354 \ub098\uc740 \uac83 \uac19\uc544\uc11c U-Net \uad6c\uc870\ub97c \uc0ac\uc6a9\ud558\uc600\uc2b5\ub2c8\ub2e4 (\uc704 \uacb0\uc815\uc740 \uc81c \uac1c\uc778\uc801\uc778 \ud734\ub9ac\uc2a4\ud2f1\uc5d0\uc11c \uae30\ubc18\ud55c \uacb0\uc815\uc774\uae30 \ub54c\ubb38\uc5d0, \uacb0\uc815\uacfc \uad00\ub828\ud574\uc11c \uc870\uc5b8\ub4e4\uc774 \uc788\uc73c\uc2dc\ub2e4\uba74 \uc5b8\uc81c\ub4e0\uc9c0 \ub9d0\uc500\ud574\uc8fc\uc2dc\uba74 \uac10\uc0ac\ud558\uaca0\uc2b5\ub2c8\ub2e4).  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.973783563895836
      ],
      "excerpt": "<h5>U-Net architecture: https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net)</h5> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9944484218006108,
        0.8111036989382164,
        0.9944484218006108
      ],
      "excerpt": "<p>(1) https://arxiv.org/abs/1701.07875</p> \n<p>(2) https://vincentherrmann.github.io/blog/wasserstein/(/p> \n<p>(3) https://arxiv.org/pdf/1704.00028.pdf </p> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9741524126558423
      ],
      "excerpt": "<h5>Difference between WGAN weight-clipping and gradient penalty (https://arxiv.org/pdf/1704.00028.pdf) </h5> \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dabsdamoon/Anime-Colorization-v0.2",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "<p>facebook: https://www.facebook.com/dabin.moon.7 </p>\n<p>email: dabsdamoon@neowiz.com</p>\n",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-05-17T03:05:47Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-30T04:58:29Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9754846419406816
      ],
      "excerpt": "This repository is an upgrade version of Anime colorization I've done previously by using Keras. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8692651862443214
      ],
      "excerpt": "Since the size of danbooru image dataset is too big, only moeimouto-faces.zip dataset has been used. Notice that in this time I only selected images without background (white background) so that the model can detect facial parts more specifically. Same as the previous repo, I've converted RGB image to LAB image and use L channel for input and AB channel as output. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9826140100638772
      ],
      "excerpt": "After reviewing previous repo, I decided to make more clear definition of objective. My objective is <h5>\"To realistically color gray images!\"</h5> Note that I exclusively tried to use GAN since I want to color the gray image into many different color images. The example similar to my objective can be found in League of Legends, where the game sells chroma packs, a original scheme with different colorizations. In regular supervised learning method, however, one grayscale should have deterministic colorization label in order to train algorithms, and the trained algorithm would yield only the given deterministic colorizations. GAN algorithm, on the other hand, is semi-supervised learning method; in other words, the trained generator from GAN would yield colorization results that seem to be fit into the distribution of colorized images, not the specific colorization. Thus, I treid to use GAN for this project, and gave different noises for testing to observe how the trained generator colorizes a gray image differently.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9789109659219261
      ],
      "excerpt": "Since I've explained about GAN in previous repo, I'll skip the explanation. It seemed that previous repo did not reveal the true power of GAN, so I tried to apply GAN again for this colorization project, hoping the result gets better in this time. Codes I've referenced are from (1) and (2). Also, many colorization projects with GAN model use either ResNet or U-Net architecture for generator. After some experiments, it seems for me that U-Net architecture works better, so I decided to use U-Net architecture (Since the decision is based on my heuristics, it would be grateful if one can give any helpful advice either supporting or objecting the decision). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8991177558278289
      ],
      "excerpt": "Below are inputs(grayscale) and outputs(colored) of the trained generator using GAN (epoch = 8,192): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9554398150606609
      ],
      "excerpt": "Well, not as well-colored as \"chroma\", but at least the generator gave me some different colorization results reasonable for me. For example, the generator detects facial parts (eyes, hair, mouth, etc) and colorizes them differently. Also, it's interesting for me to see the colorization result of Taylor, an example out of distribution, seems better than the in-distribution example. Now, I'm going to apply different GAN model called WGAN-GP. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9929930458467465
      ],
      "excerpt": "As I mentioned in previous repo, WGAN(Wasserstein GAN) is one of those new versions by Arjovsky and Bottou (2017)(1), which applies Wasserstein loss instead of KL and JS divergence used for distance for the loss function in original GAN. In (1), the paper brings in the concept of weight clipping the weights in the discriminator in order to satisfy the Lipschitz constraint on the discriminator, a constraint that has to be satisfied in order to compute WGAN loss (For more information, visit (2) since I personally think it's so fat the best explanation I've read about the relationship between WGAN and Lipschitz constraint). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9685209221874038
      ],
      "excerpt": "However, WGAN also contains some problems such as capacity underuse or exploding/vanishing gradient problem: It's quite obvious that the discriminator will not be optimal if one clips its weight values into certain clipping value. Also, WGAN itself is quite sensitive about the clipping value, so exploding/vanishing gradient problem can be easily occurred. Thus, a new technique of gradient penalty has been introduced in (3), which directly constrains the gradient norm of the discriminator\u2019s output with respect to its input (This is very brief explanation of WGAN-GP, so I recommend reading (3) in order to fully understand WGAN-GP). Interesting point to note is that the discriminator in WGAN-GP does not use BatchNormalization layer since batch normalization makes correlation among inputs of the layer. If there is a correlation among inputs of layer, the gradient norm of the discriminator's output with respect to its input will be changed.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8991177558278289
      ],
      "excerpt": "Below are inputs(grayscale) and outputs(colored) of the trained generator using WGAN-GP (epoch = 2,048): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8617623819059805
      ],
      "excerpt": "It seems to me that the quality of colorization gets better after using WGAN-GP, but I cannot \"quantify\" how much the result is improved from GAN result. Still, it was worthwhile for me to run WGAN-GP codes and get comparably decent result for colorization. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9962774028563142
      ],
      "excerpt": "So far, I've done coloriztaion of grayscale image to color image. After I've been training algorithms many times with different parameters it seems that WGAN-GP generally seems to produce better results than GAN. However, WGAN-GP is quite slow, and sometimes GAN also produces seemingly better results! It's also ambiguous to define \"better colorization\", so I've learned that professional knowledge regarding colorization is also needed for the project. Also, there are some codes needed to be improved: For example, to apply RandomWeightedAverage, I gave global parameters (batch_size, img_shape_d), which needs to be changed whenever the size of batch is changed :(:( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Colorize gray images to RGB with selected dataset (white background only)",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dabsdamoon/Anime-Colorization-v0.2/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Thu, 23 Dec 2021 08:40:10 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/dabsdamoon/Anime-Colorization-v0.2/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "dabsdamoon/Anime-Colorization-v0.2",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8918974083095406
      ],
      "excerpt": "\uc774\uc804 \uacb0\uacfc\ubb3c\uacfc \uad00\ub828\ud574\uc11c\ub294 \uc774 \ub9c1\ud06c\ub97c \ucc38\uc870\ud574\uc8fc\uc2dc\uba74 \uac10\uc0ac\ud558\uaca0\uc2b5\ub2c8\ub2e4 (https://github.com/dabsdamoon/Anime-Colorization). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8976884542297275
      ],
      "excerpt": "One can obtain the dataset used from: https://www.kaggle.com/mylesoneill/tagged-anime-illustrations#danbooru-metadata.zip.  \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8160768449886273
      ],
      "excerpt": "\uc544\ub798\uc758 \uc774\ubbf8\uc9c0\ub4e4\uc740 GAN \ubaa8\ub378\ub85c \ub9cc\ub4e0 generator\uc758 input(\ud751\ubc31 \uc774\ubbf8\uc9c0)\uc640 output(\ucc44\uc0c9 \uc774\ubbf8\uc9c0) \uc785\ub2c8\ub2e4 (epoch = 8,192): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8174540907975313,
        0.8385995582866712
      ],
      "excerpt": "\ub610\ud55c \uc81c\uac00 \uc5b8\uae09\ud55c\ub300\ub85c \ud558\ub098\uc758 \ud751\ubc31 \uc774\ubbf8\uc9c0\ub97c 25\uac00\uc9c0\uc758 \ub2e4\ub978 noise\ub4e4\uc744 \uc0ac\uc6a9\ud558\uc5ec \ucc44\uc0c9\ud558\ub294 \uc2e4\ud5d8\uc744 \uc9c4\ud589\ud558\uc600\uc2b5\ub2c8\ub2e4. \ubcf8 \uc2e4\ud5d8\uc5d0\uc11c\ub294 \ub450 \uc7a5\uc758 \ub2e4\ub978 \ud751\ubc31 \uc774\ubbf8\uc9c0\ub97c \uc0ac\uc6a9\ud588\ub294\ub370\uc694, \ud558\ub098\ub294 training set\uc5d0 \uc874\uc7ac\ud558\ub294 \ud751\ubc31\uc774\ubbf8\uc9c0\uace0, \ub2e4\ub978 \ud558\ub098\ub294 \uc81c\uac00 \ud604\uc7ac \ud50c\ub808\uc774\uc911\uc778 \ube0c\ub77c\uc6b4\ub354\uc2a4\ud2b8  \uac8c\uc784\uc758 \uce90\ub9ad\ud130\uc778 \"\ud14c\uc77c\ub7ec\"\uc758 \uc774\ubbf8\uc9c0\uc785\ub2c8\ub2e4. \n<h5>ONE IN TRAINING DATASET</h5> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8174540907975313
      ],
      "excerpt": "\uc81c\uac00 \uc6d0\ud558\ub358 chroma \uae09\uc758 \ud004\ub9ac\ud2f0\ub294 \uc544\ub2c8\uc9c0\ub9cc... \ubb50 \uc801\uc5b4\ub3c4 generator\uac00 \uc774\ud574\uac00 \ub418\ub294 \ubc94\uc704\uc758 \ub2e4\uc591\ud55c \uc0c9\uce60 \uacb0\uacfc\ubb3c\uc744 \ub0b4\uc8fc\uc5c8\uae30 \ub54c\ubb38\uc5d0 \ub9cc\uc871\ud558\uaca0\uc2b5\ub2c8\ub2e4. \uc5bc\uad74 \ubd80\ubd84\ubd80\ubd84(\ub208, \uba38\ub9ac, \uc785 \ub4f1)\uc744 \uac01\uac01 \ub2e4\ub974\uac8c \uc0c9\uce60\ud558\ub294 \uac8c \uc778\uc0c1\uc801\uc774\ub124\uc694. \ub610\ud55c, training \ubd84\ud3ec\uc5d0 \uc18d\ud574\uc788\uc9c0 \uc54a\uc740 BrownDust\uc758 \ud14c\uc77c\ub7ec \uc774\ubbf8\uc9c0\ub97c \ubd84\ud3ec\uc5d0 \ud3ec\ud568\ub41c \uc560\ub2c8\uba54\uc774\uc158 \uce90\ub9ad\ud130 \uc774\ubbf8\uc9c0\ubcf4\ub2e4 \ub354 \uc798 \uc0c9\uce60 \uac83 \uac19\ub2e4\ub294 \ub290\ub08c\uc774 \ub4e4\uc5b4 \ud765\ubbf8\ub85c\uc6e0\uc2b5\ub2c8\ub2e4. \uc774\ubc88\uc5d0\ub294 \ub2e4\ub978 algorithm\uc778 WGAN-GP\uc744 \ud55c\ubc88 \uc0ac\uc6a9\ud558\uace0\uc790 \ud569\ub2c8\ub2e4. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8693674945303581
      ],
      "excerpt": "\uc544\ub798\uc758 \uc774\ubbf8\uc9c0\ub4e4\uc740 WGAN-GP \ubaa8\ub378\ub85c \ub9cc\ub4e0 generator\uc758 input(\ud751\ubc31 \uc774\ubbf8\uc9c0)\uc640 output(\ucc44\uc0c9 \uc774\ubbf8\uc9c0) \uc785\ub2c8\ub2e4 (epoch = 2,048): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8385995582866712
      ],
      "excerpt": "<h5>ONE IN TRAINING DATASET</h5> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/dabsdamoon/Anime-Colorization-v0.2/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "HTML",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Anime-Colorization v0.2",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Anime-Colorization-v0.2",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "dabsdamoon",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dabsdamoon/Anime-Colorization-v0.2/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Thu, 23 Dec 2021 08:40:10 GMT"
    },
    "technique": "GitHub API"
  }
}