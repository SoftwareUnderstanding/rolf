{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1704.04368"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8825511166992632
      ],
      "excerpt": "pointer-generator networks (See 2017) (\"copy attention\"), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9868090328425447,
        0.855281069365366
      ],
      "excerpt": "  See et al., 2017 \nTransformer networks: \"Attention is all you need\", Vaswani et al., 2017 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9372151656983196
      ],
      "excerpt": "| Transformer | 35.10  | 17.01  | 33.09 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8109194328925066
      ],
      "excerpt": "| Attention | MLP  | Copy  | Multi-head | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9156566588472104
      ],
      "excerpt": "| Batch size | 64 | 32 | 32 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9507374082549614
      ],
      "excerpt": "| Attention LSTM | 30.25 | 12.41 | 22.93 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8109194328925066
      ],
      "excerpt": "| Attention | MLP  | Copy  | Multi-head | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9156566588472104
      ],
      "excerpt": "| Batch size | 32 | 32 | 64 | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Iwontbecreative/Abstractive-summarization-OpenNMT",
    "technique": "GitHub API"
  },
  "contributors": {
    "confidence": [
      1.0
    ],
    "excerpt": "OpenNMT-py is a community developed project and we love developer contributions.\nBefore sending a PR, please do this checklist first:\n\nPlease run tools/pull_request_chk.sh and fix any errors. When adding new functionality, also add tests to this script. Included checks:\nflake8 check for coding style;\nunittest;\ncontinuous integration tests listed in .travis.yml.\n\n\nWhen adding/modifying class constructor, please make the arguments as same naming style as its superclass in pytorch.\nIf your change is based on a paper, please include a clear comment and reference in the code. \nIf your function takes/returns tensor arguments, please include assertions to document the sizes. See GlobalAttention.py for examples.",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-11-21T18:58:32Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-06-09T14:42:25Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9425060555511572,
        0.945830586114924,
        0.9964929339707235
      ],
      "excerpt": "This is a Pytorch \nimplementation of Abstractive summarization methods on top \nof OpenNMT. It features vanilla attention seq-to-seq LSTMs, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9564038665287212,
        0.9968029537584643
      ],
      "excerpt": "as well as instructions to run the networks on both the Gigaword and the CNN/Dayly Mail datasets. \nTable of Contents \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9061877863732657
      ],
      "excerpt": "Pointer-generator networks: \"Get To The Point: Summarization with Pointer-Generator Networks\", \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9905253283031081
      ],
      "excerpt": "The data can be either Gigaword or the CNN/Daily Mail dataset. For CNN/daily mail, it is also recommended to truncate inputs and outputs: -src_seq_length_trunc 400 -tgt_seq_length_trunc 100 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9406648835821553,
        0.9357716072275218
      ],
      "excerpt": "For CNN/Daily Mail, we assume access to such files. Otherwise, these can be built from https://github.com/OpenNMT/cnn-dailymail. \nValidation files are required and used to evaluate the convergence of the training. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8047709430974503
      ],
      "excerpt": "Internally the system never touches the words themselves, but uses these indices. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8193375913015665
      ],
      "excerpt": "The main relevant parameters to be changed for summarization are: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.870929496974284
      ],
      "excerpt": "rnn_size (256 or 512 work well in practice) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9307970559418677
      ],
      "excerpt": "The parameters for our trained models are described below \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8230307497673724
      ],
      "excerpt": "Perplexity and accuracy are not the main evaluation metrics for summarization. Rather, the field uses \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8486342357264876
      ],
      "excerpt": "To evaluate for rouge, we use files2rouge, which itself uses \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9807122748452334
      ],
      "excerpt": "In the case of CNN, evaluation should be done with beginning and end of sentences tokens stripped. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Abstractive summarization leveraging opennmt",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Iwontbecreative/Abstractive-summarization-OpenNMT/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 10,
      "date": "Thu, 23 Dec 2021 19:13:52 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Iwontbecreative/Abstractive-summarization-OpenNMT/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Iwontbecreative/Abstractive-summarization-OpenNMT",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Iwontbecreative/Abstractive-summarization-OpenNMT/master/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/Iwontbecreative/Abstractive-summarization-OpenNMT/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Iwontbecreative/Abstractive-summarization-OpenNMT/master/train_nb.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Iwontbecreative/Abstractive-summarization-OpenNMT/master/test/rebuild_test_models.sh",
      "https://raw.githubusercontent.com/Iwontbecreative/Abstractive-summarization-OpenNMT/master/test/pull_request_chk.sh",
      "https://raw.githubusercontent.com/Iwontbecreative/Abstractive-summarization-OpenNMT/master/docs/generate.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8230143485334281
      ],
      "excerpt": "The following models are implemented: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9455909857143592
      ],
      "excerpt": "For Gigaword, download the data from : https://github.com/harvardnlp/sent-summary. Then, extract it (tar -xzf summary.tar.gz) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.873043748860673
      ],
      "excerpt": "gpuid (0 for the first gpu, -1 if on cpu) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9848867486995735,
        0.9984079357284334,
        0.9893272198983933,
        0.9906248903846466
      ],
      "excerpt": "Installation instructions: \npip install git+https://github.com/tagucci/pythonrouge.git \ngit clone https://github.com/pltrdy/files2rouge.git \ncd files2rouge \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9820226428242687
      ],
      "excerpt": "python setup.py install \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8245539886860519
      ],
      "excerpt": "Pretrained models \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9432137019536753
      ],
      "excerpt": "python preprocess.py -train_src data/src-train.txt -train_tgt data/tgt-train.txt -valid_src data/src-val.txt -valid_tgt data/tgt-val.txt -save_data data/demo -share_vocab -dynamic_dict -src_vocab_size 50000 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8064430517663064
      ],
      "excerpt": "The data consists of parallel source (src) and target (tgt) data containing one example per line with tokens separated by a space: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.922342277287627,
        0.8442794823728994,
        0.8413839127117699
      ],
      "excerpt": "demo.train.pt: serialized PyTorch file containing training data \ndemo.valid.pt: serialized PyTorch file containing validation data \ndemo.vocab.pt: serialized PyTorch file containing vocabulary data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9428639780773738
      ],
      "excerpt": "python train.py -data data/demo -save_model demo_model -share_embeddings \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9638864199702886
      ],
      "excerpt": "python translate.py -model demo-model_epochX_PPL.pt -src data/src-test.txt -o output_pred.txt -beam_size 10 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python setup_rouge.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8626541410348892
      ],
      "excerpt": "| Transformer | 35.10  | 17.01  | 33.09 | \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Iwontbecreative/Abstractive-summarization-OpenNMT/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2017 OpenNMT\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Abstractive summarization with OpenNMT-py\n\nThis is a [Pytorch](https://github.com/pytorch/pytorch)\nimplementation of Abstractive summarization methods on top\nof [OpenNMT](https://github.com/OpenNMT/OpenNMT). It features vanilla attention seq-to-seq LSTMs,\n[pointer-generator networks (See 2017)](https://arxiv.org/abs/1704.04368) (\"copy attention\"),\nas well as [transformer networks  (Vaswani 2017)](https://arxiv.org/pdf/1706.03762.pdf)  (\"attention is all you need\")\nas well as instructions to run the networks on both the Gigaword and the CNN/Dayly Mail datasets.\n\n\nTable of Contents",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Abstractive-summarization-OpenNMT",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Iwontbecreative",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Iwontbecreative/Abstractive-summarization-OpenNMT/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```bash\npip install -r requirements.txt\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 43,
      "date": "Thu, 23 Dec 2021 19:13:52 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "nlp",
      "deep-learning",
      "abstractive-summarization",
      "summarization"
    ],
    "technique": "GitHub API"
  }
}