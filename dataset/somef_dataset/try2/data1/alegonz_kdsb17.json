{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1406.4729"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9563634483670684
      ],
      "excerpt": "For the source code and requirements please refer to Repository info. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/alegonz/kdsb17",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-10-08T07:20:09Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-16T15:53:09Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This is an attempt at the classification task featured in the [Kaggle Data Science Bowl 2017](https://www.kaggle.com/c/data-science-bowl-2017). The task consists on predicting from CT lung scans whether a patient will develop cancer or not within a year. This is a particularly challenging problem given the very high dimensionality of data and the very limited number of samples.\n\nThe competition saw many creative approaches, such as those reported by the winning entries [here](https://github.com/lfz/DSB2017) (1st place), [here](http://blog.kaggle.com/2017/06/29/2017-data-science-bowl-predicting-lung-cancer-2nd-place-solution-write-up-daniel-hammack-and-julian-de-wit/) (2nd place) and [here](http://blog.kaggle.com/2017/05/16/data-science-bowl-2017-predicting-lung-cancer-solution-write-up-team-deep-breath/) (9th place). These approaches have in common that:\n\n1. they are based on deep CNNs;\n2. leverage external data, in particular the [LUNA dataset](https://luna16.grand-challenge.org/);\n3. make extensive use of ensemble of models.\n\nWhat I'm attempting here is a rather more \"purist\" (for lack of a better word) approach that uses no ensemble models and no external data. The purpose of this is simply to explore the possibility of achieving a decent classification accuracy using a single model and using solely the provided data. The current attempt consists of a combination of two neural networks:\n\n* Gaussian Mixture Convolutional AutoEncoder (GMCAE): A convolutional autoencoder cast as Mixture Density Network ([Bishop, 1994](https://www.microsoft.com/en-us/research/publication/mixture-density-networks/)). This network is used to learn high-level features of patches of lung scans (3D arrays of CT scans in Hounsfield Units), using unsupervised learning and maximum likelihood on a mixture of Gaussians.\n* CNN classifier: Performs binary classification upon the features extracted by the encoding layers of the GMCAE.\n\n![model_overview](illustrations/model_overview.png \"Model overview\")\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8295531076401937,
        0.8408782033455487,
        0.9616095896845046,
        0.9225520610043798,
        0.9334944395940917,
        0.9788626235364143
      ],
      "excerpt": "This is still work in progress. \nFor the source code and requirements please refer to Repository info. \nThe Keras/TensorFlow implementation of the Gaussian Mixture Negative Log-Likelihood loss is in losses.py. It is created by calling the build_gmd_log_likelihood(c, m) function. \nFor details refer to this section and the source code. \nCustom Keras objects that define the ShiftedELU, log_softmax to parametrize  the log-priors and variances are implemented here and here. \nThe data consists on a set of CT scan slices of 1,595 patients stored in DICOM format. For each patient a 3D array is constructed by merging the slices, applying appropriate preprocessing, and extracting the lung area. Each patient's 3D array constitutes a sample and it has associated a binary label indicating whether it was diagnosed with cancer or not within a year. The details of preprocessing are explained here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8858405578522274,
        0.9978328999919907,
        0.9862555810054443,
        0.9580504481675161,
        0.9928098608505366,
        0.9393569789120667,
        0.9846935324392817,
        0.9947226450548979
      ],
      "excerpt": "The current architecture of both networks is summarized in the figure below: \nThe purpose of this network is to learn features from the 3D CT lung arrays that could be transferred to the second network for classification. This is done through unsupervised learning using an autoencoder with a reconstruction task. \nAs a reconstruction objective for the autoencoder, one could attempt to minimize a MSE objective, but this would fail because the CT scan voxels have a multimodal distribution (as shown in here) and a MSE objective would tend to predict the average of the distribution and thus likely yield meaningless predictions. This is because a MSE objective is equivalent to maximizing the log-likelihood assuming a (uni-modal) Gaussian distribution for the conditional probability of the output given the data. \nThus, the conditional probability is instead formulated as a mixture of Gaussians as below: \nWhere m is the number of gaussians in the mixture, and c is the number of output dimensions (number of voxels in the reconstruction). The GMCAE is trained to produce outputs that determine the parameters alpha (priors), sigma^2 (variances) and mu (means) of the mixture of Gaussians. alpha, sigma and mu are functions of x and the network parameters theta. Since we are doing reconstruction, t=x in this case. Specifically, the network is trained to minimize the following loss function: \nIn this formulation, the priors and normalizing constants of the Gaussians are moved inside the exponential function, allowing to represent the loss as a logsumexp and improve numerical stability. \nThe task of this network is to classify the patients upon the features transferred from the GMCAE. The output is a single sigmoid unit, and the network is trained to minimize the Log Loss. Since the model should be able to handle arrays of variable size, a Spatial Pyramid Pooling layer (He, 2014) is used to interface between the convolutional and fully-connected layers. \nAn example of the reconstruction computed with the GMCAE using a mixture of 4 Gaussians is shown below. The 24 slice patches on the left are the original, and those on the right are the reconstructions produced by the model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9677859593844496
      ],
      "excerpt": "So far a validation loss of around 0.57 and an accuracy of about 74% (par with chance level), which is still quite far from the winning entries (logloss around 0.40). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9438412012991644,
        0.9001234255896895,
        0.9111886130139001
      ],
      "excerpt": "It's hard to stabilize the gradients. So far, I've been able to control the gradients with small learning rates and/or gradient norm clipping. \nI also tried to parametrize directly the inverse variance but it wasn't helpful. \nAlso tried fixing the variances to a constant value (determined empirically) but that didn't work either. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8229892841811826,
        0.9759532127978714
      ],
      "excerpt": "The Gaussians in the mixture are densities, so point estimates of the likelihood can yield negative values if the variances are small enough. \nHaving variable variances and priors makes it difficult to estimate a lower bound of the loss function, which also makes difficult to know how much the model is underfitting the data. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9585800423970959
      ],
      "excerpt": "Contains the custom modules for data pre-processing, and building and training the models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8772143767374705
      ],
      "excerpt": "Contains the scripts to preprocess the data, train the models and predict. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9616095896845046,
        0.8179068323422662
      ],
      "excerpt": "The Keras/TensorFlow implementation of the Gaussian Mixture Negative Log-Likelihood loss is in losses.py. It is created by calling the build_gmd_log_likelihood(c, m) function. \n  * build_gmd_log_likelihood(c, m) takes two parameters: the number of output dimensions c and the number of Gaussians in the mixture m. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Gaussian Mixture Convolutional AutoEncoder applied to CT lung scans from the Kaggle Data Science Bowl 2017",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/alegonz/kdsb17/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 9,
      "date": "Sat, 25 Dec 2021 10:06:24 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/alegonz/kdsb17/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "alegonz/kdsb17",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/alegonz/kdsb17/master/scripts/run_and_log.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* Input:\n  * Full 3D array of lung area.\n  * Data augmentation is performed by random rotations/mirroring of the sub-arrays. Since a cube has 48 symmetries this allows a 48-fold augmentation (ignoring the effects of gravity).\n* Output:\n  * Probability of being diagnosed with cancer within a year.\n* Loss function\n  * Log loss (aka binary crossentropy).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "* Input:\n  * A 3D sub-array corresponding to a cube patch of fixed size, big enough to contain a lung nodule.\n  * Currently the sub-arrays are set as 32x32x32 arrays corresponding to a cube of 3.2 mm.\n  * Data augmentation is performed by random rotations/mirroring of the sub-arrays. Since a cube has 48 symmetries this allows a 48-fold augmentation (ignoring the effects of gravity).\n* Outputs:\n  * **log(alpha)**: A vector of **m** elements that correspond to the log priors of each Gaussian in the mixture. The log priors are parametrized with a LogSoftmax activiation.\n  * **sigma^2**: A vector of **m** elements that correspond to the variances of each Gaussian. The original paper of Mixure Density Networks suggests parametrizing the variances with an exponential activation function. However, an exponential function is prone to numerical instability, and here instead use a ShiftedELU activation. This is just the ELU activation with an added constant of 1, such that the output is always greater than zero. [Another work on Mixture Density Networks also came up with this idea before](https://github.com/axelbrando/Mixture-Density-Networks-for-distribution-and-uncertainty-estimation).\n  * **mu**: A 4-D tensor with the means (array reconstructions) of each Gaussian. This is parametrized with a linear activation function.\n* Loss function\n  * Negative Log likelihood given by the above equation.\n  \n\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8950429067797173,
        0.8543184133080509
      ],
      "excerpt": "Train: 1397 samples (1 sample = 1 patient) \nTraining (~80%): 1117 samples (291 cancerous) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8580303084160875
      ],
      "excerpt": "Test: 198 samples \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8031758431757546
      ],
      "excerpt": "  * The target data must be of shape (samples, c). \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/alegonz/kdsb17/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Gaussian Mixture Convolutional AutoEncoder for feature learning on 3D CT lung scan data (Keras/TensorFlow implementation)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "kdsb17",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "alegonz",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/alegonz/kdsb17/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* Python 3\n* Keras 2.0.6\n* tensorflow-gpu 1.2.1\n* numpy 1.13.0\n* scipy 0.19.1\n* pydicom 0.9.9\n* CUDA Version 8.0.61\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 17,
      "date": "Sat, 25 Dec 2021 10:06:24 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "keras-tensorflow",
      "mixture-density-networks",
      "convolutional-autoencoder",
      "kaggle",
      "lung-cancer-detection",
      "medical-images",
      "gaussian-mixture-models",
      "python"
    ],
    "technique": "GitHub API"
  }
}