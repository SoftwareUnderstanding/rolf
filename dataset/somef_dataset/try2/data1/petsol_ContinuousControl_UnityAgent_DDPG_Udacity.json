{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1509.02971\n\n# Results\nThe DDPG model implemented here converged to the target value of 30. at episode 508, with a value of 30.0793.\n![Continuous Control Convergence Graph](https://github.com/petsol/ContinuousControl_UnityAgent_DDPG_Udacity/blob/master/ContinuousControl_convergence.png?raw=true"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9929598068660475
      ],
      "excerpt": "Continuous control with deep reinforcement learning (article): https://arxiv.org/abs/1509.02971 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/petsol/ContinuousControl_UnityAgent_DDPG_Udacity",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-06-10T10:59:04Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-06-10T23:49:30Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This is a Project for Continuous Control Deep Reinforcement Learning Nanodegree @ Udacity. The Task is to follow a target with a multijoint robot arm. A DDPG model is applied to accomplish the task. The model achieves the desired +30 score on average per episode.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9882963707350055,
        0.9945288027888786,
        0.9832848295727195
      ],
      "excerpt": "The environment is describen in detail at https://github.com/udacity/deep-reinforcement-learning/tree/master/p2_continuous-control repository. For this project the single arm task is used, where a single 2-join robot arm has to learn to follow a sphere target region. That moves around the robot arm with varying speeds, but stays within reach for the entire episode. The environment is called Reacher. The Environment is a UnityAgent environment. See video for untrained model: https://www.youtube.com/watch?v=bhsVB0QKvoQ (Vincent Tam) \nThe observation space has 33 variables representing position, rotation, velocity, and angular velocities of the arm. The action vector contains four continuous values, representing the torque applicable to the two joints of the arm. These values always fall between [-1, 1]. There is no mentioning in the original repository how much steps or other constraints lead to episode termination, but through testing it seems that 1000 steps is one episode. The reward for each step spent in the goal location is rewarded with +0.1. The environment is considered solved if the last 100 episodes' mean score is over +30. \nThis is a continuous input continuous output learning problem and this solution applies DDPG (Deep Deterministic Policy Gradient) a specialized Actor Critic method developed for Continuous Control problems. DDPG applies an Actor and a Critic. The actor continuously optimalized through maximalizing the critic value for a specific action-set given in a specific state by the actor, whereas the critic is optimalized through the actor's target network and its own target network, to maintain relative independence of the optimalizing process. The target networks are updated through soft updating at each step. As a result the actor network in itself achieves a policy that approximates the optimal policy for the given task. For further implementation details see the Report.md file in this Repository. For further references see sources below.   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9337825043084174
      ],
      "excerpt": "The DDPG model implemented here converged to the target value of 30. at episode 508, with a value of 30.0793. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "This is a Project for Continuous Control Deep Reinforcement Learning Nanodegree @ Udacity. The Task is to follow a target with a multijoint robot arm. A DDPG model is applied to accomplish the task. The model achieves the desired +30 score on average per episode.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/petsol/ContinuousControl_UnityAgent_DDPG_Udacity/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 28 Dec 2021 09:31:38 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/petsol/ContinuousControl_UnityAgent_DDPG_Udacity/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "petsol/ContinuousControl_UnityAgent_DDPG_Udacity",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/petsol/ContinuousControl_UnityAgent_DDPG_Udacity/master/Continuous_Control.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9616021990192716,
        0.9700625406653584,
        0.9322609392449874
      ],
      "excerpt": "Download the appropriate Unity environment for your system from above github repository. \nCreate a python 3.6* environment, containing the following packages  \npytorch 0.4* \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9405862950779037,
        0.9703897759887624,
        0.8627670300380597
      ],
      "excerpt": "Clone repo \nUpdate environment path in you copy (Continuous_Control.ipynb) \nRun the Continuous_Control.ipynb notebook \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8272199920132205
      ],
      "excerpt": "The DDPG model implemented here converged to the target value of 30. at episode 508, with a value of 30.0793. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8009227257818361
      ],
      "excerpt": "Run the Continuous_Control.ipynb notebook \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/petsol/ContinuousControl_UnityAgent_DDPG_Udacity/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Introduction",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "ContinuousControl_UnityAgent_DDPG_Udacity",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "petsol",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/petsol/ContinuousControl_UnityAgent_DDPG_Udacity/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 28 Dec 2021 09:31:38 GMT"
    },
    "technique": "GitHub API"
  }
}