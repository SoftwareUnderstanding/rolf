{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2006.03654"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\n@misc{he2020deberta,\n    title={DeBERTa: Decoding-enhanced BERT with Disentangled Attention},\n    author={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},\n    year={2020},\n    eprint={2006.03654},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{he2020deberta,\n    title={DeBERTa: Decoding-enhanced BERT with Disentangled Attention},\n    author={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},\n    year={2020},\n    eprint={2006.03654},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeOfConduct": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://raw.githubusercontent.com/huberemanuel/DeBERTa/master/CODE_OF_CONDUCT.md",
    "technique": "File Exploration"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/huberemanuel/DeBERTa",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Pengcheng He(penhe@microsoft.com), Xiaodong Liu(xiaodl@microsoft.com), Jianfeng Gao(jfgao@microsoft.com), Weizhu Chen(wzchen@microsoft.com)\n\n",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-11-19T14:39:23Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-06-28T22:28:41Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "DeBERTa (Decoding-enhanced BERT with disentangled attention) improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions. Second, an enhanced mask decoder is used to replace the output softmax layer to predict the masked tokens for model pretraining. We show that these two techniques significantly improve the efficiency of model pre-training and performance of downstream tasks.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9983187843173201,
        0.8898001110460524
      ],
      "excerpt": "This repository is the official implementation of  DeBERTa: Decoding-enhanced BERT with Disentangled Attention \nWe released the pre-trained models, source code, and fine-tuning scripts to reproduce some of the experimental results in the paper. You can follow similar scripts to apply DeBERTa to your own experiments or applications. Pre-training scripts will be released in the next step. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.908268826703693,
        0.9262999007619288
      ],
      "excerpt": "- Large MNLI: Large model fine-tuned with MNLI task \n- Base MNLI: Base model fine-tuned with MNLI task \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8910453251599003,
        0.9736009373834602
      ],
      "excerpt": "Our fine-tuning experiments are carried on half a DGX-2 node with 8x32 V100 GPU cards, the results may vary due to different GPU models, drivers, CUDA SDK versions, using FP16 or FP32, and random seeds.  \nWe report our numbers based on multple runs with different random seeds here. Here are the results from the Large model: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9280046039349895
      ],
      "excerpt": "And here are the results from the Base model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8199837720617734
      ],
      "excerpt": "a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8073737513954784,
        0.9783267603835809
      ],
      "excerpt": "This project has adopted the Microsoft Open Source Code of Conduct. \nFor more information see the Code of Conduct FAQ or \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "The implementation of DeBERTa",
      "technique": "GitHub API"
    }
  ],
  "documentation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "https://deberta.readthedocs.io/",
      "technique": "Regular expression"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/huberemanuel/DeBERTa/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Wed, 29 Dec 2021 07:10:38 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/huberemanuel/DeBERTa/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "huberemanuel/DeBERTa",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/huberemanuel/DeBERTa/master/docker/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/huberemanuel/DeBERTa/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/huberemanuel/DeBERTa/master/ABSA-DeBERTa/DeBERTa_Experiment.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/huberemanuel/DeBERTa/master/run_docker.sh",
      "https://raw.githubusercontent.com/huberemanuel/DeBERTa/master/experiments/glue/sst2_base.sh",
      "https://raw.githubusercontent.com/huberemanuel/DeBERTa/master/experiments/glue/sst2_large.sh",
      "https://raw.githubusercontent.com/huberemanuel/DeBERTa/master/experiments/glue/stsb_base.sh",
      "https://raw.githubusercontent.com/huberemanuel/DeBERTa/master/experiments/glue/mnli_large.sh",
      "https://raw.githubusercontent.com/huberemanuel/DeBERTa/master/experiments/glue/rte_large.sh",
      "https://raw.githubusercontent.com/huberemanuel/DeBERTa/master/experiments/glue/mnli_base.sh",
      "https://raw.githubusercontent.com/huberemanuel/DeBERTa/master/experiments/glue/rte_base.sh",
      "https://raw.githubusercontent.com/huberemanuel/DeBERTa/master/experiments/glue/qnli_large.sh",
      "https://raw.githubusercontent.com/huberemanuel/DeBERTa/master/experiments/glue/qqp_large.sh",
      "https://raw.githubusercontent.com/huberemanuel/DeBERTa/master/experiments/glue/cola_large.sh",
      "https://raw.githubusercontent.com/huberemanuel/DeBERTa/master/experiments/glue/stsb_large.sh",
      "https://raw.githubusercontent.com/huberemanuel/DeBERTa/master/experiments/glue/mrpc_large.sh",
      "https://raw.githubusercontent.com/huberemanuel/DeBERTa/master/experiments/glue/mnli_xlarge.sh",
      "https://raw.githubusercontent.com/huberemanuel/DeBERTa/master/experiments/glue/qqp_base.sh",
      "https://raw.githubusercontent.com/huberemanuel/DeBERTa/master/experiments/utils/train.sh",
      "https://raw.githubusercontent.com/huberemanuel/DeBERTa/master/experiments/ner/ner_large.sh",
      "https://raw.githubusercontent.com/huberemanuel/DeBERTa/master/docker/build.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "`pip install deberta`\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8727712628922192
      ],
      "excerpt": "Our fine-tuning experiments are carried on half a DGX-2 node with 8x32 V100 GPU cards, the results may vary due to different GPU models, drivers, CUDA SDK versions, using FP16 or FP32, and random seeds.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8912804829461899
      ],
      "excerpt": "|MNLI xlarge|   experiments/glue/mnli_xlarge.sh|  91.5/91.4 +/-0.1|   2.5h| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8003352366805131
      ],
      "excerpt": "a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/huberemanuel/DeBERTa/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook",
      "Shell",
      "Dockerfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'    MIT License\\n\\n    Copyright (c) Microsoft Corporation.\\n\\n    Permission is hereby granted, free of charge, to any person obtaining a copy\\n    of this software and associated documentation files (the \"Software\"), to deal\\n    in the Software without restriction, including without limitation the rights\\n    to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\n    copies of the Software, and to permit persons to whom the Software is\\n    furnished to do so, subject to the following conditions:\\n\\n    The above copyright notice and this permission notice shall be included in all\\n    copies or substantial portions of the Software.\\n\\n    THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\n    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\n    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\n    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\n    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\n    OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\n    SOFTWARE\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "DeBERTa",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "huberemanuel",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/huberemanuel/DeBERTa/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Linux system, e.g. Ubuntu 18.04LTS\n- CUDA 10.0\n- pytorch 1.3.0\n- python 3.6\n- bash shell 4.0\n- curl\n- docker (optional)\n- nvidia-docker2 (optional)\n\nThere are several ways to try our code,\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "For glue tasks, \n1. Get the data\n``` bash\ncache_dir=/tmp/DeBERTa/\ncurl -J -L https://raw.githubusercontent.com/nyu-mll/jiant/master/scripts/download_glue_data.py | python3 - --data_dir $cache_dir/glue_tasks\n```\n\n2. Run task\n\n``` bash\ntask=STS-B \nOUTPUT=/tmp/DeBERTa/exps/$task\nexport OMP_NUM_THREADS=1\npython3 -m DeBERTa.apps.train --task_name $task --do_train  \\\n  --data_dir $cache_dir/glue_tasks/$task \\\n  --eval_batch_size 128 \\\n  --predict_batch_size 128 \\\n  --output_dir $OUTPUT \\\n  --scale_steps 250 \\\n  --loss_scale 16384 \\\n  --accumulative_update 1 \\  \n  --num_train_epochs 6 \\\n  --warmup 100 \\\n  --learning_rate 2e-5 \\\n  --train_batch_size 32 \\\n  --max_seq_len 128\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 29 Dec 2021 07:10:38 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Docker is the recommended way to run the code as we already built every dependency into the our docker [bagai/deberta](https://hub.docker.com/r/bagai/deberta) and you can follow the [docker official site](https://docs.docker.com/engine/install/ubuntu/) to install docker on your machine.\n\nTo run with docker, make sure your system fullfil the requirements in the above list. Here are the steps to try the GLUE experiments: Pull the code, run `./run_docker.sh` \n, and then you can run the bash commands under `/DeBERTa/experiments/glue/`\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Pull the code and run `pip3 install -r requirements.txt` in the root directory of the code, then enter `experiments/glue/` folder of the code and try the bash commands under that folder for glue experiments.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "``` Python\n\n#: To apply DeBERTa into your existing code, you need to make two changes on your code,\n#: 1. change your model to consume DeBERTa as the encoder\nfrom DeBERTa import deberta\nimport torch\nclass MyModel(torch.nn.Module):\n  def __init__(self):\n    super().__init__()\n    #: Your existing model code\n    self.bert = deberta.DeBERTa(pre_trained='base') #: Or 'large' or 'base_mnli' or 'large_mnli'\n    #: Your existing model code\n    #: do inilization as before\n    #: \n    self.bert.apply_state() #: Apply the pre-trained model of DeBERTa at the end of the constructor\n    #:\n  def forward(self, input_ids):\n    #: The inputs to DeBERTa forward are\n    #: `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length] with the word token indices in the vocabulary\n    #: `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token types indices selected in [0, 1]. \n    #:    Type 0 corresponds to a `sentence A` and type 1 corresponds to a `sentence B` token (see BERT paper for more details).\n    #: `attention_mask`: an optional parameter for input mask or attention mask. \n    #:   - If it's an input mask, then it will be torch.LongTensor of shape [batch_size, sequence_length] with indices selected in [0, 1]. \n    #:      It's a mask to be used if the input sequence length is smaller than the max input sequence length in the current batch. \n    #:      It's the mask that we typically use for attention when a batch has varying length sentences.\n    #:   - If it's an attention mask then if will be torch.LongTensor of shape [batch_size, sequence_length, sequence_length]. \n    #:      In this case, it's a mask indicate which tokens in the sequence should be attended by other tokens in the sequence. \n    #: `output_all_encoded_layers`: whether to output results of all encoder layers, default, True\n    encoding = self.bert(input_ids)[-1]\n\n#: 2. Change your tokenizer with the the tokenizer built in DeBERta\nfrom DeBERTa import deberta\ntokenizer = deberta.GPT2Tokenizer()\n#: We apply the same schema of special tokens as BERT, e.g. [CLS], [SEP], [MASK]\nmax_seq_len = 512\ntokens = tokenizer.tokenize('Examples input text of DeBERTa')\n#: Truncate long sequence\ntokens = tokens[:max_seq_len -2]\n#: Add special tokens to the `tokens`\ntokens = ['[CLS]'] + tokens + ['[SEP]']\ninput_ids = tokenizer.convert_tokens_to_ids(tokens)\ninput_mask = [1]*len(input_ids)\n#: padding\npaddings = max_seq_len-len(input_ids)\ninput_ids = input_ids + [0]*paddings\ninput_mask = input_mask + [0]*paddings\nfeatures = {\n'input_ids': torch.tensor(input_ids, dtype=torch.int),\n'input_mask': torch.tensor(input_mask, dtype=torch.int)\n}\n\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}