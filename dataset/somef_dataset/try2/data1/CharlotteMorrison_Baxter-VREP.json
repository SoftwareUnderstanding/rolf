{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1511.05952",
      "https://arxiv.org/abs/1802.09477",
      "https://arxiv.org/abs/1802.09477, 2018.](https://arxiv.org/pdf/1802.09477.pdf)<br/>\nOriginal citation for the PyTorch implementation fo Twin Delayed Deep Deterministic Policy Gradients (TD3), [source code](https://github.com/sfujim/TD3)\n4. [Schaul, T., Quan, J., Antonoglou, I., and Silver, D. Prioritized experience replay.arXiv preprint https://arxiv.org/abs/1511.05952(2015).](https://arxiv.org/abs/1511.05952)<br/>\nPrioritized experience replay- See Overleaf article summary.\n\n## Code References \n1. [TD3 Algorithm Code](https://towardsdatascience.com/td3-learning-to-run-with-ai-40dfc512f93) from Towards Data Science implementation of Addressing function approximation error in actor-critic methods.\n2. [OpenAI Gym, Replay Buffer and Priority Replay Buffer](https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py) \n3. [TD3 Implementation](https://arxiv.org/abs/1802.09477) Used for TD3 algorithm implementation.\n4. [DQN code from Richard Lenz, UNF]()\n\n\n## Bellman Equation Notes\n* State: what the agent is observing at a time step\n* Action: the input the agent provides to the the environment, calculated by applying a policy to the state\n* Reward: the feedback for the action\n\n## Links\n* [Getting started with Vrep](http://hades.mech.northwestern.edu/index.php/Getting_Started_with_the_V-REP_Simulator)\n* [CoppeliaSim User Manual](http://www.coppeliarobotics.com/helpFiles/index.html)\n* [Vrep/Python instructions](http://fid.cl/courses/ai-robotics/vrep-tut/pythonBubbleRob.pdf)\n* [ROS Robotics by Example](https://dl.acm.org/citation.cfm?id=3200107) Baxter reference for ROS including: joint angles,... (download the book)[https://drive.google.com/open?id=11UpOH1fZd1qhXr9i8tEyVa1g4NVmL-me]\n\n### Exporting Virtual Environment Packages\nExport a list of packages\n```\npip freeze > requirements.txt\n```\n\nInstall packages\n```\n$ virtualenv <env_name>\n$ source <env_name>/bin/activate\n(<env_name>)$ pip install -r path/to/requirements.txt\n```\n### Run VREP Headless\nLaunch VREP with following command. (You'll need to update the path to the vrep file.)\n      \n    V-REP/vrep.sh -h -q /home/cislocal/Jupyter/V-REP_Scenes/baxter.ttt -gREMOTEAPISERVERSERVICE_19999_FALSE_FALSE\nThen in the Vrep_SIM class I had the following line to start the simulation:\n           \n    errorCode = vrep.simxStartSimulation(self.clientID, vrep.simx_opmode_oneshot_wait)\n            \nI placed this line after the print('Connected to remote API server') in the Vrep_SIM class.",
      "https://arxiv.org/abs/1511.05952(2015).](https://arxiv.org/abs/1511.05952)<br/>\nPrioritized experience replay- See Overleaf article summary.\n\n## Code References \n1. [TD3 Algorithm Code](https://towardsdatascience.com/td3-learning-to-run-with-ai-40dfc512f93) from Towards Data Science implementation of Addressing function approximation error in actor-critic methods.\n2. [OpenAI Gym, Replay Buffer and Priority Replay Buffer](https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py) \n3. [TD3 Implementation](https://arxiv.org/abs/1802.09477) Used for TD3 algorithm implementation.\n4. [DQN code from Richard Lenz, UNF]()\n\n\n## Bellman Equation Notes\n* State: what the agent is observing at a time step\n* Action: the input the agent provides to the the environment, calculated by applying a policy to the state\n* Reward: the feedback for the action\n\n## Links\n* [Getting started with Vrep](http://hades.mech.northwestern.edu/index.php/Getting_Started_with_the_V-REP_Simulator)\n* [CoppeliaSim User Manual](http://www.coppeliarobotics.com/helpFiles/index.html)\n* [Vrep/Python instructions](http://fid.cl/courses/ai-robotics/vrep-tut/pythonBubbleRob.pdf)\n* [ROS Robotics by Example](https://dl.acm.org/citation.cfm?id=3200107) Baxter reference for ROS including: joint angles,... (download the book)[https://drive.google.com/open?id=11UpOH1fZd1qhXr9i8tEyVa1g4NVmL-me]\n\n### Exporting Virtual Environment Packages\nExport a list of packages\n```\npip freeze > requirements.txt\n```\n\nInstall packages\n```\n$ virtualenv <env_name>\n$ source <env_name>/bin/activate\n(<env_name>)$ pip install -r path/to/requirements.txt\n```\n### Run VREP Headless\nLaunch VREP with following command. (You'll need to update the path to the vrep file.)\n      \n    V-REP/vrep.sh -h -q /home/cislocal/Jupyter/V-REP_Scenes/baxter.ttt -gREMOTEAPISERVERSERVICE_19999_FALSE_FALSE\nThen in the Vrep_SIM class I had the following line to start the simulation:\n           \n    errorCode = vrep.simxStartSimulation(self.clientID, vrep.simx_opmode_oneshot_wait)\n            \nI placed this line after the print('Connected to remote API server') in the Vrep_SIM class."
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. [TD3 Algorithm Code](https://towardsdatascience.com/td3-learning-to-run-with-ai-40dfc512f93) from Towards Data Science implementation of Addressing function approximation error in actor-critic methods.\n2. [OpenAI Gym, Replay Buffer and Priority Replay Buffer](https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py) \n3. [TD3 Implementation](https://arxiv.org/abs/1802.09477) Used for TD3 algorithm implementation.\n4. [DQN code from Richard Lenz, UNF]()\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9998838606742826
      ],
      "excerpt": "Van Hasselt, H., Guez, A., and Silver, D. Deep reinforcement learning with Double Q-Learning. In Thirtieth AAAI conference on artificial intelligence(2016).<br/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9998957653282557
      ],
      "excerpt": "Fujimoto, S., van Hoof, H., and Meger, D. Addressing function approximation error in actor-critic methods. arXiv preprint arXiv:1802.09477, 2018.<br/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9998189055501475,
        0.8824264439431111
      ],
      "excerpt": "Schaul, T., Quan, J., Antonoglou, I., and Silver, D. Prioritized experience replay.arXiv preprint arXiv:1511.05952(2015).<br/> \nPrioritized experience replay- See Overleaf article summary. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8592915573379081
      ],
      "excerpt": "$ source &lt;env_name&gt;/bin/activate \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/CharlotteMorrison/Baxter-VREP",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-01-05T19:55:10Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-11-03T02:00:33Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9896233974348824,
        0.8566819564100561
      ],
      "excerpt": "The TD3 algorithm uses two critic networks and selects the smallest value for the target network.  To prevent overestimation of policies propogating errorthe policy network is updated after a set number of timesteps and the value network is updated after each time step. Variance will be lower in policy network leading to more stable and efficient training and ultimately a better quality policy.  For this implementation, the actor network is updated every 2 timesteps.  The policy is smoothed by adding random noise and averaging over mini-batches to reduce the variance caused by overfitting. <br/> \nVan Hasselt, H., Guez, A., and Silver, D. Deep reinforcement learning with Double Q-Learning. In Thirtieth AAAI conference on artificial intelligence(2016).<br/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9719759822760526
      ],
      "excerpt": "In actor-critic networks the policy is updated very slowly making bias a concern.  The older version of Double Q Learning uses clipped double Q learning.  This takes the smaller value of the two critic networks (the better choice).   Even though this promotes underestimation, this is not a concern because the small values will not propogate through the whole algorithm. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9161384454029643,
        0.8806990449048956
      ],
      "excerpt": "State: what the agent is observing at a time step \nAction: the input the agent provides to the the environment, calculated by applying a policy to the state \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9084072797197749
      ],
      "excerpt": "Getting started with Vrep \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/CharlotteMorrison/Baxter-VREP/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Wed, 22 Dec 2021 19:28:58 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/CharlotteMorrison/Baxter-VREP/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "CharlotteMorrison/Baxter-VREP",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.9528149448586686
      ],
      "excerpt": "Vrep/Python instructions \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9344893886899112,
        0.9967226375118565
      ],
      "excerpt": "pip freeze &gt; requirements.txt \nInstall packages \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/CharlotteMorrison/Baxter-VREP/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Baxter-VREP",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Baxter-VREP",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "CharlotteMorrison",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/CharlotteMorrison/Baxter-VREP/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Launch VREP with following command. (You'll need to update the path to the vrep file.)\n      \n    V-REP/vrep.sh -h -q /home/cislocal/Jupyter/V-REP_Scenes/baxter.ttt -gREMOTEAPISERVERSERVICE_19999_FALSE_FALSE\nThen in the Vrep_SIM class I had the following line to start the simulation:\n           \n    errorCode = vrep.simxStartSimulation(self.clientID, vrep.simx_opmode_oneshot_wait)\n            \nI placed this line after the print('Connected to remote API server') in the Vrep_SIM class.\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Wed, 22 Dec 2021 19:28:58 GMT"
    },
    "technique": "GitHub API"
  }
}