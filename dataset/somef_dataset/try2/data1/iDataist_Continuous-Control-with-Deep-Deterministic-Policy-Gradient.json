{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1509.02971\n\n2. Riedmiller, Martin. \"Neural fitted Q iteration\u2013first experiences with a data efficient neural reinforcement learning method.\" European Conference on Machine Learning. Springer, Berlin, Heidelberg, 2005. http://ml.informatik.uni-freiburg.de/former/_media/publications/rieecml05.pdf\n\n3. Mnih, Volodymyr, et al. \"Human-level control through deep reinforcement learning.\" Nature518.7540 (2015",
      "https://arxiv.org/abs/1511.05952\n\n6. https://openai.com/blog/reinforcement-learning-with-prediction-based-rewards/\n\n7. https://openai.com/blog/openai-baselines-ppo/\n\n8. https://openai.com/blog/openai-five/\n\n9.  https://pathak22.github.io/noreward-rl/"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9975165612895964,
        0.9946324986447935,
        0.9359042985618814,
        0.8507161623452375,
        0.9999262533574222
      ],
      "excerpt": "Lillicrap, Hunt, et al. \"Continuous control with deep reinforcement learning.\" 2015. https://arxiv.org/abs/1509.02971 \nRiedmiller, Martin. \"Neural fitted Q iteration\u2013first experiences with a data efficient neural reinforcement learning method.\" European Conference on Machine Learning. Springer, Berlin, Heidelberg, 2005. http://ml.informatik.uni-freiburg.de/former/_media/publications/rieecml05.pdf \nMnih, Volodymyr, et al. \"Human-level control through deep reinforcement learning.\" Nature518.7540 (2015): 529. http://www.davidqiu.com:8888/research/nature14236.pdf \nMnih,  Kavukcuoglu, et al. \"Playing Atari with Deep Reinforcement Learning.\" https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf \nSchaul, Quan, et al. \"Prioritized Experience Replay.\" ICLR (2016). https://arxiv.org/abs/1511.05952 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/iDataist/Continuous-Control-with-Deep-Deterministic-Policy-Gradient",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-12-13T18:06:09Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-06-09T16:00:03Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. [requirements.txt](https://github.com/iDataist/Continuous-Control-with-Deep-Deterministic-Policy-Gradient/blob/main/requirements.txt) - Includes all the required libraries for the Conda Environment.\n2. [model.py](https://github.com/iDataist/Continuous-Control-with-Deep-Deterministic-Policy-Gradient/blob/main/model.py) - Defines the actor and critic networks.\n3. [ddpg_agent.py](https://github.com/iDataist/Continuous-Control-with-Deep-Deterministic-Policy-Gradient/blob/main/ddpg_agent.py) -  Defines the Agent that uses DDPG to determine the best action to take and maximizes the overall or total reward.\n4. [Continuous_Control.ipynb](https://github.com/iDataist/Continuous-Control-with-Deep-Deterministic-Policy-Gradient/blob/main/Continuous_Control.ipynb) - The main file that trains the actor and critic networks. This file can be run in the Conda environment.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.925142900169817,
        0.9790854969012777,
        0.9711613548199065,
        0.9806233048221759,
        0.9964337623483575,
        0.8794156136567237,
        0.8964870823951442,
        0.9820046901297079
      ],
      "excerpt": "In this project, I trained twenty double-jointed arms to move to target locations. \nUnity Machine Learning Agents (ML-Agents) is an open-source Unity plugin that enables games and simulations to serve as environments for training intelligent agents. The gif below shows the Reacher environment for this project. \nIn this environment, twenty double-jointed arms can move to target locations. A reward of +0.1 is provided for each step that the agent's hand is in the goal location. Thus, the goal of the agent is to maintain its position at the target location for as many time steps as possible. \nThe observation space consists of 33 variables corresponding to position, rotation, velocity, and angular velocities of the arm. Each action is a vector with four numbers, corresponding to torque applicable to two joints. Every entry in the action vector should be a number between -1 and 1. The agents must get an average score of +30 (over 100 consecutive episodes, and over all agents) to solve the environment (version 1 or 2). \nDDPG<sup>1</sup> is a different kind of actor-critic method. It could be seen as an approximate DQN instead of an actual actor-critic. This is because the critic in DDPG is used to approximate the maximizer over the Q values of the next state and not as a learned baseline. \nOne of the DQN agent's limitations is that it is not straightforward to use in continuous action spaces. Imagine a DQN network that takes the state and outputs the action-value function. For example, for two actions, say, up and down, Q(S, \"up\") gives you the estimated expected value for selecting the up action in state S, say -2.18. Q(S,  \"down\") gives you the estimated expected value for choosing the down action in state S, say 8.45. To find the max action-value function for this state, you just calculate the maximum of these values. Pretty easy. It's straightforward to do a max operation in this example because this is a discrete action space. Even if you had more actions say a left, a right, a jump, and so on, you still have a discrete action space. Even if it were high dimensional with many, many more actions, it would still be feasible. But how do you get the value of continuous action with this architecture? Say you want the jump action to be continuous, a variable between 1 and 100 centimeters. How do you find the value of jump, say 50 centimeters? This is one of the problems DDPG solves. \nIn DDPG, we use two deep neural networks: the actor and the critic. \nThe actor here is used to approximate the optimal policy deterministically. That means we want to always output the best-believed action for any given state. This is unlike stochastic policies in which we want the policy to learn a probability distribution over the actions. In DDPG, we want the best action every single time we query the actor network. That is a deterministic policy. The actor is learning the argmax Q(S, a), which is the best action. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9573645096329255,
        0.9103808510606991,
        0.9620181966122003
      ],
      "excerpt": "- I created a ReplayBuffer Class to enable experience replay&lt;sup&gt;2, 3&lt;/sup&gt;. Using the replay pool, the behavior distribution is averaged over many of its previous states, smoothing out learning and avoiding oscillations. The advantage is that each step of the experience is potentially used in many weight updates. \n- In DQN, there are two copies of the network weights, the regular and the target network. In the Atari paper&lt;sup&gt;4&lt;/sup&gt; in which DQN was introduced, the target network is updated every 10,000 time steps. We can simply copy the weights of the regular network into the target network. The target network is fixed for 10,000 time steps, and then gets a big update. \n- In DDPG, there are two copies of the network weights for each network: a regular for the actor, a regular for the critic, a target for the actor, and a target for the critic. The target networks are updated using a soft updates strategy. A soft update strategy consists of slowly blending your regular network weights with the target network weights. So, every time step, I make the target network be 99.99 percent of the target network weights and only 0.01 percent of the regular network weights. I slowly mix in the regular network weights into the target network weights. The regular network is the most up-to-date network, while the target network is the one we use for prediction to stabilize training. We get faster convergence by using this update strategy. Soft updates can be used with other algorithms that use target networks, including DQN. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9076716969329078
      ],
      "excerpt": "Reinforcement Learning with Prediction-Based Rewards<sup>6</sup> \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/iDataist/Continuous-Control-with-Deep-Deterministic-Policy-Gradient/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 30 Dec 2021 07:43:02 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/iDataist/Continuous-Control-with-Deep-Deterministic-Policy-Gradient/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "iDataist/Continuous-Control-with-Deep-Deterministic-Policy-Gradient",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/iDataist/Continuous-Control-with-Deep-Deterministic-Policy-Gradient/main/Continuous_Control.ipynb",
      "https://raw.githubusercontent.com/iDataist/Continuous-Control-with-Deep-Deterministic-Policy-Gradient/main/.ipynb_checkpoints/Continuous_Control-checkpoint.ipynb",
      "https://raw.githubusercontent.com/iDataist/Continuous-Control-with-Deep-Deterministic-Policy-Gradient/main/.ipynb_checkpoints/Crawler-checkpoint.ipynb"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.851448158947477
      ],
      "excerpt": "|Layer        | Input/Output Sizes | Activation Function | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.851448158947477
      ],
      "excerpt": "|Layer        | Input/Output Sizes | Activation Function | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8044290599680651
      ],
      "excerpt": "- BATCH_SIZE = 128 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/iDataist/Continuous-Control-with-Deep-Deterministic-Policy-Gradient/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) Udacity\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Continuous Control with Deep Deterministic Policy Gradient",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Continuous-Control-with-Deep-Deterministic-Policy-Gradient",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "iDataist",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/iDataist/Continuous-Control-with-Deep-Deterministic-Policy-Gradient/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Thu, 30 Dec 2021 07:43:02 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Create the Conda Environment\n\n    a. Install [`miniconda`](http://conda.pydata.org/miniconda.html) on your computer, by selecting the latest Python version for your operating system. If you already have `conda` or `miniconda` installed, you should be able to skip this step and move on to step b.\n\n    **Download** the latest version of `miniconda` that matches your system.\n\n    |        | Linux | Mac | Windows |\n    |--------|-------|-----|---------|\n    | 64-bit | [64-bit (bash installer)][lin64] | [64-bit (bash installer)][mac64] | [64-bit (exe installer)][win64]\n    | 32-bit | [32-bit (bash installer)][lin32] |  | [32-bit (exe installer)][win32]\n\n    [win64]: https://repo.continuum.io/miniconda/Miniconda3-latest-Windows-x86_64.exe\n    [win32]: https://repo.continuum.io/miniconda/Miniconda3-latest-Windows-x86.exe\n    [mac64]: https://repo.continuum.io/miniconda/Miniconda3-latest-MacOSX-x86_64.sh\n    [lin64]: https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n    [lin32]: https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86.sh\n\n    **Install** [miniconda](http://conda.pydata.org/miniconda.html) on your machine. Detailed instructions:\n\n    - **Linux:** http://conda.pydata.org/docs/install/quick.html#linux-miniconda-install\n    - **Mac:** http://conda.pydata.org/docs/install/quick.html#os-x-miniconda-install\n    - **Windows:** http://conda.pydata.org/docs/install/quick.html#windows-miniconda-install\n\n    b. Install git and clone the repository.\n\n    For working with Github from a terminal window, you can download git with the command:\n    ```\n    conda install git\n    ```\n    To clone the repository, run the following command:\n    ```\n    cd PATH_OF_DIRECTORY\n    git clone https://github.com/iDataist/Continuous-Control-with-Deep-Deterministic-Policy-Gradient\n    ```\n    c. Create local environment\n\n    - Create (and activate) a new environment, named `ddpg-env` with Python 3.7. If prompted to proceed with the install `(Proceed [y]/n)` type y.\n\n        - __Linux__ or __Mac__:\n        ```\n        conda create -n ddpg-env python=3.7\n        conda activate ddpg-env\n        ```\n        - __Windows__:\n        ```\n        conda create --name ddpg-env python=3.7\n        conda activate ddpg-env\n        ```\n\n        At this point your command line should look something like: `(ddpg-env) <User>:USER_DIR <user>$`. The `(ddpg-env)` indicates that your environment has been activated, and you can proceed with further package installations.\n\n    - Install a few required pip packages, which are specified in the requirements text file. Be sure to run the command from the project root directory since the requirements.txt file is there.\n        ```\n        pip install -r requirements.txt\n        ipython3 kernel install --name ddpg-env --user\n        ```\n    - Open Jupyter Notebook, and open the Continuous_Control.ipynb file. Run all the cells in the jupyter notebook to train the agents.\n        ```\n        jupyter notebook\n        ```\n2. Download the Unity Environment\n\n   a. Download the environment from one of the links below.  You need only select the environment that matches your operating system:\n\n   - Version 1: One (1) Agent\n       - Linux: click [here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/one_agent/Reacher_Linux.zip)\n       - Mac OSX: click [here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/one_agent/Reacher.app.zip)\n       - Windows (32-bit): [here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/one_agent/Reacher_Windows_x86.zip)\n       - Windows (64-bit): [here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/one_agent/Reacher_Windows_x86_64.zip)\n   - Version 2: Twenty (20) Agents\n       - Linux: click [here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher_Linux.zip)\n       - Mac OSX: click [here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher.app.zip)\n       - Windows (32-bit): click [here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher_Windows_x86.zip)\n       - Windows (64-bit): click [here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher_Windows_x86_64.zip)\n\n    (For Windows users) Check out this [link](https://support.microsoft.com/en-us/help/827218/how-to-determine-whether-a-computer-is-running-a-32-bit-version-or-64) if you need help with determining if your computer is running a 32-bit version or 64-bit version of the Windows operating system.\n\n    (For AWS) If you'd like to train the agent on AWS (and have not enabled a virtual screen), then please use this [link](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/one_agent/Reacher_Linux_NoVis.zip) (version 1) or this [link](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher_Linux_NoVis.zip) (version 2) to obtain the \"headless\" version of the environment. You will not be able to watch the agent without enabling a virtual screen, but you will be able to train the agent. (To watch the agent, you should follow the instructions to enable a virtual screen, and then download the environment for the Linux operating system above.)\n\n    b. Place the file in the folder with the jupyter notebook, and unzip (or decompress) the file.\n\n",
      "technique": "Header extraction"
    }
  ]
}