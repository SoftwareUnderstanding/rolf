{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1704.00109",
      "https://arxiv.org/abs/1802.10026",
      "https://arxiv.org/abs/1803.05407\n - Authors: Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, Andrew Gordon Wilson\n - Repo: https://github.com/timgaripov/swa (PyTorch"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8306369730288786,
        0.9977994744046882
      ],
      "excerpt": "Title: Averaging Weights Leads to Wider Optima and Better Generalization \nLink: https://arxiv.org/abs/1803.05407 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/simon-larsson/keras-swa",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-10-01T15:52:46Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-15T07:35:28Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Stochastic weight averaging (SWA) is build upon the same principle as [snapshot ensembling](https://arxiv.org/abs/1704.00109) and [fast geometric ensembling](https://arxiv.org/abs/1802.10026). The idea is that averaging select stages of training can lead to better models. Where as the two former methods average by sampling and ensembling models, SWA instead average weights. This has been shown to give comparable improvements confined into a single model.\n\n[![Illustration](https://raw.githubusercontent.com/simon-larsson/keras-swa/master/swa_illustration.png)](https://raw.githubusercontent.com/simon-larsson/keras-swa/master/swa_illustration.png)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9988105844691406
      ],
      "excerpt": "This is an implemention of SWA for Keras and TF-Keras. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9394449182630016
      ],
      "excerpt": "start_epoch - Starting epoch for SWA. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9498975656304852,
        0.9892176683650752,
        0.9315379485642725
      ],
      "excerpt": "swa_lr2 - Upper bound of learning rate for the cyclic schedule. \nswa_freq - Frequency of weight averagining. Used with cyclic schedules. \nbatch_size - Batch size model is being trained with (only when using batch normalization). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.969243809844638,
        0.9314462839662974,
        0.9273064356093036
      ],
      "excerpt": "Last epoch will be a forward pass, i.e. have learning rate set to zero, for models with batch normalization. This is due to the fact that batch normalization uses the running mean and variance of it's preceding layer to make a normalization. SWA will offset this normalization by suddenly changing the weights in the end of training. Therefore, it is necessary for the last epoch to be used to reset and recalculate batch normalization running mean and variance for the updated weights. Batch normalization gamma and beta values are preserved. \nWhen using manual schedule: The SWA callback will set learning rate to zero in the last epoch if batch normalization is used. This must not be undone by any external learning rate schedulers for SWA to work properly. \nThe default schedule is 'manual', allowing the learning rate to be controlled by an external learning rate scheduler or the optimizer. Then SWA will only affect the final weights and the learning rate of the last epoch if batch normalization is used. The schedules for the two predefined, 'constant' or 'cyclic' can be observed below. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Simple stochastic weight averaging callback for Keras",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/simon-larsson/keras-swa/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 9,
      "date": "Tue, 28 Dec 2021 05:26:17 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/simon-larsson/keras-swa/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "simon-larsson/keras-swa",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "    pip install keras-swa\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.974959808591204
      ],
      "excerpt": "Repo: https://github.com/timgaripov/swa (PyTorch) \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/simon-larsson/keras-swa/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 Simon Larsson\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Keras SWA - Stochastic Weight Averaging",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "keras-swa",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "simon-larsson",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/simon-larsson/keras-swa/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 48,
      "date": "Tue, 28 Dec 2021 05:26:17 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "keras",
      "tensorflow",
      "tensorflow-keras",
      "deep-learning",
      "swa",
      "stochastic-weight-averaging"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "For Tensorflow Keras (with constant LR)\n```python\nfrom sklearn.datasets import make_blobs\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import SGD\n\nfrom swa.tfkeras import SWA\n \n#: make dataset\nX, y = make_blobs(n_samples=1000, \n                  centers=3, \n                  n_features=2, \n                  cluster_std=2, \n                  random_state=2)\n\ny = to_categorical(y)\n\n#: build model\nmodel = Sequential()\nmodel.add(Dense(50, input_dim=2, activation='relu'))\nmodel.add(Dense(3, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', \n              optimizer=SGD(lr=0.1))\n\nepochs = 100\nstart_epoch = 75\n\n#: define swa callback\nswa = SWA(start_epoch=start_epoch, \n          lr_schedule='constant', \n          swa_lr=0.01, \n          verbose=1)\n\n#: train\nmodel.fit(X, y, epochs=epochs, verbose=1, callbacks=[swa])\n```\n\nOr for Keras (with Cyclic LR)\n```python\nfrom sklearn.datasets import make_blobs\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense, BatchNormalization\nfrom keras.optimizers import SGD\n\nfrom swa.keras import SWA\n\n#: make dataset\nX, y = make_blobs(n_samples=1000, \n                  centers=3, \n                  n_features=2, \n                  cluster_std=2, \n                  random_state=2)\n\ny = to_categorical(y)\n\n#: build model\nmodel = Sequential()\nmodel.add(Dense(50, input_dim=2, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(3, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', \n              optimizer=SGD(learning_rate=0.1))\n\nepochs = 100\nstart_epoch = 75\n\n#: define swa callback\nswa = SWA(start_epoch=start_epoch, \n          lr_schedule='cyclic', \n          swa_lr=0.001,\n          swa_lr2=0.003,\n          swa_freq=3,\n          batch_size=32, #: needed when using batch norm\n          verbose=1)\n\n#: train\nmodel.fit(X, y, batch_size=32, epochs=epochs, verbose=1, callbacks=[swa])\n```\n\nOutput\n```\nModel uses batch normalization. SWA will require last epoch to be a forward pass and will run with no learning rate\nEpoch 1/100\n1000/1000 [==============================] - 1s 547us/sample - loss: 0.5529\nEpoch 2/100\n1000/1000 [==============================] - 0s 160us/sample - loss: 0.4720\n...\nEpoch 74/100\n1000/1000 [==============================] - 0s 160us/sample - loss: 0.4249\n\nEpoch 00075: starting stochastic weight averaging\nEpoch 75/100\n1000/1000 [==============================] - 0s 164us/sample - loss: 0.4357\nEpoch 76/100\n1000/1000 [==============================] - 0s 165us/sample - loss: 0.4209\n...\nEpoch 99/100\n1000/1000 [==============================] - 0s 167us/sample - loss: 0.4263\n\nEpoch 00100: final model weights set to stochastic weight average\n\nEpoch 00100: reinitializing batch normalization layers\n\nEpoch 00100: running forward pass to adjust batch normalization\nEpoch 100/100\n1000/1000 [==============================] - 0s 166us/sample - loss: 0.4408\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}