{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1804.07758",
      "https://arxiv.org/abs/1908.09260",
      "https://arxiv.org/abs/1512.00567",
      "https://arxiv.org/abs/1512.00567"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9997786469437429,
        0.9914565488929398
      ],
      "excerpt": "- Lucas Bechberger and Elektra Kypridemou. \"Mapping Images to Psychological Similarity Spaces Using Neural Networks\". 6th International Workshop on Artificial Intelligence and Cognition, Palermo/Italy, July 2018. Paper Preprint Release v0.1 \n- Lucas Bechberger and Kai-Uwe K\u00fchnberger. \"Generalizing Psychological Similarity Spaces to Unseen Stimuli - Combining Multidimensional Scaling with Artificial Neural Networks\". In Lucas Bechberger, Kai-Uwe K\u00fchnberger, and Mingya Liu: \"Concepts in Action: Representation, Learning, and Application\" Language, Cognition, and Mind. Springer (forthcoming) Preprint Release v1.3 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336221125029482
      ],
      "excerpt": "2.2.3 Comparing Visual and Conceptual Similarity Ratings \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9195926162616405
      ],
      "excerpt": "3 Machine Learning \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8714162992508173
      ],
      "excerpt": "3.1.3 Defining Regression Targets \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9183820474467645
      ],
      "excerpt": "3.2.4 Regression and Baselines \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lbechberger/LearningPsychologicalSpaces",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-01-30T09:36:23Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-04T13:20:19Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Since our Shapes study makes use of multiple data sources and a specific augmentation process, we created a separate script called `prepare_Shapes_data.py` for this preprocessing stage. It can be invoked as follows:\n```python -m code.ml.preprocessing.prepare_Shapes_data path/to/folds_file.csv path/to/output_directory/ factor```\nHere, `folds_file.csv` is a csv file that contains the columns `path` (giving the relative path of the image file from the project's root directory) and `fold` (the fold to which this image belongs). For classification data, a column `class` indicates the image class, while for data with psychological similarity ratings, the column `id` gives the stimulus ID used in the similarity space. The script will read all images listed in the `path` column of the `folds_file.csv`, create `factor` augmented copies of each image (by scaling it to a random size between 168 and 224 and by randomly translating it afterwards). The resulting augmented images will be stored as individual png files in the given `output_directory` and an additional pickle file containing a (shuffled) list of paths and classes/ids is created in the same file.\n\nThe script takes the following optional arguments:\n- `-p` or `--pickle_output_folder`: If a pickle output similar to the one provided by `data_augmentation.py` is desired, you can define the output folder for the augmented images here.\n- `-n` or `--noise_prob`: A list of floats specifying the different noise levels of salt and pepper noise to be added in the pickle versions.\n- `-s` or `--seed`: Specify a seed for the random number generator in order to make the results deterministic. If no seed is given, then the random number generator is not seeded.\n- `-o` or `--output_size`: Size of the output image, defaults to 224.\n- `-m` or `--minimum_size`: Minimal size of the object, defaults to 168.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9625537577579242
      ],
      "excerpt": "The code in this repository explores learning a mapping from images to psychological similarity spaces with neural networks. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9566100512929676,
        0.9169043623274259
      ],
      "excerpt": "- Lucas Bechberger and Kai-Uwe K\u00fchnberger. \"Generalizing Psychological Similarity Spaces to Unseen Stimuli - Combining Multidimensional Scaling with Artificial Neural Networks\". In Lucas Bechberger, Kai-Uwe K\u00fchnberger, and Mingya Liu: \"Concepts in Action: Representation, Learning, and Application\" Language, Cognition, and Mind. Springer (forthcoming) Preprint Release v1.3 \n- Lucas Bechberger and Kai-Uwe K\u00fchnberger. \"Grounding Psychological Shape Space in Convolutional Neural Networks\". (in preparation) Release v1.5 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259,
        0.8347110322740566
      ],
      "excerpt": "2.1.1 Parsing NOUN Similarity Data \n2.1.2 Parsing Shapes Similarity Data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8863500590094825
      ],
      "excerpt": "2.1.4 Parsing Shape Features Data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9683341372275439,
        0.8248372964649452
      ],
      "excerpt": "2.2 Analysis of the Data Set \n2.2.1 Correlations between Psychological Features \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9018630683633211
      ],
      "excerpt": "2.2.4 Creating Visualizations of the Similarity Matrices \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8490037945672047
      ],
      "excerpt": "2.3.2 Normalizing the Similarity Spaces \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9538407076589619
      ],
      "excerpt": "2.4.5 Distances in MDS Space \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9394449182630016
      ],
      "excerpt": "2.5.1 Checking for Overlap \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9128728346903523,
        0.8979411005071259
      ],
      "excerpt": "3.1 Preparing the Data Set for Machine Learning \n3.1.1 Data Augmentation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8486652401483921
      ],
      "excerpt": "3.2.3 Cluster Analysis of Feature Vectors \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9425372602824283,
        0.974051540060766,
        0.9708896501873584,
        0.9008898454991519
      ],
      "excerpt": "The folder code contains all python scripts used in our experiments. The usage of these scripts is detailed below. \nThe folder data contains the data used for the NOUN study inside the NOUN subfolder. This includes the dissimilarity ratings, the images, as well as all intermediate products created by our scripts. In the subfolder Shapes we will at some point add the respective results for the Shape study. \nBoth studies consist of two parts: The first part focuses on the spaces produced by multidimensional scaling (to be found in the mds subfolders of both code and data), whereas the second part focuses on learning a mapping from images into these similarity spaces (to be found in the ml subfolders of both code and data). \nOur training data are the images and similarity ratings of the NOUN database (http://www.sussex.ac.uk/wordlab/noun), which were kindly provided by Jessica Horst and Michael Hout:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9813997836559786,
        0.9619278393303036
      ],
      "excerpt": "The folder code/mds contains all scripts necessary for the first part of our studies, where we apply multidimensional scaling to a matrix of dissimilarity ratings and where we analyze the resulting spaces. \nThe folder code/mds/preprocessing contains various scripts for preprocessing the data set in order to prepare it for multidimensional scaling. Depending on the data set (NOUN vs. Shapes), the first preprocessing step differs (see below), but results in the same output data structure, namely a dictionary with the following elements: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8338086273888536,
        0.9735905448442786
      ],
      "excerpt": "  - 'artificial': Does the category consist of natural ('nat') or artificial ('art') items?  \n  - 'items': A list of all items that belong into this category.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8445758124704547,
        0.9400469402106686
      ],
      "excerpt": "  - 'category': The name of the category this item belongs to.  \n- 'similarities': A dictionary using the string representation of sets of two items as keys and dictionaries as values. These dictionaries have the following elements: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9763210788562343
      ],
      "excerpt": "With respect to the resulting output.pickle file, we would like to make the following comments: As the NOUN data set does not divide the stimuli into separate categories, we store all items under a single global category which is considered to consist of artificial stimuli. We assume that the visual homogeneity of this category is unclear. As the individual stimuli do not have meaningful names, we use their IDs also as human readable names. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9654535055348962,
        0.9288023522390083,
        0.9906858830846552
      ],
      "excerpt": "- -r or --reverse can be set in order to reverse the order of similarity ratings (necessary when using conceptual similarity, as the scale there is inverted) \n- -s or --subset: Specifies which subset of the similarity ratings to use. Default is all (which means that all similarity ratings from both studies are used). Another supported option is between where only the ratings from the first study are used. Here, all items that did not appear in the first study are removed from further consideration. A third option is cats which only considers the categories used in the second study, but which keeps all items from these categories (also items that were only used in the first, but not in the second study). The fourth option within only uses data from the first study. \n- -l or --limit: Limit the number of similarity ratings to use to ensure that an equal amount of ratings is aggregated for all item pairs.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8325187541567872,
        0.8478586840384492
      ],
      "excerpt": "- --seed: The given seed is used to initialize the random number generator; if no seed is given, results are not reproducible! \nThe next step in the preprocessing pipeline is to aggregate the individual similarity ratings from the overall data set. This can be done with the script aggregate_similarities.py. You can execute it as follows from the project's root directory: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8869359468107401
      ],
      "excerpt": "input_file.pickle should be the output file generated by any of the two preprocessing scripts, output_file.pickle determines where the resulting similarity values are stored. The script also computes a dissimilarity matrix which can be used for MDS and stores it in the given output_folder_matrix. Finally, the aggregated similarity information is also stored in CSV format in output_file.csv, using the header pairID,pairType,visualType,ratingType,ratings and the given rating_type. Moreover, some information about the global matrix is printed out. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8816943072922879
      ],
      "excerpt": "- -m or --median: Use the median instead of the mean for aggregating the similarity ratings across participants. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8883542313211165,
        0.9789610873143012,
        0.8869247996422411,
        0.868331443248992
      ],
      "excerpt": "- 'items': An ordered list of item names of all the items for which the similarity values have been computed. Items are ordered alphabetically within a category and accordings to 'category_names'. \n- 'similarities': A quadratic matrix of similarity values. Both rows and columns are ordered like in 'items'. Values of nan are used to indicate that there is no similarity rating available for a pair of stimuli. \n- 'dissimilarities': A quadratic matrix of dissimilarity values analogous to 'similarities'. Here, values of 0 indicate missing similarity ratings. \n- 'category_similarities': A quadratic matrix of category-based similarity ratings (i.e., similarities within and between categories). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8117249132262021,
        0.8791741428783326
      ],
      "excerpt": "  - pre-attentive: A dictionary mapping from item names to a list of pre-attentive ratings, all normalized to the range [-1,1]. \n  - attentive: A dictionary mapping from item names to a list of attentive ratings, all normalized to the range [-1,1]. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9075580577902849,
        0.9179488984808793
      ],
      "excerpt": "  - pre-attentive: A dictionary mapping with the keys positive and negative and lists of item names as values. Contains the examples with the highest and lowest aggregated value, respectively. \n  - attentive: A dictionary mapping with the keys positive and negative and lists of item names as values. Contains the examples with the highest and lowest aggregated value, respectively. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9126600434940879
      ],
      "excerpt": "- -q or --quantile: The quantile to use for determining the set of positive and negative classification examples. Defaults to 0.25 (i.e., top quartile and bottom quartile). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8394486068627177,
        0.8356919655167705
      ],
      "excerpt": "The directory feature_folder is searched for pickle files containing feature information and the individual similarity ratings (output of preprocess_Shapes.py) are taken from similarities.pickle, using the meta-information about the rating_type (i.e., conceptual vs visual) given as a separate argument. Individual feature ratings are written to output_individual.csv using the header item,ratingType,feature,ratings (one row per individual rating). Aggregated feature ratings are written to output_aggregated.csv using the header item,ratingType,feature_1,[...],feature_n where feature_i is replaced by the ith feature (one row per item). Finally, the output_combined.csv file contains individual similarity ratings (same structure as output by preprocess_Shapes.py) along with the aggregated feature ratings for both items in the pair. More specifically, the header looks as follows: pairID,pairType,visualType,ratingType,ratings,, followed by colums like item1_FORM_pre-attentive for all combinations of item index (1 or 2), feature (depends on the input from feature_folder), and feature rating type (attentive or pre-attentive). \nThe script features_from_categories.py uses the category structure to create candidate features based on both the visSim and the artificial information. It loads the raw data from input.pickle (output of preprocess_Shapes.py) and stores the resulting feature information in the given output_folder in two pickle files structured in a way analogous to preprocess_feature.py. The script can be executed as follows: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9522089232286404
      ],
      "excerpt": "The script compare_features.py compares the scales of two different psychological features to each other, based on each of the scale types. It can be invoked as follows: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8081426242919638
      ],
      "excerpt": "The script average_images.py can be used in order to create an average image for each of the categories. It can be invoked as follows: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9109809179340903
      ],
      "excerpt": "- -r or --resolution: The desired size (width and height) of the output images, defaults to 283 (i.e, the size of the original images from the Shapes data set). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9288096824951889,
        0.9761602607256552
      ],
      "excerpt": "The folder code/mds/similarity_spaces contains scripts for transforming the given data set from pairwise similarity ratings into a conceptual space. \nThe script mds.r runs four different versions of multidimensional scaling based on the implementations in R. More specifically, it uses the Eigenvalue-based classical MDS (cmdscale), Kruskal's nonmetric MDS (isoMDS), and both metric and nonmetric SMACOF (smacofSym). For Kruskal's algorithm and for SMACOF, multiple random starts are used and the best result is kept. You can execute the script as follows from the project's root directory: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8966386440302988,
        0.9877766335198515
      ],
      "excerpt": "- -k or --dims: Specifies the maximal number of dimensions to investigate. Default value is 20, which means that the script will run the MDS algorithm 20 times, obtaining spaces of dimensionality 1 to 20. \n- -n or --n_init: Specifies how often the nondeterministic MDS algorithms are restarted with a new random initialization. Of all of these runs, only the best result (i.e., the one with the lowest resulting stress) is kept. Default value here is 64. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9110558360374488,
        0.9007567460088761
      ],
      "excerpt": "We implemented the MDS step in R and not in Python because R offers a greater variety of MDS algorithms. Moreover, nonmetric SMACOF with Python's sklearn library produced poor results which might be due to a programming bug. \nIn order to make the individual MDS solutions more comparable, we normalize them by moving their centroid to the origin and by making sure that their mean squared distance to the origin equals one. This is done by the script normalize_spaces.py, which can be invoked by simply giving it the path to the directory containing all the vector files: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8338086273888536,
        0.9735905448442786,
        0.8916263414366916
      ],
      "excerpt": "  - 'artificial': Does the category consist of natural ('nat') or artificial ('art') items?  \n  - 'items': A list of all items that belong into this category.  \n- n (for an integer n corresponding to the number of dimensions): Dictionary mapping from item names to vectors in the n-dimensional similarity space \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9052185551936844,
        0.9328877419564852
      ],
      "excerpt": "- -v or --verbose: Prints some debug information during processing (old centroid, old root mean squared distance to origin, new centroid, new root mean squared distance to origin). \nIt is important to run this script before using the MDS spaces for the machine learning task -- only by normalizing the spaces, we can make sure that the MSE values are comparable across spaces! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8306813146342031
      ],
      "excerpt": "The script reads in the vectors from vectors.pickle (output of normalize_spaces.py), creates two-dimensional plots for all pairs of dimensions, and stores them in the given output_folder.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9724426167046729
      ],
      "excerpt": "- -m or --max: Determines the dimensionality of the largest space to be visualized. Defaults to 10. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9265564438849514,
        0.9023818473364262
      ],
      "excerpt": "- -r or --region: If this flag is set, convex hulls of the different categories are included in the plots. \nFor our later analysis, we will need random configurations of points to serve as a simple baseline. The script create_baseline_spaces.py can be used to create such random configurations. It can be invoked as follows: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.936334212558681
      ],
      "excerpt": "Here, individual_ratings.pickle is the output created by preprocess_NOUN.py or preprocess_Shapes.py and is only used to get a list of all item names. The parameter n_spaces gives the number of example spaces to generate for each dimensionality and max_dims specifies the maximal number of dimensions to consider. The script takes the following optional arguments: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8637159088708631
      ],
      "excerpt": "- -m or --shuffled: This flag is followed by a list in the form name_1 path_1 name_2 path_2 ..., giving paths to vector pickle files (output of normalize_spaces.py) and their corresponding human-readable name that are shuffled in order to obtain baseline spaces. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9490948676205855,
        0.9532415742729703,
        0.8193889059161987,
        0.8787276944784321
      ],
      "excerpt": "Please note that at least one of the distribution types -u, -n, -m must be set. The resulting pickle file contains a hierarchical dictionary mapping using the baseline type, the number of dimensions, and the item as keys on the different hierarchy levels. Please note that for each number of dimensions a list of spaces is stored (of length n_spaces). \nThe folder code/mds/correlations contains various scripts for correlating distances and dissimilarities for the MDS solutions and various baselines. In all cases, we consider three distance measures (Euclidean, Manhattan, inner product) and five correlation metrics (Pearson's r, Spearman's rho, Kendall's tau, and the coefficient of determination R\u00b2). \nThe script pixel_correlations.py loads the images and downscales them using scipy's block_reduce function (see here). Here, all pixels within a block of size k times k are aggregated via one of the following aggregation functions: maximum, minimum, arithmetic mean, median. The script automatically iterates over all possible combinations of k and the aggregation function. \nThe pixels of the resulting downscaled images are interpreted as one-dimensional feature vectors. All of the distance measures are used to build distance matrices, which are then in turn correlated with the original dissimilarity ratings (both using the raw distances and using a weighted version where dimension weights are estimated in a cross-validation). The script can be executed as follows, where aggregated_ratings.pickle is the output file of aggregate_similarities.py, distances_folder is a folder containing pickle files with precomputed distances, and output_file.csv is the destination CSV file which where the results will be stored: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8990709874150976
      ],
      "excerpt": "Please note that one or more of the following flags must be set in order to specify the correlation metric(s) to use in the analysis: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.916845864242364,
        0.9434014493681564
      ],
      "excerpt": "- --r2_linear: Compute the coefficient of determination R\u00b2 for a linear regression (linear correlation). \n- --r2_isotonic: Compute the coefficient of determination R\u00b2 for an isotonic regression (monotone correlation). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9639857261916825
      ],
      "excerpt": "- -w or --width: The width (and also height) of the full images, i.e., the maximal number of k to use (default: 300, i.e. the image size of the NOUN data set). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9335843324033887,
        0.8163698098529905
      ],
      "excerpt": "- -n or --n_folds: The number of folds to use in the cross-validation process of optimizing dimension weights (defaults to 5). \n- -s or --seed: Specify a seed for the random number generator in order to make the folds and thus the overall results deterministic. If no seed is given, then the random number generator is not seeded. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8407997130617615
      ],
      "excerpt": "The script visualize_pixel_correlations.py can be used to visualize the results of the pixel baseline as a function of block size. It can be invoked as follows: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9056005080357729
      ],
      "excerpt": "Please note that one or more of the following flags must be set in order to specify the correlation metric(s) to use for visualization: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.916845864242364,
        0.9434014493681564,
        0.934967088090732
      ],
      "excerpt": "- --r2_linear: Compute the coefficient of determination R\u00b2 for a linear regression (linear correlation). \n- --r2_isotonic: Compute the coefficient of determination R\u00b2 for an isotonic regression (monotone correlation). \nAs a second baseline, we use the features extracted by the a neural network (more specifically, the Inception-v3 network) to predict the similarities between images from the data set. The corresponding script is called ann_correlations.py and is invoked as follows: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9481860668099382,
        0.8990709874150976
      ],
      "excerpt": "By default, the pre-computed distances from distances.pickle are used to compute the correlations. If however the optional parameter -i or --image_folder is specified, the images are loaded from that given folder, the distances are computed manually, and are stored in distances.pickle for future use. In the latter case, the script downloads the inception network into the given model_folder, takes all images from the image_folder, and computes the activation of the second-to-last layer of the ANN. This activation vector is then used as a feature vectors. All of the distance measures are used to build distance matrices, which are then in turn correlated with the original dissimilarity ratings from aggregated_ratings.pickle (both using the raw distances and using a weighted version where dimension weights are estimated in a cross-validation).  \nPlease note that one or more of the following flags must be set in order to specify the correlation metric(s) to use in the analysis: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.916845864242364,
        0.9434014493681564
      ],
      "excerpt": "- --r2_linear: Compute the coefficient of determination R\u00b2 for a linear regression (linear correlation). \n- --r2_isotonic: Compute the coefficient of determination R\u00b2 for an isotonic regression (monotone correlation). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9335843324033887,
        0.8163698098529905,
        0.9864845797716686
      ],
      "excerpt": "- -n or --n_folds: The number of folds to use in the cross-validation process of optimizing dimension weights (defaults to 5). \n- -s or --seed: Specify a seed for the random number generator in order to make the folds and thus the overall results deterministic. If no seed is given, then the random number generator is not seeded. \nIf we interpret the values on the scales of the (psychological) features as coordinates of a similarity space, we can use these coordinates to also compute distances between stimuli. The script feature_correlations.py does exactly this and computes the correlation to the original dissimilarity ratings (both using the raw distances and using a weighted version where dimension weights are estimated in a cross-validation). It is called as follows: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8990709874150976
      ],
      "excerpt": "Please note that one or more of the following flags must be set in order to specify the correlation metric(s) to use in the analysis: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.916845864242364,
        0.9434014493681564
      ],
      "excerpt": "- --r2_linear: Compute the coefficient of determination R\u00b2 for a linear regression (linear correlation). \n- --r2_isotonic: Compute the coefficient of determination R\u00b2 for an isotonic regression (monotone correlation). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9335843324033887,
        0.8163698098529905,
        0.931907381917645
      ],
      "excerpt": "- -n or --n_folds: The number of folds to use in the cross-validation process of optimizing dimension weights (defaults to 5). \n- -s or --seed: Specify a seed for the random number generator in order to make the folds and thus the overall results deterministic. If no seed is given, then the random number generator is not seeded. \nThe script mds_correlations.py loads the MDS vectors and derives distances between pairs of stimuli based on the three distance measures. These distances are then correlated to the human dissimilarity ratings (both using the raw distances and using a weighted version where dimension weights are estimated in a cross-validation). The script can be executed as follows: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8990709874150976
      ],
      "excerpt": "Please note that one or more of the following flags must be set in order to specify the correlation metric(s) to use in the analysis: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.916845864242364,
        0.9434014493681564
      ],
      "excerpt": "- --r2_linear: Compute the coefficient of determination R\u00b2 for a linear regression (linear correlation). \n- --r2_isotonic: Compute the coefficient of determination R\u00b2 for an isotonic regression (monotone correlation). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9433816197247441,
        0.9962133974609109,
        0.9962133974609109,
        0.9335843324033887,
        0.8163698098529905
      ],
      "excerpt": "- -b or --baseline_file: If a file generated by create_baseline_spaces.py is given as an argument, the expected correlation value for all of the baseline spaces is also computed and stored. \n- --n_min: The size of the smallest space to investigate (defaults to 1). \n- --n_max: The size of the largest space to investigate (defaults to 20). \n- -n or --n_folds: The number of folds to use in the cross-validation process of optimizing dimension weights (defaults to 5). \n- -s or --seed: Specify a seed for the random number generator in order to make the folds and thus the overall results deterministic. If no seed is given, then the random number generator is not seeded. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9460347896356421,
        0.8255325987752938,
        0.9378584158120478
      ],
      "excerpt": "- --mds or -m: The MDS vectors of the given number of dimensions is used. \n- --ann or -a: The ANN baseline is used. \n- --features or -f: The feature baseline with the given feature space is used. The following additional argument needs to be specified for the feature baseline: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8412375070950987,
        0.9183746465857004,
        0.9791138647328509
      ],
      "excerpt": "- --pixel or -p: The pixel baseline with the given aggregator is used. The following additional argument needs to be specified for the pixel baseline: \n    - --block_size or -b determines the block size (defaults to 1). \n- --random or -r: A random configuration of points with the given number of dimensions is used. The following additional argument needs to be specified: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9241896715461984,
        0.872594859209933,
        0.9335843324033887,
        0.8163698098529905,
        0.8052258987591239,
        0.981673794354504,
        0.9916181077308095,
        0.944481727323137
      ],
      "excerpt": "For all three of these cases, the parameter --distance or -d determines which distance function to use (Euclidean, Manhattan, or Cosine). \nIn its basic version, the script uses fixed identical dimension weights of one. If the flag --optimized or -o is set, then optimal dimension weights are computed based on a cross-validation approach (identical to the scripts above). The following additional parameters control the behavior of this cross-validation: \n- -n or --n_folds: The number of folds to use in the cross-validation process of optimizing dimension weights (defaults to 5). \n- -s or --seed: Specify a seed for the random number generator in order to make the folds and thus the overall results deterministic. If no seed is given, then the random number generator is not seeded. \nFinally, the flag --similarity_name can be used to determine the title of the y-axis by specifying which type of similarity file is being used. Its default setting is 'Mean' (i.e., the y-axis will be labeled as 'Mean Dissimilarity from Psychological Study'). \nThe folder code/mds/regions contains two scripts for analyzing the well-formedness of conceptual regions. This is only applicable to the Shapes data set, as there are no categories in NOUN. \nThe script analyze_overlap.py can be used to check whether the categories within the space are non-overlapping. It iterates over all categories, builds a convex hull of the items belonging to this category and counts how many points from other categories lie within this convex hull. Each point that lies in the convex hull of a different concept is counted as one violation. This analysis takes place both for all categories and for the VC-VV distinction. \nThe script can be exectued as follows (where vectors.pickle is the output of normalize_vectors.py and n_dims is the dimensionality of the space to consider): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.944481727323137
      ],
      "excerpt": "The script can be invoked as follows (where vectors.pickle is the output of normalize_vectors.py and n_dims is the dimensionality of the space to consider): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.963323974928571,
        0.971647727842581,
        0.9559734410208045
      ],
      "excerpt": "The folder code/mds/directions contains various scripts for extracting interpretable directions in the similarity space based on the given features. \nThe script find_directions.py tries to find interpretable directions in a given similarity space based on a regression or classification task. This is only applicable to the Shapes data set, as there are no categories in NOUN. \nIt can be invoked as follows (where n_dims is the number of dimensions of the underlying space to consider): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9640561280333542
      ],
      "excerpt": "Here, vectors.pickle is the output of normalize_spaces.py. Based on the feature information from feature.pickle (output of preprocess_feature.py or features_from_categories.py) the script constructs a classification and a regression problem and trains a linear SVM and a linear regression on them, respectively. The quality of the model fit is evaluated by extracting the normal vector of the separating hyperplane and by projecting all points onto this normal vector. Then, we use Cohen's kappa to measure how well a simple threshold classifier on the resulting values performs. Moreover, we compute the Spearman correlation of the projected vectors to the scale values from the regression problem. The resulting numbers are stored in output.csv, along with the extracted direction, using the header dims,data_source,space_idx,feature_type,model,kappa,spearman,d1,...,d20. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9772851570614025
      ],
      "excerpt": "The script compare_directions.py compares the interpretable directions found by find_directions.py by using the cosine similarity. More specifically, the script iterates through all spaces with a dimenionality of maximally n_dims. For each space, it computes the average cosine similarity of all the interpretable directions for the same feature (which were however constructed based on different feature rating scales and different ML algorithms). Moreover, it computes the average cosine similarity for each pair of features (by comparing all pairs of directions). The results are stored in output.csv. The script furthermore requires an input_folder, which contains all csv files created by find_directions.py (and no additional files!). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9917046158667449
      ],
      "excerpt": "The script loads the candidate directions along with their evaluation results from input_file.csv (which is the output of find_directions.py). For each space (up to n_dims), it compares all candidate directions based on Cohen's kappa and based on the Spearman correlation. For each of these evaluation metrics, the directions with the highest values are kept and averaged. The result (the dimensionality of the space, the direction_name, the averaged direction and the list of candidate directions it is based on) is written to output.csv. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9331791395881177,
        0.945998577109828,
        0.9009989745220053
      ],
      "excerpt": "- -k or --kappa_threshold: Minimal value of Cohen's kappa required to pass the filter. Defaults to 0. \n- -s or --spearman_threshold: Minimal value of the Spearman correlation required to pass the filter. Defaults to 0. \nIn order to make the subsequent analysis easier, the script aggregate_direction_results.py can be used to aggregate the evaluation results created by find_directions.py based on data_source, direction name, scale type, and ML model (by averaging over the two other conditions). It is executed as follows, where input_folder contains the csv files created by find_directions.py (and no additional files!), n_dims is the maximal dimensionality of the similarity space to consider. The results will be stored as separate csv files inside the given output_folder. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9929450282573359,
        0.9631444437518084
      ],
      "excerpt": "The folder code/ml contains all scripts necessary for the second part of our studies, where we apply machine learning techniques in order to learn a mapping from images to points in the similarity space. \nWe used ImgAug for augmenting our image data set for the NOUN data set. This is done with the script data_augmentation.py. It can be invoked as follows: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9532026521557958,
        0.8724090282055378,
        0.9380612025700366,
        0.9351298563810455
      ],
      "excerpt": "- -i or --image_size: The expected image size in pixels. Defaults to 300 (i.e., the image size of the NOUN data set). \nAugmentation is done by appling the folloing operations in random order: \n- Horizontal flips. The probability of a horizontal flip can be controlled with the optional parameter --flip_prob (defaults to 0.5). \n- Cropping. The maximal relative amount of cropping for each side of the image can be controlled with --crop_size (default: 0.1). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.836205268427731,
        0.8614728379163191
      ],
      "excerpt": "- Additive Gaussian noise. The value of sigma is set via --g_noise_sigma (default: 0.05) and the probability of drawing a different value for each color channel independently by seeting --g_noise_channel_prob (default: 0.5) \n- Varying the brightness. The borders of possible brightness values (relative to the image's original brightness) can be set via --light_min (default: 0.8) and --light_max (default: 1.2). Moreover, the probability of drawing a different value for each color channel independently is controlled by --light_channel_prob (default: 0.2) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.826568211169661,
        0.9113111963074562,
        0.9113111963074562,
        0.928494314492381
      ],
      "excerpt": "- Translation: The relative amount of translation is set with --translation (default: 0.2) \n- Rotation: The maximal rotation angle in degrees is controlled by --rotation_angle (default: 25). \n- Shearing: The maximal shear angle in degrees is controlled by --shear_angle (default: 8). \n- Salt and pepper noise: The amount of pixels to modify is set via --sp_noise_prob (default: 0.03) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8603842333141355
      ],
      "excerpt": "As our experiments are run against a wide variety of target spaces, we created a script called prepare_targets.py which for convenience collects all possible target vectors in a single pickle file. It moreover creates a shuffled version of the targets for later usage as a control case. The script can be invoked as follows: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9633375197582147,
        0.8924470290094898,
        0.9589057604055882,
        0.8907681699144868
      ],
      "excerpt": "Here, input.csv is a csv file with two columns: In each row, the first column contains a short descriptive name of the target space and the second column contains the path to the corresponding file with the MDS vectors (as created in Section 2.2.1 and normalized in Section 2.2.2). The script iterates through all these target spaces and collects the MDS vectors. When shuffling them, the same seed is used for all spaces to ensure that the results are comparable. By setting -s or --seed, the user can specify a fixed seed, otherwise a random seed is drawn in the beginning of the script.  \nThe result is stored in output.pickle as a dictionary having the names of the target spaces as keys and further dictionaries (with the keys correct and shuffled leading to dictionaries with the corresponding image-vector mappings) as values. \nAs a first pass of the regression task, we evaluate some simple baselines (which disregard the images altogether) as well as some linear regressions based on either downscaled images or the features extracted by a pretrained neural network. All scripts are contained in the code/ml/regression folder. \nIn order to create feature vectors based on the activations of the Inception-v3 network, one can use the script ann_features.py. It is invoked as follows: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9398103993425893,
        0.8848675454226282,
        0.9913411611892101
      ],
      "excerpt": "- -g or --greyscale: If this flag is set, the image is converted to greyscale before downscaling (reduces the number of output features by factor 3). \n- -b or --block_size: Size of one block that will be reduced to a single number. Defaults to 1. \nThe point of data set augmentation is to create a larger variety of input images and to introduce some additional noise into the data set. The script cluster_analysis.py takes a file of feature vectors and analyzes whether they form strong clusters (in the sense that all augmented images based on the same original are very similar to each other, but very different from other images). It uses the Silhouette coefficient to quantify this. As comparison, the Silhouette coefficient of a shuffled data set is computed. The script can be called as follows: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8571849359452661,
        0.9184442046750066
      ],
      "excerpt": "- -n or --n_sample: The number of samples to randomly draw for each original image (defaults to 100). Computing the Silhouette coefficient may be untractable for large data sets, so sampling might be required. \n- -s or --seed: The random seed to use for initializing the random number generator. If none is given, a different initialization is used in every call to the script. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8933763746821977,
        0.9532481370561985,
        0.959514394287061
      ],
      "excerpt": "Here, target_vectors.pickle is the file generated by prepare_targets.py, space_name is the name of a target space contained in this file, features.pickle contains the features to be used (either generated by ann_features.py or by pixel_features.py), folds.csv contains the fold structure (for each original image the number of the fold it belongs to), and output.csv is the file in which the results will be stored (the script appends to the file if it already exists). \nIn order to select the type of regression to be used, one needs to pass exactly one of the following flags to the script: \n- --zero: Zero baseline, always predicts the origin of the feature space (i.e., a vector where all entries are zero) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.842505519012335,
        0.9900120214221504,
        0.8375984417907156
      ],
      "excerpt": "- --linear: Linear regression, runs sklearn's LinearRegression after normalizing the feature space. \n- --lasso: Lasso regression, runs sklearn's Lasso regressor after normalizing the feature space, using the given value as relative strength of the regularization term. Computes alpha = args.lasso / len(train_features[0]) to ensure that the regularization term is in the same order of magnitude independent of the size of the feature space. \n- --random_forest: Random Forest regression* using a random forest with default parameters as given by sklearn. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9402573722738735,
        0.9621529901251357
      ],
      "excerpt": "- -s or --seed: The random seed to use for initializing the random number generator (important for nondeterministic regressors). If none is given, a different initialization is used in every call to the script. \n- --shuffled: If this flag is set, the regression is not only performed on the correct targets, but also on the shuffled ones. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8640078463285877,
        0.949468139774466
      ],
      "excerpt": "The script performs a cross-validation based on the fold structure given in folds.csv, where all augmented images that are based on the same original image belong into the same fold. The script reports MSE, MED (the mean Euclidean distance between the predicted points and the targets points), and the coefficient of determination R\u00b2 in the output csv file for both the training and the test phase. \nThe script regression.py automatically performs an internal cross-validation and only reports the averaged results. However, if the neural network, on which the feature vectors are based, is also trained in a cross-validation scheme (as it is the case in the Shapes study), regression.py will be invoked once for each of the different network versions. In order to aggregate the results over these `outer folds'', one can use the scriptaverage_folds.pyas follows: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9864396257851107
      ],
      "excerpt": "As a more complex approach, we investigated the usage of a hybrid ANN architecture which is trained on the tasks of classification, reconstruction, and/or mapping. All relevant scripts are contained in the code/ml/ann folder. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8618453475673287
      ],
      "excerpt": "Here, Shapes.pickle, Additional.pickle, Berlin.pickle, and Sketchy.pickle are the pickle files created by prepare_Shapes_data.py. Moreover, targets.pickle is the file generated by prepare_targets.py and space_name is the name of a target space contained in this file. The given image_folder contains all images of the Shapes study and dissimilarities.pickle is the output of aggregate_similarities.py and contains the target dissimilarity ratings. Both of these arguments are used to compute the correlation between the bottleneck layer activations and the dissimilarity ratings. Finally, output.csv is the file in which the results will be stored (the script appends to the file if it already exists). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8274521753636951,
        0.9069824567043259,
        0.8618746756038811,
        0.8960821510522952
      ],
      "excerpt": "- -c or --classification_weight: Relative weight of the classification objective in the overall loss function. \n- -r or --reconstruction_weight: Relative weight of the reconstruction objective in the overall loss function. \n- -m or --mapping_weight: Relative weight of the mapping objective in the overall loss function. \nPlease note that all three weights default to zero, but that they need to be set in such a way that they sum to one. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8531511367216421,
        0.9180833373503927,
        0.9180833373503927
      ],
      "excerpt": "- -b or --bottleneck_size: The number of units in the bottleneck layer, defaults to 512. \n- -w or --weight_decay_encoder: The weight decay penalty used for weights in the encoder network. Defaults to 0.0005. \n- -v or --weight_decay_decoder: The weight decay penalty used for weights in the decoder network. Defaults to 0. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9760972467832378,
        0.8574375135476159
      ],
      "excerpt": "- -n or --noise_prob: The probability for the salt and pepper noise being applied to the inputs. Defaults to 0.1 (i.e., an expected amount of 10% of the pixels) \n- --bottleneck_dropout: If this flag is set, dropout is also used in the bottleneck layer. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8896177150625915
      ],
      "excerpt": "- -s or --seed: Seeds the random number generator with the given seed in order to make the results deterministic. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9072561864273136
      ],
      "excerpt": "- -f or --fold: Determines which fold to use for testing (defaults to 0). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.944405972411621
      ],
      "excerpt": "- --stopped_epoch: Gives the epoch in which the last training was stopped. Load the model from data/Shapes/ml/experiment_N/snapshots and continue training with the next epoch (instead of starting from zero again). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8659554726636832,
        0.849589165211094,
        0.9879133258026189
      ],
      "excerpt": "- --optimizer: Define the optimizer to use (SGD or adam, defaults to adam). \n- --learning_rate: Initial learning rate for the optimizer, defaults to 0.0001. \n- --momentum: Weight of the momentum term for the optimizer, defaults to 0.9. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8859236017744613,
        0.8918603589991727,
        0.9183357844339242,
        0.8837865460530038
      ],
      "excerpt": "- --padding: Padding type for convolutions and max pooling when size reduction takes place. valid or same, defaults to valid. \n- --large_batch: If this flag is set, training uses a batch size of 256 instead of 128. \n- --initial_stride: Stride of the initial convolution, defaults to 2. \n- --image_size: Size of the quadratic input image in pixels per dimension, defaults to 128. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8367888245375714
      ],
      "excerpt": "- kendall_DISTANCE_WEIGHTS: In these columns, the kendall correlation between the bottleneck layer's activations and the dissimilarity ratings are reported. Here, DISTANCE gives the distance measure (Euclidean, Manhattan, or InnerProduct) and WEIGHTS indicates whether uniform or optimized weights were used. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9780043664364857,
        0.9780043664364857,
        0.8616323931876108,
        0.9176217604156798,
        0.9201144694066272,
        0.9201144694066272,
        0.9037699502642088,
        0.9820271029429648,
        0.9381105109573915
      ],
      "excerpt": "- berlin_loss: Value of the classification objective (i.e., the categorical cross-entropy loss) for the subset of TU Berlin data points and classes. \n- sketchy_loss: Value of the classification objective (i.e., the categorical cross-entropy loss) for the subset of Sketchy data points and classes. \n- mapping_loss: Value of the mapping objective (i.e., the MSE). \n- reconstruction_loss: Value of the reconstruction objective (i.e., the binary cross-entropy loss). \n- berlin_weighted_acc: The accuracy obtained with the classification output wrt TU Berlin data points and classes. \n- sketchy_weighted_acc: The accuracy obtained with the classification output wrt Sketchy data points and classes. \n- mapping_weighted_med: The mean Euclidean distance of the mapping task. \n- mapping_weighted_r2: The coefficient of determination of the mapping task. \nIn order to run a five-fold cross-validation, one therefore needs to execute run_ann.py five times, using a different test fold number for each call, and aggregating the results afterwards. The script will furthermore at the end of the evaluation create an hdf5 file in data/Shapes/ml/snapshots with the final configuration of the model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8896177150625915,
        0.9727328323716177,
        0.9576060440733418
      ],
      "excerpt": "- -s or --seed: Seeds the random number generator with the given seed in order to make the results deterministic. \n- -n or --noise_level: Specifies the level of salt and pepper noise to apply to the images (defaults to 0.0). \n- -i or --image_size: Size of the input image in pixels, defaults to 128. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Learning a mapping from images to psychological similarity spaces with neural networks.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lbechberger/LearningPsychologicalSpaces/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Wed, 29 Dec 2021 20:10:31 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/lbechberger/LearningPsychologicalSpaces/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "lbechberger/LearningPsychologicalSpaces",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/lbechberger/LearningPsychologicalSpaces/master/code/shell_scripts/clean_NOUN.sh",
      "https://raw.githubusercontent.com/lbechberger/LearningPsychologicalSpaces/master/code/shell_scripts/pipeline_Shapes.sh",
      "https://raw.githubusercontent.com/lbechberger/LearningPsychologicalSpaces/master/code/shell_scripts/clean_Shapes.sh",
      "https://raw.githubusercontent.com/lbechberger/LearningPsychologicalSpaces/master/code/shell_scripts/pipeline_NOUN.sh",
      "https://raw.githubusercontent.com/lbechberger/LearningPsychologicalSpaces/master/code/shell_scripts/NOUN/experiment_3.sh",
      "https://raw.githubusercontent.com/lbechberger/LearningPsychologicalSpaces/master/code/shell_scripts/NOUN/ml_setup.sh",
      "https://raw.githubusercontent.com/lbechberger/LearningPsychologicalSpaces/master/code/shell_scripts/NOUN/experiment_2.sh",
      "https://raw.githubusercontent.com/lbechberger/LearningPsychologicalSpaces/master/code/shell_scripts/NOUN/mds.sh",
      "https://raw.githubusercontent.com/lbechberger/LearningPsychologicalSpaces/master/code/shell_scripts/NOUN/correlation.sh",
      "https://raw.githubusercontent.com/lbechberger/LearningPsychologicalSpaces/master/code/shell_scripts/NOUN/experiment_1.sh",
      "https://raw.githubusercontent.com/lbechberger/LearningPsychologicalSpaces/master/code/shell_scripts/Shapes/space_analysis.sh",
      "https://raw.githubusercontent.com/lbechberger/LearningPsychologicalSpaces/master/code/shell_scripts/Shapes/experiment_3.sh",
      "https://raw.githubusercontent.com/lbechberger/LearningPsychologicalSpaces/master/code/shell_scripts/Shapes/ml_setup.sh",
      "https://raw.githubusercontent.com/lbechberger/LearningPsychologicalSpaces/master/code/shell_scripts/Shapes/experiment_6.sh",
      "https://raw.githubusercontent.com/lbechberger/LearningPsychologicalSpaces/master/code/shell_scripts/Shapes/experiment_8.sh",
      "https://raw.githubusercontent.com/lbechberger/LearningPsychologicalSpaces/master/code/shell_scripts/Shapes/experiment_2.sh",
      "https://raw.githubusercontent.com/lbechberger/LearningPsychologicalSpaces/master/code/shell_scripts/Shapes/tmp.sh",
      "https://raw.githubusercontent.com/lbechberger/LearningPsychologicalSpaces/master/code/shell_scripts/Shapes/experiment_9.sh",
      "https://raw.githubusercontent.com/lbechberger/LearningPsychologicalSpaces/master/code/shell_scripts/Shapes/data_analysis.sh",
      "https://raw.githubusercontent.com/lbechberger/LearningPsychologicalSpaces/master/code/shell_scripts/Shapes/experiment_1.sh",
      "https://raw.githubusercontent.com/lbechberger/LearningPsychologicalSpaces/master/code/shell_scripts/Shapes/experiment_4.sh",
      "https://raw.githubusercontent.com/lbechberger/LearningPsychologicalSpaces/master/code/shell_scripts/Shapes/experiment_7.sh",
      "https://raw.githubusercontent.com/lbechberger/LearningPsychologicalSpaces/master/code/shell_scripts/Shapes/experiment_5.sh"
    ],
    "technique": "File Exploration"
  },
  "identifier": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "https://doi.org/10.5281/zenodo.5524374",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "https://doi.org/10.5281/zenodo.4727605",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "https://doi.org/10.5281/zenodo.4061287",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "https://doi.org/10.5281/zenodo.3712917",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "https://doi.org/10.5281/zenodo.3340766",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "https://doi.org/10.5281/zenodo.1220053",
      "technique": "Regular expression"
    }
  ],
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Since our Shapes study makes use of multiple data sources and a specific augmentation process, we created a separate script called `prepare_Shapes_data.py` for this preprocessing stage. It can be invoked as follows:\n```python -m code.ml.preprocessing.prepare_Shapes_data path/to/folds_file.csv path/to/output_directory/ factor```\nHere, `folds_file.csv` is a csv file that contains the columns `path` (giving the relative path of the image file from the project's root directory) and `fold` (the fold to which this image belongs). For classification data, a column `class` indicates the image class, while for data with psychological similarity ratings, the column `id` gives the stimulus ID used in the similarity space. The script will read all images listed in the `path` column of the `folds_file.csv`, create `factor` augmented copies of each image (by scaling it to a random size between 168 and 224 and by randomly translating it afterwards). The resulting augmented images will be stored as individual png files in the given `output_directory` and an additional pickle file containing a (shuffled) list of paths and classes/ids is created in the same file.\n\nThe script takes the following optional arguments:\n- `-p` or `--pickle_output_folder`: If a pickle output similar to the one provided by `data_augmentation.py` is desired, you can define the output folder for the augmented images here.\n- `-n` or `--noise_prob`: A list of floats specifying the different noise levels of salt and pepper noise to be added in the pickle versions.\n- `-s` or `--seed`: Specify a seed for the random number generator in order to make the results deterministic. If no seed is given, then the random number generator is not seeded.\n- `-o` or `--output_size`: Size of the output image, defaults to 224.\n- `-m` or `--minimum_size`: Minimal size of the object, defaults to 168.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "In order to run a regression from images to MDS coordinates, multiple preprocessing steps are necessary. Firstly, we need to augment our data set by creating a large amount of slightly distorted image variants. This is done in order to achieve a data set of reasonable size for a machine learning task. Moreover, for each of the images, the target MDS coordinates need to be prepared. All scripts for these steps can be found in the `code/ml/preprocessing` folder.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "The folder `code/mds/data_analysis` contains some scripts for visualizing and analyzing the (preprocessed) data set. While the statistical analyses are done with specialized R scripts, other functionality is provided by python scripts.\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.873299911920015
      ],
      "excerpt": "Our scripts use TensorFlow 1.10 with Python 3.5 along with scikit-learn. You can find scripts for setting up a virtual environment with anaconda in the Utilities project. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8150694941445772
      ],
      "excerpt": "The script takes the following optional parameters: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8003804565409866
      ],
      "excerpt": "The script takes the following optional arguments: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8879924648483086
      ],
      "excerpt": "The script accepts the following optional parameters: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8401901989687841
      ],
      "excerpt": "python -m code.mds.data_analysis.average_images path/to/input_file.pickle path/to/image_folder \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8387905885744105
      ],
      "excerpt": "python -m code.mds.data_analysis.find_item_pair_differences path/to/visual.pickle path/to/conceptual.pickle \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8003804565409866
      ],
      "excerpt": "The script takes the following optional arguments: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8216177908267096
      ],
      "excerpt": "The script furthermore accepts the following optional arguments: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8003804565409866
      ],
      "excerpt": "The script takes the following optional arguments: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8762999117479574
      ],
      "excerpt": "python -m code.mds.similarity_spaces.create_baseline_spaces path/to/individual_ratings.pickle path/to/output.pickle n_spaces max_dims \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8295762108745083
      ],
      "excerpt": "Please note that one or more of the following flags must be set in order to specify the correlation metric(s) to use for visualization: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8003804565409866
      ],
      "excerpt": "The script takes the following optional arguments: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8003804565409866
      ],
      "excerpt": "The script takes the following optional arguments: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8003804565409866
      ],
      "excerpt": "The script takes the following optional arguments: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8150694941445772
      ],
      "excerpt": "The script takes the following optional parameters: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.866793111451647
      ],
      "excerpt": "python -m code.mds.directions.aggregate_direction_results path/to/input_folder/ n_dims path/to/output/folder \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9253051089833962
      ],
      "excerpt": "The network can be regularized by using the following optional arguments: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9311611082409588
      ],
      "excerpt": "Moreover, one can pass the following optional arguments: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8879924648483086
      ],
      "excerpt": "The script accepts the following optional parameters: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8189530879641809
      ],
      "excerpt": "- 'items': A dictionary using item names as keys and containing dictionaries as values. These dictionaries have the following elements: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8091742509446467
      ],
      "excerpt": "python -m code.mds.preprocessing.preprocess_Shape path/to/within.csv path/to/within_between.csv path/to/category_names.csv path/to/item_names.csv path/to/output.pickle path/to/output.csv rating_type \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.819073821574642
      ],
      "excerpt": "python -m code.preprocessing.aggregate_similarities path/to/input_file.pickle path/to/output_file.pickle path/to/output_folder_matrix path/to/output_file.csv rating_type \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8897139669495165
      ],
      "excerpt": "category_names.csv and item_names.csv should be the same files also used for preprocess_Shapes.py. The header of the two output csv files is item;ratingType;ratings. The output.pickle file contains a dictionary with the following structure: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.809286849727693,
        0.8143463081853177,
        0.8462581534383622
      ],
      "excerpt": "- -por --plot_folder: If a plot folder is given, the script creates a scatter plot of attentive vs. pre-attentive ratings and stores it in the given location. \n- -i or --image_folder: Path to the folder containing the images for the items. If given, it will use the item images to create scatter plots. If not given, an ordinary scatter plot will be used. \n- -z or --zoom: Determines the size of the item images in the scatter plot. Defaults to 0.15. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8143463081853177,
        0.8462581534383622
      ],
      "excerpt": "- -i or --image_folder: Path to the folder containing the images for the items. If given, it will use the item images to create scatter plots. If not given, an ordinary scatter plot will be used. \n- -z or --zoom: Determines the size of the item images in the scatter plot. Defaults to 0.15. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9095211003506105,
        0.8334387372806787
      ],
      "excerpt": "Here, input_file.pickle corresponds to the output file of preprocess_Shapes.py or preprocess_NOUN.py and image_folder points to the folder where all the original images reside. The script takes the following optional arguments: \n- -o or --output_folder: The destination folder for the output images, defaults to ., i.e., the current working directory. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8143288422410494
      ],
      "excerpt": "In order to visualize the similarity matrices, one can use the script plot_similarity_matrices.py. This script compares two sets of aggregated similarity ratings, given as first.pickle and second.pickle (output of aggregate_similarities.py). It creates one item-based heatmap (below diagonal: first set of similarities, above diagonal: second set of similarities) and two category-based heatmaps (same structure), and stores them as a two separate images output_folder/heatmap_First_Second_items.png and output_folder/heatmap_First_Second_categories.png. Moreover, it creates a scatter plot of the values in the two matrices and stores them as output_folder/scatter_First_Second.png. The script can be invoked as follows: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8486234079784739
      ],
      "excerpt": "- -f or --first_name: Descriptive name for the first set of similarities (defaults to 'First'). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8118678293430434,
        0.8374708872064769
      ],
      "excerpt": "Rscript code/mds/similarity_spaces/mds.r -d path/to/distance_matrix.csv -i path/to/item_names.csv -o path/to/output/directory \nHere, distance_matrix.csv is a CSV file which contains the matrix of pairwise dissimilarities and item_names.csv contains the item names (one name per row, same order as in the distance matrix). These two files have ideally been created by aggregate_similarities.py. The resulting vectors are stored in the given output directory. All three of these arguments are mandatory. Moreover, a CSV file is created in the output directory, which stores the stress values (metric stress and three variants of nonmetric stress) for each of the generated spaces.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8772443534409927
      ],
      "excerpt": "Here, pixel_file.csv is the output file of pixel_correlations.py and output_folder determines where the resulting plots are stored. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8003918861834989
      ],
      "excerpt": "Here, aggregated_ratings.pickle is again the output file of aggregate_similarities.py, distances.pickle contains pre-computed distances, and the results will be stored in output.csv. By default, the pre-computed distances from distances.pickle are used to compute the correlations. If however the optional flag -f or --feature_folder is given, all pickle files from this given folder are read (assumed to be generated by preprocess_feature.py or features_from_categories.py) and the underlying aggregated feature ratings are used to compute distances. In the latter case, the resulting distances are stored in distances.pickle for future use. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8134437014951887
      ],
      "excerpt": "For some further visualization, the script shepard_diagram.py can be used in order to create a Shepard plot (i.e., a scatter plot of predicted distances versus actual dissimilarities). It is invoked as follows: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9163830935914061
      ],
      "excerpt": "Here, aggregated_ratings.pickle is the output of aggregate_similarities.py, distances.pickle refers to the file generated by the respective correlation script (i.e., pixel_correlations.py, ann_correlations.py, feature_correlations.py, or mds_correlations.py), and output_image.png is the file name under which the scatter plot will be stored. There are three different modes for the scatter plot generation (based on the three correlation approaches) and exactly one of them must be picked via an optional argument: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8645660587660402
      ],
      "excerpt": "The resulting number of violations is stored in output_file.csv using the header dims,hull_category_type,intruder_category_type,data_source,violations. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8469911293988971
      ],
      "excerpt": "The resulting average category sizes are stored in output_file.csv using the header dims,category_type,data_source,size. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8498413600339951
      ],
      "excerpt": "python -m code.mds.directions.filter_directions path/to/input_file.csv direction_name n_dims path/to/output.csv \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8428539663779898
      ],
      "excerpt": "The script searches for all jpg images in the given image_folder, creates n augmented samples of each image and stores the results in the given output_folder (one pickle file per original image). The script takes the following optional command line arguments: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8062409394500897
      ],
      "excerpt": "- -a or --aggregator: Type of aggregator function to use. One of max, min, mean, median (default: mean). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8092761530592518
      ],
      "excerpt": "Here, features.pickle is the pickle file generated by either ann_features.py or pixel_features.py. The script takes the following optional arguments: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8017853849642267
      ],
      "excerpt": "Here, target_vectors.pickle is the file generated by prepare_targets.py, space_name is the name of a target space contained in this file, features.pickle contains the features to be used (either generated by ann_features.py or by pixel_features.py), folds.csv contains the fold structure (for each original image the number of the fold it belongs to), and output.csv is the file in which the results will be stored (the script appends to the file if it already exists). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8342953255181844
      ],
      "excerpt": "The script uses the giveninput_path_templateand the given number of foldsn_foldsto generate the paths to all individual csv files (e.g., if\u00ecnput_path_templateispath/to/{0}/results.csvandn_foldsis5, then it will look for the filespath/to/0/results.csv,path/to/1/results.csv, ...,path/to/4/results.csv). These individual csv files (which have been produced byregression.py) are read and the corresponding evaluation results are averaged across the different files. The aggregated results are then stored inoutput.csv`. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8177659309698684,
        0.821814458323047
      ],
      "excerpt": "- --walltime: Specifies the walltime in seconds before the job will be killed (relevant for grid execution). The script will try to stop its training before running over the walltime and store the current network weights in data/Shapes/ml/experiment_N/snapshots/ as an hdf5 file. \n- --stopped_epoch: Gives the epoch in which the last training was stopped. Load the model from data/Shapes/ml/experiment_N/snapshots and continue training with the next epoch (instead of starting from zero again). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8573562485669817
      ],
      "excerpt": "Each execution of run_ann.py appends one line to the given output.csv file, representing the results for the given test fold. The first column of output.csv encodes the overall setup used with a single signature string and the second column gives the number of the test fold. The remaining columns contain the following information:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8542208638472774
      ],
      "excerpt": "The script loads the model from model.h5 (output of run_ann.py) and passes all images from Shapes.pickle (output of prepare_Shapes_data.py) through it. It stores the bottleneck activations is the file output.pickle in a format analogous to the one provided by ann_features.py to allow for further processing by regression.py. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8195664282891512,
        0.8458601117091428,
        0.8010800326135894
      ],
      "excerpt": "The ANN script outputs one result line for each individual fold. In order to average the results across folds, you can invoke the script average_folds.py as follows: \npython -m code.ml.ann.average_folds path/to/input.csv path/to/output.csv \nThe script goes through the given input.csv file (produced by run_ann.py) and averages all evaluation columns over the different folds, storing the results in the same manner in output.csv (only removing the fold column). \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/lbechberger/LearningPsychologicalSpaces/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell",
      "R"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other",
      "url": "https://raw.githubusercontent.com/lbechberger/LearningPsychologicalSpaces/master/LICENSE"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2017-2021 Lucas Bechberger, Elektra Kypridemou, and Margit Scheibel\\n\\nCopyright of the images and similarity ratings of the NOUN database (http://www.sussex.ac.uk/wordlab/noun) belong to Jessica Horst and Michael Hout: \\nHorst, Jessica S., and Michael C. Hout. \"The Novel Object and Unusual Name (NOUN) Database: A collection of novel images for use in experimental research.\" Behavior research methods 48.4 (2016): 1393-1409.\\n\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "LearningPsychologicalSpaces",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "LearningPsychologicalSpaces",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "lbechberger",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lbechberger/LearningPsychologicalSpaces/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "lbechberger",
        "body": "- Added experiments with autoencoder structure",
        "dateCreated": "2021-09-23T09:51:08Z",
        "datePublished": "2021-09-23T09:54:04Z",
        "html_url": "https://github.com/lbechberger/LearningPsychologicalSpaces/releases/tag/v1.5",
        "name": "Machine Learning Study with CNNs on Shapes Data",
        "tag_name": "v1.5",
        "tarball_url": "https://api.github.com/repos/lbechberger/LearningPsychologicalSpaces/tarball/v1.5",
        "url": "https://api.github.com/repos/lbechberger/LearningPsychologicalSpaces/releases/50119359",
        "zipball_url": "https://api.github.com/repos/lbechberger/LearningPsychologicalSpaces/zipball/v1.5"
      },
      {
        "authorType": "User",
        "author_name": "lbechberger",
        "body": "- added code for training a CNN from scratch on sketches and line drawings\r\n- several experiments for transfer learning, multi-task learning, and generalization to other target spaces",
        "dateCreated": "2021-04-29T12:26:00Z",
        "datePublished": "2021-04-29T12:33:31Z",
        "html_url": "https://github.com/lbechberger/LearningPsychologicalSpaces/releases/tag/v1.4",
        "name": "Machine Learning Study with CNNs on Shapes Data",
        "tag_name": "v1.4",
        "tarball_url": "https://api.github.com/repos/lbechberger/LearningPsychologicalSpaces/tarball/v1.4",
        "url": "https://api.github.com/repos/lbechberger/LearningPsychologicalSpaces/releases/42199614",
        "zipball_url": "https://api.github.com/repos/lbechberger/LearningPsychologicalSpaces/zipball/v1.4"
      },
      {
        "authorType": "User",
        "author_name": "lbechberger",
        "body": "- major refactoring of the code base\r\n- added functionality with respect to Shapes study\r\n- re-ran experiments with updated code",
        "dateCreated": "2020-10-01T08:08:08Z",
        "datePublished": "2020-10-01T08:11:46Z",
        "html_url": "https://github.com/lbechberger/LearningPsychologicalSpaces/releases/tag/v1.3",
        "name": "Study on Multidimensional Scaling and Neural Networks on the NOUN Dataset",
        "tag_name": "v1.3",
        "tarball_url": "https://api.github.com/repos/lbechberger/LearningPsychologicalSpaces/tarball/v1.3",
        "url": "https://api.github.com/repos/lbechberger/LearningPsychologicalSpaces/releases/32031286",
        "zipball_url": "https://api.github.com/repos/lbechberger/LearningPsychologicalSpaces/zipball/v1.3"
      },
      {
        "authorType": "User",
        "author_name": "lbechberger",
        "body": "- Multiple minor updates on scripts (also with respect to the forthcoming Shapes study)\r\n- re-ran the whole pipeline (updated results)",
        "dateCreated": "2020-03-14T09:46:46Z",
        "datePublished": "2020-03-17T13:16:08Z",
        "html_url": "https://github.com/lbechberger/LearningPsychologicalSpaces/releases/tag/v1.2",
        "name": "Study on Multidimensional Scaling and Neural Networks on the NOUN Dataset",
        "tag_name": "v1.2",
        "tarball_url": "https://api.github.com/repos/lbechberger/LearningPsychologicalSpaces/tarball/v1.2",
        "url": "https://api.github.com/repos/lbechberger/LearningPsychologicalSpaces/releases/24594689",
        "zipball_url": "https://api.github.com/repos/lbechberger/LearningPsychologicalSpaces/zipball/v1.2"
      },
      {
        "authorType": "User",
        "author_name": "lbechberger",
        "body": "A thorough update of our study on the NOUN dataset:\r\n- analyzing different MDS algorithms (metric vs. nonmetric)\r\n- two pixel-based baselines (downscaled images and ANN activations)\r\n- Scree plot and correlation analysis\r\n- 3 machine learning experiments varying feature space, MDS algorithm, and dimensionality of target space",
        "dateCreated": "2019-07-18T08:21:45Z",
        "datePublished": "2019-07-18T08:35:51Z",
        "html_url": "https://github.com/lbechberger/LearningPsychologicalSpaces/releases/tag/v1.1",
        "name": "Study on Multidimensional Scaling and Neural Networks on the NOUN Dataset",
        "tag_name": "v1.1",
        "tarball_url": "https://api.github.com/repos/lbechberger/LearningPsychologicalSpaces/tarball/v1.1",
        "url": "https://api.github.com/repos/lbechberger/LearningPsychologicalSpaces/releases/18695836",
        "zipball_url": "https://api.github.com/repos/lbechberger/LearningPsychologicalSpaces/zipball/v1.1"
      },
      {
        "authorType": "User",
        "author_name": "lbechberger",
        "body": "A thorough update of our study on the NOUN dataset:\r\n- analyzing different MDS algorithms (metric vs. nonmetric)\r\n- two pixel-based baselines (downscaled images and ANN activations)\r\n- Scree plot and correlation analysis\r\n- 3 machine learning experiments varying feature space, MDS algorithm, and dimensionality of target space",
        "dateCreated": "2019-07-18T08:21:45Z",
        "datePublished": "2019-07-18T08:27:19Z",
        "html_url": "https://github.com/lbechberger/LearningPsychologicalSpaces/releases/tag/v1.0",
        "name": "Study on Multidimensional Scaling and Neural Networks on the NOUN Dataset",
        "tag_name": "v1.0",
        "tarball_url": "https://api.github.com/repos/lbechberger/LearningPsychologicalSpaces/tarball/v1.0",
        "url": "https://api.github.com/repos/lbechberger/LearningPsychologicalSpaces/releases/18695553",
        "zipball_url": "https://api.github.com/repos/lbechberger/LearningPsychologicalSpaces/zipball/v1.0"
      },
      {
        "authorType": "User",
        "author_name": "lbechberger",
        "body": "First pre-release of our code as used for the paper \"Mapping Images to Psychological Similarity Spaces Using Neural Networks\" (submitted to AIC 2018).",
        "dateCreated": "2018-03-22T07:18:44Z",
        "datePublished": "2018-04-18T08:06:01Z",
        "html_url": "https://github.com/lbechberger/LearningPsychologicalSpaces/releases/tag/v0.1",
        "name": "LearningPsychologicalSpaces v0.1",
        "tag_name": "v0.1",
        "tarball_url": "https://api.github.com/repos/lbechberger/LearningPsychologicalSpaces/tarball/v0.1",
        "url": "https://api.github.com/repos/lbechberger/LearningPsychologicalSpaces/releases/10598649",
        "zipball_url": "https://api.github.com/repos/lbechberger/LearningPsychologicalSpaces/zipball/v0.1"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 6,
      "date": "Wed, 29 Dec 2021 20:10:31 GMT"
    },
    "technique": "GitHub API"
  },
  "support": [
    {
      "confidence": [
        1
      ],
      "excerpt": "For the study on the NOUN data set, we compared the spaces obtainable by four MDS algorithms (classical MDS, Kruskal's MDS algorithm, metric SMACOF, and nonmetric SMACOF). We investigated both metric and nonmetric stress. Moreover, we computed the correlations between the pairwise distances in the MDS spaces and the dissimilarity ratings. For the latter analysis, we also compared to a pixel-based and an ANN-based baseline.\n\nIn a second part of the study, we then trained regressors from either downsampled images or ANN activation to the similarity spaces, investigating a linear regression, a random forest regression, and a lasso regression. We compared the results obtainable on spaces of different sizes and spaces generated by different MDS algorithms.\n\nThe script `code/shell_scripts/pipeline_NOUN.py` automatically executes all scripts necessary to reproduce our results. It requires one argument which can either be `paper` (in order to reproduce the results from our paper \"Generalizing Psychological Similarity Spaces to Unseen Stimuli - Combining Multidimensional Scaling with Artificial Neural Networks\") or `dissertation` (in order to reproduce a more comprehensive set of results discussed in the dissertation). The script then sets up some shell variables accordingly and calls the following five shell scripts which make up the five processing steps in our setup:\n- `code/shell_scripts/NOUN/mds.sh`: Preprocesses the dissimilarity ratings (stored in `data/NOUN/mds/similarities/`), applies the different MDS algorithms for target spaces of different sizes (resulting vectors are stored in `data/NOUN/mds/vectors`), and visualizes the spaces with 2D plots (stored in `data/NOUN/mds/visualizations/spaces/`)\n- `code/shell_scripts/NOUN/correlation.sh`: Computes different correlation metrics between the pairwise distances between items in the MDS spaces and the original dissimilarity ratings. Also computes the results for a pixel-based and an ANN-based baseline. All of the resulting values are stored in `data/NOUN/mds/correlations/`. Creates some line graphs illustrating how correlation develops based on the dimensionality of the MDS space, and the block size, respectively. These visualizations are stored in `data/NOUN/mds/visualizations/correlations/`.\n- `code/shell_scripts/NOUN/ml_setup.sh`: Creates a machine learning data set by appyling data augmentation to the original images, by preparing the target vectors in the similarity space, and by extracting ANN-based and pixel-based features. The resulting data set is stored in multiple files in `data/NOUN/ml/dataset/`.\n- `code/shell_scripts/NOUN/experiment_1.sh`: Executes the first machine learning experiment, where we analyze the performance of different regressors (linear regression, random forest regression, lasso regression) on different feature sets (ANN-based vs. pixel-based), using a fixed target space. The results are stored in `data/NOUN/ml/experiment_1/`.\n- `code/shell_scripts/NOUN/experiment_2.sh`: Executes the second machine learning experiment, where we analyze the performance on target spaces of the same size that have been created by different MDS algorithms. The results are stored in `data/NOUN/ml/experiment_2/`.\n- `code/shell_scripts/NOUN/experiment_3.sh`: Executes the second machine learning experiment, where we analyze the performance on target spaces of different dimensionality that have been created by a single MDS algorithm. The results are stored in `data/NOUN/ml/experiment_3/`.\n\nThe only files necessary to run all of these experiments are `data/NOUN/mds/raw_data/raw_distances.csv` (the original dissimilarity matrix from Horst and Hout's NOUN study), `data/NOUN/mds/raw_data/4D-vectors.csv` (the vectors of their four-dimensional similarity space), `data/NOUN/ml/targets.csv` (defining which similarity spaces are included as possible targets in the machine learning data set), and `data/NOUN/ml/folds.csv` (defining the structure of the folds for the cross validation). If the script `code/shell_scripts/clean_NOUN.sh` is executed, all files and folders except for the ones listed above are deleted.\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Our second study focuses on a single conceptual domain, namely the domain of shapes. The approach taken in this study is an extension of our work on the NOUN data set. It contains both the extraction and analysis of similarity spaces as well as learning a mapping from images into these similarity spaces with convolutional neural networks.\n\nThe script `code/shell_scripts/pipeline_Shapes.py` automatically executes all scripts necessary to reproduce our results. It requires one argument which can either be `mds` (in order to reproduce the results from our forthcoming paper analyzing the similarity spaces only), `ml` (in order to reproduce our machine learning results) or `dissertation` (in order to reproduce a more comprehensive set of results presented in the dissertation). The script then sets up some shell variables accordingly and calls the following shell scripts which make up the processing steps in our setup:\n- `code/shell_scripts/Shapes/data_analysis.sh`: Preprocesses the input data about conceptual and visual similarity and about three psychological features. Also does some simple analyses of the data set and produces some helpful visualizations.\n- `code/shell_scripts/Shapes/space_analysis.sh`: Extracts similarity spaces from the data set and analyzes them with respect to three criteria: Do the distances accurately reflect dissimilarities (compares the MDS spaces to the pixel baseline, the ANN baseline, and a baseline using the psychological features)? Are conceptual regions well-formed (i.e., non-overlapping, small, and convex)? Can the psychological features be identified as directions in the similarity spaces?\n- `code/shell_scripts/Shapes/ml_setup.sh`: Creates a machine learning data set by appyling data augmentation to the line drawings as well as the Sketchy and TU Berlin data sets of sketches, by preparing the target vectors in the similarity space, and by extracting ANN-based features. The resulting data set is stored in multiple files in `data/Shapes/ml/dataset/`.\n- `code/shell_scripts/Shapes/experiment_1.sh`: Investigate mapping performance of a transfer learning task (linear and lasso regression) on top of the pre-trained photo-based inception-v3 network.\n- `code/shell_scripts/Shapes/experiment_2.sh`: Train the modified Sketch-a-Net architecture on the classification task and try to find promising hyperparameter settings through a grid search.\n- `code/shell_scripts/Shapes/experiment_3.sh`: Transfer learning (linear and lasso regression) on top of the network configurations from experiment 2.\n- `code/shell_scripts/Shapes/experiment_4.sh`: Multi-task learning (network optimizes classification and mapping performance at the same time) for the hyperparameter configurations from experiment 2.\n- `code/shell_scripts/Shapes/experiment_5.sh`: Applying the most promising configuration from experiments 1, 3, and 4, respectively, without any further modification to target spaces of different dimensionality.\n\nAll files are stored in `data/Shapes` which has the following structure:\n- `raw_data`: Contains the original input csv files with all ratings. Must be present to execute our pipeline.\n- `images`: Contains the images of our stimuli. Unfortunatley, due to copyright restrictions, we are not allowed to publish the original images online. Our scripts can be run without using the original images, though some results (e.g., the pixel baseline or all ML experiments) can then not be reproduced. Please contact us if you are interested in using the images for your own studies!\n  - `Berlin-svg`: Original svg vector graphics of the [TU Berlin data set](http://cybertron.cg.tu-berlin.de/eitz/projects/classifysketch/).\n  - `Berlin`: Generated png files of the TU Berlin data set.\n  - `Sketchy`: Original png files of the [Sketchy data set](http://sketchy.eye.gatech.edu/).\n- `mds`: All the (intermediate) results from our analysis with respect to the psychological ratings.\n  - `similarities`: Contains the pre-processed individual and aggregated similarity ratings as well as the vectors produced by MDS, all as pickle files. Distinguishes between visual and conceptual similarity (aggregated by median, subfolder `rating_type`) and between mean and median aggregation (only visual similarity, subfolder `aggregator`).\n  - `features`: Contains the pre-processed individual and aggregated ratings with respect to the psychological features as well as two category-based features.\n  - `data_set`: Contains the most important information extracted in our analysis in the form of CSV files for easier reuse by other researchers.\n    - `individual`: Individual ratings with respect to both `features` and `similarities`.\n    - `aggregated`: Aggregated ratings with respect to both `features` and `similarities`.\n    - `spaces`: Information about our similarity spaces, containing the `coordinates` of the individual stimuli as well as the `directions` corresponding to the psychological features. In both cases, we make a further distinction into `mean` and `median` aggregation.\n  - `analysis`: Contains our analysis results with respect to the similarity spaces: `correlations` to dissimilarities (including the baselines), well-formedness of conceptual `regions`, and the presence of interpretable `directions`. In each case, we make a further distinction into `mean` and `median` aggregation.\n  - `visualizations`: Contains various visualizations created by our scripts (`average_images` of the categories, `correlations` between distances and dissimilarities, psychological `features`, the `similarity_matrices`, and of course the similarity `spaces` themselves).\n- `ml`: All results from our machine learning experiments.\n  - `dataset`: The data set used for training the neural network, structured into five folds. Also contains pickle files with the activation vectors of a pre-trained photo-based CNN for different levels of input noise.\n  - `experiment_N`: Results, logs, and network weights for the respective experiment.\n\nAs for the NOUN study, the script `code/shell_scripts/clean_Shapes.sh` removes everything but `data/Shapes/raw_data/`, `data/Shapes/images/`, `data/Shapes/ml/folds`, and `data/Shapes/ml/regression_targets.csv`.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Since our Shapes study makes use of multiple data sources and a specific augmentation process, we created a separate script called `prepare_Shapes_data.py` for this preprocessing stage. It can be invoked as follows:\n```python -m code.ml.preprocessing.prepare_Shapes_data path/to/folds_file.csv path/to/output_directory/ factor```\nHere, `folds_file.csv` is a csv file that contains the columns `path` (giving the relative path of the image file from the project's root directory) and `fold` (the fold to which this image belongs). For classification data, a column `class` indicates the image class, while for data with psychological similarity ratings, the column `id` gives the stimulus ID used in the similarity space. The script will read all images listed in the `path` column of the `folds_file.csv`, create `factor` augmented copies of each image (by scaling it to a random size between 168 and 224 and by randomly translating it afterwards). The resulting augmented images will be stored as individual png files in the given `output_directory` and an additional pickle file containing a (shuffled) list of paths and classes/ids is created in the same file.\n\nThe script takes the following optional arguments:\n- `-p` or `--pickle_output_folder`: If a pickle output similar to the one provided by `data_augmentation.py` is desired, you can define the output folder for the augmented images here.\n- `-n` or `--noise_prob`: A list of floats specifying the different noise levels of salt and pepper noise to be added in the pickle versions.\n- `-s` or `--seed`: Specify a seed for the random number generator in order to make the results deterministic. If no seed is given, then the random number generator is not seeded.\n- `-o` or `--output_size`: Size of the output image, defaults to 224.\n- `-m` or `--minimum_size`: Minimal size of the object, defaults to 168.\n\n",
      "technique": "Header extraction"
    }
  ]
}