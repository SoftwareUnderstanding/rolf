{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1609.03499",
      "https://arxiv.org/abs/1711.10433",
      "https://arxiv.org/abs/1712.05884",
      "https://arxiv.org/abs/1710.07654",
      "https://arxiv.org/abs/1609.03499, Sep 2016.](https://arxiv.org/abs/1609.03499)\n- [Aaron van den Oord, Yazhe Li, Igor Babuschkin, et al, \"Parallel WaveNet: Fast High-Fidelity Speech Synthesis\", \thttps://arxiv.org/abs/1711.10433, Nov 2017.](https://arxiv.org/abs/1711.10433)\n- [Tamamori, Akira, et al. \"Speaker-dependent WaveNet vocoder.\" Proceedings of Interspeech. 2017.](http://www.isca-speech.org/archive/Interspeech_2017/pdfs/0314.PDF)\n- [Jonathan Shen, Ruoming Pang, Ron J. Weiss, et al, \"Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions\", https://arxiv.org/abs/1712.05884, Dec 2017.](https://arxiv.org/abs/1712.05884)\n- [Wei Ping, Kainan Peng, Andrew Gibiansky, et al, \"Deep Voice 3: 2000-Speaker Neural Text-to-Speech\", https://arxiv.org/abs/1710.07654, Oct. 2017.](https://arxiv.org/abs/1710.07654)",
      "https://arxiv.org/abs/1711.10433, Nov 2017.](https://arxiv.org/abs/1711.10433)\n- [Tamamori, Akira, et al. \"Speaker-dependent WaveNet vocoder.\" Proceedings of Interspeech. 2017.](http://www.isca-speech.org/archive/Interspeech_2017/pdfs/0314.PDF)\n- [Jonathan Shen, Ruoming Pang, Ron J. Weiss, et al, \"Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions\", https://arxiv.org/abs/1712.05884, Dec 2017.](https://arxiv.org/abs/1712.05884)\n- [Wei Ping, Kainan Peng, Andrew Gibiansky, et al, \"Deep Voice 3: 2000-Speaker Neural Text-to-Speech\", https://arxiv.org/abs/1710.07654, Oct. 2017.](https://arxiv.org/abs/1710.07654)",
      "https://arxiv.org/abs/1712.05884, Dec 2017.](https://arxiv.org/abs/1712.05884)\n- [Wei Ping, Kainan Peng, Andrew Gibiansky, et al, \"Deep Voice 3: 2000-Speaker Neural Text-to-Speech\", https://arxiv.org/abs/1710.07654, Oct. 2017.](https://arxiv.org/abs/1710.07654)",
      "https://arxiv.org/abs/1710.07654, Oct. 2017.](https://arxiv.org/abs/1710.07654)"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- [Aaron van den Oord, Sander Dieleman, Heiga Zen, et al, \"WaveNet: A Generative Model for Raw Audio\", \tarXiv:1609.03499, Sep 2016.](https://arxiv.org/abs/1609.03499)\n- [Aaron van den Oord, Yazhe Li, Igor Babuschkin, et al, \"Parallel WaveNet: Fast High-Fidelity Speech Synthesis\", \tarXiv:1711.10433, Nov 2017.](https://arxiv.org/abs/1711.10433)\n- [Tamamori, Akira, et al. \"Speaker-dependent WaveNet vocoder.\" Proceedings of Interspeech. 2017.](http://www.isca-speech.org/archive/Interspeech_2017/pdfs/0314.PDF)\n- [Jonathan Shen, Ruoming Pang, Ron J. Weiss, et al, \"Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions\", arXiv:1712.05884, Dec 2017.](https://arxiv.org/abs/1712.05884)\n- [Wei Ping, Kainan Peng, Andrew Gibiansky, et al, \"Deep Voice 3: 2000-Speaker Neural Text-to-Speech\", arXiv:1710.07654, Oct. 2017.](https://arxiv.org/abs/1710.07654)\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8194344494262951
      ],
      "excerpt": "See https://github.com/r9y9/wavenet_vocoder/issues/1 for planned TODOs and current progress. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559,
        0.8955886365383559
      ],
      "excerpt": "| link                      | LJSpeech   | link                | 489e6fa | 1000k~  steps | \n| link | CMU ARCTIC | link | b1a1076   | 740k steps    | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8714162992508173
      ],
      "excerpt": "cmu_arctic (multi-speaker) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/HaiFengZeng/clari_wavenet_vocoder",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-07-25T01:49:18Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-28T22:15:25Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9965182632123318
      ],
      "excerpt": "The goal of the repository is to provide an implementation of the WaveNet vocoder, which can generate high quality raw speech samples conditioned on linguistic or acoustic features. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9879098645035412
      ],
      "excerpt": "Focus on local and global conditioning of WaveNet, which is essential for vocoder. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "  --conditional=./data/ljspeech/ljspeech-mel-00001.npy \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9745262405052527
      ],
      "excerpt": "There are many hyper parameters to be turned depends on data. For typical datasets, parameters known to work good (preset) are provided in the repository. See presets directory for details. Notice that \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.903158379432229
      ],
      "excerpt": "instead of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9792294258294568
      ],
      "excerpt": "When this is done, you will see time-aligned extracted features (pairs of audio and mel-spectrogram) in ./data/cmu_arctic. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9287919830445078
      ],
      "excerpt": "In [2]: [(i, s) for (i,s) in enumerate(cmu_arctic.available_speakers)] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8691877914069419
      ],
      "excerpt": "You have to disable global and local conditioning by setting gin_channels and cin_channels to negative values. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8758585211655915
      ],
      "excerpt": "--conditional=&lt;path&gt;: (Required for onditional WaveNet) Path of local conditional features (.npy). If this is specified, number of time steps to generate is determined by the size of conditional feature. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "    --conditional=./data/cmu_arctic/cmu_arctic-mel-00001.npy \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.969933983919894,
        0.8891668507096941
      ],
      "excerpt": "--data-root: Data root. This is required to collect testset. \n--num-utterances: (For multi-speaker model) number of utterances to be generated per speaker. This is useful especially when testset is large and don't want to generate all utterances. For single speaker dataset, you can hit ctrl-c whenever you want to stop evaluation. \n",
      "technique": "Supervised classification"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- CMU ARCTIC (en): http://festvox.org/cmu_arctic/\n- LJSpeech (en): https://keithito.com/LJ-Speech-Dataset/\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/HaiFengZeng/clari_wavenet_vocoder/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 17,
      "date": "Tue, 28 Dec 2021 20:34:03 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/HaiFengZeng/clari_wavenet_vocoder/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "HaiFengZeng/clari_wavenet_vocoder",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/HaiFengZeng/clari_wavenet_vocoder/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/HaiFengZeng/clari_wavenet_vocoder/master/release.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The repository contains a core library (PyTorch implementation of the WaveNet) and utility scripts. All the library and its dependencies can be installed by:\n\n```\ngit clone https://github.com/r9y9/wavenet_vocoder\ncd wavenet_vocoder\npip install -e \".[train]\"\n```\n\nIf you only need the library part, then you can install it by the following command:\n\n```\npip install wavenet_vocoder\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8379916309037938
      ],
      "excerpt": "See https://github.com/r9y9/wavenet_vocoder/issues/1 for planned TODOs and current progress. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9206633769338113
      ],
      "excerpt": "git checkout ${commit_hash} \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9206633769338113
      ],
      "excerpt": "git checkout 489e6fa \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8066128589755514
      ],
      "excerpt": "Logs are dumped in ./log directory by default. You can monitor logs by tensorboard: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8243382563818691
      ],
      "excerpt": "Note: This is not a text-to-speech (TTS) model. With a pre-trained model provided here, you can synthesize waveform given a mel spectrogram, not raw text. Pre-trained models for TTS are planed to be released once I finish up deepvoice3_pytorch/#21. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.874742116990516
      ],
      "excerpt": ": pretrained model (20180127_mixture_lj_checkpoint_step000410000_ema.pth) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.922738784878987,
        0.865954685112347
      ],
      "excerpt": "python preprocess.py ljspeech ~/data/LJSpeech-1.0 ./data/ljspeech \npython synthesis.py --hparams=\"input_type=raw,quantize_channels=65536,out_channels=30\" \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9418908076833293,
        0.932615055374559
      ],
      "excerpt": "python preprocess.py --preset=presets/cmu_arctic_8bit.json cmu_arctic ~/data/cmu_arctic \npython train.py --preset=presets/cmu_arctic_8bit.json --data-root=./data/cmu_arctic \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9382892056106165
      ],
      "excerpt": "python preprocess.py cmu_arctic ~/data/cmu_arctic \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.932615055374559
      ],
      "excerpt": "python train.py --preset=presets/cmu_arctic_8bit.json --data-root=./data/cmu_arctic \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9372828227895622
      ],
      "excerpt": "python preprocess.py cmu_arctic ~/data/cmu_arctic ./data/cmu_arctic --preset=presets/cmu_arctic_8bit.json \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589195817767492
      ],
      "excerpt": "In [1]: from nnmnkwii.datasets import cmu_arctic \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9134475537355338
      ],
      "excerpt": "python train.py --data-root=./data/cmu_arctic/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9017014390446443
      ],
      "excerpt": "python train.py --data-root=./data/cmu_arctic/ --speaker-id=0 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9134475537355338
      ],
      "excerpt": "python train.py --data-root=./data/cmu_arctic/ \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8322621917438231
      ],
      "excerpt": "python synthesis.py --hparams=\"parameters you want to override\" \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8319102384399367
      ],
      "excerpt": "python evaluate.py ${checkpoint_path} ${output_dir} --data-root=\"data location\"\\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8574814847726171
      ],
      "excerpt": "python evaluate.py --data-root=./data/cmu_arctic/ \\ \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/HaiFengZeng/clari_wavenet_vocoder/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'The wavenet_vocoder package is licensed under the MIT \"Expat\" License:\\n\\n> Copyright (c) 2017: Ryuichi Yamamoto.\\n>\\n> Permission is hereby granted, free of charge, to any person obtaining\\n> a copy of this software and associated documentation files (the\\n> \"Software\"), to deal in the Software without restriction, including\\n> without limitation the rights to use, copy, modify, merge, publish,\\n> distribute, sublicense, and/or sell copies of the Software, and to\\n> permit persons to whom the Software is furnished to do so, subject to\\n> the following conditions:\\n>\\n> The above copyright notice and this permission notice shall be\\n> included in all copies or substantial portions of the Software.\\n>\\n> THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\\n> EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\\n> MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\\n> IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\\n> CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\\n> TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\\n> SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "WaveNet vocoder",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "clari_wavenet_vocoder",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "HaiFengZeng",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/HaiFengZeng/clari_wavenet_vocoder/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Python 3\n- CUDA >= 8.0\n- TensorFlow >= v1.3\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 50,
      "date": "Tue, 28 Dec 2021 20:34:03 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "clarinet"
    ],
    "technique": "GitHub API"
  }
}