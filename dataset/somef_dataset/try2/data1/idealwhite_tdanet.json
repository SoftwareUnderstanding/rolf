{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2004.03212"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you use this code for your research, please cite our paper.\n```\n@inproceedings{10.1145/3394171.3414017,\nauthor = {Zhang, Lisai and Chen, Qingcai and Hu, Baotian and Jiang, Shuoran},\ntitle = {Text-Guided Neural Image Inpainting},\nyear = {2020},\nbooktitle = {Proceedings of the 28th ACM International Conference on Multimedia},\npages = {1302\u20131310},\nlocation = {Seattle, WA, USA},\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{10.1145/3394171.3414017,\nauthor = {Zhang, Lisai and Chen, Qingcai and Hu, Baotian and Jiang, Shuoran},\ntitle = {Text-Guided Neural Image Inpainting},\nyear = {2020},\nbooktitle = {Proceedings of the 28th ACM International Conference on Multimedia},\npages = {1302\u20131310},\nlocation = {Seattle, WA, USA},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9490753289412834
      ],
      "excerpt": "MM | ArXiv  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9455180209979359
      ],
      "excerpt": "by Lisai Zhang, Qingcai Chen, Baotian Hu and Shuoran Jiang. Given one masked image, the proposed  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8530268843043954
      ],
      "excerpt": "COCO: object detection 2014 datset from MS COCO. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/idealwhite/tdanet",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-08-10T14:21:01Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-06T07:42:07Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8844948584247287
      ],
      "excerpt": "This repository implements the paper \"Text-Guided Neural Image Inpainting\"  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "The pytorch implementation of the paper \"text-guided neural image inpainting\" at MM'2020",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/idealwhite/tdanet/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 8,
      "date": "Sat, 25 Dec 2021 07:23:35 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/idealwhite/tdanet/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "idealwhite/tdanet",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This code was tested with Pytoch 1.2.0, CUDA 10.1, Python 3.6 and Ubuntu 16.04 with a 2080Ti GPU\n\n- Install Pytoch 1.2.0, torchvision, and other dependencies from [http://pytorch.org](http://pytorch.org)\n- Install python libraries [visdom](https://github.com/facebookresearch/visdom) and [dominate](https://github.com/Knio/dominate) for visualization\n\n\n```\npip install visdom dominate\n```\n- Clone this repo (we suggest to only clone the depth 1 version):\n\n```\ngit clone https://github.com/idealwhite/tdanet --depth 1\ncd tdanet\n```\n- Download the dataset and pre-processed files as in following steps.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8087402773693313,
        0.8224689639185473,
        0.881460743319288,
        0.999746712887969
      ],
      "excerpt": " to dataset/ directory as specified in config.bird.yml/config.coco.yml. \nDownload the pre-trained models bird inpainting or coco inpainting and put them undercheckpoints/ directory. \nInstall the PyQt5 for GUI operation \npip install PyQt5 \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8134744398554441
      ],
      "excerpt": "Download the pre-trained models bird inpainting or coco inpainting and put them undercheckpoints/ directory. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/idealwhite/tdanet/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "TDANet: Text-Guided Neural Image Inpainting, MM'2020 (Oral)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "tdanet",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "idealwhite",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/idealwhite/tdanet/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 35,
      "date": "Sat, 25 Dec 2021 07:23:35 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "image-inpainting",
      "image-completion",
      "image-restoration",
      "text-to-image-synthesis"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "<img src='https://github.com/idealwhite/tdanet/blob/master/images/inpainting_example.png' align=\"center\">\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "<img src='https://github.com/idealwhite/tdanet/blob/master/images/manipulation_example.png' align=\"center\">\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```\npython train.py --name tda_bird  --gpu_ids 0 --model tdanet --mask_type 0 1 2 3 --img_file ./datasets/CUB_200_2011/train.flist --mask_file ./datasets/CUB_200_2011/train_mask.flist --text_config config.bird.yml\n```\n- **Important:** Add ```--mask_type``` in options/base_options.py for different training masks. ```--mask_file``` path is needed for **object mask**, use train_mask.flist for CUB and image_mask_coco_all.json for COCO. ```--text_config``` refer to the yml configuration file for text setup, ```--img_file``` is the image file dir or file list.\n- To view training results and loss plots, run ```python -m visdom.server``` and copy the URL [http://localhost:8097](http://localhost:8097).\n- Training models will be saved under the **./checkpoints** folder.\n- More training options can be found in **./options** folder.\n- **Suggestion:** use mask type 0 1 2 3 for CUB dataset and 0 1 2 4 for COCO dataset. Train more than 2000 epochs for CUB and 200 epochs for COCO. \n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Test \n```\npython test.py --name tda_bird  --img_file datasets/CUB_200_2011/test.flist --results_dir results/tda_bird  --mask_file datasets/CUB_200_2011/test_mask.flist --mask_type 3 --no_shuffle --gpu_ids 0 --nsampling 1 --no_variance\n```\n**Note**: \n- Remember to add  the ```--no_variance``` option to get better performance.  \n- For COCO object mask, use image_mask_coco_all.json as the mask file..\n\nA ```eval_tda_bird.flist``` will be generated after the test. Then in the evaluation, this file is used as the ground truth file list:\n\n```\npython evaluation.py --batch_test 60 --ground_truth_path eval_tda_bird.flist --save_path results/tda_bird\n```\n- Add ```--ground_truth_path``` to the dir of ground truth image path or list. ```--save_path``` as the result dir.\n\n\n",
      "technique": "Header extraction"
    }
  ]
}