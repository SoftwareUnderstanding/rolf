{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1612.03716",
      "https://arxiv.org/abs/1405.0312",
      "https://arxiv.org/abs/1412.7062",
      "https://arxiv.org/abs/1612.03716*, 2017.<br />\n\n- [2] [Microsoft COCO: Common Objects in Context](https://arxiv.org/abs/1405.0312)<br />\nT.-Y. Lin, M. Maire, S. Belongie et al.,<br />\nIn *European Conference in Computer Vision* (ECCV), 2014.<br />\n\n- [3] [Fully convolutional networks for semantic segmentation](http://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Long_Fully_Convolutional_Networks_2015_CVPR_paper.html)<br />\nJ. Long, E. Shelhammer and T. Darrell,<br />\nIn *Computer Vision and Pattern Recognition* (CVPR), 2015.<br />\n\n- [4] [Semantic image segmentation with deep convolutional nets and fully connected CRFs](https://arxiv.org/abs/1412.7062)<br />\nL.-C. Chen, G. Papandreou, I. Kokkinos et al.,<br />\nIn *International Conference on Learning Representations* (ICLR), 2015.<br />\n\n- [5] [LabelBank: Revisiting Global Perspectives for Semantic Segmentation](https://arxiv.org/pdf/1703.09891.pdf)<br />\nH. Hu, Z. Deng, G.-T. Zhou et al.<br />\nIn *arXiv preprint https://arxiv.org/abs/1703.09891*, 2017.<br />\n\n- [6] [Scene Segmentation with DAG-Recurrent Neural Networks](http://ieeexplore.ieee.org/abstract/document/7940028/)<br />\nB. Shuai, Z. Zuo, B. Wang<br />\nIn *IEEE Transactions on Pattern Analysis and Machine Intelligence* (PAMI), 2017.<br />\n\n### Licensing\nCOCO-Stuff is a derivative work of the COCO dataset. The authors of COCO do not in any form endorse this work. Different licenses apply:\n- COCO images: [Flickr Terms of use](http://mscoco.org/terms_of_use/)\n- COCO annotations: [Creative Commons Attribution 4.0 License](http://mscoco.org/terms_of_use/)\n- COCO-Stuff annotations & code: [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/legalcode)\n\n### Contact\nIf you have any questions regarding this dataset, please contact us at holger-at-it-caesar.com.",
      "https://arxiv.org/abs/1703.09891*, 2017.<br />\n\n- [6] [Scene Segmentation with DAG-Recurrent Neural Networks](http://ieeexplore.ieee.org/abstract/document/7940028/)<br />\nB. Shuai, Z. Zuo, B. Wang<br />\nIn *IEEE Transactions on Pattern Analysis and Machine Intelligence* (PAMI), 2017.<br />\n\n### Licensing\nCOCO-Stuff is a derivative work of the COCO dataset. The authors of COCO do not in any form endorse this work. Different licenses apply:\n- COCO images: [Flickr Terms of use](http://mscoco.org/terms_of_use/)\n- COCO annotations: [Creative Commons Attribution 4.0 License](http://mscoco.org/terms_of_use/)\n- COCO-Stuff annotations & code: [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/legalcode)\n\n### Contact\nIf you have any questions regarding this dataset, please contact us at holger-at-it-caesar.com."
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- [1] [COCO-Stuff: Thing and Stuff Classes in Context](https://arxiv.org/abs/1612.03716)<br />\nH. Caesar, J. Uijlings, V. Ferrari,<br />\nIn *arXiv preprint arXiv:1612.03716*, 2017.<br />\n\n- [2] [Microsoft COCO: Common Objects in Context](https://arxiv.org/abs/1405.0312)<br />\nT.-Y. Lin, M. Maire, S. Belongie et al.,<br />\nIn *European Conference in Computer Vision* (ECCV), 2014.<br />\n\n- [3] [Fully convolutional networks for semantic segmentation](http://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Long_Fully_Convolutional_Networks_2015_CVPR_paper.html)<br />\nJ. Long, E. Shelhammer and T. Darrell,<br />\nIn *Computer Vision and Pattern Recognition* (CVPR), 2015.<br />\n\n- [4] [Semantic image segmentation with deep convolutional nets and fully connected CRFs](https://arxiv.org/abs/1412.7062)<br />\nL.-C. Chen, G. Papandreou, I. Kokkinos et al.,<br />\nIn *International Conference on Learning Representations* (ICLR), 2015.<br />\n\n- [5] [LabelBank: Revisiting Global Perspectives for Semantic Segmentation](https://arxiv.org/pdf/1703.09891.pdf)<br />\nH. Hu, Z. Deng, G.-T. Zhou et al.<br />\nIn *arXiv preprint arXiv:1703.09891*, 2017.<br />\n\n- [6] [Scene Segmentation with DAG-Recurrent Neural Networks](http://ieeexplore.ieee.org/abstract/document/7940028/)<br />\nB. Shuai, Z. Zuo, B. Wang<br />\nIn *IEEE Transactions on Pattern Analysis and Machine Intelligence* (PAMI), 2017.<br />\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8242042398837521
      ],
      "excerpt": "Results and Future Plans \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.808498626019349
      ],
      "excerpt": "06 Apr 2017: Dataset version 1.1: Modified label indices \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9222383658450612,
        0.9930590849900288
      ],
      "excerpt": "09 Mar 2017: Added label hierarchy scripts \n08 Mar 2017: Corrections to table 2 in arXiv paper [1] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9987577913127723
      ],
      "excerpt": "12 Dec 2016: Dataset version 1.0 and arXiv paper [1] released \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8155157755211057
      ],
      "excerpt": "For the updated table please click here. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/nightrome/cocostuff10k",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you have any questions regarding this dataset, please contact us at holger-at-it-caesar.com.\n",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2016-12-08T13:06:53Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-25T13:27:37Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9344339575555055
      ],
      "excerpt": "The current release of COCO-Stuff-10K publishes both the training and test annotations and users report their performance individually. We invite users to report their results to us to complement this table. In the near future we will extend COCO-Stuff to all images in COCO and organize an official challenge where the test annotations will only be known to the organizers. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8877191392810879
      ],
      "excerpt": "FCN-16s [3]           | [1]   | 34.0%                   | 52.0%           | 22.7%    | - \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8877191392810879
      ],
      "excerpt": "FCN-8s [3]            | [6]   | 38.5%                   | 60.4%           | 27.2%    | - \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8177915740299866
      ],
      "excerpt": "- S: The pixel-wise label map of size [height x width]. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8401262541676303,
        0.9781833463456537,
        0.953225514938475,
        0.9278846763492331
      ],
      "excerpt": "- captions: Image captions from [2] that are annotated by 5 distinct humans on average. \n- regionMapStuff: A map of the same size as S that contains the indices for the approx. 1000 regions (superpixels) used to annotate the image. \n- regionLabelsStuff: A list of the stuff labels for each superpixel. The indices in regionMapStuff correspond to the entries in regionLabelsStuff. \nAlternatively, we also provide stuff and thing annotations in the COCO-style JSON format. The thing annotations are copied from COCO. We encode every stuff class present in an image as a single annotation using the RLE encoding format of COCO. To get the annotations: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8949650609022466
      ],
      "excerpt": "To be compatible with COCO, version 1.1 of COCO-Stuff has 91 thing classes (1-91), 91 stuff classes (92-182) and 1 class \"unlabeled\" (0). Note that 11 of the thing classes from COCO 2015 do not have any segmentation annotations. The classes desk, door and mirror could be either stuff or things and therefore occur in both COCO and COCO-Stuff. To avoid confusion we add the suffix \"-stuff\" to those classes in COCO-Stuff. The full list of classes can be found here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9254807187265376
      ],
      "excerpt": "The hierarchy of labels is stored in CocoStuffClasses. To visualize it, run CocoStuffClasses.showClassHierarchyStuffThings() (also available for just stuff and just thing classes) in Matlab. The output should look similar to the following figure: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9269046872396174
      ],
      "excerpt": "To encourage further research of stuff and things we provide the trained semantic segmentation model (see Sect. 4.4 in [1]). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9712762289660558
      ],
      "excerpt": "The default Deeplab model performs center crops of size 513*513 pixels of an image, if any side is larger than that. Since we want to segment the whole image at test time, we choose to resize the images to 513x513, perform the semantic segmentation and then rescale it elsewhere. Note that without the final step, the performance might differ slightly. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9596597620334246,
        0.9152170965072403,
        0.8204551367111874
      ],
      "excerpt": "In [1] we present a simple and efficient stuff annotation tool which was used to annotate the COCO-Stuff dataset. It uses a paintbrush tool to annotate SLICO superpixels (precomputed using the code of Achanta et al.) with stuff labels. These annotations are overlaid with the existing pixel-level thing annotations from COCO. \nWe provide a basic version of our annotation tool: \n- Prepare the required data: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.899113351841384
      ],
      "excerpt": "  - To enable or disable superpixels, thing annotations and polygon drawing, take a look at the flags at the top of CocoStuffAnnotator.m. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9253931891292555
      ],
      "excerpt": "COCO-Stuff is a derivative work of the COCO dataset. The authors of COCO do not in any form endorse this work. Different licenses apply: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "The official homepage of the (outdated) COCO-Stuff 10K dataset.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/nightrome/cocostuff10k/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 53,
      "date": "Wed, 22 Dec 2021 03:11:17 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/nightrome/cocostuff10k/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "nightrome/cocostuff10k",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/nightrome/cocostuff10k/master/models/deeplab/run_cocostuff_resnet101.sh",
      "https://raw.githubusercontent.com/nightrome/cocostuff10k/master/models/deeplab/run_cocostuff_vgg16.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8077465716299429
      ],
      "excerpt": "cocostuff-10k-v1.0.zip | COCO-Stuff dataset version 1.0, including images and annotations | 2.6 GB \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8387045882310893,
        0.9802228401192841,
        0.8036572261850616,
        0.9550410366445761,
        0.9046420952438756
      ],
      "excerpt": "Use the following steps to download and setup the DeepLab [4] semantic segmentation model trained on COCO-Stuff. It requires deeplab-public-ver2, which is built on Caffe: \nInstall Cuda. I recommend version 7.0. For version 8.0 you will need to apply the fix described here in step 3. \nDownload deeplab-public-ver2: git submodule update --init models/deeplab/deeplab-public-ver2 \nCompile and configure deeplab-public-ver2 following the author's instructions. Depending on your system setup you might have to install additional packages, but a minimum setup could look like this: \ncd models/deeplab/deeplab-public-ver2 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8246130083680622,
        0.9906248903846466,
        0.8167756553225646
      ],
      "excerpt": "make all -j8 \ncd ../.. \nConfigure the COCO-Stuff dataset: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9829454930541424
      ],
      "excerpt": "Run cd models/deeplab &amp;&amp; ./run_cocostuff_vgg16.sh to train and test the network on COCO-Stuff. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9771384723171962
      ],
      "excerpt": "cd models/deeplab \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9248669043028701
      ],
      "excerpt": "Run ./run_cocostuff_resnet101.sh to train and test the network on COCO-Stuff. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9080346804624625
      ],
      "excerpt": "<img src=\"http://calvin.inf.ed.ac.uk/wp-content/uploads/data/cocostuffdataset/cocostuff-examples.png\" alt=\"COCO-Stuff example annotations\" width=\"100%\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8794509184465833
      ],
      "excerpt": "Deeplab ResNet (no CRF) [4] | -   | 45.5%               | 65.1%           | 34.4%    | 50.4% \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8218052741541846
      ],
      "excerpt": "cocostuff-10k-v1.1.json | COCO-Stuff dataset v. 1.1, annotations in JSON format (optional) | 62.3 MB \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8483411575666896,
        0.8051983960592946
      ],
      "excerpt": "- Either download them: wget --directory-prefix=dataset/annotations-json http://calvin.inf.ed.ac.uk/wp-content/uploads/data/cocostuffdataset/cocostuff-10k-v1.1.json \n- Or extract them from the .mat file annotations using this Python script. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8286651306319724
      ],
      "excerpt": "Create a symbolic link to the images: cd models/deeplab/cocostuff/data &amp;&amp; ln -s ../../../../dataset/images images &amp;&amp; cd ../../../.. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091,
        0.9246227682586091
      ],
      "excerpt": "python rescaleImages.py \npython rescaleAnnotations.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8432458153402025
      ],
      "excerpt": "  - Specify a username in annotator/data/input/user.txt. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8000289043934353
      ],
      "excerpt": "  - To create a .png preview of the annotations, run annotator/code/exportImages.m in Matlab. The previews will be saved to annotator/data/output/preview. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/nightrome/cocostuff10k/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "MATLAB",
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "COCO-Stuff 10K dataset v1.1 ([outdated](https://github.com/nightrome/cocostuff))",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "cocostuff10k",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "nightrome",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/nightrome/cocostuff10k/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 255,
      "date": "Wed, 22 Dec 2021 03:11:17 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "coco",
      "semantic-segmentation",
      "dataset",
      "annotations",
      "stuff",
      "things"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To use the COCO-Stuff dataset, please follow these steps:\n\n1. Download or clone this repository using git: `git clone https://github.com/nightrome/cocostuff10k.git`\n2. Open the dataset folder in your shell: `cd cocostuff10k`\n3. If you have Matlab, run the following commands:\n  - Add the code folder to your Matlab path: `startup();`\n  - Run the demo script in Matlab `demo_cocoStuff();`\n  - The script displays an image, its thing, stuff and thing+stuff annotations, as well as the image captions.\n4. Alternatively run the following Linux commands or manually download and unpack the dataset:\n  - `wget --directory-prefix=downloads http://calvin.inf.ed.ac.uk/wp-content/uploads/data/cocostuffdataset/cocostuff-10k-v1.1.zip`\n  - `unzip downloads/cocostuff-10k-v1.1.zip -d dataset/`\n\n",
      "technique": "Header extraction"
    }
  ]
}