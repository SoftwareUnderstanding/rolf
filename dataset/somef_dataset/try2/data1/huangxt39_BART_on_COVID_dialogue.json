{
  "citation": [
    {
      "confidence": [
        0.9968156676108885
      ],
      "excerpt": "S-2     17250 11 6253 11 644 389 262 7460 286 39849 312 12 1129 30 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/huangxt39/BART_on_COVID_dialogue",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-04-08T06:55:36Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-01T13:25:30Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\nBART model [https://arxiv.org/pdf/1910.13461.pdf](https://arxiv.org/pdf/1910.13461.pdf)\r\n\r\nFairseq [https://github.com/pytorch/fairseq](https://github.com/pytorch/fairseq)\r\n\r\nFairseq tutorial on fine-tuning BART on Seq2Seq task [https://github.com/pytorch/fairseq/blob/master/examples/bart/README.summarization.md](https://github.com/pytorch/fairseq/blob/master/examples/bart/README.summarization.md)\r\n\r\nCOVID Dialogue Dataset [https://github.com/UCSD-AI4H/COVID-Dialogue](https://github.com/UCSD-AI4H/COVID-Dialogue)\r\n\r\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9785265388181924
      ],
      "excerpt": "During fine-tuning, the input is what the patients said and output is what the doctors said. Thus the model is playing a role of a doctor. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8896676611669855
      ],
      "excerpt": "Then fine-tuning the using train.sh in repo root directory. Before using it, edit this file to fit into your own machine. With the default setting, the model is fine-tuning on 6 GPUs and consuming around 10G GPU memory of each (totally 60G GPU memory). You can change MAX_TOKENS flag to adjust batch size. (fine more information about command-line tools at here \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9174029823270458
      ],
      "excerpt": "Hi, doctor, what are the symptoms of covid-19? \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9848790874792501
      ],
      "excerpt": "H-2     -0.13718903064727783    Symptoms. The symptom of COVID-19 begins with mild flu-like symptoms such as fatigue, sore throat and sneeze, followed by fever, dry cough. In severe cases, the cough can progress to productive cough, persistent and followed by shortness of breath. Some patients may also experience GI symptoms such as nausea vomiting and diarrhea. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Fine-tuning BART on COVID Dialogue Dataset",
      "technique": "GitHub API"
    }
  ],
  "documentation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "https://fairseq.readthedocs.io/",
      "technique": "Regular expression"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\nDownload the BART-large model from [here](https://dl.fbaipublicfiles.com/fairseq/models/bart.large.tar.gz \"here\")\r\n\r\nData is already in this Repo\r\n\r\nput the model at\r\n\r\n    REPO ROOT\r\n\t |\r\n\t |-- bart.large\r\n\t |\t  |-- dict.txt\r\n\t |\t  |-- model.pt\r\n\t |\t  |-- NOTE\r\n\t |-- data\r\n\t |\t  |...\r\n\t |-- preprocess_data\r\n\t |\t  |...\r\n\t |...\r\n\r\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/huangxt39/BART_on_COVID_dialogue/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sun, 26 Dec 2021 13:46:29 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/huangxt39/BART_on_COVID_dialogue/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "huangxt39/BART_on_COVID_dialogue",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/huangxt39/BART_on_COVID_dialogue/master/interactive.sh",
      "https://raw.githubusercontent.com/huangxt39/BART_on_COVID_dialogue/master/train.sh",
      "https://raw.githubusercontent.com/huangxt39/BART_on_COVID_dialogue/master/preprocess_data/binarize.sh",
      "https://raw.githubusercontent.com/huangxt39/BART_on_COVID_dialogue/master/preprocess_data/bpe.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9660278703057057
      ],
      "excerpt": "Fairseq (to install, follow the guidance in here. In most cases, just simply run \"pip install fairseq\") \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9465718491881494,
        0.9465718491881494
      ],
      "excerpt": "bash bpe.sh \nbash binarize.sh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9582690017232749
      ],
      "excerpt": "bash train.sh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8934924013721233,
        0.8228537878153995
      ],
      "excerpt": "run the command: \nbash interactive.sh \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python preprocess_data.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.800219093757095
      ],
      "excerpt": "bash train.sh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9129617675176488
      ],
      "excerpt": "Example output: \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/huangxt39/BART_on_COVID_dialogue/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Fine-tuning BART on COVID Dialogue Dataset #",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "BART_on_COVID_dialogue",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "huangxt39",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/huangxt39/BART_on_COVID_dialogue/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 9,
      "date": "Sun, 26 Dec 2021 13:46:29 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "fairseq",
      "bart",
      "covid-dialogue-dataset"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\nThis is from [fairseq tutorial](https://github.com/pytorch/fairseq/blob/master/examples/bart/README.summarization.md), from which you can learn how to use the model.\r\n\r\n    import torch\r\n\tfrom fairseq.models.bart import BARTModel\r\n\r\n\tbart = BARTModel.from_pretrained(\r\n    'checkpoints/',\r\n    checkpoint_file='checkpoint_best.pt',\r\n    data_name_or_path='cnn_dm-bin'\r\n\t)\r\n\r\n\tbart.cuda()\r\n\tbart.eval()\r\n\tbart.half()\r\n\tcount = 1\r\n\tbsz = 32\r\n\twith open('cnn_dm/test.source') as source, open('cnn_dm/test.hypo', 'w') as fout:\r\n    sline = source.readline().strip()\r\n    slines = [sline]\r\n    for sline in source:\r\n        if count % bsz == 0:\r\n            with torch.no_grad():\r\n                hypotheses_batch = bart.sample(slines, beam=4, lenpen=2.0, max_len_b=140, min_len=55, no_repeat_ngram_size=3)\r\n\r\n            for hypothesis in hypotheses_batch:\r\n                fout.write(hypothesis + '\\n')\r\n                fout.flush()\r\n            slines = []\r\n\r\n        slines.append(sline.strip())\r\n        count += 1\r\n    if slines != []:\r\n        hypotheses_batch = bart.sample(slines, beam=4, lenpen=2.0, max_len_b=140, min_len=55, no_repeat_ngram_size=3)\r\n        for hypothesis in hypotheses_batch:\r\n            fout.write(hypothesis + '\\n')\r\n            fout.flush()\r\n\r\nFind more information from [fairseq bart repo](https://github.com/pytorch/fairseq/tree/master/examples/bart)!",
      "technique": "Header extraction"
    }
  ]
}