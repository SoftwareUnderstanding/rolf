{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2103.02242",
      "https://arxiv.org/abs/2103.02242v1",
      "https://arxiv.org/abs/1911.04231",
      "https://arxiv.org/abs/2103.02242v1",
      "https://arxiv.org/abs/1911.04231"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "<div align=center><img width=\"100%\" src=\"figs/FFB6D_overview.png\"/></div>\n\n[FFB6D](https://arxiv.org/abs/2103.02242v1) is a general framework for representation learning from a single RGBD image, and we applied it to the 6D pose estimation task by cascading downstream prediction headers for instance semantic segmentation and 3D keypoint voting prediction from PVN3D([Arxiv](https://arxiv.org/abs/1911.04231), [Code](https://github.com/ethnhe/PVN3D), [Video](https://www.bilibili.com/video/av89408773/)). \nAt the representation learning stage of FFB6D, we build **bidirectional** fusion modules in the **full flow** of the two networks, where fusion is applied to each encoding and decoding layer. In this way, the two networks can leverage local and global complementary information from the other one to obtain better representations. Moreover, at the output representation stage, we designed a simple but effective 3D keypoints selection algorithm considering the texture and geometry information of objects, which simplifies keypoint localization for precise pose estimation.\n\nPlease cite [FFB6D](https://arxiv.org/abs/2103.02242v1) & [PVN3D](https://arxiv.org/abs/1911.04231) if you use this repository in your publications:\n\n```\n@InProceedings{He_2021_CVPR,\nauthor = {He, Yisheng and Huang, Haibin and Fan, Haoqiang and Chen, Qifeng and Sun, Jian},\ntitle = {FFB6D: A Full Flow Bidirectional Fusion Network for 6D Pose Estimation},\nbooktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\nmonth = {June},\nyear = {2021}\n}\n\n@InProceedings{He_2020_CVPR,\nauthor = {He, Yisheng and Sun, Wei and Huang, Haibin and Liu, Jianran and Fan, Haoqiang and Sun, Jian},\ntitle = {PVN3D: A Deep Point-Wise 3D Keypoints Voting Network for 6DoF Pose Estimation},\nbooktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\nmonth = {June},\nyear = {2020}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@InProceedings{He_2020_CVPR,\nauthor = {He, Yisheng and Sun, Wei and Huang, Haibin and Liu, Jianran and Fan, Haoqiang and Sun, Jian},\ntitle = {PVN3D: A Deep Point-Wise 3D Keypoints Voting Network for 6DoF Pose Estimation},\nbooktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\nmonth = {June},\nyear = {2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@InProceedings{He_2021_CVPR,\nauthor = {He, Yisheng and Huang, Haibin and Fan, Haoqiang and Chen, Qifeng and Sun, Jian},\ntitle = {FFB6D: A Full Flow Bidirectional Fusion Network for 6D Pose Estimation},\nbooktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\nmonth = {June},\nyear = {2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8654671031158477,
        0.8944178096468923
      ],
      "excerpt": "Introduction & Citation \nDemo Video \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559,
        0.8955886365383559,
        0.8955886365383559,
        0.8955886365383559,
        0.8786044417622563
      ],
      "excerpt": "      <th class=\"tg-c3ow\" colspan=\"2\" style=\"text-align: center\">PoseCNN</th> \n      <th class=\"tg-c3ow\" colspan=\"2\" style=\"text-align: center\">PointFusion</th> \n      <th class=\"tg-c3ow\" colspan=\"2\" style=\"text-align: center\">DenseFusion</th> \n      <th class=\"tg-c3ow\" colspan=\"2\" style=\"text-align: center\">PVN3D</th> \n      <th class=\"tg-c3ow\" colspan=\"2\" style=\"text-align: center\">Our FFF6D</th> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8444342525991423,
        0.8444342525991423
      ],
      "excerpt": "      <th class=\"tg-8d8j\" colspan=\"3\" style=\"text-align: center\">RGB</th> \n      <th class=\"tg-8d8j\" colspan=\"5\" style=\"text-align: center\">RGB-D</th> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "      <td class=\"tg-7zrl\">G2L-Net</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9507374082549614
      ],
      "excerpt": "      <th class=\"tg-7zrl\">Pose Estimation</th> \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ethnhe/FFB6D",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-03-03T06:48:18Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-25T01:24:42Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "<div align=center><img width=\"100%\" src=\"figs/FFB6D_overview.png\"/></div>\n\n[FFB6D](https://arxiv.org/abs/2103.02242v1) is a general framework for representation learning from a single RGBD image, and we applied it to the 6D pose estimation task by cascading downstream prediction headers for instance semantic segmentation and 3D keypoint voting prediction from PVN3D([Arxiv](https://arxiv.org/abs/1911.04231), [Code](https://github.com/ethnhe/PVN3D), [Video](https://www.bilibili.com/video/av89408773/)). \nAt the representation learning stage of FFB6D, we build **bidirectional** fusion modules in the **full flow** of the two networks, where fusion is applied to each encoding and decoding layer. In this way, the two networks can leverage local and global complementary information from the other one to obtain better representations. Moreover, at the output representation stage, we designed a simple but effective 3D keypoints selection algorithm considering the texture and geometry information of objects, which simplifies keypoint localization for precise pose estimation.\n\nPlease cite [FFB6D](https://arxiv.org/abs/2103.02242v1) & [PVN3D](https://arxiv.org/abs/1911.04231) if you use this repository in your publications:\n\n```\n@InProceedings{He_2021_CVPR,\nauthor = {He, Yisheng and Huang, Haibin and Fan, Haoqiang and Chen, Qifeng and Sun, Jian},\ntitle = {FFB6D: A Full Flow Bidirectional Fusion Network for 6D Pose Estimation},\nbooktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\nmonth = {June},\nyear = {2021}\n}\n\n@InProceedings{He_2020_CVPR,\nauthor = {He, Yisheng and Sun, Wei and Huang, Haibin and Liu, Jianran and Fan, Haoqiang and Sun, Jian},\ntitle = {PVN3D: A Deep Point-Wise 3D Keypoints Voting Network for 6DoF Pose Estimation},\nbooktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\nmonth = {June},\nyear = {2020}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9261442067521851
      ],
      "excerpt": "This is the official source code for the CVPR2021 Oral work, FFB6D: A Full Flow Biderectional Fusion Network for 6D Pose Estimation. (Arxiv, Video_Bilibili, Video_YouTube) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9968029537584643
      ],
      "excerpt": "Table of Content \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9693233393948762
      ],
      "excerpt": "  <summary>[Click to expand]</summary> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8206730678291857
      ],
      "excerpt": "        - **ffb6d/datasets/linemod/kps_orb9_fps/{obj_name}_corners.txt**: 8 corners of the 3D bounding box of an object in the object coordinate system. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8206730678291857
      ],
      "excerpt": "        - **ffb6d/datasets/ycb/ycb_kps/{obj_name}_corners.txt**: 8 corners of the 3D bounding box of an object in the object coordinate system. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9488025062354254,
        0.8513398251584968
      ],
      "excerpt": "    - **ffb6d/models/cnn** \n      - **ffb6d/models/cnn/extractors.py**: Resnet backbones. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9415932642319527,
        0.8355625695036051
      ],
      "excerpt": "      - **ffb6d/models/cnn/ResNet_pretrained_mdl**: Resnet pretraiend model weights. \n    - **ffb6d/models/loss.py**: loss calculation for training of FFB6D model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8235911957497822,
        0.8714739532824326
      ],
      "excerpt": "    - **ffb6d/utils/basic_utils.py**: basic functions for data processing, visualization and so on. \n    - **ffb6d/utils/meanshift_pytorch.py**: pytorch version of meanshift algorithm for 3D center point and keypoints voting.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8559695099478587
      ],
      "excerpt": "Pretrained model: We provide our pre-trained models for each object on onedrive, link. (The provided pretrained model here get better performance than we reported in our paper, mean ADD-0.1d 99.8). Download them and move them to their according folders. For example, move the FFB6D_ape_best.pth.tar to train_log/linemod/checkpoints/ape/. Then revise tst_mdl=train_log/linemod/checkpoints/ape/FFB6D_ape_best.path.tar for testing. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8019509518938658,
        0.8363291815243342
      ],
      "excerpt": "  You can evaluate different checkpoints by revising the tst_mdl to the path of your target model. \nPretrained model: We provide our pre-trained models on onedrive, here. Download the pre-trained model, move it to train_log/ycb/checkpoints/ and modify tst_mdl for testing. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8534165398200448
      ],
      "excerpt": "Modify info of your new dataset in FFB6D/ffb6d/common.py  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8460651943297673,
        0.9189783325229246
      ],
      "excerpt": "For inference, make sure that you load the 3D keypoints, center point, and radius of your objects in the object coordinate system properly in FFB6D/ffb6d/utils/pvn3d_eval_utils.py. \nCheck that all setting are modified properly by using the ground truth information for evaluation. The result should be high and close to 100 if everything is correct. For example, testing ground truth on the YCB_Video dataset by passing -test_gt parameters to train_ycb.py will get results higher than 99.99: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "[CVPR2021 Oral] FFB6D: A Full Flow Bidirectional Fusion Network for 6D Pose Estimation.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ethnhe/FFB6D/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 35,
      "date": "Wed, 29 Dec 2021 17:41:15 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ethnhe/FFB6D/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "ethnhe/FFB6D",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/ethnhe/FFB6D/master/ffb6d/train_ycb.sh",
      "https://raw.githubusercontent.com/ethnhe/FFB6D/master/ffb6d/demo_lm.sh",
      "https://raw.githubusercontent.com/ethnhe/FFB6D/master/ffb6d/demo_ycb.sh",
      "https://raw.githubusercontent.com/ethnhe/FFB6D/master/ffb6d/train_lm.sh",
      "https://raw.githubusercontent.com/ethnhe/FFB6D/master/ffb6d/test_lm.sh",
      "https://raw.githubusercontent.com/ethnhe/FFB6D/master/ffb6d/test_ycb.sh",
      "https://raw.githubusercontent.com/ethnhe/FFB6D/master/ffb6d/models/RandLA/compile_op.sh",
      "https://raw.githubusercontent.com/ethnhe/FFB6D/master/ffb6d/models/RandLA/utils/download_semantic3d.sh",
      "https://raw.githubusercontent.com/ethnhe/FFB6D/master/ffb6d/models/RandLA/utils/cpp_wrappers/compile_wrappers.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Install CUDA 10.1 / 10.2\n- Set up python3 environment from requirement.txt:\n  ```shell\n  pip3 install -r requirement.txt \n  ```\n- Install [apex](https://github.com/NVIDIA/apex):\n  ```shell\n  git clone https://github.com/NVIDIA/apex\n  cd apex\n  export TORCH_CUDA_ARCH_LIST=\"6.0;6.1;6.2;7.0;7.5\"  #: set the target architecture manually, suggested in issue https://github.com/NVIDIA/apex/issues/605#:issuecomment-554453001\n  pip3 install -v --disable-pip-version-check --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./\n  cd ..\n  ```\n- Install [normalSpeed](https://github.com/hfutcgncas/normalSpeed), a fast and light-weight normal map estimator:\n  ```shell\n  git clone https://github.com/hfutcgncas/normalSpeed.git\n  cd normalSpeed/normalSpeed\n  python3 setup.py install --user\n  cd ..\n  ```\n- Install tkinter through ``sudo apt install python3-tk``\n\n- Compile [RandLA-Net](https://github.com/qiqihaer/RandLA-Net-pytorch) operators:\n  ```shell\n  cd ffb6d/models/RandLA/\n  sh compile_op.sh\n  ```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9486489623592528
      ],
      "excerpt": "    - **ffb6d/models/RandLA/**: pytorch version of RandLA-Net from [RandLA-Net-pytorch](https://github.com/qiqihaer/RandLA-Net-pytorch) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9052919919358386,
        0.9416184217550847,
        0.8938986333316902
      ],
      "excerpt": "  - **ffb6d/train_ycb.sh**: Bash scripts to start the training on the YCB_Video dataset. \n  - **ffb6d/test_ycb.sh**: Bash scripts to start the testing on the YCB_Video dataset. \n  - **ffb6d/demo_ycb.sh**: Bash scripts to start the demo on the YCB_Video_dataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9052919919358386,
        0.9416184217550847,
        0.8963714493493045
      ],
      "excerpt": "  - **ffb6d/train_lm.sh**: Bash scripts to start the training on the LineMOD dataset. \n  - **ffb6d/test_lm.sh**: Bash scripts to start the testing on the LineMOD dataset. \n  - **ffb6d/demo_lm.sh**: Bash scripts to start the demo on the LineMOD dataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9885264200513099
      ],
      "excerpt": "- **requirement.txt**: python3 environment requirements for pip3 install. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9906248903846466
      ],
      "excerpt": "  cd ffb6d \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8025303718880077
      ],
      "excerpt": "  n_gpu=8  #: number of gpu to use \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8051022324579896
      ],
      "excerpt": "Model parameters and speed on the LineMOD dataset (one object / frame) with one 2080Ti GPU: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8215129206796412
      ],
      "excerpt": "Install and generate required mesh info following DSTOOL_README. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9093598922110971,
        0.8186919169533166
      ],
      "excerpt": "      - **ffb6d/datasets/linemod/linemod_dataset.py**: Data loader for LineMOD dataset. \n      - **ffb6d/datasets/linemod/dataset_config/models_info.yml**: Object model info of LineMOD dataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9093598922110971
      ],
      "excerpt": "      - **ffb6d/datasets/ycb/ycb_dataset.py**\uff1a Data loader for YCB_Video dataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.812793583896225,
        0.8339911288932772
      ],
      "excerpt": "        - **ffb6d/datasets/ycb/dataset_config/train_data_list.txt**: Training set of YCB_Video datset. \n        - **ffb6d/datasets/ycb/dataset_config/test_data_list.txt**: Testing set of YCB_Video dataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8085416450267291
      ],
      "excerpt": "      - **ffb6d/models/cnn/extractors.py**: Resnet backbones. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8924976426181745
      ],
      "excerpt": "  - **ffb6d/utils** \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8924976426181745,
        0.8122920191936448,
        0.8334081125564143,
        0.8435479797776744,
        0.851485802177248,
        0.8478800540304969,
        0.8605032581908556,
        0.9413495032574013,
        0.8635125623018725
      ],
      "excerpt": "    - **ffb6d/utils/dataset_tools** \n      - **ffb6d/utils/dataset_tools/DSTOOL_README.md**: README for dataset tools. \n      - **ffb6d/utils/dataset_tools/requirement.txt**: Python3 requirement for dataset tools. \n      - **ffb6d/utils/dataset_tools/gen_obj_info.py**: Generate object info, including SIFT-FPS 3d keypoints, radius etc. \n      - **ffb6d/utils/dataset_tools/rgbd_rnder_sift_kp3ds.py**: Render rgbd images from mesh and extract textured 3d keypoints (SIFT/ORB). \n      - **ffb6d/utils/dataset_tools/utils.py**: Basic utils for mesh, pose, image and system processing. \n      - **ffb6d/utils/dataset_tools/fps**: Furthest point sampling algorithm. \n      - **ffb6d/utils/dataset_tools/example_mesh**: Example mesh models. \n  - **ffb6d/train_ycb.py**: Training & Evaluating code of FFB6D models for the YCB_Video dataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8635125623018725
      ],
      "excerpt": "  - **ffb6d/train_lm.py**: Training & Evaluating code of FFB6D models for the LineMOD dataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8213778157599012
      ],
      "excerpt": "  python3 -m torch.distributed.launch --nproc_per_node=1 train_lm.py --gpu '0' --cls $cls -eval_net -checkpoint $tst_mdl -test -test_pose #: -debug \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.829345788469597
      ],
      "excerpt": "  python3 -m demo -dataset linemod -checkpoint $tst_mdl -cls $cls -show \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8389492121216885,
        0.8536239106927584
      ],
      "excerpt": "  tst_mdl=train_log/ycb/checkpoints/FFB6D_best.pth.tar  #: checkpoint to test. \n  python3 -m torch.distributed.launch --nproc_per_node=1 train_ycb.py --gpu '0' -eval_net -checkpoint $tst_mdl -test -test_pose #: -debug \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8230800126804281,
        0.8014811411687542
      ],
      "excerpt": "Pretrained model: We provide our pre-trained models on onedrive, here. Download the pre-trained model, move it to train_log/ycb/checkpoints/ and modify tst_mdl for testing. \nAfter training your model or downloading the pre-trained model, you can start the demo by: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8704281464995636
      ],
      "excerpt": "  python3 -m demo -checkpoint $tst_mdl -dataset ycb \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8467618115023552
      ],
      "excerpt": "<div align=center><img width=\"50%\" src=\"figs/occlusion.png\"/></div> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8404361288114941
      ],
      "excerpt": "<div align=center><img width=\"60%\" src=\"figs/shown_kp_ycb.png\"/></div> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ethnhe/FFB6D/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "C++",
      "C",
      "Shell",
      "Cython"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2021 Yisheng He\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "FFB6D",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "FFB6D",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "ethnhe",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ethnhe/FFB6D/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 123,
      "date": "Wed, 29 Dec 2021 17:41:15 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "6d-pose-estimation",
      "6dof-pose",
      "rgbd-scene-recognition",
      "rgbd-camera",
      "rgbd-segmentation",
      "fusion"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "See our demo video on [YouTube](https://www.youtube.com/watch?v=SSi2TnyD6Is) or [bilibili](https://www.bilibili.com/video/BV1YU4y1a7Kp?from=search&seid=8306279574921937158).\n",
      "technique": "Header extraction"
    }
  ]
}