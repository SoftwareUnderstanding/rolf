{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2005.10247",
      "https://arxiv.org/abs/1804.04732",
      "https://arxiv.org/abs/1212.5701",
      "https://arxiv.org/abs/1706.06083",
      "https://arxiv.org/abs/1804.04732",
      "https://arxiv.org/abs/2005.10247"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{robey2020model,\n  title={Model-Based Robust Deep Learning},\n  author={Robey, Alexander and Hassani, Hamed and Pappas, George J},\n  journal={arXiv preprint arXiv:2005.10247},\n  year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8001118323515268
      ],
      "excerpt": "export N_CLASSES=10         #: number of classes \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/arobey1/mbrdl",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-10-06T20:10:57Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-08-01T11:10:05Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9531579862530861,
        0.8105399041516462
      ],
      "excerpt": "In this repository, we include the code necessary for reproducing the code used in Model-Based Robust Deep Learning.  In particular, we include the code necessary for both training models of natural variation as well as the code needed to train classifiers using these learned models.  A brief summary of the functionality provided in this repo is provided below in the table of contents.  If you find this repository useful in your research, please consider citing: \nFirst, we given instructions for how to setup the appropriate environment for this repository. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9916330179478493
      ],
      "excerpt": "Next, we give details about how to train classifiers using the MBRDL paradigm.  ur implementation is based on the Lambda Labs implementation of the ImageNet training repository.   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9932614009017545,
        0.9873347770732582,
        0.9782221286148959
      ],
      "excerpt": "Following this, we give a high-level overview of the structure of this repository, as well as an overview of the scripts that are included for training models of natural variation and classifiers that are robust to natural variation. \nAn overview of the structure of this repository \nWe also provide code that can be used to train models of natural variation using the MUNIT framework.  The code that we use to train these models is largely based on the original implementation of MUNIT. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9006828150335618,
        0.9713487613664215,
        0.9577704102834753,
        0.8283739443085154
      ],
      "excerpt": "Retrieving a saved model of natural variation \nUsing other architectures for models of natural variation \nIn addition to providing functionality to train new models of natural variation, we also provide a library of pre-trained models of natural variation in a Google Drive folder. \nA library of pre-trained models of natural variation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9731342368939738
      ],
      "excerpt": "All of the components to train models of natural variation and classifiers is included in the core/ directory.  In particular, core/ is organized in the following way: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "\u251c\u2500\u2500 data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8530374501805772
      ],
      "excerpt": "The classifiers/ directory contains all code necessary to initialize different classifier architectures.  The data/ directory contains dataloaders for MUNIT and for training classifiers for SVHN, CURE-TSR, GTSRB, and ImageNet/ImageNet-c.  The models/ directory has code that can be used to load and train models of natural variation using the MUNIT framework.  The training/ directory contains the training algorithms (including implementations of MAT, MRT, and MDA) and schedulers that are used to train classifiers.  The utils/ directory has a variety of utilities that are used throughout the scripts that train classifiers in the MBRDL paradigm. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.99222547203761
      ],
      "excerpt": "We also include the code needed to train classifiers that are robust to natural variation.  Our implementation is based on the Lambda Labs implementation of the ImageNet training repository.   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.868588026474172
      ],
      "excerpt": "The choices for SOURCE for SVHN are choices are 'brightness', 'contrast', and 'contrast+brightness'.  The same choices are also available for GTSRB: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8923572823989261
      ],
      "excerpt": "For CURE-TSR, you can select any of the sources of natural variation listed in the original repository for CURE-TSR (e.g. snow, rain, haze, decolorization, etc.).  For example, you can set \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.975894116008052
      ],
      "excerpt": "The 'basic' architecture is a simple CNN with two convolutional layers and two feed-forward layers.  The program will also accept any of the architectures in torchvision.models, including AlexNet and ResNet50.  For example, an appropriate ImageNet configuration could be something like this: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8523512489413928,
        0.9822913247696655,
        0.9370415070071086
      ],
      "excerpt": "These flags will also allow you to set the number of output classes for the given architecture, the size of the images in the dataset, and the (training) batch size.   \nYou can also select the optimizer to be used for training the given ARCHITECTURE.  Currently, two optimizers are supported: SGD with momentum and AdaDelta.  In our paper, we used SGD for the experiments on ImageNet, and AdaDelta for every other experiment (e.g. MNIST, SVHN, CURE-TSR, GTSRB, etc.).  To select an optimizer, you can use the --optimizer flag, which currently supports the arguments sgd or adadelta. \nYu can set the path to a saved model of natural variation and the dimension of the nuisance space \u0394 by setting \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8310443751745082,
        0.8740659508886145
      ],
      "excerpt": "The CONFIG_PATH should point to a .yaml file with appropriate settings for the MUNIT architecture.  An example is given in core/models/munit/munit.yaml.  Note that the dimension must match the style_dim parameter in core/models/munit/munit.yaml if you are using the MUNIT framework. \nTo compose two models of natural variation, you can simply pass multiple paths after the --model-paths argument.  For example, to compose models of contrast and brightness for SVHN, first set \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8266761755204032
      ],
      "excerpt": "By default, the script will train a classifier with the standard ERM formulation.  However, by adding flags, you can train classifiers using the three model-based algorithms from our paper (MAT, MRT, and MDA) as well as PGD.  For example, to train with MRT and k=10, you can add the flags --mrt -k 10 to the python -m torch.distributed.launch ... command at the bottom of the file.  By replacing --mrt with --mat or --mda, you can change the algorithm to MAT or MDA respectively.  Similarly, you can use the --pgd flag to train with the PGD algorithm.  By default, PGD runs with a step size of 0.01, \u025b=8/255, and 20 steps of gradient ascent. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9840305343289543
      ],
      "excerpt": "In this work, we used the MUNIT framework to learn models of natural variation.  The code that we use to train these models is largely based on the original implementation of MUNIT.  To train a model of natural variation with MUNIT, you can run the following shell script: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.980989963192436
      ],
      "excerpt": "The CONFIG_PATH should point to a .yaml file with appropriate settings for the MUNIT architecture.  An example is given in core/models/munit/munit.yaml.  Note that the parameter style_dim in this file corresponds to the dimension that will be used for the nuisance space \u0394.  By default, we have set this to 8, which was the dimension used throughout the experiments section of our paper. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8412318643509465
      ],
      "excerpt": "will return a model of natural variation that can be called in the following way: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8301109024184108
      ],
      "excerpt": "Here, mb_images will be a batch of images that look semantically similar to imgs but will have different levels of natural variation.  Note that delta_dim must be set appropriately in this code snippet to match the style_dim parameter from the .yaml file located at OUTPUT_PATH.   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9692940220326367
      ],
      "excerpt": "To use other architectures for G, you can simply replace the MUNITModelOfNatVar instantiation in the load_model function in core/models/load.py.  In particular, the only requirement is that a model of natural variation should be instantiated as a torch.nn.Module with a forward pass function forward that takes as input a batch of images and a suitably sized nuisance parameter, i.e. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8952797577181123
      ],
      "excerpt": "We provide a library of pre-trained models of natural variation in a public Google Drive folder.  In particular, this folder contains models for MNIST, SVHN, GTSRB, CURE-TSR and ImageNet/ImageNet-c: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9682665305108008
      ],
      "excerpt": "We plan to update this folder frequently with more models of natural variation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Repository for reproducing `Model-Based Robust Deep Learning`",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/arobey1/mbrdl/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 5,
      "date": "Mon, 27 Dec 2021 23:19:54 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/arobey1/mbrdl/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "arobey1/mbrdl",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/arobey1/mbrdl/master/train_basic.sh",
      "https://raw.githubusercontent.com/arobey1/mbrdl/master/train_imagenet.sh",
      "https://raw.githubusercontent.com/arobey1/mbrdl/master/train_munit_imagenet.sh",
      "https://raw.githubusercontent.com/arobey1/mbrdl/master/train_munit.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "After cloning this repository, the first step is to setup a virtual environment.\n\n```bash\npython3 -m venv mbrdl\nsource mbrdl/bin/activate\npip3 install -r requirements.txt\npip3 install https://download.pytorch.org/whl/cu100/torch-1.1.0-cp36-cp36m-linux_x86_64.whl\n```\n\nWe also need to install NVIDIA's half-precision training tool [apex](https://github.com/NVIDIA/apex).  The setup instructions for `apex` are [here](https://github.com/NVIDIA/apex#quick-start).    \n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9633085449227379
      ],
      "excerpt": "* Setup instructions \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.980396591352415,
        0.8468902673898102
      ],
      "excerpt": "To train a classifier on MNIST, SVHN, GTSRB, or CURE-TSR, you can run the following shell script: \nchmod +x train_basic.sh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8468902673898102
      ],
      "excerpt": "chmod +x train_imagenet_script.sh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8828996559976371
      ],
      "excerpt": "To run with SVHN, you can set: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8499745412742987
      ],
      "excerpt": "To select the classifier architecture, you can set the following flags: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8057138698943367
      ],
      "excerpt": "and then add --model-paths $MODEL_PATH_1 $MODEL_PATH_2 to the python command at the bottom of train_bash.sh or train_imagenet.sh. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8392754419512543
      ],
      "excerpt": "You can set the distributed settings with the following flags: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8468902673898102,
        0.8552201370215289
      ],
      "excerpt": "chmod +x train_munit.sh \nYou can change the dataset and various directories using the flags in train_munit.sh.  In particular, you can set the DATASET and SOURCE environmental variables in the same was as in train_basic.sh and train_imagenet.sh.  You can also set various paths, such as the path to the MUNIT configuration file and to directory where you would like to save your output: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8411004553040458
      ],
      "excerpt": "mb_images = G(imgs.cuda(), delta) \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8098356787608533
      ],
      "excerpt": "Training models of natural variation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8174540907975313,
        0.9336801098518991,
        0.950563948951535,
        0.8924976426181745
      ],
      "excerpt": "\u251c\u2500\u2500 training \n\u251c\u2500\u2500 train_munit.py \n\u251c\u2500\u2500 train.py \n\u2514\u2500\u2500 utils \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9602310156114895
      ],
      "excerpt": "The two main training python files are train_munit.py and train.py.  As the name suggests, train_munit.py can be used to train models of natural variation using MUNIT.  train.py is a python file that trains classifiers in a distributed fashion. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8179459366774735
      ],
      "excerpt": "    #: Load model from file and return \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/arobey1/mbrdl/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'This License governs use of the accompanying Model-Based Robust Deep Learning (MBRDL) repository and all accompanying utilities, forms, libraries, etc. (\"Software\"), and your use of the Software, Platform, Algorithms, and all Utilities constitutes acceptance of this license.\\n\\n \\n\\nThe Software was originally created by Alexander Robey, Hamed Hassani, and George J. Pappas and the Trustees of the University of Pennsylvania (\"Licensor\") to enable Faculty, Departments and Researchers for train models of natural variation as well as classifiers that are robust to natural variation in data.\\n\\n \\n\\nYou may use this Software WITHOUT CHARGE for any purpose, subject to the restrictions in this license. Some of those allowable uses or purposes which can be non-commercial are teaching, academic research, use in your own environment (whether that is a commercial, academic or non-profit company or organization) and personal experimentation.\\n\\n \\n\\nYou may use the software if you are a commercial entity. There are two things you cannot do with this Software: The first is you cannot incorporate it into a commercial product (\"Commercial Use\"), the second is you cannot distribute this software or any modifications (\"Derivative Work\") of this software and beyond that, you must share your changes to the Model-Based Robust Deep Learning (MBRDL) repository with Alexander Robey, Hamed Hassani, and George J. Pappas and the Model-Based Robust Deep Learning (MBRDL) repository Community. We want everyone to benefit from the use of this product, we want it to stay free, and we want to avoid it forking (or splintering) into disconnected versions. Therefore; you may not use or distribute this Software or any Derivative Works in any form for any purpose. Examples of prohibited purposes would be licensing, leasing, or selling the Software, or distributing the Software for use with other commercial products, or incorporating the Software into a commercial product.\\n\\n \\n\\nYou may create Derivative Works of the software for your own use. You may modify this Software and contribute it back to Alexander Robey, Hamed Hassani, and George J. Pappas and the Model-Based Robust Deep Learning (MBRDL) repository Community, but you may not distribute the modified Software; all distribution must happen via the Model-Based Robust Deep Learning (MBRDL) repository. You may not grant rights to the Software or Derivative Works to this software under this License. For example, you may not distribute modifications of the Software under any terms, or sublicense this software to others.\\n\\n \\n\\nFor purposes of clarity, this license covers your institution or organization, and all other institutions or organizations directly affiliated with your institution or organization. For example, in the case of a University, all campuses would be covered under this license and directly affiliated schools, such as members of a higher education consortium, would also be covered under this license. In the case of a non-profit institution, all subsidiaries and directly affiliated companies or entities would be covered.'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Model-Based Robust Deep Learning (MBRDL)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "mbrdl",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "arobey1",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/arobey1/mbrdl/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 13,
      "date": "Mon, 27 Dec 2021 23:19:54 GMT"
    },
    "technique": "GitHub API"
  }
}