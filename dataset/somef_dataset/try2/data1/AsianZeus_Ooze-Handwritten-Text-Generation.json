{
  "citation": [
    {
      "confidence": [
        0.9917136734499049
      ],
      "excerpt": "    Learning came into focus, in classical machine learning approach there were OCR for character recognition. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8744509615041938
      ],
      "excerpt": "&lt;p class=\"s16\" style=\"padding-left: 206pt;text-indent: 0pt;text-align: center;\"&gt;Figure 2 GANS&lt;/p&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9648889021544168
      ],
      "excerpt": "    &lt;li style=\"padding-top: 10pt;padding-left: 10pt;text-indent: 0pt;text-align: justify;\"&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9156566588472104,
        0.9648889021544168
      ],
      "excerpt": "    &lt;/li&gt; \n    &lt;li style=\"padding-top: 1pt;padding-left: 28pt;text-indent: -18pt;text-align: justify;\"&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9156566588472104,
        0.9648889021544168
      ],
      "excerpt": "    &lt;/li&gt; \n    &lt;li style=\"padding-top: 1pt;padding-left: 28pt;text-indent: -18pt;text-align: justify;\"&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9156566588472104,
        0.9648889021544168
      ],
      "excerpt": "    &lt;/li&gt; \n    &lt;li style=\"padding-top: 1pt;padding-left: 28pt;text-indent: -18pt;text-align: justify;\"&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9156566588472104
      ],
      "excerpt": "    &lt;/li&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9156566588472104
      ],
      "excerpt": "    &lt;/li&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8159466748498895,
        0.8522132482229521
      ],
      "excerpt": "    image-to-image translation tasks. The approach was presented by Phillip Isola, et al. in their 2016 paper[6] \n    titled \u201cImage-to-Image Translation with Conditional Adversarial Networks\u201d and presented at CVPR in 2017.&lt;/p&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9126448562336145
      ],
      "excerpt": "&lt;p class=\"s16\" style=\"padding-top: 11pt;padding-left: 206pt;text-indent: 0pt;text-align: center;\"&gt;Figure 3 U-Net&lt;/p&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9648889021544168
      ],
      "excerpt": "    &lt;li style=\"padding-top: 12pt;padding-left: 28pt;text-indent: -18pt;text-align: justify;\"&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9156566588472104
      ],
      "excerpt": "    &lt;/li&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9156566588472104
      ],
      "excerpt": "    &lt;/li&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9156566588472104
      ],
      "excerpt": "    &lt;/li&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9894396851275176
      ],
      "excerpt": "    model was described by Jun-Yan Zhu, et al. in their 2017 paper[7] titled \u201cUnpaired Image-to- Image Translation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9156566588472104
      ],
      "excerpt": "    &lt;/li&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9156566588472104
      ],
      "excerpt": "    &lt;/li&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9156566588472104
      ],
      "excerpt": "    &lt;/li&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.913999346918293
      ],
      "excerpt": "&lt;p class=\"s16\" style=\"padding-top: 10pt;padding-left: 230pt;text-indent: 0pt;text-align: left;\"&gt;Figure 10 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8744509615041938
      ],
      "excerpt": "&lt;p class=\"s16\" style=\"padding-left: 195pt;text-indent: 0pt;text-align: center;\"&gt;Figure 11 CycleGAN Discriminator&lt;/p&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9648889021544168
      ],
      "excerpt": "    &lt;li style=\"padding-top: 12pt;padding-left: 28pt;text-indent: -18pt;text-align: justify;\"&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9156566588472104,
        0.9648889021544168
      ],
      "excerpt": "    &lt;/li&gt; \n    &lt;li style=\"padding-top: 2pt;padding-left: 28pt;text-indent: -18pt;text-align: justify;\"&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9156566588472104,
        0.9648889021544168
      ],
      "excerpt": "    &lt;/li&gt; \n    &lt;li style=\"padding-top: 2pt;padding-left: 28pt;text-indent: -18pt;text-align: justify;\"&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9156566588472104,
        0.9648889021544168
      ],
      "excerpt": "    &lt;/li&gt; \n    &lt;li style=\"padding-top: 2pt;padding-left: 28pt;text-indent: -18pt;text-align: justify;\"&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9156566588472104
      ],
      "excerpt": "    &lt;/li&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8130108623877175
      ],
      "excerpt": "            &lt;p class=\"s27\" style=\"padding-left: 5pt;text-indent: 0pt;text-align: left;\"&gt;Identity Loss&lt;/p&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8130108623877175
      ],
      "excerpt": "            &lt;p class=\"s27\" style=\"padding-left: 5pt;text-indent: 0pt;text-align: left;\"&gt;Identity Loss&lt;/p&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8609352805919467
      ],
      "excerpt": "&lt;p class=\"s16\" style=\"padding-top: 7pt;padding-left: 239pt;text-indent: 0pt;text-align: center;\"&gt;Figure 12 DATASET \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9648889021544168
      ],
      "excerpt": "    &lt;li style=\"padding-top: 12pt;padding-left: 23pt;text-indent: -18pt;text-align: justify;\"&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9156566588472104,
        0.9648889021544168
      ],
      "excerpt": "    &lt;/li&gt; \n    &lt;li style=\"padding-top: 1pt;padding-left: 23pt;text-indent: -18pt;text-align: justify;\"&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9156566588472104,
        0.9648889021544168
      ],
      "excerpt": "    &lt;/li&gt; \n    &lt;li style=\"padding-top: 1pt;padding-left: 23pt;text-indent: -18pt;text-align: justify;\"&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9156566588472104,
        0.9648889021544168,
        0.8567000257535553,
        0.9156566588472104,
        0.9648889021544168
      ],
      "excerpt": "    &lt;/li&gt; \n    &lt;li style=\"padding-top: 1pt;padding-left: 23pt;text-indent: -18pt;text-align: justify;\"&gt; \n        &lt;p style=\"display: inline;\"&gt;Find Contours&lt;/p&gt; \n    &lt;/li&gt; \n    &lt;li style=\"padding-left: 23pt;text-indent: -18pt;text-align: justify;\"&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9156566588472104,
        0.9648889021544168
      ],
      "excerpt": "    &lt;/li&gt; \n    &lt;li style=\"padding-top: 1pt;padding-left: 23pt;text-indent: -18pt;text-align: justify;\"&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9156566588472104
      ],
      "excerpt": "    &lt;/li&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9156566588472104,
        0.9648889021544168
      ],
      "excerpt": "    &lt;/li&gt; \n    &lt;li style=\"padding-left: 28pt;text-indent: -18pt;text-align: justify;\"&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9156566588472104
      ],
      "excerpt": "    &lt;/li&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9072595767468146
      ],
      "excerpt": "&lt;p class=\"s16\" style=\"padding-top: 12pt;padding-left: 195pt;text-indent: 0pt;text-align: center;\"&gt;Figure 18 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8002236376841722
      ],
      "excerpt": "&lt;p class=\"s17\" style=\"padding-top: 3pt;padding-left: 10pt;text-indent: 0pt;text-align: justify;\"&gt;RESIZING AND \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8130108623877175
      ],
      "excerpt": "            &lt;p class=\"s27\" style=\"padding-left: 5pt;text-indent: 0pt;text-align: left;\"&gt;35,264,003&lt;/p&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8130108623877175
      ],
      "excerpt": "            &lt;p class=\"s27\" style=\"padding-left: 5pt;text-indent: 0pt;text-align: left;\"&gt;35,264,003&lt;/p&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8130108623877175
      ],
      "excerpt": "            &lt;p class=\"s27\" style=\"padding-left: 5pt;text-indent: 0pt;text-align: left;\"&gt;35,264,003&lt;/p&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8130108623877175
      ],
      "excerpt": "            &lt;p class=\"s27\" style=\"padding-left: 5pt;text-indent: 0pt;text-align: left;\"&gt;35,264,003&lt;/p&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.837474530564915
      ],
      "excerpt": "&lt;p style=\"padding-top: 4pt;text-indent: 0pt;text-align: center;\"&gt;&lt;span class=\"s14\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8907380912291473
      ],
      "excerpt": "&lt;p class=\"s16\" style=\"padding-top: 11pt;padding-left: 206pt;text-indent: 0pt;text-align: center;\"&gt;Figure 28 Results \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8071058506052579
      ],
      "excerpt": "&lt;p style=\"text-indent: 0pt;text-align: center;\"&gt;&lt;span class=\"s14\" style=\" background-color: #D4EAF3;\"&gt; &lt;/span&gt;&lt;span \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9537814227432281
      ],
      "excerpt": "&lt;p class=\"s16\" style=\"padding-top: 12pt;padding-left: 206pt;text-indent: 0pt;text-align: center;\"&gt;Figure 29 Results \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9288732270922924,
        0.9156566588472104
      ],
      "excerpt": "                https://github.com/1j01/scribble (accessed Dec. 29, 2020).&lt;/span&gt;&lt;/p&gt; \n    &lt;/li&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9969327229404801,
        0.9156566588472104
      ],
      "excerpt": "            text recognition.\u201d https://github.com/Belval/TextRecognitionDataGenerator (accessed Dec. 29, 2020).&lt;/p&gt; \n    &lt;/li&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9885310661956346
      ],
      "excerpt": "        &lt;p style=\"display: inline;\"&gt;&lt;a href=\"http://arxiv.org/abs/1308.0850\" class=\"a\" target=\"_blank\"&gt;A. Graves, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.879465991398426,
        0.9663647201823446,
        0.9156566588472104,
        0.9648889021544168
      ],
      "excerpt": "                Available: &lt;/a&gt;&lt;a href=\"http://arxiv.org/abs/1308.0850\" \n                target=\"_blank\"&gt;http://arxiv.org/abs/1308.0850.&lt;/a&gt;&lt;/p&gt; \n    &lt;/li&gt; \n    &lt;li style=\"padding-top: 9pt;padding-left: 39pt;text-indent: -29pt;text-align: left;\"&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9156566588472104
      ],
      "excerpt": "    &lt;/li&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9113870972108766
      ],
      "excerpt": "        &lt;p class=\"s9\" style=\"display: inline;\"&gt;I. J. Goodfellow &lt;i&gt;et al.&lt;/i&gt;&lt;a \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9156566588472104
      ],
      "excerpt": "    &lt;/li&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.868298948811756,
        0.9998231306077278,
        0.9999999497087811,
        0.9991460869588846,
        0.9486382136480312,
        0.9864364909053388,
        0.9156566588472104
      ],
      "excerpt": "        &lt;p class=\"s9\" style=\"display: inline;\"&gt;P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, \u201cImage-to-Image \n            Translation with Conditional Adversarial Networks,\u201d &lt;i&gt;Proceedings - 30th IEEE Conference on Computer \n                Vision and Pattern Recognition, CVPR 2017&lt;/i&gt;&lt;a href=\"http://arxiv.org/abs/1611.07004\" class=\"a\" \n                target=\"_blank\"&gt;, vol. 2017-January, pp. 5967\u20135976, Nov. 2016, Accessed: Dec. 29, 2020. [Online]. \n                Available: &lt;/a&gt;&lt;a href=\"http://arxiv.org/abs/1611.07004\" \n                target=\"_blank\"&gt;http://arxiv.org/abs/1611.07004.&lt;/a&gt;&lt;/p&gt; \n    &lt;/li&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9620476851575293,
        0.8524048831546682,
        0.9999962420627764,
        0.9992679852394774,
        0.879465991398426,
        0.9663647201823446,
        0.9156566588472104
      ],
      "excerpt": "        &lt;p class=\"s9\" style=\"display: inline;\"&gt;J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, \u201cUnpaired \n            Image-to-Image Translation using Cycle-Consistent Adversarial Networks,\u201d &lt;i&gt;Proceedings of the IEEE \n                International Conference on Computer Vision&lt;/i&gt;&lt;a href=\"http://arxiv.org/abs/1703.10593\" class=\"a\" \n                target=\"_blank\"&gt;, vol. 2017-October, pp. 2242\u20132251, Mar. 2017, Accessed: Dec. 29, 2020. [Online]. \n                Available: &lt;/a&gt;&lt;a href=\"http://arxiv.org/abs/1703.10593\" \n                target=\"_blank\"&gt;http://arxiv.org/abs/1703.10593.&lt;/a&gt;&lt;/p&gt; \n    &lt;/li&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9933033000897807,
        0.9969368211327289,
        0.9999858516759673,
        0.9156566588472104
      ],
      "excerpt": "        &lt;p class=\"s9\" style=\"display: inline;\"&gt;X. Huang, L. Wen, and J. DIng, \u201cSAR and Optical Image Registration \n            Method Based on Improved CycleGAN,\u201d &lt;i&gt;2019 6th Asia-Pacific Conference on Synthetic Aperture Radar, \n                APSAR 2019&lt;/i&gt;, 2019, doi: 10.1109/APSAR46974.2019.9048448.&lt;/p&gt; \n    &lt;/li&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8002236376841722,
        0.9441018267299571,
        0.894110301647197,
        0.9156566588472104
      ],
      "excerpt": "        &lt;p class=\"s9\" style=\"display: inline;\"&gt;L. Kang, M. Rusi\u00f1ol, A. Forn\u00e9s, P. Riba, and M. Villegas, \n            \u201cUnsupervised adaptation for synthetic-to-real handwritten word recognition,\u201d &lt;i&gt;arXiv&lt;/i&gt;, pp. \n            3491\u20133500, 2019.&lt;/p&gt; \n    &lt;/li&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.988109762061828,
        0.9989773413426221,
        0.9156566588472104
      ],
      "excerpt": "            CycleGAN and Spectral Blending for Adaptive Radiotherapy,\u201d &lt;i&gt;Proceedings - International Symposium on \n                Biomedical Imaging&lt;/i&gt;, vol. 2020-April, pp. 638\u2013641, 2020, doi: 10.1109/ISBI45749.2020.9098367.&lt;/p&gt; \n    &lt;/li&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8002236376841722
      ],
      "excerpt": "        &lt;p class=\"s9\" style=\"display: inline;\"&gt;G. Senthil, K. Nandhakumar, and G. R. K. S. Subrahmanyam, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999550443364407,
        0.8690140377415217,
        0.9156566588472104
      ],
      "excerpt": "                International Conference on Signal Processing and Communications&lt;/i&gt;, 2020, doi: \n            10.1109/SPCOM50965.2020.9179634.&lt;/p&gt; \n    &lt;/li&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8393254600078464,
        0.9992099298683753,
        0.9156566588472104
      ],
      "excerpt": "            Identical-pair Adversarial Networks,\u201d &lt;i&gt;Applied Sciences (Switzerland)&lt;/i&gt;, vol. 9, no. 13, pp. 1\u201315, \n            2019, doi: 10.3390/app9132668.&lt;/p&gt; \n    &lt;/li&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8907912767784436,
        0.9840420566463718,
        0.9999990660209291,
        0.9156566588472104
      ],
      "excerpt": "        &lt;p class=\"s9\" style=\"display: inline;\"&gt;E. Alonso, B. Moysset, and R. Messina, \u201cAdversarial generation of \n            handwritten text images conditioned on sequences,\u201d &lt;i&gt;Proceedings of the International Conference on \n                Document Analysis and Recognition, ICDAR&lt;/i&gt;, pp. 481\u2013486, 2019, doi: 10.1109/ICDAR.2019.00083.&lt;/p&gt; \n    &lt;/li&gt; \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/AsianZeus/Ooze-Handwritten-Text-Generation",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-10-12T12:42:10Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-07T07:06:01Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9912937113685328
      ],
      "excerpt": "    this project is to generate the human like natural handwritten text from the text data. It tries to synthesize \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9057984495869335
      ],
      "excerpt": "    synthesis is the automatic generation of data that mimic the natural handwriting. This field is a popular from \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9757678280526,
        0.9633689408239644,
        0.8257656249532629
      ],
      "excerpt": "    has lot of possibilities because the use of Deep Learning Neural Networks. It can learn different features and \n    styles based on the probability distribution over the input samples. The main idea is to collect the different \n    writing styles from the user and to generate the same looking handwritings. Few modern models are published in \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9671185514076848
      ],
      "excerpt": "    approaches we used in this project. We are using the Pix2Pix and CycleGAN models. Both models use the image to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9400797965955688,
        0.8826753432567315,
        0.9845675161313291
      ],
      "excerpt": "&lt;p style=\"padding-left: 10pt;text-indent: 0pt;text-align: justify;\"&gt;For dataset of this project we have created our \n    custom dataset of handwritten text with around 1200+ lines and 9000+ words. Different pre-processing techniques \n    are applied for data cleaning and structuring. Then words are labelled with the help of a custom GUI and saved \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8311492891148861
      ],
      "excerpt": "    function. Then Results are compared with the line and words dataset for both the models. A Web App is also \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.858127746745244
      ],
      "excerpt": "    is this field is mostly done by the pasting character images together to generate the sequence&lt;span \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.827999817776151
      ],
      "excerpt": "    is fairly simply and not efficient as each character will look the same every time and won\u2019t give realistic \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8584086619975521,
        0.9402128795545092
      ],
      "excerpt": "&lt;p style=\"padding-top: 10pt;padding-left: 10pt;text-indent: 0pt;text-align: left;\"&gt;Our approach is based on the \n    learning of the data distribution to generate the human like handwritten text.&lt;/p&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8747265039783586,
        0.9846420491851445,
        0.9809844808022071,
        0.9627548739259579,
        0.8910418501997573
      ],
      "excerpt": "    Learning came into focus, in classical machine learning approach there were OCR for character recognition. \n    Machine Learning models only gives the results based on the input features we feed to the model to mimic pattern \n    in the data. While in Deep Learning Neural Networks learns the features to generate the distribution according \n    the given input. Neural Networks are capable enough to capture the little details in the data in compare to \n    Machine Learning models, Therefore, Deep Learning is better choice than Machine Learning in this case.&lt;/p&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9638239386010831
      ],
      "excerpt": "    Neural Networks are preferred choice for working with image data in Deep Learning approach. CNNs are special \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8973580095779355,
        0.9521127067435553
      ],
      "excerpt": "    layer performs the convolution operation on the given image with the help of a fixed size kernel. Pooling layer \n    performs the pooling operation with different approaches. Pooling is simply a method of down sampling the image. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9321927311051081
      ],
      "excerpt": "    classical machine learning algorithms to generate characters but it was not natural human like handwriting. He \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9913295877319647,
        0.9935494010419977,
        0.8110015681600197,
        0.9448485131661303
      ],
      "excerpt": "    is to simulate and generate handwritten text and understand the flow of ligatures of a human handwriting. A \n    Generative Model is a powerful way of learning any kind of data distribution using unsupervised learning and it \n    has achieved tremendous success in just few years. All types of generative models aim at learning the true data \n    distribution of the training set so as to generate new data points with some variations.&lt;/p&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8817244915086557,
        0.8176195918214137
      ],
      "excerpt": "    information about the input data distribution. But in a vanilla autoencoder, the encoded vector can only be \n    mapped to the corresponding input using a decoder. It certainly can\u2019t be used to generate similar images with \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9571032513413075,
        0.9872972925700665,
        0.9485835758631563,
        0.963232402485731,
        0.9697328136020275,
        0.8275938235936514,
        0.9483013197904238,
        0.9628507955130272,
        0.9979643101358042,
        0.9824606974966025
      ],
      "excerpt": "        to learn the complicated data distribution such as images using neural networks in an unsupervised fashion. \n        It is a probabilistic graphical model rooted in Bayesian inference i.e., the model aims to learn the \n        underlying probability distribution of the training data so that it could easily sample new data from that \n        learned distribution. The idea is to learn a low-dimensional latent representation of the training data \n        called latent variables which we assume to have generated our actual training data. These latent variables \n        can store useful information about the type of output the model needs to generate. The probability \n        distribution of latent variables z is denoted by P(z). A Gaussian distribution is selected as a prior to \n        learn the distribution P(z) so as to easily sample new data points during inference time. Now the primary \n        objective is to model the data with some parameters which maximizes the likelihood of training data X. Under \n        this generative process, our aim is to maximize the probability of each data in X which is given as,&lt;/span&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.891716688285323,
        0.8858719102204288,
        0.9604434026307872
      ],
      "excerpt": "    Good Fellow introduced a paper[5] on Generative Adversarial Networks with two Neural Networks saying \u201cA new \n    framework for estimating generative models via an adversarial process, in which training of two models: a \n    generative model G that captures the data distribution, and a discriminative model D that estimates the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9632090981205618
      ],
      "excerpt": "    an actual image whose distribution is subject to pdata(x). z is a random vector (or hidden state) whose \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.840190669623199
      ],
      "excerpt": "    confidence that the input is a real image. G(z) stands for generator and outputs generated images.&lt;/p&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8281770155150511,
        0.9113801188335809
      ],
      "excerpt": "    words, we are happy if D(x) is more confident in classifying real images as \u201creal\u201d. The second term gets higher \n    if D(x) is less confident about classifying the image generated by G(z) as \u201creal\u201d. also, the second term gets \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9776060146590765,
        0.9926893777302486
      ],
      "excerpt": "    distribution with smaller dimensionality than the dimensionality of the training data but that hopefully \n    represents the training data. Consequently, the model is forced to learn the essential features of the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8113199118713137,
        0.8977007608719896,
        0.8576819537874986,
        0.9309321202549281,
        0.8130256432244692
      ],
      "excerpt": "&lt;p class=\"s20\" style=\"padding-left: 10pt;text-indent: 0pt;text-align: justify;\"&gt;The best thing of VAE is that it \n    learns both the generative model and an inference model. Although both VAE and GANs are very exciting approaches \n    to learn the underlying data distribution using unsupervised learning but &lt;span class=\"s23\"&gt;GANs yield better \n        results as compared to VAE&lt;/span&gt;. In VAE, we optimize the lower variational bound whereas in GAN, there is \n    no such assumption. In fact, GANs don\u2019t deal with any explicit probability density estimation. The&lt;/p&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8116750712104776
      ],
      "excerpt": "    generating sharp images implies that the model is not able to learn the true posterior distribution.&lt;/p&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9307768353386452
      ],
      "excerpt": "    small latent dimension, the information of input is so hard to pass through this bottleneck, meanwhile it tries \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9467889959040829
      ],
      "excerpt": "    to learn every possible ligature for sequencing which requires many deep layers and complexity of the problem \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9897463061829125
      ],
      "excerpt": "    Translation is an approach where the we feed the text input to the model and it outputs an image based on \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8712213911397331
      ],
      "excerpt": "        graphics problems where the goal is to learn the mapping between an input image and an output image.&lt;/span&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9889197646124965
      ],
      "excerpt": "    The problem with text to image translation is that the model will only able to learn and understand from the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9629065730024242,
        0.916314471544827,
        0.9884137160743014,
        0.978725132714372
      ],
      "excerpt": "        Unicode Consortium, and as of March 2020, Every symbol and character is different and there might be a need \n        to make changes in the architecture itself for few of them. So it is not an effective and general approach \n        to proceed with, In Image to Image translation the model just have to learn and understand the binary pixel \n        interpretation of image and we\u2019re good to go. Hence the preferred choice is Image to Image \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9675471519000541
      ],
      "excerpt": "    Image Translation is a growing area of research due to variety of applications and we\u2019ve listed few popular \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.904780384024782
      ],
      "excerpt": "    Generative Adversarial Network, or GAN, is an approach to training a deep convolutional neural network for \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.965553538601257,
        0.8063427974163871,
        0.9388699504473045,
        0.9305960882836503
      ],
      "excerpt": "    comprised of a generator model for outputting new plausible synthetic images, and a discriminator model that \n    classifies images as real (from the dataset) or fake (generated). The discriminator model is updated directly, \n    whereas the generator model is updated via the discriminator model. As such, the two models are trained \n    simultaneously in an adversarial process where the generator seeks to better fool the discriminator and the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9146042189699247
      ],
      "excerpt": "    conditional GAN, or cGAN, where the generation of the output image is conditional on an input, in this case, a \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8762213899756114,
        0.9359571688779238
      ],
      "excerpt": "    domain. The generator is also updated via L1 loss measured between the generated image and the expected output \n    image. This additional loss encourages the generator model to create plausible translations of the source image. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8794124210152248,
        0.936890734313359
      ],
      "excerpt": "    the generator is a convolutional network with U-net architecture. It takes in the input image passes it through \n    a series of convolution and up-sampling layers. Finally, it produces an output image that is of the same size as \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9339171950188364,
        0.8397133464667585
      ],
      "excerpt": "    also interesting because they do not require any resizing, projections etc. since the spatial resolution of the \n    layers being connected already match each other. The encoder and decoder of the generator are comprised of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8608536502827002,
        0.9693082156671705,
        0.9575190952814076,
        0.9787202747574898,
        0.9443716397982043
      ],
      "excerpt": "    discriminator is a deep convolutional neural network that performs image classification. Specifically, \n    conditional-image classification. The discriminator design is based on the effective receptive field of the \n    model, which defines the relationship between one output of the model to the number of pixels in the input \n    image. This is called a PatchGAN model and is carefully designed so that each output prediction of the model \n    maps to a 70\u00d770 square or patch of the input image. The benefit of this approach is that the same model can be \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.964129128164729,
        0.8447840981431469
      ],
      "excerpt": "    of the model depends on the size of the input image but may be one value or a square activation map of values. \n    Each value is a probability for the likelihood that a patch in the input image is real. These values can be \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8591286408862102
      ],
      "excerpt": "        &lt;p style=\"display: inline;\"&gt;Since pix2pix\u2019s generator is using the U-Net architecture which implies that the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8715216133569882,
        0.9669632717213466
      ],
      "excerpt": "    the general dimension of any text between 2 guidelines is (MxN). Even each word brings its own inconsistencies \n    of size, since the number of characters in a word can commonly range from 3 to 12, hence the dimension of the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9203631423270513,
        0.9837665294460791
      ],
      "excerpt": "    few tricks that could be done for example cropping the image, resizing the image and leaving the rest of the \n    area with white space or squishing the words to resize it in form of (NxN), yet it will be very difficult to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8631187197270186,
        0.9518489555183415,
        0.8766780949246628,
        0.8156014168361045,
        0.8978030624946055,
        0.8984376761592088,
        0.8627308838624168,
        0.879250066723143,
        0.9818834006438162
      ],
      "excerpt": "    of the CycleGAN model is that it can be trained without paired examples. Recent methods such as Pix2Pix depend \n    on the availability of training examples where the same data is available in both domains. The power of CycleGAN \n    lies in being able to learn such transformations without one-to-one mapping between training data in source and \n    target domains, i.e. it does not require examples of photographs before and after the translation in order to \n    train the model. Instead, the model is able to use a collection of photographs from each domain and extract and \n    harness the underlying style of images in the collection in order to perform the translation. The need for a \n    paired image in the target domain is eliminated by making a two-step transformation of source domain image - \n    first by trying to map it to target domain and then back to the original image. Mapping the image to target \n    domain is done using a generator network and the quality of this generated image is improved by pitching the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9254750215184692
      ],
      "excerpt": "    proposes four models instead of the usual two. The first generator, which we\u2019ll call Generator A, is for \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8249133526606682,
        0.8529643208489324
      ],
      "excerpt": "    perform image translation, they\u2019ll accept an image of the other domain as input. Meaning, Generator A will take \n    an image from Domain B as input to translate it to Domain A and similarly, Generator B takes an image from \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8195350978424096,
        0.8750815551083089
      ],
      "excerpt": "    discriminator and generator models are trained in an adversarial zero-sum process, like normal GAN models. The \n    generators learn to better fool the discriminators and the discriminator learns to better detect fake images. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.920406014389595,
        0.8729876192029277,
        0.9687224226616903
      ],
      "excerpt": "    generator is the same in that it also an encoder-decoder model architecture that takes a source image and \n    generates a target image by down-sampling or encoding the input image down to a bottleneck layer. But it is \n    different than the pix2pix generator in that the encodings are interpreted with a number of ResNet (Residual) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9786671960805308,
        0.9172981151183923,
        0.8473032861686408
      ],
      "excerpt": "            using Convolutions and compressed the representation of image but increase the number of channels. The \n            encoder consists of 3 convolution that reduces the representation by 1/4th of actual image size. \n            Consider an image of size (256, 256, 3) which we input into the encoder, the output of encoder will be \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8711779632484195
      ],
      "excerpt": "            transformer contains 9 residual blocks based on the size of input.&lt;/p&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.939440799122592
      ],
      "excerpt": "            -deconvolution block of fraction strides to increase the size of representation to original size.&lt;/p&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8909507795461622,
        0.9822721423413606,
        0.9811859573942515,
        0.883738403284854,
        0.9693804410792485
      ],
      "excerpt": "    discriminator is a Convolutional Neural Network that performs image classification. The discriminator design of \n    the CycleGAN is based on the effective receptive field of the model, which defines the relationship between one \n    output of the model to the number of pixels in the input image. This is called a Patch GAN model and is \n    carefully designed so that each output prediction of the model maps to a 70\u00d770 square or patch of the input \n    image. The benefit of this approach is that the same model can be applied to input images of different sizes, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.981106631248048,
        0.8447840981431469
      ],
      "excerpt": "    of the model depends on the size of the input image but maybe one value or a square activation map of values. \n    Each value is a probability for the likelihood that a patch in the input image is real. These values can be \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9378830410530955
      ],
      "excerpt": "    discriminator uses Instance Normalization instead of Batch Normalization. It is a very simple type of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8501924913292881
      ],
      "excerpt": "    map, rather than across features in a batch which is done by Batch Normalization.&lt;/p&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9774290276862388
      ],
      "excerpt": "    updated based on how effective they are at the regeneration of a source image when used with the other generator \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9498603937876069,
        0.9525853039890059,
        0.9125886290829002,
        0.8952197308672145
      ],
      "excerpt": "    discriminator is connected to the output of the generator in order to classify generated images as real or fake. \n    A second input for the composite model is defined as an image from the target domain (instead of the source \n    domain), which the generator is expected to output without translation for the identity mapping. Next, forward \n    cycle loss involves connecting the output of the generator to the other generator, which will reconstruct the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9381092215141116
      ],
      "excerpt": "    mapping that is also passed through the other generator whose output is connected to our main generator as input \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8383612054991987
      ],
      "excerpt": "    outputs of the model are as follows:&lt;/p&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8383612054991987
      ],
      "excerpt": "    outputs of the model are as follows:&lt;/p&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9842768669829725
      ],
      "excerpt": "    Handwritten Dataset is used in this project. Dataset is consists of texts from the books named \u201cThe Biography of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8782919183898764
      ],
      "excerpt": "    contains 1200+ lines and around 9000+ words. Each Page contains average 23 lines on a page with size of A4.&lt;/p&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8778862902083181
      ],
      "excerpt": "    image has no guidelines; the text is written on the simple paper, In that case, we can simply find the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9435795679574028
      ],
      "excerpt": "    particular orientation. Once the coloured image is converted to the binary image only black and white pixels are \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8034749615629057
      ],
      "excerpt": "    not foreground pixels are called Background pixels. It is our choice whether a foreground pixel should be a \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9611807472356361,
        0.9752222742318144
      ],
      "excerpt": "    Histogram Projection: In this method, we count the number of foreground pixels along the rows of the image and \n    the resultant array is of the size equal to number of rows in the image (Height of the image).&lt;/p&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9055820165613404
      ],
      "excerpt": "    of contours of white and black colour representing text and blank side respectively, Later we extract lines by \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8446035473753483,
        0.8329675896523161
      ],
      "excerpt": "    the column black (assuming we have white as a background colour) But there are some thin ligatures which can \n    cross the threshold so to remove that errorious decision we use rectify function which checks connectivity of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8053321567040962,
        0.9861713529107597
      ],
      "excerpt": "    not foreground pixels are called Background pixels. In this method, we count the number of foreground pixels \n    along the columns of the image and the resultant array is of the size equal to number of columns in the image \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9249300887203543
      ],
      "excerpt": "    of total pixels) cross the threshold of 0.945 then we can declare it as word and applying the rectify() to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8126688666881777
      ],
      "excerpt": "    blocks of contours of black and white colour representing text and black side respectively, Later we extract \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8247688955898114,
        0.947269626380995,
        0.9965227755835536
      ],
      "excerpt": "    values in images must be scaled prior to providing the images as input to a deep learning neural network model \n    during the training or evaluation of the model. We rescale the pixel values from the range of 0-255 to the range \n    0-1 preferred for neural network models. Scaling data to the range of 0-1 is traditionally referred to as \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8160497247891371
      ],
      "excerpt": "    another way, we can add padding (white space) around the images to make each image of same height and width, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9756731053294425,
        0.8848236381905353,
        0.9113086540102777,
        0.8299895159652436
      ],
      "excerpt": "    Tkinter library to label the words. It Loads the words and text from lines in which the word is there. Based on \n    the text associated with lines in which arrives GUI gives the Recommendation for labelling the words images. \n    Enter Label Text Area gives the flexibility of Typing the correct word if it is not shown in the recommendation \n    section. The final labels are pickled in a dictionary where the key is image\u2019s name and value is the actual \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8217086983404679
      ],
      "excerpt": "    approach was fairly typical for an image-to-image translation task. we load our paired images dataset in \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.842999386784041,
        0.8276565267086368,
        0.8660631368760459,
        0.8746241646378493
      ],
      "excerpt": "    second for corresponding target images. The Adam optimizer, a common variant of gradient descent, was used to \n    make training more stable and efficient. The learning rate was set to 0.0002 for the first half of training, and \n    then linearly reduced to zero over the remaining iterations. The batch size was set to 1, which is why we refer \n    to instance normalization, rather than batch normalization. To monitor and visualize the loss in real time, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9777453625297943,
        0.822933387920799
      ],
      "excerpt": "    This is model summary, on the left we have generator and on the right we have discriminator and GAN composite \n    model. As we know by now that CycleGAN has 2 generator 2 discriminator and hence 2 GAN composite models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9777453625297943,
        0.822933387920799
      ],
      "excerpt": "    This is model summary, on the left we have generator and on the right we have discriminator and GAN composite \n    model. As we know by now that CycleGAN has 2 generator 2 discriminator and hence 2 GAN composite models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9823527606254073
      ],
      "excerpt": "    one of the major issues during the training of Model because our Dataset contains only 9000+ images which is not \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9715200974878385
      ],
      "excerpt": "    Non-Convergence: Here the parameters of model oscillate and become unstable and never converge. Model Collapse: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030889636654718,
        0.9481816769608923
      ],
      "excerpt": "    makes it difficult to resize since words are of variable length and lines have rectangular dimension i.e. have \n    more width than height, it hinders the training since the generated feature map won\u2019t be able to grasp all the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8121355109199055,
        0.9859526453925352
      ],
      "excerpt": "    is another cause with we observed with Pix2Pix, the validation loss overshoot after 7-10 epochs even after \n    applying regularization and changing the architecture of the network, this could be because of lack of data, but \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8109051412808843,
        0.9471657396975502
      ],
      "excerpt": "    still CycleGAN performs much better because of its ingenious cycle consistency loss, the problem of mode \n    collapse and overshooting the validation loss won\u2019t occur. Yet it is very hard to train and understand when to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9562925140910483
      ],
      "excerpt": "    CycleGAN is optimizing it\u2019s network fast and trying hard to compete with the discriminator, yet the gradients \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9795834497982804,
        0.921555752350974,
        0.850521110388095,
        0.9635595745664037
      ],
      "excerpt": "    fluctuations in the generator loss, that\u2019s because of the instance normalization we are taking the batch size of \n    1 and now we are updating weights for every iteration rather than for every epoch, this may sometime lead to \n    false gradient and wrong direction hence the fluctuations. CycleGAN performed much better than pix2Pix in this \n    case and it didn\u2019t face any mode collapse issue which is very common while training any GAN model. Lack of data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9820485563266447
      ],
      "excerpt": "    Design is created using the CSS and Animated using JavaScript. It also contains the Bootstrap support for more \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8867271317831684
      ],
      "excerpt": "    microframework Flask which is used to create web apps using Python Language.&lt;/p&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8900172568916738,
        0.8065343330393209
      ],
      "excerpt": "    input text is first converted to Image. This converted image is then feed to the trained model and from this \n    input sample model generate the image which again sent to the user back.&lt;/p&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9150317886633694,
        0.8475484368238709,
        0.9260961863743224,
        0.9348267476415655
      ],
      "excerpt": "    before we used the Generative model approach for this project. It had few challenging problems during the \n    training. The first problem was of amount of the dataset we could manage. We created dataset of around 1200+ \n    lines and size of words in that dataset was around 9000+. However, for a Deep Learning Neural Network the amount \n    of Data is not sufficient to train. This problem can be neglected using the large amount of data.&lt;/p&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9066426627775044,
        0.8723729743285071,
        0.9295168344103374
      ],
      "excerpt": "    Image translations is a good approach for handwritten text imitation/generation as the model doesn\u2019t have to \n    have any knowledge about symbols or characters and it can easily decipher from pixels and this model can be used \n    for any language as the input format is always going to be an image.&lt;/p&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665934252676482,
        0.9809327402003604,
        0.9436549580659025
      ],
      "excerpt": "    our model on line dataset as it has fixed rectangular shape instead of words which has variable lengths. This is \n    result in better performance as the model will be able to grasp more features. There are other architectures of \n    generative models which came out this years and showed really promising results in the domain of image to image \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8729377877505996
      ],
      "excerpt": "                \u201cGenerating Sequences With Recurrent Neural Networks,\u201d Aug. 2013, Accessed: Dec. 29, 2020. [Online]. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/AsianZeus/Ooze-Handwritten-Text-Generation/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 29 Dec 2021 17:32:01 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/AsianZeus/Ooze-Handwritten-Text-Generation/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "AsianZeus/Ooze-Handwritten-Text-Generation",
    "technique": "GitHub API"
  },
  "invocation": [
    {
      "confidence": [
        0.8112280467360953
      ],
      "excerpt": "<p style=\"padding-top: 4pt;padding-left: 5pt;text-indent: 0pt;text-align: left;\"><span class=\"s7\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8221581701804148
      ],
      "excerpt": "&lt;p style=\"padding-top: 9pt;padding-left: 10pt;text-indent: 0pt;line-height: 114%;text-align: justify;\"&gt;The first \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8275173770064729
      ],
      "excerpt": "&lt;p style=\"padding-top: 5pt;padding-left: 10pt;text-indent: 0pt;line-height: 123%;text-align: justify;\"&gt;The Pix2Pix \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.857115324338414
      ],
      "excerpt": "&lt;p style=\"padding-left: 10pt;text-indent: 0pt;line-height: 123%;text-align: justify;\"&gt;The Pix2Pix model is a type of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8173387425063755
      ],
      "excerpt": "&lt;p style=\"padding-top: 5pt;padding-left: 10pt;text-indent: 0pt;line-height: 123%;text-align: justify;\"&gt;The generator \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8201874547818002
      ],
      "excerpt": "&lt;p style=\"padding-top: 10pt;padding-left: 10pt;text-indent: 0pt;line-height: 115%;text-align: justify;\"&gt;The output \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8201874547818002
      ],
      "excerpt": "&lt;p style=\"padding-top: 10pt;padding-left: 10pt;text-indent: 0pt;line-height: 114%;text-align: justify;\"&gt;The output \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8021537767783191
      ],
      "excerpt": "            &lt;p class=\"s27\" style=\"padding-left: 5pt;text-indent: 0pt;line-height: 115%;text-align: left;\"&gt;Original \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8233566043546291
      ],
      "excerpt": "                style=\"padding-left: 5pt;padding-right: 20pt;text-indent: 0pt;line-height: 115%;text-align: left;\"&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8233566043546291
      ],
      "excerpt": "                style=\"padding-left: 5pt;padding-right: 10pt;text-indent: 0pt;line-height: 115%;text-align: left;\"&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8021537767783191
      ],
      "excerpt": "            &lt;p class=\"s27\" style=\"padding-left: 5pt;text-indent: 0pt;line-height: 115%;text-align: left;\"&gt;Original \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8233566043546291
      ],
      "excerpt": "                style=\"padding-left: 5pt;padding-right: 15pt;text-indent: 0pt;line-height: 115%;text-align: left;\"&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8233566043546291
      ],
      "excerpt": "                style=\"padding-left: 5pt;padding-right: 10pt;text-indent: 0pt;line-height: 115%;text-align: left;\"&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8021537767783191
      ],
      "excerpt": "            &lt;p class=\"s27\" style=\"padding-left: 5pt;text-indent: 0pt;line-height: 115%;text-align: left;\"&gt;Original \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8152587757317709
      ],
      "excerpt": "                style=\"padding-left: 5pt;padding-right: 10pt;text-indent: 0pt;line-height: 113%;text-align: left;\"&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8152587757317709
      ],
      "excerpt": "                style=\"padding-left: 5pt;padding-right: 9pt;text-indent: 0pt;line-height: 113%;text-align: left;\"&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8152587757317709
      ],
      "excerpt": "                style=\"padding-left: 5pt;padding-right: 9pt;text-indent: 0pt;line-height: 113%;text-align: left;\"&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8152587757317709
      ],
      "excerpt": "                style=\"padding-left: 5pt;padding-right: 10pt;text-indent: 0pt;line-height: 113%;text-align: left;\"&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8152587757317709
      ],
      "excerpt": "                style=\"padding-left: 5pt;padding-right: 9pt;text-indent: 0pt;line-height: 113%;text-align: left;\"&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8152587757317709
      ],
      "excerpt": "                style=\"padding-left: 5pt;padding-right: 9pt;text-indent: 0pt;line-height: 113%;text-align: left;\"&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8152587757317709
      ],
      "excerpt": "                style=\"padding-left: 5pt;padding-right: 10pt;text-indent: 0pt;line-height: 113%;text-align: left;\"&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8152587757317709
      ],
      "excerpt": "                style=\"padding-left: 5pt;padding-right: 9pt;text-indent: 0pt;line-height: 113%;text-align: left;\"&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8152587757317709
      ],
      "excerpt": "                style=\"padding-left: 5pt;padding-right: 9pt;text-indent: 0pt;line-height: 113%;text-align: left;\"&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8152587757317709
      ],
      "excerpt": "                style=\"padding-left: 5pt;padding-right: 9pt;text-indent: 0pt;line-height: 113%;text-align: left;\"&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8152587757317709
      ],
      "excerpt": "                style=\"padding-left: 5pt;padding-right: 9pt;text-indent: 0pt;line-height: 113%;text-align: left;\"&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8152587757317709
      ],
      "excerpt": "                style=\"padding-left: 5pt;padding-right: 10pt;text-indent: 0pt;line-height: 113%;text-align: left;\"&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8152587757317709
      ],
      "excerpt": "                style=\"padding-left: 5pt;padding-right: 9pt;text-indent: 0pt;line-height: 113%;text-align: left;\"&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8152587757317709
      ],
      "excerpt": "                style=\"padding-left: 5pt;padding-right: 9pt;text-indent: 0pt;line-height: 113%;text-align: left;\"&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8152587757317709
      ],
      "excerpt": "                style=\"padding-left: 5pt;padding-right: 9pt;text-indent: 0pt;line-height: 113%;text-align: left;\"&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8152587757317709
      ],
      "excerpt": "                style=\"padding-left: 5pt;padding-right: 9pt;text-indent: 0pt;line-height: 113%;text-align: left;\"&gt; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8019018096474682
      ],
      "excerpt": "&lt;p style=\"padding-top: 9pt;padding-left: 5pt;text-indent: 0pt;line-height: 114%;text-align: left;\"&gt;The output blocks \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8201874547818002
      ],
      "excerpt": "&lt;p style=\"padding-top: 10pt;padding-left: 10pt;text-indent: 0pt;line-height: 114%;text-align: justify;\"&gt;The output \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8139683087513716
      ],
      "excerpt": "&lt;p style=\"padding-top: 5pt;padding-left: 12pt;text-indent: 2pt;line-height: 114%;text-align: justify;\"&gt;The training \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8462070113106198
      ],
      "excerpt": "    compressed NumPy array format. This will return a list of two NumPy arrays: the first for source images and the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8260594376137368
      ],
      "excerpt": "&lt;p class=\"s16\" style=\"padding-left: 10pt;text-indent: 0pt;text-align: left;\"&gt;Figure 24 CycleGAN model with line \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8124846779810823
      ],
      "excerpt": "&lt;p style=\"padding-left: 10pt;text-indent: 0pt;line-height: 114%;text-align: left;\"&gt;When user gives input text, the \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/AsianZeus/Ooze-Handwritten-Text-Generation/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "HTML",
      "CSS"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Ooze",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Ooze-Handwritten-Text-Generation",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "AsianZeus",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/AsianZeus/Ooze-Handwritten-Text-Generation/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3,
      "date": "Wed, 29 Dec 2021 17:32:01 GMT"
    },
    "technique": "GitHub API"
  }
}