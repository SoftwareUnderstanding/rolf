{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* **Title**:MLP-Mixer: An all-MLP Architecture for Vision\n* **Authors**: Ilya Tolstikhin\u2217\n, Neil Houlsby\u2217\n, Alexander Kolesnikov\u2217\n, Lucas Beyer\u2217\n,\nXiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner,\nDaniel Keysers, Jakob Uszkoreit, Mario Lucic, Alexey Dosovitskiy\n* **Link**: https://arxiv.org/pdf/2105.01601v4.pdf\n* **Year**: 2021\n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9618188552511033,
        0.8955886365383559,
        0.9278824608274014
      ],
      "excerpt": "       Linear-10              [-1, 32, 512]         262,656 \n         GELU-11              [-1, 32, 512]               0 \n      Dropout-12              [-1, 32, 512]               0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "    LayerNorm-29              [-1, 32, 512]           1,024 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "    LayerNorm-35              [-1, 32, 512]           1,024 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9278824608274014
      ],
      "excerpt": "         GELU-37              [-1, 32, 512]               0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8109194328925066
      ],
      "excerpt": "      Dropout-64              [-1, 32, 512]               0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "     Dropout-157              [-1, 32, 512]               0 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/imad08/MLP-Mixer",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-07-25T22:41:37Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-07-27T20:08:58Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "MLP Mixer is based on multi layer perceptron it does not use modern days CNN , It has two kinds of multi layer preceptrons one is directly applied to image patches , which are created original image then we transpose the layer and apply MLP layer across patches In the extreme case, Multi layer perceptron architecture can be seen as a very special CNN, which uses 1\u00d71 convolutions\nfor channel mixing, and single-channel depth-wise convolutions of a full receptive field and parameter\nsharing for token mixing. However, the converse is not true as typical CNNs are not special cases of\nMixer. Furthermore, a convolution is more complex than the plain matrix multiplication in MLPs as\nit requires an additional costly reduction to matrix multiplication and/or specialized implementation.\nIf you want to see training of cifar10 dataset using above architecture you can refer [here](https://github.com/imad08/MLP-Mixer/blob/main/MLP.ipynb)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9186596053911179,
        0.8920895713833077,
        0.9733788950909448,
        0.9141690022053452,
        0.9506363026729449
      ],
      "excerpt": "Figure depicts the macro-structure of Mixer. It accepts a sequence of linearly projected image \npatches (also referred to as tokens) shaped as a \u201cpatches \u00d7 channels\u201d table as an input, and maintains \nthis dimensionality. Mixer makes use of two types of MLP layers as told above one is applied to image patches , which are created original image then we transpose the layer and apply MLP layer across patches  . The channel-mixing MLPs allow communication between different channels; It is similar to attention models. \nModern deep vision architectures consist of layers that mix features (i) at a given spatial location, \n(ii) between different spatial locations, or both at once. In CNNs, (ii) is implemented with N \u00d7 N \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9167030218409821
      ],
      "excerpt": "and (ii) and the MLP-blocks perform (i). The idea behind the Mixer architecture is to clearly separate \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/imad08/MLP-Mixer/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Fri, 24 Dec 2021 17:59:30 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/imad08/MLP-Mixer/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "imad08/MLP-Mixer",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/imad08/MLP-Mixer/main/MLP.ipynb"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.906614234580968
      ],
      "excerpt": "    Layer (type)               Output Shape         Param # \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8702815774991463
      ],
      "excerpt": "      Dropout-45             [-1, 512, 512]               0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8377621879540922
      ],
      "excerpt": "     Dropout-123             [-1, 512, 512]               0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8934745846565956
      ],
      "excerpt": "Total params: 6,782,858 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/imad08/MLP-Mixer/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "MLP-Mixer: An all-MLP Architecture for Vision",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "MLP-Mixer",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "imad08",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/imad08/MLP-Mixer/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Fri, 24 Dec 2021 17:59:30 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```bash\n$ python3 main.py \n```\nNOTE: on Colab Notebook use following command:\n```python\n!git clone link-to-repo\n%run main.py \n```\n\n",
      "technique": "Header extraction"
    }
  ]
}