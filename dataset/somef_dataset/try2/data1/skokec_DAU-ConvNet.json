{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We thank Vitjan Zavrtanik (VitjanZ) for TensorFlow C++/Python wrapper. \n\n",
      "technique": "Header extraction"
    }
  ],
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please cite our CVPR 2018 paper when using DAU code:\n\n```\n@inproceedings{Tabernik2018,\n\ttitle = {{Spatially-Adaptive Filter Units for Deep Neural Networks}},\n\tauthor = {Tabernik, Domen and Kristan, Matej and Leonardis, Ale{\\v{s}}},\n\tbooktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n\tyear = {2018}\n\tpages = {9388--9396}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{Tabernik2018,\n    title = {{Spatially-Adaptive Filter Units for Deep Neural Networks}},\n    author = {Tabernik, Domen and Kristan, Matej and Leonardis, Ale{\\v{s}}},\n    booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n    year = {2018}\n    pages = {9388--9396}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/skokec/DAU-ConvNet",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-03-26T11:31:36Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-06-30T09:40:17Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8450486655637268,
        0.9864558759785498
      ],
      "excerpt": "Official implementation of Displaced Aggregation Units for Convolutional Networks from CVPR 2018 paper titled \"Spatially-Adaptive Filter Units for Deep Neural Networks\" that was developed as part of Deep Compositional Networks. \nThis repository is a self-contained DAU layer implementation in C++ and CUDA, plus a TensorFlow plugin. Use this library to implement DAU layers for any deep learning framework. For more details on DAUs see ViCoS research page. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9613410024341873,
        0.9862164030424379
      ],
      "excerpt": "See below for more details on each implementation. \nA Caffe implementation based on this library is available in DAU-ConvNet-caffe repository.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8612148299124583
      ],
      "excerpt": "We provide TensorFlow plugin and appropriate Python wrappers that can be used to directly replace the tf.contrib.layers.conv2d function. Note, our C++/CUDA code natively supports only NCHW format for input, please update your TensorFlow models to use this format.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8813912110324772
      ],
      "excerpt": "Pre-compiled docker images for TensorFlow are also available on Docker Hub that are build using the plugins/tensorflow/docker/Dockerfile.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9785227641575353
      ],
      "excerpt": "Mean values (e.g. learned offsets) of DAU units are always based on (0,0) being at the center of the kernel. Default initialization (when passing None) is to arrange units equally over the available space using dau_conv.DAUGridMean initializer class: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9469902626491552
      ],
      "excerpt": "Other TensorFlow initializer classes can be used. For instance distributing them uniformly over the center of the kernel is accomplished by: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9678087261667134,
        0.8368721885077703
      ],
      "excerpt": "Current implementation is limited to using only the following settings: \n * data_format = 'NCHW': only 'NCHW' format available in our C++/CUDA implementation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Displaced Aggregation Units for Convolutional Networks from \"Spatially-Adaptive Filter Units for Deep Neural Networks\" paper",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/skokec/DAU-ConvNet/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 4,
      "date": "Fri, 24 Dec 2021 12:43:53 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/skokec/DAU-ConvNet/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "skokec/DAU-ConvNet",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/skokec/DAU-ConvNet/master/plugins/tensorflow/docker/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/skokec/DAU-ConvNet/master/plugins/tensorflow/scripts/start_main_build.sh",
      "https://raw.githubusercontent.com/skokec/DAU-ConvNet/master/plugins/tensorflow/build-ci/build-whl.sh",
      "https://raw.githubusercontent.com/skokec/DAU-ConvNet/master/plugins/tensorflow/docker/test_dau.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Requirements and dependency libraries to compile DAU-ConvNet:\n * Ubuntu 16.04 (not tested on other OS and other versions)\n * C++11\n * CMake 2.8 or newer (tested on version 3.5)\n * CUDA SDK Toolkit (tested on version 8.0 and 9.0)\n * BLAS (ATLAS or OpenBLAS)\n * cuBlas\n\nOn Ubuntu 16.04 with pre-installed CUDA and cuBLAS (e.g. using nvidia/cuda:9.0-cudnn7-devel-ubuntu16.04 or nvidia/cuda:8.0-cudnn5-devel-ubuntu16.04 docker) install dependencies first:\n\n```bash\napt-get update\napt-get install cmake python python-pip libopenblas-dev\n \npip install tensorflow-gpu>=1.6\n#: Note: during instalation tensorflow package is sufficent, but during running the tensorflow-gpu is required.\n```\n\nThen clone the repository and build from source:\n```bash\ngit clone https://github.com/skokec/DAU-ConvNet\ngit submodule update --init --recursive\n\nmkdir DAU-ConvNet/build\ncd DAU-ConvNet/build\n\ncmake -DBLAS=Open -DBUILD_TENSORFLOW_PLUGIN=on ..\n\nmake -j #: creates whl file in build/plugin/tensorflow/wheelhouse\nmake install #: will install whl package (with .so files) into python dist-packages folder \n\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8848011516290252
      ],
      "excerpt": "Requirements and dependency libraries for TensorFlow plugin: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8233588558014837
      ],
      "excerpt": " * Numpy \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9831196441645343,
        0.9099150869254904,
        0.9839377276530699,
        0.9959326751040033
      ],
      "excerpt": "If you are using TensorFlow from pip, then install a pre-compiled binaries (.whl) from the RELEASE page (mirror server also available http://box.vicos.si/skokec/dau-convnet): \n: install dependency library (OpenBLAS) \nsudo apt-get install libopenblas-dev  wget \n: install dau-conv package \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9901850763227746
      ],
      "excerpt": "sudo pip install https://github.com/skokec/DAU-ConvNet/releases/download/v1.0/dau_conv-1.0_TF[TF_VERSION]-cp35-cp35m-manylinux1_x86_64.whl \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9771033288028359
      ],
      "excerpt": "Note that pip packages were compiled against the specific version of TensorFlow from pip, which must be installed beforhand. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9470803266279425,
        0.9973303249994832,
        0.9989664568736946
      ],
      "excerpt": "To validate installation using unit tests also install scipy, matplotlib and python-tk, and then run dau_conv_test.py: \napt-get install python-tk                 \npip install scipy matplotlib \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9781205547416655
      ],
      "excerpt": "Please make sure that your TensorFlow is compiled against GPU/CUDA. In pip the tensroflow and tensorflow-gpu packages provide the same libtensorflow_framework.so in the same folder but only tensorflow-gpu has the .so that is compiled against the CUDA. If tensroflow gets installed after the tensorflow-gpu then .so with CUDA support will be overriden by the .so without it. Make sure to install tensorflow-gpu the last or not to install tensroflow at all. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9216257729225074
      ],
      "excerpt": " * (optional) Scipy, matplotlib and python-tk  for running unit test in dau_conv_test.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9084078206464392
      ],
      "excerpt": "python DAU-ConvNet/plugins/tensorflow/tests/dau_conv_test.py DAUConvTest.test_DAUConvQuick \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9185291611602425,
        0.9090624869423164,
        0.9185291611602425,
        0.9090624869423164
      ],
      "excerpt": "          mu1_initializer = tf.random_uniform_initializer(minval=-np.floor(max_kernel_size/2.0),  \n                                                          maxval=np.floor(max_kernel_size/2.0),dtype=tf.float32), \n          mu2_initializer = tf.random_uniform_initializer(minval=-np.floor(max_kernel_size/2.0),  \n                                                          maxval=np.floor(max_kernel_size/2.0),dtype=tf.float32),  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8409255096003898,
        0.8375562373184183
      ],
      "excerpt": "dau_conv.DAUGridMean(dau_units, #: number of DAU units per image axis e.g. (2,2) for 4 DAUs total  \n                     max_value, #: max offset  \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/skokec/DAU-ConvNet/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "C++",
      "Cuda",
      "Python",
      "CMake",
      "Shell",
      "Dockerfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "DAU-ConvNet #",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "DAU-ConvNet",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "skokec",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/skokec/DAU-ConvNet/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "skokec",
        "body": "Versions compatible with the CVPR18 and IJCV19 papers.\r\n\r\nKnown bugs:\r\n - minor memory leak in some large architectures after large number of iterations (>500.000).",
        "dateCreated": "2019-09-02T15:09:30Z",
        "datePublished": "2020-07-05T15:42:58Z",
        "html_url": "https://github.com/skokec/DAU-ConvNet/releases/tag/v1.0",
        "name": "Main stable release",
        "tag_name": "v1.0",
        "tarball_url": "https://api.github.com/repos/skokec/DAU-ConvNet/tarball/v1.0",
        "url": "https://api.github.com/repos/skokec/DAU-ConvNet/releases/18617896",
        "zipball_url": "https://api.github.com/repos/skokec/DAU-ConvNet/zipball/v1.0"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 17,
      "date": "Fri, 24 Dec 2021 12:43:53 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "There are two available methods to use our DAU convolution. Using `dau_conv.DAUConv2d` class based on `base.Layer` or using wrapper `dau_conv.dau_conv2d` functions. See below for example on using `dau_conv2d` method.  \n\n\nMethod `dau_conv.dau_conv2d`: \n```python\ndau_conv2d(inputs,\n             filters, #: number of output filters\n             dau_units, #: number of DAU units per image axis, e.g, (2,2) for 4 DAUs per filter \n             max_kernel_size, #: maximal possible size of kernel that limits the offset of DAUs (highest value that can be used=17)  \n             stride=1, #: only stride=1 supported \n             mu_learning_rate_factor=500, #: additional factor for gradients of mu1 and mu2\n             data_format=None,\n             activation_fn=tf.nn.relu,\n             normalizer_fn=None,\n             normalizer_params=None,\n             weights_initializer=tf.random_normal_initializer(stddev=0.1), \n             weights_regularizer=None,\n             mu1_initializer=None, #: see below for default initialization values\n             mu1_regularizer=None, #: see below for default initialization values\n             mu2_initializer=None,\n             mu2_regularizer=None,\n             sigma_initializer=None,\n             sigma_regularizer=None,\n             biases_initializer=tf.zeros_initializer(),\n             biases_regularizer=None,\n             reuse=None,\n             variables_collections=None,\n             outputs_collections=None,\n             trainable=True,\n             scope=None)\n```\n\nClass `dau_conv.DAUConv2d`: \n```python\n\nDAUConv2d(filters, #: number of output filters\n           dau_units, #: number of DAU units per image axis, e.g, (2,2) for 4 DAUs total per one filter\n           max_kernel_size, #: maximal possible size of kernel that limits the offset of DAUs (highest value that can be used=17)\n           strides=1, #: only stride=1 supported\n           data_format='channels_first', #: supports only 'channels_last' \n           activation=None,\n           use_bias=True,\n           weight_initializer=tf.random_normal_initializer(stddev=0.1),\n           mu1_initializer=None, #: see below for default initialization values\n           mu2_initializer=None, #: see below for default initialization values\n           sigma_initializer=None,\n           bias_initializer=tf.zeros_initializer(),\n           weight_regularizer=None,\n           mu1_regularizer=None,\n           mu2_regularizer=None,\n           sigma_regularizer=None,\n           bias_regularizer=None,\n           activity_regularizer=None,\n           weight_constraint=None,\n           mu1_constraint=None,\n           mu2_constraint=None,\n           sigma_constraint=None,\n           bias_constraint=None,\n           trainable=True,\n           mu_learning_rate_factor=500, #: additional factor for gradients of mu1 and mu2 \n           unit_testing=False, #: for competability between CPU and GPU version (where gradients of last edge need to be ignored) during unit testing\n           name=None)\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "CIFAR-10 example is available [here](https://github.com/skokec/DAU-ConvNet-cifar10-example).\n\nExample of three DAU convolutional layer and one fully connected using batch norm and L2 regularization on weights:\n\n```python\nimport tensorflow as tf\n\nfrom tensorflow.contrib.framework import arg_scope\n\nfrom dau_conv import dau_conv2d\n\nwith arg_scope([dau_conv2d, tf.contrib.layers.fully_connected],\n                weights_regularizer=tf.contrib.layers.l2_regularizer(0.0005),\n                weights_initializer=tf.contrib.layers.xavier_initializer(uniform=False),\n                biases_initializer=None,\n                normalizer_fn=tf.layers.batch_normalization,\n                normalizer_params=dict(center=True,\n                                       scale=True,\n                                       momentum=0.9999, \n                                       epsilon=0.001, \n                                       axis=1, #: NOTE: use axis=1 for NCHW format !!\n                                       training=in_training)):\n            \n            inputs = ...\n            \n            #: convert from NHWC to NCHW format\n            inputs = tf.transpose(inputs, [0,3,1,2])\n            \n            net = dau_conv2d(inputs, 96, dau_units=(2,2), max_kernel_size=9,\n                                    mu_learning_rate_factor=500, data_format='NCHW', scope='dau_conv1')\n            net = tf.contrib.layers.max_pool2d(net, [2, 2], scope='pool1', data_format=\"NCHW\")\n\n            net = dau_conv2d(net, 96, dau_units=(2,2), max_kernel_size=9,\n                                    mu_learning_rate_factor=500, data_format='NCHW', scope='dau_conv2')\n            net = tf.contrib.layers(net, [2, 2], scope='pool2', data_format=\"NCHW\")\n\n            net = dau_conv2d(net, 192, dau_units=(2,2), max_kernel_size=9,\n                                    mu_learning_rate_factor=500, data_format='NCHW', scope='dau_conv3')\n            net = tf.contrib.layers.max_pool2d(net, [2, 2], scope='pool3', data_format=\"NCHW\")\n            net = tf.reshape(net, [net.shape[0], -1])\n\n            net = tf.contrib.layers.fully_connected(net, NUM_CLASSES, scope='fc4',\n                                                    activation_fn=None,\n                                                    normalizer_fn=None,\n                                                    biases_initializer=tf.constant_initializer(0))\n\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}