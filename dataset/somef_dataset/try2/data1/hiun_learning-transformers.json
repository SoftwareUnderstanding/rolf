{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1409.0473",
      "https://arxiv.org/abs/1706.03762",
      "https://arxiv.org/abs/1706.03762"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Preprocessing, Train-Eval loop (except model): https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n\n- Transformer model: https://github.com/huggingface/transformers/blob/a7d46a060930242cd1de7ead8821f6eeebb0cd06/src/transformers/models/bert/modeling_bert.py\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/hiun/learning-transformers",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-10-31T09:23:01Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-12-03T05:54:55Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "What is a model? What is their role? I think a good model needs to have a good [Inductive Bias](https://en.wikipedia.org/wiki/Inductive_bias), which means it has good generalization capability to unseen example during training.\n\nThe difference between the Neural Network method of learning and other learning paradigm is that the Neural Network method learns from data by making a good representation of that data. On the contrary, many other methods learn by features that are manually selected by humans.\n\nThe Transformer model is one of the most popular representation generators of Neural Network methods of learning. Because of its general representation processing mechanism such as Attention-based learning, many recent advancements of deep learning rely on it.\n\nSo what actually Transformers do? What modules comprise Transformers? What are their implications? This is a natural question of mine as a beginner.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9880568659310551,
        0.9669856839507182,
        0.8501286207356216
      ],
      "excerpt": "This repository contains a fork of the Transformer model from https://github.com/huggingface/transformers. Unlike the original source code, which is a library, this repository is runnable stand-alone code for language modeling tasks. \nThe source code on such open source libraries are great, however, sometimes it is difficult to read all code for simply running and experimenting with a model. For example, preprocessing data, define a train-eval loop, integrating a model into that loop, these tasks are essential to write machine learning programs but it not always easy to looking source code from a large open source project. \nThis repository combines open source model code and tutorial level preprocessing, and train-eval code. As a result, we expect readers can easily understand how Transformer models are implemented and work from scratch. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9852753067692508
      ],
      "excerpt": "Because the Transformer model is a special case of the deep learning model. Let's look at what the Deep Learning model generally does. The Deep Learning model can be viewed as Directed Acyclic Graph (DAG; a graph structure that has no connection to ancestry node). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9874440817432903
      ],
      "excerpt": "Deep learning model process matrix, which roles a computable representation of input (such as text, image, etc.). The computation of the Deep Learning model to retrieve more abstract representation of input representation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9920438447560251
      ],
      "excerpt": "As example shows in imagenet, the earlier part of DAG (a matrix transformation 'layer' of Deep Learning model) do generate low-level features (e.g. shape of stroke that consists cars) from input data. While the latter part of DAG does transform them into high-level features (e.g. wheels of a car or this is Ford Model T). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9968347308404996,
        0.9567349784956787,
        0.8650975930218217
      ],
      "excerpt": "We can observe the same for natural language processing. The above example shows a step in making a representation of an input sentence for a Machine Translation task using the Transformer Model. The Encoder Layer 2 shows attention in the earlier part of DAG. It visualizes what to Attend for given sentences in given sentences. The diagonal line shows most of the words are self-attended since it is natural that low-level features of the text are in word-level. However, in the latter part of DAG, we show the trend that attention is concentrated on specific words, which refers to an abstract representation of given sentences that are crucial for a given task (such as word 'bad' is important for movie review sentiment analysis) \nYou may wonder about rules for shaping abstract representation, that is the role of the learning algorithm. \nFor example, Neural Network is a Learning algorithm that depends on its task setup. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9789149972505912,
        0.9852457759715763,
        0.9823029071984073,
        0.9785610271217312,
        0.9918917295058509,
        0.9924053675370472,
        0.9517469305325686,
        0.9743241664790943
      ],
      "excerpt": "In Supervised Deep Learning cases, shaping abstract representation is the role of optimization methods (e.g. backpropagation with learning rate scheduling) that utilize label data. \nMore specifically in the training model, when the model outputs inference result, that is compared against the label, and the difference is backpropagated through the model, this enforces (thus trains) model parameters to generate more task-oriented representation. (Note that the task can be multiple, in the case of multi-task learning. Or task can be much more general, in the case of few-shot learning.) \nPreviously, we revisit the single significant property of the machine learning model that good models have good inductive bias performance to achieve good generalization performance. \nDeep learning is Neural Network methods of learning, focusing on learning thus able to deriving task-oriented representation from data. \nThe Transformer is one of the popular methods of Deep learning that is good at learning textual or visual representation from data. \nThe Deep Learning model can be viewed as matrix processing DAG; where input data is matrix and DAG is gradually learning representation of data from its low to high-level features. \nHigh-level features are determined by a learning algorithm, especially backpropagation with label data.  \nI think all of the above parts have meaning to build good machine learning algorithms and systems. Let's look at how the above concepts reflect in the Deep Learning model (the DAG) of Transformers Encoder. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8228249535411783,
        0.9685553426733412,
        0.9941351388744545
      ],
      "excerpt": "We only look at the Transformer encoder, because the encoder and decoder share core functionality DAGs. \nThe role of the input embedding layer is to create a meaningful (task-oriented) representation of sentences - embeddings - from natural language. In this example, we receive word ids and create a 768 dimension vector. \nThe 768 dimension vector - the embeddings is a low-level representation of sentences, it is shaped by a mixture of the literal representation of text w and weight defined by backpropagation for objectives of the task. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8945291405141995
      ],
      "excerpt": "To add regularization for embedding, it applies layer normalization and Dropout to reduce training time and improve generalization respectively. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8021387300873148
      ],
      "excerpt": "The BertEncoder class stacks multiple self-attention layers of a transformer to implement this. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9723939873417417
      ],
      "excerpt": "The single transformer model is comprised of the attention part, intermediate processing part, and the output processing part. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8019231240830966,
        0.8805039477274169
      ],
      "excerpt": "The second module BertIntermediate layer is for increasing model capacity, by adding one sparser layer (hidden_size 768 dim to intermediate_size 3072 dim). \nThe last layer BertOutput is for bottleneck layer (intermediate_size 3072 dim to hidden_size 768 dim again) and do additional regularization. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8848397047906723
      ],
      "excerpt": "I think the concept of attention no more than just matrix transformation using representations from relevant information sources. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9311451302731019,
        0.9642872359889664,
        0.9706928664298603,
        0.9233246799906213,
        0.9613184211405543,
        0.814158279350032,
        0.8383394012760441,
        0.9107270656474334,
        0.8455015902463261,
        0.9141267864799509,
        0.9481791735638976
      ],
      "excerpt": "In this case, the sequential encoder (RNN) generates a matrix in a bidirectional context, and their output probabilities are concatenated and applied to a given step in the decoding process for making relevant representation for a given task. \nInformally, attention is just a probability-contained matrix to scale representation to be more appropriate for the task. \nAttention is a signal for adjustment of representation, thus it reduces the model size to achieve the same quality performance. \nIn terms of viewing attention by scale representation, what is a good information source for such scaling? \nThe idea of self-attention is finding the importance of each word and score them, and apply them to scale representation for a given task. \nThe mechanism of finding scores are multiply query and key matrix. \nMore specifically it obtains attention score matrix by element-wise multiplication result between single query word and all key words. \nFinally, the value vector is scaled by the respected attention score of its word. \nA higher score implies that given words have a strong relation to words on those sentences, the training procedures reveal a neural network to define a relationship by adjusting weights which are guided by loss function through backpropagation. \nAttention score totally depends on representation, which is the result of linear transformations from the embedding.  \nIn the training setting, weights of such embedding and linear transformations will be adjusted for generating maximum attention score helpful to resolve the given task. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8233069428993058
      ],
      "excerpt": "Value is an adjusted representation for specific tasks. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8113217028887423
      ],
      "excerpt": "Above image shows encoder self-attention (input-input) for machine translation. The image illustrates words related to it_. At layer 1, the word has a relationship to most words however, as layer getting compute abstract representation, it shows a strong relationship to a noun like animal_ or street_. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9896913120249405
      ],
      "excerpt": "To allows the model to attend different subspaces of representation without averaging it (ref and original paper). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9394628904396393
      ],
      "excerpt": "In summary, self-attention is just matrix multiplication that essentially does adjust the importance of each word in a sentence to make a better representation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8510066725272487,
        0.9852095753260738,
        0.918078841200398,
        0.9547586328085611,
        0.9676727151732742,
        0.8343792897532816,
        0.8787187053902653,
        0.9831526498091597,
        0.9603548725017854,
        0.8377056069029736,
        0.8805349742227924
      ],
      "excerpt": "The BertSelfOutput module provides additional linear transformation with layer normalization and Dropout. \nI think the role of this component is to provide additional model capacity and add Regularization. \nWe use our model as language modeling. \nLanguage modeling is an NLP task that predicts the next word from a given sequence of words. \nThe Transformer approach to language modeling is text classification, where calculate conditional probabilities of the next word in given previous words. \nBertModel creates embedding and passes it to the encoder. \n The encoder provides a representation of a sentence, \n and also the whole sentence (accumulated hidden states from all layers) is passed to the pooler for the whole sentence by making representation for whole sentences. \nBertForLM takes the last hidden state (since language modeling is a classification for the next word) and apply Dropout and add passes to the linear layer to get a conditional distribution result. \nIt calculates loss after inference is done. \nFirst of all, it splits data with N number of the batch for each train, eval loop. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9492140310085369
      ],
      "excerpt": "Each row in the tensor contains a batch size number of words (20 in our case). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.908925214220865,
        0.8979411005071259,
        0.9315023787811952
      ],
      "excerpt": "Define criterion, optimizer, and scheduler \nFetch data \nPass to model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259,
        0.9004327723016802
      ],
      "excerpt": "Fetch data \nPass to model (deterministic process) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8755369826173677
      ],
      "excerpt": "A common configuration for constructing the bert model. Options include, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8511040822666713
      ],
      "excerpt": "num_hidden_layers for number of transformer layer to transforms hidden representation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8246374073070617,
        0.810121538924549
      ],
      "excerpt": "max_position_embeddings max size of positional embedding legnth \ntype_vocab_size max size of type vocab length (...) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.978642784120802
      ],
      "excerpt": "Transformers: State-of-the-art Natural Language Processing for Pytorch and TensorFlow 2.0. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8136592746001741
      ],
      "excerpt": "Sequence-to-Sequence Modeling with nn.Transformer and TorchText \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8490037945672047
      ],
      "excerpt": "The Annotated Transformer \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.915021222620809
      ],
      "excerpt": "Transfer learning and Image classification using Keras on Kaggle kernels. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Transformers Tutorials with Open Source Implementations",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/hiun/learning-transformers/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 21 Dec 2021 07:45:21 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/hiun/learning-transformers/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "hiun/learning-transformers",
    "technique": "GitHub API"
  },
  "invocation": [
    {
      "confidence": [
        0.8994906389678163
      ],
      "excerpt": "<img src=\"./assets/dag.png\" width=\"400\" /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8860429271360217
      ],
      "excerpt": "<img src=\"./assets/imagenet.png\" width=\"500\" /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8687210699295087
      ],
      "excerpt": "<img src=\"./assets/self-attention.png\" width=\"700\" /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8890818307099057
      ],
      "excerpt": "<img src=\"./assets/seq2seq.png\" width=\"250\" /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8698163800029862
      ],
      "excerpt": "First of all, it splits data with N number of the batch for each train, eval loop. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9078564040380478
      ],
      "excerpt": "<img src=\"./assets/lm-data.png\" width=\"750\" /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8332944030054653
      ],
      "excerpt": "Set model to training mode \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/hiun/learning-transformers/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "learning-transformers",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "learning-transformers",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "hiun",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/hiun/learning-transformers/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Tue, 21 Dec 2021 07:45:21 GMT"
    },
    "technique": "GitHub API"
  },
  "support": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- `train_eval.log` shows numbers. Since train data is small, the model is close to underfitting.\n- In larger transformer layer, more underfitting occur, thus loss function gets higher\n\n- 1layer transformer encoder for LM task\n![train-valid-graph](./assets/train_val_graph.png)\n\n- 12layer transformer encoder for LM task (underfitting due to large model capacity)\n![train-valid-graph_12layer](./assets/train_val_graph_12layer.png)\n\n\n\n",
      "technique": "Header extraction"
    }
  ]
}