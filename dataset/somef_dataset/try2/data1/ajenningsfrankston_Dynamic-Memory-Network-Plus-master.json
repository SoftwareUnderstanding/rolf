{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1603.01417",
      "https://arxiv.org/abs/1607.06450",
      "https://arxiv.org/abs/1603.01417](https://arxiv.org/abs/1603.01417). \n\nThe original Dynamic Memory Network was introduced in \n\n[\"Ask Me Anything: Dynamic Memory Networks for Natural Language Processing\" \nby Ankit Kumar, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong,Romain Paulus, Richard Socher, https://arxiv.org/abs/1506.07285](https://arxiv.org/pdf/1506.07285.pdf) \n\n(I had to refer to this paper too).\n\nThis DMN+ Model uses:\n\n* Word vectors in facts are positionally encoded, and added to create sentence representations.\n* Bi-directional GRU is used over the sentence representations in the funsion layer. The forward and backward list of hidden states are added.\n* Attention Based GRU is used in the episodic memory module.\n* A linear layer with ReLu activation is used along with untied weights to update the memory for the next pass. \n\nI also included layer normalization ([Layer Normalization - Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton](https://arxiv.org/abs/1607.06450)) before every activation, barring the pre-activation state of the final layer. \n\nI used pre-trained GloVe embedding downloaded from [here](https://nlp.stanford.edu/projects/glove/).\nI used the 100 dimensional embeddings. \n\nI trained the model on basic induction tasks from [bAbi-tasks dataset](https://research.fb.com/downloads/babi/). \n\nHyperparameters are different from the original implementation.\n\n## Hyperparameters used:\n\n* Hidden size = 100\n* Embedding dimensions = 100\n* Learning rate = 0.001\n* Passes = 3\n* Mini Batch Size = 128\n* L2 Regularization = 0.0001\n* Dropout Rate = 0.1\n* Initialization = 0 for biases, Xavier for weights\n\n(last 10% of data samples used for validation.)\n\n## Result Discussion: \n\nI trained the model in a weakly supervised fashion. That is, the model won't be told which supporting facts are relevant for inductive reasoning in order to derive an answer. \n\nThe network starts to overfit around the 35th epoch. The validation cost starts to increase, while the training cost keeps on decreasing. \n\nThe published classification error of QA task 16 (basic induction) of bAbi Dataset of the DMN+ model (as given here: https://arxiv.org/pdf/1603.01417.pdf - page 7) is 45.3. \n\nFrom the paper:\n\n>One notable deficiency in our model is that of QA16: Basic\nInduction. In Sukhbaatar et al. (2015), an untied model\nusing only summation for memory updates was able to\nachieve a near perfect error rate of 0.4. When the memory\nupdate was replaced with a linear layer with ReLU activation,\nthe end-to-end memory network\u2019s overall mean error\ndecreased but the error for QA16 rose sharply. Our model\nexperiences the same difficulties, suggesting that the more\ncomplex memory update component may prevent convergence\non certain simpler tasks.\n\nMy implementation of the model on pretrained 100 dimensional GloVe vectors seems to produce about **51% classification accuracy**  on Test Data for induction tasks (check DMN+.ipynb)...i.e the **classification error is 49%**. . \n\nThe error is less than what the original DMN model acheived (error 55.1%) as specified in the paper, but still greater than the errors achieved achieved by the original implementation of the improved versions of DMN (DMN1, DMN2, DMN3, DMN+) in the paper.\n\nThis could be due to using different hyperparameters and embeddings, or I may have missed something in my implementations.\n\n## File Descriptions:\n\n**QA_PreProcess.py\\QA_PreProcess.ipynb:** Converts the raw induction tasks data set to separate ndarrays containing questions, answers, and facts with all words being in the form of GloVe pre-trained vector representations.  \n\n**DMN+.py\\DMN+.ipynb:** The DMN+ model, along with training, validation and testing. \n\n## Tested on:\n\n* Tensorflow 1.4 \n* Numpy 1.13.3\n* Python 2.7.12",
      "https://arxiv.org/abs/1506.07285](https://arxiv.org/pdf/1506.07285.pdf) \n\n(I had to refer to this paper too).\n\nThis DMN+ Model uses:\n\n* Word vectors in facts are positionally encoded, and added to create sentence representations.\n* Bi-directional GRU is used over the sentence representations in the funsion layer. The forward and backward list of hidden states are added.\n* Attention Based GRU is used in the episodic memory module.\n* A linear layer with ReLu activation is used along with untied weights to update the memory for the next pass. \n\nI also included layer normalization ([Layer Normalization - Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton](https://arxiv.org/abs/1607.06450)) before every activation, barring the pre-activation state of the final layer. \n\nI used pre-trained GloVe embedding downloaded from [here](https://nlp.stanford.edu/projects/glove/).\nI used the 100 dimensional embeddings. \n\nI trained the model on basic induction tasks from [bAbi-tasks dataset](https://research.fb.com/downloads/babi/). \n\nHyperparameters are different from the original implementation.\n\n## Hyperparameters used:\n\n* Hidden size = 100\n* Embedding dimensions = 100\n* Learning rate = 0.001\n* Passes = 3\n* Mini Batch Size = 128\n* L2 Regularization = 0.0001\n* Dropout Rate = 0.1\n* Initialization = 0 for biases, Xavier for weights\n\n(last 10% of data samples used for validation.)\n\n## Result Discussion: \n\nI trained the model in a weakly supervised fashion. That is, the model won't be told which supporting facts are relevant for inductive reasoning in order to derive an answer. \n\nThe network starts to overfit around the 35th epoch. The validation cost starts to increase, while the training cost keeps on decreasing. \n\nThe published classification error of QA task 16 (basic induction) of bAbi Dataset of the DMN+ model (as given here: https://arxiv.org/pdf/1603.01417.pdf - page 7) is 45.3. \n\nFrom the paper:\n\n>One notable deficiency in our model is that of QA16: Basic\nInduction. In Sukhbaatar et al. (2015), an untied model\nusing only summation for memory updates was able to\nachieve a near perfect error rate of 0.4. When the memory\nupdate was replaced with a linear layer with ReLU activation,\nthe end-to-end memory network\u2019s overall mean error\ndecreased but the error for QA16 rose sharply. Our model\nexperiences the same difficulties, suggesting that the more\ncomplex memory update component may prevent convergence\non certain simpler tasks.\n\nMy implementation of the model on pretrained 100 dimensional GloVe vectors seems to produce about **51% classification accuracy**  on Test Data for induction tasks (check DMN+.ipynb)...i.e the **classification error is 49%**. . \n\nThe error is less than what the original DMN model acheived (error 55.1%) as specified in the paper, but still greater than the errors achieved achieved by the original implementation of the improved versions of DMN (DMN1, DMN2, DMN3, DMN+) in the paper.\n\nThis could be due to using different hyperparameters and embeddings, or I may have missed something in my implementations.\n\n## File Descriptions:\n\n**QA_PreProcess.py\\QA_PreProcess.ipynb:** Converts the raw induction tasks data set to separate ndarrays containing questions, answers, and facts with all words being in the form of GloVe pre-trained vector representations.  \n\n**DMN+.py\\DMN+.ipynb:** The DMN+ model, along with training, validation and testing. \n\n## Tested on:\n\n* Tensorflow 1.4 \n* Numpy 1.13.3\n* Python 2.7.12"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9431169165013048,
        0.9471260241127721
      ],
      "excerpt": "\"Dynamic Memory Networks for Visual and Textual Question Answering\"  \nby Caiming Xiong, Stephen Merity, Richard Socher, arXiv:1603.01417.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.834861314340703,
        0.9641182796700074
      ],
      "excerpt": "\"Ask Me Anything: Dynamic Memory Networks for Natural Language Processing\"  \nby Ankit Kumar, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong,Romain Paulus, Richard Socher, arXiv:1506.07285  \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ajenningsfrankston/Dynamic-Memory-Network-Plus-master",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-01-20T04:41:48Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-09-14T02:31:09Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "**QA_PreProcess.py\\QA_PreProcess.ipynb:** Converts the raw induction tasks data set to separate ndarrays containing questions, answers, and facts with all words being in the form of GloVe pre-trained vector representations.  \n\n**DMN+.py\\DMN+.ipynb:** The DMN+ model, along with training, validation and testing. \n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9104469867078988,
        0.9908166796019624
      ],
      "excerpt": "Implementation of Dynamic Memory Network+ (for question answering) using Tensorflow. \nThe implementation is based on the model proposed in  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8057926079737359,
        0.8273925666990035,
        0.9708991409002485,
        0.9242289754930962,
        0.9635525012453628
      ],
      "excerpt": "This DMN+ Model uses: \nWord vectors in facts are positionally encoded, and added to create sentence representations. \nBi-directional GRU is used over the sentence representations in the funsion layer. The forward and backward list of hidden states are added. \nAttention Based GRU is used in the episodic memory module. \nA linear layer with ReLu activation is used along with untied weights to update the memory for the next pass.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8357825588392742
      ],
      "excerpt": "Hyperparameters are different from the original implementation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8250478003547322,
        0.8685150961252146,
        0.9559035210196488,
        0.9294489313723188,
        0.9060343142013316
      ],
      "excerpt": "Initialization = 0 for biases, Xavier for weights \n(last 10% of data samples used for validation.) \nI trained the model in a weakly supervised fashion. That is, the model won't be told which supporting facts are relevant for inductive reasoning in order to derive an answer.  \nThe network starts to overfit around the 35th epoch. The validation cost starts to increase, while the training cost keeps on decreasing.  \nThe published classification error of QA task 16 (basic induction) of bAbi Dataset of the DMN+ model (as given here: https://arxiv.org/pdf/1603.01417.pdf - page 7) is 45.3.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9891244244606126
      ],
      "excerpt": "One notable deficiency in our model is that of QA16: Basic \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9225091095903475,
        0.827257369561812,
        0.8380041727003206,
        0.834286604932392,
        0.9772135021557232,
        0.8857418226702356
      ],
      "excerpt": "using only summation for memory updates was able to \nachieve a near perfect error rate of 0.4. When the memory \nupdate was replaced with a linear layer with ReLU activation, \nthe end-to-end memory network\u2019s overall mean error \ndecreased but the error for QA16 rose sharply. Our model \nexperiences the same difficulties, suggesting that the more \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9546421468169192,
        0.9645340682276137,
        0.9137056237639856
      ],
      "excerpt": "My implementation of the model on pretrained 100 dimensional GloVe vectors seems to produce about 51% classification accuracy  on Test Data for induction tasks (check DMN+.ipynb)...i.e the classification error is 49%. .  \nThe error is less than what the original DMN model acheived (error 55.1%) as specified in the paper, but still greater than the errors achieved achieved by the original implementation of the improved versions of DMN (DMN1, DMN2, DMN3, DMN+) in the paper. \nThis could be due to using different hyperparameters and embeddings, or I may have missed something in my implementations. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ajenningsfrankston/Dynamic-Memory-Network-Plus-master/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 29 Dec 2021 06:35:21 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ajenningsfrankston/Dynamic-Memory-Network-Plus-master/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "ajenningsfrankston/Dynamic-Memory-Network-Plus-master",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/ajenningsfrankston/Dynamic-Memory-Network-Plus-master/master/clrp_pytorch_roberta_large_finetune.ipynb",
      "https://raw.githubusercontent.com/ajenningsfrankston/Dynamic-Memory-Network-Plus-master/master/QA_PreProcess.ipynb",
      "https://raw.githubusercontent.com/ajenningsfrankston/Dynamic-Memory-Network-Plus-master/master/lstm_by_keras_with_unified_wi_fi_feats.ipynb",
      "https://raw.githubusercontent.com/ajenningsfrankston/Dynamic-Memory-Network-Plus-master/master/download_kaggle_datasets.ipynb",
      "https://raw.githubusercontent.com/ajenningsfrankston/Dynamic-Memory-Network-Plus-master/master/Riiid_data.ipynb",
      "https://raw.githubusercontent.com/ajenningsfrankston/Dynamic-Memory-Network-Plus-master/master/DMN%2B.ipynb",
      "https://raw.githubusercontent.com/ajenningsfrankston/Dynamic-Memory-Network-Plus-master/master/clrp_pytorch_roberta_base_finetune.ipynb",
      "https://raw.githubusercontent.com/ajenningsfrankston/Dynamic-Memory-Network-Plus-master/master/clrp_pytorch_roberta_finetune.ipynb",
      "https://raw.githubusercontent.com/ajenningsfrankston/Dynamic-Memory-Network-Plus-master/master/lstm_by_keras_with_unified_wi_fi_feats_floor_model.ipynb",
      "https://raw.githubusercontent.com/ajenningsfrankston/Dynamic-Memory-Network-Plus-master/master/tree_regression.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8857217190166231
      ],
      "excerpt": "Numpy 1.13.3 \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.809634249280772
      ],
      "excerpt": "Hidden size = 100 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ajenningsfrankston/Dynamic-Memory-Network-Plus-master/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2017 Jishnu Ray Chowdhury\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Implementation of Dynamic Memory Network+",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Dynamic-Memory-Network-Plus-master",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "ajenningsfrankston",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ajenningsfrankston/Dynamic-Memory-Network-Plus-master/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 29 Dec 2021 06:35:21 GMT"
    },
    "technique": "GitHub API"
  }
}