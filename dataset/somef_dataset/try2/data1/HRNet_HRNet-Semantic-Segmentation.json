{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We adopt sync-bn implemented by [InplaceABN](https://github.com/mapillary/inplace_abn) for PyTorch 0.4.1 experiments and the official \nsync-bn provided by PyTorch for PyTorch 1.10 experiments.\n\nWe adopt data precosessing on the PASCAL-Context dataset, implemented by [PASCAL API](https://github.com/zhanghang1989/detail-api).\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2005.10821",
      "https://arxiv.org/abs/1904.04514"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[1] Deep High-Resolution Representation Learning for Visual Recognition. Jingdong Wang, Ke Sun, Tianheng Cheng, \n    Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, Wenyu Liu, Bin Xiao. Accepted by TPAMI.  [download](https://arxiv.org/pdf/1908.07919.pdf)\n    \n[2] Object-Contextual Representations for Semantic Segmentation. Yuhui Yuan, Xilin Chen, Jingdong Wang. [download](https://arxiv.org/pdf/1909.11065.pdf)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find this work or code is helpful in your research, please cite:\n````\n@inproceedings{SunXLW19,\n  title={Deep High-Resolution Representation Learning for Human Pose Estimation},\n  author={Ke Sun and Bin Xiao and Dong Liu and Jingdong Wang},\n  booktitle={CVPR},\n  year={2019}\n}\n\n@article{WangSCJDZLMTWLX19,\n  title={Deep High-Resolution Representation Learning for Visual Recognition},\n  author={Jingdong Wang and Ke Sun and Tianheng Cheng and \n          Borui Jiang and Chaorui Deng and Yang Zhao and Dong Liu and Yadong Mu and \n          Mingkui Tan and Xinggang Wang and Wenyu Liu and Bin Xiao},\n  journal={TPAMI},\n  year={2019}\n}\n\n@article{YuanCW19,\n  title={Object-Contextual Representations for Semantic Segmentation},\n  author={Yuhui Yuan and Xilin Chen and Jingdong Wang},\n  booktitle={ECCV},\n  year={2020}\n}\n````\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{YuanCW19,\n  title={Object-Contextual Representations for Semantic Segmentation},\n  author={Yuhui Yuan and Xilin Chen and Jingdong Wang},\n  booktitle={ECCV},\n  year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{WangSCJDZLMTWLX19,\n  title={Deep High-Resolution Representation Learning for Visual Recognition},\n  author={Jingdong Wang and Ke Sun and Tianheng Cheng and \n          Borui Jiang and Chaorui Deng and Yang Zhao and Dong Liu and Yadong Mu and \n          Mingkui Tan and Xinggang Wang and Wenyu Liu and Bin Xiao},\n  journal={TPAMI},\n  year={2019}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{SunXLW19,\n  title={Deep High-Resolution Representation Learning for Human Pose Estimation},\n  author={Ke Sun and Bin Xiao and Dong Liu and Jingdong Wang},\n  booktitle={CVPR},\n  year={2019}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9353051976633957
      ],
      "excerpt": "[2020/03/13] Our paper is accepted by TPAMI: Deep High-Resolution Representation Learning for Visual Recognition. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9405897239820948,
        0.8065621406933711
      ],
      "excerpt": "| HRNetV2-W48 | Yes | Yes | Yes | 37.9 | Github/BaiduYun(Access Code:92gw) | \n| HRNetV2-W48 + OCR | Yes | Yes | Yes | 40.6 | Github/BaiduYun(Access Code:sjc4) | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8065621406933711
      ],
      "excerpt": "| HRNetV2-W48 | Yes | Yes | Yes | 44.2 | Github/BaiduYun(Access Code:f6xf) | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9606712260438129
      ],
      "excerpt": "Human pose estimation \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/HRNet/HRNet-Semantic-Segmentation",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-04-09T13:24:09Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-28T07:13:57Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This is the official code of [high-resolution representations for Semantic Segmentation](https://arxiv.org/abs/1904.04514). \nWe augment the HRNet with a very simple segmentation head shown in the figure below. We aggregate the output representations at four different resolutions, and then use a 1x1 convolutions to fuse these representations. The output representations is fed into the classifier. We evaluate our methods on three datasets, Cityscapes, PASCAL-Context and LIP.\n\n<!-- ![](figures/seg-hrnet.png) -->\n<figure>\n  <text-align: center;>\n  <img src=\"./figures/seg-hrnet.png\" alt=\"hrnet\" title=\"\" width=\"900\" height=\"150\" />\n</figcaption>\n</figure>\n\nBesides, we further combine HRNet with [Object Contextual Representation](https://arxiv.org/pdf/1909.11065.pdf) and achieve higher performance on the three datasets. The code of HRNet+OCR is contained in this branch. We illustrate the overall framework of OCR in the Figure and the equivalent Transformer pipelines:\n\n<figure>\n  <text-align: center;>\n  <img src=\"./figures/OCR.PNG\" alt=\"OCR\" title=\"\" width=\"900\" height=\"200\" />\n</figure>\n  \n <figure>\n  <text-align: center;>\n  <img src=\"./figures/SegmentationTransformerOCR.png\" alt=\"Segmentation Transformer\" title=\"\" width=\"600\" />\n</figure>\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9861009865340731
      ],
      "excerpt": "This is the implementation for HRNet + OCR. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9443088759297738,
        0.97654315521664,
        0.9570636454608743
      ],
      "excerpt": "[2020/07/20] The researchers from AInnovation have achieved Rank#1 on ADE20K Leaderboard via training our HRNet + OCR with a semi-supervised learning scheme. More details are in their Technical Report. \n[2020/07/09] Our paper is accepted by ECCV 2020: Object-Contextual Representations for Semantic Segmentation. Notably, the reseachers from Nvidia set a new state-of-the-art performance on Cityscapes leaderboard: 85.4% via combining our HRNet + OCR with a new hierarchical mult-scale attention scheme.  \n[2020/03/13] Our paper is accepted by TPAMI: Deep High-Resolution Representation Learning for Visual Recognition. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8094797465151952,
        0.833604739806103,
        0.8398937432121247,
        0.9769433178798654,
        0.9425258090779068
      ],
      "excerpt": "Thanks Google and UIUC researchers. A modified HRNet combined with semantic and instance multi-scale context achieves SOTA panoptic segmentation result on the Mapillary Vista challenge. See the paper. \nSmall HRNet models for Cityscapes segmentation. Superior to MobileNetV2Plus .... \nRank #1 (83.7) in Cityscapes leaderboard. HRNet combined with an extension of object context \nPytorch-v1.1 and the official Sync-BN supported. We have reproduced the cityscapes results on the new codebase. Please check the pytorch-v1.1 branch. \nThe models are initialized by the weights pretrained on the ImageNet. ''Paddle'' means the results are based on PaddleCls pretrained HRNet models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.932121331119063
      ],
      "excerpt": "Performance on the Cityscapes dataset. The models are trained and tested with the input size of 512x1024 and 1024x2048 respectively. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9269902060819821
      ],
      "excerpt": "Performance on the LIP dataset. The models are trained and tested with the input size of 473x473. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.944573991573787,
        0.9152341790542763
      ],
      "excerpt": "Note Currently we could only reproduce HRNet+OCR results on LIP dataset with PyTorch 0.4.1. \nPerformance on the PASCAL-Context dataset. The models are trained and tested with the input size of 520x520. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8869535527273119
      ],
      "excerpt": "Performance on the COCO-Stuff dataset. The models are trained and tested with the input size of 520x520. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8799432973812512
      ],
      "excerpt": "Performance on the ADE20K dataset. The models are trained and tested with the input size of 520x520. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8804323604019457
      ],
      "excerpt": ": For PyTorch 0.4.1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8804323604019457
      ],
      "excerpt": ": For PyTorch 1.1.0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955127366052505
      ],
      "excerpt": "For example, train the HRNet-W48 on Cityscapes with a batch size of 12 on 4 GPUs: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955127366052505
      ],
      "excerpt": "For example, train the HRNet-W48 + OCR on Cityscapes with a batch size of 12 on 4 GPUs: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8129639886906787
      ],
      "excerpt": "For example, evaluating HRNet+OCR on the Cityscapes validation set with multi-scale and flip testing: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8097817881210213
      ],
      "excerpt": "Evaluating HRNet+OCR on the PASCAL-Context validation set with multi-scale and flip testing: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8776965361961042
      ],
      "excerpt": "Evaluating HRNet+OCR on the LIP validation set with flip testing: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "The OCR approach is rephrased as Segmentation Transformer: https://arxiv.org/abs/1909.11065. This is an official implementation of semantic segmentation for HRNet. https://arxiv.org/abs/1908.07919",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/HRNet/HRNet-Semantic-Segmentation/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 596,
      "date": "Wed, 29 Dec 2021 17:51:44 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/HRNet/HRNet-Semantic-Segmentation/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "HRNet/HRNet-Semantic-Segmentation",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/HRNet/HRNet-Semantic-Segmentation/HRNet-OCR/run_local.sh",
      "https://raw.githubusercontent.com/HRNet/HRNet-Semantic-Segmentation/HRNet-OCR/run_dist.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You need to download the [Cityscapes](https://www.cityscapes-dataset.com/), [LIP](http://sysu-hcp.net/lip/) and [PASCAL-Context](https://cs.stanford.edu/~roozbeh/pascal-context/) datasets.\n\nYour directory tree should be look like this:\n````bash\n$SEG_ROOT/data\n\u251c\u2500\u2500 cityscapes\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 gtFine\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 test\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 train\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 val\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 leftImg8bit\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 test\n\u2502\u00a0\u00a0  \u00a0\u00a0 \u251c\u2500\u2500 train\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 val\n\u251c\u2500\u2500 lip\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 TrainVal_images\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 train_images\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 val_images\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 TrainVal_parsing_annotations\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 train_segmentations\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 train_segmentations_reversed\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 val_segmentations\n\u251c\u2500\u2500 pascal_ctx\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 common\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 PythonAPI\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 res\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 VOCdevkit\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 VOC2010\n\u251c\u2500\u2500 cocostuff\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 train\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 image\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 label\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 val\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 image\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 label\n\u251c\u2500\u2500 ade20k\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 train\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 image\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 label\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 val\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 image\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 label\n\u251c\u2500\u2500 list\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 cityscapes\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 test.lst\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 trainval.lst\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 val.lst\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 lip\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 testvalList.txt\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 trainList.txt\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 valList.txt\n````\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "1. For LIP dataset, install PyTorch=0.4.1 following the [official instructions](https://pytorch.org/). For Cityscapes and PASCAL-Context, we use PyTorch=1.1.0.\n2. `git clone https://github.com/HRNet/HRNet-Semantic-Segmentation $SEG_ROOT`\n3. Install dependencies: pip install -r requirements.txt\n\nIf you want to train and evaluate our models on PASCAL-Context, you need to install [details](https://github.com/zhanghang1989/detail-api).\n````bash\npip install git+https://github.com/zhanghang1989/detail-api.git#:subdirectory=PythonAPI\n````\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9010102322852148,
        0.8490405243673376
      ],
      "excerpt": "The PyTroch 1.1 version ia available here. \nThe PyTroch 0.4.1 version is available here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8243339525769834
      ],
      "excerpt": "You can download the pretrained models from  https://github.com/HRNet/HRNet-Image-Classification. Slightly different, we use align_corners = True for upsampling in HRNet. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.921507801671738
      ],
      "excerpt": "<!-- **Note** We reproduce HRNet+OCR results on COCO-Stuff dataset with PyTorch 0.4.1. --> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.894063499024525
      ],
      "excerpt": "<!-- **Note** We reproduce HRNet+OCR results on ADE20K dataset with PyTorch 0.4.1. --> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.902526005410836,
        0.902526005410836
      ],
      "excerpt": ": For PyTorch 0.4.1 \n: For PyTorch 1.1.0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9565194688653897
      ],
      "excerpt": "Note that we only reproduce HRNet+OCR on LIP dataset using PyTorch 0.4.1. So we recommend to use PyTorch 0.4.1 if you want to train on LIP dataset. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9112577633140337
      ],
      "excerpt": "$PY_CMD tools/train.py --cfg experiments/cityscapes/seg_hrnet_ocr_w48_train_512x1024_sgd_lr1e-2_wd5e-4_bs_12_epoch484.yaml \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8615165938593338,
        0.8472576821943623,
        0.8059503422323497
      ],
      "excerpt": "python -m torch.distributed.launch --nproc_per_node=4 tools/train.py --cfg experiments/cityscapes/seg_hrnet_ocr_w48_train_512x1024_sgd_lr1e-2_wd5e-4_bs_12_epoch484.yaml \nJust specify the configuration file for tools/train.py. \nFor example, train the HRNet-W48 on Cityscapes with a batch size of 12 on 4 GPUs: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9112577633140337,
        0.8059503422323497
      ],
      "excerpt": "$PY_CMD tools/train.py --cfg experiments/cityscapes/seg_hrnet_w48_train_512x1024_sgd_lr1e-2_wd5e-4_bs_12_epoch484.yaml \nFor example, train the HRNet-W48 + OCR on Cityscapes with a batch size of 12 on 4 GPUs: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9112577633140337
      ],
      "excerpt": "$PY_CMD tools/train.py --cfg experiments/cityscapes/seg_hrnet_ocr_w48_train_512x1024_sgd_lr1e-2_wd5e-4_bs_12_epoch484.yaml \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.920407038937141,
        0.8863936634958686,
        0.8194641898831284,
        0.9206908753818739
      ],
      "excerpt": "python tools/test.py --cfg experiments/cityscapes/seg_hrnet_ocr_w48_train_512x1024_sgd_lr1e-2_wd5e-4_bs_12_epoch484.yaml \\ \n                     TEST.MODEL_FILE hrnet_ocr_cs_8162_torch11.pth \\ \n                     TEST.SCALE_LIST 0.5,0.75,1.0,1.25,1.5,1.75 \\ \n                     TEST.FLIP_TEST True \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.920407038937141
      ],
      "excerpt": "python tools/test.py --cfg experiments/cityscapes/seg_hrnet_ocr_w48_train_512x1024_sgd_lr1e-2_wd5e-4_bs_12_epoch484.yaml \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8863936634958686,
        0.8194641898831284,
        0.9206908753818739
      ],
      "excerpt": "                     TEST.MODEL_FILE hrnet_ocr_trainval_cs_8227_torch11.pth \\ \n                     TEST.SCALE_LIST 0.5,0.75,1.0,1.25,1.5,1.75 \\ \n                     TEST.FLIP_TEST True \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.920407038937141
      ],
      "excerpt": "python tools/test.py --cfg experiments/pascal_ctx/seg_hrnet_ocr_w48_cls59_520x520_sgd_lr1e-3_wd1e-4_bs_16_epoch200.yaml \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8863936634958686,
        0.8194641898831284,
        0.9206908753818739
      ],
      "excerpt": "                     TEST.MODEL_FILE hrnet_ocr_pascal_ctx_5618_torch11.pth \\ \n                     TEST.SCALE_LIST 0.5,0.75,1.0,1.25,1.5,1.75,2.0 \\ \n                     TEST.FLIP_TEST True \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.920407038937141
      ],
      "excerpt": "python tools/test.py --cfg experiments/lip/seg_hrnet_w48_473x473_sgd_lr7e-3_wd5e-4_bs_40_epoch150.yaml \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8863936634958686,
        0.9206908753818739,
        0.8633989807152664
      ],
      "excerpt": "                     TEST.MODEL_FILE hrnet_ocr_lip_5648_torch04.pth \\ \n                     TEST.FLIP_TEST True \\ \n                     TEST.NUM_SAMPLES 0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.920407038937141
      ],
      "excerpt": "python tools/test.py --cfg experiments/cocostuff/seg_hrnet_ocr_w48_520x520_ohem_sgd_lr1e-3_wd1e-4_bs_16_epoch110.yaml \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8863936634958686,
        0.8194641898831284,
        0.9206908753818739
      ],
      "excerpt": "                     TEST.MODEL_FILE hrnet_ocr_cocostuff_3965_torch04.pth \\ \n                     TEST.SCALE_LIST 0.5,0.75,1.0,1.25,1.5,1.75,2.0 \\ \n                     TEST.MULTI_SCALE True TEST.FLIP_TEST True \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8733507926560679
      ],
      "excerpt": "python tools/test.py --cfg experiments/ade20k/seg_hrnet_ocr_w48_520x520_ohem_sgd_lr2e-2_wd1e-4_bs_16_epoch120.yaml \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8863936634958686,
        0.8194641898831284,
        0.9206908753818739
      ],
      "excerpt": "                     TEST.MODEL_FILE hrnet_ocr_ade20k_4451_torch04.pth \\ \n                     TEST.SCALE_LIST 0.5,0.75,1.0,1.25,1.5,1.75,2.0 \\ \n                     TEST.MULTI_SCALE True TEST.FLIP_TEST True \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/HRNet/HRNet-Semantic-Segmentation/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Cuda",
      "C++",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other",
      "url": "https://raw.githubusercontent.com/HRNet/HRNet-Semantic-Segmentation/HRNet-OCR/LICENSE"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'    \\nBSD 3-Clause License\\n\\nCopyright (c) 2017, mapillary\\nAll rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met:\\n\\n Redistributions of source code must retain the above copyright notice, this\\n  list of conditions and the following disclaimer.\\n\\n Redistributions in binary form must reproduce the above copyright notice,\\n  this list of conditions and the following disclaimer in the documentation\\n  and/or other materials provided with the distribution.\\n\\n* Neither the name of the copyright holder nor the names of its\\n  contributors may be used to endorse or promote products derived from\\n  this software without specific prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "High-resolution networks and Segmentation Transformer for Semantic Segmentation",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "HRNet-Semantic-Segmentation",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "HRNet",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/HRNet/HRNet-Semantic-Segmentation/blob/HRNet-OCR/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2378,
      "date": "Wed, 29 Dec 2021 17:51:44 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "segmentation",
      "semantic-segmentation",
      "cityscapes",
      "pascal-context",
      "lip",
      "high-resolution",
      "high-resolution-net",
      "hrnets",
      "transformer",
      "segmentation-transformer"
    ],
    "technique": "GitHub API"
  }
}