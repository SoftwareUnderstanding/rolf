{
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/realdingke/mscai_softwareengineering_project",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-03-22T15:54:22Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-04-30T12:04:00Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9684181317212396,
        0.9659161250066591,
        0.8505921302792153,
        0.8505921302792153,
        0.9439244666736938,
        0.846168012858638,
        0.8912269402896411
      ],
      "excerpt": "This is the Imperial College Msc AI group project on multi-object tracking, originating from a proposal made by Imperial's industrial partner Cord AI which also grants access to the object tracking datasets stored on its own Cord platform.  \nThe project essentially rests upon ifzhang's fundamental approach of FairMOT (Github homepage is here; paper: FairMOT: On the Fairness of Detection and Re-Identification in Multiple Object Tracking), linking up with the Cord's database and constructing an highly automated pipeline that can enable user to either train new models from scratch or track with existing models (from after the training scheme or pre-trained model downloadable from public URL). It is also worth noting the training-tracking pipeline possesses full multi-class capabilities (multi-class training & evaluation), building onto the powerful MCMOT modifications by CaptainEven whose homepage is: MCMOT. \nHighlights on traffic datasets are below: \nHighlights on visdrone datasets are below: \nThe entire pipeline is containerised to remove system and environment constraints. As such, we have prepared the dockerfile which includes all the dependencies and packages that are required by the pipeline. \nThe entry point is located inside the /src folder which also corresponds to the default root directory of the program. \nThe first step always is to run the entry point file with --gen_info flag to see the important dataset information, facilitating user's decision to run desired pipeline branch with appropriate data and model: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.859185038227048,
        0.9914903111849179
      ],
      "excerpt": "Behind --project and --api flags you need to enter the project id and api key distributed by Cord. These information must be manually entered because for safety reasons there are no default values for these variables. After this on the command line window you should see key information related to the dataset that is being specified by the projectid-apikey pair, i.e. how many labelled video & unlabelled video does this particular dataset have. \nShould the user choose to go through the full length of our pipeline, that is to train a new model and track and evaluate with that model. Under /src (root dir) run: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8858306675176234
      ],
      "excerpt": "Using the --train_track flag will move onto the pipeline branch where a new model is trained with architecture, learning rate, epoch number, batch size etc. of user's choosing. Again you still need to manually enter the project id and api key to specify the dataset. Note by default this will train on all video with labels, should the user want to train only on certain video use --ds flag to specify the video names(this flag has action \"append\" meaning it can take multiple arguments in a row). If the user intends to split the datasets into training and testing datasets, use --split_perc flag to specificy each video's train split ratio (The default setting of the train split ratio for all videos is 0.8 which is designed for the model selection part, if the user doesn't want to split, --split_perc should be 1 for all the videos). --rand_split flag would specify whether the training sequences would be randomly chosen. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8040264125085794
      ],
      "excerpt": "The --exp_id will specify the savename of the model after it is trained; --arch will specify the architecture of the model; --batch_size will specify the batch size parameter for model training dataloader; --num_epochs will specify the number of epochs to be used; --lr will specify the learning rate of the optimizer; --lr_step specify after how many steps would the learning rate be decayed. All these training parameters/hyperparameters have their default optimal values, so you can choose not to bother specifying them.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8159681382765276
      ],
      "excerpt": "After the trained model is saved, the user could track the testing dataset and generate the statistical evaluation if the dataset has been split. The user could select a better model based on the statistical evaluation.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8043733780579668
      ],
      "excerpt": "Should the user wants to directly track using a pretrained model or previously-trained model, similar to above, under /src (root dir) first run: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9623448258421887,
        0.937870136067755
      ],
      "excerpt": "Check the below the flow of the API to have a general idea of how to use it. \nThe main page you should arrive at is shown below, type in the Project ID and API key provided by Cord and click generate project information, you will be redirected to a page conatining vital project dataset info i.e. videos with labels or without labels, helping you to a decision which video to be used for training or direct tracking. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "The Imperial College Msc AI group project on multi-object tracking",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/realdingke/mscai_softwareengineering_project/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Mon, 27 Dec 2021 06:11:18 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/realdingke/mscai_softwareengineering_project/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "realdingke/mscai_softwareengineering_project",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/realdingke/mscai_softwareengineering_project/main/Dockerfile",
      "https://raw.githubusercontent.com/realdingke/mscai_softwareengineering_project/main/flask/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/realdingke/mscai_softwareengineering_project/main/DCNv2/make.sh",
      "https://raw.githubusercontent.com/realdingke/mscai_softwareengineering_project/main/src/start.sh",
      "https://raw.githubusercontent.com/realdingke/mscai_softwareengineering_project/main/flask/start.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Alternatively, you can choose to set up the pipeline on your local machine. First git clone this repository to your local folder and cd to the main:\r\n\r\n    cd mscai_softwareengineering_project\r\n\r\nAssuming your local machine has met the GPU/cuda requirements, build the docker image by running:\r\n\r\n    docker build -t [image_name]\r\n\r\nNow create a container by running(select the image that you have just built):\r\n\r\n    docker run \u2013it \u2013-name [container_name] \u2013d [image_name] /bin/bash\r\n\r\nThen entering the container will automatically bring you to the program's root:\r\n\r\n    docker exec -ti -u root [container_name] /bin/bash\r\n\r\n\r\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "It is our recommendation that the pipeline is to be run using Google Cloud's computing service. First create a Virtual Machine(VM) instance on Google cloud: check the GPU option and select the operating system to be `Ubuntu 16.04`. Then ssh connect to the VM and install the necessary Nvidia driver (recommended version==418.126.02), Docker as well as Nividia docker for the VM. To check if everything is installed correctly, run the Nividia-smi application:\r\n\r\n    docker run --runtime=nvidia --rm nvidia/cuda:9.0-base nvidia-smi\r\n\r\nOnly with a ready gpu & the driver can the pipeline be properly run on the Google VM. If docker permission error pops up, run the following line:\r\n\r\n    sudo chmod a+rw /var/run/docker.sock\r\n\r\nNow git clone this repository to the VM folder, and cd to the repository main:\r\n\r\n    cd mscai_softwareengineering_project\r\n\r\nThen build the docker image:\r\n\r\n    sudo docker build -t [image_name]\r\n\r\n This will build a docker image with the name `image_name`\r\n\r\n    sudo nvidia-docker run  --ipc=host \u2013it \u2013-name [container_name] \u2013d [image_name] /bin/bash\r\n\r\nRunning the above command with the `--name` flag will create a container named `container_name`; the `-d` flag will keep the container running at the background. Now you can enter the newly built & up-and-running container with:\r\n\r\n    sudo docker exec -ti -u root [container_name] /bin/bash\r\n\r\nAs we have specified the program's root to be under `/src` in the dockerfile, the above command will bring you directly to the root directory. Now you can essentially run the pipeline unimpeded as if you are on local machine.\r\n\r\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8441242091103431
      ],
      "excerpt": "The entire pipeline is containerised to remove system and environment constraints. As such, we have prepared the dockerfile which includes all the dependencies and packages that are required by the pipeline. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9556319658802956,
        0.9133674089662569
      ],
      "excerpt": "Then start training by running the train.py under /src, an example would be: \n python3 train.py --exp_id test_model_1 --arch 'dla_34' --batch_size 3 --num_epochs 10 --lr 1e-5 --lr_step 50 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8266596975565715
      ],
      "excerpt": "After the trained model is saved, should the user want to track video with this model, run the entry_point.py again but with the --track flag, one example would be: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8807106811421043
      ],
      "excerpt": "Start tracking by running the mctrack.py under /src, an example would be: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8324774629003473,
        0.8977475287978481
      ],
      "excerpt": "The --exp_name will specify the savename of the final evaluation result and the output videos; --load_model will specify the absolute path of the model to be used for tracking. --output_format will specify the output format, the default format is .txt file. \nShould the user wants to directly track using a pretrained model or previously-trained model, similar to above, under /src (root dir) first run: \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/realdingke/mscai_softwareengineering_project/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "C++",
      "Cuda",
      "HTML",
      "Dockerfile",
      "C",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'BSD 3-Clause License\\n\\nCopyright (c) 2019, Charles Shang\\nAll rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met:\\n\\n1. Redistributions of source code must retain the above copyright notice, this\\n   list of conditions and the following disclaimer.\\n\\n2. Redistributions in binary form must reproduce the above copyright notice,\\n   this list of conditions and the following disclaimer in the documentation\\n   and/or other materials provided with the distribution.\\n\\n3. Neither the name of the copyright holder nor the names of its\\n   contributors may be used to endorse or promote products derived from\\n   this software without specific prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "MscAI Software Engineering Group Project (Group 6)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "mscai_softwareengineering_project",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "realdingke",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/realdingke/mscai_softwareengineering_project/blob/main/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The entire pipeline is designed to be able to run via two approaches: traditional CLI approach or the more user-friendly API.\r\n- - - -\r\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\n- [x] CLI approach\r\n- [ ] API approach\r\n\r\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\n- [ ] CLI approach\r\n- [x] API approach\r\n\r\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "First cd to the root folder, then you can either open up a cmd window and run the below command to enter the API's main page endpoint:\r\n    \r\n    ./start.sh\r\n\r\nOr you can simply double click on the start.sh shell file and run it. You will still then end up in the main page.\r\n\r\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Mon, 27 Dec 2021 06:11:18 GMT"
    },
    "technique": "GitHub API"
  }
}