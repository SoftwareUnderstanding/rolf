{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1610.02357"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[1]: Chollet, F. (2017). Xception: Deep learning with depthwise separable convolutions. \nArXiv:1610.02357 [Cs]. http://arxiv.org/abs/1610.02357\n\n[2]: Simonyan, K., and Zisserman, A. (2015). Very deep convolutional networks for large-scale image recognition. ArXiv:1409.1556 [Cs]. http://arxiv.org/abs/1409.1556\n\n[3]: He, K., Zhang, X., Ren, S., & Sun, J. (2015). Deep residual learning for image recognition. ArXiv:1512.03385 [Cs]. http://arxiv.org/abs/1512.03385",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9671795466030755
      ],
      "excerpt": "This repository contains the source code for neural networks used in facial detection, emotion recognition, \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/amogh7joshi/engagement-detection",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-11-20T18:20:36Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-20T14:15:14Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8718088390902665,
        0.9405884831812512
      ],
      "excerpt": "This repository contains the source code for neural networks used in facial detection, emotion recognition, \nand the overarching framework of engagement detection.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9826302097757169
      ],
      "excerpt": "on the top of the file contain more information on usage of the different detectors. The facedetect.py also contains  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9232174343720876
      ],
      "excerpt": "The repository also contains multiple convolutional neural networks for facial emotion recognition. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8459434979123084
      ],
      "excerpt": "For information on the neural network models being used, see the Neural Network Information section below. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8571011211353248
      ],
      "excerpt": "there on downloading any necessary files and the location of saved files. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9679388299957162
      ],
      "excerpt": "in the data subdirectory. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8906771810467848,
        0.8472539558139442,
        0.8818297867868065
      ],
      "excerpt": "The model architecture I am currently using (architecture is the model on the upper right) for the emotion recognition convolutional neural network uses a very basic inception architecture, \nwith each block containing a triple convolution and max pooling branch and an average pooling branch.  \nPreviously I had used a model (architecture is on the upper left) roughly developed as a miniature version of the  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8468059117412745,
        0.9413102757284649
      ],
      "excerpt": "the second with only convolution, and a third with one convolution and average pooling.  \nInitially, I had chosen to use one similar to the likes of VGG16 and VGG19  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9377104650233611,
        0.8711982527812968
      ],
      "excerpt": "although regular convolution layers seem to yield better results with the image sizes of the fer2013 dataset. \nThe deep neural network for face detection makes use of a pre-trained model using the  ResNet architecture  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9146076475181418
      ],
      "excerpt": "The directories in this repository are integrated for a seamless transition sequence. All necessary data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Engagement Detection, including facial detection and emotion recognition, using CNNs/LSTMs.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/amogh7joshi/fer/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 4,
      "date": "Tue, 21 Dec 2021 20:19:13 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/amogh7joshi/engagement-detection/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "amogh7joshi/engagement-detection",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/amogh7joshi/fer/master/scripts/bestmodel.sh",
      "https://raw.githubusercontent.com/amogh7joshi/fer/master/scripts/fixcv.sh",
      "https://raw.githubusercontent.com/amogh7joshi/fer/master/scripts/preprocess.sh",
      "https://raw.githubusercontent.com/amogh7joshi/fer/master/scripts/getdata.sh",
      "https://raw.githubusercontent.com/amogh7joshi/fer/master/scripts/editconstant.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "For setup, a Makefile is provided:\n\n```shell script\nmake install\n```\n\nOr, you can manually run:\n\n```shell script\n#: Install System Requirements\npython3 -m pip install -r requirements.txt\n```\n\nIn either case, you should delete the `.cloud` directory. Either the Makefile will do it for you or \nyou can manually delete it. It contains operations that I use personally when working with the Google API,\nso unless you are working with any of the same Google APIs, you should delete it.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "To use the repository, it can be directly cloned from the command line:\n\n```shell script\ngit clone --recurse-submodules https://github.com/amogh7joshi/engagement-detection.git\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8504183455002915,
        0.8080962538080847
      ],
      "excerpt": "Then, use the scripts provided in the scripts directory to install the necessary data: \n1. To install the model and caffemodel files for the DNN, use the getdata.sh script.  \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8648833057223703,
        0.9213478496187444
      ],
      "excerpt": "They are still in progress, but the general usage is as follows: Train the model from the trainmodel.py file, \nand test the model using the testmodel.py file.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8509816472853311
      ],
      "excerpt": "2. Download the fer2013.csv file from here, follow the directions in the data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8900486270063179
      ],
      "excerpt": "from models.model_factory import * \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/amogh7joshi/engagement-detection/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell",
      "Makefile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 Amogh Joshi\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Engagement Detection",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "engagement-detection",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "amogh7joshi",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/amogh7joshi/engagement-detection/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 8,
      "date": "Tue, 21 Dec 2021 20:19:13 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "cnn",
      "tensorflow",
      "keras",
      "facial-detection",
      "fer2013",
      "emotion-recognition",
      "lstm",
      "neural-networks",
      "engagement-prediction",
      "computer-vision"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Once the datasets are preprocessed, they can be called through the following functions:\n\n```python\nfrom data.load_data import get_fer2013_data\nfrom data.load_data import get_ckplus_data\n\n#: Load the training, validation, and testing data (repeat with other datasets).\nX_train, X_validation, X_test, y_train, y_validation, y_test = get_fer2013_data()\n```\n\nFor more information, visit the `data` subdirectory.\n\nThe other tools in the Makefile are for convenience purposes only when committing to this repository, \nin addition to the `editconstant.sh` script. Do not use them unless you are committing to your own repository.\n\nThe `info.json` file contains the relevant locations of the cascade classifiers and DNN model files.\nYou can replace the current locations with those on your computer, and then load the detectors as follows.\n\n```python\nfrom util.info import load_info\n\n#: Set the `eyes` option to true if you want to load the eye cascade.\ncascade_face, cascade_eyes, net = load_info(eyes = True)\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "![GitHub Workflow Status](https://img.shields.io/github/workflow/status/amogh7joshi/chemsolve/CodeQL)\n\nCurrently, all models have been configured to work with the `fer2013` and `ck+` datasets.\n\n**Model Training**: Run the `trainmodel.py` script. You can edit the number of epochs in the argparse argument\nat the top of the file. Alternatively, you can run itt from the command line using the flags as mentioned by the \nargparse arguments. Model weights will be saved to the `data/model` directory, and at the completion of the training,\nthe best model will be moved into the `data/savedmodels` directory. The json file containing the model\narchitecture will also be saved there. You can control what models to keep in the `data/savedmodels` directory manually.\n\n**Model Testing**: Run the `testmodel.py` script. You can edit which model weights and architecture you want to use at the \nlocation at the top of the file. From there, you can run `model.evaluate` on the pre-loaded training and testing data, \nyou can run `model.predict` on any custom images you want to test, or run any other operations with the model. \nA confusion matrix is also present, which will display if `plt.show()` is uncommented.\n\n**Live Emotion Detection**: Run the `videoclassification.py` script. If you already have a trained model, set it at the top of the \nscript, and it will detect emotions live. For just facial detection, run the `facevideo.py` script. You can choose which detector you\nwant to use, as described at the top of the file. If you want to save images, set the `-s` flag to `True`, and they will save to a \ncustom directory `imageruntest` at the top-level. More information is included at the top of the file. \n\n**Image Emotion Detection**: Run the `emotionclassification.py` script. Choose the images you want to detect emotions on and place their paths in \nthe `userimages` variable. If running from the command line, then write out the paths to each of the images when running the script. Optionally, if you\njust want facial detection,  run the `facedetect.py` script. If running from the command line, then read the argument information at the top of the file. \nOtherwise, insert the paths of the images that you want to detect faces from into a list called `user_images` midway through the file. The changed images will save\nto a custom directory called `modded`, but you can change that from the `savedir` variable. For each image inputted, the script will output the same image\nwith a bounding box around the faces detected from the image.\n\n",
      "technique": "Header extraction"
    }
  ]
}