{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1801.01973\n<img src=\"./Report/suck.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>\n\n\n```python\n\n```"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9965271881743462
      ],
      "excerpt": "    See paper https://arxiv.org/pdf/1606.03498.pdf \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/BeyondCloud/Comp04_ReverseImageCaption",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-01-09T06:06:46Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-06-28T22:15:55Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "<img src=\"./Report/25_inf.png\">\n    (first row ~ last row)<br>\n    0633: this flower has petals that are yellow with red blotches <br>\n    0194: the flower has white stringy petals with yellow and purple pollen tubes<br>\n    2014: this flower is pink in color with only one large petal<br>\n    4683: this flower is yellow in color with petals that are rounded<br>\n    3327: the flower has a several pieces of yellow colored petals that looks similar to its leaves\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9706452206964447,
        0.9295327380911059
      ],
      "excerpt": "into CNNencoder, which generates encoded vector x and x_w. As for the textencoder , we feed correct captions and \nmismatched captions into encoder to generate v and v_w. Finally, the similarity of text and image can be calculated by: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9395943394031522,
        0.9474906284143406,
        0.8166054210725288
      ],
      "excerpt": "time when passing each of the last 4 nets. Plus, the network uses highway connection which adds two  \nfeatures( before passing CNN and after passing CNN) together. This could be regarded as \"adding some images \ndetail to the output of previous layer.\" Since the pixel value of the output image is 0~1, we use  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8137248592186919
      ],
      "excerpt": "    In scipy image the pixel value &lt; 0 will be converted to zero automatically. In our small-scale model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9064763840972904
      ],
      "excerpt": "information of the image. At the 4th layer the model concatenates the textvector and 3rd layer  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8209388860499645
      ],
      "excerpt": "Below are the equations used for calculating the generator/discriminator loss \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8817878782207937
      ],
      "excerpt": "The small model also use GAN-CLS but without high-way connection and RNN encoder \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.903643410039594
      ],
      "excerpt": ", where rnn_loss is the the same as the loss in GAN-CLS. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8890852702932611
      ],
      "excerpt": "   could easily tell the difference between real and fake image. Making this change could also prevent  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8963179554043318
      ],
      "excerpt": "Using normal distribution noise is more intuitive for generating natural images. However, we found that whether  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9624348911816215
      ],
      "excerpt": "In our experiment, we found that dropout is useful in the small-scale model. At the early stage of training.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8570289166062438
      ],
      "excerpt": "    dropout in several generator's layers. However, the large-scale model could generate wide variety of flower \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9963838231304252
      ],
      "excerpt": "    Plus, we found that the inception score of small model is more fluctuating than the large model for different \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8525378053427571,
        0.801345501559434
      ],
      "excerpt": "Sometimes the synthesised image might have similar color for each batch of input(see gif below).  \n    Whenever this happens, changing the batch_normalization layer to VBN could solve the problem. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8905958018418919
      ],
      "excerpt": "    Using the same setting for training the GAN-CLS leads the speed of convergence very slow. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8278678555290407
      ],
      "excerpt": "    Unfortunately, such modigication also made the training slightly unstable. Sometimes the generated image  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9606655917155837,
        0.8421085899975999
      ],
      "excerpt": "    We expect that tuning the model of discriminator and making it stronger than generator can solve this problem. \nAlthough the generator is able to synthesis images with right color, we found that some images have \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9646506625487228,
        0.9769071670055233,
        0.8408062932669952,
        0.833610242942183
      ],
      "excerpt": "    In order to deal with this problem, we add two extra losses to the discriminator, hoping the model could \n    care more about the shape of the flower: \n    Below is what we'd done \n    1. loss1 = convert the real image to grayscale, duplicate its value to 3 channels, label it as 1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8845628986064237
      ],
      "excerpt": "    Unfortunately, the generator outputs grayscale image rather than the image with better shape \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9431911784717185
      ],
      "excerpt": "We found that the image generating quality is highly depends on the noise vector input. Consequently, the  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9042703931202523
      ],
      "excerpt": "   The algorithm is discribed below: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9636114080517881,
        0.8596966327728873
      ],
      "excerpt": "   This method could more or less increase the stability of output score. \nIn an attempt to generate more training data, the input image will be flipped left-right, up-down, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8450284950376703
      ],
      "excerpt": "Saving the processed image (resize/rotate) and caption(encoded text vector) to npy file could \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.970155127089207
      ],
      "excerpt": "            leaderboard since the competition is not allowed to do so. There are two benefits about using the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8644389792134873,
        0.9401381921107416
      ],
      "excerpt": "            1. No RNN network required. Thus, the training speed is faster and the memory usage is relatively  \n            low. (&lt;2G GPU memory for small-scale model (batch size = 64)). Also the text embedded is more  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9587629179734368,
        0.8315871639811803,
        0.9643060570966537,
        0.8737241943071393,
        0.9205240725537356
      ],
      "excerpt": "            2. When using RNN we need to preprocess the sentence to the same length and the max length of  \n            sentence/vocabulary size are also limited. Skip tought vector could generate vector from different \n            length sentence and the vocabulary dictionary is also large. \n            However, the skipthought vector also comes with some drawbacks: \n            1.Generating word embedding is time consuming. It takes about 5 hours to build the skipthought vector \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9822709336629594,
        0.8395760050046607,
        0.929885180573285,
        0.9619288778196841,
        0.841681491745403,
        0.8854867552730796,
        0.9826075031132044,
        0.8159013163985044
      ],
      "excerpt": "            2. We found that it takes more time for the model to learn the color information from skipthought \n            vector in comparison with RNN text-encoder. \nAlthough the generated image looks like the flower, the image is lack of detail information. We could use  \n       stack-GAN to solve this problem. However, the implementation of stack-GAN is complicated and we also don't \n       have sufficient memory. Therefore, We attempted to solve this problem by increasing the low resolution  \n       generated image using the super resolution network post-processing. \n       We loaded the pretrained model, converting the image from 64x64 to 256x256 and resizing it back to 64x64. \n       Nevertheless, the inception score doesn't improve since some image is distorted after resizing. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9751218074202364
      ],
      "excerpt": "        is more likely to get higher inception score. Consequently, we apply the sharpen filter to the output  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9776672152979717
      ],
      "excerpt": "        Our model is able to achieve 0.117 inception score after applying the sharpen filter \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9523625032832211
      ],
      "excerpt": "We found that GAN is really hard to train since the loss of generator and discriminator are unstable and they \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9669821598999059
      ],
      "excerpt": "of tf.variable_scope. The model have three networks (RNN,G,D) and three optimizers, so we need to control which \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9601723106139005
      ],
      "excerpt": "We also found that the inception score is a mediocre indicator since it's easy to generate high inception score  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9390771100263648
      ],
      "excerpt": "and adding some noise to them (see figure below). There's also a paper talking about how to generate near perfect  \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/BeyondCloud/Comp04_ReverseImageCaption/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Mon, 27 Dec 2021 19:09:26 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/BeyondCloud/Comp04_ReverseImageCaption/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "BeyondCloud/Comp04_ReverseImageCaption",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/BeyondCloud/Comp04_ReverseImageCaption/master/Report/team2_report.ipynb",
      "https://raw.githubusercontent.com/BeyondCloud/Comp04_ReverseImageCaption/master/Report/.ipynb_checkpoints/Report-checkpoint.ipynb",
      "https://raw.githubusercontent.com/BeyondCloud/Comp04_ReverseImageCaption/master/Code/Small_GANCLS/team2_model.ipynb",
      "https://raw.githubusercontent.com/BeyondCloud/Comp04_ReverseImageCaption/master/Code/Large_GANCLS/%20team2_model.ipynb",
      "https://raw.githubusercontent.com/BeyondCloud/Comp04_ReverseImageCaption/master/Code/WGAN_GANCLS/team2_model.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8192331112861488
      ],
      "excerpt": "network's variable should be update. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8037999584444988
      ],
      "excerpt": "tanh(last_layer_output) or tanh(last_layer_output)*0.5+0.5 to map the output value to reasonable range. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8016273055652675
      ],
      "excerpt": "logit with shape = (batch_size,). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8939708962829795
      ],
      "excerpt": "<img src=\"./Report/test.gif\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.884643429697894
      ],
      "excerpt": "<img src=\"./Report/bk.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9007088265783593
      ],
      "excerpt": "<img src=\"./Report/ST.jpg\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8110399253234066
      ],
      "excerpt": "            from all training and testing captions (5 captions per image) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.884643429697894
      ],
      "excerpt": "<img src=\"./Report/SRCNN.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8684052847166519
      ],
      "excerpt": "            img = Image.fromarray(np.uint8(images*255)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9007088265783593
      ],
      "excerpt": "<img src=\"./Report/sharp.jpg\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.884643429697894
      ],
      "excerpt": "<img src=\"./Report/suck.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/BeyondCloud/Comp04_ReverseImageCaption/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Student ID, name of each team member.",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Comp04_ReverseImageCaption",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "BeyondCloud",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/BeyondCloud/Comp04_ReverseImageCaption/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Mon, 27 Dec 2021 19:09:26 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "<img src=\"./Report/25_inf.png\">\n    (first row ~ last row)<br>\n    0633: this flower has petals that are yellow with red blotches <br>\n    0194: the flower has white stringy petals with yellow and purple pollen tubes<br>\n    2014: this flower is pink in color with only one large petal<br>\n    4683: this flower is yellow in color with petals that are rounded<br>\n    3327: the flower has a several pieces of yellow colored petals that looks similar to its leaves\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "    \n   ",
      "technique": "Header extraction"
    }
  ]
}