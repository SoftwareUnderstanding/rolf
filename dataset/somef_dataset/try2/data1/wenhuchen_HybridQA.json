{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2106.00200",
      "https://arxiv.org/abs/2004.07347",
      "https://arxiv.org/abs/2004.07347",
      "https://arxiv.org/abs/2010.12623"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{chen2020hybridqa,\n  title={HybridQA: A Dataset of Multi-Hop Question Answering over Tabular and Textual Data},\n  author={Chen, Wenhu and Zha, Hanwen and Chen, Zhiyu and Xiong, Wenhan and Wang, Hong and Wang, William},\n  journal={Findings of EMNLP 2020},\n  year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8491520732036005,
        0.8491520732036005,
        0.9854121748519129,
        0.9412821523323863
      ],
      "excerpt": "POINTR + MATE    | Google    | Eisenschlos et al. (2021)         |   63.3  | 70.8 |  62.7  | 70.0 | \nPOINTR + TAPAS   | Google    | Eisenschlos et al. (2021)         |   63.4  | 71.0 |  62.8  | 70.2 | \nDocHopper        | CMU       | Sun et al. (2021)                     |   47.7  | 55.0 |  46.3  | 53.3 | \nHYBRIDER         | UCSB      | Chen et al. (2020)                    |   43.5  | 50.6 |  42.2  | 49.9 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9806436886311394,
        0.9944863065907796
      ],
      "excerpt": "Unsupervised-QG  | NUS\\&UCSB |  Pan et al. (2020)                    |    25.7 | 30.5 |   -    |  -   | \nIf you find this project useful, please use the following format to cite the paper: \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/wenhuchen/HybridQA",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-04-08T07:19:07Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-29T06:45:03Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9885017338627755
      ],
      "excerpt": "This repository contains the dataset and code for the EMNLP2020 paper HybridQA: A Dataset of Multi-Hop Question Answeringover Tabular and Textual Data, which is the first large-scale multi-hop question answering dataset on heterogeneous data including tabular and textual data. The whole dataset contains over 70K question-answer pairs based on 13,000 tables, each table is in average linked to 44 passages, more details in https://hybridqa.github.io/. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9237386360846899
      ],
      "excerpt": "The questions are annotated to require aggregation of information from both the table and its hyperlinked text passages, which poses challenges to existing homongeneous text-based or KB-based models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8060320824563189
      ],
      "excerpt": "The released data contains the following files: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9407885558119722,
        0.8462595140262675
      ],
      "excerpt": "The output will be saved into predictions.intermediate.json, which contain all the answers for non hyper-linked cells, with the hyperlinked cells, we need the MRC model in stage3 to extract the span. \nEvaluating command for stage3 as follows (replace the model_name_or_path with your own): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8551053101175731
      ],
      "excerpt": "The output is finally saved to predictoins.json, which can be used to calculate F1/EM with reference file. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8014611201405422
      ],
      "excerpt": "We host CodaLab challenge in HybridQA Competition, you should submit your results to the competition to obtain your testing score. The submitted file should first be named \"test_answers.json\" and then zipped. The required format of the submission file is described as follows: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8074687156456256
      ],
      "excerpt": "The reported scores are EM and F1. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Dataset and code for EMNLP2020 paper \"HybridQA: A Dataset of Multi-Hop Question Answeringover Tabular and Textual Data\"",
      "technique": "GitHub API"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\nwget https://hybridqa.s3-us-west-2.amazonaws.com/models.zip\nunzip BERT-base-uncased.zip\n```\nIt will download and generate folder stage1/stage2/stage3/\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/wenhuchen/HybridQA/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 14,
      "date": "Wed, 29 Dec 2021 18:47:46 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/wenhuchen/HybridQA/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "wenhuchen/HybridQA",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.9879863063452118
      ],
      "excerpt": "git clone https://github.com/wenhuchen/WikiTables-WithLinks \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9272628778172368
      ],
      "excerpt": "<img src=\"example.png\" width=\"850\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8099194818651142
      ],
      "excerpt": "python preprocessing.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8082022887640018,
        0.8956059192083362
      ],
      "excerpt": "unzip preprocessed_data.zip \npython evaluate_script.py predictions.json released_data/dev_reference.json \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8904962390989898
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES=0 python train_stage12.py --do_lower_case --do_train --train_file preprocessed_data/stage1_training_data.json --learning_rate 2e-6 --option stage1 --num_train_epochs 3.0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8563551411981517,
        0.8006724214691922,
        0.8920974155381226
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES=0 python train_stage12.py --model_name_or_path bert-large-uncased --do_train --train_file preprocessed_data/stage1_training_data.json --learning_rate 2e-6 --option stage1 --num_train_epochs 3.0 \nRunning training command for stage2 as follows: \nCUDA_VISIBLE_DEVICES=0 python train_stage12.py --do_lower_case --do_train --train_file preprocessed_data/stage2_training_data.json --learning_rate 5e-6 --option stage2 --num_train_epochs 3.0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8006724214691922,
        0.8738225040927383
      ],
      "excerpt": "Running training command for stage3 as follows: \nCUDA_VISIBLE_DEVICES=0 python train_stage3.py --do_train  --do_lower_case   --train_file preprocessed_data/stage3_training_data.json  --per_gpu_train_batch_size 12   --learning_rate 3e-5   --num_train_epochs 4.0   --max_seq_length 384   --doc_stride 128  --threads 8 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8904962390989898
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES=0 python train_stage12.py --do_lower_case --do_eval --option stage1 --output_dir stage1/[OWN_PATH]/ --predict_file preprocessed_data/stage1_dev_data.json \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8904962390989898
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES=0 python train_stage12.py --stage1_model stage1/[OWN_PATH] --stage2_model stage2/[OWN_PATH] --do_lower_case --predict_file preprocessed_data/dev_inputs.json --do_eval --option stage12 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8677254483218267
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES=0 python train_stage3.py --model_name_or_path stage3/[OWN_PATH] --do_stage3   --do_lower_case  --predict_file predictions.intermediate.json --per_gpu_train_batch_size 12  --max_seq_length 384   --doc_stride 128 --threads 8 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8956059192083362,
        0.8179018499287385
      ],
      "excerpt": "python evaluate_script.py predictions.json released_data/dev_reference.json \nWe host CodaLab challenge in HybridQA Competition, you should submit your results to the competition to obtain your testing score. The submitted file should first be named \"test_answers.json\" and then zipped. The required format of the submission file is described as follows: \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/wenhuchen/HybridQA/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "HTML"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 wenhu chen\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "HybridQA",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "HybridQA",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "wenhuchen",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/wenhuchen/HybridQA/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- [huggingface transformer 2.6.0](https://github.com/huggingface/transformers)\n- [pytorch 1.4.0](https://pytorch.org/)\n- tensorboardX\n- tqdm\n\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\nCUDA_VISIBLE_DEVICES=0 python train_stage12.py --stage1_model stage1/2020_10_03_22_47_34/checkpoint-epoch2 --stage2_model stage2/2020_10_03_22_50_31/checkpoint-epoch2/ --do_lower_case --predict_file preprocessed_data/dev_inputs.json --do_eval --option stage12 --model_name_or_path  bert-large-uncased\n```\nThis command generates a intermediate result file\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```\nCUDA_VISIBLE_DEVICES=0 python train_stage3.py --model_name_or_path stage3/2020_10_03_22_51_12/checkpoint-epoch3/ --do_stage3   --do_lower_case  --predict_file predictions.intermediate.json --per_gpu_train_batch_size 12  --max_seq_length 384   --doc_stride 128 --threads 8\n```\nThis command generates the prediction file\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 127,
      "date": "Wed, 29 Dec 2021 18:47:46 GMT"
    },
    "technique": "GitHub API"
  }
}