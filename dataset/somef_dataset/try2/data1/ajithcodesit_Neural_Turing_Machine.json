{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1410.5401 [cs.NE].][Graves]\n- [Collier M. and Beel J. (2018) *'Implementing Neural Turing Machine'*, Machine Learning,\n   https://arxiv.org/abs/1807.08518 [cs.LG].][Collier]\n\n## Issues\n- The training is very slow.\n- Very high loss when trying to lear the bias vectors.\n- The code is in eager execution mode and a static graph is not generated using `tf.function`.\n- Stable for small sequences but not for large sequences\n\n[Graves]:https://arxiv.org/pdf/1410.5401.pdf\n[Collier]:https://arxiv.org/pdf/1807.08518.pdf\n[PyTorchNTM]:https://github.com/loudinthecloud/pytorch-ntm\n[LSTMCopy]:https://github.com/ajithcodesit/lstm_copy_tas",
      "https://arxiv.org/abs/1807.08518 [cs.LG].][Collier]\n\n## Issues\n- The training is very slow.\n- Very high loss when trying to lear the bias vectors.\n- The code is in eager execution mode and a static graph is not generated using `tf.function`.\n- Stable for small sequences but not for large sequences\n\n[Graves]:https://arxiv.org/pdf/1410.5401.pdf\n[Collier]:https://arxiv.org/pdf/1807.08518.pdf\n[PyTorchNTM]:https://github.com/loudinthecloud/pytorch-ntm\n[LSTMCopy]:https://github.com/ajithcodesit/lstm_copy_tas"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- [Graves A., Wayne, G. and Danihelka, I. (2014) *\u2018Neural Turing Machines\u2019*, Neural and\n   Evolutionary Computing, arXiv:1410.5401 [cs.NE].][Graves]\n- [Collier M. and Beel J. (2018) *'Implementing Neural Turing Machine'*, Machine Learning,\n   arXiv:1807.08518 [cs.LG].][Collier]\n\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ajithcodesit/Neural_Turing_Machine",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-04-09T21:18:55Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-01T00:52:47Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9677850297916722
      ],
      "excerpt": "This is an implementation of the Neural Turing Machine (NTM) by [Graves et al.][Graves] in TensorFlow 2.0 Alpha. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9315495462995063,
        0.9398518730555525,
        0.9048936720097772,
        0.9906939806566897,
        0.9873970668450721,
        0.9395370989368758,
        0.9881476994909235,
        0.9950562772786111,
        0.9917881061645296,
        0.9839068898568122,
        0.9922137053327579
      ],
      "excerpt": "Execution. Also with TensorFlow moving from sessions to function as the mechanism to connect the operations  \ntogether, the implementation of the architecture is quite straight forward.  \nThe other motivation was to study the NTM architecture as it can learn basic algorithms and can perform better than LSTMs in large sequence or time series applications. \nThe architecture of the NTM implemented here is for the copy task as described in the paper by Graves et al.. The NTM has one read and write head with a 3-layer feedforward controller. The authors of the NTM paper are quite vague about the exact architecture and implementation details which are left to the readers to figure out. The architecture used here is based on the PyTorch implementation by [loudinthecloud][PyTorchNTM] and using suggestions from the paper by [Collier and Beel][Collier] on implementing an NTM. \nSince most of the available implementation of NTMs uses LSTM layers as the controller and the activation used by them are tanh. The activations function of the feedforward controller also uses tanh which produces the better results. The read bias and the write bias vectors are not learned but initialized to random value like the PyTorch implementation and also utilizes constant memory initialization by providing a small value of 10e-6 as suggested in the paper by Collier and Beel. \nTraining the NTM was the most difficult part in this exercise. The above implementation suggestions and examples are what lead to the convergence of the model. It was found that just following the proposals in the paper by Graves et al. made it quite hard to train the NTM. Even with following the above mentioned suggestion the training could not be completed in one session.  \nThe above graph is for training the NTM, with the Y-axis showing the cost per sequence and the X-axis showing the number of sequences shown to the model. Trying to train the neural network initially with sequences between 1 and 20 caused the  \nNTM model to not converge at all. Instead of training in one go, an incremental learning approach seemed to produce better results. In the graph above, the the orange plot is for training sequences of length between 1 and 5, blue for sequences between 1 and 10 and finally the red and cyan plots are for sequences between 1 and 20. This strategy of incremental learning proved to successful in training the NTM and for model convergence. \nTrying to learn the read and weight bias vector initially appeared to be working as the loss steadily decreases but after sometime the loss starts to climb rapidly to very high values and the training had to be cut short. Collier and Beel suggests learning the bias vector for better initialization of the NTM. The problem is most likely some implementation error and further investigation is required. \nThe training consumed a large amount of time and the two large spikes in the red plot required addition time to converge again, after that the cost is very close to zero and stayed stable in the cyan plot. The training took at least 13 hours to complete and was done in seperate sessions for each sequence lengths. The delay is further increase by not converting the NTM into a static graph by using tf.function and instead relying on eager execution. \nFor sequences of lengths between 1 and 20 the copy task is executed perfectly by the NTM with no errors and it generalizes well to sequences of lengths between 30 and 45 without additional training. For sequence lengths above 45, errors start showing up and most of the error occurs during the start of the sequence copy with missing 8-Bit vectors. Input sequence lengths of 70 and above to the NTM are not stable, that is, it would not produce any comprehensible output or would contain a significant amount of errors at the start and end like for the sequence length of 120. But these results beat the [LSTMs in the copy task][LSTMCopy]. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A Neural Turing Machine implemented in TensorFlow 2.0 alpha",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ajithcodesit/Neural_Turing_Machine/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Wed, 29 Dec 2021 23:21:24 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ajithcodesit/Neural_Turing_Machine/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "ajithcodesit/Neural_Turing_Machine",
    "technique": "GitHub API"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ajithcodesit/Neural_Turing_Machine/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 Ajith Thomas\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "**Neural Turing Machine**",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Neural_Turing_Machine",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "ajithcodesit",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ajithcodesit/Neural_Turing_Machine/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 5,
      "date": "Wed, 29 Dec 2021 23:21:24 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "neural-turing-machine",
      "tensorflow",
      "python",
      "tensorflow-2-example"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "For training the NTM\n```\n./train --train --min_sequence=1 --max_sequence=20\n```\nFor testing the NTM with a test input sequence with fixed sequence length\n```\n./train --test --max_sequence=30\n```\nFor testing with random sequence length\n```\n./train --test --max_sequence=120 --random_seq_len\n```\nFor visualizing the internal working of the NTM\n```\n./train --test --visualize --max_sequence=20\n```\nAvailable options\n```\n./train --help\n\nusage: train.py [-h] [--train] [--test] [--visualize] [--random_seq_len]\n                [--epochs EPOCHS] [--batches BATCH_SIZE]\n                [--steps_per_epoch STEPS_PER_EPOCH]\n                [--learning_rate LEARNING_RATE] [--momentum MOMENTUM]\n                [--clip_grad_min CLIP_GRAD_MIN]\n                [--clip_grad_max CLIP_GRAD_MAX]\n                [--controller_size CONTROLLER_SIZE]\n                [--memory_locations MEMORY_LOCATIONS]\n                [--memory_vector_size MEMORY_VECTOR_SIZE]\n                [--maximum_shifts MAXIMUM_SHIFTS] [--learn_r_bias]\n                [--learn_w_bias] [--learn_m_bias]\n                [--max_sequence MAX_SEQUENCE] [--min_sequence MIN_SEQUENCE]\n                [--in_bits IN_BITS] [--out_bits OUT_BITS]\n                [--checkpoint_dir CHECKPOINT_DIR] [--max_to_keep MAX_TO_KEEP]\n                [--report_interval REPORT_INTERVAL]\n                [--train_log_dir TRAIN_LOG_DIR]\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}