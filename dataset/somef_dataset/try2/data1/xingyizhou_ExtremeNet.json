{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1901.08043"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find this model useful for your research, please use the following BibTeX entry.\n\n    @inproceedings{zhou2019bottomup,\n      title={Bottom-up Object Detection by Grouping Extreme and Center Points},\n      author={Zhou, Xingyi and Zhuo, Jiacheng and Kr{\\\"a}henb{\\\"u}hl, Philipp},\n      booktitle={CVPR},\n      year={2019}\n    }\n    \nPlease also considering citing the CornerNet paper (where this code is heavily borrowed from) and Deep Extreme Cut paper (if you use the instance segmentation part).\n\n    @inproceedings{law2018cornernet,\n      title={CornerNet: Detecting Objects as Paired Keypoints},\n      author={Law, Hei and Deng, Jia},\n      booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},\n      pages={734--750},\n      year={2018}\n    }\n\n    @Inproceedings{Man+18,\n      Title          = {Deep Extreme Cut: From Extreme Points to Object Segmentation},\n      Author         = {K.K. Maninis and S. Caelles and J. Pont-Tuset and L. {Van Gool}},\n      Booktitle      = {Computer Vision and Pattern Recognition (CVPR)},\n      Year           = {2018}\n    }\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@Inproceedings{Man+18,\n  Title          = {Deep Extreme Cut: From Extreme Points to Object Segmentation},\n  Author         = {K.K. Maninis and S. Caelles and J. Pont-Tuset and L. {Van Gool}},\n  Booktitle      = {Computer Vision and Pattern Recognition (CVPR)},\n  Year           = {2018}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{law2018cornernet,\n  title={CornerNet: Detecting Objects as Paired Keypoints},\n  author={Law, Hei and Deng, Jia},\n  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},\n  pages={734--750},\n  year={2018}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{zhou2019bottomup,\n  title={Bottom-up Object Detection by Grouping Extreme and Center Points},\n  author={Zhou, Xingyi and Zhuo, Jiacheng and Kr{\\\"a}henb{\\\"u}hl, Philipp},\n  booktitle={CVPR},\n  year={2019}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9990439029492845
      ],
      "excerpt": "CVPR 2019 (arXiv 1901.08043)          \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/xingyizhou/ExtremeNet",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-01-07T00:56:51Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-28T02:56:01Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9845605337364578
      ],
      "excerpt": "This project is developed upon the CornerNet code and contains the code from Deep Extreme Cut(DEXTR). Thanks to the original authors! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9846589278470569
      ],
      "excerpt": "With the advent of deep learning, object detection drifted from a bottom-up to a top-down recognition problem. State of the art algorithms enumerate a near-exhaustive list of object locations and classify each into: object or not. In this paper, we show that bottom-up approaches still perform competitively. We detect four extreme points (top-most, left-most, bottom-most, right-most) and one center point of objects using a standard keypoint estimation network. We group the five keypoints into a bounding box if they are geometrically aligned. Object detection is then a purely appearance-based keypoint estimation problem, without region classification or implicit feature learning. The proposed method performs on-par with the state-of-the-art region based detection methods, with a bounding box AP of 43.2% on COCO test-dev. In addition, our estimated extreme points directly span a coarse octagonal mask, with a COCO Mask AP of 18.9%, much better than the Mask AP of vanilla bounding boxes. Extreme point guided segmentation further improves this to 34.6% Mask AP. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8755055341780796
      ],
      "excerpt": "You will need 5x 12GB GPUs to reproduce our training. Our model is fine-tuned on the 10-GPU pre-trained CornerNet model. After downloading the CornerNet model and put it in cache/, run \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Bottom-up Object Detection by Grouping Extreme and Center Points",
      "technique": "GitHub API"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Download the images (2017 Train, 2017 Val, 2017 Test) from [coco website](http://cocodataset.org/#download).\n- Download annotation files (2017 train/val and test image info) from [coco website](http://cocodataset.org/#download). \n- Place the data (or create symlinks) to make the data folder like:\n\n  ~~~\n  ${ExtremeNet_ROOT}\n  |-- data\n  `-- |-- coco\n      `-- |-- annotations\n          |   |-- instances_train2017.json\n          |   |-- instances_val2017.json\n          |   |-- image_info_test-dev2017.json\n          `-- images\n              |-- train2017\n              |-- val2017\n              |-- test2017\n  ~~~\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/xingyizhou/ExtremeNet/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 174,
      "date": "Wed, 29 Dec 2021 00:13:46 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/xingyizhou/ExtremeNet/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "xingyizhou/ExtremeNet",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "~~~\ncd $ExtremeNet_ROOT/data\ngit clone https://github.com/cocodataset/cocoapi.git coco\ncd $ExtremeNet_ROOT/data/coco/PythonAPI\nmake\npython setup.py install --user\n~~~\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "If you want to reproduce the results in the paper for benchmark evaluation and training, you will need to setup dataset.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "The code was tested with [Anaconda](https://www.anaconda.com/download) Python 3.6 and [PyTorch]((http://pytorch.org/)) v0.4.1. After install Anaconda:\n\n1. Clone this repo:\n\n    ~~~\n    ExtremeNet_ROOT=/path/to/clone/ExtremeNet\n    git clone --recursive https://github.com/xingyizhou/ExtremeNet $ExtremeNet_ROOT\n    ~~~\n\n\n2. Create an Anaconda environment using the provided package list from [Cornernet](https://github.com/princeton-vl/CornerNet).\n\n    ~~~\n    conda create --name CornerNet --file conda_packagelist.txt\n    source activate CornerNet\n    ~~~\n\n3. Compiling NMS (originally from [Faster R-CNN](https://github.com/rbgirshick/py-faster-rcnn/blob/master/lib/nms/cpu_nms.pyx) and [Soft-NMS](https://github.com/bharatsingh430/soft-nms/blob/master/lib/nms/cpu_nms.pyx)).\n\n    ~~~\n    cd $ExtremeNet_ROOT/external\n    make\n    ~~~\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9223334315953018
      ],
      "excerpt": "cd $ExtremeNet_ROOT/tools/ \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python gen_coco_extreme_points.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9515752551715031
      ],
      "excerpt": "  python test.py ExtremeNet [--suffix multi_scale] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8980605473562568
      ],
      "excerpt": "  python eval_dextr_mask.py results/ExtremeNet/250000/validation/multi_scale/results.json \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9503189345333785
      ],
      "excerpt": "python train.py ExtremeNet \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9503189345333785
      ],
      "excerpt": "python train.py ExtremeNet --iter xxxx \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/xingyizhou/ExtremeNet/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "C++"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "BSD 3-Clause \"New\" or \"Revised\" License",
      "url": "https://api.github.com/licenses/bsd-3-clause"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'BSD 3-Clause License\\n\\nCopyright (c) 2018, University of Michigan\\nAll rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met:\\n\\n Redistributions of source code must retain the above copyright notice, this\\n  list of conditions and the following disclaimer.\\n\\n Redistributions in binary form must reproduce the above copyright notice,\\n  this list of conditions and the following disclaimer in the documentation\\n  and/or other materials provided with the distribution.\\n\\n* Neither the name of the copyright holder nor the names of its\\n  contributors may be used to endorse or promote products derived from\\n  this software without specific prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "ExtremeNet: Training and Evaluation Code",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "ExtremeNet",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "xingyizhou",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/xingyizhou/ExtremeNet/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 991,
      "date": "Wed, 29 Dec 2021 00:13:46 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Download our [pre-trained model](https://drive.google.com/file/d/1re-A74WRvuhE528X6sWsg1eEbMG8dmE4/view?usp=sharing) and put it in `cache/`.\n- Optionally, if you want to test instance segmentation with [Deep Extreme Cut](https://github.com/scaelles/DEXTR-PyTorch), download their [PASCAL + SBD pertained model](https://data.vision.ee.ethz.ch/kmaninis/share/DEXTR/Downloads/models/dextr_pascal-sbd.pth) and put it in `cache/`. \n- Run the demo \n\n    ~~~\n    python demo.py [--demo /path/to/image/or/folder] [--show_mask]\n    ~~~\n\n    Contents in `[]` are optional. By default, it runs the sample images provided in `$ExtremeNet_ROOT/images/` (from [Detectron](https://github.com/facebookresearch/Detectron/tree/master/demo)). We show the predicted extreme point heatmaps (combined four heatmaps and overlaid on the input image), the predicted center point heatmap, and the detection and octagon mask results. If setup correctly, the output will look like:\n    \n    <img src='readme/extreme.png' align=\"center\" width=\"400px\"> <img src='readme/center.png' align=\"center\" width=\"400px\">\n    \n    <p align=\"center\"> \n        <img src='readme/octagon.png' align=\"center\" width=\"400px\">\n    </p>\n\n    If `--show_mask` is turned on, it further pipelined with [DEXTR](https://github.com/scaelles/DEXTR-PyTorch) for instance segmentation. The output will look like:\n    <p align=\"center\"> \n        <img src='readme/mask.png' align=\"center\" width=\"400px\">\n    </p>\n\n\n",
      "technique": "Header extraction"
    }
  ]
}