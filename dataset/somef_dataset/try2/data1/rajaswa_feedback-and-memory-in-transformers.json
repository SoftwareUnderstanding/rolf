{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2002.09402",
      "https://arxiv.org/abs/2002.09402"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you use the code in this repository in any manner, cite the repository:\n```python\n@misc{patil2021-feedback-github,\n    author       = {Rajaswa Patil},\n    title        = {feedback-and-memory-in-transformers},\n    month        = apr,\n    year         = 2021,\n    publisher    = {Github},\n    url          = \"https://github.com/rajaswa/feedback-and-memory-in-transformers\"\n    }\n```\n\nIf you use the code for Feedback Transfomer or the Sequence Copy & Reverse task, cite the Feedback Transformer paper:\n```python \n@misc{fan2021addressing,\n      title={Addressing Some Limitations of Transformers with Feedback Memory}, \n      author={Angela Fan and Thibaut Lavril and Edouard Grave and Armand Joulin and Sainbayar Sukhbaatar},\n      year={2021},\n      eprint={2002.09402},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}\n```\n\nIf you use the code from COGS Benchmark data processing and loading, cite the COGS paper:\n```python\n@inproceedings{kim-linzen-2020-cogs,\n    title = \"{COGS}: A Compositional Generalization Challenge Based on Semantic Interpretation\",\n    author = \"Kim, Najoung  and\n      Linzen, Tal\",\n    booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)\",\n    month = nov,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.emnlp-main.731\",\n    doi = \"10.18653/v1/2020.emnlp-main.731\",\n    pages = \"9087--9105\",\n    abstract = \"Natural language is characterized by compositionality: the meaning of a complex expression is constructed from the meanings of its constituent parts. To facilitate the evaluation of the compositional abilities of language processing architectures, we introduce COGS, a semantic parsing dataset based on a fragment of English. The evaluation portion of COGS contains multiple systematic gaps that can only be addressed by compositional generalization; these include new combinations of familiar syntactic structures, or new combinations of familiar words and familiar structures. In experiments with Transformers and LSTMs, we found that in-distribution accuracy on the COGS test set was near-perfect (96{--}99{\\%}), but generalization accuracy was substantially lower (16{--}35{\\%}) and showed high sensitivity to random seed (+-6{--}8{\\%}). These findings indicate that contemporary standard NLP models are limited in their compositional generalization capacity, and position COGS as a good way to measure progress.\",\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{kim-linzen-2020-cogs,\n    title = \"{COGS}: A Compositional Generalization Challenge Based on Semantic Interpretation\",\n    author = \"Kim, Najoung  and\n      Linzen, Tal\",\n    booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)\",\n    month = nov,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.emnlp-main.731\",\n    doi = \"10.18653/v1/2020.emnlp-main.731\",\n    pages = \"9087--9105\",\n    abstract = \"Natural language is characterized by compositionality: the meaning of a complex expression is constructed from the meanings of its constituent parts. To facilitate the evaluation of the compositional abilities of language processing architectures, we introduce COGS, a semantic parsing dataset based on a fragment of English. The evaluation portion of COGS contains multiple systematic gaps that can only be addressed by compositional generalization; these include new combinations of familiar syntactic structures, or new combinations of familiar words and familiar structures. In experiments with Transformers and LSTMs, we found that in-distribution accuracy on the COGS test set was near-perfect (96{--}99{\\%}), but generalization accuracy was substantially lower (16{--}35{\\%}) and showed high sensitivity to random seed (+-6{--}8{\\%}). These findings indicate that contemporary standard NLP models are limited in their compositional generalization capacity, and position COGS as a good way to measure progress.\",\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{patil2021-feedback-github,\n    author       = {Rajaswa Patil},\n    title        = {feedback-and-memory-in-transformers},\n    month        = apr,\n    year         = 2021,\n    publisher    = {Github},\n    url          = \"https://github.com/rajaswa/feedback-and-memory-in-transformers\"\n    }\nIf you use the code for Feedback Transfomer or the Sequence Copy & Reverse task, cite the Feedback Transformer paper:\npython \n@misc{fan2021addressing,\n      title={Addressing Some Limitations of Transformers with Feedback Memory}, \n      author={Angela Fan and Thibaut Lavril and Edouard Grave and Armand Joulin and Sainbayar Sukhbaatar},\n      year={2021},\n      eprint={2002.09402},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9252690500369315
      ],
      "excerpt": "|       False       |       True        |      12.3k        |        74.1%          |          70.86%           |       4441.7 s        |     645.08 ms         |     1039.30 ms        |    365.49 ms      | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rajaswa/feedback-and-memory-in-transformers",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Submit an issue.\n",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-04-17T05:44:29Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-05-06T16:35:01Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9896585417881026
      ],
      "excerpt": "My final project submission for the Meta Learning course at BITS Goa (conducted by TCS Research & BITS Goa). The project is based on the Feedback Transformer paper. The paper introduces a feedback mechanism in transformer models by adding a recurrent memory-attention based approach. This helps the transformer model in: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9146618377475882,
        0.931974988025989,
        0.9165299311907915
      ],
      "excerpt": "The key contributions of this project can be listed as follows: \n1. Implementing and Open-sourcing a modular customizable Feedback Transformer Model in PyTorch \n2. Experimenting the Feedback Transformer Model with COGS Benchmark (Compositional Generalization) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9407394472947926
      ],
      "excerpt": "The Feedback Transformer Model has been implemented as PyTorch model class in the given notebook. You can adjust the various hyperparameters and turn the feedback ON/OFF in the Encoder and Decoder of the Model independently. Use the model in the following manner: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = FeedbackTransformerModel( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8592472997145237
      ],
      "excerpt": "            memory_context = 8,         #: How long to look in the past for Memory-attention \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8016940697564862,
        0.8917485107165143
      ],
      "excerpt": "            dropout = 0.1,              #: Model Dropout Probability  \n            PAD_IDX = 0,                #: PAD Token ID to Mask Padding tokens for Attention \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9806096090451192
      ],
      "excerpt": "The COGS Benchmark is a benchmark for evaluating compositional generalization & reasoning in natural language. The COGS task is that of mapping a natural language sentence to a lambda-expression based semantic logical form: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8600494004758744
      ],
      "excerpt": "1. Novel (unseen in training) Combination of Familiar Primitives and Grammatical Roles \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9102294818467196
      ],
      "excerpt": "You can check the COGS Paper for more details on the benchmark. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9100936000801788
      ],
      "excerpt": "                        num_workers=2,          #: Number of workers for Data Loading \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9810310516420125,
        0.9129117443898922
      ],
      "excerpt": "NOTE: The feedback transformer paper does not include this benchmark or any related task. This is the first attempt (to the best of my knowledge) to inspect the effect of incoroporating feedback and memory based architectural biases in solving compositional generalization problem in natural language. \nWhile the PyTorch-Lightning profiler and Tensorboard logger (included in the notebook) will give a detailed insights into the experiments, here are key metrics to report: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9026929704418138,
        0.927214456184967,
        0.8258737130110915,
        0.9581918837719111
      ],
      "excerpt": "NOTE: The results are subject to change in hyperparameters and training settings. The above results are obtained from the current settings given in the notebook. The results can be increased significantly by training bigger models for longer times. \nThe Validation accuracy (roughly equal to the Normal test accuracy) reflects the Expressivity of the models towards the COGS task \nAccess to higher level representations might help in semantic-parsing by allowing top-down processing \nIn general, incorporating feedback gives the model more expressivity with lesser number of parameters \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9588883965116967
      ],
      "excerpt": "This needs accurate inference on previously unseen novel linguistic structures and an ability to maintain a belief state for longer contexts \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9339640589307907
      ],
      "excerpt": "High Expressivity can lead to poor Compositional Generalization in Vanilla Transformer models (as reported in the COGS Paper) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589798498138933,
        0.9675812819445846
      ],
      "excerpt": "Enabling feedback in Encoder further reduces the the drop in Generalization accuracy to 4.98% \nThe Sequence Copy & Reverse task is included in the Feedback Transformer paper as an Algorithmic task to test the role of memory in long-sequence processing. Since the official dataset is not publicly available, we generate the dataset synthetically.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9100936000801788,
        0.8042200213615164
      ],
      "excerpt": "    num_workers=2,                  #: Number of workers for Data Loading \n    num_samples_train=10000,        #: Number of samples to generate for training set \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "My final project submission for the Meta Learning course at BITS Goa (conducted by TCS Research)",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rajaswa/feedback-and-memory-in-transformers/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Mon, 27 Dec 2021 08:46:09 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/rajaswa/feedback-and-memory-in-transformers/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "rajaswa/feedback-and-memory-in-transformers",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/rajaswa/feedback-and-memory-in-transformers/main/Feedback_and_Memory_in_Transformers.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8129906975811632
      ],
      "excerpt": "The COGS dataset can be loaded as a PyTorch-Lightning Module in the following manner: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8533991930350605
      ],
      "excerpt": "The sequence copy & reverse dataset can be loaded as a PyTorch-Lightning Module in the following manner: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8461407110879455
      ],
      "excerpt": "            output_vocab_size = 800,    #: Output Vocabulary Size \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8772036350513425
      ],
      "excerpt": "                        batch_size=128,         #: Batch Size for Training  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8394779563088418
      ],
      "excerpt": "                        use_Gen=True            #: Whether to use normal test set or generaliztion test set \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8598972764940559,
        0.8510916487379581,
        0.8452968479869877
      ],
      "excerpt": "|       False       |       False       |      12.7k        |        69.5%          |          65.44%           |       193.43 s        |      22.58 ms         |      25.17 ms         |    20.08 ms       | \n|       False       |       True        |      12.3k        |        74.1%          |          70.86%           |       4441.7 s        |     645.08 ms         |     1039.30 ms        |    365.49 ms      | \n|       True        |       True        |      12.2k        |        74.4%          |          70.69%           |       7402.4 s        |     701.85 ms         |      1129.4 ms        |    404.65 ms      | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8506174080035639
      ],
      "excerpt": "    batch_size=64,                  #: Batch Size for Training \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8394563676299724,
        0.8504295041453396,
        0.8376074570986117
      ],
      "excerpt": "    num_samples_train=10000,        #: Number of samples to generate for training set \n    num_samples_eval=1000,          #: Number of samples to generate for validation and test set \n    max_length_train=10,            #: Sequence length in training samples \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.803486292581016
      ],
      "excerpt": "    reverse=True,                   #: Whether to Copy the Input Sequence or Reverse the Input Sequence \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/rajaswa/feedback-and-memory-in-transformers/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2021 Rajaswa Patil\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Project-22: Feedback and Memory in Transformers",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "feedback-and-memory-in-transformers",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "rajaswa",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rajaswa/feedback-and-memory-in-transformers/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 5,
      "date": "Mon, 27 Dec 2021 08:46:09 GMT"
    },
    "technique": "GitHub API"
  }
}