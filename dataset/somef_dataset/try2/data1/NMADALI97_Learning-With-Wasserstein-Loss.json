{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1701.07875",
      "https://arxiv.org/abs/1703.05921"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.98768380516008
      ],
      "excerpt": "1) The original WGAN paper: https://arxiv.org/pdf/1701.07875.pdf \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/NMADALI97/Learning-With-Wasserstein-Loss",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-02-15T12:57:45Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-02-15T13:24:15Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9726413644417956,
        0.8622351820994001,
        0.9150364433673499,
        0.9944023507097631,
        0.9935730075641591,
        0.9157166108107667,
        0.9068774133024133,
        0.8870387928067555
      ],
      "excerpt": "Implementation of the  paper \"Wasserstein GAN\" \nWasserstein Generative Adversarial Network or WGAN is a recently introduced variant of the popular Generative Adversarial Networks or GAN's.  \nIt is commonly known in the machine learning community that GAN's are notoriously difficult to train  and suffer from the following issues: \n1) Training the Generator and the Discriminator in GAN's is difficult. This is due to the nature of the Adversarial loss itself which induces a minmax game between the Generator and Discriminator networks such that one tries to fool the other until they reach a Nash equilibrium(at least theoretically !!) which is a state in which the Generator produces realistic samples which the Discriminator is unable to distinguish from the original samples. In practice this is very difficult to achieve using Gradient descent based training methods and results in model oscillation. \n2) The training of the GAN's is majorly dependent on the architecture of the Generator and the Discriminator. It is seen that the DCGAN's(https://github.com/kpandey008/DCGANS) which are the convolutional variants of GAN's tend to perform better than the traditional MLP based GAN's on several metrics (eg. Visual quality of samples, Better classification accuracy on some intrinsic tasks using the faetures learned by the Discriminator.) \n3) GAN's are highly susceptible to Mode Collapse. Mode Collapse is a scenario in which GAN falls to a mode and drops it's generated samples around that mode. (eg. generating only samples of a single digit out of many digits when trained over the MNIST dataset) \n4) GAN's might learn to draw samples which are visually pleasing but might lie outside the data manifold.(Seems like a paradox but this can happen!!) \nThe WGAN paper addresses some of these problems by using a distance measure called as Wasserstein distance or the Earth's Movers Distance.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9183637118763319,
        0.9675259874400879
      ],
      "excerpt": "1) The Wasserstein distance is shown to be a better metric than other distance metrics like Jensen-Shannon Distance or the Total Variation Distance. Use of Wasserstein loss stablizes the GAN training. The authors claim that the use of this metric removes the dependence of GAN training to network architectural constraints. \n2) Wasserstein provides a stable loss metric that can be used to assess the performance of the GAN. Until now there was no definite measure to assess the training performance of GAN's apart from manally inspecting the quality of samples generated by the Generator. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9614874806644061,
        0.9125400365799251
      ],
      "excerpt": "is much stable than the latter. However I feel that the sample quality can be increased by training a network with larger capacity over more number of steps. Also presence of batchnorm in both the generator and the discriminator helps stabilize training to a great extent. \nAfter learn WGAN model with normal dataset (not contains anomalies),  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.936758042004263,
        0.9737167440015133
      ],
      "excerpt": "When unseen data comes, the model tries to find latent variable z that generates input image using backpropagation. (similar with style transfer) \nAnomaly Score is based on residual and discrimination losses. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8110248473438456,
        0.8384407726174368,
        0.9035821190163669,
        0.9758688971723479,
        0.9472122977223608,
        0.9465678446600523,
        0.9057430345437443,
        0.9294722030005595
      ],
      "excerpt": "- Discrimination loss: L1 distacne between hidden representations of generated and test image, extracted by discriminators. \nTotal Loss for finding latent variable z is weighted sum of the two. (defualt lambda = 0.1) \n1) The Wasserstein GAN indeeds provides a stable gradient during traning provided the Lipschitz constraint is enforced on the Discriminator network.(This is probably the key takeaway from the paper). For me the training sometimes suffered from the exploding gradient problem. \n2) I tested the WGAN's with multiple network architectures. The performance as well as the visual quality of the samples of the algorithm was not affected much. Thus the paper's claim of architectural robustness holds good. \n3) However the model training still showed oscillations for me. Maybe a bit of hyperparameter tuning needed(However in the paper is claimed that the training is quite robust to the hyperparameter tuning). \n4) Weight Clipping is probably not the best solution for enforcing the Lipschitz constraint on the Discriminator network. However, in this case it performs fairly well except in cases where the gradient explosion takes place. \n5) WGAN's do not suffer from mode collapse which can be observed in the above results). This is because of the fact that the discriminator is not saturated during the training stage ini comparison to the standard GAN networks which can suffer from considerable mode collapse. \nSome good resources for knowing more about WGAN's and improving their training are : \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8544067967012686,
        0.9414907742546436
      ],
      "excerpt": "2) The paper Improved Training of Wasserstein GANs by Gulrajani et.al is an excellent resource for knowing more about WGAN training. The paper can be found at https://arxiv.org/pdf/1704.00028.pdf \n4) Tensorflow implementation of Anomaly GAN (AnoGAN). \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/NMADALI97/Learning-With-Wasserstein-Loss/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 22 Dec 2021 09:43:40 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/NMADALI97/Learning-With-Wasserstein-Loss/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "NMADALI97/Learning-With-Wasserstein-Loss",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/NMADALI97/Learning-With-Wasserstein-Loss/master/Unsupervised_Anomaly_Detection.ipynb",
      "https://raw.githubusercontent.com/NMADALI97/Learning-With-Wasserstein-Loss/master/Cifar10.ipynb",
      "https://raw.githubusercontent.com/NMADALI97/Learning-With-Wasserstein-Loss/master/MNIST.ipynb"
    ],
    "technique": "File Exploration"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/NMADALI97/Learning-With-Wasserstein-Loss/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Wasserstein Generative Adversarial Networks (WGANS)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Learning-With-Wasserstein-Loss",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "NMADALI97",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/NMADALI97/Learning-With-Wasserstein-Loss/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The following python packages must be installed for running the code\n\n- Python 2.7 or Python 3.3+\n- Tensorflow 0.12.1\n- Numpy\n- Matplotlib\n- ImageIO\n- Scikit-learn\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 22 Dec 2021 09:43:40 GMT"
    },
    "technique": "GitHub API"
  }
}