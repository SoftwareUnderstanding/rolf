{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1910.13461",
      "https://arxiv.org/abs/1910.13461"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find this repository useful for your research or development, please cite the following [paper](https://aclanthology.org/2021.findings-emnlp.10/):\n```\n@inproceedings{xie2021factual,\n    title = \"Factual Consistency Evaluation for Text Summarization via Counterfactual Estimation\",\n    author = \"Xie, Yuexiang  and Sun, Fei  and Deng, Yang  and Li, Yaliang  and Ding, Bolin\",\n    booktitle = \"Findings of the Association for Computational Linguistics: EMNLP 2021\",\n    month = nov,\n    year = \"2021\",\n    url = \"https://aclanthology.org/2021.findings-emnlp.10\",\n    pages = \"100--110\"\n}\n\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{xie2021factual,\n    title = \"Factual Consistency Evaluation for Text Summarization via Counterfactual Estimation\",\n    author = \"Xie, Yuexiang  and Sun, Fei  and Deng, Yang  and Li, Yaliang  and Ding, Bolin\",\n    booktitle = \"Findings of the Association for Computational Linguistics: EMNLP 2021\",\n    month = nov,\n    year = \"2021\",\n    url = \"https://aclanthology.org/2021.findings-emnlp.10\",\n    pages = \"100--110\"\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/xieyxclack/factual_coco",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-08-30T11:32:28Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-11T05:08:23Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9392942571238678
      ],
      "excerpt": "The implementation of Factual Consistency Evaluation for Text Summarization via Counterfactual Estimation in PyTorch. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9855669598439533
      ],
      "excerpt": "* model_path: The path to the scoring model, which is an independent summarization model, and it is not necessary to be the model that generates the evaluated summaries. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9842006206962991,
        0.8016835981965754
      ],
      "excerpt": "In this repository, we adopt BART as the scoring model and implement it via fairseq. \nThe checkpoints can be downloaded from here, including &nbsp;bart.large.cnn&nbsp; and &nbsp;bart.large.xsum&nbsp;. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "\u2502   model.pt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "The implementation of <Factual Consistency Evaluation for Text Summarization via Counterfactual Estimation> in PyTorch.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/xieyxclack/factual_coco/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Mon, 27 Dec 2021 09:58:51 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/xieyxclack/factual_coco/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "xieyxclack/factual_coco",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/xieyxclack/factual_coco/tree/main/fairseq/examples/simultaneous_translation/docs"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/xieyxclack/factual_coco/main/fairseq/examples/language_model/prepare-wikitext-103.sh",
      "https://raw.githubusercontent.com/xieyxclack/factual_coco/main/fairseq/examples/multilingual/finetune_multilingual_model.sh",
      "https://raw.githubusercontent.com/xieyxclack/factual_coco/main/fairseq/examples/multilingual/multilingual_fairseq_gen.sh",
      "https://raw.githubusercontent.com/xieyxclack/factual_coco/main/fairseq/examples/multilingual/train_multilingual_model.sh",
      "https://raw.githubusercontent.com/xieyxclack/factual_coco/main/fairseq/examples/speech_recognition/datasets/prepare-librispeech.sh",
      "https://raw.githubusercontent.com/xieyxclack/factual_coco/main/fairseq/examples/m2m_100/install_dependecies.sh",
      "https://raw.githubusercontent.com/xieyxclack/factual_coco/main/fairseq/examples/m2m_100/tok.sh",
      "https://raw.githubusercontent.com/xieyxclack/factual_coco/main/fairseq/examples/m2m_100/tokenizers/seg_ja.sh",
      "https://raw.githubusercontent.com/xieyxclack/factual_coco/main/fairseq/examples/m2m_100/tokenizers/seg_ko.sh",
      "https://raw.githubusercontent.com/xieyxclack/factual_coco/main/fairseq/examples/m2m_100/tokenizers/tokenizer_ar.sh",
      "https://raw.githubusercontent.com/xieyxclack/factual_coco/main/fairseq/examples/criss/download_and_preprocess_tatoeba.sh",
      "https://raw.githubusercontent.com/xieyxclack/factual_coco/main/fairseq/examples/criss/download_and_preprocess_flores_test.sh",
      "https://raw.githubusercontent.com/xieyxclack/factual_coco/main/fairseq/examples/criss/sentence_retrieval/sentence_retrieval_tatoeba.sh",
      "https://raw.githubusercontent.com/xieyxclack/factual_coco/main/fairseq/examples/criss/unsupervised_mt/eval.sh",
      "https://raw.githubusercontent.com/xieyxclack/factual_coco/main/fairseq/examples/criss/mining/mine_example.sh",
      "https://raw.githubusercontent.com/xieyxclack/factual_coco/main/fairseq/examples/byte_level_bpe/get_data.sh",
      "https://raw.githubusercontent.com/xieyxclack/factual_coco/main/fairseq/examples/joint_alignment_translation/prepare-wmt18en2de_no_norm_no_escape_no_agressive.sh",
      "https://raw.githubusercontent.com/xieyxclack/factual_coco/main/fairseq/examples/roberta/preprocess_GLUE_tasks.sh",
      "https://raw.githubusercontent.com/xieyxclack/factual_coco/main/fairseq/examples/roberta/preprocess_RACE.sh",
      "https://raw.githubusercontent.com/xieyxclack/factual_coco/main/fairseq/examples/roberta/commonsense_qa/download_cqa_data.sh",
      "https://raw.githubusercontent.com/xieyxclack/factual_coco/main/fairseq/examples/backtranslation/sacrebleu.sh",
      "https://raw.githubusercontent.com/xieyxclack/factual_coco/main/fairseq/examples/backtranslation/tokenized_bleu.sh",
      "https://raw.githubusercontent.com/xieyxclack/factual_coco/main/fairseq/examples/backtranslation/prepare-wmt18en2de.sh",
      "https://raw.githubusercontent.com/xieyxclack/factual_coco/main/fairseq/examples/backtranslation/prepare-de-monolingual.sh",
      "https://raw.githubusercontent.com/xieyxclack/factual_coco/main/fairseq/examples/translation/prepare-wmt14en2fr.sh",
      "https://raw.githubusercontent.com/xieyxclack/factual_coco/main/fairseq/examples/translation/prepare-iwslt14.sh",
      "https://raw.githubusercontent.com/xieyxclack/factual_coco/main/fairseq/examples/translation/prepare-iwslt17-multilingual.sh",
      "https://raw.githubusercontent.com/xieyxclack/factual_coco/main/fairseq/examples/translation/prepare-wmt14en2de.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We provide [BART](https://arxiv.org/abs/1910.13461) as the scoring model adopted in CoCo, and implement it via [fairseq](https://github.com/pytorch/fairseq) (which is provided in this repository). And you can install fairseq via:\n```\ncd factual_coco\npip install --editable ./\n```\nIf you would like to adopt other summarization model as the scoring model, you can skip this step and implement your own scoring model.\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9716193339135423,
        0.8861196609169606
      ],
      "excerpt": "git clone http://gitlab.alibaba-inc.com/yuexiang.xyx/factual_coco.git \nBefore execute run_coco.py to get the coco score, you should provide: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8220156901659884
      ],
      "excerpt": "* data_path: The path to the source documents (named as source.txt) and summaries (named as summary.txt). One document/summary per line. (We provide an example in the data folder) \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/xieyxclack/factual_coco/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell",
      "C++",
      "Cython",
      "Cuda"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Factual CoCo: A metric for factual consistency in text summarization via counterfactual estimation",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "factual_coco",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "xieyxclack",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/xieyxclack/factual_coco/blob/main/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* Python version 3.6 \n* Torch version 1.6.0\n* spaCy v3.1 (Install from [here](https://spacy.io/usage)) \n\nBesides, you need to download the model used in spaCy for part-of-speech (pos) tagging\n```\npython -m spacy download en_core_web_sm\n```\nor download from &nbsp;*en_core_web_sm-3.1.0-py3-none-any.whl*&nbsp; from [here](https://github.com/explosion/spacy-models/releases/tag/en_core_web_sm-3.1.0), and then run `pip install en_core_web_sm-3.1.0-py3-none-any.whl`.\n<br/> <br/>\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3,
      "date": "Mon, 27 Dec 2021 09:58:51 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "pytorch",
      "text-summarization",
      "factual-consistency"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\npython3 run_coco.py --model_path /path/to/model --data_path /path/to/data --output_file coco_score.txt --mask token\n```\n* `mask` is used to set up the mask strategy (one of ['token', 'span', 'sent', 'doc'], more details can be found in the paper). And you can design your own mask strategies in the mask function.\n* `output_file` denotes the file to save the generated coco scores. \n\n\n<br/> <br/>\n",
      "technique": "Header extraction"
    }
  ]
}