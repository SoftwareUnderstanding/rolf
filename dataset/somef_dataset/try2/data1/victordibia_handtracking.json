{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This work also served as an intense weekend crash course for me to learn Python and Tensorflow. It would be impossible without the Egohands Dataset, many thanks to the authors! The tensorflow custom object detection guides by [Harrison from pythonprogramming](https://pythonprogramming.net/training-custom-objects-tensorflow-object-detection-api-tutorial/) and [Dat Tran](https://towardsdatascience.com/how-to-train-your-own-object-detector-with-tensorflows-object-detector-api-bec72ecfe1d9) were immensely helpful to this learning process. And ofcourse, many thanks to the Tensorflow authors! Its a great frameworks!\n\n",
      "technique": "Header extraction"
    }
  ],
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Some related and referenced papers.\n\n\n\nBambach, S., Lee, S., Crandall, D. J., and Yu, C. 2015. \u201cLending A Hand: Detecting Hands and Recognizing Activities in Complex Egocentric Interactions,\u201d in ICCV, pp. 1949\u20131957 (available at https://www.cv-foundation.org/openaccess/content_iccv_2015/html/Bambach_Lending_A_Hand_ICCV_2015_paper.html).\n\nErol, A., Bebis, G., Nicolescu, M., Boyle, R. D., and Twombly, X. 2007. \u201cVision-based hand pose estimation: A review,\u201d Computer Vision and Image Understanding (108:1\u20132), pp. 52\u201373 (doi: 10.1016/j.cviu.2006.10.012).\n\nLiu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C. Y., and Berg, A. C. 2016. \u201cSSD: Single shot multibox detector,\u201d in European conference on computer vision (Vol. 9905 LNCS), Springer Cham, pp. 21\u201337 (doi: 10.1007/978-3-319-46448-0_2).\n\nBetancourt, A., Morerio, P., Regazzoni, C. S., and Rauterberg, M. 2015. \u201cThe Evolution of First Person Vision Methods: A Survey,\u201d IEEE Transactions on Circuits and Systems for Video Technology (25:5), pp. 744\u2013760 (doi: 10.1109/TCSVT.2015.2409731)\n\nApache Licence. See [LICENSE](LICENSE) for details. Copyright (c) 2017 Victor Dibia.\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "> Paper abstract of the [paper is here](https://github.com/victordibia/handtracking/tree/master/docs/handtrack.pdf). (a full paper will be added when complete).\n\nIf you use this code in your project/paper/research and would like to cite this work, use the below.\n\nVictor Dibia, HandTrack: A Library For Prototyping Real-time Hand TrackingInterfaces using Convolutional Neural Networks, https://github.com/victordibia/handtracking\n\n\n\n```bib\n@article{Dibia2017,\n  author = {Victor, Dibia},\n  title = {HandTrack: A Library For Prototyping Real-time Hand TrackingInterfaces using Convolutional Neural Networks},\n  year = {2017},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  url = {https://github.com/victordibia/handtracking/tree/master/docs/handtrack.pdf}, \n}\n```\n\n\nP.S. I came across this excellent [repository](https://github.com/xinghaochen/awesome-hand-pose-estimation) for those interested in `hand pose estimation`. There are several great papers and videos there, check them out!\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{Dibia2017,\n  author = {Victor, Dibia},\n  title = {HandTrack: A Library For Prototyping Real-time Hand TrackingInterfaces using Convolutional Neural Networks},\n  year = {2017},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  url = {https://github.com/victordibia/handtracking/tree/master/docs/handtrack.pdf}, \n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9646015035409302
      ],
      "excerpt": "If you use this tutorial or models in your research or project, please cite this. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9230513810226604
      ],
      "excerpt": "Realtime detection on video stream from a webcam . \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9660247242643438
      ],
      "excerpt": "Detection on a Youtube video. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.810250485749543
      ],
      "excerpt": "| 11  | 640 * 480  | Macbook pro (i7, 2.5GHz, 16GB) | Run while visualizing results (image above) | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/victordibia/handtracking",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-11-26T05:47:39Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-20T08:27:19Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9151163935300748,
        0.9800127614153952
      ],
      "excerpt": "This repo documents steps and scripts used to train a hand detector using Tensorflow (Object Detection API). As with any DNN based task, the most expensive (and riskiest) part of the process has to do with finding or creating the right (annotated) dataset. I was interested mainly in detecting hands on a table (egocentric view point). I experimented first with the Oxford Hands Dataset (the results were not good). I then tried the Egohands Dataset which was a much better fit to my requirements. \nThe goal of this repo/post is to demonstrate how neural networks can be applied to the (hard) problem of tracking hands (egocentric and other views). Better still, provide code that can be adapted to other uses cases. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8251342935809535
      ],
      "excerpt": "Here is the detector in action. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8679053826243281,
        0.8988075339868843
      ],
      "excerpt": "Note: The code in this repo is written and tested with Tensorflow 1.4.0-rc0. Using a different version may result in some errors. \nYou may need to generate your own frozen model graph using the model checkpoints in the repo to fit your TF version. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "    --model-checkpoint/model.ckpt-200002 \\  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9794727350874087
      ],
      "excerpt": "Content of this document \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8994647305968726,
        0.8372396012058242,
        0.8523663670869708,
        0.9645542564141741,
        0.9669017739225717,
        0.9912728369655727
      ],
      "excerpt": "- Using the Detector to Detect/Track hands \n- Thoughts on Optimizations. \nP.S if you are using or have used the models provided here, feel free to reach out on twitter (@vykthur) and share your work! \nThere are several existing approaches to tracking hands in the computer vision domain. Incidentally, many of these approaches are rule based (e.g extracting background based on texture and boundary features, distinguishing between hands and background using color histograms and HOG classifiers,) making them not very robust. For example, these algorithms might get confused if the background is unusual or in situations where sharp changes in lighting conditions cause sharp changes in skin color or the tracked object becomes occluded.(see here for a review paper on hand pose estimation from the HCI perspective) \nWith sufficiently large datasets, neural networks provide opportunity to train models that perform well and address challenges of existing object tracking/detection algorithms - varied/poor lighting, noisy environments, diverse viewpoints and even occlusion. The main drawbacks to usage for real-time tracking/detection is that they can be complex, are relatively slow compared to tracking-only algorithms and it can be quite expensive to assemble a good dataset. But things are changing with advances in fast neural networks. \nFurthermore, this entire area of work has been made more approachable by deep learning frameworks (such as the tensorflow object detection api) that simplify the process of training a model for custom object detection. More importantly, the advent of fast neural network models like ssd, faster r-cnn, rfcn (see here ) etc make neural networks an attractive candidate for real-time detection (and tracking) applications. Hopefully, this repo demonstrates this. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8805160225269303,
        0.9853032027231529
      ],
      "excerpt": "Training a model is a multi-stage process (assembling dataset, cleaning, splitting into training/test partitions and generating an inference graph). While I lightly touch on the details of these parts, there are a few other tutorials cover training a custom object detector using the tensorflow object detection api in more detail[ see here and here ]. I recommend you walk through those if interested in training a custom object detector from scratch. \nNow that the dataset has been assembled (and your tfrecords), the next task is to train a model based on this. With neural networks, it is possible to use a process called transfer learning to shorten the amount of time needed to train the entire model. This means we can take an existing model (that has been trained well on a related domain (here image classification) and retrain its final layer(s) to detect hands for us. Sweet!. Given that neural networks sometimes have thousands or millions of parameters that can take weeks or months to train, transfer learning helps shorten training time to possibly hours. Tensorflow does offer a few models (in the tensorflow model zoo) and I chose to use the ssd_mobilenet_v1_coco model as my start point given it is currently (one of) the fastest models (read the SSD research paper here). The training process can be done locally on your CPU machine which may take a while or better on a (cloud) GPU machine (which is what I did). For reference, training on my macbook pro (tensorflow compiled from source to take advantage of the mac's cpu architecture) the maximum speed I got was 5 seconds per step as opposed to the ~0.5 seconds per step I got with a GPU. For reference it would take about 12 days to run 200k steps on my mac (i7, 2.5GHz, 16GB) compared to ~5hrs on a GPU. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9644677429357323
      ],
      "excerpt": "As the training process progresses, the expectation is that total loss (errors) gets reduced to its possible minimum (about a value of 1 or thereabout). By observing the tensorboard graphs for total loss(see image below), it should be possible to get an idea of when the training process is complete (total loss does not decrease with further iterations/steps). I ran my training job for 200k steps (took about 5 hours) and stopped at a total Loss (errors) value of 2.575.(In retrospect, I could have stopped the training at about 50k steps and gotten a similar total loss value). With tensorflow, you can also run an evaluation concurrently that assesses your model to see how well it performs on the test data. A commonly used metric for performance is mean average precision (mAP) which is single number used to summarize the area under the precision-recall curve.  mAP is a measure of how well the model generates a bounding box that has at least a 50% overlap with the ground truth bounding box in our test dataset. For the hand detector trained here, the mAP value was 0.9686@0.5IOU. mAP values range from 0-1, the higher the better.   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9124965406046404
      ],
      "excerpt": "Once training is completed, the trained inference graph (frozen_inference_graph.pb) is then exported (see the earlier referenced guides for how to do this) and saved in the hand_inference_graph folder. Now its time to do some interesting detection. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8307623530222488
      ],
      "excerpt": "Load the frozen_inference_graph.pb trained on the hands dataset as well as the corresponding label map. In this repo, this is done in the utils/detector_utils.py script by the load_inference_graph method. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9567588029116127
      ],
      "excerpt": "    with detection_graph.as_default(): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9319708861235405
      ],
      "excerpt": "        with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.865073115025059
      ],
      "excerpt": "This repo contains two scripts that tie all these steps together. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9205077326946317
      ],
      "excerpt": "Update: If you do have errors loading the frozen inference graph in this repo, feel free to generate a new graph that fits your TF version from the model-checkpoint in this repo. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8958348047071314
      ],
      "excerpt": "More guidance on this here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8575976351558106
      ],
      "excerpt": "Threading: Turns out that reading images from a webcam is a heavy I/O event and if run on the main application thread can slow down the program. I implemented some good ideas from Adrian Rosebuck on parrallelizing image capture across multiple worker threads. This mostly led to an FPS increase of about 5 points. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.806137802065811,
        0.9041173590064704,
        0.9698295975112151
      ],
      "excerpt": "Keeping your input image small will increase fps without any significant accuracy drop.(I used about 320 x 240 compared to the 1280 x 720 which my webcam provides). \nModel Quantization. Moving from the current 32 bit to 8 bit can achieve up to 4x reduction in memory required to load and store models. One way to further speed up this model is to explore the use of 8-bit fixed point quantization. \nPerformance can also be increased by a clever combination of tracking algorithms with the already decent detection and this is something I am still experimenting with. Have ideas for optimizing better, please share! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Building a Real-time Hand-Detector using Neural Networks (SSD) on Tensorflow",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/victordibia/handtracking/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 445,
      "date": "Tue, 21 Dec 2021 05:17:20 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/victordibia/handtracking/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "victordibia/handtracking",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/victordibia/handtracking/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "**The Egohands Dataset**\n\nThe hand detector model is built using data from the [Egohands Dataset](http://vision.soic.indiana.edu/projects/egohands/) dataset. This dataset works well for several reasons. It contains high quality, pixel level annotations (>15000 ground truth labels) where hands are located across 4800 images. All images are captured from an egocentric view (Google glass) across 48 different environments (indoor, outdoor) and activities (playing cards, chess, jenga, solving puzzles etc).\n\n<img src=\"images/egohandstrain.jpg\" width=\"100%\">\n\nIf you will be using the Egohands dataset, you can cite them as follows:\n\n> Bambach, Sven, et al. \"Lending a hand: Detecting hands and recognizing activities in complex egocentric interactions.\" Proceedings of the IEEE International Conference on Computer Vision. 2015.\n\nThe Egohands dataset (zip file with labelled data) contains 48 folders of locations where video data was collected (100 images per folder).\n```\n-- LOCATION_X\n  -- frame_1.jpg\n  -- frame_2.jpg\n  ...\n  -- frame_100.jpg\n  -- polygons.mat  // contains annotations for all 100 images in current folder\n-- LOCATION_Y\n  -- frame_1.jpg\n  -- frame_2.jpg\n  ...\n  -- frame_100.jpg\n  -- polygons.mat  // contains annotations for all 100 images in current folder\n  ```\n\n**Converting data to Tensorflow Format**\n\nSome initial work needs to be done to the Egohands dataset to transform it into the format (`tfrecord`) which Tensorflow needs to train a model. This repo contains `egohands_dataset_clean.py` a script that will help you generate these csv files.\n\n- Downloads the egohands datasets\n- Renames all files to include their directory names to ensure each filename is unique\n- Splits the dataset into train (80%), test (10%) and eval (10%) folders.\n- Reads in `polygons.mat` for each folder, generates bounding boxes and visualizes them to ensure correctness (see image above).\n- Once the script is done running, you should have an images folder containing three folders - train, test and eval. Each of these folders should also contain a csv label document each - `train_labels.csv`, `test_labels.csv`  that can be used to generate `tfrecords`\n\n> `python egohands_dataset_clean.py`\n\nNote: While the egohands dataset provides four separate labels for hands (own left, own right, other left, and other right), for my purpose, I am only interested in the general `hand` class and label all training data as `hand`. You can modify the data prep script to generate `tfrecords` that support 4 labels.\n\nNext: convert your dataset + csv files to tfrecords. A helpful guide on this can be found [here](https://pythonprogramming.net/creating-tfrecord-files-tensorflow-object-detection-api-tutorial/).For each folder, you should be able to generate  `train.record`, `test.record` required in the training process.\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8354003181971899
      ],
      "excerpt": "Both examples above were run on a macbook pro CPU (i7, 2.5GHz, 16GB). Some fps numbers are: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8559777622699289
      ],
      "excerpt": "The tensorflow object detection repo has a python file for exporting a checkpoint to frozen graph here.  You can copy it to the current directory and use it as follows \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8837680365796365
      ],
      "excerpt": "  python \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8837680365796365
      ],
      "excerpt": "  python \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8895456601305671
      ],
      "excerpt": "<img src=\"images/hand1.gif\" width=\"33.3%\"><img src=\"images/hand2.gif\" width=\"33.3%\"><img src=\"images/hand3.gif\" width=\"33.3%\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8895456601305671
      ],
      "excerpt": "<img src=\"images/chess1.gif\" width=\"33.3%\"><img src=\"images/chess2.gif\" width=\"33.3%\"><img src=\"images/chess3.gif\" width=\"33.3%\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991
      ],
      "excerpt": "python3 export_inference_graph.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8124741446138858,
        0.8040454297027735
      ],
      "excerpt": "    --model-checkpoint/ssd_mobilenet_v1_pets.config \\ \n    --model-checkpoint/model.ckpt-200002 \\  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9090441646139499
      ],
      "excerpt": "- Data preparation and network training in Tensorflow (Dataset, Import, Training) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9128918385403588
      ],
      "excerpt": "<img src=\"images/accuracy.jpg\" width=\"100%\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123763140827432,
        0.8134972907485951,
        0.8329015283702058
      ],
      "excerpt": "        od_graph_def = tf.GraphDef() \n        with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid: \n            serialized_graph = fid.read() \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8878024139826244
      ],
      "excerpt": "            tf.import_graph_def(od_graph_def, name='') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9273667005804193
      ],
      "excerpt": "<img src=\"images/general.jpg\" width=\"100%\"> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/victordibia/handtracking/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Starlark"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 Victor Dibia\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Real-time Hand-Detection using Neural Networks (SSD) on Tensorflow.",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "handtracking",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "victordibia",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/victordibia/handtracking/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1509,
      "date": "Tue, 21 Dec 2021 05:17:20 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "tensorflow",
      "hand-detector",
      "detector",
      "hand-detection",
      "neural-network",
      "computer-vision",
      "ssd"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "I exported the model using the Tensorflow.js converter and have it wrapped into an easy to use javascript library - [Handtrack.js](https://victordibia.github.io/handtrack.js/). You can do hand tracking in 3 lines of code, no installation, no model training, all in the browser.\n\n<img src=\"images/doodle.gif\" width=\"100%\">\n\nLearn more below\n\n- Blog Post:  [Hand Tracking Interactions in the Browser using Tensorflow.js and 3 lines of code.](https://medium.com/@victor.dibia/handtrackjs-677c29c1d585)\n- Github: [Handtrack.js Github Repo](https://github.com/victordibia/handtrack.js)\n- Live Demo : [Handtrack.js Examples in the Browser](https://victordibia.github.io/handtrack.js/)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "![android_sample_1](images/android_sample_1.png)\n\nThe trained model checkpoints are converted to the [TensorFlow Lite](https://www.tensorflow.org/lite) format so that they can used in both Android and iOS apps. \n\nThe Android app which uses the hand tracking model from this repo is available here -> [shubham0204/Hand_Detection_TFLite_Android](https://github.com/shubham0204/Hand_Detection_TFLite_Android)\n\nAlso, a step-by-step guide on how to convert the model checkpoints to a TFLite model ( `.tflite` ) is available as a IPYNB notebook ( open it in Google Colab ) -> [shubham0204/Google_Colab_Notebooks/Hand_Tracking_Model_TFLite_Conversion.ipynb](https://github.com/shubham0204/Google_Colab_Notebooks/blob/main/Hand_Tracking_Model_TFLite_Conversion.ipynb)\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "A few people have used the handtracking sample code/models in creating some awesome projects and I'd like to highlight them here!\n- (Alphabot)[https://github.com/drewgillson/alphabot]: a screen-less interactive spelling primer powered by computer vision\n- (Wall Z the Robot)[https://challengerocket.com/megatran/Wall-Z-the-Robot-8a34db.html]\n- (Predicting hand pose)[https://github.com/MrEliptik/HandPose] : Using the output of a hand detector to predict hand pose by Victor Meunier.\n- (Hand Tracking Pong)[https://github.com/alvinwan/hand-tracking-pong]: Hand Tracking ping pong.\n- (AiryDraw!)[https://github.com/amirhossein-ahmadian/airydraw] is a simple augmented reality program which gives you the feeling that you can draw in the air just using your hand.\n\n- (Gesture Recognition)[https://github.com/zzeitt/Gesture-Recognition] Uses this repo to  extract hand bounding boxes and runs a classifier on top of the extracted hand to detect hand gestures (fist, two fingers and open palm. Neat!)\n\n- (Video Gesture Recognition)[https://github.com/ashwatc/Video_Gesture_Overlay] and Overlay (Using Machine Learning and Computer Vision). This project uses the handtracking models to prototype quick controls for video conferencing scenarios (e.g. I agree, yes, or stepping away from the video call). Check it out!\n\nIf you have created something cool, send me a note (or (tweet)[https://twitter.com/vykthur]) and I'll be happy to include it here!\n\n",
      "technique": "Header extraction"
    }
  ]
}