{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[image1]: https://user-images.githubusercontent.com/10624937/42135623-e770e354-7d12-11e8-998d-29fc74429ca2.gif \"Trained Agent\"\n[image2]: https://user-images.githubusercontent.com/10624937/42135622-e55fb586-7d12-11e8-8a54-3c31da15a90a.gif \"Soccer\"\n[image3]: \"tennis\"\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.996819264442358
      ],
      "excerpt": "Multi Agent PPO as presented in this paper (https://arxiv.org/pdf/1710.03748.pdf) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/vgudapati/DRLND_Collaboration_Competetion",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-01-14T11:14:34Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-17T08:27:42Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The objective of the project is to train the agents to play Tennis using the [Tennis](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#tennis) environment.\n\n\n![tennis](images/trained_agent.gif)\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9887586092487681,
        0.9874232373116522,
        0.8777176990411285,
        0.8990420647077751
      ],
      "excerpt": "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play. \nThe observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Each agent receives its own, local observation.  Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \nIn order to solve the environment, the agents must get an average score of +0.5 (over 100 consecutive episodes, after taking the maximum over both agents). Specifically, \nAfter each episode, we add up the rewards that each agent received (without discounting), to get a score for each agent. This yields 2 (potentially different) scores. We then take the maximum of these 2 scores. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9437894947513777,
        0.9010588327670369,
        0.9232835613387185
      ],
      "excerpt": "The environment is considered solved, when the average (over 100 episodes) of those scores is at least +0.5. \nFollow the instructions in Tennis.ipynb to get started with training your own agent! \nIn addition to the current work, we can do the following to improve performance: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9513190970617259
      ],
      "excerpt": "Multi Agent DQN as presented in this report (http://cs231n.stanford.edu/reports/2016/pdfs/122_Report.pdf). While using MADQN, we can try various combinations of DQN algorithms and as we know the most effective one is the rainbow method. This significantly improves performance on DQN networks.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Multi Agent RL project (Tennis) using MADDPG for Udacity Deep Reinforcement Learning Nano Degree program",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/vgudapati/DRLND_Collaboration_Competetion/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Mon, 27 Dec 2021 11:22:43 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/vgudapati/DRLND_Collaboration_Competetion/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "vgudapati/DRLND_Collaboration_Competetion",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/vgudapati/DRLND_Collaboration_Competetion/master/Tennis.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Clone the repository from https://github.com/vgudapati/DRLND_Collaboration_Competetion.git\n2. Setup the dependencies as described [here](https://github.com/udacity/deep-reinforcement-learning/blob/master/README.md).\n3. Download the environment from one of the links below.  You need only select the environment that matches your operating system:\n    - Linux: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Linux.zip)\n    - Mac OSX: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis.app.zip)\n    - Windows (32-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Windows_x86.zip)\n    - Windows (64-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Windows_x86_64.zip)\n4. Place the file in the DRLND GitHub repository, in the `DRLND_Collaboration_Competetion` folder, and unzip (or decompress) the file. \n\n",
      "technique": "Header extraction"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/vgudapati/DRLND_Collaboration_Competetion/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "(Image References)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "DRLND_Collaboration_Competetion",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "vgudapati",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/vgudapati/DRLND_Collaboration_Competetion/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 4,
      "date": "Mon, 27 Dec 2021 11:22:43 GMT"
    },
    "technique": "GitHub API"
  }
}