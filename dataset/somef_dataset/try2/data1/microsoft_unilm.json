{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2101.07597",
      "https://arxiv.org/abs/2110.07205",
      "https://arxiv.org/abs/2106.16138\" target=\"_blank\">XLM-E</a>/<a href=\"https://arxiv.org/abs/2007.07834\" target=\"_blank\">InfoXLM</a>",
      "https://arxiv.org/abs/2007.07834\" target=\"_blank\">InfoXLM</a>",
      "https://arxiv.org/abs/2012.14740",
      "https://arxiv.org/abs/2007.07834",
      "https://arxiv.org/abs/2110.08518",
      "https://arxiv.org/abs/2109.10282",
      "https://arxiv.org/abs/2108.11591",
      "https://arxiv.org/abs/2106.13736",
      "https://arxiv.org/abs/2106.08254",
      "https://arxiv.org/abs/2104.08836",
      "https://arxiv.org/abs/2012.15828",
      "https://arxiv.org/abs/2012.14740",
      "https://arxiv.org/abs/1912.13318",
      "https://arxiv.org/abs/2002.10957",
      "https://arxiv.org/abs/2002.12804",
      "https://arxiv.org/abs/1905.03197"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8742079650496138
      ],
      "excerpt": "Large-scale self-supervised pre-training across tasks (predictive and generative), languages (100+ languages), and modalities (language, image, audio, layout/format + language, vision + language, audio + language, etc.) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9096875621580559
      ],
      "excerpt": "UniLM: unified pre-training for language understanding and generation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8989331235955447
      ],
      "excerpt": "DeltaLM/mT6: encoder-decoder pre-training for language generation and translation for 100+ languages \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8769891646593567
      ],
      "excerpt": "November, 2021: Multilingual translation at scale: 10000 language pairs and beyond \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.847178406627917
      ],
      "excerpt": "[Model Release] November, 2021: VLMo - Unified vision-language pre-training w/ BEiT \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9998854256272499
      ],
      "excerpt": "September 28th, 2021: T-ULRv5 (aka <a href=\"https://arxiv.org/abs/2106.16138\" target=\"_blank\">XLM-E</a>/<a href=\"https://arxiv.org/abs/2007.07834\" target=\"_blank\">InfoXLM</a>) as the SOTA on the <a href=\"https://sites.research.google/xtreme\" target=\"_blank\">XTREME</a> leaderboard. // <a href=\"https://www.microsoft.com/en-us/research/blog/microsoft-turing-universal-language-representation-model-t-ulrv5-tops-xtreme-leaderboard-and-trains-100x-faster/\" target=\"_blank\">Blog</a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8114219188120323
      ],
      "excerpt": "[Model Release] August, 2021: DeltaLM - Encoder-decoder pre-training for language generation and translation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.887167692142383
      ],
      "excerpt": "July 16, 2020: InfoXLM (Multilingual UniLM) arXiv \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9896766473309723
      ],
      "excerpt": "September, 2019: UniLMv1 was accepted by NeurIPS 2019. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8511535834563826
      ],
      "excerpt": "***** June, 2021: LayoutXLM | AdaLM | MiniLMv2 release ***** \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9524130631085533
      ],
      "excerpt": "***** October 1st, 2019: UniLM v1 release ***** \n",
      "technique": "Supervised classification"
    }
  ],
  "codeOfConduct": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://raw.githubusercontent.com/microsoft/unilm/master/CODE_OF_CONDUCT.md",
    "technique": "File Exploration"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/microsoft/unilm",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\nFor help or issues using the pre-trained models, please submit a GitHub issue.\r\n\r\nFor other communications, please contact [Furu Wei](http://gitnlp.org/) (`fuwei@microsoft.com`).\r\n\r\n",
      "technique": "Header extraction"
    }
  ],
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contributing to Facebook AI Research Sequence-to-Sequence Toolkit (fairseq)\nWe want to make contributing to this project as easy and transparent as\npossible.\nPull Requests\nWe actively welcome your pull requests.\n\nFork the repo and create your branch from master.\nIf you've added code that should be tested, add tests.\nIf you've changed APIs, update the documentation.\nEnsure the test suite passes.\nMake sure your code lints.\nIf you haven't already, complete the Contributor License Agreement (\"CLA\").\n\nContributor License Agreement (\"CLA\")\nIn order to accept your pull request, we need you to submit a CLA. You only need\nto do this once to work on any of Facebook's open source projects.\nComplete your CLA here: https://code.facebook.com/cla\nIssues\nWe use GitHub issues to track public bugs. Please ensure your description is\nclear and has sufficient instructions to be able to reproduce the issue.\nLicense\nBy contributing to Facebook AI Research Sequence-to-Sequence Toolkit (fairseq),\nyou agree that your contributions will be licensed under the LICENSE file in\nthe root directory of this source tree.",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-07-23T04:15:28Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-27T16:40:51Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8368177042119571
      ],
      "excerpt": "AdaLM: domain, language, and task adaptation of pre-trained models \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8257894678291335
      ],
      "excerpt": "WavLM (NEW): speech pre-training for full stack tasks \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8632524868249908
      ],
      "excerpt": "MarkupLM (NEW): markup language model pre-training for visually-rich document understanding \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8443255073666726
      ],
      "excerpt": "VLMo (NEW): Unified vision-language pre-training - evolution of BEiT to multimodal \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8119946935341826
      ],
      "excerpt": "LayoutReader: pre-training of text and layout for reading order detection \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9823719919467689,
        0.8927895134032688
      ],
      "excerpt": "[Model Release] December 16th, 2021: TrOCR small models for handwritten and printed texts, with 3x inference speedup. \nNovember 24th, 2021: VLMo as the new SOTA on the VQA Challenge \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8217370397064988
      ],
      "excerpt": "[Model Release] November, 2021: MarkupLM \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9387585156978075
      ],
      "excerpt": "October, 2021: WavLM Large achieves state-of-the-art performance on the SUPERB benchmark \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.975182764178763
      ],
      "excerpt": "[Model Release] October 2021: TrOCR is on HuggingFace \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9227131723480954,
        0.8154741529431266,
        0.9378061233684437,
        0.9110012585867872
      ],
      "excerpt": "[Model Release] September, 2021: LayoutLM-cased are on HuggingFace \n[Model Release] September, 2021: TrOCR - Transformer-based OCR w/ pre-trained BEiT and RoBERTa models. \nAugust 2021: LayoutLMv2 and LayoutXLM are on HuggingFace \n[Model Release] August, 2021: LayoutReader - Built with LayoutLM to improve general reading order detection. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9868209214679863
      ],
      "excerpt": "August 2021: BEiT is on HuggingFace \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9031973346095965
      ],
      "excerpt": "[Model Release] June, 2021: LayoutLMv2, LayoutXLM, MiniLMv2, and AdaLM. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9718244058597402
      ],
      "excerpt": "April, 2021: LayoutXLM is coming by extending the LayoutLM into multilingual support! A multilingual form understanding benchmark XFUND is also introduced, which includes forms with human labeled key-value pairs in 7 languages (Chinese, Japanese, Spanish, French, Italian, German, Portuguese). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9914489010857002,
        0.9474562402759235
      ],
      "excerpt": "December 29th, 2020: LayoutLMv2 is coming with the new SOTA on a wide varierty of document AI tasks, including DocVQA and SROIE leaderboard. \nOctober 8th, 2020: T-ULRv2 (aka InfoXLM) as the SOTA on the XTREME leaderboard. // Blog \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8604187321779371,
        0.9934936290903233
      ],
      "excerpt": "[x] WavLM (October 27, 2021):  WavLM, a new pre-trained speech model, to solve full-stack downstream speech tasks.  \nWavLM integrates the gated relative position embedding structure and the utterance mixing method, to model both spoken content and speaker identity preservation. WavLM is trained on  94k hours of public audio data, which is larger than other released checkpoints for English Speech modeling. WavLM Large achieves state-of-the-art performance on the SUPERB benchmark, and brings significant improvements for various speech processing tasks on their representative benchmarks. \"WavLM: Large-Scale Self-Supervised  Pre-training   for Full Stack Speech Processing\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9846826359328377
      ],
      "excerpt": "[x] MarkupLM (October 19, 2021): MarkupLM, a simple yet effective pre-training approach for text and markup language. With the Transformer architecture, MarkupLM integrates different input embeddings including text embeddings, position embeddings, and XPath embeddings. Furthermore, we also propose new pre-training objectives that are specially designed for understanding the markup language. We evaluate the pre-trained MarkupLM model on the WebSRC and SWDE datasets. Experiments show that MarkupLM significantly outperforms several SOTA baselines in these tasks. \"MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document Understanding\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9355424008239067
      ],
      "excerpt": "[x] TrOCR (September 22, 2021): Transformer-based OCR with pre-trained models, which leverages the Transformer architecture for both image understanding and bpe-level text generation. The TrOCR model is simple but effective (convolution free), and can be pre-trained with large-scale synthetic data and fine-tuned with human-labeled datasets. \"TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8771966746490473
      ],
      "excerpt": "[x] LayoutReader (August 26, 2021): pre-training of text and layout for reading order detection. The pre-trained LayoutReader significantly improves both open-source and commercial OCR engines in ordering text lines. Meanwhile, we also created a reading order benchmark dataset ReadingBank to further empower the research in this area. \"LayoutReader: Pre-training of Text and Layout for Reading Order Detection EMNLP 2021\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9618819769591738
      ],
      "excerpt": "[x] BEiT (June 15, 2021): BERT Pre-Training of Image Transformers. BEiT-large achieves state-of-the-art results on ADE20K (a big jump to 57.0 mIoU) for semantic segmentation. BEiT-large achieves state-of-the-art ImageNet top-1 accuracy (88.6%) under the setting without extra data other than ImageNet-22k. \"BEiT: BERT Pre-Training of Image Transformers\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8623708286257306
      ],
      "excerpt": "[x] AdaLM (June 2021): a simple yet effective approach for domain adaptation of pre-trained models. Biomedical specific pre-trained models are released. \"Adapt-and-Distill: Developing Small, Fast and Effective Pretrained Language Models for Domains ACL 2021\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9346941447843191
      ],
      "excerpt": "[x] LayoutLM 2.0 (December 29, 2020): multimodal pre-training for visually-rich document understanding by leveraging text, layout and image information in a single framework. It is coming with new SOTA on a wide range of document understanding tasks, including FUNSD (0.7895 -> 0.8420), CORD (0.9493 -> 0.9601), SROIE (0.9524 -> 0.9781), Kleister-NDA (0.834 -> 0.852), RVL-CDIP (0.9443 -> 0.9564), and DocVQA (0.7295 -> 0.8672). \"LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding ACL 2021\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8771052466893284
      ],
      "excerpt": "[x] LayoutLM 1.0 (February 18, 2020): pre-trained models for document (image) understanding (e.g. receipts, forms, etc.) . It achieves new SOTA results in several downstream tasks, including form understanding (the FUNSD dataset from 70.72 to 79.27), receipt understanding (the ICDAR 2019 SROIE leaderboard from 94.02 to 95.24) and document image classification (the RVL-CDIP dataset from 93.07 to 94.42). \"LayoutLM: Pre-training of Text and Layout for Document Image Understanding KDD 2020\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8966304875097314,
        0.9218934194327509
      ],
      "excerpt": "[x] MiniLM 1.0 (February 26, 2020): deep self-attention distillation is all you need (for task-agnostic knowledge distillation of pre-trained Transformers). MiniLM (12-layer, 384-hidden) achieves 2.7x speedup and comparable results over BERT-base (12-layer, 768-hidden) on NLU tasks as well as strong results on NLG tasks. The even smaller MiniLM (6-layer, 384-hidden) obtains 5.3x speedup and produces very competitive results. \"MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers NeurIPS 2020\" \n[x] UniLM 2.0 (February 28, 2020): unified pre-training of bi-directional LM (via autoencoding) and sequence-to-sequence LM (via partially autoregressive) w/ Pseudo-Masked Language Model for language understanding and generation. UniLM v2 achieves new SOTA in a wide range of natural language understanding and generation tasks. \"UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training ICML 2020\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Large-scale Self-supervised Pre-training Across Tasks, Languages, and Modalities",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/microsoft/unilm/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 825,
      "date": "Mon, 27 Dec 2021 18:14:29 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/microsoft/unilm/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "microsoft/unilm",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/microsoft/unilm/tree/master/infoxlm/fairseq/docs"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/microsoft/unilm/master/trocr/pic_inference.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/microsoft/unilm/master/layoutlm/deprecated/examples/seq_labeling/preprocess.sh",
      "https://raw.githubusercontent.com/microsoft/unilm/master/beit/semantic_segmentation/tools/dist_test.sh",
      "https://raw.githubusercontent.com/microsoft/unilm/master/beit/semantic_segmentation/tools/dist_train.sh",
      "https://raw.githubusercontent.com/microsoft/unilm/master/xtune/scripts/train.sh",
      "https://raw.githubusercontent.com/microsoft/unilm/master/xtune/scripts/download_data.sh",
      "https://raw.githubusercontent.com/microsoft/unilm/master/xtune/scripts/preprocess_udpos.sh",
      "https://raw.githubusercontent.com/microsoft/unilm/master/xtune/scripts/preprocess_panx.sh",
      "https://raw.githubusercontent.com/microsoft/unilm/master/xtune/scripts/download_model.sh",
      "https://raw.githubusercontent.com/microsoft/unilm/master/xtune/scripts/cross-lingual-transfer/train_xquad.sh",
      "https://raw.githubusercontent.com/microsoft/unilm/master/xtune/scripts/cross-lingual-transfer/train_xnli.sh",
      "https://raw.githubusercontent.com/microsoft/unilm/master/xtune/scripts/cross-lingual-transfer/train_tydiqa.sh",
      "https://raw.githubusercontent.com/microsoft/unilm/master/xtune/scripts/cross-lingual-transfer/train_pawsx.sh",
      "https://raw.githubusercontent.com/microsoft/unilm/master/xtune/scripts/cross-lingual-transfer/train_panx.sh",
      "https://raw.githubusercontent.com/microsoft/unilm/master/xtune/scripts/cross-lingual-transfer/train_mlqa.sh",
      "https://raw.githubusercontent.com/microsoft/unilm/master/xtune/scripts/cross-lingual-transfer/train_udpos.sh",
      "https://raw.githubusercontent.com/microsoft/unilm/master/xtune/scripts/translate-train-all/train_xquad.sh",
      "https://raw.githubusercontent.com/microsoft/unilm/master/xtune/scripts/translate-train-all/train_xnli.sh",
      "https://raw.githubusercontent.com/microsoft/unilm/master/xtune/scripts/translate-train-all/train_tydiqa.sh",
      "https://raw.githubusercontent.com/microsoft/unilm/master/xtune/scripts/translate-train-all/train_pawsx.sh",
      "https://raw.githubusercontent.com/microsoft/unilm/master/xtune/scripts/translate-train-all/train_panx.sh",
      "https://raw.githubusercontent.com/microsoft/unilm/master/xtune/scripts/translate-train-all/train_mlqa.sh",
      "https://raw.githubusercontent.com/microsoft/unilm/master/xtune/scripts/translate-train-all/train_udpos.sh",
      "https://raw.githubusercontent.com/microsoft/unilm/master/infoxlm/fairseq/scripts/compound_split_bleu.sh",
      "https://raw.githubusercontent.com/microsoft/unilm/master/infoxlm/fairseq/scripts/sacrebleu_pregen.sh",
      "https://raw.githubusercontent.com/microsoft/unilm/master/infoxlm/fairseq/examples/language_model/prepare-wikitext-103.sh",
      "https://raw.githubusercontent.com/microsoft/unilm/master/infoxlm/fairseq/examples/speech_recognition/datasets/prepare-librispeech.sh",
      "https://raw.githubusercontent.com/microsoft/unilm/master/infoxlm/fairseq/examples/joint_alignment_translation/prepare-wmt18en2de_no_norm_no_escape_no_agressive.sh",
      "https://raw.githubusercontent.com/microsoft/unilm/master/infoxlm/fairseq/examples/roberta/preprocess_GLUE_tasks.sh",
      "https://raw.githubusercontent.com/microsoft/unilm/master/infoxlm/fairseq/examples/roberta/preprocess_RACE.sh",
      "https://raw.githubusercontent.com/microsoft/unilm/master/infoxlm/fairseq/examples/roberta/commonsense_qa/download_cqa_data.sh",
      "https://raw.githubusercontent.com/microsoft/unilm/master/infoxlm/fairseq/examples/translation/prepare-wmt14en2fr.sh",
      "https://raw.githubusercontent.com/microsoft/unilm/master/infoxlm/fairseq/examples/translation/prepare-iwslt14.sh",
      "https://raw.githubusercontent.com/microsoft/unilm/master/infoxlm/fairseq/examples/translation/prepare-iwslt17-multilingual.sh",
      "https://raw.githubusercontent.com/microsoft/unilm/master/infoxlm/fairseq/examples/translation/prepare-wmt14en2de.sh",
      "https://raw.githubusercontent.com/microsoft/unilm/master/deltalm/examples/train_iwslt14.sh",
      "https://raw.githubusercontent.com/microsoft/unilm/master/deltalm/examples/evaluate_iwslt14.sh",
      "https://raw.githubusercontent.com/microsoft/unilm/master/deltalm/examples/prepare_iwslt14.sh",
      "https://raw.githubusercontent.com/microsoft/unilm/master/deltalm/examples/binary_iwslt14.sh",
      "https://raw.githubusercontent.com/microsoft/unilm/master/deltalm/examples/spm_iwslt14.sh"
    ],
    "technique": "File Exploration"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/microsoft/unilm/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell",
      "Cuda",
      "C++",
      "Cython",
      "Lua",
      "Jupyter Notebook",
      "Makefile",
      "Batchfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) Facebook, Inc. and its affiliates.\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Large-scale Self-supervised Pre-training Across Tasks, Languages, and Modalities-->",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "unilm",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "microsoft",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/microsoft/unilm/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "donglixp",
        "body": "fix an issue running on CPUs: https://github.com/microsoft/unilm/commit/13641268b59df5cf90d27b451d87ab58b6a07055",
        "dateCreated": "2020-04-02T10:40:19Z",
        "datePublished": "2020-04-02T10:46:23Z",
        "html_url": "https://github.com/microsoft/unilm/releases/tag/s2s-ft.v0.3",
        "name": "s2s-ft version 0.3",
        "tag_name": "s2s-ft.v0.3",
        "tarball_url": "https://api.github.com/repos/microsoft/unilm/tarball/s2s-ft.v0.3",
        "url": "https://api.github.com/repos/microsoft/unilm/releases/25127561",
        "zipball_url": "https://api.github.com/repos/microsoft/unilm/zipball/s2s-ft.v0.3"
      },
      {
        "authorType": "User",
        "author_name": "donglixp",
        "body": "- Support MiniLM in s2s-ft",
        "dateCreated": "2020-03-14T13:35:09Z",
        "datePublished": "2020-03-13T00:30:06Z",
        "html_url": "https://github.com/microsoft/unilm/releases/tag/s2s-ft.v0.2",
        "name": "s2s-ft version 0.2",
        "tag_name": "s2s-ft.v0.2",
        "tarball_url": "https://api.github.com/repos/microsoft/unilm/tarball/s2s-ft.v0.2",
        "url": "https://api.github.com/repos/microsoft/unilm/releases/24485080",
        "zipball_url": "https://api.github.com/repos/microsoft/unilm/zipball/s2s-ft.v0.2"
      },
      {
        "authorType": "User",
        "author_name": "donglixp",
        "body": "seq2seq fine-tuning with unilmv1/unilmv2",
        "dateCreated": "2020-03-02T04:22:12Z",
        "datePublished": "2020-03-10T01:27:10Z",
        "html_url": "https://github.com/microsoft/unilm/releases/tag/s2s-ft.v0.0",
        "name": "s2s-ft version 0.0",
        "tag_name": "s2s-ft.v0.0",
        "tarball_url": "https://api.github.com/repos/microsoft/unilm/tarball/s2s-ft.v0.0",
        "url": "https://api.github.com/repos/microsoft/unilm/releases/24370869",
        "zipball_url": "https://api.github.com/repos/microsoft/unilm/zipball/s2s-ft.v0.0"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 4227,
      "date": "Mon, 27 Dec 2021 18:14:29 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "nlp",
      "language-understanding",
      "language-generation",
      "pre-trained-model",
      "small-pre-trained-model",
      "unilm",
      "minilm",
      "layoutlm",
      "s2s-ft",
      "infoxlm",
      "multimodal-pre-trained-model",
      "layoutxlm",
      "beit",
      "document-ai",
      "layoutreader",
      "ocr",
      "trocr",
      "text-recognition",
      "markuplm",
      "vlmo"
    ],
    "technique": "GitHub API"
  }
}