{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1706.03762 \"Attention Is All You Need\"\n[2]: https://arxiv.org/abs/1807.03819 \"Universal Transformers\"\n[3]: https://arxiv.org/abs/1810.04805 \"BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding\"\n[4]: https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf\n     \"Improving Language Understanding by Generative Pre-Training\"",
      "https://arxiv.org/abs/1807.03819 \"Universal Transformers\"\n[3]: https://arxiv.org/abs/1810.04805 \"BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding\"\n[4]: https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf\n     \"Improving Language Understanding by Generative Pre-Training\"",
      "https://arxiv.org/abs/1810.04805 \"BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding\"\n[4]: https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf\n     \"Improving Language Understanding by Generative Pre-Training\""
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8048856213960038
      ],
      "excerpt": "described in arguably one of the most impressive DL papers of 2017 and 2018: \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kpot/keras-transformer",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-09-26T19:25:15Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-06T14:12:47Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9150643420775911
      ],
      "excerpt": "for building (Universal) Transformer models using Keras, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8065224345117222,
        0.8720078544266223,
        0.908925214220865
      ],
      "excerpt": "of how it can be applied. \nThe library supports: \npositional encoding and embeddings, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.921912212796828,
        0.8449169253558676
      ],
      "excerpt": "a general implementation of [BERT][3] (because the Transformer \n  is mainly applied to NLP tasks). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9207839451013058
      ],
      "excerpt": "All pieces of the model (like self-attention, activation function, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8852872383935377,
        0.9340023645596258,
        0.8350262199593748,
        0.9109770294488916,
        0.8440175491925249
      ],
      "excerpt": "differently or replacing some of them. \nThe (Universal) Transformer is a deep learning architecture \ndescribed in arguably one of the most impressive DL papers of 2017 and 2018: \nthe \"[Attention is all you need][1]\" and the \"[Universal Transformers][2]\" \nby Google Research and Google Brain teams. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8466753518144876,
        0.923619930245938
      ],
      "excerpt": "which has inspired a big wave of new research models that keep coming ever since. \nThese models demonstrate new state-of-the-art results in various NLP tasks, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Keras library for building (Universal) Transformers, facilitating BERT and GPT models",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kpot/keras-transformer/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 137,
      "date": "Wed, 22 Dec 2021 01:53:05 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/kpot/keras-transformer/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "kpot/keras-transformer",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To install the library you need to clone the repository\n\n    git clone https://github.com/kpot/keras-transformer.git\n\nthen switch to the cloned directory and run pip\n\n    cd keras-transformer\n    pip install .\n\nPlease note that the project requires Python >= 3.6.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8661176197453521
      ],
      "excerpt": "    name='transformer', \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8661176197453521
      ],
      "excerpt": "    name='coordinate_embedding') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.961477756838743
      ],
      "excerpt": "you can build your version of Transformer, by re-arranging them \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8421074476017179
      ],
      "excerpt": "    name='transformer', \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "    use_masking=True) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8421074476017179,
        0.8789462919902599
      ],
      "excerpt": "    name='coordinate_embedding') \noutput = transformer_input # shape: (<batch size>, <sequence length>, <input size>) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8289669050403863
      ],
      "excerpt": "    output = transformer_block( \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/kpot/keras-transformer/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'The MIT License\\n\\nCopyright 2018 Kirill Mavreshko (https://www.linkedin.com/in/kirill-mavreshko/)\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is furnished\\nto do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included\\nin all copies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES\\nOF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\\nIN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\\nDAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\\nTORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE\\nOR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Keras-Transformer",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "keras-transformer",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "kpot",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kpot/keras-transformer/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 511,
      "date": "Wed, 22 Dec 2021 01:53:05 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This repository contains simple [examples](./example) showing how\nKeras-transformer works.\nIt's not a rigorous evaluation of the model's capabilities,\nbut rather a demonstration on how to use the code.\n\nThe code trains [simple language-modeling networks](./example/models.py) on the\n[WikiText-2](https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset)\ndataset and evaluates their perplexity. The model is either a [vanilla\nTransformer][1], or an [Adaptive Universal Transformer][2] (by default)\nwith five layers, each can be trained using either:\n\n* [Generative pre-training][4] (GPT), which involves using masked self-attention\n  to prevent the model from \"looking into the future\".\n* [BERT][3], which doesn't restrict self-attention, allowing the model\n  to fill the gaps using both left and right context.\n\n\nTo launch the code, you will first need to install the requirements listed\nin [example/requirements.txt](./example/requirements.txt). Assuming you work\nfrom a Python virtual environment, you can do this by running\n\n    pip install -r example/requirements.txt\n\nYou will also need to make sure you have a backend for Keras.\nFor instance, you can install Tensorflow (the sample was tested using\nTensorflow and PlaidML as backends):\n\n    pip install tensorflow\n\nNow you can launch the GPT example as\n\n    python -m example.run_gpt --save lm_model.h5\n\nto see all command line options and their default values, try\n\n    python -m example.run_gpt --help\n\nIf all goes well, after launching the example you should see\nthe perplexity falling with each epoch.\n\n    Building vocabulary: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 36718/36718 [00:04<00:00, 7642.33it/s]\n    Learning BPE...Done\n    Building BPE vocabulary: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 36718/36718 [00:06<00:00, 5743.74it/s]\n    Train on 9414 samples, validate on 957 samples\n    Epoch 1/50\n    9414/9414 [==============================] - 76s 8ms/step - loss: 7.0847 - perplexity: 1044.2455\n        - val_loss: 6.3167 - val_perplexity: 406.5031\n    ...\n\nAfter 200 epochs (~5 hours) of training on GeForce 1080 Ti, I've got\nvalidation perplexity about 51.61 and test perplexity 50.82. The score\ncan be further improved, but that is not the point of this demo.\n\nBERT model example can be launched similarly\n\n    python -m example.run_bert --save lm_model.h5 --model vanilla\n\nbut you will need to be patient. BERT easily achieves better performance\nthan GPT, but requires much more training time to converge.\n\n[1]: https://arxiv.org/abs/1706.03762 \"Attention Is All You Need\"\n[2]: https://arxiv.org/abs/1807.03819 \"Universal Transformers\"\n[3]: https://arxiv.org/abs/1810.04805 \"BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding\"\n[4]: https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf\n     \"Improving Language Understanding by Generative Pre-Training\"\n",
      "technique": "Header extraction"
    }
  ]
}