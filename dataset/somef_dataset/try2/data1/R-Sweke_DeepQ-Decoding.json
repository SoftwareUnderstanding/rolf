{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1511.06581\">two</a> landmark papers on deepQ learning.\n\nAt this point all that remains is to discuss how decoding is done in practice once training has been completed and the agent has converged to an optimal Q-function. As illustrated below, this is quite straightforward:\n\n<p align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/6330346/45884796-20186500-bdb5-11e8-96e1-36ef03160bcf.png\" width=\"80%\" height=\"80%\">\n</p>\n\nSpecifically, decoding proceeds as follows:\n\n<ol>\n  <li> Firstly, a syndrome volume is extracted from the code lattice and encoded as previously discussed (a,b",
      "https://arxiv.org/abs/1511.06581",
      "https://arxiv.org/abs/1810.07207 [quant-ph], 2018.  \n\n<hr>\n\n#### 0) Quickstart\n\nIn this readme, we will provide a summary and walkthrough of all the information contained within the included notebooks. However, we recommend starting by reading the included manuscript <a href=\"https://arxiv.org/pdf/1810.07207.pdf\">Reinforcement Learning Decoders for Fault-Tolerant Quantum Computation</a>. To explore the code used for training and evaluating agents, as well as take a more detailed look at the results, please see the example notebooks. In order to run the code given in these notebooks the following is required:\n\n<ol>\n  <li> <b>Python 3</b> (with numpy and scipy)</li>\n  <li> <b>Jupyter</b> </li>\n  <li> <b>tensorflow</b> </li>\n  <li> <b>keras</b> </li> \n  <li> <b>gym</b> </li> \n  <li> a modified <b>keras-rl</b>, installed from <a href=\"https://github.com/R-Sweke/keras-rl\">this fork</a> </li>\n</ol> \n\nIf you have any questions, please feel free to contact any of the contributors.\n\nEnjoy!\n\n<hr>\n\n#### 1) Setting\n\nTopological quantum error correcting codes, and in particular the surface code, currently provide the most promising path to scalable fault tolerant quantum computation. While a variety of decoders exist for such codes, recently decoders obtained via machine learning techniques have attracted attention due to both their potential flexibility, with respect to codes and noise models, and their potentially fast run times. Here, we demonstrate how reinforcement learning techniques, and in particular deepQ learning, can be utilized to solve this problem and obtain such decoders.\n\n##### 1a) The Surface Code\n\nWhile the techniques presented here could be applied to any stabilizer code we focus on the surface code, as shown below: \n\n<p align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/6330346/45884807-21499200-bdb5-11e8-97f7-60c9682c299e.png\" width=\"75%\" height=\"75%\">\n</p>\n\n<ul>\n  <li> We consider a d by d lattice, with qubits on the vertices (referred to as physical qubits), and plaquette stabilizers (a).</li>\n  <li> Orange (blue) plaquettes indicate stabilizers which check the Z (X) parity of qubits on the vertices of the plaquette (b).</li>\n  <li> Using red circles to indicate violated stabilizers we see here some basic examples of the syndromes created from X, Y or Z Pauli flips on a given vertex qubit (c).</li>\n  <li> The X logical operator of the surface code we consider is given by any continuous string of X errors which connect the top and bottom boundaries of the code (d).\n  <li> Similarly, The Z logical operator is given by any continuous string of Z errors which connect the left and right boundaries of the code (e).\n</ul> \n\nIn order to get intuition for the decoding problem, which we will present in detail further down, it is useful to see some examples of the syndromes (configurations of violated stabilizers) generated by various error configurations...\n\n<p align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/6330346/45884806-21499200-bdb5-11e8-9770-f7eaf37159fb.png\" width=\"40%\" height=\"40%\">\n</p>\n\nIn particular, it is very important to note that the map from syndromes to error configurations is _not_ one-to-one! For example, one can see that the error configurations given in the top-left and bottom-left codes both lead to the same syndrome. This ambiguity in the error configuration leading to a given syndrome gives rise to the decoding problem, which we describe below.\n\n##### 1b) The Decoding Problem\n\nGiven the above introduction to the surface code it is now possible to understand the decoding problem, within the fault tolerant setting. Quite loosely, given any state in the ground state space of the code, the aim of decoding is keep the code in this given state by exploiting _faulty_ syndrome information to determine which corrections need to be applied to the code to compensate for continuous noise and errors.\n\n<p align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/6330346/45884801-20b0fb80-bdb5-11e8-957d-e84e99fee7d6.png\" width=\"80%\" height=\"80%\">\n</p>\n\nTo be more specific, let's consider the above illustration:\n\n<ol>\n  <li>In the top left, we start with a state in the code space - i.e. a state for which no stabilizers are violated. Our goal is to maintain the logical qubit in this state. </li>\n  <li>Now, while storing the logical qubit (between gates for instance) the physical qubits are subject to noise. We consider depolarizing noise here for simplicity, for which in each unit of time each physical qubit is subject to either a Pauli X, Y or Z flip with a given probability (the physical error rate). In the above illustration, we imagine an X flip occurring on the physical qubit in the third row and second column.  </li>\n  <li>In order to maintain the code in the state it was given to us, we therefore need to perform a correction by applying an X gate to the qubit which was randomly flipped. To do this, we need perform a syndrome extraction, from which our decoding algorithm can attempt to diagnose the error configuration which gave rise to the received syndrome. However, as illustrated in the diagram, the syndrome extraction process is also noisy, and for each stabilizer there is a probability (the measurement error rate) that the measured stabilizer value is incorrect - i.e. that we see a violated stabilizer where there is not one, or no violated stabilizer where there actually is one.</li>\n   <li> To deal with this situation, instead of providing a single syndrome to the decoder, we perform multiple (faulty) syndrome measurements, between which physical errors may also occur. We then provide as input to our decoder not a single syndrome, but a stacked volume of successive syndrome slices.\n   <li> From this syndrome volume the decoding algorithm needs to suggest corrections which when applied to the code lattice move the logical qubit back into the original state (in practice, these corrections are not actually implemented, but rather tracked through the computation, and applied in a single step at the end).\n   <li> In the ideal case the decoder will be able to correctly diagnose a sufficient proportion of syndrome volumes, such that the probability of an error occurring on the logical qubit is lower than the physical error rate on a physical qubit.\n</ol> \n\n##### 1c) DeepQ Learning as a Tool for Obtaining Decoders\n\nGiven the problem as specified above, we utilize DeepQ reinforcement learning, a technique which has been successfully used to obtain agents capable of super-human performance in domains such as Atari, to obtain decoders which are capable of dealing with faulty measurements up to a threshold physical and measurement error rate. We will not go too deeply into the details and theory of Q-learning here, as an excellent introduction can be found in the fantastic textbook of <a href=\"http://incompleteideas.net/book/bookdraft2017nov5.pdf\">Sutton and Barto</a>, which is strongly recommended.\n\nHowever, to give a brief overview, the rough idea is that we will utilize a deep neural network (a convolutional neural network in our case) to parameterize the Q-function of a decoding agent, which interacts with the code lattice (the environment). This Q-function is a function which maps from states of the environment - syndrome volumes plus histories of previously applied corrections - to a Q-value for each available correction, where the Q-value of a given action, with respect to a particular environment state, encodes the expected long term benefit (not the exact technical definition!) to the agent of applying that correction when in that state. Given the Q-values corresponding to a given environment state, the optimal correction strategy then corresponds to applying the correction with the largest Q-value. Within this framework, the goal is then to obtain the optimal Q-function, which is done by letting the agent interact with the environment, during which the agents experiences are used to iteratively update the Q-function.\n\nIn order to present our approach it is therefore necessary to discuss:\n\n<ul>\n  <li> The manner in which we encode the environment state.</li>\n  <li> The parameterization of our Q-function via a deep neural network.</li>\n  <li> The procedure via which the agent interacts with the environment to gain experience, from which the Q-function can be updated.</li>\n</ul> \n\nLet's begin with the manner in which the environment state is encoded. In particular, at any given time we consider the environment state to consist of:\n\n<ol>\n  <li> A representation of the most recently measured faulty syndrome volume.</li>\n  <li> A representation of the actions which have been taken since receiving the most recent syndrome volume.</li>\n</ol> \n\nGiven a d by d surface code lattice, we encode a single syndrome slice in a (2d+1) by (2d + 1) binary matrix, as illustrated below:\n\n<p align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/6330346/45884802-20b0fb80-bdb5-11e8-906e-b758177a7c63.png\" width=\"60%\" height=\"60%\">\n</p>\n\nSimilarly, we encode the history of either X or Z Pauli corrections applied since the last syndrome volume was received in a (2d+1) by (2d + 1) binary matrix of the following form:\n\n<p align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/6330346/45884804-21499200-bdb5-11e8-86f6-f56a46c59567.png\" width=\"60%\" height=\"60%\">\n</p>\n\nFinally, given these conventions for syndrome and action history slices we can construct the complete environment state by stacking syndrome slices on top of an action history slice for each allowed Pauli operator (in practice we only need to allow for X and Z corrections). This gives us a total environment state in this form:\n\n<p align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/6330346/45884799-20b0fb80-bdb5-11e8-9313-0ffa63ae7382.png\" width=\"80%\" height=\"80%\">\n</p>\n\nIn the above image we have shown just three syndrome slices for simplicity, but as we will see later the depth of the syndrome volume (the number of slices) can be chosen at will.\n\nNow that we know how the state of the environment is encoded at any given time step we can proceed to examine the way in which we choose to parameterize the Q-function of our agent via a deep convolutional neural network. For an introduction to such networks, see <a href=\"http://colah.github.io/posts/2014-07-Conv-Nets-Modular/\">here</a> or <a href=\"https://github.com/R-Sweke/CrashCourseInNeuralNetworksWithKeras\">here</a>.\n\n<p align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/6330346/45884803-20b0fb80-bdb5-11e8-8d20-c6c2fd337649.png\" width=\"80%\" height=\"80%\">\n</p>\n\nAs illustrated above, our deepQ network is given by a simple convolutional neural network, consisting of:\n\n<ol>\n  <li> A user-specified number of convolutional layers (a-b).</li>\n  <li> A user specified number of feed-forward layers (c).</li>\n  <li> A final layer providing Q-values for each available correction (d), with respect to the input state.</li>\n</ol>  \n\nGiven these ingredients we can now examine in detail the training procedure, through which an optimal Q-function is updated via iterative updates from experience generated by interaction with the environment. As per the majority of reinforcement learning techniques, and illustrated below, this procedure involves a sequence of alternating steps in which:\n\n<p align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/6330346/45888570-a6857480-bdbe-11e8-92b0-d9730a95a455.png\" width=\"40%\" height=\"40%\">\n\n<ol>\n  <li> The environment provides a state to the agent.</li>\n  <li> The agent uses its current strategy to choose an action, with which it acts on the environment.</li>\n  <li> The environment updates its internal state appropriately, and responds to the agent by providing a new state along with a numerical reward and a binary signal which illustrates whether the agent is \"dead\" or \"alive\".</li>\n    <li> If the agent hasn't \"died\", it can then use this reward signal to update its internal strategy before once again acting on the environment and starting another round of interaction. If it has died, a new episode is started. </li>  \n</ol> \n\nFrom the agent's perspective the goal is to converge to a strategy which allows it to maximise the expected value of its (discounted) cumulative reward. In our particular context of the decoding problem, an episode works as illustrated below:\n\n<p align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/6330346/45884798-20b0fb80-bdb5-11e8-9ef3-23b55b52a498.png\" width=\"80%\" height=\"80%\">\n</p>\n\nIn particular:\n\n<ol>\n  <li> As illustrated and described in Section 1b. (the \"Decoding Problem\"), an episode starts with the extraction of a (faulty) syndrome volume from the code (a, b). If the syndrome volume is trivial, i.e. there is not a single violated stabilizer in the entire volume, then another syndrome volume is extracted.</li>\n  <li> As a new syndrome volume has just been extracted, the action history is reset to all zeros (c).</li>\n  <li> The just extracted syndrome volume is combined with the reset action history, as previously described in the \"state construction\" figure, and then provided to the agent as the initial state (d).</li>\n    <li> Now the agent must choose an action (e). As per most RL algorithms it is helpful to balance a period of exploration, with a period of exploiting previously obtained knowledge. As such, with a given probability \\epsilon, which is annealed during the course of training, the agent will choose an action at random, and with a probability 1-\\epsilon the agent will choose the action with the maximal Q-value according to its current parameterization. In order to aid training, we restrict the agents random choice to actions which are either adjacent to violated stabilizer, or adjacent to previously acted on qubits.</li>\n    <li> When the agents acts on the environment with the chosen action, provided the action is not the identity action (request new syndrome action), multiple things then happen simultaneously. Firstly, the action history slices of the visible state are updated to indicate the action that has been applied (f). Then, the action is actually applied to the code lattice, whose error configuration is updated accordingly (g). Then finally, in order to determine the reward, a \"referee\" decoder takes in the true non-faulty syndrome corresponding to the updated error configuration (h). If the referee decoder can succesfully decode the current syndrome, then the agent remains alive and the episode continues, if not then the agent dies and the episode ends. If the agent remains alive and its action has resulted in putting the code back into the desired initial state, the agent is giving a reward of 1, in any other case the agent is given a reward of 0.</li>\n    <li> The reward and game over signal is then combined with the updated state (in which only the action history was updated) and provided to the agent (i,j). In addition, the tuple of (state, action, reward, new state, game_over) is added to an external memory which is used to update the parametrization of the agent via backpropagation. </li>\n        <li> The procedure detailed above is then repeated (k-p) until the point at which the agent chooses to do the identity (q), which can be done explicitly, or by repeating an action. Conceptually, the identity action is meant as a tool for the agent to signal its belief that it has applied all the corrections necessary to return the code to the desired initial state. </li>    \n    <li> Given the identity signal from the agent, the environment then provides a new faulty syndrome volume (s,t), the action history slices of the state are reset, the new visible state is constructed from the rest action history and the updated syndrome (u,t) and fed to the agent, from which the episode continues as per steps (4-7), until the agent dies. </li> \n</ol> \n\nWhat has not been specifically illustrated in the above diagram is the procedure via which the parametrization of the Q-function is updated from batches of experience tuples. We will not present the details here as this is done using the exact same Q learning methodology described in  <a href=\"https://www.nature.com/articles/nature14236\">these</a>  <a href=\"https://arxiv.org/abs/1511.06581\">two</a> landmark papers on deepQ learning.\n\nAt this point all that remains is to discuss how decoding is done in practice once training has been completed and the agent has converged to an optimal Q-function. As illustrated below, this is quite straightforward:\n\n<p align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/6330346/45884796-20186500-bdb5-11e8-96e1-36ef03160bcf.png\" width=\"80%\" height=\"80%\">\n</p>\n\nSpecifically, decoding proceeds as follows:\n\n<ol>\n  <li> Firstly, a syndrome volume is extracted from the code lattice and encoded as previously discussed (a,b). This encoded syndrome volume is then stacked with a blank action history to create the initial input state to the decoder (c, d).</li>\n  <li> Given this input state, one forward pass of the neural network is executed and an argmax is taken over the output Q-values to obtain the first suggested correction. This suggested correction is then added to a memory (f) and used to update the action history slices of the visible state (e). These updated action history slices are then combined with the original syndrome volume (g) and passed to the decoder (h)</li>\n  <li> Step 2 is then repeated (i,j,k,l) until the point at which the agent chooses the identity action (m).</li>\n    <li> At this point, given that the agent has signalled that it believes it has supplied all the necessary corrections, the accumulated corrections are applied to the code lattice (n), or in practice, tracked through the computation. </li>\n</ol> \n\n\n#### 2) Training Decoders in Practice\n\nNow that we have discussed the conceptual foundations, strategies and techniques involved, we will provide detailed examples of how train decoders via the procedures discussed. In particular, we will first walk through a very simple script for training a decoder with a given set of hyper-parameters, which will lay the foundation for the discussion in Section 4 of how to perform a large scale iterated training procedure, involving multiple hyper-parameter optimizations at each stage, in order to obtain optimal decoders over a large range of error rates.\n\nIf you would like to run the code discussed in this section, you can find the simple single point training script within the \"Training Example\" notebook in the example_notebooks folder of the repo.\n\n##### 2a) Requirements\n\nThe following packages are required, and can be installed via PIP:\n\n<ol>\n  <li> Python 3 (with numpy and scipy)</li>\n  <li> tensorflow </li>\n  <li> keras </li> \n  <li> gym </li> \n</ol> \n\nIn addition, a modified version of the Keras-RL package is required, which should be installed from <a href=\"https://github.com/R-Sweke/keras-rl\">this fork</a>\n\n##### 2b) A Simple Training Script\n\nWe begin by importing all required packages and methods:\n\n\n```python\nimport numpy as np\nimport keras\nimport tensorflow\nimport gym\n\nfrom Function_Library import *\nfrom Environments import *\n\nimport rl as rl\nfrom rl.agents.dqn import DQNAgent\nfrom rl.policy import BoltzmannQPolicy, EpsGreedyQPolicy, LinearAnnealedPolicy, GreedyQPolicy\nfrom rl.memory import SequentialMemory\nfrom rl.callbacks import FileLogger\n\nimport json\nimport copy\nimport sys\nimport os\nimport shutil\nimport datetime\n```\n\nWe then proceed by providing all required hyperparameters and physical configuration settings. In order to allow for easier grid searching and incremented training later on we choose to split all hyperparameters into two categories:\n\n   - fixed configs: These remain constant during the course of a grid search or incremented training procedure.\n   - variable configs: We will later set up training grids over these hyperparameters.\n    \nIn particular, the fixed parameters one must provide are:\n\n   1. **d**: The lattice width (equal to the lattice height)\n   2. **use_Y**: If true then the agent can perform Y Pauli flips directly, if False then the agent can only perform X and Z Pauli flips.\n   3. **train_freq**: The number of agent-environment interaction steps which occur between each updating of the agent's weights.\n   4. **batch_size**: The size of batches used for calculating loss functions for gradient descent updates of agent weights.\n   5. **print_freq**: Every print_freq episodes the statistics of the training procedure will be logged.\n   6. **rolling_average_length**: The number of most recent episodes over which any relevant rolling average will be calculated.\n   7. **stopping_patience**: The number of episodes after which no improvement will result in the early stopping of the training procedure.\n   8. **error_model**: A string in [\"X\", \"DP\"], specifiying the noise model of the environment as X flips only or depolarizing noise.\n   9. **c_layers**: A list of lists specifying the structure of the convolutional layers of the agent deepQ network. Each inner list describes a layer and has the form [num_filters, filter_width, stride].\n   10. **ff_layers**: A list of lists specifying the structure of the feed-forward neural network sitting on top of the convolutional neural network. Each inner list has the form [num_neurons, output_dropout_rate].\n   11. **max_timesteps**: The maximum number of training timesteps allowed.\n   12. **volume_depth**: The number of syndrome measurements taken each time a new syndrome extraction is performed - i.e. the depth of the syndrome volume passed to the agent.\n   13. **testing_length**: The number of episodes uses to evaluate the trained agents performance. \n   14. **buffer_size**: The maximum number of experience tuples held in the memory from which the update batches for agent updating are drawn.\n   15. **dueling**: A boolean indicating whether or not a [dueling architecture](https://arxiv.org/abs/1511.06581) should be used.\n   16. **masked_greedy**: A boolean which indicates whether the agent will only be allowed to choose legal actions (actions next to a violated stabilizer or previously flipped qubit) when acting greedily (i.e. when choosing actions via the argmax of the Q-values)\n   17. **static_decoder**: For training within the fault tolerant setting (multi-cycle decoding) this should always be set to True.\n   \nIn addition, the parameters which we will later incrementally vary or grid search around are:\n\n   1. **p_phys**: The physical error probability\n   2. **p_meas**: The measurement error probability\n   3. **success_threshold**: The qubit lifetime rolling average at which training has been deemed succesfull and will be stopped.\n   4. **learning_starts**: The number of initial steps taken to contribute experience tuples to memory before any weight updates are made.\n   5. **learning_rate**: The learning rate for gradient descent optimization (via the Adam optimizer)\n   6. **exploration_fraction**: The number of time steps over which epsilon, the parameter controlling the probability of a random explorative action, is annealed.\n   7. **max_eps**: The initial maximum value of epsilon.\n   8. **target_network_update_freq**: In order to achieve stable training, a target network is cloned off from the active deepQ agent every target_network_update_freq interval of steps. This target network is then used to generate the target Q-function over the following interval.\n   9. **gamma**: The discount rate used for calculating the expected discounted cumulative return (the Q-values).\n   10. **final_eps**: The final value at which annealing of epsilon will be stopped.\n   \nFurthermore, in addition to all the above parameters one must provide a directory into which results and training progress as logged, as well as the path to a pre-trained referee decoder. Here e provide two pre-trained feed forward classification based referee decoders, one for X noise and one for DP noise. However, in principle any perfect-measurement decoding algorithm (such as MWPM) could be used here.\n\n\n```python\nfixed_configs = {\"d\": 5,\n                \"use_Y\": False,\n                \"train_freq\": 1,\n                \"batch_size\": 32,\n                \"print_freq\": 250,\n                \"rolling_average_length\": 500,\n                \"stopping_patience\": 500,\n                \"error_model\": \"X\",\n                \"c_layers\": [[64,3,2],[32,2,1],[32,2,1]],\n                \"ff_layers\": [[512,0.2]],\n                \"max_timesteps\": 1000000,\n                \"volume_depth\": 5,\n                \"testing_length\": 101,\n                \"buffer_size\": 50000,\n                \"dueling\": True,\n                \"masked_greedy\": False,\n                \"static_decoder\": True"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9680395895535876,
        0.8356013927728488
      ],
      "excerpt": "If you use any of the code provided here, please cite: \nR. Sweke, M.S. Kesselring, E.P.L. van Nieuwenburg, J. Eisert, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9974941384929114
      ],
      "excerpt": "arXiv:1810.07207 [quant-ph], 2018. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9909844968086463,
        0.9821300997599387,
        0.9942757182925156,
        0.9942757182925156,
        0.9873288065038525
      ],
      "excerpt": "  <li> <b>Jupyter</b> </li> \n  <li> <b>tensorflow</b> </li> \n  <li> <b>keras</b> </li>  \n  <li> <b>gym</b> </li>  \n  <li> a modified <b>keras-rl</b>, installed from <a href=\"https://github.com/R-Sweke/keras-rl\">this fork</a> </li> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9753615791715302
      ],
      "excerpt": "  <li> A representation of the most recently measured faulty syndrome volume.</li> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.810198237386479
      ],
      "excerpt": "  <li> A user specified number of feed-forward layers (c).</li> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9156566588472104
      ],
      "excerpt": "                \"c_layers\": [[64,3,2],[32,2,1],[32,2,1]], \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.971679416046096
      ],
      "excerpt": "    all_configs[key] = fixed_configs[key] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.971679416046096
      ],
      "excerpt": "    all_configs[key] = variable_configs[key] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "               policy=policy, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9507374082549614
      ],
      "excerpt": "training_history = history.history[\"episode_lifetimes_rolling_avg\"] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9906967194907662
      ],
      "excerpt": "_ = plt.title(\"Training History\") \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8840467439138965,
        0.8840467439138965
      ],
      "excerpt": "fixed_configs = pickle.load( open(fixed_configs_path, \"rb\" ) ) \nvariable_configs = pickle.load( open(variable_configs_path, \"rb\" ) ) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.971679416046096
      ],
      "excerpt": "    all_configs[key] = fixed_configs[key] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.971679416046096
      ],
      "excerpt": "    all_configs[key] = variable_configs[key] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "               policy=policy, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9146894306581498
      ],
      "excerpt": "results = testing_history.history[\"episode_lifetime\"] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8634524144265767
      ],
      "excerpt": "if action not in corrections and action != env.identity_index: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8654671031158477
      ],
      "excerpt": "    corrections.append(action) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/R-Sweke/DeepQ-Decoding",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-09-21T13:20:19Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-22T15:18:07Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9967215309138736
      ],
      "excerpt": "This repository is intended as a companion to the manuscript <a href=\"https://arxiv.org/pdf/1810.07207.pdf\">Reinforcement Learning Decoders for Fault-Tolerant Quantum Computation</a>. In particular, this repository provides all the tools necessary to reproduce all results presented in the above mentioned paper. Furthermore, it is hoped that this repository may serve as a starting-point for extending these tools and techniques. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8371446696587119
      ],
      "excerpt": "  <li> <b>Example Notebooks:</b> A collection of jupyter notebooks, intended to serve as detailed documentation for all utilised code, and for exploring the obtained results.</li> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8187214672856709
      ],
      "excerpt": "Reinforcement Learning Decoders for Fault-Tolerant Quantum Computation, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9366971287357653
      ],
      "excerpt": "In this readme, we will provide a summary and walkthrough of all the information contained within the included notebooks. However, we recommend starting by reading the included manuscript <a href=\"https://arxiv.org/pdf/1810.07207.pdf\">Reinforcement Learning Decoders for Fault-Tolerant Quantum Computation</a>. To explore the code used for training and evaluating agents, as well as take a more detailed look at the results, please see the example notebooks. In order to run the code given in these notebooks the following is required: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9308296056170584
      ],
      "excerpt": "While the techniques presented here could be applied to any stabilizer code we focus on the surface code, as shown below:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8475810699286872,
        0.9472818345429533,
        0.8888047569080556,
        0.9844193220138946,
        0.9851314863311479
      ],
      "excerpt": "  <li> We consider a d by d lattice, with qubits on the vertices (referred to as physical qubits), and plaquette stabilizers (a).</li> \n  <li> Orange (blue) plaquettes indicate stabilizers which check the Z (X) parity of qubits on the vertices of the plaquette (b).</li> \n  <li> Using red circles to indicate violated stabilizers we see here some basic examples of the syndromes created from X, Y or Z Pauli flips on a given vertex qubit (c).</li> \n  <li> The X logical operator of the surface code we consider is given by any continuous string of X errors which connect the top and bottom boundaries of the code (d). \n  <li> Similarly, The Z logical operator is given by any continuous string of Z errors which connect the left and right boundaries of the code (e). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.978046008893493
      ],
      "excerpt": "In order to get intuition for the decoding problem, which we will present in detail further down, it is useful to see some examples of the syndromes (configurations of violated stabilizers) generated by various error configurations... \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9708547208702505,
        0.9954493008384584,
        0.9913141128195433,
        0.9733317156649807
      ],
      "excerpt": "In particular, it is very important to note that the map from syndromes to error configurations is not one-to-one! For example, one can see that the error configurations given in the top-left and bottom-left codes both lead to the same syndrome. This ambiguity in the error configuration leading to a given syndrome gives rise to the decoding problem, which we describe below. \nGiven the problem as specified above, we utilize DeepQ reinforcement learning, a technique which has been successfully used to obtain agents capable of super-human performance in domains such as Atari, to obtain decoders which are capable of dealing with faulty measurements up to a threshold physical and measurement error rate. We will not go too deeply into the details and theory of Q-learning here, as an excellent introduction can be found in the fantastic textbook of <a href=\"http://incompleteideas.net/book/bookdraft2017nov5.pdf\">Sutton and Barto</a>, which is strongly recommended. \nHowever, to give a brief overview, the rough idea is that we will utilize a deep neural network (a convolutional neural network in our case) to parameterize the Q-function of a decoding agent, which interacts with the code lattice (the environment). This Q-function is a function which maps from states of the environment - syndrome volumes plus histories of previously applied corrections - to a Q-value for each available correction, where the Q-value of a given action, with respect to a particular environment state, encodes the expected long term benefit (not the exact technical definition!) to the agent of applying that correction when in that state. Given the Q-values corresponding to a given environment state, the optimal correction strategy then corresponds to applying the correction with the largest Q-value. Within this framework, the goal is then to obtain the optimal Q-function, which is done by letting the agent interact with the environment, during which the agents experiences are used to iteratively update the Q-function. \nIn order to present our approach it is therefore necessary to discuss: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8030543750222965
      ],
      "excerpt": "  <li> The parameterization of our Q-function via a deep neural network.</li> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.968773753902189
      ],
      "excerpt": "Let's begin with the manner in which the environment state is encoded. In particular, at any given time we consider the environment state to consist of: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9261759199355165
      ],
      "excerpt": "Given a d by d surface code lattice, we encode a single syndrome slice in a (2d+1) by (2d + 1) binary matrix, as illustrated below: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9508868633765551
      ],
      "excerpt": "Similarly, we encode the history of either X or Z Pauli corrections applied since the last syndrome volume was received in a (2d+1) by (2d + 1) binary matrix of the following form: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9128682415602155
      ],
      "excerpt": "Finally, given these conventions for syndrome and action history slices we can construct the complete environment state by stacking syndrome slices on top of an action history slice for each allowed Pauli operator (in practice we only need to allow for X and Z corrections). This gives us a total environment state in this form: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.90056005718316,
        0.9900509924831745
      ],
      "excerpt": "In the above image we have shown just three syndrome slices for simplicity, but as we will see later the depth of the syndrome volume (the number of slices) can be chosen at will. \nNow that we know how the state of the environment is encoded at any given time step we can proceed to examine the way in which we choose to parameterize the Q-function of our agent via a deep convolutional neural network. For an introduction to such networks, see <a href=\"http://colah.github.io/posts/2014-07-Conv-Nets-Modular/\">here</a> or <a href=\"https://github.com/R-Sweke/CrashCourseInNeuralNetworksWithKeras\">here</a>. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9547784537904158
      ],
      "excerpt": "As illustrated above, our deepQ network is given by a simple convolutional neural network, consisting of: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9675645867518383
      ],
      "excerpt": "Given these ingredients we can now examine in detail the training procedure, through which an optimal Q-function is updated via iterative updates from experience generated by interaction with the environment. As per the majority of reinforcement learning techniques, and illustrated below, this procedure involves a sequence of alternating steps in which: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9354963782435877
      ],
      "excerpt": "  <li> The environment updates its internal state appropriately, and responds to the agent by providing a new state along with a numerical reward and a binary signal which illustrates whether the agent is \"dead\" or \"alive\".</li> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.986718593183805
      ],
      "excerpt": "From the agent's perspective the goal is to converge to a strategy which allows it to maximise the expected value of its (discounted) cumulative reward. In our particular context of the decoding problem, an episode works as illustrated below: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8246635645256778,
        0.9837626187224562,
        0.9150505769089988,
        0.9878543693753311,
        0.9625925656212402,
        0.8630838519197078
      ],
      "excerpt": "  <li> The just extracted syndrome volume is combined with the reset action history, as previously described in the \"state construction\" figure, and then provided to the agent as the initial state (d).</li> \n    <li> Now the agent must choose an action (e). As per most RL algorithms it is helpful to balance a period of exploration, with a period of exploiting previously obtained knowledge. As such, with a given probability \\epsilon, which is annealed during the course of training, the agent will choose an action at random, and with a probability 1-\\epsilon the agent will choose the action with the maximal Q-value according to its current parameterization. In order to aid training, we restrict the agents random choice to actions which are either adjacent to violated stabilizer, or adjacent to previously acted on qubits.</li> \n    <li> When the agents acts on the environment with the chosen action, provided the action is not the identity action (request new syndrome action), multiple things then happen simultaneously. Firstly, the action history slices of the visible state are updated to indicate the action that has been applied (f). Then, the action is actually applied to the code lattice, whose error configuration is updated accordingly (g). Then finally, in order to determine the reward, a \"referee\" decoder takes in the true non-faulty syndrome corresponding to the updated error configuration (h). If the referee decoder can succesfully decode the current syndrome, then the agent remains alive and the episode continues, if not then the agent dies and the episode ends. If the agent remains alive and its action has resulted in putting the code back into the desired initial state, the agent is giving a reward of 1, in any other case the agent is given a reward of 0.</li> \n    <li> The reward and game over signal is then combined with the updated state (in which only the action history was updated) and provided to the agent (i,j). In addition, the tuple of (state, action, reward, new state, game_over) is added to an external memory which is used to update the parametrization of the agent via backpropagation. </li> \n        <li> The procedure detailed above is then repeated (k-p) until the point at which the agent chooses to do the identity (q), which can be done explicitly, or by repeating an action. Conceptually, the identity action is meant as a tool for the agent to signal its belief that it has applied all the corrections necessary to return the code to the desired initial state. </li>     \n    <li> Given the identity signal from the agent, the environment then provides a new faulty syndrome volume (s,t), the action history slices of the state are reset, the new visible state is constructed from the rest action history and the updated syndrome (u,t) and fed to the agent, from which the episode continues as per steps (4-7), until the agent dies. </li>  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9516367948549522,
        0.9567723523047553
      ],
      "excerpt": "What has not been specifically illustrated in the above diagram is the procedure via which the parametrization of the Q-function is updated from batches of experience tuples. We will not present the details here as this is done using the exact same Q learning methodology described in  <a href=\"https://www.nature.com/articles/nature14236\">these</a>  <a href=\"https://arxiv.org/abs/1511.06581\">two</a> landmark papers on deepQ learning. \nAt this point all that remains is to discuss how decoding is done in practice once training has been completed and the agent has converged to an optimal Q-function. As illustrated below, this is quite straightforward: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.955138354818606
      ],
      "excerpt": "  <li> Given this input state, one forward pass of the neural network is executed and an argmax is taken over the output Q-values to obtain the first suggested correction. This suggested correction is then added to a memory (f) and used to update the action history slices of the visible state (e). These updated action history slices are then combined with the original syndrome volume (g) and passed to the decoder (h)</li> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8826205924967817
      ],
      "excerpt": "    <li> At this point, given that the agent has signalled that it believes it has supplied all the necessary corrections, the accumulated corrections are applied to the code lattice (n), or in practice, tracked through the computation. </li> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9286832560806957
      ],
      "excerpt": "Now that we have discussed the conceptual foundations, strategies and techniques involved, we will provide detailed examples of how train decoders via the procedures discussed. In particular, we will first walk through a very simple script for training a decoder with a given set of hyper-parameters, which will lay the foundation for the discussion in Section 4 of how to perform a large scale iterated training procedure, involving multiple hyper-parameter optimizations at each stage, in order to obtain optimal decoders over a large range of error rates. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8411517823417787
      ],
      "excerpt": "We begin by importing all required packages and methods: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9685472426443692
      ],
      "excerpt": "We then proceed by providing all required hyperparameters and physical configuration settings. In order to allow for easier grid searching and incremented training later on we choose to split all hyperparameters into two categories: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.879348993458109
      ],
      "excerpt": "d: The lattice width (equal to the lattice height) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.821721591285387,
        0.9542913125630343,
        0.8148141235559166
      ],
      "excerpt": "train_freq: The number of agent-environment interaction steps which occur between each updating of the agent's weights. \nbatch_size: The size of batches used for calculating loss functions for gradient descent updates of agent weights. \nprint_freq: Every print_freq episodes the statistics of the training procedure will be logged. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246102577174318,
        0.8796033999827031,
        0.9182421165898584
      ],
      "excerpt": "error_model: A string in [\"X\", \"DP\"], specifiying the noise model of the environment as X flips only or depolarizing noise. \nc_layers: A list of lists specifying the structure of the convolutional layers of the agent deepQ network. Each inner list describes a layer and has the form [num_filters, filter_width, stride]. \nff_layers: A list of lists specifying the structure of the feed-forward neural network sitting on top of the convolutional neural network. Each inner list has the form [num_neurons, output_dropout_rate]. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9292840739416607,
        0.8542932908287462,
        0.9332831771751752
      ],
      "excerpt": "volume_depth: The number of syndrome measurements taken each time a new syndrome extraction is performed - i.e. the depth of the syndrome volume passed to the agent. \ntesting_length: The number of episodes uses to evaluate the trained agents performance.  \nbuffer_size: The maximum number of experience tuples held in the memory from which the update batches for agent updating are drawn. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9610604122972193
      ],
      "excerpt": "masked_greedy: A boolean which indicates whether the agent will only be allowed to choose legal actions (actions next to a violated stabilizer or previously flipped qubit) when acting greedily (i.e. when choosing actions via the argmax of the Q-values) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8965209967026181,
        0.8334708665061369,
        0.9769491797964129
      ],
      "excerpt": "learning_starts: The number of initial steps taken to contribute experience tuples to memory before any weight updates are made. \nlearning_rate: The learning rate for gradient descent optimization (via the Adam optimizer) \nexploration_fraction: The number of time steps over which epsilon, the parameter controlling the probability of a random explorative action, is annealed. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9151069432307869
      ],
      "excerpt": "target_network_update_freq: In order to achieve stable training, a target network is cloned off from the active deepQ agent every target_network_update_freq interval of steps. This target network is then used to generate the target Q-function over the following interval. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "for key in fixed_configs.keys(): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "for key in variable_configs.keys(): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9672611836660926,
        0.9042472206747697
      ],
      "excerpt": "The environment class is defined to mirror the environments of https://gym.openai.com/, and such contains the required \"reset\" and \"step\" methods, via which the agent can interact with the environment, in addition to decoding specific methods and attributes whose details can be found in the relevant method docstrings. \nWe can now proceed to define the agent. We being by specifying the memory to be used, as well as the exploration and testing policies. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8105563095332426
      ],
      "excerpt": "Finally, we can then build the deep convolutional neural network which will represent our Q-function and compile our agent. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = build_convolutional_nn(all_configs[\"c_layers\"],  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "dqn = DQNAgent(model=model,  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9517381193836489
      ],
      "excerpt": "In order to evaluate the agent later on, or apply the agent in a production decoding scenario we can easily save the weights: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9771093953536268
      ],
      "excerpt": "And finally, in order to evaluate the training procedure we may be interested in viewing any of the metrics which were logged. These are all saved within the history.history dictionary. For example, we are often most interested in analyzing the training procedure by looking at the rolling average of the qubit lifetime, which we can do as follows: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9689439178469496,
        0.9799233190351836
      ],
      "excerpt": "From the above plot one can see that during the exploration phase the agent was unable to do well, due to constant exploratory random actions, but was able to exploit this knowledge effectively once the exploration probability became sufficiently low. Again, it is also clear that the agent was definitely still learning and improving when we chose to stop the training procedure. \nGiven a trained decoder we would of course like to benchmark the decoder to evaluate how well it performs. This procedure is very similar to training the decoder, in that we run multiple decoding episodes in which the agent interacts with the environment until it \"dies\" - however in this context we would like the agent to use only a greedy policy for action selection, i.e. to never make random moves, and we do not need to update the agents parameters in time. As we will see benchmarking an agent is made easy by use of the DQNAgent class \"test\" method. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8588410239762817,
        0.9313482336431191
      ],
      "excerpt": "The hyper-parameters of the agent we would like to test \nThe weights of the agent \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "for key in fixed_configs.keys(): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "for key in variable_configs.keys(): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9646693155162268
      ],
      "excerpt": "Now we build a model and instantiate an agent with all the parameters of the pre-trained agent. Notice that we insist on a greedy policy! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = build_convolutional_nn(all_configs[\"c_layers\"],all_configs[\"ff_layers\"],  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "dqn = DQNAgent(model=model,  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9980480492309969
      ],
      "excerpt": "It is important to note that the reported episode length is the number of non-trivial syndrome volumes that the agent received, as these are the steps during which a decision needs to be taken on the part of the agent. The qubit lifetime, whose rolling average is reported, is the total number of syndrome measurements (between which an error may occur) for which the agent survived, as this is the relevant metric to compare with a single faulty qubit whose expected lifetime is 1/(error_probability). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9746689974038909,
        0.9434327227976284
      ],
      "excerpt": "Here we see that on average, over 1001 test episodes, the qubit survives for 329 syndrome measurements on average, which is better than the average lifetime of 143 syndrome measurements for a single faulty qubit. \nIn addition to benchmarking a decoder via the agent test method, we would like to demonstrate how to use the decoder in practice, given a faulty syndrome volume. In principle all the information on how to do this is contained within the environments and test method, but to aid in applying these decoders quickly and easily in practice we make everything explicit here: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9212792516033594
      ],
      "excerpt": "By viewing the final hidden_state (the lattice state) we can see what errors occured, which here was a single error on the 21st qubit (we start counting from 0, and move row wise left to right). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9520030012320876
      ],
      "excerpt": "And we can view the faulty_syndromes that we received, which is what would come out of an experiment. As we can see, measurement errors occurred in syndrome slices 2 and 5, and it appears as if the actual error occurred between extraction of syndrome 2 and 3: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.972162499417721
      ],
      "excerpt": "And now we would like to decode and obtain the suggested corrections. To do this, we begin by padding the faulty syndromes as required and by concatenating the obtained volume with an action history slice, in which all the actions are initially zero: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.875614596905155
      ],
      "excerpt": ": embed and place the faulty syndrome slices in the correct place \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8499887196233388
      ],
      "excerpt": "And now we can run the agent, collecting the suggested actions, until the agent does the identity, which suggests that it is finished decoding: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8490037945672047
      ],
      "excerpt": "#: Fetch the suggested correction \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9816638512281275
      ],
      "excerpt": "    #: append the suggested correction to the list of corrections \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8908291353949209
      ],
      "excerpt": "    #: Update the input state to the agent to indicate the correction it would have made \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9857492609361483,
        0.9752544153149784,
        0.9609962343261709
      ],
      "excerpt": "Note that in general if there is more than one error, or if the agent is uncertain about a given configuration, it may choose to do the identity, therefore triggering a new syndrome volume from which it may be more certain which action to take - The crucial point is that in practice we are interested in how long the qubit survives for, and an optimal strategy for achieving long qubit lifetimes may not be to attempt to fully decode into the ground state after each syndrome volume - in fact, that is one of the primary advantages of this approach! \nNow that we have seen how to train and test decoders at a fixed error rate, for a given set of hyper-parameters, we would like to turn our attention to how we might be able to obtain good decoders for a large range of error rates. In order to achieve this we have developed an iterative training procedure involving hyper-parameter searches at each level of the iteration. In this section we will first outline the procedure before proceeding to discuss in detail how one can implement this procedure on a high-performance computing cluster. The scripts required for this implementation are contained in the cluster_scripts folder of the repo. \nAs illustrated in the figure below, the fundemental idea is to iterate through increasing error rates, performing hyper-parameter optimizations at each iteration, and using various attributes of the optimization at one step of the iteration as a starting point for the subsequent step. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9640466579728201
      ],
      "excerpt": "We begin by fixing the error rate to some small initial value which we estimate to be far below the threshold of the decoder and at which the agent will be able to learn a good strategy. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9435056925757582
      ],
      "excerpt": "Next, we create a hyper-parameter grid over the hyperparameters which we would like to optimize (the variable hyper-parameters) and proceed to train a decoder for each set of hyper-parameters in the grid. For each of these initial simulations the decoder/agent is initialized with no memory and with random initial weights. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356550189601307
      ],
      "excerpt": "The hyper-parameter settings for this point. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.878061059680211,
        0.9449396687553152
      ],
      "excerpt": "The weights of the final agent. \nThe memory of the agent. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9164500214945904,
        0.9857141400330627,
        0.9574738661549375,
        0.9524276880372609
      ],
      "excerpt": "At this point, provided the results from just completed iteration are above some specified performance threshold, we then increase the error rate. To do this we start by fetching the experience memory and weights of the optimal decoder from the just completed iteration. Then, at the increased error rate we create a new hyper-parameter grid over the variable hyper-parameters, and train a decoder at each point in this new hyper-parameter grid. However, in this subsequent step of the iteration the agents are not initialized with random memories and weights, but with the memory and weights of the optimal performing decoder from the previous iteration. \nThis procedure then iterates until the point at which, with respect to the current error rate, the logical qubit lifetime when corrected by the optimal decoder falls beneath that of a single faulty-qubit - i.e until we are able to identify the pseudo-threshold of the decoder. \nIn the cluster_scripts folder of the repo we provide scripts for practically implementing the above iterating training procedure on an HPC cluster using the slurm workload manager. In this section we provide detailed documentation for how to set up and implement this procedure using the provided scripts. \nAs an example, we will work through the iterative training procedure in detail for d=5 and X noise only, although the steps here can be easily modified to different physical scenarios, and we have also provided all the scripts necessary for d=5 with depolarizing noise.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8888737265841892
      ],
      "excerpt": "static_decoder (an appropriate referee decoder with the corresponding lattice size and error model) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9006190835042647
      ],
      "excerpt": "In order to run a customized/modified version of this procedure this exact directory and file structure should be replicated, as we will see below all that is necessary is to modify: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9335854835098057
      ],
      "excerpt": "1) From \"../d5_x/\" start a \"screen\" - this provides a persistent terminal which we can later detach and re-attach at will to keep track of the training procedure, without having to remain logged in to the cluster. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8151796666834408
      ],
      "excerpt": "3) Using vim or some other in-terminal editor, modify the following in Controller.py: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9071059362976855
      ],
      "excerpt": "-For each error rate, provide the expected lifetime of a single faulty qubit (i.e. the threshold for decoding sucess) as well as the average qubit lifetime you would like to use as a threshold for stopping training. We recommend setting this training threshold extremely high, so that training ends due to convergence. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8089702889557676
      ],
      "excerpt": "- make sure the time threshold for evaluating whether simulations have timed out corresponds to the cluster configurations \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8895277591274308
      ],
      "excerpt": "8) Now we have to get the script Controller.py to run periodically. Every time this script runs it will check for the current error rate and collect all available results from simulations from that error rate. If all the simulations at the specified error rate are finished, or if the time threshold for an error rate has passed, then it will write and sort the results, generate a new hyperparameter grid and simulation scripts for an increased error rate, copy the memory and weights of the optimal model from the old error rate into the appropriate directories, and submit a new batch of jobs for all the new grid points at the increased error rates. To get the controller to run periodically we do the following: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8213971261325718,
        0.8938813903511033
      ],
      "excerpt": "9) At this stage we are looking at the watch screen, which displays the difference in output between successive calls to Controller.py \n- We want to detach this screen so that we can safely logout of the cluster without interrupting training \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8669145876031197
      ],
      "excerpt": "10) We can now log out and the training procedure will continue safely, as directed by the Controller. We can log in and out of the cluster to see how training is proceeding whenever we want. In particular we can view the contents of: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9441947741575327,
        0.9766957315474967
      ],
      "excerpt": "11) When training is finished and we want to kill the controller we have to login to the cluster and run the following commands: \n- reattach the screen with \"screen -r\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9565246047608601,
        0.9357401735944872
      ],
      "excerpt": "As we have discussed, during the course of the training procedure the results of all trained agents in \"test_mode\" are written out and stored, as well as the results from the best agent at each error rate. However, in order to ease computational time during training, each agent was only benchmarked for 100 episodes. \nHere we present the full results and training histories for each best performing agent, as selected by the preliminary benchmarking during training. In particular, these full results were obtained by testing each best performing trained agent, at each error rate, for the number of episodes that guaranteed at least 10^6 syndromes were seen by the agent. All the trained agents from which these results were obtained, along with the fully detailed results (i.e. episode length of every single tested episode at each error rate for each agent), can be found in the \"trained_models\" directory of the repo. In addition to these evaluation results, we also provide the learning curves for all best performing agents. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8778705909275845
      ],
      "excerpt": "We start by presenting the results obtained when using the best performing agent from each iterative training step, for both bitflip and depolarizing noise, as well as the results obtained when using the best performing agent for each specific error rate. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9907941180122004
      ],
      "excerpt": "In addition, it is of interest to view the learning curves for all the agents whose performance is shown above. The following plots provide these training histories, along with the hyper-parameter settings for the agent, given as a list in the form [num_exploration_steps, initial_epsilon, final_epsilon, learning_rate, target_network_update_frequency]. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Decoders for fault tolerant quantum computation via deepQ reinforcement learning",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/R-Sweke/DeepQ-Decoding/releases",
    "technique": "GitHub API"
  },
  "faq": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Given the above introduction to the surface code it is now possible to understand the decoding problem, within the fault tolerant setting. Quite loosely, given any state in the ground state space of the code, the aim of decoding is keep the code in this given state by exploiting _faulty_ syndrome information to determine which corrections need to be applied to the code to compensate for continuous noise and errors.\n\n<p align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/6330346/45884801-20b0fb80-bdb5-11e8-957d-e84e99fee7d6.png\" width=\"80%\" height=\"80%\">\n</p>\n\nTo be more specific, let's consider the above illustration:\n\n<ol>\n  <li>In the top left, we start with a state in the code space - i.e. a state for which no stabilizers are violated. Our goal is to maintain the logical qubit in this state. </li>\n  <li>Now, while storing the logical qubit (between gates for instance) the physical qubits are subject to noise. We consider depolarizing noise here for simplicity, for which in each unit of time each physical qubit is subject to either a Pauli X, Y or Z flip with a given probability (the physical error rate). In the above illustration, we imagine an X flip occurring on the physical qubit in the third row and second column.  </li>\n  <li>In order to maintain the code in the state it was given to us, we therefore need to perform a correction by applying an X gate to the qubit which was randomly flipped. To do this, we need perform a syndrome extraction, from which our decoding algorithm can attempt to diagnose the error configuration which gave rise to the received syndrome. However, as illustrated in the diagram, the syndrome extraction process is also noisy, and for each stabilizer there is a probability (the measurement error rate) that the measured stabilizer value is incorrect - i.e. that we see a violated stabilizer where there is not one, or no violated stabilizer where there actually is one.</li>\n   <li> To deal with this situation, instead of providing a single syndrome to the decoder, we perform multiple (faulty) syndrome measurements, between which physical errors may also occur. We then provide as input to our decoder not a single syndrome, but a stacked volume of successive syndrome slices.\n   <li> From this syndrome volume the decoding algorithm needs to suggest corrections which when applied to the code lattice move the logical qubit back into the original state (in practice, these corrections are not actually implemented, but rather tracked through the computation, and applied in a single step at the end).\n   <li> In the ideal case the decoder will be able to correctly diagnose a sufficient proportion of syndrome volumes, such that the probability of an error occurring on the logical qubit is lower than the physical error rate on a physical qubit.\n</ol> \n\n",
      "technique": "Header extraction"
    }
  ],
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 7,
      "date": "Wed, 22 Dec 2021 03:05:18 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/R-Sweke/DeepQ-Decoding/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "R-Sweke/DeepQ-Decoding",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/R-Sweke/DeepQ-Decoding/master/example_notebooks/1%29%20Introduction%20and%20Documentation.ipynb",
      "https://raw.githubusercontent.com/R-Sweke/DeepQ-Decoding/master/example_notebooks/4%29%20Large%20Scale%20Iterative%20Training.ipynb",
      "https://raw.githubusercontent.com/R-Sweke/DeepQ-Decoding/master/example_notebooks/2%29%20Training%20Example.ipynb",
      "https://raw.githubusercontent.com/R-Sweke/DeepQ-Decoding/master/example_notebooks/3%29%20Testing%20Example.ipynb",
      "https://raw.githubusercontent.com/R-Sweke/DeepQ-Decoding/master/example_notebooks/5%29%20Final%20Results%20and%20Training%20Histories.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/R-Sweke/DeepQ-Decoding/master/cluster_scripts/d5_x/make_executable.sh",
      "https://raw.githubusercontent.com/R-Sweke/DeepQ-Decoding/master/cluster_scripts/d5_x/0.001/Start_Simulations.sh",
      "https://raw.githubusercontent.com/R-Sweke/DeepQ-Decoding/master/cluster_scripts/d5_x/0.015/Start_Continuing_Simulations.sh",
      "https://raw.githubusercontent.com/R-Sweke/DeepQ-Decoding/master/cluster_scripts/d5_x/0.009/Start_Continuing_Simulations.sh",
      "https://raw.githubusercontent.com/R-Sweke/DeepQ-Decoding/master/cluster_scripts/d5_x/0.011/Start_Continuing_Simulations.sh",
      "https://raw.githubusercontent.com/R-Sweke/DeepQ-Decoding/master/cluster_scripts/d5_x/0.005/Start_Continuing_Simulations.sh",
      "https://raw.githubusercontent.com/R-Sweke/DeepQ-Decoding/master/cluster_scripts/d5_x/0.003/Start_Continuing_Simulations.sh",
      "https://raw.githubusercontent.com/R-Sweke/DeepQ-Decoding/master/cluster_scripts/d5_x/0.013/Start_Continuing_Simulations.sh",
      "https://raw.githubusercontent.com/R-Sweke/DeepQ-Decoding/master/cluster_scripts/d5_x/0.007/Start_Continuing_Simulations.sh",
      "https://raw.githubusercontent.com/R-Sweke/DeepQ-Decoding/master/cluster_scripts/d5_x/0.017/Start_Continuing_Simulations.sh",
      "https://raw.githubusercontent.com/R-Sweke/DeepQ-Decoding/master/cluster_scripts/d5_dp/make_executable.sh",
      "https://raw.githubusercontent.com/R-Sweke/DeepQ-Decoding/master/cluster_scripts/d5_dp/0.001/Start_Simulations.sh",
      "https://raw.githubusercontent.com/R-Sweke/DeepQ-Decoding/master/cluster_scripts/d5_dp/0.015/Start_Continuing_Simulations.sh",
      "https://raw.githubusercontent.com/R-Sweke/DeepQ-Decoding/master/cluster_scripts/d5_dp/0.009/Start_Continuing_Simulations.sh",
      "https://raw.githubusercontent.com/R-Sweke/DeepQ-Decoding/master/cluster_scripts/d5_dp/0.011/Start_Continuing_Simulations.sh",
      "https://raw.githubusercontent.com/R-Sweke/DeepQ-Decoding/master/cluster_scripts/d5_dp/0.005/Start_Continuing_Simulations.sh",
      "https://raw.githubusercontent.com/R-Sweke/DeepQ-Decoding/master/cluster_scripts/d5_dp/0.003/Start_Continuing_Simulations.sh",
      "https://raw.githubusercontent.com/R-Sweke/DeepQ-Decoding/master/cluster_scripts/d5_dp/0.013/Start_Continuing_Simulations.sh",
      "https://raw.githubusercontent.com/R-Sweke/DeepQ-Decoding/master/cluster_scripts/d5_dp/0.007/Start_Continuing_Simulations.sh",
      "https://raw.githubusercontent.com/R-Sweke/DeepQ-Decoding/master/cluster_scripts/d5_dp/0.017/Start_Continuing_Simulations.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Topological quantum error correcting codes, and in particular the surface code, currently provide the most promising path to scalable fault tolerant quantum computation. While a variety of decoders exist for such codes, recently decoders obtained via machine learning techniques have attracted attention due to both their potential flexibility, with respect to codes and noise models, and their potentially fast run times. Here, we demonstrate how reinforcement learning techniques, and in particular deepQ learning, can be utilized to solve this problem and obtain such decoders.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9364673318232433
      ],
      "excerpt": "Now that we have specified all the required parameters we can instantiate our environment: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8347691101404546,
        0.8075219384950185
      ],
      "excerpt": "2) Run the command \"bash make_executable.sh\". This will allow the controller - a python script which will be run periodically to control the training process - to submit jobs via slurm. \n3) Using vim or some other in-terminal editor, modify the following in Controller.py: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9023697225149864
      ],
      "excerpt": "- simulation_script.sh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8348461001498279
      ],
      "excerpt": "- run the command \"watch -n interval_in_seconds python Controller.py\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8423394397999961
      ],
      "excerpt": "- We are now looking at the watch output - kill this with ctrl+c \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9457175861910134,
        0.9133368656218674,
        0.9000504886472166,
        0.9133368656218674,
        0.8801854956928516,
        0.8801854956928516,
        0.9012248701992861,
        0.8801854956928516
      ],
      "excerpt": "import numpy as np \nimport keras \nimport tensorflow \nimport gym \nfrom Function_Library import * \nfrom Environments import * \nimport rl as rl \nfrom rl.agents.dqn import DQNAgent \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8099604977282218,
        0.8801854956928516,
        0.9180062578030207,
        0.8869999123707137
      ],
      "excerpt": "from rl.memory import SequentialMemory \nfrom rl.callbacks import FileLogger \nimport json \nimport copy \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8401558704798054,
        0.8567035315341892,
        0.9133368656218674
      ],
      "excerpt": "import os \nimport shutil \nimport datetime \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8229300659404541
      ],
      "excerpt": "fixed configs: These remain constant during the course of a grid search or incremented training procedure. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "                \"dueling\": True, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "                \"static_decoder\": True} \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "test_policy = GreedyQPolicy(masked_greedy=True) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8673978324500141
      ],
      "excerpt": "Total Training Time: 42.201s \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8673978324500141
      ],
      "excerpt": "Total Training Time: 106.792s \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8673978324500141
      ],
      "excerpt": "Total Training Time: 2478.817s \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8673978324500141
      ],
      "excerpt": "Total Training Time: 3490.853s \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "dqn.save_weights(weights_file, overwrite=True) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9024270527458111
      ],
      "excerpt": "from matplotlib import pyplot as plt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9457175861910134,
        0.9133368656218674,
        0.9000504886472166,
        0.9133368656218674,
        0.8801854956928516,
        0.8801854956928516,
        0.9012248701992861,
        0.8801854956928516
      ],
      "excerpt": "import numpy as np \nimport keras \nimport tensorflow \nimport gym \nfrom Function_Library import * \nfrom Environments import * \nimport rl as rl \nfrom rl.agents.dqn import DQNAgent \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8099604977282218,
        0.8801854956928516,
        0.9180062578030207,
        0.8869999123707137
      ],
      "excerpt": "from rl.memory import SequentialMemory \nfrom rl.callbacks import FileLogger \nimport json \nimport copy \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8401558704798054,
        0.8567035315341892,
        0.9133368656218674
      ],
      "excerpt": "import os \nimport shutil \nimport datetime \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "test_policy = GreedyQPolicy(masked_greedy=True) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8276102184089881
      ],
      "excerpt": "                           interval=100, single_cycle=False) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9153914450833603
      ],
      "excerpt": "print(\"Mean Qubit Lifetime:\", np.mean(results)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8749754072023638
      ],
      "excerpt": "hidden_state = np.zeros((d, d), int) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9023027047808938,
        0.936606094659785,
        0.936606094659785,
        0.936606094659785
      ],
      "excerpt": "    print(\"syndrome slice\", j+1) \n    print() \n    print(faulty_syndromes[j]) \n    print() \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "still_decoding = True \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8263484699885432
      ],
      "excerpt": "For each point in the hyperparameter grid we store  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9178023606457084
      ],
      "excerpt": "A text file \"current_error_rate.txt\" containing one line with the lowest error rate - i.e. 0.001 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.817342153601791
      ],
      "excerpt": "- Set the base configuration grid (fixed hyperparameters) in Generate_Base_Configs_and_Simulation_Scripts.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.882066324371587,
        0.8717271707841271
      ],
      "excerpt": "- Set the maximum run times for each job (each grid point will be submitted as a seperate job). \n- run this script with the command \"python Generate_Base_Configs_and_Simulation_Scripts.py\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991
      ],
      "excerpt": "- variable_config_x.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8807316865358898,
        0.8001049216175089
      ],
      "excerpt": "- run the command \"watch -n interval_in_seconds python Controller.py\" \n- eg: for ten minute intervals: \"watch -n 600 Controller.py\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8180206913302173
      ],
      "excerpt": "<img src=\"https://user-images.githubusercontent.com/6330346/47023954-10483280-d161-11e8-9d82-bce6f66b077a.png\" width=\"100%\" height=\"100%\"> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/R-Sweke/DeepQ-Decoding/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python",
      "TeX",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "GNU General Public License v3.0",
      "url": "https://api.github.com/licenses/gpl-3.0"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'                    GNU GENERAL PUBLIC LICENSE\\n                       Version 3, 29 June 2007\\n\\n Copyright (C) 2007 Free Software Foundation, Inc. https://fsf.org/\\n Everyone is permitted to copy and distribute verbatim copies\\n of this license document, but changing it is not allowed.\\n\\n                            Preamble\\n\\n  The GNU General Public License is a free, copyleft license for\\nsoftware and other kinds of works.\\n\\n  The licenses for most software and other practical works are designed\\nto take away your freedom to share and change the works.  By contrast,\\nthe GNU General Public License is intended to guarantee your freedom to\\nshare and change all versions of a program--to make sure it remains free\\nsoftware for all its users.  We, the Free Software Foundation, use the\\nGNU General Public License for most of our software; it applies also to\\nany other work released this way by its authors.  You can apply it to\\nyour programs, too.\\n\\n  When we speak of free software, we are referring to freedom, not\\nprice.  Our General Public Licenses are designed to make sure that you\\nhave the freedom to distribute copies of free software (and charge for\\nthem if you wish), that you receive source code or can get it if you\\nwant it, that you can change the software or use pieces of it in new\\nfree programs, and that you know you can do these things.\\n\\n  To protect your rights, we need to prevent others from denying you\\nthese rights or asking you to surrender the rights.  Therefore, you have\\ncertain responsibilities if you distribute copies of the software, or if\\nyou modify it: responsibilities to respect the freedom of others.\\n\\n  For example, if you distribute copies of such a program, whether\\ngratis or for a fee, you must pass on to the recipients the same\\nfreedoms that you received.  You must make sure that they, too, receive\\nor can get the source code.  And you must show them these terms so they\\nknow their rights.\\n\\n  Developers that use the GNU GPL protect your rights with two steps:\\n(1) assert copyright on the software, and (2) offer you this License\\ngiving you legal permission to copy, distribute and/or modify it.\\n\\n  For the developers\\' and authors\\' protection, the GPL clearly explains\\nthat there is no warranty for this free software.  For both users\\' and\\nauthors\\' sake, the GPL requires that modified versions be marked as\\nchanged, so that their problems will not be attributed erroneously to\\nauthors of previous versions.\\n\\n  Some devices are designed to deny users access to install or run\\nmodified versions of the software inside them, although the manufacturer\\ncan do so.  This is fundamentally incompatible with the aim of\\nprotecting users\\' freedom to change the software.  The systematic\\npattern of such abuse occurs in the area of products for individuals to\\nuse, which is precisely where it is most unacceptable.  Therefore, we\\nhave designed this version of the GPL to prohibit the practice for those\\nproducts.  If such problems arise substantially in other domains, we\\nstand ready to extend this provision to those domains in future versions\\nof the GPL, as needed to protect the freedom of users.\\n\\n  Finally, every program is threatened constantly by software patents.\\nStates should not allow patents to restrict development and use of\\nsoftware on general-purpose computers, but in those that do, we wish to\\navoid the special danger that patents applied to a free program could\\nmake it effectively proprietary.  To prevent this, the GPL assures that\\npatents cannot be used to render the program non-free.\\n\\n  The precise terms and conditions for copying, distribution and\\nmodification follow.\\n\\n                       TERMS AND CONDITIONS\\n\\n  0. Definitions.\\n\\n  \"This License\" refers to version 3 of the GNU General Public License.\\n\\n  \"Copyright\" also means copyright-like laws that apply to other kinds of\\nworks, such as semiconductor masks.\\n\\n  \"The Program\" refers to any copyrightable work licensed under this\\nLicense.  Each licensee is addressed as \"you\".  \"Licensees\" and\\n\"recipients\" may be individuals or organizations.\\n\\n  To \"modify\" a work means to copy from or adapt all or part of the work\\nin a fashion requiring copyright permission, other than the making of an\\nexact copy.  The resulting work is called a \"modified version\" of the\\nearlier work or a work \"based on\" the earlier work.\\n\\n  A \"covered work\" means either the unmodified Program or a work based\\non the Program.\\n\\n  To \"propagate\" a work means to do anything with it that, without\\npermission, would make you directly or secondarily liable for\\ninfringement under applicable copyright law, except executing it on a\\ncomputer or modifying a private copy.  Propagation includes copying,\\ndistribution (with or without modification), making available to the\\npublic, and in some countries other activities as well.\\n\\n  To \"convey\" a work means any kind of propagation that enables other\\nparties to make or receive copies.  Mere interaction with a user through\\na computer network, with no transfer of a copy, is not conveying.\\n\\n  An interactive user interface displays \"Appropriate Legal Notices\"\\nto the extent that it includes a convenient and prominently visible\\nfeature that (1) displays an appropriate copyright notice, and (2)\\ntells the user that there is no warranty for the work (except to the\\nextent that warranties are provided), that licensees may convey the\\nwork under this License, and how to view a copy of this License.  If\\nthe interface presents a list of user commands or options, such as a\\nmenu, a prominent item in the list meets this criterion.\\n\\n  1. Source Code.\\n\\n  The \"source code\" for a work means the preferred form of the work\\nfor making modifications to it.  \"Object code\" means any non-source\\nform of a work.\\n\\n  A \"Standard Interface\" means an interface that either is an official\\nstandard defined by a recognized standards body, or, in the case of\\ninterfaces specified for a particular programming language, one that\\nis widely used among developers working in that language.\\n\\n  The \"System Libraries\" of an executable work include anything, other\\nthan the work as a whole, that (a) is included in the normal form of\\npackaging a Major Component, but which is not part of that Major\\nComponent, and (b) serves only to enable use of the work with that\\nMajor Component, or to implement a Standard Interface for which an\\nimplementation is available to the public in source code form.  A\\n\"Major Component\", in this context, means a major essential component\\n(kernel, window system, and so on) of the specific operating system\\n(if any) on which the executable work runs, or a compiler used to\\nproduce the work, or an object code interpreter used to run it.\\n\\n  The \"Corresponding Source\" for a work in object code form means all\\nthe source code needed to generate, install, and (for an executable\\nwork) run the object code and to modify the work, including scripts to\\ncontrol those activities.  However, it does not include the work\\'s\\nSystem Libraries, or general-purpose tools or generally available free\\nprograms which are used unmodified in performing those activities but\\nwhich are not part of the work.  For example, Corresponding Source\\nincludes interface definition files associated with source files for\\nthe work, and the source code for shared libraries and dynamically\\nlinked subprograms that the work is specifically designed to require,\\nsuch as by intimate data communication or control flow between those\\nsubprograms and other parts of the work.\\n\\n  The Corresponding Source need not include anything that users\\ncan regenerate automatically from other parts of the Corresponding\\nSource.\\n\\n  The Corresponding Source for a work in source code form is that\\nsame work.\\n\\n  2. Basic Permissions.\\n\\n  All rights granted under this License are granted for the term of\\ncopyright on the Program, and are irrevocable provided the stated\\nconditions are met.  This License explicitly affirms your unlimited\\npermission to run the unmodified Program.  The output from running a\\ncovered work is covered by this License only if the output, given its\\ncontent, constitutes a covered work.  This License acknowledges your\\nrights of fair use or other equivalent, as provided by copyright law.\\n\\n  You may make, run and propagate covered works that you do not\\nconvey, without conditions so long as your license otherwise remains\\nin force.  You may convey covered works to others for the sole purpose\\nof having them make modifications exclusively for you, or provide you\\nwith facilities for running those works, provided that you comply with\\nthe terms of this License in conveying all material for which you do\\nnot control copyright.  Those thus making or running the covered works\\nfor you must do so exclusively on your behalf, under your direction\\nand control, on terms that prohibit them from making any copies of\\nyour copyrighted material outside their relationship with you.\\n\\n  Conveying under any other circumstances is permitted solely under\\nthe conditions stated below.  Sublicensing is not allowed; section 10\\nmakes it unnecessary.\\n\\n  3. Protecting Users\\' Legal Rights From Anti-Circumvention Law.\\n\\n  No covered work shall be deemed part of an effective technological\\nmeasure under any applicable law fulfilling obligations under article\\n11 of the WIPO copyright treaty adopted on 20 December 1996, or\\nsimilar laws prohibiting or restricting circumvention of such\\nmeasures.\\n\\n  When you convey a covered work, you waive any legal power to forbid\\ncircumvention of technological measures to the extent such circumvention\\nis effected by exercising rights under this License with respect to\\nthe covered work, and you disclaim any intention to limit operation or\\nmodification of the work as a means of enforcing, against the work\\'s\\nusers, your or third parties\\' legal rights to forbid circumvention of\\ntechnological measures.\\n\\n  4. Conveying Verbatim Copies.\\n\\n  You may convey verbatim copies of the Program\\'s source code as you\\nreceive it, in any medium, provided that you conspicuously and\\nappropriately publish on each copy an appropriate copyright notice;\\nkeep intact all notices stating that this License and any\\nnon-permissive terms added in accord with section 7 apply to the code;\\nkeep intact all notices of the absence of any warranty; and give all\\nrecipients a copy of this License along with the Program.\\n\\n  You may charge any price or no price for each copy that you convey,\\nand you may offer support or warranty protection for a fee.\\n\\n  5. Conveying Modified Source Versions.\\n\\n  You may convey a work based on the Program, or the modifications to\\nproduce it from the Program, in the form of source code under the\\nterms of section 4, provided that you also meet all of these conditions:\\n\\n    a) The work must carry prominent notices stating that you modified\\n    it, and giving a relevant date.\\n\\n    b) The work must carry prominent notices stating that it is\\n    released under this License and any conditions added under section\\n    7.  This requirement modifies the requirement in section 4 to\\n    \"keep intact all notices\".\\n\\n    c) You must license the entire work, as a whole, under this\\n    License to anyone who comes into possession of a copy.  This\\n    License will therefore apply, along with any applicable section 7\\n    additional terms, to the whole of the work, and all its parts,\\n    regardless of how they are packaged.  This License gives no\\n    permission to license the work in any other way, but it does not\\n    invalidate such permission if you have separately received it.\\n\\n    d) If the work has interactive user interfaces, each must display\\n    Appropriate Legal Notices; however, if the Program has interactive\\n    interfaces that do not display Appropriate Legal Notices, your\\n    work need not make them do so.\\n\\n  A compilation of a covered work with other separate and independent\\nworks, which are not by their nature extensions of the covered work,\\nand which are not combined with it such as to form a larger program,\\nin or on a volume of a storage or distribution medium, is called an\\n\"aggregate\" if the compilation and its resulting copyright are not\\nused to limit the access or legal rights of the compilation\\'s users\\nbeyond what the individual works permit.  Inclusion of a covered work\\nin an aggregate does not cause this License to apply to the other\\nparts of the aggregate.\\n\\n  6. Conveying Non-Source Forms.\\n\\n  You may convey a covered work in object code form under the terms\\nof sections 4 and 5, provided that you also convey the\\nmachine-readable Corresponding Source under the terms of this License,\\nin one of these ways:\\n\\n    a) Convey the object code in, or embodied in, a physical product\\n    (including a physical distribution medium), accompanied by the\\n    Corresponding Source fixed on a durable physical medium\\n    customarily used for software interchange.\\n\\n    b) Convey the object code in, or embodied in, a physical product\\n    (including a physical distribution medium), accompanied by a\\n    written offer, valid for at least three years and valid for as\\n    long as you offer spare parts or customer support for that product\\n    model, to give anyone who possesses the object code either (1) a\\n    copy of the Corresponding Source for all the software in the\\n    product that is covered by this License, on a durable physical\\n    medium customarily used for software interchange, for a price no\\n    more than your reasonable cost of physically performing this\\n    conveying of source, or (2) access to copy the\\n    Corresponding Source from a network server at no charge.\\n\\n    c) Convey individual copies of the object code with a copy of the\\n    written offer to provide the Corresponding Source.  This\\n    alternative is allowed only occasionally and noncommercially, and\\n    only if you received the object code with such an offer, in accord\\n    with subsection 6b.\\n\\n    d) Convey the object code by offering access from a designated\\n    place (gratis or for a charge), and offer equivalent access to the\\n    Corresponding Source in the same way through the same place at no\\n    further charge.  You need not require recipients to copy the\\n    Corresponding Source along with the object code.  If the place to\\n    copy the object code is a network server, the Corresponding Source\\n    may be on a different server (operated by you or a third party)\\n    that supports equivalent copying facilities, provided you maintain\\n    clear directions next to the object code saying where to find the\\n    Corresponding Source.  Regardless of what server hosts the\\n    Corresponding Source, you remain obligated to ensure that it is\\n    available for as long as needed to satisfy these requirements.\\n\\n    e) Convey the object code using peer-to-peer transmission, provided\\n    you inform other peers where the object code and Corresponding\\n    Source of the work are being offered to the general public at no\\n    charge under subsection 6d.\\n\\n  A separable portion of the object code, whose source code is excluded\\nfrom the Corresponding Source as a System Library, need not be\\nincluded in conveying the object code work.\\n\\n  A \"User Product\" is either (1) a \"consumer product\", which means any\\ntangible personal property which is normally used for personal, family,\\nor household purposes, or (2) anything designed or sold for incorporation\\ninto a dwelling.  In determining whether a product is a consumer product,\\ndoubtful cases shall be resolved in favor of coverage.  For a particular\\nproduct received by a particular user, \"normally used\" refers to a\\ntypical or common use of that class of product, regardless of the status\\nof the particular user or of the way in which the particular user\\nactually uses, or expects or is expected to use, the product.  A product\\nis a consumer product regardless of whether the product has substantial\\ncommercial, industrial or non-consumer uses, unless such uses represent\\nthe only significant mode of use of the product.\\n\\n  \"Installation Information\" for a User Product means any methods,\\nprocedures, authorization keys, or other information required to install\\nand execute modified versions of a covered work in that User Product from\\na modified version of its Corresponding Source.  The information must\\nsuffice to ensure that the continued functioning of the modified object\\ncode is in no case prevented or interfered with solely because\\nmodification has been made.\\n\\n  If you convey an object code work under this section in, or with, or\\nspecifically for use in, a User Product, and the conveying occurs as\\npart of a transaction in which the right of possession and use of the\\nUser Product is transferred to the recipient in perpetuity or for a\\nfixed term (regardless of how the transaction is characterized), the\\nCorresponding Source conveyed under this section must be accompanied\\nby the Installation Information.  But this requirement does not apply\\nif neither you nor any third party retains the ability to install\\nmodified object code on the User Product (for example, the work has\\nbeen installed in ROM).\\n\\n  The requirement to provide Installation Information does not include a\\nrequirement to continue to provide support service, warranty, or updates\\nfor a work that has been modified or installed by the recipient, or for\\nthe User Product in which it has been modified or installed.  Access to a\\nnetwork may be denied when the modification itself materially and\\nadversely affects the operation of the network or violates the rules and\\nprotocols for communication across the network.\\n\\n  Corresponding Source conveyed, and Installation Information provided,\\nin accord with this section must be in a format that is publicly\\ndocumented (and with an implementation available to the public in\\nsource code form), and must require no special password or key for\\nunpacking, reading or copying.\\n\\n  7. Additional Terms.\\n\\n  \"Additional permissions\" are terms that supplement the terms of this\\nLicense by making exceptions from one or more of its conditions.\\nAdditional permissions that are applicable to the entire Program shall\\nbe treated as though they were included in this License, to the extent\\nthat they are valid under applicable law.  If additional permissions\\napply only to part of the Program, that part may be used separately\\nunder those permissions, but the entire Program remains governed by\\nthis License without regard to the additional permissions.\\n\\n  When you convey a copy of a covered work, you may at your option\\nremove any additional permissions from that copy, or from any part of\\nit.  (Additional permissions may be written to require their own\\nremoval in certain cases when you modify the work.)  You may place\\nadditional permissions on material, added by you to a covered work,\\nfor which you have or can give appropriate copyright permission.\\n\\n  Notwithstanding any other provision of this License, for material you\\nadd to a covered work, you may (if authorized by the copyright holders of\\nthat material) supplement the terms of this License with terms:\\n\\n    a) Disclaiming warranty or limiting liability differently from the\\n    terms of sections 15 and 16 of this License; or\\n\\n    b) Requiring preservation of specified reasonable legal notices or\\n    author attributions in that material or in the Appropriate Legal\\n    Notices displayed by works containing it; or\\n\\n    c) Prohibiting misrepresentation of the origin of that material, or\\n    requiring that modified versions of such material be marked in\\n    reasonable ways as different from the original version; or\\n\\n    d) Limiting the use for publicity purposes of names of licensors or\\n    authors of the material; or\\n\\n    e) Declining to grant rights under trademark law for use of some\\n    trade names, trademarks, or service marks; or\\n\\n    f) Requiring indemnification of licensors and authors of that\\n    material by anyone who conveys the material (or modified versions of\\n    it) with contractual assumptions of liability to the recipient, for\\n    any liability that these contractual assumptions directly impose on\\n    those licensors and authors.\\n\\n  All other non-permissive additional terms are considered \"further\\nrestrictions\" within the meaning of section 10.  If the Program as you\\nreceived it, or any part of it, contains a notice stating that it is\\ngoverned by this License along with a term that is a further\\nrestriction, you may remove that term.  If a license document contains\\na further restriction but permits relicensing or conveying under this\\nLicense, you may add to a covered work material governed by the terms\\nof that license document, provided that the further restriction does\\nnot survive such relicensing or conveying.\\n\\n  If you add terms to a covered work in accord with this section, you\\nmust place, in the relevant source files, a statement of the\\nadditional terms that apply to those files, or a notice indicating\\nwhere to find the applicable terms.\\n\\n  Additional terms, permissive or non-permissive, may be stated in the\\nform of a separately written license, or stated as exceptions;\\nthe above requirements apply either way.\\n\\n  8. Termination.\\n\\n  You may not propagate or modify a covered work except as expressly\\nprovided under this License.  Any attempt otherwise to propagate or\\nmodify it is void, and will automatically terminate your rights under\\nthis License (including any patent licenses granted under the third\\nparagraph of section 11).\\n\\n  However, if you cease all violation of this License, then your\\nlicense from a particular copyright holder is reinstated (a)\\nprovisionally, unless and until the copyright holder explicitly and\\nfinally terminates your license, and (b) permanently, if the copyright\\nholder fails to notify you of the violation by some reasonable means\\nprior to 60 days after the cessation.\\n\\n  Moreover, your license from a particular copyright holder is\\nreinstated permanently if the copyright holder notifies you of the\\nviolation by some reasonable means, this is the first time you have\\nreceived notice of violation of this License (for any work) from that\\ncopyright holder, and you cure the violation prior to 30 days after\\nyour receipt of the notice.\\n\\n  Termination of your rights under this section does not terminate the\\nlicenses of parties who have received copies or rights from you under\\nthis License.  If your rights have been terminated and not permanently\\nreinstated, you do not qualify to receive new licenses for the same\\nmaterial under section 10.\\n\\n  9. Acceptance Not Required for Having Copies.\\n\\n  You are not required to accept this License in order to receive or\\nrun a copy of the Program.  Ancillary propagation of a covered work\\noccurring solely as a consequence of using peer-to-peer transmission\\nto receive a copy likewise does not require acceptance.  However,\\nnothing other than this License grants you permission to propagate or\\nmodify any covered work.  These actions infringe copyright if you do\\nnot accept this License.  Therefore, by modifying or propagating a\\ncovered work, you indicate your acceptance of this License to do so.\\n\\n  10. Automatic Licensing of Downstream Recipients.\\n\\n  Each time you convey a covered work, the recipient automatically\\nreceives a license from the original licensors, to run, modify and\\npropagate that work, subject to this License.  You are not responsible\\nfor enforcing compliance by third parties with this License.\\n\\n  An \"entity transaction\" is a transaction transferring control of an\\norganization, or substantially all assets of one, or subdividing an\\norganization, or merging organizations.  If propagation of a covered\\nwork results from an entity transaction, each party to that\\ntransaction who receives a copy of the work also receives whatever\\nlicenses to the work the party\\'s predecessor in interest had or could\\ngive under the previous paragraph, plus a right to possession of the\\nCorresponding Source of the work from the predecessor in interest, if\\nthe predecessor has it or can get it with reasonable efforts.\\n\\n  You may not impose any further restrictions on the exercise of the\\nrights granted or affirmed under this License.  For example, you may\\nnot impose a license fee, royalty, or other charge for exercise of\\nrights granted under this License, and you may not initiate litigation\\n(including a cross-claim or counterclaim in a lawsuit) alleging that\\nany patent claim is infringed by making, using, selling, offering for\\nsale, or importing the Program or any portion of it.\\n\\n  11. Patents.\\n\\n  A \"contributor\" is a copyright holder who authorizes use under this\\nLicense of the Program or a work on which the Program is based.  The\\nwork thus licensed is called the contributor\\'s \"contributor version\".\\n\\n  A contributor\\'s \"essential patent claims\" are all patent claims\\nowned or controlled by the contributor, whether already acquired or\\nhereafter acquired, that would be infringed by some manner, permitted\\nby this License, of making, using, or selling its contributor version,\\nbut do not include claims that would be infringed only as a\\nconsequence of further modification of the contributor version.  For\\npurposes of this definition, \"control\" includes the right to grant\\npatent sublicenses in a manner consistent with the requirements of\\nthis License.\\n\\n  Each contributor grants you a non-exclusive, worldwide, royalty-free\\npatent license under the contributor\\'s essential patent claims, to\\nmake, use, sell, offer for sale, import and otherwise run, modify and\\npropagate the contents of its contributor version.\\n\\n  In the following three paragraphs, a \"patent license\" is any express\\nagreement or commitment, however denominated, not to enforce a patent\\n(such as an express permission to practice a patent or covenant not to\\nsue for patent infringement).  To \"grant\" such a patent license to a\\nparty means to make such an agreement or commitment not to enforce a\\npatent against the party.\\n\\n  If you convey a covered work, knowingly relying on a patent license,\\nand the Corresponding Source of the work is not available for anyone\\nto copy, free of charge and under the terms of this License, through a\\npublicly available network server or other readily accessible means,\\nthen you must either (1) cause the Corresponding Source to be so\\navailable, or (2) arrange to deprive yourself of the benefit of the\\npatent license for this particular work, or (3) arrange, in a manner\\nconsistent with the requirements of this License, to extend the patent\\nlicense to downstream recipients.  \"Knowingly relying\" means you have\\nactual knowledge that, but for the patent license, your conveying the\\ncovered work in a country, or your recipient\\'s use of the covered work\\nin a country, would infringe one or more identifiable patents in that\\ncountry that you have reason to believe are valid.\\n\\n  If, pursuant to or in connection with a single transaction or\\narrangement, you convey, or propagate by procuring conveyance of, a\\ncovered work, and grant a patent license to some of the parties\\nreceiving the covered work authorizing them to use, propagate, modify\\nor convey a specific copy of the covered work, then the patent license\\nyou grant is automatically extended to all recipients of the covered\\nwork and works based on it.\\n\\n  A patent license is \"discriminatory\" if it does not include within\\nthe scope of its coverage, prohibits the exercise of, or is\\nconditioned on the non-exercise of one or more of the rights that are\\nspecifically granted under this License.  You may not convey a covered\\nwork if you are a party to an arrangement with a third party that is\\nin the business of distributing software, under which you make payment\\nto the third party based on the extent of your activity of conveying\\nthe work, and under which the third party grants, to any of the\\nparties who would receive the covered work from you, a discriminatory\\npatent license (a) in connection with copies of the covered work\\nconveyed by you (or copies made from those copies), or (b) primarily\\nfor and in connection with specific products or compilations that\\ncontain the covered work, unless you entered into that arrangement,\\nor that patent license was granted, prior to 28 March 2007.\\n\\n  Nothing in this License shall be construed as excluding or limiting\\nany implied license or other defenses to infringement that may\\notherwise be available to you under applicable patent law.\\n\\n  12. No Surrender of Others\\' Freedom.\\n\\n  If conditions are imposed on you (whether by court order, agreement or\\notherwise) that contradict the conditions of this License, they do not\\nexcuse you from the conditions of this License.  If you cannot convey a\\ncovered work so as to satisfy simultaneously your obligations under this\\nLicense and any other pertinent obligations, then as a consequence you may\\nnot convey it at all.  For example, if you agree to terms that obligate you\\nto collect a royalty for further conveying from those to whom you convey\\nthe Program, the only way you could satisfy both those terms and this\\nLicense would be to refrain entirely from conveying the Program.\\n\\n  13. Use with the GNU Affero General Public License.\\n\\n  Notwithstanding any other provision of this License, you have\\npermission to link or combine any covered work with a work licensed\\nunder version 3 of the GNU Affero General Public License into a single\\ncombined work, and to convey the resulting work.  The terms of this\\nLicense will continue to apply to the part which is the covered work,\\nbut the special requirements of the GNU Affero General Public License,\\nsection 13, concerning interaction through a network will apply to the\\ncombination as such.\\n\\n  14. Revised Versions of this License.\\n\\n  The Free Software Foundation may publish revised and/or new versions of\\nthe GNU General Public License from time to time.  Such new versions will\\nbe similar in spirit to the present version, but may differ in detail to\\naddress new problems or concerns.\\n\\n  Each version is given a distinguishing version number.  If the\\nProgram specifies that a certain numbered version of the GNU General\\nPublic License \"or any later version\" applies to it, you have the\\noption of following the terms and conditions either of that numbered\\nversion or of any later version published by the Free Software\\nFoundation.  If the Program does not specify a version number of the\\nGNU General Public License, you may choose any version ever published\\nby the Free Software Foundation.\\n\\n  If the Program specifies that a proxy can decide which future\\nversions of the GNU General Public License can be used, that proxy\\'s\\npublic statement of acceptance of a version permanently authorizes you\\nto choose that version for the Program.\\n\\n  Later license versions may give you additional or different\\npermissions.  However, no additional obligations are imposed on any\\nauthor or copyright holder as a result of your choosing to follow a\\nlater version.\\n\\n  15. Disclaimer of Warranty.\\n\\n  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY\\nAPPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT\\nHOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY\\nOF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,\\nTHE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\\nPURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM\\nIS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF\\nALL NECESSARY SERVICING, REPAIR OR CORRECTION.\\n\\n  16. Limitation of Liability.\\n\\n  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING\\nWILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS\\nTHE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY\\nGENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE\\nUSE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF\\nDATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD\\nPARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),\\nEVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF\\nSUCH DAMAGES.\\n\\n  17. Interpretation of Sections 15 and 16.\\n\\n  If the disclaimer of warranty and limitation of liability provided\\nabove cannot be given local legal effect according to their terms,\\nreviewing courts shall apply local law that most closely approximates\\nan absolute waiver of all civil liability in connection with the\\nProgram, unless a warranty or assumption of liability accompanies a\\ncopy of the Program in return for a fee.\\n\\n                     END OF TERMS AND CONDITIONS\\n\\n            How to Apply These Terms to Your New Programs\\n\\n  If you develop a new program, and you want it to be of the greatest\\npossible use to the public, the best way to achieve this is to make it\\nfree software which everyone can redistribute and change under these terms.\\n\\n  To do so, attach the following notices to the program.  It is safest\\nto attach them to the start of each source file to most effectively\\nstate the exclusion of warranty; and each file should have at least\\nthe \"copyright\" line and a pointer to where the full notice is found.\\n\\n    <one line to give the program\\'s name and a brief idea of what it does.>\\n    Copyright (C) <year>  <name of author>\\n\\n    This program is free software: you can redistribute it and/or modify\\n    it under the terms of the GNU General Public License as published by\\n    the Free Software Foundation, either version 3 of the License, or\\n    (at your option) any later version.\\n\\n    This program is distributed in the hope that it will be useful,\\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n    GNU General Public License for more details.\\n\\n    You should have received a copy of the GNU General Public License\\n    along with this program.  If not, see https://www.gnu.org/licenses/.\\n\\nAlso add information on how to contact you by electronic and paper mail.\\n\\n  If the program does terminal interaction, make it output a short\\nnotice like this when it starts in an interactive mode:\\n\\n    <program>  Copyright (C) <year>  <name of author>\\n    This program comes with ABSOLUTELY NO WARRANTY; for details type show w\\'.\\n    This is free software, and you are welcome to redistribute it\\n    under certain conditions; typeshow c\\' for details.\\n\\nThe hypothetical commands show w\\' andshow c\\' should show the appropriate\\nparts of the General Public License.  Of course, your program\\'s commands\\nmight be different; for a GUI interface, you would use an \"about box\".\\n\\n  You should also get your employer (if you work as a programmer) or school,\\nif any, to sign a \"copyright disclaimer\" for the program, if necessary.\\nFor more information on this, and how to apply and follow the GNU GPL, see\\nhttps://www.gnu.org/licenses/.\\n\\n  The GNU General Public License does not permit incorporating your program\\ninto proprietary programs.  If your program is a subroutine library, you\\nmay consider it more useful to permit linking proprietary applications with\\nthe library.  If this is what you want to do, use the GNU Lesser General\\nPublic License instead of this License.  But first, please read\\nhttps://www.gnu.org/licenses/why-not-lgpl.html.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "##  _DeepQ Decoding_",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "DeepQ-Decoding",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "R-Sweke",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/R-Sweke/DeepQ-Decoding/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The following packages are required, and can be installed via PIP:\n\n<ol>\n  <li> Python 3 (with numpy and scipy)</li>\n  <li> tensorflow </li>\n  <li> keras </li> \n  <li> gym </li> \n</ol> \n\nIn addition, a modified version of the Keras-RL package is required, which should be installed from <a href=\"https://github.com/R-Sweke/keras-rl\">this fork</a>\n\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Now that we know how to train a decoder, we would like to see how to evaluate the performance of that decoder, as well as how to use the decoder in a production setting. In this section we will demonstrate how to perform both of these tasks. Once again, all of this code can be found within the \"Testing Examples\" notebook of the example_notebooks folder of the repo.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 34,
      "date": "Wed, 22 Dec 2021 03:05:18 GMT"
    },
    "technique": "GitHub API"
  }
}