{
  "citation": [
    {
      "confidence": [
        0.8997098193315181
      ],
      "excerpt": "  http://robohub.org/artificial-general-intelligence-that-plays-atari-video-games-how-did-deepmind-do-it/ \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/spragunr/deep_q_rl",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2014-09-29T16:09:34Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-30T02:55:03Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This package provides a Lasagne/Theano-based implementation of the deep\nQ-learning algorithm described in:\n\n[Playing Atari with Deep Reinforcement Learning](http://arxiv.org/abs/1312.5602)\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis\nAntonoglou, Daan Wierstra, Martin Riedmiller\n\nand \n\nMnih, Volodymyr, et al. \"Human-level control through deep reinforcement learning.\" Nature 518.7540 (2015): 529-533.\n\nHere is a video showing a trained network playing breakout (using an earlier version of the code):\n\n http://youtu.be/SZ88F82KLX4\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9053910237448624
      ],
      "excerpt": "significantly improves performance at the expense of a slight increase \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9508672601721868,
        0.971697145541323
      ],
      "excerpt": "This is the code DeepMind used for the Nature paper.  The license \n  only permits the code to be used for \"evaluating and reviewing\" the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8629206103762034,
        0.963772552007359
      ],
      "excerpt": "Working Caffe-based implementation.  (I haven't tried it, but there \n  is a video of the agent playing Pong successfully.) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9458478724383588
      ],
      "excerpt": "Defunct?  As far as I know, this package was never fully functional.  The project is described here:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.802988752209957
      ],
      "excerpt": "This is an almost-working implementation developed during Spring \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Theano-based implementation of Deep Q-learning",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/spragunr/deep_q_rl/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 351,
      "date": "Thu, 30 Dec 2021 10:58:11 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/spragunr/deep_q_rl/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "spragunr/deep_q_rl",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/spragunr/deep_q_rl/master/dep_script.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8561219640672015
      ],
      "excerpt": "in memory usage on the GPU. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/spragunr/deep_q_rl/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "BSD 3-Clause \"New\" or \"Revised\" License",
      "url": "https://api.github.com/licenses/bsd-3-clause"
    },
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Introduction",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "deep_q_rl",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "spragunr",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/spragunr/deep_q_rl/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* A reasonably modern NVIDIA GPU\n* OpenCV\n* [Theano](http://deeplearning.net/software/theano/) ([https://github.com/Theano/Theano](https://github.com/Theano/Theano))\n* [Lasagne](http://lasagne.readthedocs.org/en/latest/) ([https://github.com/Lasagne/Lasagne](https://github.com/Lasagne/Lasagne)\n* [Pylearn2](http://deeplearning.net/software/pylearn2/) ([https://github.com/lisa-lab/pylearn2](https://github.com/lisa-lab/pylearn2))\n* [Arcade Learning Environment](http://www.arcadelearningenvironment.org/) ([https://github.com/mgbellemare/Arcade-Learning-Environment](https://github.com/mgbellemare/Arcade-Learning-Environment))\n\nThe script `dep_script.sh` can be used to install all dependencies under Ubuntu.\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Use the scripts `run_nips.py` or `run_nature.py` to start all the necessary processes:\n\n`$ ./run_nips.py --rom breakout`\n\n`$ ./run_nature.py --rom breakout`\n\nThe `run_nips.py` script uses parameters consistent with the original\nNIPS workshop paper.  This code should take 2-4 days to complete.  The\n`run_nature.py` script uses parameters consistent with the Nature\npaper.  The final policies should be better, but it will take 6-10\ndays to finish training.\n\nEither script will store output files in a folder prefixed with the\nname of the ROM.  Pickled version of the network objects are stored\nafter every epoch.  The file `results.csv` will contain the testing\noutput.  You can plot the progress by executing `plot_results.py`:\n\n`$ python plot_results.py breakout_05-28-17-09_0p00025_0p99/results.csv`\n\nAfter training completes, you can watch the network play using the \n`ale_run_watch.py` script: \n\n`$ python ale_run_watch.py breakout_05-28-17-09_0p00025_0p99/network_file_99.pkl`\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1053,
      "date": "Thu, 30 Dec 2021 10:58:11 GMT"
    },
    "technique": "GitHub API"
  },
  "support": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The [deep Q-learning web-forum](https://groups.google.com/forum/#!forum/deep-q-learning)\ncan be used for discussion and advice related to deep Q-learning in\ngeneral and this package in particular.\n\n",
      "technique": "Header extraction"
    }
  ],
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The [deep Q-learning web-forum](https://groups.google.com/forum/#!forum/deep-q-learning)\ncan be used for discussion and advice related to deep Q-learning in\ngeneral and this package in particular.\n\n",
      "technique": "Header extraction"
    }
  ]
}