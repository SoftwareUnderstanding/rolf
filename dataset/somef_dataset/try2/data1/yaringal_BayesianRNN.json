{
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/yaringal/BayesianRNN",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2016-02-18T19:32:04Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-12T09:50:56Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9065256119411962,
        0.9625763105637986
      ],
      "excerpt": "Keras now supports dropout in RNNs following the implementation above. A simplified example of the sentiment analysis experiment using the latest keras implementation is given in here. \nThe script main_new_dropout_SOTA implements Bayesian LSTM (Gal, 2015) for the large model of Zaremba et al. (2014). In the setting of Zaremba et al. the states are not reset and the testing is done with a single pass through the test set. The only changes I've made to the setting of Zaremba et al. are: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9053984546425382,
        0.9367742232195374
      ],
      "excerpt": "All other hypers being identical to Zaremba et al.: learning rate decay was not tuned for my setting and is used following Zaremba et al., and the sequences are initialised with the previous state following Zaremba et al. (unlike in main_dropout.lua). Dropout parameters were optimised with grid search (tying dropout_x & dropout_h and dropout_i & dropout_o) over validation perplexity (optimal values are 0.3 and 0.5 compared Zaremba et al.'s 0.6). \nSingle model validation perplexity is improved from Zaremba et al.'s 82.2 to 79.1. Test perplexity is reduced from 78.4 to 76.5, see log. Evaluating the model with MC dropout with 2000 samples, test perplexity is further reduced to 75.06 (with 100 samples test perplexity is 75.3). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9390673957915522,
        0.8541033986840911
      ],
      "excerpt": "main_new_dropout_SOTA_v3 implements the MC dropout experiment used in the paper, with single model test perplexity improved from Zaremba et al.'s 78.4 to 73.4 (using MC dropout at test time) and 75.2 with the dropout approximation. Validation perplexity is reduced from 82.2 to 77.9. \nI updated the script main_new_dropout_SOTA_v3.lua fixing a bug that @helson73 found (issue #4). In the original script, word embedding dropout was erroneously sampled anew for each word token (ie the word token masks were not tied in the LM experiment, unlike the sentiment analysis experiment). I fixed the code and re-ran the experiments with Variational (untied weights) large LSTM, giving a small improvement in perplexity: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9541515833556852
      ],
      "excerpt": "The improvement is rather small because the sequence length in the LM exps is 20. This means that most sequences will have unique words (ie a word would not appear multiple times in the sequence), hence having the masks untied in such sequences is the same as having the masks tied. Note that in longer sequences such as in the sentiment analysis exps (with sequence length of 200) most sequences will have common words (such as stop words) appearing multiple times in the sequence. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9375479496729893
      ],
      "excerpt": "Gal, Y, \"A Theoretically Grounded Application of Dropout in Recurrent Neural Networks\", 2015. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Code for the paper \"A Theoretically Grounded Application of Dropout in Recurrent Neural Networks\"",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/yaringal/BayesianRNN/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 77,
      "date": "Sat, 25 Dec 2021 12:23:30 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/yaringal/BayesianRNN/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "yaringal/BayesianRNN",
    "technique": "GitHub API"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/yaringal/BayesianRNN/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Lua",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'The MIT License (MIT)\\n\\nCopyright (c) 2015 yaringal\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Gal2015Theoretically). The [sentiment analysis experiment](Sentiment_analysis_code/) relies on a [fork of keras](https://github.com/yaringal/keras/tree/BayesianRNN) which implements Bayesian LSTM, Bayesian GRU, embedding dropout, and MC dropout. The [language model experiment](LM_code/) extends [wojzaremba's lua code](https://github.com/wojzaremba/lstm).",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "BayesianRNN",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "yaringal",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/yaringal/BayesianRNN/blob/master/readme.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 366,
      "date": "Sat, 25 Dec 2021 12:23:30 GMT"
    },
    "technique": "GitHub API"
  }
}