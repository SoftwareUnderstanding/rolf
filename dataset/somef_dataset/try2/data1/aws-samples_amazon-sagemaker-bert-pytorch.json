{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[1] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,\nhttps://arxiv.org/pdf/1810.04805.pdf\n\n[2] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and\nSanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching\nmovies and reading books. In Proceedings of the IEEE international conference on computer vision,\npages 19\u201327.\n\n",
      "technique": "Header extraction"
    }
  ],
  "codeOfConduct": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://raw.githubusercontent.com/aws-samples/amazon-sagemaker-bert-pytorch/master/CODE_OF_CONDUCT.md",
    "technique": "File Exploration"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/aws-samples/amazon-sagemaker-bert-pytorch",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contributing Guidelines\nThank you for your interest in contributing to our project. Whether it's a bug report, new feature, correction, or additional\ndocumentation, we greatly value feedback and contributions from our community.\nPlease read through this document before submitting any issues or pull requests to ensure we have all the necessary\ninformation to effectively respond to your bug report or contribution.\nReporting Bugs/Feature Requests\nWe welcome you to use the GitHub issue tracker to report bugs or suggest features.\nWhen filing an issue, please check existing open, or recently closed, issues to make sure somebody else hasn't already\nreported the issue. Please try to include as much information as you can. Details like these are incredibly useful:\n\nA reproducible test case or series of steps\nThe version of our code being used\nAny modifications you've made relevant to the bug\nAnything unusual about your environment or deployment\n\nContributing via Pull Requests\nContributions via pull requests are much appreciated. Before sending us a pull request, please ensure that:\n\nYou are working against the latest source on the master branch.\nYou check existing open, and recently merged, pull requests to make sure someone else hasn't addressed the problem already.\nYou open an issue to discuss any significant work - we would hate for your time to be wasted.\n\nTo send us a pull request, please:\n\nFork the repository.\nModify the source; please focus on the specific change you are contributing. If you also reformat all the code, it will be hard for us to focus on your change.\nEnsure local tests pass.\nCommit to your fork using clear commit messages.\nSend us a pull request, answering any default questions in the pull request interface.\nPay attention to any automated CI failures reported in the pull request, and stay involved in the conversation.\n\nGitHub provides additional document on forking a repository and\ncreating a pull request.\nFinding contributions to work on\nLooking at the existing issues is a great way to find something to contribute on. As our projects, by default, use the default GitHub issue labels (enhancement/bug/duplicate/help wanted/invalid/question/wontfix), looking at any 'help wanted' issues is a great place to start.\nCode of Conduct\nThis project has adopted the Amazon Open Source Code of Conduct.\nFor more information see the Code of Conduct FAQ or contact\nopensource-codeofconduct@amazon.com with any additional questions or comments.\nSecurity issue notifications\nIf you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our vulnerability reporting page. Please do not create a public github issue.\nLicensing\nSee the LICENSE file for our project's licensing. We will ask you to confirm the licensing of your contribution.\nWe may ask you to sign a Contributor License Agreement (CLA) for larger changes.",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-01-31T00:22:50Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-18T17:25:04Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8714341197877542,
        0.8698184788602992,
        0.9254112870810528,
        0.8133228924817977,
        0.9267490604196816,
        0.9276791514794673,
        0.8786329548835795,
        0.8838864095142607,
        0.8631092712699437,
        0.8287489808231507,
        0.9577372147161533,
        0.9758898204338038
      ],
      "excerpt": "Text classification is a technique for putting text into different categories and has a wide range \nof applications: email providers use text classification to detect to spam emails, marketing \nagencies use it for sentiment analysis of customer reviews, and moderators of discussion forums use \nit to detect inappropriate comments. \nIn the past, data scientists used methods such as tf-idf, \nword2vec, or bag-of-words (BOW) \nto generate features for training classification models. While these techniques have been very \nsuccessful in many NLP tasks, they don't always capture the meanings of words accurately when they \nappear in different contexts. Recently, we see increasing interest in using Bidirectional Encoder \nRepresentations from Transformers (BERT) to achieve better results in text classification tasks, \ndue to its ability more accurately encode the meaning of words in different contexts. \nAmazon SageMaker is a fully managed service that provides developers and data scientists with the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.936047107072765
      ],
      "excerpt": "the heavy lifting from each step of the machine learning process to make it easier to develop \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9221392513243882,
        0.862787555580642,
        0.8299779782981412
      ],
      "excerpt": "easy to train and deploy models in Amazon SageMaker with several different machine learning and \ndeep learning frameworks. We use an Amazon SageMaker Notebook Instance for running the code. \nFor information on how to use Amazon SageMaker Notebook Instances, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.917861799465963,
        0.852038070431547
      ],
      "excerpt": "Our customers often ask for quick fine-tuning and easy deployment of their NLP models. Furthermore, \ncustomers prefer low inference latency and low model inference cost. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9444031574338164
      ],
      "excerpt": "attaching GPU-powered inference acceleration to endpoints, reducing the cost of deep learning \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9722847625978507,
        0.9576049042026229,
        0.9456829614881165,
        0.908925214220865
      ],
      "excerpt": "The notebook in this repository demonstrates how to use Amazon SageMaker to fine tune a PyTorch \nBERT model and deploy it with Elastic Inference. We walk through our dataset, the training process, \nand finally model deployment. This work is inspired by a post by \nChris McCormick and Nick Ryan. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8625148601934314,
        0.8517713677169102,
        0.941382166229458,
        0.9515455699751012,
        0.8460093578091802,
        0.9330537515440218,
        0.9706683053334487,
        0.9266783079280485,
        0.8893448875268035,
        0.8909017175738522,
        0.8442636515317746,
        0.9821916110674341
      ],
      "excerpt": "sentences are intentionally masked. BERT takes in these masked sentences as input and trains itself \nto predict the masked word. In addition, BERT uses a \"next sentence prediction\" task that pre-trains \ntext-pair representations. BERT is a substantial breakthrough and has helped researchers and data \nengineers across industry to achieve state-of-art results in many Natural Language Processing (NLP) \ntasks. BERT offers representation of each word conditioned on its context (rest of the sentence). \nFor more information about BERT, please refer to [1]. \nOne of the biggest challenges data scientists face for NLP projects is lack of training data; they \noften have only a few thousand pieces of human-labeled text data for their model training. However, \nmodern deep learning NLP tasks require a large amount of labeled data. One way to solve this problem \nis to use transfer learning. \nTransfer learning is a machine learning method where a pre-trained model, such as a pre-trained \nResNet model for image classification, is reused as the starting point for a different but related \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8705036638742941,
        0.8705098048032058
      ],
      "excerpt": "time and cost. \nBERT was trained on BookCorpus and English Wikipedia data, which contain 800 million words and \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8651971903865145,
        0.9764717602226664
      ],
      "excerpt": "By taking advantage of transfer learning, one can quickly fine tune BERT for another use case with a \nrelatively small amount of training data to achieve state-of-the-art results for common NLP tasks, \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/aws-samples/amazon-sagemaker-bert-pytorch/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 18,
      "date": "Sun, 26 Dec 2021 05:05:56 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/aws-samples/amazon-sagemaker-bert-pytorch/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "aws-samples/amazon-sagemaker-bert-pytorch",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/aws-samples/amazon-sagemaker-bert-pytorch/master/bert-sm-python-SDK.ipynb",
      "https://raw.githubusercontent.com/aws-samples/amazon-sagemaker-bert-pytorch/master/.ipynb_checkpoints/bert-sm-python-SDK-checkpoint.ipynb"
    ],
    "technique": "File Exploration"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/aws-samples/amazon-sagemaker-bert-pytorch/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other",
      "url": "https://raw.githubusercontent.com/aws-samples/amazon-sagemaker-bert-pytorch/master/LICENSE"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy of\\nthis software and associated documentation files (the \"Software\"), to deal in\\nthe Software without restriction, including without limitation the rights to\\nuse, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\\nthe Software, and to permit persons to whom the Software is furnished to do so.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\\nFOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\\nCOPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\\nIN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\\nCONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\\n\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Fine tune a PyTorch BERT model and deploy it with Elastic Inference on Amazon SageMaker",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "amazon-sagemaker-bert-pytorch",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "aws-samples",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/aws-samples/amazon-sagemaker-bert-pytorch/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 48,
      "date": "Sun, 26 Dec 2021 05:05:56 GMT"
    },
    "technique": "GitHub API"
  }
}