{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1508.01211\n\n[2] J. Chorowski _et al._, \"Attention-Based Models for Speech Recognition\",\nhttps://arxiv.org/abs/1506.07503\n\n[3] M. Luong _et al._, \"Effective Approaches to Attention-based Neural Machine Translation\",\nhttps://arxiv.org/abs/1508.04025\n\n[4] D. Park _et al._, \"SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition\",\nhttps://arxiv.org/abs/1904.08779",
      "https://arxiv.org/abs/1506.07503\n\n[3] M. Luong _et al._, \"Effective Approaches to Attention-based Neural Machine Translation\",\nhttps://arxiv.org/abs/1508.04025\n\n[4] D. Park _et al._, \"SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition\",\nhttps://arxiv.org/abs/1904.08779",
      "https://arxiv.org/abs/1508.04025\n\n[4] D. Park _et al._, \"SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition\",\nhttps://arxiv.org/abs/1904.08779",
      "https://arxiv.org/abs/1904.08779"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[1] W. Chan _et al._, \"Listen, Attend and Spell\",\nhttps://arxiv.org/abs/1508.01211\n\n[2] J. Chorowski _et al._, \"Attention-Based Models for Speech Recognition\",\nhttps://arxiv.org/abs/1506.07503\n\n[3] M. Luong _et al._, \"Effective Approaches to Attention-based Neural Machine Translation\",\nhttps://arxiv.org/abs/1508.04025\n\n[4] D. Park _et al._, \"SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition\",\nhttps://arxiv.org/abs/1904.08779\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8857082268549297
      ],
      "excerpt": "End-to-end speech recognition on AISHELL dataset using Pytorch. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/biyoml/End-to-End-Mandarin-ASR",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-06-24T03:55:21Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-22T06:55:30Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8299350265050334,
        0.9301082452415301,
        0.9721953772976792,
        0.9756347080555258,
        0.9527976628006629
      ],
      "excerpt": "End-to-end speech recognition on AISHELL dataset using Pytorch. \nThe entire system is an attention-based sequence-to-sequence model<sup>1</sup>. \nThe encoder is a bidirectional GRU net with BatchNorm, and the decoder is another GRU net that applies Luong-based attention<sup>3</sup>. \nThe acoustic features are 80-dimensional filter banks. We apply SpecAugment<sup>4</sup> to these features to improve generalization. \nThey are also stacked every 3 consecutive frames, so the time resolution is reduced. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "End-to-end speech recognition on AISHELL dataset.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jackjhliu/End-to-End-Mandarin-ASR/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 4,
      "date": "Sat, 25 Dec 2021 02:06:46 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/biyoml/End-to-End-Mandarin-ASR/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "biyoml/End-to-End-Mandarin-ASR",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```bash\n$ pip install -r requirements.txt\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9246227682586091,
        0.8402342789897141
      ],
      "excerpt": "$ python extract_aishell.py ${PATH_TO_FILE} \nCreate lists (*.csv) of audio file paths along with their transcripts: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "$ python prepare_data.py ${DIRECTORY_OF_AISHELL} \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9503189345333785
      ],
      "excerpt": "$ python train.py -h \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9383004500983586
      ],
      "excerpt": "$ python train.py exp/default.yaml \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9503189345333785
      ],
      "excerpt": "$ python train.py ${PATH_TO_YOUR_CONFIG} \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8157944488586287,
        0.841662304846247
      ],
      "excerpt": "The checkpoint with the lowest error rate will be saved in the logging directory (by default exp/default/best.pth). \nTo evalutate the checkpoint on test set (with a beam width of 5), run: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8901532013527039
      ],
      "excerpt": "$ python eval.py exp/default/best.pth --beams 5 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8402526537601729
      ],
      "excerpt": "$ python inference.py exp/default/best.pth --beams 5 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/biyoml/End-to-End-Mandarin-ASR/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "End-to-End-Mandarin-ASR",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "End-to-End-Mandarin-ASR",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "biyoml",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/biyoml/End-to-End-Mandarin-ASR/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```bash\n$ pip install -r requirements.txt\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 16,
      "date": "Sat, 25 Dec 2021 02:06:46 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "asr",
      "end-to-end",
      "mandarin",
      "pytorch",
      "chinese-speech-recognition",
      "aishell",
      "specaugment",
      "speech-recognition"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "With the default configuration, the training logs are stored in `exp/default/history.csv`.\nYou should specify your training logs accordingly.\n```bash\n$ python show_history.py exp/default/history.csv\n```\n![](./img/Figure_1.png)\n\n",
      "technique": "Header extraction"
    }
  ]
}