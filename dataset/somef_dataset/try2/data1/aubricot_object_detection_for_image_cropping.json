{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2004.10934",
      "https://arxiv.org/abs/1801.04381",
      "https://arxiv.org/abs/1512.00567"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* [AlexeyAB 2020](https://github.com/AlexeyAB/darknet). Darknet. GitHub.\n* [Bochkovskiy et al. 2020](https://arxiv.org/abs/2004.10934). YOLOv4: Optimal Speed and Accuracy of Object Detection. arXiv.\n* [Encyclopedia of Life](http://eol.org) \n* [Goeau et al. 2016](http://ceur-ws.org/Vol-1609/16090428.pdf). Plant identification in an open-world (LifeCLEF 2016). CEUR Workshop Proceedings. \n* [Hui 2018](medium.com/@jonathan_hui/object-detection-speed-and-accuracy-comparison-faster-r-cnn-r-fcn-ssd-and-yolo-5425656ae359). Object detection: speed and accuracy comparison (Faster R-CNN, R-FCN, SSD, FPN, RetinaNet and YOLOv3). Medium. 27 March 2018.   \n* [Krasin et al. 2017](https://github.com/openimages). Open- images: A public dataset for large-scale multi-label and multi-class image classification. GitHub.\n* [Lin et al. 2015](https://arxiv.org/pdf/1405.0312.pdf). Microsoft COCO: Common Objects in Context.  \n* [PlantCLEF 2016 Image dataset](https://www.imageclef.org/lifeclef/2016/plant)\n* [Redmon and Farhadi 2018](https://arxiv.org/pdf/1804.02767.pdf). YOLOv3: An Incremental Improvement. \n* [Sandler et al. 2018](https://arxiv.org/abs/1801.04381). Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation.\" arXiv.\n* [Sharma 2019](medium.com/analytics-vidhya/image-classification-vs-object-detection-vs-image-segmentation-f36db85fe81). Image Classification vs. Object Detection vs. Image Segmentation. Medium. 23 Feb 2020.\n* [Szegedy et al. 2015](https://arxiv.org/abs/1512.00567). Rethinking the Inception Architecture for Computer Vision. arXiv.\n\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/aubricot/computer_vision_with_eol_images",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-11-18T04:53:08Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-17T17:11:05Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9844915638883229
      ],
      "excerpt": "The Encyclopedia of Life (EOL) is an online biodiversity resource that seeks to provide information about all ~1.9 million species known to science. A goal for the latest version of EOL (v3) is to better leverage the older, less structured image content. To improve discoverability and display of EOL images, automated image processing pipelines that use computer vision with the goal of being scalable to millions of diverse images are being developed and tested. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9848679770170155
      ],
      "excerpt": "Object detection for image cropping: Three object detection frameworks (Faster-RCNN Resnet 50 and either SSD/R-FCN/Faster-RCNN Inception v2 detection via the Tensorflow Object Detection API and YOLO via Darkflow) were used to perform customized, large-scale image processing for different groups of animals (birds, bats, butterflies & moths, beetles, frogs, carnivores, snakes & lizards) found in the Encyclopedia of Life v3 database by employing transfer learning and/or fine-tuning. The three frameworks differ in their speeds and accuracy: YOLO has been found to be the fastest but least accurate, while Faster RCNN was found to be the slowest but most accurate, with MobileNet SSD and R-FCN falling somewhere in between (Lin et al. 2017, Hui 2018, Redmon and Farhadi 2018). The model with the best trade-off between speed and accuracy for each group was selected to generate final cropping data for EOL images. The location and dimensions of the detected animals within each framework are used to crop images to square dimensions that are centered and padded around the detection box.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9089276387992057
      ],
      "excerpt": "* For <ins>beetles, frogs, carnivores and snakes & lizards</ins>, object detection models were custom-trained to detect all classes simultaneously using EOL user-generated cropping data.   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8700550383826018,
        0.9864319961479292
      ],
      "excerpt": "<sub><sup> Screenshot of object detection results using trained multitaxa detector model displayed in a Jupyter Notebook running in Google Colab. Image is hosted by Encyclopedia of Life (<a href=\"https://content.eol.org/data/media/28/09/c5/18.https___www_inaturalist_org_photos_1003389.jpg\"><i>Lampropeltis californiae</i></a>, licensed under <a href=\"https://creativecommons.org/licenses/by-nc/4.0/\"></a>CC BY-NC 4.0</a>.</sup></sub></p> \nClassification for image tagging: Pre-trained and custom-built classification frameworks will be used to perform automated, large-scale image processing for different classes of images (flowers, maps/labels/illustrations, image ratings) found in the Encyclopedia of Life v3 database by employing transfer learning and/or fine-tuning. Image tagging will improve EOL user experience by allowing users to search for features within images that are not already noted in metadata, as well as improving gallery display and API user functionality with increased image sorting abilities. The process of image classification is slightly different than object detection (Sharma 2019). While object detection includes classification and localization of the object of interest, image classification only includes the former step. Classification will be used to identify images with flowers present, images of maps/collection labels/illustrations, and to generate image quality ratings. For each of the three classifiers, a research phase is first used to identify available datasets and pre-trained classification models to be used for developing processing pipelines.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Testing different computer vision methods (object detection, image classification) to do customized, large-scale image processing for Encylopedia of Life images.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/aubricot/object_detection_for_image_cropping/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 4,
      "date": "Sat, 25 Dec 2021 06:01:26 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/aubricot/computer_vision_with_eol_images/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "aubricot/computer_vision_with_eol_images",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/display_EOLbundle_images.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/generate_drive_fileid.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/object_detection_for_image_cropping/inspect_crop_data.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/object_detection_for_image_cropping/coordinates_display_test.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/object_detection_for_image_cropping/play_for_beginners/squarespace_yolo.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/object_detection_for_image_cropping/play_for_beginners/squarespace_augment_images.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/object_detection_for_image_cropping/lepidoptera/lepidoptera_train_yolo.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/object_detection_for_image_cropping/lepidoptera/lepidoptera_preprocessing.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/object_detection_for_image_cropping/lepidoptera/lepidoptera_generate_crops_tf2.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/object_detection_for_image_cropping/lepidoptera/lepidoptera_train_tf2_ssd_rcnn.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/object_detection_for_image_cropping/lepidoptera/archive/lepidoptera_train_tf_ssd_rcnn.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/object_detection_for_image_cropping/lepidoptera/archive/lepidoptera_split_train_test.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/object_detection_for_image_cropping/aves/aves_generate_crops_tf2.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/object_detection_for_image_cropping/aves/aves_generate_crops_yolo.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/object_detection_for_image_cropping/aves/archive/aves_yolo.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/object_detection_for_image_cropping/aves/archive/aves_tf_ssd_rcnn.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/object_detection_for_image_cropping/archive/calculate_error_mAP.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/object_detection_for_image_cropping/multitaxa/multitaxa_generate_crops_tf2.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/object_detection_for_image_cropping/multitaxa/multitaxa_train_yolo.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/object_detection_for_image_cropping/multitaxa/multitaxa_train_tf2_rcnns.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/object_detection_for_image_cropping/multitaxa/multitaxa_preprocessing.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/object_detection_for_image_cropping/multitaxa/archive/multitaxa_train_tf_rcnns.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/object_detection_for_image_cropping/multitaxa/archive/multitaxa_split_train_test.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/object_detection_for_image_cropping/chiroptera/chiroptera_preprocessing.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/object_detection_for_image_cropping/chiroptera/chiroptera_train_yolo.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/object_detection_for_image_cropping/chiroptera/chiroptera_generate_crops_tf2.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/object_detection_for_image_cropping/chiroptera/chiroptera_train_tf2_ssd_rcnn.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/object_detection_for_image_cropping/chiroptera/archive/chiroptera_preprocessing.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/object_detection_for_image_cropping/chiroptera/archive/chiroptera_train_tf_ssd_rcnn.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/object_detection_for_image_cropping/chiroptera/archive/chiroptera_split_train_test.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/classification_for_image_tagging/rating/inspect_train_results.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/classification_for_image_tagging/rating/classify_images.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/classification_for_image_tagging/rating/rating_preprocessing.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/classification_for_image_tagging/rating/rating_train.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/classification_for_image_tagging/image_type/inspect_train_results.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/classification_for_image_tagging/image_type/image_type_train.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/classification_for_image_tagging/image_type/classify_images.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/classification_for_image_tagging/image_type/image_type_preprocessing.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/classification_for_image_tagging/image_type/cartoonify_images.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/classification_for_image_tagging/flowers/flowers_train.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/classification_for_image_tagging/flowers/classification_display_test.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/classification_for_image_tagging/flower_fruit/det_conf_threshold.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/classification_for_image_tagging/flower_fruit/flower_fruit_train.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/classification_for_image_tagging/flower_fruit/classify_images.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/classification_for_image_tagging/flower_fruit/flower_fruit_preprocessing.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/object_detection_for_image_tagging/scat_footprint/scat_footprint_train_yolov3.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/object_detection_for_image_tagging/scat_footprint/scat_footprint_train_yolo_darkflow.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/object_detection_for_image_tagging/scat_footprint/scat_footprint_preprocessing.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/object_detection_for_image_tagging/life_stages/insect_life_stages_generate_tags_yolov3.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/object_detection_for_image_tagging/life_stages/archive/insect_lifestages_yolov3.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/object_detection_for_image_tagging/plant_pollinator/plant_poll_generate_tags_yolov3.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/object_detection_for_image_tagging/plant_pollinator/archive/plant_poll_cooccurrence_yolov3.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/object_detection_for_image_tagging/flower_fruit/flower_fruit_generate_tags_yolov3.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/object_detection_for_image_tagging/human_present/human_present_generate_tags_yolov3.ipynb",
      "https://raw.githubusercontent.com/aubricot/object_detection_for_image_cropping/master/object_detection_for_image_tagging/human_present/archive/human_present_yolov3.ipynb"
    ],
    "technique": "File Exploration"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/aubricot/computer_vision_with_eol_images/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2021 aubricot\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Computer Vision with EOL v3 Images",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "computer_vision_with_eol_images",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "aubricot",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/aubricot/computer_vision_with_eol_images/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 4,
      "date": "Sat, 25 Dec 2021 06:01:26 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "image-classification",
      "object-detection",
      "eol",
      "encyclopedia-of-life",
      "computer-vision",
      "image-management"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "All files in this repository are run in [Google Colab](https://research.google.com/colaboratory/faq.html)*. This repository is set up so that each notebook can be run as a standalone script. It is not necessary to clone the entire repository. Instead, you can find project sections that are interesting and directly try the notebooks for yourself! All needed files and directories are set up within the notebook.\n\nFor additional details on steps below, see the [project wiki](https://github.com/aubricot/computer_vision_with_eol_images/wiki).\n\n**New to Google Colab?   \nMore info: Google Colaboratory is \"a free cloud service, based on Jupyter Notebooks for machine-learning education and research.\" Notebooks run entirely on VMs in the cloud and links to you Google Drive for accessing files. This means no local software or library installs are requried. If running locally and using a GPU, there are several softwares that need to be installed first and take up ~10 GB and a few workarounds are required if running on a Windows OS. Working in the cloud eliminates these problems and makes it easier to collaborate if multiple users are on different operating systems. If you prefer to use your local machine for object detection, refer to the [Tensorflow Object Detection API Tutorial](https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb).*   \n\n",
      "technique": "Header extraction"
    }
  ]
}