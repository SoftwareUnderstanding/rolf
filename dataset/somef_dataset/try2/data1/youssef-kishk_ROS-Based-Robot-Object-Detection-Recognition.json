{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This repository contains a description of the Object detection and recognition Task for (**Benzo**), a personal assistance robot based on **ROS (Robot Operating System)** that is able to perform multiple tasks such as:\n- Handling voice commands\n- Map Building\n- Objects Detection and recognition\n- Navigation and Localization\n\nYou can check the project summary from the following **video** (https://youtu.be/AEgZd6wD7dk)\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "citation": [
    {
      "confidence": [
        0.9944484218006108,
        0.9944484218006108,
        0.9944484218006108
      ],
      "excerpt": " - https://arxiv.org/pdf/1506.02640.pdf \n - https://arxiv.org/pdf/1612.08242.pdf \n - https://arxiv.org/pdf/1804.02767.pdf \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/youssef-kishk/ROS-Based-Robot-Object-Detection-Recognition",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-09-21T11:13:42Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-09-24T15:36:34Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This repository shows how the Object detection and Recognition task can be performed for a personal assistance robot based on **ROS (Robot Operating System)**.\n\nThe whole project can be found in this youtube [video](https://youtu.be/AEgZd6wD7dk)\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "The following are the detailed task steps:\n1) Extracting the real time image from the Kinect Camera\n2) Detect and Recognize different objects in each image frame from the camera\n3) Check distance between the robot camera and each of the detected objects\n4) Each of the detected objects within a certain distance from the robot camera is stored with its equivelant position on the envirnoment map extracted from the Rtabmap current odometry published topic\n5) While performing the above 4 steps, the robot keep waiting for any voice command sent from the user using the mobile app including the name of an object, in order to start navigating towards the required it if exists in the stored dictionary of previously detected objects and their location, as shown in the figure below.\n\n<p align=\"center\">\n<img src=\"https://github.com/youssef-kishk/ROS-Based-Robot-Object-Detection-Recognition-Module/blob/master/images/image.png\" width=\"600\" height=\"300\" />\n </p>\n \n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "In the Object detection and recognition task, we are dealing with a **Microsoft xbox 360 Kinect camera RGB D** to have a real time image of the environment of the robot, which is then used to detect different objects using **You Only Look Once (YOLO) version 2** model, pretrained on a dataset of 80 different object categories [Common Objects in Context (COCO) dataset](https://cocodataset.org/).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9270684308629354
      ],
      "excerpt": "Using the cv_bridge ROS package which converts between ROS Image messages the kinect camera sends on the ROS Topic /camera/rgb/image_color to OpenCV images so we can deal with in our detection and recognition process. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8566741438415315
      ],
      "excerpt": " We were able to detect different objects in each of the opencv images \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9664245186724496,
        0.9410408348509934,
        0.9579462120991221
      ],
      "excerpt": "One of the main challenges of the task was to determine the distance between the robot and the different objects detected, so we can store each of the objects with its approximated exact position on the map in order to navigate towards the object successfully then. \nUsing the Kinect 3D depth sensor cameras we were able to extract a 2D array for the depth from the Robot on each pixel of the detection image containing multiple objects as the one shown in the previous figure. \nAs shown in below figure, Assume a,b,c,d is the frame of a detected object so we can get the depth values from the 2D depth array at the edges of that frame which is values: a, b, c, d, e in mm \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8255850552341406,
        0.9239042650351824
      ],
      "excerpt": "By checking those depth values we can make an accurate estimation of how far the object is from the Robot and accept it if within an acceptable range and close enough. \nFor the objects accepted within the range from the previous step, we store each detected object name as a string associated with its position on the map captured from the ROS topic /rtabmap/odom , so it can be used later when the user wants the robot to navigate to any specific object by simply extracting the position value from the map and pass it to the GO-TO module. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Brief Description of the Object detection and recognition Task for a ROS Based Personal Assistance Robot",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/youssef-kishk/ROS-Based-Robot-Object-Detection-Recognition/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 28 Dec 2021 09:19:52 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/youssef-kishk/ROS-Based-Robot-Object-Detection-Recognition/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "youssef-kishk/ROS-Based-Robot-Object-Detection-Recognition",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.9118106785405024
      ],
      "excerpt": "Using YOLO pretrained on COCO dataset, \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/youssef-kishk/ROS-Based-Robot-Object-Detection-Recognition/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "ROS Based Robot Object detection and Recognition Task",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "ROS-Based-Robot-Object-Detection-Recognition",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "youssef-kishk",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/youssef-kishk/ROS-Based-Robot-Object-Detection-Recognition/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Tue, 28 Dec 2021 09:19:52 GMT"
    },
    "technique": "GitHub API"
  }
}