{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1603.06937](http://arxiv.org/abs/1603.06937), 2016.\n\nA pretrained model is available on the [project site](http://www-personal.umich.edu/~alnewell/pose). Include the model in the main directory of this repository to run the demo code.\n\n**Check out the training and experimentation code now available at: [https://github.com/anewell/pose-hg-train](https://github.com/anewell/pose-hg-train)**\n\nIn addition, if you download the full [MPII Human Pose dataset](http://human-pose.mpi-inf.mpg.de) and replace this repository's `images` directory you can generate full predictions on the validation and test sets.\n\nTo run this code, the following must be installed:\n\n- [Torch7](https://github.com/torch/torch7)\n- hdf5 (and the [torch-hdf5](https://github.com/deepmind/torch-hdf5/) package)\n- cudnn\n- qlua (for displaying results)\n\nFor displaying the demo images:\n`qlua main.lua demo`\n\nFor generating predictions:\n`th main.lua predict-[valid or test]`\n\nFor evaluation on a set of validation predictions:\n`th main.lua eval` \n\n## Testing your own images\n\nTo use the network off-the-shelf, it is critical that the target person is centered in the input image. There is some robustness to scale, but for best performance the person should be sized such that their full height is roughly three-quarters of the input height. Play around with different scale settings to see the impact it has on the network output. We offer a convenient function for generating an input image:\n\n`inputImg = crop(img, center, scale, rot, res)`\n\n`res` should be set to 256 for our network. `rot` is offered if you wish to rotate the image (in degrees). You can run the input image through the network, and get the (x,y) coordinates with:\n\n`outputHm = m:forward(inputImg:view(1,3,256,256):cuda())`\n\n`predsHm,predsImg = getPreds(outputHm, center, scale)`\n\nThe two outputs of `getPreds` are coordinates with respect to either the heatmap or the original image (using center and scale to apply the appropriate transformation back to the image space).\n\nThe MPII images come with center and scale annotations already. An important detail with regards to the annotations: we have modified their format slightly for ease of use with our code. In addition, we adjusted the original center and scale annotations uniformly across all images so as to reduce the chances of our function cropping out feet from the bottom of the image. This mostly involved moving the center down a fraction.\n"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/princeton-vl/pose-hg-demo",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2016-03-17T16:33:13Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-12T10:29:12Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9921984584372839
      ],
      "excerpt": "To use the network off-the-shelf, it is critical that the target person is centered in the input image. There is some robustness to scale, but for best performance the person should be sized such that their full height is roughly three-quarters of the input height. Play around with different scale settings to see the impact it has on the network output. We offer a convenient function for generating an input image: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8470801024604194
      ],
      "excerpt": "res should be set to 256 for our network. rot is offered if you wish to rotate the image (in degrees). You can run the input image through the network, and get the (x,y) coordinates with: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Code to test and use the model from \"Stacked Hourglass Networks for Human Pose Estimation\"",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/umich-vl/pose-hg-demo/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 91,
      "date": "Sun, 26 Dec 2021 18:29:17 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/princeton-vl/pose-hg-demo/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "princeton-vl/pose-hg-demo",
    "technique": "GitHub API"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/princeton-vl/pose-hg-demo/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Lua"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "BSD 3-Clause \"New\" or \"Revised\" License",
      "url": "https://api.github.com/licenses/bsd-3-clause"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'Copyright (c) 2016, University of Michigan\\nAll rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\\n\\n1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\\n\\n2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\\n\\n3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Stacked Hourglass Networks for Human Pose Estimation (Demo Code)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "pose-hg-demo",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "princeton-vl",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/princeton-vl/pose-hg-demo/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 318,
      "date": "Sun, 26 Dec 2021 18:29:17 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This repository includes Torch code for evaluation and visualization of the network presented in:\n\nAlejandro Newell, Kaiyu Yang, and Jia Deng,\n**Stacked Hourglass Networks for Human Pose Estimation**,\n[arXiv:1603.06937](http://arxiv.org/abs/1603.06937), 2016.\n\nA pretrained model is available on the [project site](http://www-personal.umich.edu/~alnewell/pose). Include the model in the main directory of this repository to run the demo code.\n\n**Check out the training and experimentation code now available at: [https://github.com/anewell/pose-hg-train](https://github.com/anewell/pose-hg-train)**\n\nIn addition, if you download the full [MPII Human Pose dataset](http://human-pose.mpi-inf.mpg.de) and replace this repository's `images` directory you can generate full predictions on the validation and test sets.\n\nTo run this code, the following must be installed:\n\n- [Torch7](https://github.com/torch/torch7)\n- hdf5 (and the [torch-hdf5](https://github.com/deepmind/torch-hdf5/) package)\n- cudnn\n- qlua (for displaying results)\n\nFor displaying the demo images:\n`qlua main.lua demo`\n\nFor generating predictions:\n`th main.lua predict-[valid or test]`\n\nFor evaluation on a set of validation predictions:\n`th main.lua eval` \n\n",
      "technique": "Header extraction"
    }
  ]
}