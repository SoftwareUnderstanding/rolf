{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2008.00951\"><img src=\"https://img.shields.io/badge/arXiv-2008.00951-b31b1b.svg\" height=22.5></a>\n<a href=\"https://opensource.org/licenses/MIT\"><img src=\"https://img.shields.io/badge/License-MIT-yellow.svg\" height=22.5></a>  \n\n<a href=\"https://www.youtube.com/watch?v=bfvSwhqsTgM\"><img src=\"https://img.shields.io/static/v1?label=CVPR 2021&message=5 Minute Video&color=red\" height=22.5></a>  \n<a href=\"https://replicate.ai/eladrich/pixel2style2pixel\"><img src=\"https://img.shields.io/static/v1?label=Replicate&message=Demo and Docker Image&color=darkgreen\" height=22.5></a>\n\n<a href=\"http://colab.research.google.com/github/eladrich/pixel2style2pixel/blob/master/notebooks/inference_playground.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" height=22.5></a>  \n\n> We present a generic image-to-image translation framework, pixel2style2pixel (pSp",
      "https://arxiv.org/abs/2011.12799",
      "https://arxiv.org/abs/2102.02766",
      "https://arxiv.org/abs/2008.00951\">Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation</a>:\n\n```\n@InProceedings{richardson2021encoding,\n      author = {Richardson, Elad and Alaluf, Yuval and Patashnik, Or and Nitzan, Yotam and Azar, Yaniv and Shapiro, Stav and Cohen-Or, Daniel},\n      title = {Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation},\n      booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you use this code for your research, please cite our paper <a href=\"https://arxiv.org/abs/2008.00951\">Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation</a>:\n\n```\n@InProceedings{richardson2021encoding,\n      author = {Richardson, Elad and Alaluf, Yuval and Patashnik, Or and Nitzan, Yotam and Azar, Yaniv and Shapiro, Stav and Cohen-Or, Daniel},\n      title = {Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation},\n      booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n      month = {June},\n      year = {2021}\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "**StyleGAN2 implementation:**  \nhttps://github.com/rosinality/stylegan2-pytorch  \nCopyright (c) 2019 Kim Seonghyeon  \nLicense (MIT) https://github.com/rosinality/stylegan2-pytorch/blob/master/LICENSE  \n\n**MTCNN, IR-SE50, and ArcFace models and implementations:**  \nhttps://github.com/TreB1eN/InsightFace_Pytorch  \nCopyright (c) 2018 TreB1eN  \nLicense (MIT) https://github.com/TreB1eN/InsightFace_Pytorch/blob/master/LICENSE  \n\n**CurricularFace model and implementation:**   \nhttps://github.com/HuangYG123/CurricularFace  \nCopyright (c) 2020 HuangYG123  \nLicense (MIT) https://github.com/HuangYG123/CurricularFace/blob/master/LICENSE  \n\n**Ranger optimizer implementation:**  \nhttps://github.com/lessw2020/Ranger-Deep-Learning-Optimizer   \nLicense (Apache License 2.0) https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer/blob/master/LICENSE  \n\n**LPIPS implementation:**  \nhttps://github.com/S-aiueo32/lpips-pytorch  \nCopyright (c) 2020, Sou Uchida  \nLicense (BSD 2-Clause) https://github.com/S-aiueo32/lpips-pytorch/blob/master/LICENSE  \n\n**Please Note**: The CUDA files under the [StyleGAN2 ops directory](https://github.com/eladrich/pixel2style2pixel/tree/master/models/stylegan2/op) are made available under the [Nvidia Source Code License-NC](https://nvlabs.github.io/stylegan2/license.html)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@InProceedings{richardson2021encoding,\n      author = {Richardson, Elad and Alaluf, Yuval and Patashnik, Or and Nitzan, Yotam and Azar, Yaniv and Shapiro, Stav and Cohen-Or, Daniel},\n      title = {Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation},\n      booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n      month = {June},\n      year = {2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9998049529230129,
        0.9428847177018095,
        0.9763709397001502
      ],
      "excerpt": "<a href=\"https://arxiv.org/abs/2008.00951\"><img src=\"https://img.shields.io/badge/arXiv-2008.00951-b31b1b.svg\" height=22.5></a> \n<a href=\"https://opensource.org/licenses/MIT\"><img src=\"https://img.shields.io/badge/License-MIT-yellow.svg\" height=22.5></a>   \n<a href=\"https://www.youtube.com/watch?v=bfvSwhqsTgM\"><img src=\"https://img.shields.io/static/v1?label=CVPR 2021&message=5 Minute Video&color=red\" height=22.5></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.866028625533202
      ],
      "excerpt": "<a href=\"http://colab.research.google.com/github/eladrich/pixel2style2pixel/blob/master/notebooks/inference_playground.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" height=22.5></a>   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8051836954530404
      ],
      "excerpt": "2020.10.04: Initial code release \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9380142677289868
      ],
      "excerpt": "--latent_mask=8,9,10,11,12,13,14,15,16,17  \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/eladrich/pixel2style2pixel",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-10-01T11:01:03Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-24T03:33:48Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Official Implementation of our pSp paper for both training and evaluation. The pSp method extends the StyleGAN model to \nallow solving different image-to-image translation problems using its encoder.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8076861724250853,
        0.9822235464084835,
        0.9095471808516218,
        0.9165906623847849
      ],
      "excerpt": "We present a generic image-to-image translation framework, pixel2style2pixel (pSp).  \nOur pSp framework is based on a novel encoder network that directly generates a series of style vectors which are fed into a pretrained StyleGAN generator,  \nforming the extended W+ latent space. We first show that our encoder can directly embed real images into W+, with no additional optimization. \nNext, we propose utilizing our encoder to directly solve image-to-image translation tasks, defining them as encoding problems from some input domain into the  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9277305035465678,
        0.9034881000803084,
        0.9963507913957769
      ],
      "excerpt": "tasks even when the input image is not represented in the StyleGAN domain. We show that solving translation tasks through StyleGAN significantly simplifies the training process, as no adversary is required, has better support  \nfor solving tasks without pixel-to-pixel correspondence, and inherently supports multi-modal synthesis via the resampling of styles.  \nFinally, we demonstrate the potential of our framework on a variety of facial image-to-image translation tasks, even when compared to state-of-the-art solutions designed specifically for a single task, and further show that it can be extended beyond the human facial domain.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9522030525473868
      ],
      "excerpt": "The proposed pixel2style2pixel framework can be used to solve a wide variety of image-to-image translation tasks. Here we show results of pSp on StyleGAN inversion, multi-modal conditional image synthesis, facial frontalization, inpainting and super-resolution. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8651205655089669
      ],
      "excerpt": "2020.10.06: Add pSp toonify model (Thanks to the great work from Doron Adler and Justin Pinkney)! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9657628950064033,
        0.877146863568,
        0.9224193739767544,
        0.8408590882161251
      ],
      "excerpt": "  - Added supported for StyleGANs of different resolutions (e.g., 256, 512, 1024). This can be set using the flag --output_size, which is set to 1024 by default.  \n  - Added support for the MoCo-Based similarity loss introduced in encoder4editing (Tov et al. 2021). More details are provided below.   \n2021.07.06: Added support for training with Weights & Biases. See below for details. \nHere, we use pSp to find the latent code of real images in the latent domain of a pretrained StyleGAN generator.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8636577253190403
      ],
      "excerpt": "Given a low-resolution input image, we generate a corresponding high-resolution image. As this too is an ambiguous task, we can use style-mixing to produce several plausible results. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.967274152896222
      ],
      "excerpt": "To help visualize the pSp framework on multiple tasks and to help you get started, we provide a Jupyter notebook found in notebooks/inference_playground.ipynb that allows one to visualize the various applications of pSp.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.928181889172213
      ],
      "excerpt": "For the tasks of conditional image synthesis and super resolution, the notebook also demonstrates pSp's ability to perform multi-modal synthesis using  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8784064601843594
      ],
      "excerpt": "In addition, we provide various auxiliary models needed for training your own pSp model from scratch as well as pretrained models needed for computing the ID metrics reported in the paper. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8149304801789785,
        0.8751423388718108,
        0.9695908458270817,
        0.8388407567928262,
        0.887632503231055
      ],
      "excerpt": "|FFHQ StyleGAN | StyleGAN model pretrained on FFHQ taken from rosinality with 1024x1024 output resolution. \n|IR-SE50 Model | Pretrained IR-SE50 model taken from TreB1eN for use in our ID loss during pSp training. \n|MoCo ResNet-50  | Pretrained ResNet-50 model trained using MOCOv2 for computing MoCo-based similarity loss on non-facial domains. The model is taken from the official implementation. \n|CurricularFace Backbone  | Pretrained CurricularFace model taken from HuangYG123 for use in ID similarity metric computation. \n|MTCNN  | Weights for MTCNN model taken from TreB1eN for use in ID similarity metric computation. (Unpack the tar.gz to extract the 3 model weights.) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.912002370988404
      ],
      "excerpt": "is the number of semantic categories.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8741787236250912
      ],
      "excerpt": "In pSp, we introduce a facial identity loss using a pre-trained ArcFace network for facial recognition. When operating on the human facial domain, we  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9310663829118603,
        0.9807213854849813
      ],
      "excerpt": "In a more recent paper, encoder4editing, the authors generalize this identity loss to other domains by  \nusing a MoCo-based ResNet to extract features instead of an ArcFace network. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8830796397624118
      ],
      "excerpt": "To help track your experiments, we've integrated Weights & Biases into our training process.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.801387479491222,
        0.8669928655133754
      ],
      "excerpt": "- During inference, the options used during training are loaded from the saved checkpoint and are then updated using the  \ntest options passed to the inference script. For example, there is no need to pass --dataset_type or --label_nc to the  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9708692445746252
      ],
      "excerpt": "- When running inference for segmentation-to-image or sketch-to-image, it is highly recommend to do so with a style-mixing, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8189310315705046
      ],
      "excerpt": "- When running inference for super-resolution, please provide a single down-sampling value using --resize_factors. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8641618141236352
      ],
      "excerpt": "style mixing on every image in the given data_path.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9856607083751786
      ],
      "excerpt": "To better show the flexibility of our pSp framework we present additional applications below. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8630454798830108,
        0.8274825430340824,
        0.9248236086058947
      ],
      "excerpt": "Using the toonify StyleGAN built by Doron Adler and Justin Pinkney, \nwe take a real face image and generate a toonified version of the given image. We train the pSp encoder to directly reconstruct real  \nface images inside the toons latent space resulting in a projection of each image to the closest toon. We do so without requiring any labeled pairs \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8649540361885054
      ],
      "excerpt": "This is trained exactly like the StyleGAN inversion task with several changes:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8284017457586235,
        0.8685632794602368
      ],
      "excerpt": "    - The toonify generator is taken from Doron Adler and Justin Pinkney  \n      and converted to Pytorch using rosinality's conversion script. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9531085564623333,
        0.8006263234279007
      ],
      "excerpt": "| &boxv;&nbsp; &boxur;&nbsp; psp.py | Implementation of our pSp framework \n| &boxvr;&nbsp; notebook | Folder with jupyter notebook containing pSp inference playground \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9822431240788075,
        0.8902073937920019,
        0.9810953203256128
      ],
      "excerpt": "Using our pSp encoder, artist Nathan Shipley transformed animated figures and paintings into real life. Check out his amazing work on his twitter page and website.    \nDeploying pSp with StyleSpace for Editing \nAwesome work from Justin Pinkney who deployed our pSp model on Runway and provided support for editing the resulting inversions using the StyleSpace Analysis paper. Check out his repository here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9543011854388184
      ],
      "excerpt": "Building on the work of pSp, Tov et al. design an encoder to enable high quality edits on real images. Check out their paper and code. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9798318338741793
      ],
      "excerpt": "Leveraging pSp and the rich semantics of StyleGAN, SAM learns non-linear latent space paths for modeling the age transformation of real face images. Check out the project page here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9670407115268098
      ],
      "excerpt": "ReStyle builds on recent encoders such as pSp and e4e by introducing an iterative refinment mechanism to gradually improve the inversion of real images. Check out the project page here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Official Implementation for \"Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation\" (CVPR 2021) presenting the pixel2style2pixel (pSp) framework",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/eladrich/pixel2style2pixel/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 368,
      "date": "Fri, 24 Dec 2021 06:29:20 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/eladrich/pixel2style2pixel/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "eladrich/pixel2style2pixel",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/eladrich/pixel2style2pixel/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/eladrich/pixel2style2pixel/master/notebooks/inference_playground.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/eladrich/pixel2style2pixel/master/download-weights.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Currently, we provide support for numerous datasets and experiments (encoding, frontalization, etc.).\n    - Refer to `configs/paths_config.py` to define the necessary data paths and model paths for training and evaluation. \n    - Refer to `configs/transforms_config.py` for the transforms defined for each dataset/experiment. \n    - Finally, refer to `configs/data_configs.py` for the source/target data paths for the train and test sets\n      as well as the transforms.\n- If you wish to experiment with your own dataset, you can simply make the necessary adjustments in \n    1. `data_configs.py` to define your data paths.\n    2. `transforms_configs.py` to define your own data transforms.\n    \nAs an example, assume we wish to run encoding using ffhq (`dataset_type=ffhq_encode`). \nWe first go to `configs/paths_config.py` and define:\n``` \ndataset_paths = {\n    'ffhq': '/path/to/ffhq/images256x256'\n    'celeba_test': '/path/to/CelebAMask-HQ/test_img',\n}\n```\nThe transforms for the experiment are defined in the class `EncodeTransforms` in `configs/transforms_config.py`.   \nFinally, in `configs/data_configs.py`, we define:\n``` \nDATASETS = {\n   'ffhq_encode': {\n        'transforms': transforms_config.EncodeTransforms,\n        'train_source_root': dataset_paths['ffhq'],\n        'train_target_root': dataset_paths['ffhq'],\n        'test_source_root': dataset_paths['celeba_test'],\n        'test_target_root': dataset_paths['celeba_test'],\n    },\n}\n``` \nWhen defining our datasets, we will take the values in the above dictionary.\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "- Clone this repo:\n``` \ngit clone https://github.com/eladrich/pixel2style2pixel.git\ncd pixel2style2pixel\n```\n- Dependencies:  \nWe recommend running this repository using [Anaconda](https://docs.anaconda.com/anaconda/install/). \nAll dependencies for defining the environment are provided in `environment/psp_env.yaml`.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8312833552945635
      ],
      "excerpt": "By default, we assume that all auxiliary models are downloaded and saved to the directory pretrained_models. However, you may use your own paths by changing the necessary values in configs/path_configs.py. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8195283490507721
      ],
      "excerpt": "Additionally, if you have tensorboard installed, you can visualize tensorboard logs in opts.exp_dir/logs. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8211833326964869,
        0.9795200922779087
      ],
      "excerpt": "To enable Weights & Biases (wandb), first make an account on the platform's webpage and install wandb using  \npip install wandb. Then, to train pSp using wandb, simply add the flag --use_wandb.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8148421774936367
      ],
      "excerpt": "[ ] Add multi-gpu support \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8739891000537701
      ],
      "excerpt": "<img src=\"docs/teaser.png\" width=\"800px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9042456532942793,
        0.9042456532942793
      ],
      "excerpt": "<img src=\"docs/encoding_inputs.jpg\" width=\"800px\"/> \n<img src=\"docs/encoding_outputs.jpg\" width=\"800px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9042456532942793,
        0.9042456532942793
      ],
      "excerpt": "<img src=\"docs/frontalization_inputs.jpg\" width=\"800px\"/> \n<img src=\"docs/frontalization_outputs.jpg\" width=\"800px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8739891000537701,
        0.8739891000537701
      ],
      "excerpt": "<img src=\"docs/seg2image.png\" width=\"800px\"/> \n<img src=\"docs/sketch2image.png\" width=\"800px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9042456532942793,
        0.9042456532942793
      ],
      "excerpt": "<img src=\"docs/super_res_32.jpg\" width=\"800px\"/> \n<img src=\"docs/super_res_style_mixing.jpg\" width=\"800px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8790923594498573
      ],
      "excerpt": "The main training script can be found in scripts/train.py.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8808400718595312
      ],
      "excerpt": "python scripts/train.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8808400718595312
      ],
      "excerpt": "python scripts/train.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8828665034782968
      ],
      "excerpt": "--l2_lambda_crop=0.01 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8808400718595312
      ],
      "excerpt": "python scripts/train.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8808400718595312
      ],
      "excerpt": "python scripts/train.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8808400718595312
      ],
      "excerpt": "python scripts/train.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8372399752020275,
        0.8501751782558576
      ],
      "excerpt": "See options/train_options.py for all training-specific flags.  \nSee options/test_options.py for all test-specific flags. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8178323041507995
      ],
      "excerpt": "Specifying --label_nc=0 (the default value), will directly use the RGB colors as input. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8216270093103228
      ],
      "excerpt": "For example,  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9042456532942793,
        0.9042456532942793
      ],
      "excerpt": "<img src=\"docs/toonify_input.jpg\" width=\"800px\"/> \n<img src=\"docs/toonify_output.jpg\" width=\"800px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8146715012507655
      ],
      "excerpt": "| &boxvr; models | Folder containting all the models and training objects \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9272716345557604
      ],
      "excerpt": "| &boxvr;&nbsp; options | Folder with training and test command-line options \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8328916229266603
      ],
      "excerpt": "| &boxvr;&nbsp; utils | Folder with various utility functions \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/eladrich/pixel2style2pixel/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python",
      "Cuda",
      "Shell",
      "C++"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 Elad Richardson, Yuval Alaluf\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "pixel2style2pixel",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "eladrich",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/eladrich/pixel2style2pixel/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Linux or macOS\n- NVIDIA GPU + CUDA CuDNN (CPU may be possible with some modifications, but is not inherently supported)\n- Python 2 or 3\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2191,
      "date": "Fri, 24 Dec 2021 06:29:20 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "image-translation",
      "stylegan",
      "generative-adversarial-network",
      "stylegan-encoder",
      "cvpr2021",
      "pixel2style2pixel",
      "psp-model",
      "psp-framework"
    ],
    "technique": "GitHub API"
  }
}