{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2004.13639",
      "https://arxiv.org/abs/1810.04805",
      "https://arxiv.org/abs/1907.10529",
      "https://arxiv.org/abs/1907.11692",
      "https://arxiv.org/abs/1910.01108"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{sun2020joint,\n    title={Joint Keyphrase Chunking and Salience Ranking with BERT},\n    author={Si Sun, Zhenghao Liu, Chenyan Xiong, Zhiyuan Liu and Jie Bao},\n    year={2020},\n    eprint={2004.13639},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9904289819775982
      ],
      "excerpt": "Please cite our paper if our experimental results, analysis conclusions or the code are helpful to you ~ \ud83d\ude0a \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/thunlp/BERT-KPE",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "For any question, feel free to create an issue, and we will try our best to solve. \\\nIf the problem is more urgent, you can send an email to me at the same time (I check email almost everyday \ud83d\ude09).\n\n```\nNAME: Si Sun\nEMAIL: s-sun17@mails.tsinghua.edu.cn\n```\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-11-08T07:05:10Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-20T01:42:02Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9634086225335486,
        0.9942624177838875,
        0.8915260504636822
      ],
      "excerpt": "This repository provides the code of the paper Capturing Global Informativeness in Open Domain Keyphrase Extraction. \nIn this paper, we conduct an empirical study of <u>5 keyphrase extraction models</u> with <u>3 BERT variants</u>, and then propose a multi-task model BERT-JointKPE. Experiments on two KPE benchmarks, OpenKP with Bing web pages and KP20K demonstrate JointKPE\u2019s state-of-the-art and robust effectiveness. Our further analyses also show that JointKPE has advantages in predicting <u>long keyphrases</u> and <u>non-entity keyphrases</u>, which were challenging for previous KPE techniques. \nPlease cite our paper if our experimental results, analysis conclusions or the code are helpful to you ~ \ud83d\ude0a \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8758040420354641
      ],
      "excerpt": "Compared with the OpenKP dataset we downloaded from MS MARCO in October of 2019 (all our experiments are based on this version of the dataset), we found that the dataset has been updated. We remind you to download the latest data from the official website. For comparison, we also provide the data version we use. (The dataset version issue was raised by Yansen Wang et al from CMU, thank them ! ) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8986226293373843
      ],
      "excerpt": "Amit also shared their zero-shot results on the Wikinews (French), Cacic (Spanish), Pak2018 (Polish), wicc (spanish), 110-PT-BN-KP (Portugese). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9276960452502296,
        0.9474353402644582
      ],
      "excerpt": "|1|BERT-JointKPE (Bert2Joint)|A <u>multi-task</u> model is trained jointly on the chunking task and the ranking task, balancing the estimation of keyphrase quality and salience. | \n|2|BERT-RankKPE (Bert2Rank)|Learn the salience phrases in the documents using a <u>ranking</u> network. | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9352911463640435,
        0.8237843533120746,
        0.9276960452502296
      ],
      "excerpt": "|4|BERT-TagKPE (Bert2Tag)|We modified the <u>sequence tagging</u> model to generate enough candidate keyphrases for a document. | \n|5|BERT-SpanKPE (Bert2Span)|We modified the <u>span extraction</u> model to extract multiple keyphrases from a document. | \n|6|DistilBERT-JointKPE (DistilBert2Joint)|A <u>multi-task</u> model is trained jointly on the chunking task and the ranking task, balancing the estimation of keyphrase quality and salience. | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9314617442782533
      ],
      "excerpt": "  --output_path           The dir to save preprocess data; default: ../data/prepro_dataset \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8386744151671134
      ],
      "excerpt": "  PS. Running the training script for the first time will take some time to perform preprocess such as tokenization, and by default, the processed features will be saved under ../data/cached_features, which can be directly loaded next time. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9104586854837566
      ],
      "excerpt": "We always keep the following settings in all our experiments: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8707563406948582
      ],
      "excerpt": "We recommend using DistributedDataParallel to train models on multiple GPUs (It's faster than DataParallel, but it will take up more memory) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9308305399959483
      ],
      "excerpt": "--eval_checkpoint       The filepath of our provided checkpoint \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8150771532802734
      ],
      "excerpt": "|Rank|Method|F1 @1,@3,@5|Precision @1,@3,@5|Recall @1,@3,@5| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8150771532802734
      ],
      "excerpt": "|Rank|Method|F1 @1,@3,@5|Precision @1,@3,@5|Recall @1,@3,@5| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8150771532802734
      ],
      "excerpt": "|Rank|Method|F1 @1,@3,@5|Precision @1,@3,@5|Recall @1,@3,@5| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9508197791355082,
        0.9344228545636208,
        0.936430798701692,
        0.8838285136700594,
        0.8938145407771622,
        0.995360413060193
      ],
      "excerpt": "Word-Level Representations :   We encode an input document into a sequence of WordPiece tokens' vectors with a pretrained BERT (or its variants), and then we pick up the first sub-token vector of each word to represent the input in word-level. \nPhrase-Level Representations : We perform a soft-select method to decode phrase from word-level vector instead of hard-select used in the standard sequence tagging task . \nThe word-level representation is feed into an classification layer to obtain the tag probabilities of each word on 5 classes  (O, B, I, E, U) , and then we employ different tag patterns for extracting different n-grams ( 1 \u2264 n \u2264 5 ) over the whole sequence. \nLast there are a collect of n-gram candidates, each word of the n-gram just has one score. \nSoft-select Example : considering all 3-grams (B I E) on the L-length document, we can extract (L-3+1)  3-grams sequentially like sliding window. In each 3-gram, we only keep B score for the first word, I score for the middle word, and E score for the last word, etc. \nO : Non Keyphrase ;  B : Begin word of the keyprase ;  I : Middle word of the keyphrase ;  E : End word of keyprhase ;  U : Uni-word keyphrase \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9926267652355384
      ],
      "excerpt": "Incorporating with term frequency, we employ Min Pooling to get the final score of each n-gram (we called it Buckets Effect: No matter how high a bucket, it depends on the height of the water in which the lowest piece of wood) . Based on the final scores, we extract 5 top ranked keyprhase candidates for each document. \n",
      "technique": "Supervised classification"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- First download and decompress our data folder to this repo, the folder includes benchmark datasets and pre-trained BERT variants.\n\n  - [Data Download Link](https://drive.google.com/open?id=1UugkRsKM8GXPPrrZxWa8HvGe1nyWdd6F)\n\n- We also provide 15 checkpoints (5 KPE models * 3 BERT variants) trained on OpenKP training dataset.\n\n  - [Checkpoint Download Link](https://drive.google.com/open?id=13FvONBTM4NZZCR-I7LVypkFa0xihxWnM)\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/thunlp/BERT-KPE/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 52,
      "date": "Mon, 27 Dec 2021 22:23:56 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/thunlp/BERT-KPE/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "thunlp/BERT-KPE",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/thunlp/BERT-KPE/master/scripts/test.sh",
      "https://raw.githubusercontent.com/thunlp/BERT-KPE/master/scripts/train.sh",
      "https://raw.githubusercontent.com/thunlp/BERT-KPE/master/scripts/.ipynb_checkpoints/test-checkpoint.sh",
      "https://raw.githubusercontent.com/thunlp/BERT-KPE/master/scripts/.ipynb_checkpoints/train-checkpoint.sh",
      "https://raw.githubusercontent.com/thunlp/BERT-KPE/master/preprocess/preprocess.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8837680365796365,
        0.9925331173772742,
        0.9982980841443657,
        0.8526112798470697,
        0.9654765604684976
      ],
      "excerpt": "python 3.8 \nconda install --file conda-requirements.txt \npip install -r pip-requirements.txt \nTo preprocess the source datasets using preprocess.sh in the preprocess folder: \nsource preprocess.sh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9036881717605214
      ],
      "excerpt": "  --source_dataset_dir    The path to the source dataset \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.968557623089146
      ],
      "excerpt": "source train.sh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.945956423953397
      ],
      "excerpt": "source test.sh \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8525863574207089,
        0.805406267651813
      ],
      "excerpt": "  --output_path           The dir to save preprocess data; default: ../data/prepro_dataset \nTo preprocess the multilingual dataset, download respective datasets from  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8313509317662244,
        0.8152249660077784
      ],
      "excerpt": "The dataset can be split into train, dev, and test sets using split_json.py. \nTo train a new model from scratch using train.sh in the scripts folder: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8168107655428365
      ],
      "excerpt": "  Complete optional arguments can be seen in config.py in the scripts folder. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8162717594042472
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES=0,1 OMP_NUM_THREADS=2 python -m torch.distributed.launch --nproc_per_node=2 --master_port=1234 train.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8086314445935857
      ],
      "excerpt": "  --eval_checkpoint       The checkpoint file to be evaluated \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/thunlp/BERT-KPE/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2021 THUNLP\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "**BERT for Keyphrase Extraction** (PyTorch)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "BERT-KPE",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "thunlp",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/thunlp/BERT-KPE/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 281,
      "date": "Mon, 27 Dec 2021 22:23:56 GMT"
    },
    "technique": "GitHub API"
  }
}