{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We would like to thank our teaching assistant Pranjal Rajput for his guidance and advice during this project. He helped us think more outside of the box and steered the project in the right direction. Furthermore, we would also like to give thanks to Jan van Gemert for his enthusiasm while giving lectures and motivating us to successfully complete this project.\n\n",
      "technique": "Header extraction"
    }
  ],
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\n@inproceedings{isola2017image,\n  title={Image-to-Image Translation with Conditional Adversarial Networks},\n  author={Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},\n  booktitle={Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on},\n  year={2017}\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{isola2017image,\n  title={Image-to-Image Translation with Conditional Adversarial Networks},\n  author={Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},\n  booktitle={Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on},\n  year={2017}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9486597947859837
      ],
      "excerpt": "  Equation 2 [Mathworks (https://nl.mathworks.com/help/vision/ref/psnr.html)] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9486597947859837
      ],
      "excerpt": "  Equation 3 [Mathworks (https://nl.mathworks.com/help/vision/ref/psnr.html)] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8834756699654108
      ],
      "excerpt": "    img = Image.open(old_directory+'\\'+data_file) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/PieterBijl/Group28",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-03-11T12:47:57Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-04-20T14:16:46Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The paper Image-to-Image Translation with Conditional Adversarial Networks (https://arxiv.org/pdf/1611.07004.pdf) showed that a general purpose solution could be made for image-to-image translation. Since the time that this paper was published, multiple artists and researchers have made their own models and experiments, with stunning results. These models range from creating cats out of drawings to creating videos of the sea by using an input from general household appliances.<br>\nThe objective of the pix2pix model is to find a model that can map one picture to a desired paired image, which is indistinguishable from the real thing. An example is shown in Figure 1, where 4 different models attempt the mapping from the pixelated image to the real image. Pix2pix uses Conditional Generative Adversarial Networks to achieve this objective. Conditional means that the loss here is structured, there exists a conditional dependency between the pixels, meaning that the loss of one pixel is influenced by the loss of another. The loss function that is used by the model is shown in Equation 1.\n\n<p align=\"center\">\n  <img src=\"/ImagesInText/LossFunction.png\" width=\"60%\" height=\"60%\"><br>\n  Equation 1 [P. Isola et al. (https://arxiv.org/pdf/1611.07004.pdf)]\n</p>\n\nThe model can then be trained and results evaluated, which has been done for a great variety of experiments. We wanted to see if another application could be made, that of image restoration of blurry pictures. We have most likely all seen a movie or tv-series in which a spy agency needed someone to \"enhance\" a photo in order to see smaller details, with the advent of deep learning these techniques are becoming more of a reality. We wanted to see if this general architecture of pix2pix for image translation could also be used for this application to see if you could enhance your images by using pix2pix.\n\nThis blog first starts with the method of our project, what exactly is the type of data that we investigate and in what type of datasets they are stored. This is followed up by an explanation about the hyperparameter tuning that was performed and why this was important. The last part of the method is how we would evaluate our results. The method is followed up by our experiments, which also gives a sample of the data that we used as well as the result of some of our experiments, these results are then discussed in our discussion as well with our conclusion about the experiment if pix2pix can be used for image restoration.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9817790930658687,
        0.9798212966133123,
        0.9800214232043707
      ],
      "excerpt": "In this section, four topics will be touched. Firstly, a short description of the datasets is given and will be further elaborated upon later in the text. Secondly, insight is given in how the models were trained and tested. Thirdly, the hyperparameters that could be changed are discussed. Fourthly, it is explained how the performance of the models is evaluated. \nFor our initial research it was decided to use images of which the resolution was lower than that of the target/true image. Pix2pix generally uses 256x256 images, which is the size that we want to create. For input we wanted to initially use a dataset which used 64x64 images, but later an additional dataset was created where the resolution of the pictures was allowed to vary between 48x48 to 128x128 pixels, because initial tests showed that the model did not generalize well on random photos taken from the internet. \nTo train the algorithm a github repository was set up, in which the original pix2pix repository was cloned (https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix). Later we included our own data in this repository. This repository could be cloned to the virtual machine provided by Google Colaboratory, using the following code: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9377749503604016
      ],
      "excerpt": "Or alternatively with: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9811123110005368
      ],
      "excerpt": "The following snippet of code is an example for training the algorithm with batch size 64, 50 normal epochs and 0 decay epochs: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8180810928507272
      ],
      "excerpt": "Similar to\u200a-n_epochs, as used above, the optional parameters for training are the following: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8681146840279678
      ],
      "excerpt": "After discussing the location and the testing of the results, the optional parameters will be discussed more in detail in the following section. The resulting models are stored in the folder checkpoints and can be accessed using the following command: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9911847315135468
      ],
      "excerpt": "These results were used for later analysis of the performance of the model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8927763233692995,
        0.8638356591488655,
        0.8796311884548363,
        0.9606231573842072
      ],
      "excerpt": "- Momentum term of Adam optimization algorithm, default = 0.5. \n- The initial learning rate for the Adam optimization algorithm, default = 0.0002. \n- The GAN mode, of which the options are vanilla, lsgan and wgangp, default = lsgan. \n- The pool size, which is the size of the image buffer that stores previously generated images, default = 50. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9285041035887541,
        0.9751244121385951,
        0.9834326890094345
      ],
      "excerpt": "Additionally we looked at the algorithms training performance for different batch sizes. Using a batch size greater than one and smaller than the training set size leads to mini-batch gradient descent, in which the internal model parameters are updated after a number of samples have been worked through. This mini-batch training both requires less memory and trains faster compared to both stochastic gradient descent, where the batch size is equal to 1, and batch gradient descent, where the batch size is equal to the training set size. \nIn their paper, P. Isola et al. use a perceptual study on the generated images. Turkers from Amazon Mechanical Turk were shown 40 images and for each they had to tell whether it was a generated image or an actual image. Since, for our study, multiple hyperparameters were changed, there was a large number of models created. A perceptual study on the generated images would simply take too much time, so instead a quantitative method seemed more interesting. Such a method is also used in the paper: FCN score. If a semantic classifier is trained on real images and the generated image looks real, it should also be able to classify the generated image successfully. However, a dataset would then have to be created which provides the true semantic segmentation of the images, which would again be too time consuming. Instead, it was decided to go for another method that is able to quantify the similarity between the two images: The Peak Signal-to-Noise-Ratio, from now on called PSNR. \nThe PSNR is a measure of similarity between the generated images and the original images. First it calculates the mean squared error, from now on called MSE, of the generated image, which can be seen in Equation 2. It subtracts the pixel values of the generated image from the real image and then sums the square of these outcomes. In the last step, this is divided by the amount of pixels in the image, 256x256. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.974805521348134
      ],
      "excerpt": "The calculation of the PSNR is shown in Equation 3, where R is the maximum pixel value, 255. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9902223767360242
      ],
      "excerpt": "The higher the MSE, the lower the PSNR will be. So, when testing the models, higher values for PSNR will result in better performance. Furthermore, as a double check for the results, the cosine similarity was also computed using Equation 4. Every single pixel of the generated image is multiplied with the pixel at the same coordinates in the real image. Then, this product is divided by the multiplication of the magnitudes of both images. The magnitude of an image basically means a dot product by itself. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9066211693234715
      ],
      "excerpt": "To conclude, every model was tested for performance and a value was given to its PSNR and its cosine similarity. After testing all the models, the combinations of these scores were plotted in Figure 2. A trend line can be seen which shows the relationship between the two values. When the model gets a high value for its PSNR, it will also get a high value for its cosine similarity, and vice versa. The code can be found at Evaluation.py. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8787046883656451,
        0.9591141401980734
      ],
      "excerpt": "This section will inform about how the datasets are created and it will discuss the selected hyperparameters to tune. \nAs specified in the method section, two datasets were created: one with images with a resolution of 64x64 pixels, scaled up to 256x256 and one where resolution was allowed to vary between 48x48 pixels to 128x128 pixels, also scaled up to 256x256. To create these datasets the van gogh dataset from pix2pix was used, this dataset included over 6000 images varying from landscapes to persons, all with a resolution of 256x256. Using python these images were used to create the two datasets that were used as the input.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "  for data_file in os.listdir(data_directory): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9262643096278462
      ],
      "excerpt": "    #:determine what the resolution is going to be, from 48x48 to 128x128 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8823890073468724,
        0.8258200457563152
      ],
      "excerpt": "    #:reduce the image to the desired resolution \n    #: Scale back up using NEAREST to original size \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9781171470874807
      ],
      "excerpt": "Next to the batch size, we choose 3 hyperparameters to tune: the number of epochs, both normal and decaying epochs, and the learning rate. First, considering the batch size, the algorithm was trained multiple times under default parameters but with increasing batch sizes. The batch size was increased with a factor of 2 each time. We found that a batch size of 64 gave the best results regarding training time on Google Colaboratory. The results in Figure 3 show the total training duration of one normal epoch and one decaying epoch. The quality difference of the output was not assessed for all these different batch sizes since it was assumed to be constant. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/PieterBijl/Group28/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Fri, 24 Dec 2021 05:06:52 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/PieterBijl/Group28/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "PieterBijl/Group28",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/PieterBijl/Group28/master/docs/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/PieterBijl/Group28/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/PieterBijl/Group28/master/CycleGAN.ipynb",
      "https://raw.githubusercontent.com/PieterBijl/Group28/master/pix2pix.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/PieterBijl/Group28/master/scripts/test_cyclegan.sh",
      "https://raw.githubusercontent.com/PieterBijl/Group28/master/scripts/download_pix2pix_model.sh",
      "https://raw.githubusercontent.com/PieterBijl/Group28/master/scripts/conda_deps.sh",
      "https://raw.githubusercontent.com/PieterBijl/Group28/master/scripts/download_cyclegan_model.sh",
      "https://raw.githubusercontent.com/PieterBijl/Group28/master/scripts/install_deps.sh",
      "https://raw.githubusercontent.com/PieterBijl/Group28/master/scripts/test_pix2pix.sh",
      "https://raw.githubusercontent.com/PieterBijl/Group28/master/scripts/test_single.sh",
      "https://raw.githubusercontent.com/PieterBijl/Group28/master/scripts/test_colorization.sh",
      "https://raw.githubusercontent.com/PieterBijl/Group28/master/scripts/train_colorization.sh",
      "https://raw.githubusercontent.com/PieterBijl/Group28/master/scripts/train_cyclegan.sh",
      "https://raw.githubusercontent.com/PieterBijl/Group28/master/scripts/train_pix2pix.sh",
      "https://raw.githubusercontent.com/PieterBijl/Group28/master/scripts/eval_cityscapes/download_fcn8s.sh",
      "https://raw.githubusercontent.com/PieterBijl/Group28/master/datasets/download_cyclegan_dataset.sh",
      "https://raw.githubusercontent.com/PieterBijl/Group28/master/datasets/download_pix2pix_dataset.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9893272198983933,
        0.9250631019316092
      ],
      "excerpt": "!git clone https://github.com/PieterBijl/Group28.git/ \nFrom here the working directory should be set to the cloned folder and the requirements for pix2pix should be installed. This was done using the following piece of code: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9979947896609701
      ],
      "excerpt": "!pip install -r requirements.txt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.897236615707984,
        0.9979947896609701
      ],
      "excerpt": "cd /content/Group28/ \n!pip install -r requirements.txt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8113578056806766
      ],
      "excerpt": "Cd /content/Group28/checkpoints/variational_data_batch_size_64_normal_epochs_50_decay_0/ \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9115584538534853
      ],
      "excerpt": "  <img src=\"/ImagesInText/005158_real_A.png\" width=\"100%\" height=\"100%\"><br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8401558704798054
      ],
      "excerpt": "import os \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9548481737139773,
        0.9010644680705217
      ],
      "excerpt": "!python train.py \u200a-\u200adataroot\u00a0./datasets/variational_data \u200a-\u200aname variational_data_batch_size_64_normal_epochs_50_decay_0 \u200a-\u200abatch_size 64 \u200a-\u200amodel pix2pix \u200a-\u200an_epochs 50 \u200a-\u200an_epochs_decay 0 \nThe\u00a0./datasets/variational_data refers to the data stored in the folder datasets/variatonal_data. In this folder two folders are located: train and test. As the name suggests, the training data is located in the train data folder and the test data in the test data folder. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9456041749568265
      ],
      "excerpt": "!python test.py \u200a-\u200adataroot/content/Group28/datasets/variatonal_data/test/ \u200a-\u200aname variational_data_batch_size_64_normal_epochs_50_decay_ 0 \u200a-\u200amodel test \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.903281498235012
      ],
      "excerpt": "  <img src=\"/ImagesInText/MSE.png\" width=\"60%\" height=\"60%\"><br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9138098394056625
      ],
      "excerpt": "  <img src=\"/ImagesInText/PSNR.png\" width=\"60%\" height=\"60%\"><br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.903281498235012
      ],
      "excerpt": "  <img src=\"/ImagesInText/Similarity.png\" width=\"60%\" height=\"60%\"><br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8266052235309973
      ],
      "excerpt": "  <img src=\"/ImagesInText/Figure2.png\" width=\"70%\" height=\"70%\"><br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8044146895679722,
        0.8401558704798054,
        0.9457175861910134
      ],
      "excerpt": "from PIL import Image \nimport os \nimport numpy as np \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8437350931418188
      ],
      "excerpt": "    pixel_number = round(256/np.random.randint(2,4))\u00a0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8211035449525859
      ],
      "excerpt": "    result.save(new_directory+'\\'+data_file) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8419365417525021
      ],
      "excerpt": "python datasets/combine_A_and_B.py \u200a-\u200afold_A /path/to/data/A \u200a-\u200afold_B /path/to/data/B \u200a-\u200afold_AB /path/to/data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8266052235309973
      ],
      "excerpt": "  <img src=\"/ImagesInText/Figure3.png\" width=\"70%\" height=\"70%\"><br> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/PieterBijl/Group28/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook",
      "Shell",
      "MATLAB",
      "TeX"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other",
      "url": "https://raw.githubusercontent.com/PieterBijl/Group28/master/LICENSE"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'Copyright (c) 2017, Jun-Yan Zhu and Taesung Park\\nAll rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met:\\n\\n Redistributions of source code must retain the above copyright notice, this\\n  list of conditions and the following disclaimer.\\n\\n Redistributions in binary form must reproduce the above copyright notice,\\n  this list of conditions and the following disclaimer in the documentation\\n  and/or other materials provided with the distribution.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n\\n\\n--------------------------- LICENSE FOR pix2pix --------------------------------\\nBSD License\\n\\nFor pix2pix software\\nCopyright (c) 2016, Phillip Isola and Jun-Yan Zhu\\nAll rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met:\\n\\n Redistributions of source code must retain the above copyright notice, this\\n  list of conditions and the following disclaimer.\\n\\n Redistributions in binary form must reproduce the above copyright notice,\\n  this list of conditions and the following disclaimer in the documentation\\n  and/or other materials provided with the distribution.\\n\\n----------------------------- LICENSE FOR DCGAN --------------------------------\\nBSD License\\n\\nFor dcgan.torch software\\n\\nCopyright (c) 2015, Facebook, Inc. All rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\\n\\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\\n\\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\\n\\nNeither the name Facebook nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Image Restoration using Pix2Pix",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Group28",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "PieterBijl",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/PieterBijl/Group28/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Fri, 24 Dec 2021 05:06:52 GMT"
    },
    "technique": "GitHub API"
  },
  "support": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The first models were trained on dataset AB with batch size of 100. This was prior to the knowledge that a batch size of 64 was the faster training option. The model name consists of a BS, N and D: Batch Size, Normal epochs and Decay epochs respectively. The results are shown in Figure 4.\n\n<p align=\"center\">\n  <img src=\"/ImagesInText/Figure4.png\" width=\"70%\" height=\"70%\"><br>\n  Figure 4: The PSNR plotted for different models for dataset AB and batch size 64\n</p>\n\nTwo interesting conclusions can already be drawn from these models. The first is that the PSNR of the pixelated image was higher than the best model so far could produce and the second is that after a certain point the model does not improve anymore. In the figure this point is at 40 normal epochs and 40 decay epochs.<br>\nThese results were discussed with the teaching assistant and this is when the decision was made to continue with a batch size of 64. Figure 5 combines this batch size with normal epochs up to 120, for both the AB and variational dataset. Again, the PSNR of the pixelated is not achieved and the curve flattens after around 30 or 40 epochs. The models showed similar results for both datasets. The average PSNR is higher for the variational dataset, however the pixelated image also has a higher PSNR. The two black dots, originally blue, are models that are used later for comparison in a perceptual study of the images.\n\n<p align=\"center\">\n  <img src=\"/ImagesInText/Figure5.png\" width=\"70%\" height=\"70%\"><br>\n  Figure 5: The PSNR plotted for different models for both datasets\n</p>\n\nIt was questioned whether the flattening of the curve around 30 and 40 epochs could be prevented by using decaying epochs. Dataset AB was used in Figure 6. The figure on the left shows 30 normal epochs followed by 80 decay epochs and the figure on the right shows 40 normal epochs followed by 80 decay epochs. The figure on the right misses one data point due to an error that occurred in saving the model.\n\n<p align=\"center\">\n  <img src=\"/ImagesInText/Figure6.png\" width=\"100%\" height=\"100%\"><br>\n  Figure 6 left: The PSNR plotted for different models with batch size 64. Figure 6 right: The PSNR plotted for different models with batch size 64\n</p>\n\nThe figure on the left shows no major improvement in the results after 30 normal epochs. The PSNR slightly increases during the first 40 decay epochs, but it is enough to challenge the PSNR of the pixelated images. The figure on the right actually shows a decrease in PSNR after the training with decay epochs started. It also shows a small jump for 40 normal epochs. When comparing the exact same model in Figure 4, it can be seen that the first PSNR is 21.12 and the second 21.56. This caused interested in how much this difference could be for different models with the same hyperparameters. Figure 7 shows the results on the variational datasets, tested on 4 models with exactly the same hyperparameters.\n\n<p align=\"center\">\n  <img src=\"/ImagesInText/Figure7.png\" width=\"70%\" height=\"70%\"><br>\n  Figure 7: The PSNR plotted for 4 identical models\n</p>\n\nThree of the results are around 22.3 and one around 22.8, meaning that the results are not always identical, but still close to each other. This deviation will not show major impact on whether the PSNR of the pixelated images can be reached or not.<br>\nSince the decay epochs were not showing great impact on the results, the learning rate was changed instead. The default setting was at LR = 0.0002. Figure 8 shows the results for a learning rate that is 0.1 and 10 times the default value of 0.0002.\n\n<p align=\"center\">\n  <img src=\"/ImagesInText/Figure8.png\" width=\"70%\" height=\"70%\"><br>\n  Figure 8: The PSNR plotted for models with different learning rates\n</p>\n\nThe higher learning rate shows a better performance on the test data. Furthermore, even higher learning rates of 20 times the default value were tested and these showed similar results to the 10 times the original learning rate. To extend on this study, a large model with 120 normal epochs was trained on the AB dataset with two different learning rates: the default value and 20 times this value, as shown in Figure 9. Again, the black dot, originally red, is a model that is used later for comparison in a perceptual study of the images.\n\n<p align=\"center\">\n  <img src=\"/ImagesInText/Figure9.png\" width=\"70%\" height=\"70%\"><br>\n  Figure 9: The PSNR plotted for models with different learning rates\n</p>\n\nResults show that the model with the higher learning rate gets a better start, but becomes unstable after 40 epochs and the PSNR of the pixelated image still remains untouched. Nevertheless, a higher maximum value of the PSNR is achieved using a higher learning rate.<br>\nStill, there was no clear sign of a combination of hyperparameters that would lead to a model that could beat the PSNR of the pixelated image. It was decided to look back on the assumptions made, especially the one where it was assumed that the batch size only had an influence on training time and not so much on the result. To put this to the test, the model that takes the longest to complete one epoch, a model with batch size 1, was tested and the results are shown in Figure 10. Again, the black dot, originally blue, is a model that is used later for comparison in a perceptual study of the images.\n\n<p align=\"center\">\n  <img src=\"/ImagesInText/Figure10.png\" width=\"70%\" height=\"70%\"><br>\n  Figure 10: The PSNR plotted for different normal epochs with batch size 1\n</p>\n\nThese results show that the  assumption that was made can be thrown off the table. The model is almost able to reach the PSNR of the pixelated image at only 10 normal epochs.\u00a0<br>\nThe question that remains is: How to create the most optimal model out of the results so far? It was found before that a higher learning rate showed better results, so it is worth to try this on the model with batch size 1. A model with a learning rate of 20 times the default, using 20 normal epochs and a batch size of 1 was created to put the PSNR of the pixelated images to the test. The results are shown in Figure 11.\n\n<p align=\"center\">\n  <img src=\"/ImagesInText/figure11.png\" width=\"70%\" height=\"70%\"><br>\n  Figure 11: The PSNR plotted for different learning rates with batch size 1\n</p>\n\nUnfortunately, the models with a higher learning rate do not show any improvement compared to the regular learning rate and the model with the default learning rate and batch size 1 remains the model with the best performance. Now, would the PSNR values increase if the model is tested on a set of training images instead of test images? An expected answer is yes, since this is what the model's weights are trained on. To put this to the test, a model trained on the variational data with batch size 64 has been tested on both the training and the test set with different amounts of normal epochs. The results are shown in Figure 12.\n\n<p align=\"center\">\n  <img src=\"/ImagesInText/Figure13.png\" width=\"70%\" height=\"70%\"><br>\n  Figure 12: The PSNR plotted for both the training and test set of the variational data with batch size 64\n</p>\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "In this section, the models labeled with the  black dots will be used for comparison. Ten images from the AB dataset have been selected from the test set and are used to compare four different models, as shown in Figure 13. The first column shows the pixelated image and the last column shows the expected output of the model, the real image. In between, there are results from four different models. The order of the models is based on their average PSNR on the test set, from low to high.<br>\nThe first model used a batch size of 64 and has been trained for 10 normal epochs. It can be seen that the model still has got difficulty in getting the colour right, plus there are quite some noisy parts in the image. Its average PSNR is 18.36.<br>\nThe second model used a batch size of 64 and has been trained for 40 normal epochs. The improvement is noticeable. The colours look more similar to the real image than before and in most images the noise is decreased. Its average PSNR is 21.56.<br>\nThe third model used a batch size of 64 and has been trained for 40 normal epochs, but the difference here is that the learning rate is multiplied by 20. The results looks like a blurry variant of the real image, but shows a good improvement compared to the pixelated image. The expected reason that this model is not the best, is because it tends to create small black holes in the image, which are disastrous for the MSE and thus the PSNR. Its average PSNR is 22.10.<br>\nThe fourth model used a batch size of 1 and has been trained for 10 normal epochs. Although some noise is visible, the images are sharper than the third model and no major wrong predictions are observed. Its average PSNR is 23.16.\n\n<p align=\"center\">\n  <img src=\"/ImagesInText/005509_real_A.png\" width=\"100%\" height=\"100%\"><br>\n  <img src=\"/ImagesInText/005414_real_A.png\" width=\"100%\" height=\"100%\"><br>\n  <img src=\"/ImagesInText/005241_real_A.png\" width=\"100%\" height=\"100%\"><br>\n  <img src=\"/ImagesInText/005200_real_A.png\" width=\"100%\" height=\"100%\"><br>\n  <img src=\"/ImagesInText/005176_real_A.png\" width=\"100%\" height=\"100%\"><br>\n  <img src=\"/ImagesInText/005158_real_A.png\" width=\"100%\" height=\"100%\"><br>\n  <img src=\"/ImagesInText/005147_real_A.png\" width=\"100%\" height=\"100%\"><br>\n  <img src=\"/ImagesInText/005110_real_A.png\" width=\"100%\" height=\"100%\"><br>\n  <img src=\"/ImagesInText/005097_real_A.png\" width=\"100%\" height=\"100%\"><br>\n  <img src=\"/ImagesInText/005077_real_A.png\" width=\"100%\" height=\"100%\"><br>\n  Figure 13: For 10 test images, from left to right: Pixelated image\u200a-\u200aBS64 N10\u200a-\u200aBS64 N40\u200a-\u200aBS64 N40 LRx20\u200a-\u200aBS1 N10\u200a-\u200aReal image\n</p>\n\n",
      "technique": "Header extraction"
    }
  ]
}