{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1707.06347 \n\nPart of the code base is from https://github.com/liziniu/RL-PPO-Keras . However, the code there had errors\nbut mainly it did not use a GAE type reward and no entropy bonus system.\n\nI gave my best to comment the code but I did not include a fundamental lecutre on the logic behind PPO. I highly \nrecommend to watch these two videos to undestand what happens:\n\nhttps://youtu.be/WxQfQW48A4A\n\nhttps://youtu.be/5P7I-xPq8u8\n\nThe most complete explenation and also part of the code (i.e. Memory Class"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/nric/ProximalPolicyOptimizationContinuousKeras",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-05-07T10:29:40Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-08-01T10:52:43Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9786576681290408
      ],
      "excerpt": "This is an Tensorflow 2.0 (Keras) implementation of a Open Ai's proximal policy optimization PPO algorithem for continuous action spaces. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8733193817379186,
        0.8579540477291108
      ],
      "excerpt": "I gave my best to comment the code but I did not include a fundamental lecutre on the logic behind PPO. I highly  \nrecommend to watch these two videos to undestand what happens: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8642256641244414
      ],
      "excerpt": "The most complete explenation and also part of the code (i.e. Memory Class) is from the open ai spinning up project: https://spinningup.openai.com/en/latest/algorithms/ppo.html \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9490573183866454
      ],
      "excerpt": "which seems to show some leraning but not great learning. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9841162300408298
      ],
      "excerpt": "3) Currently, the two outputs of actor (mu and sigma) are concatenated and then disassembled for the loss. Because the loss depends on both outputs at the same time (mu and sigma). I found this to be the only alternative to writing a custom train fuction with keras.function which seems not to work with TF 2.0 alpha. I should at least try to find a more elegant method. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "This is an Tensorflow 2.0 (Keras) implementation of a Open Ai's proximal policy optimization PPO algorithem for continuous action spaces.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/nric/ProximalPolicyOptimizationContinuousKeras/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3,
      "date": "Wed, 29 Dec 2021 23:51:17 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/nric/ProximalPolicyOptimizationContinuousKeras/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "nric/ProximalPolicyOptimizationContinuousKeras",
    "technique": "GitHub API"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/nric/ProximalPolicyOptimizationContinuousKeras/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "ProximalPolicyOptimizationContinuousKeras",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "ProximalPolicyOptimizationContinuousKeras",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "nric",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/nric/ProximalPolicyOptimizationContinuousKeras/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3,
      "date": "Wed, 29 Dec 2021 23:51:17 GMT"
    },
    "technique": "GitHub API"
  }
}