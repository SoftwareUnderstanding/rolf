{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2006.03236"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9934273497841208,
        0.9559715772848645,
        0.9559715772848645,
        0.9559715772848645,
        0.9559715772848645
      ],
      "excerpt": "| B10-10-10H1024 | Link | Link | Link | \n| B8-8-8H1024    | Link | Link | Link | \n| B6-6-6H768     | Link | Link | Link | \n| B6-3x2-3x2H768 | Link | Link | Link | \n| B4-4-4H768     | Link | Link | Link | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/laiguokun/Funnel-Transformer",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-06-07T21:01:57Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-02T07:12:40Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "**Funnel-Transformer** is a new self-attention model that gradually compresses the sequence of hidden states to a shorter one and hence reduces the computation cost. More importantly, by re-investing the saved FLOPs from length reduction in constructing a deeper or wider model, Funnel-Transformer usually has a higher capacity given the same FLOPs. In addition, with a decoder, Funnel-Transformer is able to recover the token-level deep representation for each token from the reduced hidden sequence, which enables standard pretraining.\n\n\n\nFor a detailed description of technical details and experimental results, please refer to our paper:\n\n> [Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing](https://arxiv.org/abs/2006.03236)\n>\n> Zihang Dai\\*, Guokun Lai*, Yiming Yang, Quoc V. Le \n>\n> (*: equal contribution) \n>\n> Preprint 2020\n>\n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9098124782857012,
        0.8903691095184477,
        0.9036199557269677,
        0.9550507887827533,
        0.9067182928823011,
        0.918595774841016,
        0.8659047196806174
      ],
      "excerpt": "The corresponding source code is in the tensorflow folder, which was developed and exactly used for TPU pretraining & finetuning as presented in the paper. \nThe TensorFlow funetuning code mainly supports TPU finetuining on GLUE benchmark, text classification, SQuAD and RACE. \nPlease refer to tensorflow/README.md for details. \nThe source code is in the pytorch folder, which only serves as an example PyTorch implementation of Funnel-Transformer.  \nHence, the PyTorch code only supports GPU finetuning for the GLUE benchmark & text classification. \nPlease refer to pytorch/README.md for details. \n| Model Size     | PyTorch                                                      | TensorFlow                                                   | TensorFlow-Full                                              | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8189838731540764
      ],
      "excerpt": "A TensorFlow or PyTorch checkpoint (model.ckpt-* or model.ckpt.pt)  checkpoint containing the pre-trained weights (Note: The TensorFlow checkpoint actually corresponds to 3 files). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8321162764061653
      ],
      "excerpt": "A config file (net_config.json or net_config.pytorch.json) which specifies the hyperparameters of the model. \n",
      "technique": "Supervised classification"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- The corresponding source code and instructions are in the `data-scrips` folder, which specifies how to access the raw data we used in this work.\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/laiguokun/Funnel-Transformer/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 12,
      "date": "Mon, 27 Dec 2021 03:35:39 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/laiguokun/Funnel-Transformer/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "laiguokun/Funnel-Transformer",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/laiguokun/Funnel-Transformer/master/download_all_ckpts.sh",
      "https://raw.githubusercontent.com/laiguokun/Funnel-Transformer/master/data-scripts/download_glue.sh",
      "https://raw.githubusercontent.com/laiguokun/Funnel-Transformer/master/data-scripts/download_textcls.sh",
      "https://raw.githubusercontent.com/laiguokun/Funnel-Transformer/master/pytorch/scripts/convert_ckpt2pt.sh",
      "https://raw.githubusercontent.com/laiguokun/Funnel-Transformer/master/pytorch/scripts/multigpu_classifier.sh",
      "https://raw.githubusercontent.com/laiguokun/Funnel-Transformer/master/pytorch/scripts/classifier.sh",
      "https://raw.githubusercontent.com/laiguokun/Funnel-Transformer/master/tensorflow/scripts/tpu_squad.sh",
      "https://raw.githubusercontent.com/laiguokun/Funnel-Transformer/master/tensorflow/scripts/finetune.sh",
      "https://raw.githubusercontent.com/laiguokun/Funnel-Transformer/master/tensorflow/scripts/prepro_squad.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8595928143928534
      ],
      "excerpt": "Hence, the PyTorch code only supports GPU finetuning for the GLUE benchmark & text classification. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9597424135843613
      ],
      "excerpt": "You also can use download_all_ckpts.sh to download all checkpoints mentioned above.  \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8147238470473156
      ],
      "excerpt": "A TensorFlow or PyTorch checkpoint (model.ckpt-* or model.ckpt.pt)  checkpoint containing the pre-trained weights (Note: The TensorFlow checkpoint actually corresponds to 3 files). \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/laiguokun/Funnel-Transformer/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 Zihang Dai, Gguokun Lai, Yiming Yang, Quoc V. Le\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Introduction",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Funnel-Transformer",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "laiguokun",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/laiguokun/Funnel-Transformer/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 185,
      "date": "Mon, 27 Dec 2021 03:35:39 GMT"
    },
    "technique": "GitHub API"
  }
}