{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1908.02144",
      "https://arxiv.org/abs/1908.02144",
      "https://arxiv.org/abs/1908.02144},\n  year={2019}\n}\n```\n\n\n## General Structure of the Repositiory\n\n* acs: This directory contains the core code used in the repository. Inside you will find the following subdirectories and files:\n    - Baselines: contains implementations of the following baseline methods used for comparison in the paper:\n        - K-center greedy (https://arxiv.org/abs/1708.00489",
      "https://arxiv.org/abs/1708.00489",
      "https://arxiv.org/abs/https://arxiv.org/abs/1908.02144"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you use this code, please cite our [paper](https://arxiv.org/abs/1908.02144):\n```\n@article{pinsler2019bayesian,\n  title={Bayesian Batch Active Learning as Sparse Subset Approximation},\n  author={Pinsler, Robert and Gordon, Jonathan and Nalisnick, Eric and Hern{\\'a}ndez-Lobato, Jos{\\'e} Miguel},\n  journal={arXiv preprint arXiv:https://arxiv.org/abs/1908.02144},\n  year={2019}\n}\n```\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{pinsler2019bayesian,\n  title={Bayesian Batch Active Learning as Sparse Subset Approximation},\n  author={Pinsler, Robert and Gordon, Jonathan and Nalisnick, Eric and Hern{\\'a}ndez-Lobato, Jos{\\'e} Miguel},\n  journal={arXiv preprint arXiv:https://arxiv.org/abs/1908.02144},\n  year={2019}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9977994744046882
      ],
      "excerpt": "K-center greedy (https://arxiv.org/abs/1708.00489) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8246350245501425
      ],
      "excerpt": "used for larger regression datasets such as year and protein. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rpinsler/active-bayesian-coresets",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-11-13T17:45:04Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-29T04:59:36Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9492089843000577
      ],
      "excerpt": "This repository contains the code to reproduce the experiments carried out in \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9294690597606006
      ],
      "excerpt": "Code to generate active learning curves as exhibited in the paper is also provided. To generate appropriate learning curves, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9557039814757095
      ],
      "excerpt": "Baselines: contains implementations of the following baseline methods used for comparison in the paper: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8947097543082468
      ],
      "excerpt": "acquisition_functions: implementation of all the functions necessary for constructing acquisition functions (e.g., \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9600435400732001
      ],
      "excerpt": "al_data_set: implementation of a special data handling class that supports active learning in PyTorch (e.g., handles \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9040228516568256,
        0.8962676925977408,
        0.8948585914552984,
        0.9607130136811822,
        0.8032178356767093,
        0.881967161392917
      ],
      "excerpt": "-coresets: core file handling the method proposed in the paper. Implements classes such as Coreset (general use for \nactive learning querying), Frank-Wolfe coresets, random / argmax acquisition. Also contains implementations of baseline \nmethods mentioned above in our active learning framework, \n-model: implements the model classes used in our experiments, including linear  regression, neural networks, \nand approximate Bayesian inference layers such as variational inference, (local) reparametrization, etc. \nutils: collection of utility and probability functionalities required in the code base. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8288664600087258
      ],
      "excerpt": "linear_regression: simple training script for linear regression models on the UCI datasets. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9777385221741574,
        0.8505509718796964
      ],
      "excerpt": "with our methods (Frank-Wolf) optimization. \nresnet: Contains code necessary for loading and training ResNet modules. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rpinsler/active-bayesian-coresets/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 10,
      "date": "Tue, 28 Dec 2021 06:42:17 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/rpinsler/active-bayesian-coresets/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "rpinsler/active-bayesian-coresets",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/rpinsler/active-bayesian-coresets/master/scripts/run_active_torchvision.sh",
      "https://raw.githubusercontent.com/rpinsler/active-bayesian-coresets/master/scripts/run_active_regression_projections.sh",
      "https://raw.githubusercontent.com/rpinsler/active-bayesian-coresets/master/scripts/run_active_regression.sh",
      "https://raw.githubusercontent.com/rpinsler/active-bayesian-coresets/master/scripts/run_active_torchvision_projections.sh"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.8295832871760034
      ],
      "excerpt": "python3 ./scripts/enjoy_learning_curves.py --load_dir=LOAD_DIR --metric=METRIC --eval_at=EVAL_AT --format=FORMAT \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8366785039639878
      ],
      "excerpt": "the pool set in batch mode until budget is exhausted) with a specified model, acquisition function, dataset, and  \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/rpinsler/active-bayesian-coresets/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other",
      "url": "https://raw.githubusercontent.com/rpinsler/active-bayesian-coresets/master/LICENSE"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'# Bayesian Batch Active Learning as Sparse Subset Approximation\\n\\n###Academic and Non-Commercial Research Use Software License and Terms of Use\\n\\nBayesian Batch Active Learning as Sparse Subset Approximation is a software\\npackage to perform Bayesian batch active learning according to specific\\nalgorithms (the \\xe2\\x80\\x9cSoftware\\xe2\\x80\\x9d).  The Software is designed to automatically run\\nactive learning experiments in a manner that iteratively selects batches of\\nquery points to be labelled in such a way that learning is achieved in as\\nfew queries as possible.\\n\\nThe Software was developed by Robert Pinsler, Jonathan Gordon, Eric Nalisnick,\\nand Jose Miguel Hernandez-Lobato at the University of Cambridge. Pursuant to an\\ninter-institutional agreement between the parties it is distributed for free\\nacademic and non-commercial research use by the Chancellor, Masters and Scholars\\nof the University of Cambridge (\"Cambridge\") and Samsung Electronics Co., Ltd (\"Samsung\").\\n\\nUsing the Software indicates your agreement to be bound by the terms\\nof this Software Use Agreement (\\xe2\\x80\\x9cAgreement\\xe2\\x80\\x9d). Absent your agreement\\nto the terms below, you (the \\xe2\\x80\\x9cEnd User\\xe2\\x80\\x9d) have no rights to hold or\\nuse the Software whatsoever.\\n\\nCambridge and Samsung agree to grant hereunder the limited\\nnon-exclusive license to End User for the use of the Software in the\\nperformance of End User\\xe2\\x80\\x99s internal, non-commercial research and academic\\nuse at End User\\xe2\\x80\\x99s academic or not-for-profit research institution\\n(\\xe2\\x80\\x9cInstitution\\xe2\\x80\\x9d) on the following terms and conditions:\\n\\n1.  NO REDISTRIBUTION. The Software remains the property of Cambridge and\\nSamsung, and except as set forth in Section 4, End User\\nshall not publish, distribute, or otherwise transfer or make\\navailable the Software to any other party.\\n\\n2.  NO COMMERCIAL USE. End User shall not use the Software for\\ncommercial purposes and any such use of the Software is expressly\\nprohibited. This includes, but is not limited to, use of the\\nSoftware in fee-for-service arrangements, core facilities or\\nlaboratories or to provide research services to (or in collaboration\\nwith) third parties for a fee, and in industry-sponsored\\ncollaborative research projects where any commercial rights are\\ngranted to the sponsor. If End User wishes to use the Software for\\ncommercial purposes or for any other restricted purpose, End User\\nmust execute a separate license agreement with Samsung.\\n\\n3.  OWNERSHIP AND COPYRIGHT NOTICE. Cambridge and Samsung own\\nall intellectual property in the Software. End User shall gain no\\nownership to the Software. End User shall not remove or delete and\\nshall retain in the Software, in any modifications to Software and\\nin any Derivative Works, the copyright, trademark, or other notices\\npertaining to Software as provided with the Software.\\n\\n4.  DERIVATIVE WORKS. End User may create and use Derivative Works,\\nas such term is defined under U.S. copyright laws, provided that any\\nsuch Derivative Works shall be restricted to non-commercial,\\ninternal research and academic use at End User\\xe2\\x80\\x99s Institution. End\\nUser may distribute Derivative Works to other Institutions solely\\nfor the performance of non-commercial, internal research and\\nacademic use on terms substantially similar to this License and\\nTerms of Use.\\n\\n5.  PUBLICATION & ATTRIBUTION. End User has the right to publish,\\npresent, or share results from the use of the Software.  In\\naccordance with customary academic practice, End User will\\nacknowledge Cambridge and Samsung as the providers\\nof the Software and may cite the relevant reference(s) from the\\nfollowing list of publications:\\n\\nBayesian Batch Active Learning as Sparse Subset Approximation\\nRobert Pinsler, Jonathan Gordon, Eric Nalisnick, and Jose Miguel Hernandez-Lobato\\nAdvances in Neural Information Processing Systems, 2019\\n\\n6.  NO WARRANTIES. THE SOFTWARE IS PROVIDED \"AS IS.\" TO THE FULLEST\\nEXTENT PERMITTED BY LAW, CAMBRIDGE AND SAMSUNG HEREBY DISCLAIM ALL WARRANTIES\\nOF ANY KIND (EXPRESS, IMPLIED OR OTHERWISE) REGARDING THE SOFTWARE, INCLUDING\\nBUT NOT LIMITED TO ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A\\nPARTICULAR PURPOSE, OWNERSHIP, AND NON-INFRINGEMENT.  CAMBRIDGE AND SAMSUNG\\nMAKE NO WARRANTY ABOUT THE ACCURACY, RELIABILITY, COMPLETENESS, TIMELINESS,\\nSUFFICIENCY OR QUALITY OF THE SOFTWARE.  CAMBRIDGE AND SAMSUNG DO NOT WARRANT\\nTHAT THE SOFTWARE WILL OPERATE WITHOUT ERROR OR INTERRUPTION.\\n\\n7.  LIMITATIONS OF LIABILITY AND REMEDIES. USE OF THE SOFTWARE IS AT\\nEND USER\\xe2\\x80\\x99S OWN RISK. IF END USER IS DISSATISFIED WITH THE SOFTWARE,\\nITS EXCLUSIVE REMEDY IS TO STOP USING IT.  IN NO EVENT SHALL\\nCAMBRIDGE OR SAMSUNG BE LIABLE TO END USER OR ITS INSTITUTION, IN CONTRACT,\\nTORT OR OTHERWISE, FOR ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL,\\nCONSEQUENTIAL, PUNITIVE OR OTHER DAMAGES OF ANY KIND WHATSOEVER ARISING\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE, EVEN IF CAMBRIDGE OR SAMSUNG IS\\nNEGLIGENT OR OTHERWISE AT FAULT, AND REGARDLESS OF WHETHER CAMBRIDGE,\\nSAMSUNG IS ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\\n\\n8. NON-USE OF NAME.  Nothing in this License and Terms of Use shall\\nbe construed as granting End Users or their Institutions any rights\\nor licenses to use any trademarks, service marks or logos associated\\nwith the Software.  You may not use the terms \\xe2\\x80\\x9cCambridge\\xe2\\x80\\x9d or \\xe2\\x80\\x9cSamsung\\xe2\\x80\\x9d\\n(or a substantially similar term) in any way that is inconsistent with\\nthe permitted uses described herein. You agree not to use any name or\\nemblem of Cambridge, Samsung, or any of their subdivisions for any purpose,\\nor to falsely suggest any relationship between End User (or its\\nInstitution) and Cambridge or Samsung, or in any manner that would infringe\\nor violate any of their rights.\\n\\n9. End User represents and warrants that it has the legal authority\\nto enter into this License and Terms of Use on behalf of itself and\\nits Institution.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Bayesian Batch Active Learning as Sparse Subset Approximation",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "active-bayesian-coresets",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "rpinsler",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rpinsler/active-bayesian-coresets/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This code requires the following:\n* Python  >= 3.5\n* torch >= 1.0\n* torchvision >= 0.2.2\n* numpy\n* scipy\n* sklearn\n* pandas\n* matplotlib\n* gtimer\n\nTo run the regression experiments, please [download](http://archive.ics.uci.edu/ml/datasets.php) the UCI regression datasets and place them into ./data.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "* The code supports experiments on either GPU or CPU processors.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 30,
      "date": "Tue, 28 Dec 2021 06:42:17 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The experiments provided in this code base include active learning on standard vision based datasets (classification)\nand UCI datasets (regression). The following experiments are provided (see section 7 of the\npaper):\n\n1. Active learning for regression: run the following command \n\n     ```./scripts/run_active_regression.sh DATASET ACQ CORESET```  \n\n    where \n    - ```DATASET``` may be one of ```{yacht, boston, energy, power, year}``` (determines the dataset to be used).\n    - ```ACQ```  may be one of ```{BALD, Entropy, ACS}``` (determines the acquisition function to be used).\n    - ```CORESET``` may be one of ```{Argmax, Random, Best, FW}``` (determines the querying strategy to be used)  \n\n    For example, to run the proposed method on the boston dataset, please run:\n    \n    ```./scripts/run_active_regression.sh boston ACS FW```\n\n    This will automatically generate an experimental directory with the appropriate name, and place results from 40 seeds in\n    the directory. Hyper-parameters for the experiments can all be found in the main body of the paper.\n    \n2. Active learning for regression (with projections -- should be used for large datasets e.g., year and power):\nrun the following command \n\n     ```./scripts/run_active_regression_projections.sh DATASET NUM_PROJECTIONS```  \n\n    where \n    - ```DATASET``` may be one of ```{yacht, boston, power, protein, year}``` (determines the dataset to be used).\n    - ```NUM_PROJECTIONS```  is an integer (determines the number of samples used to estimate values).  \n\n    For example, to run the proposed method on the year dataset, please run:\n    \n    ```./scripts/run_active_regression.sh year 10```\n\n    This will automatically generate an experimental directory with the appropriate name, and place results from 40 seeds in\n    the directory. Hyper-parameters for the experiments can all be found in the main body of the paper.\n    \n3. Active learning for classification (using standard active learning methods): run the following command\n\n   ```./scripts/run_active_torchvision.sh ACQ CORESET DATASET ```   \n   \n   where \n    - ```ACQ```  may be one of ```{BALD, Entropy}``` (determines the acquisition function to be used).\n    - ```CORESET``` may be one of ```{Argmax, Random, Best}``` (determines the querying strategy to be used)\n    - ```DATASET``` may be one of ```{cifar10, svhn, fashion_mnist}``` (determines the dataset to be used).\n    \n   For example, to run greedy BALD on CIFAR10, run the following command:\n   \n   ```./scripts/run_active_torchvision.sh BALD Argmax cifar10```\n   \n   This will automaticall generate an experimental directory with an appropriate name, and place results from\n   5 runs in the directory.\n\n4. Active learning for classification (using projections as in section 5 of the paper): run the following command\n\n    ```./scripts/run_active_torchvision_projections.sh CORESET DATASET ```    \n\n    where \n    -```CORESET``` may be one of ```{Argmax, Random, Best, FW}``` (determines the querying strategy to be used)\n    -```DATASET``` may be one of ```{cifar10, svhn, fashion_mnist}``` (determines the dataset to be used).\n    \n    For example, to run the proposed method on the CIFAR10 dataset, please run:\n    \n    ```./scripts/run_active_torchvision_projections.sh FW cifar10```\n    \n    This will automaticall generate an experimental directory with an appropriate name, and place results from\n    5 runs in the directory.\n    \n",
      "technique": "Header extraction"
    }
  ]
}