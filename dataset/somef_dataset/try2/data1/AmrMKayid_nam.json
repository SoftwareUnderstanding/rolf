{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2004.13912",
      "https://arxiv.org/abs/2004.13912"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```bibtex\n@misc{kayid2020nams,\n  title={Neural additive models Library},\n  author={Kayid, Amr and Frosst, Nicholas and Hinton, Geoffrey E},\n  year={2020}\n}\n```\n\n```bibtex\n@article{agarwal2020neural,\n  title={Neural additive models: Interpretable machine learning with neural nets},\n  author={Agarwal, Rishabh and Frosst, Nicholas and Zhang, Xuezhou and Caruana, Rich and Hinton, Geoffrey E},\n  journal={arXiv preprint arXiv:2004.13912},\n  year={2020}\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{agarwal2020neural,\n  title={Neural additive models: Interpretable machine learning with neural nets},\n  author={Agarwal, Rishabh and Frosst, Nicholas and Zhang, Xuezhou and Caruana, Rich and Hinton, Geoffrey E},\n  journal={arXiv preprint arXiv:2004.13912},\n  year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{kayid2020nams,\n  title={Neural additive models Library},\n  author={Kayid, Amr and Frosst, Nicholas and Hinton, Geoffrey E},\n  year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8109194328925066
      ],
      "excerpt": "config.regression = True \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8109194328925066
      ],
      "excerpt": "        (2): LinReLU(in_features=64, out_features=32) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8109194328925066
      ],
      "excerpt": "        (2): LinReLU(in_features=64, out_features=32) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8109194328925066
      ],
      "excerpt": "        (2): LinReLU(in_features=64, out_features=32) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/AmrMKayid/nam",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-11-10T14:59:51Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-09T10:48:08Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9862948896884178
      ],
      "excerpt": "Neural Additive Models (NAMs) combine some of the expressivity of DNNs with the inherent intelligibility of generalized additive models. NAMs learn a linear combination of neural networks that each attend to a single input feature. These networks are trained jointly and can learn arbitrarily complex relationships between their input feature and the output. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = NAM( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "      (model): ModuleList( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "      (model): ModuleList( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "      (model): ModuleList( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "for fold, (trainloader, valloader) in enumerate(dataloaders): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Neural Additive Models (Google Research)",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/AmrMKayid/nam/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 8,
      "date": "Sun, 26 Dec 2021 17:25:45 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/AmrMKayid/nam/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "AmrMKayid/nam",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/AmrMKayid/nam/main/tutorial.ipynb",
      "https://raw.githubusercontent.com/AmrMKayid/nam/main/notebooks/tutorial.ipynb",
      "https://raw.githubusercontent.com/AmrMKayid/nam/main/notebooks/wandb.ipynb",
      "https://raw.githubusercontent.com/AmrMKayid/nam/main/old/main.ipynb",
      "https://raw.githubusercontent.com/AmrMKayid/nam/main/old/experimental.ipynb",
      "https://raw.githubusercontent.com/AmrMKayid/nam/main/old/graphing.ipynb",
      "https://raw.githubusercontent.com/AmrMKayid/nam/main/old/gallup_runs.ipynb",
      "https://raw.githubusercontent.com/AmrMKayid/nam/main/old/debugging-backup.ipynb",
      "https://raw.githubusercontent.com/AmrMKayid/nam/main/old/EDA.ipynb",
      "https://raw.githubusercontent.com/AmrMKayid/nam/main/old/debugging.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9769420979004304
      ],
      "excerpt": "| Installation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8661176197453521
      ],
      "excerpt": "  name=\"NAM_GALLUP\", \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9717106327039013
      ],
      "excerpt": "                            version=f'fold_{fold + 1}') \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8600689777569865,
        0.9079185768576028,
        0.8900486270063179,
        0.8214116752289031,
        0.9416522774131079
      ],
      "excerpt": "from nam.config import defaults \nfrom nam.data import FoldedDataset, NAMDataset \nfrom nam.models import NAM, get_num_units \nfrom nam.trainer import LitNAM \nfrom nam.utils import * \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9161421958484287
      ],
      "excerpt": "Config(activation='exu', batch_size=1024, cross_val=False, data_path='data/GALLUP.csv', decay_rate=0.995, device='cpu', dropout=0.5, early_stopping_patience=50, experiment_name='NAM', feature_dropout=0.5, fold_num=1, hidden_sizes=[64, 32], l2_regularization=0.5, logdir='output', lr=0.0003, num_basis_functions=1000, num_epochs=1, num_folds=5, num_models=1, num_splits=3, num_workers=16, optimizer='adam', output_regularization=0.5, regression=False, save_model_frequency=2, save_top_k=3, seed=2021, shuffle=True, units_multiplier=2, use_dnn=False, wandb=True) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9133368656218674
      ],
      "excerpt": "import sklearn \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8051051573918885
      ],
      "excerpt": "dataset = pd.DataFrame(data=housing.data, columns=housing.feature_names) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8421074476017179,
        0.8520186037847441
      ],
      "excerpt": "  name=\"NAM_GALLUP\", \n  num_inputs=len(dataset[0][0]), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8670836796821269
      ],
      "excerpt": "                            name=f'{model.name}', \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8575435024620189
      ],
      "excerpt": "checkpoint_callback = ModelCheckpoint(filename=tb_logger.log_dir + \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8289201960872105,
        0.8289669050403863
      ],
      "excerpt": ":#: Testing the trained model \n:#: Output \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8078145619394667,
        0.8078145619394667
      ],
      "excerpt": "fig1 = plot_mean_feature_importance(litmodel.model, dataset) \nfig2 = plot_nams(litmodel.model, dataset, num_cols= 3) \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/AmrMKayid/nam/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "HTML",
      "Python",
      "Makefile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 Amr M. Kayid\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "NAM: Neural Additive Models - Interpretable Machine Learning with Neural Nets",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "nam",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "AmrMKayid",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/AmrMKayid/nam/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 12,
      "date": "Sun, 26 Dec 2021 17:25:45 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```bash\n$ python main.py -h\nusage: Neural Additive Models [-h] [--num_epochs NUM_EPOCHS]\n                              [--learning_rate LEARNING_RATE]\n                              [--batch_size BATCH_SIZE] --data_path DATA_PATH\n                              --features_columns FEATURES_COLUMNS\n                              [FEATURES_COLUMNS ...] --targets_column\n                              TARGETS_COLUMN [TARGETS_COLUMN ...]\n                              [--weights_column WEIGHTS_COLUMN]\n                              [--experiment_name EXPERIMENT_NAME]\n                              [--regression REGRESSION] [--logdir LOGDIR]\n                              [--wandb WANDB]\n                              [--hidden_sizes HIDDEN_SIZES [HIDDEN_SIZES ...]]\n                              [--activation {exu,relu}] [--dropout DROPOUT]\n                              [--feature_dropout FEATURE_DROPOUT]\n                              [--decay_rate DECAY_RATE]\n                              [--l2_regularization L2_REGULARIZATION]\n                              [--output_regularization OUTPUT_REGULARIZATION]\n                              [--dataset_name DATASET_NAME] [--seed SEED]\n                              [--num_basis_functions NUM_BASIS_FUNCTIONS]\n                              [--units_multiplier UNITS_MULTIPLIER]\n                              [--shuffle SHUFFLE] [--cross_val CROSS_VAL]\n                              [--num_folds NUM_FOLDS]\n                              [--num_splits NUM_SPLITS] [--fold_num FOLD_NUM]\n                              [--num_models NUM_MODELS]\n                              [--early_stopping_patience EARLY_STOPPING_PATIENCE]\n                              [--use_dnn USE_DNN] [--save_top_k SAVE_TOP_K]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --num_epochs NUM_EPOCHS\n                        The number of epochs to run training for.\n  --learning_rate LEARNING_RATE\n                        Hyperparameter: learning rate.\n  --batch_size BATCH_SIZE\n                        Hyperparameter: batch size.\n  --data_path DATA_PATH\n                        The path for the training data\n  --features_columns FEATURES_COLUMNS [FEATURES_COLUMNS ...]\n                        Name of the feature columns in the dataset\n  --targets_column TARGETS_COLUMN [TARGETS_COLUMN ...]\n                        Name of the target column in the dataset\n  --weights_column WEIGHTS_COLUMN\n                        Name of the weights column in the dataset\n  --experiment_name EXPERIMENT_NAME\n                        The name for the experiment\n  --regression REGRESSION\n                        Boolean flag indicating whether we are solving a\n                        regression task or a classification task.\n  --logdir LOGDIR       Path to dir where to store summaries.\n  --wandb WANDB         Using wandb for experiments tracking and logging\n  --hidden_sizes HIDDEN_SIZES [HIDDEN_SIZES ...]\n                        Feature Neural Net hidden sizes\n  --activation {exu,relu}\n                        Activation function to used in the hidden layer.\n                        Possible options: (1) relu, (2) exu\n  --dropout DROPOUT     Hyperparameter: Dropout rate\n  --feature_dropout FEATURE_DROPOUT\n                        Hyperparameter: Prob. with which features are dropped\n  --decay_rate DECAY_RATE\n                        Hyperparameter: Optimizer decay rate\n  --l2_regularization L2_REGULARIZATION\n                        Hyperparameter: l2 weight decay\n  --output_regularization OUTPUT_REGULARIZATION\n                        Hyperparameter: feature reg\n  --dataset_name DATASET_NAME\n                        Name of the dataset to load for training.\n  --seed SEED           seed for torch\n  --num_basis_functions NUM_BASIS_FUNCTIONS\n                        Number of basis functions to use in a FeatureNN for a\n                        real-valued feature.\n  --units_multiplier UNITS_MULTIPLIER\n                        Number of basis functions for a categorical feature\n  --shuffle SHUFFLE     Shuffle the training data\n  --cross_val CROSS_VAL\n                        Boolean flag indicating whether to perform cross\n                        validation or not.\n  --num_folds NUM_FOLDS\n                        Number of N folds\n  --num_splits NUM_SPLITS\n                        Number of data splits to use\n  --fold_num FOLD_NUM   Index of the fold to be used\n  --num_models NUM_MODELS\n                        the number of models to train.\n  --early_stopping_patience EARLY_STOPPING_PATIENCE\n                        Early stopping epochs\n  --use_dnn USE_DNN     Deep NN baseline.\n  --save_top_k SAVE_TOP_K\n                        Indicates the maximum number of recent checkpoint\n                        files to keep.\n```\n\n\n",
      "technique": "Header extraction"
    }
  ]
}