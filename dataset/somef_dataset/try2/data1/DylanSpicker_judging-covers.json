{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/ 1804.03599.\n    * Irina Higgins et al. \u201cbeta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework\u201d. In: Iclr July (2017), pp. 1\u201313. url: https://openreview.net/forum?id=Sy2fzU9gl.\n    * Diederik P Kingma and Max Welling. \u201cAuto-Encoding Variational Bayes\u201d. In: Ml (2013), pp. 1\u201314. issn: 1312.6114v10. doi: 10.1051/0004- 6361/201527329. https://arxiv.org/abs/ 1312.6114. url: http://arxiv.org/abs/1312.6114.\n* For a brief introduction to developing standard VAEs, I suggest [this](http://kvfrans.com/variational-autoencoders-explained/) blog post, walking through the development in Python.\n* There are an incredible number of tutorials explaining SVMs, and their implementation across various lanaguages. [This](https://blog.statsbot.co/support-vector-machines-tutorial-c1618e635e93) is one that does a good visual job of justifying the use.\n\n## Qualifications on the Findings\nThe major qualification on the findings here have to do with claiming that all of the information used comes directly from the book covers. While it is true that the only data used in the modelling process was the projected images, it is seldom the case that data carries with it no further information. In particular, the way that SVMs are fit, the representative nature of the training data contains valuable information for the model - namely that a user is more likely to select one rating versus another. As such, you can use an informed baseline where instead of selecting a random prediction with equal probabilities, you select a a random prediction given the individuals previous selection patterns. This will certainly outperform the random selection, though, there is still added benefit to the information contained in the covers. \n\nThis is a worthwhile lesson to keep in mind for all models that are correctly constructed. In fact, one of the great strengths of modelling in particular ways in the implicit ability to account for the prior distribution of the data.\n\n## Navigating the Repository\nThe ```Exploration Notebook.ipynb``` contains my initial building of the VAE and the exploration of the data briefly. There are comments to let you know what the cells are doing, and looking through it showcases my initial programatic exploration of the idea to use a VAE to encode the covers.\n\nThe ```User Analysis.ipynb``` contains the actual user-level analysis that is reported in the write-up. The actual SVMs are fit, and the independent analysis is run, based on the users particular ratings.",
      "https://arxiv.org/abs/ 1312.6114. url: http://arxiv.org/abs/1312.6114.\n* For a brief introduction to developing standard VAEs, I suggest [this](http://kvfrans.com/variational-autoencoders-explained/) blog post, walking through the development in Python.\n* There are an incredible number of tutorials explaining SVMs, and their implementation across various lanaguages. [This](https://blog.statsbot.co/support-vector-machines-tutorial-c1618e635e93) is one that does a good visual job of justifying the use.\n\n## Qualifications on the Findings\nThe major qualification on the findings here have to do with claiming that all of the information used comes directly from the book covers. While it is true that the only data used in the modelling process was the projected images, it is seldom the case that data carries with it no further information. In particular, the way that SVMs are fit, the representative nature of the training data contains valuable information for the model - namely that a user is more likely to select one rating versus another. As such, you can use an informed baseline where instead of selecting a random prediction with equal probabilities, you select a a random prediction given the individuals previous selection patterns. This will certainly outperform the random selection, though, there is still added benefit to the information contained in the covers. \n\nThis is a worthwhile lesson to keep in mind for all models that are correctly constructed. In fact, one of the great strengths of modelling in particular ways in the implicit ability to account for the prior distribution of the data.\n\n## Navigating the Repository\nThe ```Exploration Notebook.ipynb``` contains my initial building of the VAE and the exploration of the data briefly. There are comments to let you know what the cells are doing, and looking through it showcases my initial programatic exploration of the idea to use a VAE to encode the covers.\n\nThe ```User Analysis.ipynb``` contains the actual user-level analysis that is reported in the write-up. The actual SVMs are fit, and the independent analysis is run, based on the users particular ratings."
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* The Data used for the Analysis was pulled from [zygmuntz/goodbooks-10k](https://github.com/zygmuntz/goodbooks-10k)\n* There are a number of technical references for VAEs, of particular note are: \n    * Christopher P Burgess et al. \u201cUnderstanding disentangling in \u03b2 -VAE\u201d. In: Nips 2017 Nips (2017). arXiv: 1804.03599.\n    * Irina Higgins et al. \u201cbeta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework\u201d. In: Iclr July (2017), pp. 1\u201313. url: https://openreview.net/forum?id=Sy2fzU9gl.\n    * Diederik P Kingma and Max Welling. \u201cAuto-Encoding Variational Bayes\u201d. In: Ml (2013), pp. 1\u201314. issn: 1312.6114v10. doi: 10.1051/0004- 6361/201527329. arXiv: 1312.6114. url: http://arxiv.org/abs/1312.6114.\n* For a brief introduction to developing standard VAEs, I suggest [this](http://kvfrans.com/variational-autoencoders-explained/) blog post, walking through the development in Python.\n* There are an incredible number of tutorials explaining SVMs, and their implementation across various lanaguages. [This](https://blog.statsbot.co/support-vector-machines-tutorial-c1618e635e93) is one that does a good visual job of justifying the use.\n\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/DylanSpicker/judging-covers",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-06-18T00:11:30Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-07-03T22:18:16Z",
    "technique": "GitHub API"
  },
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/DylanSpicker/judging-covers/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 28 Dec 2021 17:24:55 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/DylanSpicker/judging-covers/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "DylanSpicker/judging-covers",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/DylanSpicker/judging-covers/master/Exploration%20Notebook.ipynb",
      "https://raw.githubusercontent.com/DylanSpicker/judging-covers/master/User%20Analysis.ipynb",
      "https://raw.githubusercontent.com/DylanSpicker/judging-covers/master/.ipynb_checkpoints/User%20Analysis-checkpoint.ipynb",
      "https://raw.githubusercontent.com/DylanSpicker/judging-covers/master/.ipynb_checkpoints/GR_Explore-checkpoint.ipynb"
    ],
    "technique": "File Exploration"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/DylanSpicker/judging-covers/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Who Judges Books by Their Covers?",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "judging-covers",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "DylanSpicker",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/DylanSpicker/judging-covers/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 28 Dec 2021 17:24:55 GMT"
    },
    "technique": "GitHub API"
  }
}