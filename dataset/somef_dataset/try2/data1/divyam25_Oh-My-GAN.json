{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* \"Unsupervised representation learning with deep convolutional generative adversarial networks.\" [[arxiv]](https://arxiv.org/pdf/1511.06434)\r\n* \"Wasserstein GAN\" [[arxiv]](https://arxiv.org/pdf/1701.07875)\r\n* \"Improved Training of Wasserstein GANs\" [[arxiv]](https://arxiv.org/pdf/1704.00028)\r\n* https://pytorch.org/\r\n* https://github.com/soumith/ganhacks ~ GAN Hacks\r\n\r\n\r\n\r\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/divyam25/Oh-My-GAN",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-12-20T16:02:59Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-06-11T18:32:26Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8458559515132522,
        0.8934662269156022,
        0.8740971339916267,
        0.8321744283196343
      ],
      "excerpt": "1. A generator G to map a Z following a [simple] fixed distribution to the desired \"real\" distribution, and \n2. a discriminator D to classify data points as \"real\" or \"fake\" (i.e. from G). \nThe approach is adversarial since the two networks have antagonistic objectives. \nGANs have been known to be unstable to train, often resulting in generators that produce nonsensical outputs. There has been very limited published research in trying to understand and visualize what GANs learn, and the intermediate representations of multi-layer GANs.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9899147590559635,
        0.8741296507654472,
        0.8216829244450861
      ],
      "excerpt": "\"We also encountered difficulties attempting to scale GANs using CNN architectures commonly used in the supervised literature. However, after extensive model exploration we identified a family of architectures that resulted in stable training across a range of datasets and allowed for training higher resolution and deeper generative models.\" \nPaper on DCGAN(Radford et.al 2015) proposes and evaluates a set of constraints on the architectural topology of Convolutional GANs that make them stable to train in most settings. \nReplace any pooling layers with strided convolutions (discriminator) and fractional-strided convolutions  (generator) ; often miscoined as Deconvolution. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.867412305313153
      ],
      "excerpt": "Use ReLU activation in generator for all layers except for the output, which uses Tanh. (bounded activation saturates the model quickly) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9127534984627738,
        0.9488420923372927,
        0.9533781075821117,
        0.981581296600546,
        0.9344373401678335,
        0.8851004567635752
      ],
      "excerpt": "I will be running a few more epochs to check for any kind of improvements in Generator Performance further. \nWith a uniform distribution Z constant for every epoch (same digit in the block throughout all epochs) \nWith a uniform distribution Z changing every epoch. (different digits in the block for different epochs) \nThe traditional GAN loss function works makes use of Jensen-Shannon Divergence which does not account much for the metric space. An alternative choice is the \"earth moving distance\", which intuitively is the minimum mass displacement to transform one distribution into the other. \nWGANs cure the main training problems of GANs. In particular, training WGANs does not require maintaining a careful balance in training of the discriminator and the generator, and does not require a careful design of the network architecture either. One of the most compelling practical benefits of WGANs is the ability to continuously estimate the EM (Wasserstein) distance by training the discriminator to optimality. \nThe two benefits observed on using Wasserstein Distace for training : \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9891762659469594
      ],
      "excerpt": "- A greater interpretability of the loss, which is a better indicator of the quality of the samples. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9622559660987784,
        0.9488420923372927,
        0.9533781075821117
      ],
      "excerpt": "\"If the clipping parameter is large, then it can take a long time for any weights to reach their limit, thereby making it harder to train the critic till optimality. If the clipping is small, this can easily lead to vanishing gradients when the number of layers is big, or batch normalization is not used\" \nWith a uniform distribution Z constant for every epoch (same digit in the block throughout all epochs) \nWith a uniform distribution Z changing every epoch. (different digits in the block for different epochs) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "GAN Paper Implementations on PyTorch",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/divyam25/Oh-My-GAN/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Sat, 25 Dec 2021 00:49:18 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/divyam25/Oh-My-GAN/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "divyam25/Oh-My-GAN",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/divyam25/Oh-My-GAN/master/DCGAN_MNIST.ipynb",
      "https://raw.githubusercontent.com/divyam25/Oh-My-GAN/master/wDCGAN_Clip_MNIST.ipynb",
      "https://raw.githubusercontent.com/divyam25/Oh-My-GAN/master/wDCGAN_clip_CelebA.ipynb",
      "https://raw.githubusercontent.com/divyam25/Oh-My-GAN/master/.ipynb_checkpoints/wDCGAN_clip_CelebA-checkpoint.ipynb"
    ],
    "technique": "File Exploration"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/divyam25/Oh-My-GAN/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "## GAN Architectures",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Oh-My-GAN",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "divyam25",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/divyam25/Oh-My-GAN/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Sat, 25 Dec 2021 00:49:18 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "dcgan",
      "pytorch",
      "gan",
      "adversarial-networks",
      "generative-model",
      "wgan",
      "wgan-cp"
    ],
    "technique": "GitHub API"
  }
}