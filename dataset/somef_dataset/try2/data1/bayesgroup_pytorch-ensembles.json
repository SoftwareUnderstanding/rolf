{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2002.06470",
      "https://arxiv.org/abs/2002.06470"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you found this code useful, please cite our paper\n```\n@article{ashukha2020pitfalls,\n  title={Pitfalls of In-Domain Uncertainty Estimation and Ensembling in Deep Learning},\n  author={Ashukha, Arsenii and Lyzhov, Alexander and Molchanov, Dmitry and Vetrov, Dmitry},\n  journal={arXiv preprint arXiv:2002.06470},\n  year={2020}\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{ashukha2020pitfalls,\n  title={Pitfalls of In-Domain Uncertainty Estimation and Ensembling in Deep Learning},\n  author={Ashukha, Arsenii and Lyzhov, Alexander and Molchanov, Dmitry and Vetrov, Dmitry},\n  journal={arXiv preprint arXiv:2002.06470},\n  year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8001118323515268
      ],
      "excerpt": "  --num_workers M    number of workers (default: 10) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9535428095058772
      ],
      "excerpt": "|Architecture | CIFAR-10 | CIFAR-10-aug | CIFAR-100 | CIFAR-100-aug | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8028046190715653
      ],
      "excerpt": "- Stochastic Weight Averaging (SWA). Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, Andrew Gordon Wilson. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999380918993473,
        0.8354896428020421
      ],
      "excerpt": "- Cyclical Stochastic Gradient MCMC for Bayesian Deep Learning. Ruqi Zhang, Chunyuan Li, Jianyi Zhang, Changyou Chen and Andrew Gordon Wilson. \n- Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs.  Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry Vetrov and Andrew Gordon Wilson. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/SamsungLabs/pytorch-ensembles",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-11-29T13:46:57Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-13T03:21:45Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8799856625731813
      ],
      "excerpt": "- Saved logs with all computed results  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8295511211969333
      ],
      "excerpt": "The evaluation of ensembling methods can be done using the scripts from the ens folder, which contains a separate script for each ensembling method. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9607341521575864
      ],
      "excerpt": "* The interface for K-FAC-Laplace differs and is described below. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8150771532802734
      ],
      "excerpt": "--method regular/vi \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8769826816052998
      ],
      "excerpt": "Given a checkpoint, ens/ens-kfacl.py builds the Laplace approximation and produces the results of the approximate posterior averaging. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8997330201825394
      ],
      "excerpt": "We have used the following noise scales (also listed in Table 3, Appendix D in the paper): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9503930906811119
      ],
      "excerpt": "Refer to `ens/ens-kfacl.py' for the full list of arguments and default values. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9640721900940192
      ],
      "excerpt": "Parts of this code are based on the following repositories: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Pitfalls of In-Domain Uncertainty Estimation and Ensembling in Deep Learning, ICLR 2020",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/bayesgroup/pytorch-ensembles/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 18,
      "date": "Fri, 24 Dec 2021 06:37:52 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/SamsungLabs/pytorch-ensembles/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "SamsungLabs/pytorch-ensembles",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/bayesgroup/pytorch-ensembles/master/notebooks/tabsles.ipynb",
      "https://raw.githubusercontent.com/bayesgroup/pytorch-ensembles/master/notebooks/plots.ipynb",
      "https://raw.githubusercontent.com/bayesgroup/pytorch-ensembles/master/notebooks/DEE%20camera%20ready%20plots.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/bayesgroup/pytorch-ensembles/master/train/train_sse_mcmc.sh",
      "https://raw.githubusercontent.com/bayesgroup/pytorch-ensembles/master/train/train_cifar.sh",
      "https://raw.githubusercontent.com/bayesgroup/pytorch-ensembles/master/train/train_fge.sh",
      "https://raw.githubusercontent.com/bayesgroup/pytorch-ensembles/master/train/train_imagenet.sh",
      "https://raw.githubusercontent.com/bayesgroup/pytorch-ensembles/master/train/train_swag.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The following allows to create and to run a python environment with all required dependencies using [miniconda](https://docs.conda.io/en/latest/miniconda.html): \n\n```(bash)\nconda env create -f condaenv.yml\nconda activate megabayes\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9173764645360128
      ],
      "excerpt": "pip3 install wldhx.yadisk-direct \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8057675898656085
      ],
      "excerpt": "                     [--models_dir PATH] [--aug_test] [--batch_size N] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9821054585582006
      ],
      "excerpt": "* All scripts assume thatpytorch-ensemblesis the current working directory (cd  pytorch-ensembles). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9582690017232749
      ],
      "excerpt": "bash train/train_cifar.sh \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.87010436978358,
        0.87010436978358,
        0.87010436978358,
        0.87010436978358
      ],
      "excerpt": "bash train/train_fge.sh CIFAR10 WideResNet28x10 1 ~/weights ~/datasets \nbash train/train_swag.sh CIFAR10 WideResNet28x10 1 ~/weights ~/datasets \nbash train/train_sse_mcmc.sh CIFAR10 WideResNet28x10 1 ~/weights ~/datasets SSE \nbash train/train_sse_mcmc.sh CIFAR10 WideResNet28x10 1 ~/weights ~/datasets cSGLD \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8503196860760109
      ],
      "excerpt": "bash train/train_imagenet.sh --method regular/sse/fge/vi \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9322609392449874
      ],
      "excerpt": "- PyTorch \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8082022887640018
      ],
      "excerpt": "unzip deepens_imagenet.zip  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8082022887640018
      ],
      "excerpt": "unzip deepens_cifars.zip  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9157870474782381
      ],
      "excerpt": "usage: ens-onenet.py [-h] --dataset DATASET [--data_path PATH] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8717043103779678,
        0.8330107448797127
      ],
      "excerpt": "  --dataset DATASET  dataset name CIFAR10/CIFAR100/ImageNet (default: None) \n  --data_path PATH   path to a data-folder (default: ~/data) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8421730270059533
      ],
      "excerpt": "  --batch_size N     input batch size (default: 256) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8924312570040221
      ],
      "excerpt": "  --fname FNAME      comment to a log file name (default: unnamed) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8640304151270447
      ],
      "excerpt": "* The scripts will write .csv logs inpytorch-ensembles/logsin the following formatrowid, dataset, architecture, ensemble_method, n_samples, metric, value, info.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8289305381227843,
        0.8289305381227843,
        0.8289305381227843,
        0.8279624741583713,
        0.8289305381227843,
        0.8279624741583713,
        0.8289305381227843,
        0.8279624741583713
      ],
      "excerpt": "ipython -- ens/ens-onenet.py  --dataset=CIFAR10/CIFAR100/ImageNet \nipython -- ens/ens-deepens.py --dataset=CIFAR10/CIFAR100/ImageNet \nipython -- ens/ens-sse.py     --dataset=CIFAR10/CIFAR100/ImageNet \nipython -- ens/ens-csgld.py   --dataset=CIFAR10/CIFAR100 \nipython -- ens/ens-fge.py     --dataset=CIFAR10/CIFAR100/ImageNet \nipython -- ens/ens-dropout.py --dataset=CIFAR10/CIFAR100 \nipython -- ens/ens-vi.py      --dataset=CIFAR10/CIFAR100/ImageNet \nipython -- ens/ens-swag.py    --dataset=CIFAR10/CIFAR100 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8195056685499529
      ],
      "excerpt": "Examples of training commands: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.800219093757095
      ],
      "excerpt": "bash train/train_cifar.sh \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8195056685499529
      ],
      "excerpt": "Examples of training commands: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8290073713211236,
        0.8290073713211236,
        0.8290073713211236,
        0.8290073713211236,
        0.8022657288499184
      ],
      "excerpt": "bash train/train_fge.sh CIFAR10 WideResNet28x10 1 ~/weights ~/datasets \nbash train/train_swag.sh CIFAR10 WideResNet28x10 1 ~/weights ~/datasets \nbash train/train_sse_mcmc.sh CIFAR10 WideResNet28x10 1 ~/weights ~/datasets SSE \nbash train/train_sse_mcmc.sh CIFAR10 WideResNet28x10 1 ~/weights ~/datasets cSGLD \nScript parameters: dataset, architecture name, training run id, root directory for saving snapshots (created automatically), root directory for datasets (downloaded automatically) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9125517700760102,
        0.8195056685499529
      ],
      "excerpt": "ipython -- ens/ens-kfacl.py --file CHECKPOINT --data_path DATA --dataset CIFAR10 --model PreResNet110 --scale 0.213 \nExamples of training commands: \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/SamsungLabs/pytorch-ensembles/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other",
      "url": "https://raw.githubusercontent.com/bayesgroup/pytorch-ensembles/master/LICENSE"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'BSD 2-Clause License\\n\\nCopyright (c) 2020 Samsung AI Center Moscow, Arsenii Ashukha, Alexander Lyzhov, Dmitry Molchanov, Dmitry Vetrov\\nAll rights reserved.\\n\\nParts of this software are based on the following repositories:\\n- Stochastic Weight Averaging (SWA), https://github.com/timgaripov/swa, Copyright (c) 2018, Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, Andrew Gordon Wilson\\n- A Simple Baseline for Bayesian Deep Learning, https://github.com/wjmaddox/swa_gaussian, Copyright (c) 2019, Wesley Maddox, Timur Garipov, Pavel Izmailov,  Dmitry Vetrov, Andrew Gordon Wilson\\n- PyTorch Examples, https://github.com/pytorch/examples/tree/ee964a2/imagenet, Copyright (c) 2017\\n- Cyclical Stochastic Gradient MCMC for Bayesian Deep Learning, https://github.com/ruqizhang/csgmcmc, Copyright (c) 2019 Ruqi Zhang, Chunyuan Li, Jianyi Zhang, Changyou Chen and Andrew Gordon Wilson\\n- Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs, https://github.com/timgaripov/dnn-mode-connectivity, Copyright (c) 2018  Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry Vetrov and Andrew Gordon Wilson\\n- PyTorch, https://github.com/pytorch/pytorch, Copyright (c) 2016-present, Facebook Inc\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met:\\n\\n1. Redistributions of source code must retain the above copyright notice, this\\n   list of conditions and the following disclaimer.\\n\\n2. Redistributions in binary form must reproduce the above copyright notice,\\n   this list of conditions and the following disclaimer in the documentation\\n   and/or other materials provided with the distribution.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Poster video (5 mins)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "pytorch-ensembles",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "SamsungLabs",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/SamsungLabs/pytorch-ensembles/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 161,
      "date": "Fri, 24 Dec 2021 06:37:52 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "ensembles",
      "pytorch",
      "iclr2020",
      "uncertainty",
      "deep-learning"
    ],
    "technique": "GitHub API"
  }
}