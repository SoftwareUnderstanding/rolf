{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find this work or code is helpful in your research, please cite:\n```\n@article{gao2019res2net,\n  title={Res2Net: A New Multi-scale Backbone Architecture},\n  author={Gao, Shang-Hua and Cheng, Ming-Ming and Zhao, Kai and Zhang, Xin-Yu and Yang, Ming-Hsuan and Torr, Philip},\n  journal={IEEE TPAMI},\n  year={2020},\n  doi={10.1109/TPAMI.2019.2938758}, \n}\n@misc{massa2018mrcnn,\nauthor = {Massa, Francisco and Girshick, Ross},\ntitle = {{maskrnn-benchmark: Fast, modular reference implementation of Instance Segmentation and Object Detection algorithms in PyTorch}},\nyear = {2018},\nhowpublished = {\\url{https://github.com/facebookresearch/maskrcnn-benchmark}},\nnote = {Accessed: [Insert date here]}\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{massa2018mrcnn,\nauthor = {Massa, Francisco and Girshick, Ross},\ntitle = {{maskrnn-benchmark: Fast, modular reference implementation of Instance Segmentation and Object Detection algorithms in PyTorch}},\nyear = {2018},\nhowpublished = {\\url{https://github.com/facebookresearch/maskrcnn-benchmark}},\nnote = {Accessed: [Insert date here]}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{gao2019res2net,\n  title={Res2Net: A New Multi-scale Backbone Architecture},\n  author={Gao, Shang-Hua and Cheng, Ming-Ming and Zhao, Kai and Zhang, Xin-Yu and Yang, Ming-Hsuan and Torr, Philip},\n  journal={IEEE TPAMI},\n  year={2020},\n  doi={10.1109/TPAMI.2019.2938758}, \n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8090016440670298
      ],
      "excerpt": "class MyDataset(object): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8140608737051792
      ],
      "excerpt": "        #: as you would do normally \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9783595426586629
      ],
      "excerpt": "    boxes = [[0, 0, 10, 10], [10, 20, 50, 50]] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8550101043698384
      ],
      "excerpt": "    labels = torch.tensor([10, 20]) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeOfConduct": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://raw.githubusercontent.com/Res2Net/Res2Net-maskrcnn/master/CODE_OF_CONDUCT.md",
    "technique": "File Exploration"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Res2Net/Res2Net-maskrcnn",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contributing to Mask-RCNN Benchmark\nWe want to make contributing to this project as easy and transparent as\npossible.\nOur Development Process\nMinor changes and improvements will be released on an ongoing basis. Larger changes (e.g., changesets implementing a new paper) will be released on a more periodic basis.\nPull Requests\nWe actively welcome your pull requests.\n\nFork the repo and create your branch from master.\nIf you've added code that should be tested, add tests.\nIf you've changed APIs, update the documentation.\nEnsure the test suite passes.\nMake sure your code lints.\nIf you haven't already, complete the Contributor License Agreement (\"CLA\").\n\nContributor License Agreement (\"CLA\")\nIn order to accept your pull request, we need you to submit a CLA. You only need\nto do this once to work on any of Facebook's open source projects.\nComplete your CLA here: https://code.facebook.com/cla\nIssues\nWe use GitHub issues to track public bugs. Please ensure your description is\nclear and has sufficient instructions to be able to reproduce the issue.\nFacebook has a bounty program for the safe\ndisclosure of security bugs. In those cases, please go through the process\noutlined on that page and do not file a public issue.\nCoding Style\n\n4 spaces for indentation rather than tabs\n80 character line length\nPEP8 formatting following Black\n\nLicense\nBy contributing to Mask-RCNN Benchmark, you agree that your contributions will be licensed\nunder the LICENSE file in the root directory of this source tree.",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-08-10T14:36:10Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-09-06T04:16:29Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This repo uses *MaskRCNN* as the baseline method for Instance segmentation and Object detection. We use the [maskrcnn-benchmark](https://github.com/facebookresearch/maskrcnn-benchmark) as the baseline. \n\n[Res2Net](https://github.com/gasvn/Res2Net) is a powerful backbone architecture that can be easily implemented into state-of-the-art models by replacing the bottleneck with Res2Net module.\nMore detail can be found on [ \"Res2Net: A New Multi-scale Backbone Architecture\"](https://arxiv.org/pdf/1904.01169.pdf) and our [project page](https://mmcheng.net/res2net) .\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9514053503451713
      ],
      "excerpt": "2020.3.10 The mmdetection based implementation of object detection and instance segmentation using Res2Net_v1b has the SOTA performance. We have released our code on: https://github.com/Res2Net/mmdetection. Our Res2Net_v1b achieves a considerable performance gain on mmdetection compared with existing backbone models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8679584484468544,
        0.8008429079547844
      ],
      "excerpt": "For that, all you need to do is to modify maskrcnn_benchmark/config/paths_catalog.py to \npoint to the location where your dataset is stored. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9290588165016882
      ],
      "excerpt": "Most of the configuration files that we provide assume that we are running on 8 GPUs. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9068075750396347,
        0.9515987478979276,
        0.9150702841864603
      ],
      "excerpt": "This should work out of the box and is very similar to what we should do for multi-GPU training. \nBut the drawback is that it will use much more GPU memory. The reason is that we set in the \nconfiguration files a global batch size that is divided over the number of GPUs. So if we only \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.912799824094703
      ],
      "excerpt": "to out-of-memory errors. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9846648517808161
      ],
      "excerpt": "Here is an example for Mask R-CNN Res2Net-50 FPN with the 2x schedule: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9356291442486512
      ],
      "excerpt": "We also changed the batch size during testing, but that is generally not necessary because testing \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8774325842748769
      ],
      "excerpt": "    #: add the labels to the boxlist \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8593993976119897
      ],
      "excerpt": "    #: return the image, the boxlist and the idx in your dataset \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9165259544213658,
        0.9385084359224708
      ],
      "excerpt": "    #: we want to split the batches according to the aspect ratio \n    #: of the image, as it can be more efficient than loading the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9602131827359933
      ],
      "excerpt": "For a full example of how the COCODataset is implemented, check maskrcnn_benchmark/data/datasets/coco.py. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Res2Net for Instance segmentation and Object detection using MaskRCNN",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Res2Net/Res2Net-maskrcnn/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 17,
      "date": "Mon, 27 Dec 2021 08:37:43 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Res2Net/Res2Net-maskrcnn/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Res2Net/Res2Net-maskrcnn",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Res2Net/Res2Net-maskrcnn/master/docker/Dockerfile",
      "https://raw.githubusercontent.com/Res2Net/Res2Net-maskrcnn/master/docker/docker-jupyter/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "(**This repo is based on the [mask-rcnn benchmark]((https://github.com/facebookresearch/maskrcnn-benchmark))**, the useage is remain the same with the original repo.)\n\nCheck [INSTALL.md](INSTALL.md) for installation instructions.\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9241342904531881,
        0.957524036534409,
        0.8385218634777329
      ],
      "excerpt": "For the following examples to work, you need to first install maskrcnn_benchmark. \nYou will also need to download the COCO dataset. \nWe recommend to symlink the path to the coco dataset to datasets/ as follows \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8453586014693425,
        0.8957597120131504,
        0.9134125685853338
      ],
      "excerpt": ": symlink the coco dataset \ncd ~/github/maskrcnn-benchmark \nmkdir -p datasets/coco \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9253569476729215
      ],
      "excerpt": ": or use COCO 2017 version \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8569965685513768
      ],
      "excerpt": "You can also configure your own paths to the datasets. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8684410238522189
      ],
      "excerpt": "1. Run the following without modifications \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.806186973744316
      ],
      "excerpt": "But the drawback is that it will use much more GPU memory. The reason is that we set in the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8106025703118147
      ],
      "excerpt": "have a single GPU, this means that the batch size for that GPU will be 8x larger, which might lead \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8988821909411175
      ],
      "excerpt": "python /path_to_maskrcnn_benchmark/tools/train_net.py --config-file \"/path/to/config/file.yaml\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.811147856781077
      ],
      "excerpt": "python tools/train_net.py --config-file \"configs/pytorch_mask_rcnn_R2_50_s4_FPN_2x.yaml\" SOLVER.IMS_PER_BATCH 2 SOLVER.BASE_LR 0.0025 SOLVER.MAX_ITER 720000 SOLVER.STEPS \"(480000, 640000)\" TEST.IMS_PER_BATCH 1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8370783134214607
      ],
      "excerpt": "python -m torch.distributed.launch --nproc_per_node=$NGPUS /path_to_maskrcnn_benchmark/tools/train_net.py --config-file \"configs/pytorch_mask_rcnn_R2_50_s4_FPN_2x.yaml\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8798195837401794,
        0.8801854956928516
      ],
      "excerpt": "from maskrcnn_benchmark.config import cfg \nfrom predictor import COCODemo \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8023167441917616
      ],
      "excerpt": ": update the config options with the config file \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8066881629701139
      ],
      "excerpt": "For a full example of how the COCODataset is implemented, check maskrcnn_benchmark/data/datasets/coco.py. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Res2Net/Res2Net-maskrcnn/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Cuda",
      "C++",
      "Dockerfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2018 Facebook\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Res2Net for Instance segmentation and Object detection using MaskRCNN",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Res2Net-maskrcnn",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Res2Net",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Res2Net/Res2Net-maskrcnn/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 52,
      "date": "Mon, 27 Dec 2021 08:37:43 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "mask-rcnn",
      "object-detection",
      "instance-segmentation",
      "res2net"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "**Since the MaskRCNN-benchmark of facebook is deprecated, we suggest to use our mmdetection based res2net for object detection and instance segmentation to get the SOTA performance on both two tasks.** https://github.com/Res2Net/mmdetection \n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "**Performance on Instance segmentation:**\n\n| Backbone     | Setting      | AP      | AP50    | AP75     | APs       |APm        |    APl    |\n|--------------|--------------|---------|---------|----------|-----------|-----------|-----------|\n| ResNet-50    |  64w         | 33.9    | 55.2    | 36.0     | 14.8      | 36.0      | 50.9      |\n| ResNet-50    |  48w\u00d72s      | 34.2    | 55.6    | 36.3     | 14.9      | 36.8      | 50.9      |\n| Res2Net-50   |  26w\u00d74s      | 35.6    | 57.6    | 37.6     | 15.7      | 37.9      | 53.7      |\n| Res2Net-50   |  18w\u00d76s      | 35.7    | 57.5    | 38.1     | 15.4      | 38.1      | 53.7      |\n| Res2Net-50   |  14w\u00d78s      | 35.3    | 57.0    | 37.5     | 15.6      | 37.5      | 53.4      |\n| ResNet-101   |  64w         | 35.5    | 57.0    | 37.9     | 16.0      | 38.2      | 52.9      |\n| Res2Net-101  |  26w\u00d74s      | 37.1    | 59.4    | 39.4     | 16.6      | 40.0      | 55.6      |\n\n**Performance on Object detection:**\n\n| Backbone     | Setting      | AP      | AP50    | AP75     | APs       |APm        |    APl    |\n|--------------|--------------|---------|---------|----------|-----------|-----------|-----------|\n| ResNet-50    |  64w         | 37.5    | 58.4    | 40.3     | 20.6      | 40.1      | 49.7      |\n| ResNet-50    |  48w\u00d72s      | 38.0    | 58.9    | 41.3     | 20.5      | 41.0      | 49.9      |\n| Res2Net-50   |  26w\u00d74s      | 39.6    | 60.9    | 43.1     | 22.0      | 42.3      | 52.8      |\n| Res2Net-50   |  18w\u00d76s      | 39.9    | 60.9    | 43.3     | 21.8      | 42.8      | 53.7      |\n| Res2Net-50   |  14w\u00d78s      | 39.1    | 60.2    | 42.1     | 21.7      | 41.7      | 52.8      |\n| ResNet-101   |  64w         | 39.6    | 60.6    | 43.2     | 22.0      | 43.2      | 52.4      |\n| Res2Net-101  |  26w\u00d74s      | 41.8    | 62.6    | 45.6     | 23.4      | 45.5      | 55.6      |\n\n\n(Noted that pretrained models trained with pytorch usually achieve slightly worse performance than the caffe pretrained models, we took [advice](https://github.com/facebookresearch/maskrcnn-benchmark/issues/504) from the author of MaskRCNN-benchmark to use 2x schedule in all experiments including baseline and our method.)\n\n",
      "technique": "Header extraction"
    }
  ]
}