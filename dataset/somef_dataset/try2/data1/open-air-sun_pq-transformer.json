{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We want to thank codebase of [VoteNet](https://github.com/facebookresearch/votenet) and [Group-Free](https://github.com/zeliu98/Group-Free-3D). And also thanks to dataset [ScanNet](https://github.com/ScanNet/ScanNet) and layout annotation from [SceneCAD](https://github.com/skanti/SceneCAD).\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2109.05566",
      "https://arxiv.org/abs/2109.05566"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find our work useful in your research, please consider citing:\n\n    @article{chen2021pq,\n    title={PQ-Transformer: Jointly Parsing 3D Objects and Layouts from Point Clouds},\n    author={Chen, Xiaoxue and Zhao, Hao and Zhou, Guyue and Zhang, Ya-Qin},\n    journal={arXiv preprint arXiv:2109.05566},\n    year={2021}\n    }\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{chen2021pq,\ntitle={PQ-Transformer: Jointly Parsing 3D Objects and Layouts from Point Clouds},\nauthor={Chen, Xiaoxue and Zhao, Hao and Zhou, Guyue and Zhang, Ya-Qin},\njournal={arXiv preprint arXiv:2109.05566},\nyear={2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9996351845260121
      ],
      "excerpt": "Created by Xiaoxue Chen, Hao Zhao, Guyue Zhou and Ya-Qin Zhang from <a href=\"http://air.tsinghua.edu.cn/EN/\" target=\"_blank\">Institute for AI Industry Research(AIR), Tsinghua University</a>. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/OPEN-AIR-SUN/PQ-Transformer",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-09-12T08:11:06Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-19T05:36:29Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "3D scene understanding from point clouds plays a vital role for various robotic applications. Unfortunately, current state-of-the-art methods use separate neural networks for different tasks like object detection or room layout estimation. Such a scheme has two limitations: (1) Storing and running several networks for different tasks are expensive for typical robotic platforms. (2) The intrinsic structure of separate outputs are ignored and potentially violated. \n\nIn this repository, we propose the first transformer architecture that predicts 3D objects and layouts simultaneously, using point cloud inputs. Unlike existing methods that either estimate layout keypoints or edges, we directly parameterize room layout as a set of quads. As such, the proposed architecture is termed as P(oint)Q(uad)-Transformer. Along with the novel quad representation, we propose a tailored physical constraint loss function that discourages object-layout interference. The quantitative and qualitative evaluations on the public benchmark ScanNet show that the proposed PQ-Transformer succeeds to jointly parse 3D objects and layouts, running at a quasi-real-time (8.91 FPS) rate without efficiency-oriented optimization. Moreover, the new physical constraint loss can improve strong baselines, and the F1-score of the room layout is significantly promoted from 37.9\\% to 57.9\\%. Code and models will be made publicly available.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8137573902589125
      ],
      "excerpt": "To train a PQ-Transformer model on Scannet with a single GPU: \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/open-air-sun/pq-transformer/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Thu, 30 Dec 2021 09:12:07 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/OPEN-AIR-SUN/PQ-Transformer/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "OPEN-AIR-SUN/PQ-Transformer",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "For 3D detection on ScanNet, follow the [README](https://github.com/facebookresearch/votenet/blob/master/scannet/README.md) under the `scannet` folder.\n\nFor layout estimation on ScanNet, download the sceneCAD layout dataset from \n[HERE](http://kaldir.vc.in.tum.de/scannet_planes).  Unzip it into `/path/to/project/scannet/`.\n\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8306244746908337
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES=0 python -m torch.distributed.launch --nproc_per_node 1 train.py  --log_dir log/[log_dir] --pc_loss \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.885998867348182
      ],
      "excerpt": "To test the trained model with its checkpoint: \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/OPEN-AIR-SUN/PQ-Transformer/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Cuda",
      "C++",
      "C"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "PQ-Transformer: Jointly Parsing 3D Objects and Layouts from Point Clouds",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "PQ-Transformer",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "OPEN-AIR-SUN",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/OPEN-AIR-SUN/PQ-Transformer/blob/main/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "    \n    python =3.6\n    CUDA>=10.1\n    Pytorch>=1.3\n    matplotlib\n    opencv-python\n    tensorboard\n    termcolor\n    plyfile\n    trimesh>=2.35.39\n    networkx>=2.2\n    scripy\n    \n\n\nCompile the CUDA layers for [PointNet++](http://arxiv.org/abs/1706.02413):\n\n    cd pointnet2\n    python setup.py install\n\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You can download pre-trained model [HERE](https://drive.google.com/file/d/1yawlsprl-bhRotpZS29inQo4f4ZSZSY-/view?usp=sharing).\nMove the file to the project root path (`/path/to/project/pretrained_model`) and then run:\n\n    CUDA_VISIBLE_DEVICES=0 python -m torch.distributed.launch --nproc_per_node 1 eval.py --checkpoint_path /path/to/project/pretrained_model/ckpt_epoch_last.pth\n\nThe demo uses the pre-trained model to jointly detect objects and layouts from point cloud of an indoor scene. You can get dump visualized results with:\n\n    CUDA_VISIBLE_DEVICES=0 python -m torch.distributed.launch --nproc_per_node 1 eval.py --checkpoint_path /path/to/project/pretrained_model/ckpt_epoch_last.pth --dump\n\nAnd then you can use 3D visualization software such as the [MeshLab](http://www.meshlab.net/) to open the `.ply` files under `/path/to/project/demo/` to see the 3D detection output.\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 37,
      "date": "Thu, 30 Dec 2021 09:12:07 GMT"
    },
    "technique": "GitHub API"
  }
}