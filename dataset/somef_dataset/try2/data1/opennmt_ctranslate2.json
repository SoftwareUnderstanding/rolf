{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1912.03393",
      "https://arxiv.org/abs/1706.03762",
      "https://arxiv.org/abs/1803.02155"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8460369606795559
      ],
      "excerpt": "biasing translations towards a given prefix (see section 4.2 in Arivazhagan et al. 2020) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "tar xf wmt16.en-de.joined-dict.transformer.tar.bz2 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "    --data_dir wmt16.en-de.joined-dict.transformer \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9916116566211683,
        0.9861586135483766
      ],
      "excerpt": "| Transformer (Vaswani et al. 2017) | \u2713 | \u2713 | \u2713 | \n| + relative position representations (Shaw et al. 2018) | \u2713 | \u2713 | | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9222383658450612
      ],
      "excerpt": "Intel MKL (>=2019.5) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "cuBLAS (>=10.0) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/OpenNMT/CTranslate2",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-09-23T08:10:42Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-21T14:02:23Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9823895066718726
      ],
      "excerpt": "CTranslate2 is a fast and full-featured inference engine for Transformer models. It aims to provide comprehensive inference features and be the most efficient and cost-effective solution to deploy standard neural machine translation systems on CPU and GPU. It currently supports Transformer models trained with: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9984128153482572,
        0.9968029537584643,
        0.8930901044020226
      ],
      "excerpt": "The project is production-oriented and comes with backward compatibility guarantees, but it also includes experimental features related to model compression and inference acceleration. \nTable of contents \nKey features \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9420127521066314
      ],
      "excerpt": "Fast and efficient execution on CPU and GPU<br/>The execution is significantly faster and requires less resources than general-purpose deep learning frameworks on supported models and tasks thanks to many advanced optimizations: padding removal, batch reordering, in-place operations, caching mechanism, etc. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9650267890628611,
        0.8423736456177218
      ],
      "excerpt": "Multiple CPU architectures support<br/>The project supports x86-64 and ARM64 processors and integrates multiple backends that are optimized for these platforms: Intel MKL, oneDNN, OpenBLAS, Ruy, and Apple Accelerate. \nAutomatic CPU detection and code dispatch<br/>One binary can include multiple backends (e.g. Intel MKL and oneDNN) and instruction set architectures (e.g. AVX, AVX2) that are automatically selected at runtime based on the CPU information. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8412763686075126,
        0.9965392158010633
      ],
      "excerpt": "Interactive decoding<br/>Advanced decoding features allow autocompleting a partial translation and returning alternatives at a specific location in the translation. \nSome of these features are difficult to achieve with standard deep learning frameworks and are the motivation for this project. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9377749503604016
      ],
      "excerpt": "decoding with greedy or beam search \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8592262946067487
      ],
      "excerpt": ": The Fairseq model uses a BPE tokenization: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9442292147318618
      ],
      "excerpt": "The opennmt/ctranslate2 repository contains images with prebuilt libraries and clients: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9814969248643537,
        0.8854163323315183
      ],
      "excerpt": "The core CTranslate2 implementation is framework agnostic. The framework specific logic is moved to a conversion step that serializes trained models into a simple binary format. \nThe following frameworks and models are currently supported: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8027319073300655
      ],
      "excerpt": "If you are using a model that is not listed above, consider opening an issue to discuss future integration. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9684135601696366,
        0.8008933540098573,
        0.8565346934172458
      ],
      "excerpt": "The converters support reducing the weights precision to save on space and possibly accelerate the model execution. See the Quantization documentation. \nEach converter should populate a model specification with trained weights coming from an existing model. The model specification declares the variable names and layout expected by the CTranslate2 core engine. \nSee the existing converters implementation which could be used as a template. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8771720181605331
      ],
      "excerpt": "See the Python reference for more advanced usages. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9263029775306932
      ],
      "excerpt": "See the TranslatorPool class for more advanced usages such as asynchronous translations. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8611555171947998
      ],
      "excerpt": "CT2_USE_EXPERIMENTAL_PACKED_GEMM: Enable the packed GEMM API for Intel MKL (see Performance). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8230656896846756
      ],
      "excerpt": "The project uses CMake for compilation. The following options can be set with -DOPTION=VALUE: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8687375555827912
      ],
      "excerpt": "| ENABLE_CPU_DISPATCH | OFF, ON | Compiles CPU kernels for multiple ISA and dispatches at runtime (should be disabled when explicitly targeting an architecture with the -march compilation flag) | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.903242057809646,
        0.903242057809646,
        0.903242057809646,
        0.903242057809646,
        0.903242057809646
      ],
      "excerpt": "| WITH_DNNL | OFF, ON | Compiles with the oneDNN backend (a.k.a. DNNL) | \n| WITH_MKL | OFF, ON | Compiles with the Intel MKL backend | \n| WITH_ACCELERATE | OFF, ON | Compiles with the Apple Accelerate backend | \n| WITH_OPENBLAS | OFF, ON | Compiles with the OpenBLAS backend | \n| WITH_RUY | OFF, ON | Compiles with the Ruy backend | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8022020173261334
      ],
      "excerpt": "Under the project root, run the following commands: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8372396012058242
      ],
      "excerpt": "cmake -DWITH_MKL=ON -DWITH_CUDA=ON .. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9043419798792195
      ],
      "excerpt": "These steps should produce the cli/translate binary. You can try it with the model converted in the Quickstart section: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9158907544875726
      ],
      "excerpt": "To enable the tests, you should configure the project with cmake -DBUILD_TESTS=ON. The binary tests/ctranslate2_test runs all tests using Google Test. It expects the path to the test data as argument: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9674659504915205,
        0.9603932838045846,
        0.9494019532426727
      ],
      "excerpt": "For a fair comparison, we restrict the benchmark to toolkits compatible with the pretrained English-German Transformer model from OpenNMT-py or OpenNMT-tf. \nWe translate the test set newstest2014 and report the number of target tokens generated per second. The results are aggregated over multiple runs (see the benchmark scripts for more details). \nPlease note that the results presented below are only valid for the configuration used during this benchmark: absolute and relative performance may change with different settings. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.867917464175435
      ],
      "excerpt": "| OpenNMT-tf 2.22.0 (with TensorFlow 2.6.0) | 356.2 | 2641MB | 26.93 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9093208089305351
      ],
      "excerpt": "Executed with 8 threads on a c5.metal Amazon EC2 instance equipped with an Intel(R) Xeon(R) Platinum 8275CL CPU. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.867917464175435
      ],
      "excerpt": "| OpenNMT-tf 2.22.0 (with TensorFlow 2.6.0) | 1361.4 | 2660MB | 1737MB | 26.93 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9904167626262936
      ],
      "excerpt": "The table below compares the model size on disk of the pretrained Transformer models which are \"base\" Transformers without shared embeddings and a vocabulary of size 32k: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9594146002198297,
        0.9945475989008689,
        0.890736752485647,
        0.8615722052871577
      ],
      "excerpt": "How does it relate to the original CTranslate project? \nWhat is the state of this project? \nWhy and when should I use this implementation instead of PyTorch or TensorFlow? \nWhat hardware is supported? \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8894733404899787
      ],
      "excerpt": "What is the difference between intra_threads and inter_threads? \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9867077292907308,
        0.8768142403160781
      ],
      "excerpt": "The original CTranslate project shares a similar goal which is to provide a custom execution engine for OpenNMT models that is lightweight and fast. However, it has some limitations that were hard to overcome: \na strong dependency on LuaTorch and OpenNMT-lua, which are now both deprecated in favor of other toolkits; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9758014101425128,
        0.8490242820238132,
        0.8285225640886356
      ],
      "excerpt": "the core implementation is framework agnostic, moving the framework specific logic to a model conversion step; \nthe call to external libraries (Intel MKL, cuBLAS, etc.) occurs as late as possible in the execution to not rely on a library specific logic. \nThe implementation has been generously tested in production environment so people can rely on it in their application. The project versioning follows Semantic Versioning 2.0.0. The following APIs are covered by backward compatibility guarantees: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9596567924369389
      ],
      "excerpt": "Other APIs are expected to evolve to increase efficiency, genericity, and model support. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9802420383880516,
        0.9773404217201209
      ],
      "excerpt": "CTranslate2 supports x86-64 and ARM64 processors. It includes optimizations for AVX, AVX2, and NEON and supports multiple BLAS backends that should be selected based on the target platform (see Building). \nPrebuilt binaries are designed to run on any x86-64 processors supporting at least SSE 4.2. The binaries implement runtime dispatch to select the best backend and instruction set architecture (ISA) for the platform. In particular, they are compiled with both Intel MKL and oneDNN so that Intel MKL is only used on Intel processors where it performs best, whereas oneDNN is used on other x86-64 processors such as AMD. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.874968409681376
      ],
      "excerpt": "CTranslate2 supports NVIDIA GPUs with a Compute Capability greater or equal to 3.5. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9474444465818388,
        0.9933715975868024,
        0.966465987207744,
        0.9406396820769507
      ],
      "excerpt": "The current approach only exports the weights from existing models and redefines the computation graph via the code. This implies a strong assumption of the graph architecture executed by the original framework. \nThere are many ways to make this project better and even faster. See the open issues for an overview of current and planned features. \nintra_threads is the number of OpenMP threads that is used per translation: increase this value to decrease the latency of CPU translations. \ninter_threads is the maximum number of translations executed in parallel: increase this value to increase the throughput. Even though the model data are shared, this execution mode will increase the memory usage as some internal buffers are duplicated for thread safety. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9886600527509484,
        0.9559336919589507
      ],
      "excerpt": "The OpenNMT-py REST server is able to serve CTranslate2 models. See the code integration to learn more. \nThe vocabulary mapping file (a.k.a. vmap) maps source N-grams to a list of target tokens. During translation, the target vocabulary will be dynamically reduced to the union of all target tokens associated with the N-grams from the batch to translate. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Fast inference engine for Transformer models",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/opennmt/ctranslate2/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 61,
      "date": "Tue, 28 Dec 2021 00:35:45 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/OpenNMT/CTranslate2/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "OpenNMT/CTranslate2",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/opennmt/ctranslate2/master/docker/Dockerfile",
      "https://raw.githubusercontent.com/opennmt/ctranslate2/master/tools/benchmark/pretrained_transformer_base/opennmt_py/Dockerfile",
      "https://raw.githubusercontent.com/opennmt/ctranslate2/master/tools/benchmark/pretrained_transformer_base/fastertransformer/Dockerfile",
      "https://raw.githubusercontent.com/opennmt/ctranslate2/master/tools/benchmark/pretrained_transformer_base/opennmt_tf/Dockerfile",
      "https://raw.githubusercontent.com/opennmt/ctranslate2/master/tools/benchmark/pretrained_transformer_base/ctranslate2/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/opennmt/ctranslate2/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/opennmt/ctranslate2/master/python/tools/prepare_build_environment_linux.sh",
      "https://raw.githubusercontent.com/opennmt/ctranslate2/master/python/tools/prepare_build_environment_windows.sh",
      "https://raw.githubusercontent.com/opennmt/ctranslate2/master/python/tools/prepare_test_environment.sh",
      "https://raw.githubusercontent.com/opennmt/ctranslate2/master/python/tools/prepare_build_environment_macos.sh",
      "https://raw.githubusercontent.com/opennmt/ctranslate2/master/docker/build_all.sh",
      "https://raw.githubusercontent.com/opennmt/ctranslate2/master/examples/wngt2020/run.sh",
      "https://raw.githubusercontent.com/opennmt/ctranslate2/master/tools/benchmark/pretrained_transformer_base/opennmt_py/tokenize.sh",
      "https://raw.githubusercontent.com/opennmt/ctranslate2/master/tools/benchmark/pretrained_transformer_base/opennmt_py/detokenize.sh",
      "https://raw.githubusercontent.com/opennmt/ctranslate2/master/tools/benchmark/pretrained_transformer_base/opennmt_py/translate.sh",
      "https://raw.githubusercontent.com/opennmt/ctranslate2/master/tools/benchmark/pretrained_transformer_base/fastertransformer/tokenize.sh",
      "https://raw.githubusercontent.com/opennmt/ctranslate2/master/tools/benchmark/pretrained_transformer_base/fastertransformer/detokenize.sh",
      "https://raw.githubusercontent.com/opennmt/ctranslate2/master/tools/benchmark/pretrained_transformer_base/fastertransformer/translate.sh",
      "https://raw.githubusercontent.com/opennmt/ctranslate2/master/tools/benchmark/pretrained_transformer_base/opennmt_tf/tokenize.sh",
      "https://raw.githubusercontent.com/opennmt/ctranslate2/master/tools/benchmark/pretrained_transformer_base/opennmt_tf/detokenize.sh",
      "https://raw.githubusercontent.com/opennmt/ctranslate2/master/tools/benchmark/pretrained_transformer_base/opennmt_tf/translate.sh",
      "https://raw.githubusercontent.com/opennmt/ctranslate2/master/tools/benchmark/pretrained_transformer_base/ctranslate2/tokenize.sh",
      "https://raw.githubusercontent.com/opennmt/ctranslate2/master/tools/benchmark/pretrained_transformer_base/ctranslate2/detokenize.sh",
      "https://raw.githubusercontent.com/opennmt/ctranslate2/master/tools/benchmark/pretrained_transformer_base/ctranslate2/translate.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "See the [NVIDIA documentation](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html) for information on how to download and install CUDA.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Use the following instructions to install Intel MKL:\n\n```bash\nwget https://apt.repos.intel.com/intel-gpg-keys/GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB\nsudo apt-key add GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB\nsudo sh -c 'echo \"deb https://apt.repos.intel.com/oneapi all main\" > /etc/apt/sources.list.d/oneAPI.list'\nsudo apt-get update\nsudo apt-get install intel-oneapi-mkl-devel\n```\n\nSee the [Intel MKL documentation](https://software.intel.com/content/www/us/en/develop/tools/math-kernel-library.html) for other installation methods.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "See [Building](#building).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8034276758812499
      ],
      "excerpt": "Dynamic memory usage<br/>The memory usage changes dynamically depending on the request size while still meeting performance requirements thanks to caching allocators on both CPU and GPU. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9787471577652278,
        0.9969225242499727,
        0.9958776959782978,
        0.999746712887969
      ],
      "excerpt": "The steps below assume a Linux OS and a Python installation (3.6 or above). \n1. Install the Python package: \npip install --upgrade pip \npip install ctranslate2 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9809573077276337,
        0.9261694814709758,
        0.8644500257064217
      ],
      "excerpt": "pip install OpenNMT-py \nwget https://s3.amazonaws.com/opennmt-models/transformer-ende-wmt-pyOnmt.tar.gz \ntar xf transformer-ende-wmt-pyOnmt.tar.gz \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9897411650844645,
        0.9261694814709758,
        0.8644500257064217
      ],
      "excerpt": "pip install OpenNMT-tf \nwget https://s3.amazonaws.com/opennmt-models/averaged-ende-ckpt500k-v2.tar.gz \ntar xf averaged-ende-ckpt500k-v2.tar.gz \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.999746712887969,
        0.81750622385449
      ],
      "excerpt": "pip install fairseq \nwget https://dl.fbaipublicfiles.com/fairseq/models/wmt16.en-de.joined-dict.transformer.tar.bz2 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9492616900124807,
        0.999746712887969,
        0.9748742481330029,
        0.9857659135807597,
        0.9393243926056358,
        0.9840573921189139,
        0.9638966433146303
      ],
      "excerpt": "Python packages are published on PyPI: \npip install ctranslate2 \nOS: Linux, macOS, Windows \nPython version: >= 3.6 \npip version: >= 19.3 \n(optional) CUDA version: 11.x \n(optional) GPU driver version: >= 450.80.02 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9638966433146303
      ],
      "excerpt": "(optional) GPU driver version: >= 450.80.02 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.941907916478743
      ],
      "excerpt": "CT2_CUDA_ALLOCATOR: Select the CUDA memory allocator. Possible values are: cub_caching (default), cuda_malloc_async (requires CUDA >= 11.2). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8159157408644503,
        0.8360466735513817
      ],
      "excerpt": "Some build options require external dependencies: \n-DWITH_MKL=ON requires: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8360466735513817
      ],
      "excerpt": "-DWITH_DNNL=ON requires: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8360466735513817
      ],
      "excerpt": "-DWITH_ACCELERATE=ON requires: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8360466735513817
      ],
      "excerpt": "-DWITH_OPENBLAS=ON requires: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8360466735513817
      ],
      "excerpt": "-DWITH_CUDA=ON requires: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8270538936472002,
        0.9935309876697507
      ],
      "excerpt": "git submodule update --init --recursive \nmkdir build &amp;&amp; cd build \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8484747135519968,
        0.8901556167675883
      ],
      "excerpt": "make -j4 \n(If you did not install one of Intel MKL or CUDA, set its corresponding flag to OFF in the CMake command line.) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.934998491688414,
        0.9994982306121674,
        0.9714887101700235,
        0.9943731987556338,
        0.9954427483597357
      ],
      "excerpt": ": Install the CTranslate2 library. \ncd build && make install && cd .. \n: Build and install the Python wheel. \ncd python \npip install -r install_requirements.txt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9384831559202097,
        0.806345614328669,
        0.9902618359758885
      ],
      "excerpt": "pip install dist/*.whl \n: Run the tests with pytest. \npip install -r tests/requirements.txt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8907746002459852
      ],
      "excerpt": "| | Tokens per second | Max. GPU memory | Max. CPU memory | BLEU | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9698607974176625
      ],
      "excerpt": "Executed with CUDA 11 on a g4dn.xlarge Amazon EC2 instance equipped with a NVIDIA T4 GPU (driver version: 460.91.03). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8076005195654562
      ],
      "excerpt": "The implementation has been generously tested in production environment so people can rely on it in their application. The project versioning follows Semantic Versioning 2.0.0. The following APIs are covered by backward compatibility guarantees: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8837680365796365
      ],
      "excerpt": "Python symbols: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9140484309779526
      ],
      "excerpt": "The driver requirement depends on the CUDA version. See the CUDA Compatibility guide for more information. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8468432490890284
      ],
      "excerpt": "On GPU, translations executed in parallel are using separate CUDA streams. Depending on the workload and GPU specifications this may or may not improve the translation throughput. For better parallelism on GPU, consider running the translation on multiple GPUs. See the option device_index that accepts multiple device IDs. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9199725006564894,
        0.9336801098518991
      ],
      "excerpt": "2. Convert a Transformer model trained with OpenNMT-py, OpenNMT-tf, or Fairseq: \na. OpenNMT-py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9280216699538913
      ],
      "excerpt": "ct2-opennmt-py-converter --model_path averaged-10-epoch.pt --output_dir ende_ctranslate2 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123763140827432
      ],
      "excerpt": "b. OpenNMT-tf \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8480281792035447
      ],
      "excerpt": "ct2-opennmt-tf-converter --model_path averaged-ende-ckpt500k-v2 --output_dir ende_ctranslate2 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9133368656218674
      ],
      "excerpt": "import ctranslate2 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8554343276871229
      ],
      "excerpt": ": The OpenNMT-py and OpenNMT-tf models use a SentencePiece tokenization: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9239334276963144
      ],
      "excerpt": "|     | OpenNMT-tf | OpenNMT-py | Fairseq | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9133368656218674
      ],
      "excerpt": "import ctranslate2 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8050564878185018
      ],
      "excerpt": "int main() { \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8185610438213133
      ],
      "excerpt": "for (const auto& token : results[0].output()) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9013591301444691
      ],
      "excerpt": "3 = trace \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8401558704798054
      ],
      "excerpt": "import os \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9133368656218674
      ],
      "excerpt": "import ctranslate2 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8169187160383975
      ],
      "excerpt": "To enable the tests, you should configure the project with cmake -DBUILD_TESTS=ON. The binary tests/ctranslate2_test runs all tests using Google Test. It expects the path to the test data as argument: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8831614508112177
      ],
      "excerpt": "./tests/ctranslate2_test ../tests/data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8712624124811189
      ],
      "excerpt": "python setup.py bdist_wheel \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8694668154914371
      ],
      "excerpt": ": Run the tests with pytest. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9504261931955071
      ],
      "excerpt": "pytest tests/test.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8140332775632015,
        0.8034347476879824
      ],
      "excerpt": "Depending on your build configuration, you might need to set LD_LIBRARY_PATH if missing libraries are reported when running tests/test.py. \nFor a fair comparison, we restrict the benchmark to toolkits compatible with the pretrained English-German Transformer model from OpenNMT-py or OpenNMT-tf. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8331115679195388
      ],
      "excerpt": "| OpenNMT-tf 2.22.0 (with TensorFlow 2.6.0) | 356.2 | 2641MB | 26.93 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8331115679195388
      ],
      "excerpt": "| OpenNMT-tf 2.22.0 (with TensorFlow 2.6.0) | 1361.4 | 2660MB | 1737MB | 26.93 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991,
        0.8123763140827432
      ],
      "excerpt": "| OpenNMT-py | 542MB | \n| OpenNMT-tf | 367MB | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8101417337570508
      ],
      "excerpt": "Python converters options \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8262564875894192
      ],
      "excerpt": "It is a text file where each line has the following format: \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/OpenNMT/CTranslate2/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "C++",
      "Python",
      "Cuda",
      "CMake",
      "Shell",
      "Dockerfile",
      "C"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2018-     SYSTRAN.\\nCopyright (c) 2019-     The OpenNMT Authors.\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "CTranslate2",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "CTranslate2",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "OpenNMT",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/OpenNMT/CTranslate2/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "guillaumekln",
        "body": "## Fixes and improvements\r\n\r\n* Fix stuck execution when loading a model on a second GPU\r\n* Fix numerical error in INT8 quantization on macOS",
        "dateCreated": "2021-12-15T16:41:30Z",
        "datePublished": "2021-12-15T17:36:56Z",
        "html_url": "https://github.com/OpenNMT/CTranslate2/releases/tag/v2.10.1",
        "name": "CTranslate2 2.10.1",
        "tag_name": "v2.10.1",
        "tarball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/tarball/v2.10.1",
        "url": "https://api.github.com/repos/OpenNMT/CTranslate2/releases/55387561",
        "zipball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/zipball/v2.10.1"
      },
      {
        "authorType": "User",
        "author_name": "guillaumekln",
        "body": "## Changes\r\n\r\n* `inter_threads` now also applies to GPU translation, where each translation thread is using a different CUDA stream to allow some parts of the GPU execution to overlap\r\n\r\n## New features\r\n\r\n* Add option `disable_unk` to disable the generation of unknown tokens\r\n* Add function `set_random_seed` to fix the seed in random sampling\r\n* [C++] Add constructors in `Translator` and `TranslatorPool` classes with `ModelReader` parameter\r\n\r\n## Fixes and improvements\r\n\r\n* Fix incorrect output from the Multinomial op when running on GPU with a small batch size\r\n* Fix Thrust and CUB headers that were included from the CUDA installation instead of the submodule\r\n* Fix static library compilation with the default build options (`cmake -DBUILD_SHARED_LIBS=OFF`)\r\n* Compile the Docker image and the Linux Python wheels with SSE 4.1 (vectorized kernels are still compiled for AVX and AVX2 with automatic dispatch, but other source files are now compiled with SSE 4.1)\r\n* Enable `/fp:fast` for MSVC to mirror `-ffast-math` that is enabled for GCC and Clang\r\n* Statically link against oneDNN to reduce the size of published binaries:\r\n  * Linux Python wheels: 43MB -> 17MB\r\n  * Windows Python wheels: 41MB -> 11MB\r\n  * Docker image: 733MB -> 600MB",
        "dateCreated": "2021-12-13T13:07:15Z",
        "datePublished": "2021-12-13T13:55:29Z",
        "html_url": "https://github.com/OpenNMT/CTranslate2/releases/tag/v2.10.0",
        "name": "CTranslate2 2.10.0",
        "tag_name": "v2.10.0",
        "tarball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/tarball/v2.10.0",
        "url": "https://api.github.com/repos/OpenNMT/CTranslate2/releases/55186611",
        "zipball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/zipball/v2.10.0"
      },
      {
        "authorType": "User",
        "author_name": "guillaumekln",
        "body": "## New features\r\n\r\n* Add GPU support to the Windows Python wheels\r\n* Support OpenNMT-py and Fairseq options `--alignment_layer` and `--alignment_heads` which specify how the multi-head attention is reduced and returned by the Transformer decoder\r\n* Support dynamic loading of CUDA libraries on Windows\r\n\r\n## Fixes and improvements\r\n\r\n* Fix division by zero when normalizing the score of an empty target\r\n* Fix error that was not raised when the input length is greater than the number of position encodings\r\n* Improve performance of random sampling on GPU for large values of `sampling_topk` or when sampling over the full vocabulary\r\n* Include `transformer_align` and `transformer_wmt_en_de_big_align` in the list of supported Fairseq architectures\r\n* Add a CUDA kernel to prepare the length mask to avoid moving back to the CPU",
        "dateCreated": "2021-12-01T15:08:13Z",
        "datePublished": "2021-12-01T15:50:31Z",
        "html_url": "https://github.com/OpenNMT/CTranslate2/releases/tag/v2.9.0",
        "name": "CTranslate2 2.9.0",
        "tag_name": "v2.9.0",
        "tarball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/tarball/v2.9.0",
        "url": "https://api.github.com/repos/OpenNMT/CTranslate2/releases/54422294",
        "zipball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/zipball/v2.9.0"
      },
      {
        "authorType": "User",
        "author_name": "guillaumekln",
        "body": "## Fixes and improvements\r\n\r\n* Fix dtype error when reading float16 scores in greedy search\r\n* Fix usage of MSVC linker option `/nodefaultlib` that was not correctly passed to the linker",
        "dateCreated": "2021-11-17T15:52:27Z",
        "datePublished": "2021-11-17T16:26:22Z",
        "html_url": "https://github.com/OpenNMT/CTranslate2/releases/tag/v2.8.1",
        "name": "CTranslate2 2.8.1",
        "tag_name": "v2.8.1",
        "tarball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/tarball/v2.8.1",
        "url": "https://api.github.com/repos/OpenNMT/CTranslate2/releases/53572086",
        "zipball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/zipball/v2.8.1"
      },
      {
        "authorType": "User",
        "author_name": "guillaumekln",
        "body": "## Changes\r\n\r\n* The Linux Python wheels now use Intel OpenMP instead of GNU OpenMP for consistency with other published binaries\r\n\r\n## New features\r\n\r\n* Build Python wheels for Windows\r\n\r\n## Fixes and improvements\r\n\r\n* Fix segmentation fault when calling `Translator.unload_model` while an asynchronous translation is running\r\n* Fix implementation of repetition penalty that should be applied to all previously generated tokens and not just the tokens of the last step\r\n* Fix missing application of repetition penalty in greedy search\r\n* Fix incorrect token index when using a target prefix and a vocabulary mapping file\r\n* Set the OpenMP flag when compiling on Windows with `-DOPENMP_RUNTIME=INTEL` or `-DOPENMP_RUNTIME=COMP`",
        "dateCreated": "2021-11-15T09:27:25Z",
        "datePublished": "2021-11-15T09:55:41Z",
        "html_url": "https://github.com/OpenNMT/CTranslate2/releases/tag/v2.8.0",
        "name": "CTranslate2 2.8.0",
        "tag_name": "v2.8.0",
        "tarball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/tarball/v2.8.0",
        "url": "https://api.github.com/repos/OpenNMT/CTranslate2/releases/53374810",
        "zipball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/zipball/v2.8.0"
      },
      {
        "authorType": "User",
        "author_name": "guillaumekln",
        "body": "## Changes\r\n\r\n* Inputs are now truncated after 1024 tokens by default to limit the maximum memory usage (see translation option `max_input_length`)\r\n\r\n## New features\r\n\r\n* Add translation option `max_input_length` to limit the model input length\r\n* Add translation option `repetition_penalty` to apply an exponential penalty on repeated sequences\r\n* Add scoring option `with_tokens_score` to also output token-level scores when scoring a file\r\n\r\n## Fixes and improvements\r\n\r\n* Adapt the length penalty formula when using `normalize_scores` to match other implementations: the scores are divided by `pow(length, length_penalty)`\r\n* Implement `LayerNorm` with a single CUDA kernel instead of 2\r\n* Simplify the beam search implementation",
        "dateCreated": "2021-11-04T15:27:39Z",
        "datePublished": "2021-11-04T15:59:15Z",
        "html_url": "https://github.com/OpenNMT/CTranslate2/releases/tag/v2.7.0",
        "name": "CTranslate2 2.7.0",
        "tag_name": "v2.7.0",
        "tarball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/tarball/v2.7.0",
        "url": "https://api.github.com/repos/OpenNMT/CTranslate2/releases/52707271",
        "zipball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/zipball/v2.7.0"
      },
      {
        "authorType": "User",
        "author_name": "guillaumekln",
        "body": "## New features\r\n\r\n* Build wheels for Python 3.10\r\n* Accept passing the vocabulary as a `opennmt.data.Vocab` object or a list of tokens in the OpenNMT-tf converter\r\n\r\n## Fixes and improvements\r\n\r\n* Fix segmentation fault in greedy search when `normalize_scores` is enabled but not `return_scores`\r\n* Fix segmentation fault when `min_decoding_length` and `max_decoding_length` are both set to 0\r\n* Fix segmentation fault when `sampling_topk` is larger than the vocabulary size\r\n* Fix incorrect score normalization in greedy search when `max_decoding_length` is reached\r\n* Fix incorrect score normalization in the `return_alternatives` translation mode\r\n* Improve error checking when reading the binary model file\r\n* Apply `LogSoftMax` in-place during decoding and scoring",
        "dateCreated": "2021-10-15T14:02:19Z",
        "datePublished": "2021-10-15T14:27:06Z",
        "html_url": "https://github.com/OpenNMT/CTranslate2/releases/tag/v2.6.0",
        "name": "CTranslate2 2.6.0",
        "tag_name": "v2.6.0",
        "tarball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/tarball/v2.6.0",
        "url": "https://api.github.com/repos/OpenNMT/CTranslate2/releases/51436583",
        "zipball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/zipball/v2.6.0"
      },
      {
        "authorType": "User",
        "author_name": "guillaumekln",
        "body": "## Fixes and improvements\r\n\r\n* Fix logic error in the in-place implementation of the `Gather` op that could lead to incorrect beam search outputs",
        "dateCreated": "2021-10-04T16:11:14Z",
        "datePublished": "2021-10-04T16:34:52Z",
        "html_url": "https://github.com/OpenNMT/CTranslate2/releases/tag/v2.5.1",
        "name": "CTranslate2 2.5.1",
        "tag_name": "v2.5.1",
        "tarball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/tarball/v2.5.1",
        "url": "https://api.github.com/repos/OpenNMT/CTranslate2/releases/50756907",
        "zipball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/zipball/v2.5.1"
      },
      {
        "authorType": "User",
        "author_name": "guillaumekln",
        "body": "## New features\r\n\r\n* Add an 8-bit GEMM backend on AArch64 using [Ruy](https://github.com/google/ruy)\r\n\r\n## Fixes and improvements\r\n\r\n* Skip unnecessary transpositions of the projected decoder queries in the multi-head attention\r\n* Use 32-bit indexing in all CUDA kernels to slightly improve performance\r\n* Let the compiler auto-vectorize the `LayerNorm` CPU kernel\r\n* Update Intel oneAPI to 2021.4",
        "dateCreated": "2021-10-01T11:34:42Z",
        "datePublished": "2021-10-01T12:22:48Z",
        "html_url": "https://github.com/OpenNMT/CTranslate2/releases/tag/v2.5.0",
        "name": "CTranslate2 2.5.0",
        "tag_name": "v2.5.0",
        "tarball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/tarball/v2.5.0",
        "url": "https://api.github.com/repos/OpenNMT/CTranslate2/releases/50629164",
        "zipball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/zipball/v2.5.0"
      },
      {
        "authorType": "User",
        "author_name": "guillaumekln",
        "body": "## New features\r\n\r\n* [Python] Support asynchronous translation: `translate_batch` can return future-like objects with argument `asynchronous=True`\r\n* [Python] `translate_batch` now returns a list of `TranslationResult` objects instead of a list of dictionaries (this object can also be indexed as a list of dictionaries for backward compatibility)\r\n* Add options `--source_lang` and `--target_lang` to the Fairseq converter for models that do not include these information\r\n\r\n## Fixes and improvements\r\n\r\n* Fix Fairseq model conversion when the model options are stored in `model[\"cfg\"][\"model\"]`\r\n* Compile the CPU INT8 quantization kernel with FMA instructions\r\n* Enable packing of the last linear weight when not using dynamic vocabulary reduction\r\n* Replace the generic `Tile` implementation by dedicated CPU and CUDA kernels\r\n* [Python] Implement `__repr__` method for `TranslationStats` objects\r\n* [Python] Update pybind11 to 2.7.1",
        "dateCreated": "2021-09-10T08:53:37Z",
        "datePublished": "2021-09-10T09:13:18Z",
        "html_url": "https://github.com/OpenNMT/CTranslate2/releases/tag/v2.4.0",
        "name": "CTranslate2 2.4.0",
        "tag_name": "v2.4.0",
        "tarball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/tarball/v2.4.0",
        "url": "https://api.github.com/repos/OpenNMT/CTranslate2/releases/49334353",
        "zipball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/zipball/v2.4.0"
      },
      {
        "authorType": "User",
        "author_name": "guillaumekln",
        "body": "## Fixes and improvements\r\n\r\n* Fix GPU execution that gets stuck when applying the GELU activation",
        "dateCreated": "2021-08-05T07:52:25Z",
        "datePublished": "2021-08-05T08:42:07Z",
        "html_url": "https://github.com/OpenNMT/CTranslate2/releases/tag/v2.3.2",
        "name": "CTranslate2 2.3.2",
        "tag_name": "v2.3.2",
        "tarball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/tarball/v2.3.2",
        "url": "https://api.github.com/repos/OpenNMT/CTranslate2/releases/47346088",
        "zipball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/zipball/v2.3.2"
      },
      {
        "authorType": "User",
        "author_name": "guillaumekln",
        "body": "## Fixes and improvements\r\n\r\n* Fix compilation with CUDA < 10.2",
        "dateCreated": "2021-07-28T13:26:29Z",
        "datePublished": "2021-07-28T13:27:29Z",
        "html_url": "https://github.com/OpenNMT/CTranslate2/releases/tag/v2.3.1",
        "name": "CTranslate2 2.3.1",
        "tag_name": "v2.3.1",
        "tarball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/tarball/v2.3.1",
        "url": "https://api.github.com/repos/OpenNMT/CTranslate2/releases/46904918",
        "zipball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/zipball/v2.3.1"
      },
      {
        "authorType": "User",
        "author_name": "guillaumekln",
        "body": "## New features\r\n\r\n* Add compute type `int8_float16` for mixed INT8 and FP16 computation on GPU (requires Compute Capability >= 7.0)\r\n* Add methods `Translator.score_batch` and `Translator.score_file` to score existing translations\r\n\r\n## Fixes and improvements\r\n\r\n* Lower the GPU driver requirement for running the Docker image to >= 450.80.02 (same as the published Python package)",
        "dateCreated": "2021-07-26T10:38:36Z",
        "datePublished": "2021-07-26T12:07:21Z",
        "html_url": "https://github.com/OpenNMT/CTranslate2/releases/tag/v2.3.0",
        "name": "CTranslate2 2.3.0",
        "tag_name": "v2.3.0",
        "tarball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/tarball/v2.3.0",
        "url": "https://api.github.com/repos/OpenNMT/CTranslate2/releases/46764318",
        "zipball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/zipball/v2.3.0"
      },
      {
        "authorType": "User",
        "author_name": "guillaumekln",
        "body": "## New features\r\n\r\n* Add Python utility functions to query the system capabilities:\r\n  * `ctranslate2.get_cuda_device_count`\r\n  * `ctranslate2.get_supported_compute_types`\r\n* Add option `fixed_dictionary` in the Fairseq converter to support multilingual models\r\n* Extend environment variable `CT2_VERBOSE` to configure more log levels (see README)\r\n\r\n## Fixes and improvements\r\n\r\n* Fuse activation with bias addition on GPU for a small performance increase\r\n* Make the GELU activation compatible with FP16 execution\r\n* Improve the log format using the [spdlog](https://github.com/gabime/spdlog) library\r\n* Improve the accuracy of the profiling results on GPU\r\n* Update Intel oneAPI to 2021.3",
        "dateCreated": "2021-07-06T14:40:47Z",
        "datePublished": "2021-07-06T15:06:23Z",
        "html_url": "https://github.com/OpenNMT/CTranslate2/releases/tag/v2.2.0",
        "name": "CTranslate2 2.2.0",
        "tag_name": "v2.2.0",
        "tarball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/tarball/v2.2.0",
        "url": "https://api.github.com/repos/OpenNMT/CTranslate2/releases/45778931",
        "zipball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/zipball/v2.2.0"
      },
      {
        "authorType": "User",
        "author_name": "guillaumekln",
        "body": "## New features\r\n\r\n* Support conversion of Transformer models trained with [Fairseq](https://github.com/pytorch/fairseq/) (see script `ct2-fairseq-converter`)\r\n* Support conversion of models using GELU activations\r\n* Add translation option `normalize_scores` to return scores normalized by the hypotheses length: enabling this option can improve the beam search output for some models\r\n* Add translation option `allow_early_exit` to toggle the beam search early exit optimization: disabling this option has a small negative impact on performance, but it can improve the beam search output when using penalties or normalized scores\r\n* [C++] Add class `BufferedTranslationWrapper` to buffer and batch independent inputs to the same model\r\n\r\n## Fixes and improvements\r\n\r\n* Read value of environment variable `OMP_NUM_THREADS` when `intra_threads` is not set\r\n* Improve file translation performance by enabling local sorting by default\r\n* [Python] Improve error message when converting unsupported models and list all options that are unsupported\r\n* [Python] Return statistics of `Translator.translate_file` as an object with named properties\r\n* [C++] Fix compilation of method `TranslatorPool::consume_raw_text_file` that takes streams as inputs",
        "dateCreated": "2021-06-14T14:39:07Z",
        "datePublished": "2021-06-14T15:01:23Z",
        "html_url": "https://github.com/OpenNMT/CTranslate2/releases/tag/v2.1.0",
        "name": "CTranslate2 2.1.0",
        "tag_name": "v2.1.0",
        "tarball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/tarball/v2.1.0",
        "url": "https://api.github.com/repos/OpenNMT/CTranslate2/releases/44579277",
        "zipball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/zipball/v2.1.0"
      },
      {
        "authorType": "User",
        "author_name": "guillaumekln",
        "body": "This major version introduces some breaking changes to simplify model conversion, improve the consistency of user options, and update the Python package to CUDA 11.x. It also comes with internal improvements to facilitate future changes.\r\n\r\n## Breaking changes\r\n\r\n### General\r\n\r\n* Disable `return_scores` by default as most applications do not use translation scores\r\n* Replace all Docker images by a single one: `<version>-ubuntu20.04-cuda11.2`\r\n* Replace CMake option `LIB_ONLY` by `BUILD_CLI`\r\n* Require CMake version >= 3.15 for GPU compilation\r\n\r\n### Python\r\n\r\n* For GPU execution, the Linux Python wheels published on PyPI now require CUDA 11.x to be installed on the system. The CUDA dependencies (e.g. cuBLAS) are no longer included in the package and are loaded dynamically.\r\n* Remove support for converting the TensorFlow SavedModel format (checkpoints should be converted instead)\r\n* Remove the `model_spec` option for converters that can automatically detect it from the checkpoints\r\n* Force translation options to be set with keyword arguments only (see the API reference)\r\n* Rename tokenization callables arguments in `translate_file` for clarity:\r\n  * `tokenize_fn` to `source_tokenize_fn`\r\n  * `detokenize_fn` to `target_detokenize_fn`\r\n\r\n### CLI\r\n\r\n* Rename length constraints options for consistency with other APIs:\r\n  * `max_sent_length` to `max_decoding_length`\r\n  * `min_sent_length` to `min_decoding_length`\r\n\r\n### C++\r\n\r\n* Move the `max_batch_size` and `batch_type` options from the `TranslationOptions` structure to the translation methods of `TranslatorPool`\r\n* Simplify the `TranslationResult` structure with public attributes instead of methods\r\n* Asynchronous translation API now returns one future per example instead of a single future for the batch\r\n\r\n## New features\r\n\r\n* Add translation option `prefix_bias_beta` to bias the decoding towards the target prefix (see [Arivazhagan et al. 2020](https://arxiv.org/abs/1912.03393))\r\n* Automatically detect the model specification when converting OpenNMT-py models\r\n* Support conversion and execution of Post-Norm Transformers\r\n* Add an experimental asynchronous memory allocator for CUDA 11.2 and above (can be enabled with the environment variable `CT2_CUDA_ALLOCATOR=cuda_malloc_async`)\r\n* Expose the Python package version in `ctranslate2.__version__`\r\n\r\n## Fixes and improvements\r\n\r\n* Fix silent activation of `replace_unknowns` when enabling `return_attention`\r\n* Improve support for the NVIDIA Ampere architecture in prebuilt binaries\r\n* Reduce the size of the Python wheels published on PyPI\r\n* Define a custom CUDA kernel for the GEMM output dequantization instead of a Thrust-based implementation\r\n* Update Thrust to 1.12.0",
        "dateCreated": "2021-06-03T08:37:22Z",
        "datePublished": "2021-06-03T08:56:34Z",
        "html_url": "https://github.com/OpenNMT/CTranslate2/releases/tag/v2.0.0",
        "name": "CTranslate2 2.0.0",
        "tag_name": "v2.0.0",
        "tarball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/tarball/v2.0.0",
        "url": "https://api.github.com/repos/OpenNMT/CTranslate2/releases/44028444",
        "zipball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/zipball/v2.0.0"
      },
      {
        "authorType": "User",
        "author_name": "guillaumekln",
        "body": "## Fixes and improvements\r\n\r\n* Do not return scores for empty outputs when `return_scores` is disabled\r\n* Do not include google/cpu\\_features library in CTranslate2 installation",
        "dateCreated": "2021-04-29T09:04:39Z",
        "datePublished": "2021-04-29T09:43:45Z",
        "html_url": "https://github.com/OpenNMT/CTranslate2/releases/tag/v1.20.1",
        "name": "CTranslate2 1.20.1",
        "tag_name": "v1.20.1",
        "tarball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/tarball/v1.20.1",
        "url": "https://api.github.com/repos/OpenNMT/CTranslate2/releases/42190374",
        "zipball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/zipball/v1.20.1"
      },
      {
        "authorType": "User",
        "author_name": "guillaumekln",
        "body": "## Changes\r\n\r\n* Drop Python 3.5 support\r\n* Docker image tags suffixed with `-gpu` are no longer updated to prefer tags with an explicit CUDA version\r\n\r\n## Fixes and improvements\r\n\r\n* Fix int8 quantization for rows that only contain zeros\r\n* Fix type error when running the CUDA code path of the Multinomial operator\r\n* Add EOS score to the greedy search final score for consistency with the beam search output\r\n* Use third party library [google/cpu\\_features](https://github.com/google/cpu_features) to resolve CPU features at runtime\r\n* Small optimizations when manipulating tensor shapes and indices\r\n* Internal refactoring of Transformer layers",
        "dateCreated": "2021-04-20T12:51:46Z",
        "datePublished": "2021-04-20T13:19:35Z",
        "html_url": "https://github.com/OpenNMT/CTranslate2/releases/tag/v1.20.0",
        "name": "CTranslate2 1.20.0",
        "tag_name": "v1.20.0",
        "tarball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/tarball/v1.20.0",
        "url": "https://api.github.com/repos/OpenNMT/CTranslate2/releases/41712094",
        "zipball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/zipball/v1.20.0"
      },
      {
        "authorType": "User",
        "author_name": "guillaumekln",
        "body": "## Changes\r\n\r\n* Rename CMake option `WITH_TESTS` to `BUILD_TESTS`\r\n\r\n## New features\r\n\r\n* Add \"auto\" compute type to automatically select the fastest compute type on the current system\r\n\r\n## Fixes and improvements\r\n\r\n* [Python] Clear memory allocator cache when calling `unload_model`\r\n* [Python] Make methods `unload_model` and `load_model` thread safe\r\n* Fix conversion of TensorFlow SavedModel with shared embeddings\r\n* Update Intel oneAPI to 2021.2\r\n* Compile core library with C++14 standard",
        "dateCreated": "2021-03-31T12:54:18Z",
        "datePublished": "2021-03-31T13:33:26Z",
        "html_url": "https://github.com/OpenNMT/CTranslate2/releases/tag/v1.19.0",
        "name": "CTranslate2 1.19.0",
        "tag_name": "v1.19.0",
        "tarball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/tarball/v1.19.0",
        "url": "https://api.github.com/repos/OpenNMT/CTranslate2/releases/40759738",
        "zipball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/zipball/v1.19.0"
      },
      {
        "authorType": "User",
        "author_name": "guillaumekln",
        "body": "## Fixes and improvements\r\n\r\n* Use Intel OpenMP instead of GNU OpenMP in the Docker images as a workaround for issue #409",
        "dateCreated": "2021-03-02T08:18:59Z",
        "datePublished": "2021-03-02T08:50:30Z",
        "html_url": "https://github.com/OpenNMT/CTranslate2/releases/tag/v1.18.3",
        "name": "CTranslate2 1.18.3",
        "tag_name": "v1.18.3",
        "tarball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/tarball/v1.18.3",
        "url": "https://api.github.com/repos/OpenNMT/CTranslate2/releases/39102721",
        "zipball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/zipball/v1.18.3"
      },
      {
        "authorType": "User",
        "author_name": "guillaumekln",
        "body": "## Fixes and improvements\r\n\r\n* Fix crash when enabling coverage penalty in GPU translation\r\n* Fix incorrect value of AVX2 flag in `CT2_VERBOSE` output",
        "dateCreated": "2021-02-23T16:55:22Z",
        "datePublished": "2021-02-23T17:38:19Z",
        "html_url": "https://github.com/OpenNMT/CTranslate2/releases/tag/v1.18.2",
        "name": "CTranslate2 1.18.2",
        "tag_name": "v1.18.2",
        "tarball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/tarball/v1.18.2",
        "url": "https://api.github.com/repos/OpenNMT/CTranslate2/releases/38501397",
        "zipball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/zipball/v1.18.2"
      },
      {
        "authorType": "User",
        "author_name": "guillaumekln",
        "body": "## Fixes and improvements\r\n\r\n* Fix conversion of models setting the attributes `with_source_bos` or `with_source_eos`",
        "dateCreated": "2021-02-01T14:31:23Z",
        "datePublished": "2021-02-01T15:09:34Z",
        "html_url": "https://github.com/OpenNMT/CTranslate2/releases/tag/v1.18.1",
        "name": "CTranslate2 1.18.1",
        "tag_name": "v1.18.1",
        "tarball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/tarball/v1.18.1",
        "url": "https://api.github.com/repos/OpenNMT/CTranslate2/releases/37182782",
        "zipball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/zipball/v1.18.1"
      },
      {
        "authorType": "User",
        "author_name": "guillaumekln",
        "body": "## Changes\r\n\r\n* Some options default value in the `translate` client have been changed to match the Python API:\r\n  * `batch_size` = 32 (instead of 30)\r\n  * `beam_size` = 2 (instead of 5)\r\n  * `intra_threads` = 4 (instead of 0)\r\n\r\n## New features\r\n\r\n* Support multi-GPU translation: `device_index` argument can now be set to a list of GPU IDs (see [example](https://github.com/OpenNMT/CTranslate2/blob/master/docs/python.md#note-on-parallel-translations))\r\n\r\n## Fixes and improvements\r\n\r\n* Improve performance when using multiple GPU translators concurrently in the same process\r\n* [Python] Do nothing when calling `unload_model(to_cpu=True)` on CPU translators\r\n* [Python] Set a default value for `max_batch_size` argument in method `Translator.translate_file`\r\n* Disable `CT2_TRANSLATORS_CORE_OFFSET` in OpenMP builds as setting thread affinity does not work when OpenMP is enabled",
        "dateCreated": "2021-01-28T11:03:28Z",
        "datePublished": "2021-01-28T11:38:00Z",
        "html_url": "https://github.com/OpenNMT/CTranslate2/releases/tag/v1.18.0",
        "name": "CTranslate2 1.18.0",
        "tag_name": "v1.18.0",
        "tarball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/tarball/v1.18.0",
        "url": "https://api.github.com/repos/OpenNMT/CTranslate2/releases/37023005",
        "zipball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/zipball/v1.18.0"
      },
      {
        "authorType": "User",
        "author_name": "guillaumekln",
        "body": "## Fixes and improvements\r\n\r\n* Fix Python wheel loading error on macOS",
        "dateCreated": "2021-01-15T13:24:59Z",
        "datePublished": "2021-01-15T13:50:57Z",
        "html_url": "https://github.com/OpenNMT/CTranslate2/releases/tag/v1.17.1",
        "name": "CTranslate2 1.17.1",
        "tag_name": "v1.17.1",
        "tarball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/tarball/v1.17.1",
        "url": "https://api.github.com/repos/OpenNMT/CTranslate2/releases/36449782",
        "zipball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/zipball/v1.17.1"
      },
      {
        "authorType": "User",
        "author_name": "guillaumekln",
        "body": "## Changes\r\n\r\n* Linux Python wheels are now compiled under `manylinux2014` and require `pip` version >= 19.3\r\n\r\n## New features\r\n\r\n* Publish Python wheels for macOS (CPU only)\r\n* Support compilation for ARM 64-bit architecture and add NEON vectorization\r\n* Add new optional GEMM backends: [Apple Accelerate](https://developer.apple.com/documentation/accelerate) and [OpenBLAS](https://www.openblas.net/)\r\n* Add `replace_unknowns` translation option to replace unknown target tokens by source tokens with the highest attention\r\n* Add flags in the model specification to declare that BOS and/or EOS tokens should be added to the source sequences\r\n\r\n## Fixes and improvements\r\n\r\n* Fix segmentation fault when the model is converted with a wrong vocabulary and predicts an out-of-vocabulary index\r\n* Fix result of vectorized array reduction when the array length is not a multiple of the SIMD registers width\r\n* Fix exit code when running `cli/translate -h`\r\n* Improve performance of vectorized vector math by inlining calls to intrinsics functions\r\n* Improve accuracy of LogSoftMax CUDA implementation\r\n* Improve error message when `--model` option is not set in `cli/translate`\r\n* Update oneMKL to 2020.1 in published binaries\r\n* Update oneDNN to 2.0 in published binaries\r\n* Update default search paths to support compilation with oneMKL and oneDNN installed from the oneAPI toolkit",
        "dateCreated": "2021-01-11T16:35:51Z",
        "datePublished": "2021-01-11T17:02:37Z",
        "html_url": "https://github.com/OpenNMT/CTranslate2/releases/tag/v1.17.0",
        "name": "CTranslate2 1.17.0",
        "tag_name": "v1.17.0",
        "tarball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/tarball/v1.17.0",
        "url": "https://api.github.com/repos/OpenNMT/CTranslate2/releases/36237495",
        "zipball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/zipball/v1.17.0"
      },
      {
        "authorType": "User",
        "author_name": "guillaumekln",
        "body": "## Fixes and improvements\r\n\r\n* Fix cuBLAS version included in the Python wheels published to PyPI. The included library was targeting CUDA 10.2 instead of CUDA 10.1.\r\n* Re-add Python 3.5 wheels on PyPI to give users more time to transition",
        "dateCreated": "2020-11-27T10:00:57Z",
        "datePublished": "2020-11-27T10:31:13Z",
        "html_url": "https://github.com/OpenNMT/CTranslate2/releases/tag/v1.16.2",
        "name": "CTranslate2 1.16.2",
        "tag_name": "v1.16.2",
        "tarball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/tarball/v1.16.2",
        "url": "https://api.github.com/repos/OpenNMT/CTranslate2/releases/34497971",
        "zipball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/zipball/v1.16.2"
      },
      {
        "authorType": "User",
        "author_name": "guillaumekln",
        "body": "## Fixes and improvements\r\n\r\n* Fuse dequantization and bias addition on GPU for improved INT8 performance\r\n* Improve performance of masked softmax on GPU\r\n* Fix error when building the CentOS 7 GPU Docker image\r\n* The previous version listed \"Pad size of INT8 matrices to a multiple of 16 when the GPU has INT8 Tensor Cores\". However, the padding was not applied due to a bug and fixing it degraded the performance, so this behavior is not implemented for now.",
        "dateCreated": "2020-11-23T11:14:41Z",
        "datePublished": "2020-11-23T11:52:58Z",
        "html_url": "https://github.com/OpenNMT/CTranslate2/releases/tag/v1.16.1",
        "name": "CTranslate2 1.16.1",
        "tag_name": "v1.16.1",
        "tarball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/tarball/v1.16.1",
        "url": "https://api.github.com/repos/OpenNMT/CTranslate2/releases/34305773",
        "zipball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/zipball/v1.16.1"
      },
      {
        "authorType": "User",
        "author_name": "guillaumekln",
        "body": "## Changes\r\n\r\n* Drop support for Python 2.7 and 3.5\r\n\r\n## New features\r\n\r\n* Add Docker images using CUDA 11.0\r\n\r\n## Fixes and improvements\r\n\r\n* [Python] Enable parallel CPU translations from `translate_batch` when setting `inter_threads` > 1 and `max_batch_size` > 0\r\n* Improve GPU performance on Turing architecture when using a Docker image or the Python package\r\n* Pad size of INT8 matrices to a multiple of 16 when the GPU has INT8 Tensor Cores\r\n* Add information about detected GPU devices in `CT2_VERBOSE` output\r\n* Update oneDNN to 1.7\r\n* [Python] Improve type checking for some arguments",
        "dateCreated": "2020-11-18T13:50:32Z",
        "datePublished": "2020-11-18T16:13:31Z",
        "html_url": "https://github.com/OpenNMT/CTranslate2/releases/tag/v1.16.0",
        "name": "CTranslate2 1.16.0",
        "tag_name": "v1.16.0",
        "tarball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/tarball/v1.16.0",
        "url": "https://api.github.com/repos/OpenNMT/CTranslate2/releases/34126409",
        "zipball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/zipball/v1.16.0"
      },
      {
        "authorType": "User",
        "author_name": "guillaumekln",
        "body": "## New features\r\n\r\n* [Experimental] The Python package published on PyPI now includes GPU support. The binary is compiled with CUDA 10.1, but all CUDA dependencies are integrated in the package and do not need to be installed on the system. The only requirement should be a working GPU with driver version >= 418.39.\r\n\r\n## Fixes and improvements\r\n\r\n* Remove the TensorRT dependency to simplify installation and reduce memory usage:\r\n  * Reduce GPU Docker images size by 600MB\r\n  * Reduce memory usage on the GPU and the system by up 1GB\r\n  * Reduce initialization time during the first GPU translation\r\n* Improve TopK performance on GPU for K < 5\r\n* Improve INT8 performance on GPU\r\n* Accept linear layers without bias when converting models\r\n* Update Intel MKL to 2020.4\r\n* [Python] Improve compatibility with Python 3.9",
        "dateCreated": "2020-11-06T14:02:14Z",
        "datePublished": "2020-11-06T15:15:46Z",
        "html_url": "https://github.com/OpenNMT/CTranslate2/releases/tag/v1.15.0",
        "name": "CTranslate2 1.15.0",
        "tag_name": "v1.15.0",
        "tarball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/tarball/v1.15.0",
        "url": "https://api.github.com/repos/OpenNMT/CTranslate2/releases/33569189",
        "zipball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/zipball/v1.15.0"
      },
      {
        "authorType": "User",
        "author_name": "guillaumekln",
        "body": "## New features\r\n\r\n* Accept target prefix in file translation APIs\r\n\r\n## Fixes and improvements\r\n\r\n* Fix CUDA illegal memory access when changing the beam size in the same process\r\n* Fix decoding with target prefix that sometimes did not go beyond the prefix\r\n* Fix Intel MKL search paths on macOS\r\n* Update Intel MKL to 2020.3\r\n* Clarify error message when selecting a CUDA device in CPU-only builds",
        "dateCreated": "2020-10-13T13:38:59Z",
        "datePublished": "2020-10-14T08:26:05Z",
        "html_url": "https://github.com/OpenNMT/CTranslate2/releases/tag/v1.14.0",
        "name": "CTranslate2 1.14.0",
        "tag_name": "v1.14.0",
        "tarball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/tarball/v1.14.0",
        "url": "https://api.github.com/repos/OpenNMT/CTranslate2/releases/32550099",
        "zipball_url": "https://api.github.com/repos/OpenNMT/CTranslate2/zipball/v1.14.0"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 215,
      "date": "Tue, 28 Dec 2021 00:35:45 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "neural-machine-translation",
      "cpp",
      "mkl",
      "quantization",
      "cuda",
      "thrust",
      "opennmt",
      "deep-neural-networks",
      "openmp",
      "onednn",
      "intrinsics",
      "avx2",
      "avx",
      "parallel-computing",
      "gemm",
      "neon",
      "transformer-models"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Here are some scenarios where this project could be used:\n\n* You want to accelarate standard translation models for production usage, especially on CPUs.\n* You need to embed translation models in an existing C++ application without adding large dependencies.\n* Your application requires custom threading and memory usage control.\n* You want to reduce the model size on disk and/or memory.\n\nHowever, you should probably **not** use this project when:\n\n* You want to train custom architectures not covered by this project.\n* You see no value in the key features listed at the top of this document.\n\n",
      "technique": "Header extraction"
    }
  ]
}