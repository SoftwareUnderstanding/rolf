{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1409.1556\r\n    # This is going to take some time...\r\n    base_model = VGG16(weights='imagenet'"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9301959214461287,
        0.892070432444913,
        0.9907514293294977,
        0.9999815307965596,
        0.9400145405652369,
        0.9648889021544168
      ],
      "excerpt": "A partir des features de toutes les images du jeu de donn\u00e9es d\u2019apprentissage, il faut r\u00e9aliser l\u2019apprentissage du mod\u00e8le de classification. Ceci sera r\u00e9alis\u00e9 \u00e0 l\u2019aide de la fonction SVMWithSGD (Support Vector Machine with Stochastic Gradient Descent).  \nUne fois le mod\u00e8le cr\u00e9\u00e9, on l'applique aux features des images de test. Le mod\u00e8le permet de pr\u00e9dire pour chaque image une classe, qui est correcte ou incorrecte. Le taux de succ\u00e8s du mod\u00e8le est le pourcentage de bonnes classifications. \nVotre mission est de cr\u00e9er une application Spark permettant de s\u00e9parer les images en jeux d'apprentissage et de test, d'extraire les features des images en provenance de deux classes diff\u00e9rentes, d'apprendre un mod\u00e8le sur les donn\u00e9es d'apprentissage et de mesurer les performances du mod\u00e8le sur les donn\u00e9es de test. \u00c0 titre d'exemple, avec 100 images d'apprentissage dans chaque classe, il est possible d'obtenir 98% de bonnes classifications dans la t\u00e2che Wheaten Terrier vs Yorkshire Terrier. \nNotez que vous pouvez apprendre un mod\u00e8le \"Classe X vs Classe Y\" pour toutes les paires de classes (X, Y) possibles : c'est ce qu'on appelle la classification \"1 vs 1\". Une alternative est de cr\u00e9er un mod\u00e8le \"Class X vs Toutes les autres classes\" : C'est ce qu'on appelle la classification \"1 vs All\". La diff\u00e9rence est que dans la classification 1 vs 1, on part du principe que l'on sait d\u00e9j\u00e0 que les images appartiennent \u00e0 la classe X ou Y : il n'y a pas d'images qui appartiennent \u00e0 une classe autre que X ou Y. Alors qu'un mod\u00e8le 1 vs All permet, \u00e0 partir de n'importe quelle image de test, de pr\u00e9dire si elle appartient \u00e0 la classe X ou non. Je vous conseille de commencer par la classification 1 vs 1, puis d'adapter votre programme pour faire du 1 vs All. \nUne fois que vous aurez prototyp\u00e9 votre application en local, vous pourriez d\u00e9ployer un cluster de calcul sur Amazon Web Services qui vous permettra de r\u00e9aliser l'apprentissage complet de vos mod\u00e8les. D'ailleurs, les performances de classification pourront \u00eatre r\u00e9colt\u00e9es dans S3. \nVous pourriez \u00e9galement d\u00e9tecter les goulots d'\u00e9tranglement de votre application et,si possible, proposer des solutions permettant de s'en affranchir. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/andersoncarlosfs/image_classification",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-07-27T13:31:43Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-07-27T14:25:06Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Prenez un data scientist qui aurait con\u00e7u un algorithme de classification de donn\u00e9es avec des r\u00e9sultats corrects. Demandez-lui comment l\u2019am\u00e9liorer. Neuf fois sur dix, vous vous entendrez r\u00e9pondre : \u201cIl me faut plus de donn\u00e9es !\u201d. \r\n\r\nEn g\u00e9n\u00e9ral, avoir plus de donn\u00e9es permet de cr\u00e9er des mod\u00e8les plus proches de la r\u00e9alit\u00e9, qui se g\u00e9n\u00e9raliseront mieux aux nouvelles donn\u00e9es (qui n'ont pas servi \u00e0 l'apprentissage) et permettront donc d\u2019obtenir de meilleurs r\u00e9sultats. Mais le probl\u00e8me c\u2019est que pour r\u00e9aliser l\u2019apprentissage de mod\u00e8les sur de gros volumes de donn\u00e9es, il faut mettre en place une architecture de stockage et de calcul appropri\u00e9e.\r\n\r\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/andersoncarlosfs/image_classification/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sat, 25 Dec 2021 23:37:01 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/andersoncarlosfs/image_classification/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "andersoncarlosfs/image_classification",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/andersoncarlosfs/image_classification/master/notebooks/rdd.ipynb",
      "https://raw.githubusercontent.com/andersoncarlosfs/image_classification/master/notebooks/data_frame.ipynb",
      "https://raw.githubusercontent.com/andersoncarlosfs/image_classification/master/notebooks/.ipynb_checkpoints/rdd-checkpoint.ipynb",
      "https://raw.githubusercontent.com/andersoncarlosfs/image_classification/master/notebooks/.ipynb_checkpoints/data_frame-checkpoint.ipynb"
    ],
    "technique": "File Exploration"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/andersoncarlosfs/image_classification/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Introduction",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "image_classification",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "andersoncarlosfs",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/andersoncarlosfs/image_classification/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sat, 25 Dec 2021 23:37:01 GMT"
    },
    "technique": "GitHub API"
  },
  "support": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Binance Pay: [andersoncarlosfs](https://app.binance.com/cn/qr/dplk69e279fff5e8445ea2060689c0d56291) / [151298424](https://app.binance.com/cn/qr/dplk69e279fff5e8445ea2060689c0d56291) / [QR Code](https://raw.githubusercontent.com/andersoncarlosfs/resume/main/assets/images/binance_pay.jpeg)\r\n- Binance P2P: andersoncarlosfs / [QR Code](https://raw.githubusercontent.com/andersoncarlosfs/resume/main/assets/images/binance_p2p.jpeg)\r\n- Revolut: [andersoncarlosfs](https://revolut.me/andersoncarlosfs) \r\n",
      "technique": "Header extraction"
    }
  ],
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Le r\u00f4le d\u2019un classifieur d\u2019images est d\u2019indiquer de mani\u00e8re automatique quel est le contenu principal d\u2019une image, \u00e0 partir d\u2019une liste de classes possibles. \r\n\r\nNous allons commencer par t\u00e9l\u00e9charger les images d'u dataset  qui comprend 7390 images en provenance de 37 classes diff\u00e9rentes, chacune de ces classes correspondant \u00e0 une race diff\u00e9rente de chien et de chat ([Oxford IIIT-Pet Dataset]('http://www.robots.ox.ac.uk/~vgg/data/pets/')). Puis nous allons scinder nos donn\u00e9es en deux groupes. Par exemple, pour chacune des 37 classes, nous allons mettre les 100 premi\u00e8res dans le jeu de donn\u00e9es d'apprentissage (soit 3700 images en tout). Les 3690 autres images seront dans le jeu de donn\u00e9es de test.\r\n\r\nPour chacune des images, il faut d\u2019abord produire une repr\u00e9sentation de l\u2019image sous la forme d\u2019un tableau de valeurs flottantes, ce qui permettra de r\u00e9aliser des calculs sur cette repr\u00e9sentation. Pour calculer cette repr\u00e9sentation, nous allons utiliser l'\u00e9tat de l'art du domaine \u00e0 savoir les r\u00e9seaux de neurones convolutionnels (CNN) sous la forme d'une architecture profonde (Deep Learning). Il faudra pour chaque image appliquer un CNN et extraire les valeurs de sortie d\u2019une des couches interm\u00e9diaires. \r\n\r\nEn gros, on va obtenir une repr\u00e9sentation d'images sous la forme d'un tableau de 4096 valeurs flottantes. C'est ce qu'on appelle des features. Ces features proviennent de l'avant-derni\u00e8re couche de notre r\u00e9seau de neurones.\r\n\r\nVoil\u00e0 un script Python qui permet d'extraire les features des images pass\u00e9es en argument et de sauvegarder les features au format JSON :\r\n\r\n```python\r\n#:! /usr/bin/env python\r\nimport json\r\nimport sys\r\n\r\n#: Dependencies can be installed by running:\r\n#: pip install keras tensorflow h5py pillow\r\n\r\n#: Run script as:\r\n#: ./extract-features.py images/*.jpg\r\n\r\nfrom keras.applications.vgg16 import VGG16\r\nfrom keras.models import Model\r\nfrom keras.preprocessing import image\r\nfrom keras.applications.vgg16 import preprocess_input\r\nimport numpy as np\r\n\r\ndef main():\r\n    #: Load model VGG16 as described in https://arxiv.org/abs/1409.1556\r\n    #: This is going to take some time...\r\n    base_model = VGG16(weights='imagenet')\r\n    #: Model will produce the output of the 'fc2'layer which is the penultimate neural network layer\r\n    #: (see the paper above for mode details)\r\n    model = Model(input=base_model.input, output=base_model.get_layer('fc2').output)\r\n\r\n    #: For each image, extract the representation\r\n    for image_path in sys.argv[1:]:\r\n        features = extract_features(model, image_path)\r\n        with open(image_path + \".json\", \"w\") as out:\r\n            json.dump(features, out)\r\n\r\ndef extract_features(model, image_path):\r\n    img = image.load_img(image_path, target_size=(224, 224))\r\n    x = image.img_to_array(img)\r\n    x = np.expand_dims(x, axis=0)\r\n    x = preprocess_input(x)\r\n\r\n    features = model.predict(x)\r\n    return features.tolist()[0]\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n``` \r\n\r\n",
      "technique": "Header extraction"
    }
  ]
}