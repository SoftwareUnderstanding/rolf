{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1509.02971",
      "https://arxiv.org/abs/1509.02971](https://arxiv.org/abs/1509.02971) paper implementing a DDPG agent.\n\nDDPG is an actor-critic method.\n\nThe actor network is responsible for chosing actions based on the state and the critic network try to estimate the reward for the given state-action pair.\n\nDDPG in continuous control is particularly useful as, unlike discrete actions, all the actions are \"choose\" at every timestep with a continuous value making non-trivial to build a loss function based on these values.\n\nInstead, the actor network is indirectly trained using gradient ascent on the critic network, reducing the problem of building a loss function to a more classic RL problem of maximize the expected reward.\n\nThe agent exploits the initial lack of knowledge as well as [Ornstein\u2013Uhlenbeck process](https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process) -generated noise to explore the environment.\n\nThe algorithm also leverages the fixed-Q target, double network, soft-updates and experience replay.\n\nThe hyperparameters selected for the demonstration are:\n\n- Actor learning rate: 0.0001\n- Critic learning rate: 0.0001\n- Update rate: 1\n- Memory size: 100000\n- Batch size: 128\n- Gamma: 0.99\n- Tau: 0.001\n- Adam weight decay: 0\n- Number of episodes: 9000\n\nActor & Critic networks:\n```\nActor(\n  (fc1): Linear(in_features=24, out_features=256, bias=True)\n  (fc2): Linear(in_features=256, out_features=128, bias=True)\n  (fc3): Linear(in_features=128, out_features=2, bias=True)\n)\n```\n```\nCritic(\n  (fcs1): Linear(in_features=24, out_features=256, bias=True)\n  (fc2): Linear(in_features=258, out_features=128, bias=True)\n  (fc3): Linear(in_features=128, out_features=1, bias=True)\n)\n```\n\n\n## Plot of Rewards \n\n![](https://github.com/prajwalgatti/DRL-Collaboration-and-Competition/blob/master/plot.png)\n\nThe saved weights of the Actor and Critic networks can be found [here.](https://github.com/prajwalgatti/DRL-Collaboration-and-Competition/tree/master/savedmodels)\n\nIt took the networks 2087 episodes to be able to perform with not less than score of 0.500 as an average of 100 episodes.\nTraining it different times give us more or less number of episodes to solve sometimes. It can also be reduced by tuning the hyperparameters.\n\nFollow the setup [here.](https://github.com/prajwalgatti/DRL-Collaboration-and-Competition/blob/master/Setup_instructions.md)\n\nFollow this [notebook](https://github.com/prajwalgatti/DRL-Collaboration-and-Competition/blob/master/Tennis.ipynb) for training models.\n\n\n## Ideas for Future Work\n\n- Perform search for better hyperparameters of algorithm as well as the neural networks\n- Implement a state to state predictor to improve the explorative capabilities of the agent"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.966089571429146
      ],
      "excerpt": "The reinforcement learning agent implementation follows the ideas of arXiv:1509.02971 paper implementing a DDPG agent. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/prajwalgatti/DRL-Collaboration-and-Competition",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-12-23T08:13:53Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-01-30T03:29:44Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "![Trained Agent][image1]\n\nThe goal of the project is to create an agent that learns how to efficiently solve a Tennis environment made with Unity-ML agents. While active the agent is trying to approximate the policy that defines his behaviour and tries to maximize the performance in the context of the environment.\n\nIn this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1. If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01. Thus, the goal of each agent is to keep the ball in play.\n\nThe observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Each agent receives its own, local observation. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping.\n\nThe environment is considered solved, when the average (over 100 episodes) of those **scores** is at least +0.5.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8768181754458461,
        0.975380455474442,
        0.9698794292692778,
        0.9435270476096312,
        0.9383815973970425,
        0.909532384498047,
        0.8444980187340926,
        0.8737395798580185
      ],
      "excerpt": "The reinforcement learning agent implementation follows the ideas of arXiv:1509.02971 paper implementing a DDPG agent. \nDDPG is an actor-critic method. \nThe actor network is responsible for chosing actions based on the state and the critic network try to estimate the reward for the given state-action pair. \nDDPG in continuous control is particularly useful as, unlike discrete actions, all the actions are \"choose\" at every timestep with a continuous value making non-trivial to build a loss function based on these values. \nInstead, the actor network is indirectly trained using gradient ascent on the critic network, reducing the problem of building a loss function to a more classic RL problem of maximize the expected reward. \nThe agent exploits the initial lack of knowledge as well as Ornstein\u2013Uhlenbeck process -generated noise to explore the environment. \nThe algorithm also leverages the fixed-Q target, double network, soft-updates and experience replay. \nThe hyperparameters selected for the demonstration are: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8939004018843169,
        0.9898876762317376,
        0.9394885465134686
      ],
      "excerpt": "The saved weights of the Actor and Critic networks can be found here. \nIt took the networks 2087 episodes to be able to perform with not less than score of 0.500 as an average of 100 episodes. \nTraining it different times give us more or less number of episodes to solve sometimes. It can also be reduced by tuning the hyperparameters. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "DDPG implementation for collaboration and competition for a Tennis environment.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/prajwalgatti/DRL-Collaboration-and-Competition/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Fri, 24 Dec 2021 11:19:48 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/prajwalgatti/DRL-Collaboration-and-Competition/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "prajwalgatti/DRL-Collaboration-and-Competition",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/prajwalgatti/DRL-Collaboration-and-Competition/master/Tennis.ipynb",
      "https://raw.githubusercontent.com/prajwalgatti/DRL-Collaboration-and-Competition/master/.ipynb_checkpoints/Tennis-checkpoint.ipynb",
      "https://raw.githubusercontent.com/prajwalgatti/DRL-Collaboration-and-Competition/master/.ipynb_checkpoints/Untitled-checkpoint.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9058512987322769
      ],
      "excerpt": "Follow the setup here. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.823687576256224
      ],
      "excerpt": "Batch size: 128 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/prajwalgatti/DRL-Collaboration-and-Competition/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "ASP"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Description",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "DRL-Collaboration-and-Competition",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "prajwalgatti",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/prajwalgatti/DRL-Collaboration-and-Competition/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Fri, 24 Dec 2021 11:19:48 GMT"
    },
    "technique": "GitHub API"
  }
}