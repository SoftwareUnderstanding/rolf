{
  "citation": [
    {
      "confidence": [
        0.8611805266332359
      ],
      "excerpt": "Key words : Deep learning, supervised classification, decision tree, interpretability \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.956009411840235
      ],
      "excerpt": "<li> Spliting answers allow to a better understanging of neural network decision. </li> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.969794112164439
      ],
      "excerpt": "<li>  Experiments of our multi-optional-softmax summarized at the end of this page </li> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9865374311292784,
        0.9865374311292784,
        0.9865374311292784
      ],
      "excerpt": "<li> optional-softmax1: P<sub>animal</sub>;P<sub>vehicle</sub> </li> \n<li> optional-softmax2: P<sub>bird</sub>; P<sub>cat</sub>; P<sub>deer</sub>; P<sub>dog</sub>; P<sub>frog</sub>; P<sub>horse</sub>;</li> \n<li> optional-softmax3: P<sub>air</sub>; P<sub>car</sub>; P<sub>ship</sub>; P<sub>truck</sub></li> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8550101043698384,
        0.9156566588472104
      ],
      "excerpt": "  <tr> <th> softmax <br/> 10 output </th> <td> 92.63% <red>*</red>               </td> <td> <b>66.69%</b> </td> </tr> \n <tr> <th> multi optional softmax </th> <td> <b>93.30%</b>             </td> <td> 65.86% </td> </tr> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8550101043698384,
        0.8786044417622563
      ],
      "excerpt": "  <tr> <th> softmax <br/> 10 output </th> <td> 93.15% <red>*</red>               </td> <td> <b>68.33%</b> </td> </tr> \n <tr> <th> multi optional softmax </th> <td> <b>93.47%</b>             </td> <td> 66.11% </td> </tr> \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/PierrickPochelu/word_tree_label",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-05-31T22:14:04Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-06-26T14:49:54Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9957514950367976,
        0.9102426796479657,
        0.9945154919559243
      ],
      "excerpt": "The main characterist of supervised classification with neural network is produce predictions as vector probability with one softmax layer. To allow neural network to learn more complex decisions and give them a better representation of the world we suggest to teach them path in decision tree instead. Those works are important to increase the spectrum of possibilities with neural network on real case. Our experiment show better comprehension of super-class of objects. <br/> \nExperiments were held on the CIFAR10 dataset with the Keras suggested implementation, the softmax layer were remplaced by a new kind of layer that return path in decision tree we named multi-optional-softmax. Result show a better accuracy by decreasing by 4.9% error of super class but an increase of 6.6% in inner class due to propagation of error.<br/> \nOur work are generic enough to work on any classification neural network. There is a strong applicability on classification of biological species because beings are naturally classified by the tree of life. <br/> <br/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8976391092950539
      ],
      "excerpt": "<h1> Neural network to learn paths in decision tree </h1> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9938058880764721
      ],
      "excerpt": "Our approach recursively partition the semantic of input space and assign label to final nodes. Our neural network jointly learns to extract features from image via CNN and classify object via decision tree. The structure of neural network is fixed and each gate is answered with one classic softmax layer. The neural network is derivable and can be learn end-to-end manner as usual. To use our architecture dataset need to be labeled as tree-based structure. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9556631075734782
      ],
      "excerpt": "Recent developpement in deep learning regarding smarter results introduced decision tree as output of neural network (rf:Yolo9000). Decision tree are a more informative manner to explain a decision better than class MLP vector-like prediction. Decision tree structure class like an hierarchy of ideas. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8662088866381259
      ],
      "excerpt": "Classic softmax layer is already exclusive so why do we bother with an exlcusive decision tree ? There are many answers.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9645324599895824
      ],
      "excerpt": "<li> If you split one big question Q to a sequence of question q<sub>1</sub> q<sub>2</sub> q<sub>3</sub>. Give good answer to q<sub>1</sub> and q<sub>2</sub> but fail to q<sub>3</sub> give you an limited distance to the groundtruth in the decision tree because you are arrived and succeed to q<sub>2</sub>. For example detect \"cat\" as \"dog\" is more acceptable than a \"cat\" with \"car\", because cats and dogs are of the same super-class \"animals\". </li> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8097515853367753,
        0.8961723418969911,
        0.9551358086910268,
        0.9531552520330705
      ],
      "excerpt": "<li> Spliting answers allow to a better understanging of neural network decision. </li> \n<li> if the network sees a picture and detect with high confidence an animal but is uncertain \nwhat type of animal it is, we still know it is an animal. </li> \n<li> As a consequence of above bullet, bottom of tree can be poorly sampled because data is rare, but super-class can be correctly chosen. Our approach can be more robust to the lack of data. </li> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9032271472065496
      ],
      "excerpt": "<li> Keras implementation of a new block of layers I called \"multi-optional-softmax\". It unify code of exclusive and inclusive nodes. </li> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8570497044855847
      ],
      "excerpt": "<li>  Experiments of our multi-optional-softmax summarized at the end of this page </li> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9119840201241614
      ],
      "excerpt": "Our innovative neural network is implemented by changing both the softmax layer and the structure of labels.  The classic softmax is replaced by new kind of layer called multi-softmax-layer which predict path in decision tree returning probability for each node. The structure of labels are not one class but path list of good direction in decision tree describing the correct path. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8247217803626478
      ],
      "excerpt": "Decision tree can contains 2 kinds of nodes : exclusive gate (one answer among N) and parallel gate (all N sub-questions are asked). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8096709471766119
      ],
      "excerpt": "Neural network can answer to some questions at the same time. The neural network take all pathes from one inclusive node  (\"+\" symbol below).  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9753971282764364
      ],
      "excerpt": " In this example we answer to two questions independently. Is the  point to the west or east ? Is the point to the north or south ? \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.95202048255063,
        0.9523015930497897
      ],
      "excerpt": "Neural network can answer to a succession of questions. The neural network answer to a question by taking one path from one exclusive node (\"X\" symbol below). For example we can answare : Is the point to the west or east ? If it is in the west, is it in the south or north ? \nIn our multi-one-hot-vector exclusive gates are coded as classic softmax layers. The decision took lead to the next question and the other way is ignored. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9564442891044493
      ],
      "excerpt": "The point : (0.92;-0.15) have label [(0;1);(-1;-1)] meaning the point is to the East, so know South/North softmax is disabled with \"-1\". \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.908316703987335
      ],
      "excerpt": "To experiment our contributions we split the famous CIFAR10 dataset to 2 super-classes : animals and vehicles. We will try classify the best we can if one image belong to \"animal\" or \"vehicle\" category and then which animal/vehicle is it. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9242831025066501
      ],
      "excerpt": "To code label as see in section \"Decision Tree implementation\" when animal is cat optional-softmax3 is disabled with -1 values. optional-softmax1 label is P<sub>animal</sub>=1;P<sub>vehicle</sub>=0. optional-softmax2 label contains P<sub>cat</sub>=1 and other probabilities=0. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9297748356985462
      ],
      "excerpt": "Here the results of classic softmax and our multi-optional-softmax implementation.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9112255434227762
      ],
      "excerpt": "<b> We can observe super-classes are better described when we add their sub-classes information in the learning process. </b> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "neural network to learn paths in decision tree",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/PierrickPochelu/word_tree_label/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 23 Dec 2021 08:39:53 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/PierrickPochelu/word_tree_label/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "PierrickPochelu/word_tree_label",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8572109131027699
      ],
      "excerpt": "More complex decision logic are possible, like \"at least N path among M with N<M\", but not possible apriori with softmax layer build to choose one decision to each stage of decision tree. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9079153805167889
      ],
      "excerpt": "<img src=\"AND2.jpg\" width=\"75%\" height=\"75%\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9022371953987088
      ],
      "excerpt": "<img src=\"XOR.jpg\" width=\"50%\" height=\"50%\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.903281498235012
      ],
      "excerpt": "<img src=\"cifar10_dataset.PNG\"  width=\"60%\" height=\"60%\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9202509175650415
      ],
      "excerpt": "<img src=\"cifar10_XOR.jpg\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8430357326096398,
        0.8411245894559387
      ],
      "excerpt": "  <tr> <th> softmax <br/>  2 output </th> <td> 92.67%               </td> <td> - </td> </tr> \n  <tr> <th> softmax <br/> 10 output </th> <td> 93.15% <red>*</red>               </td> <td> <b>68.33%</b> </td> </tr> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/PierrickPochelu/word_tree_label/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "42; To classify \"animals or vehicles\" with 10 output softmax we look if the class predicted belong to animal or vehicle super-class.",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "word_tree_label",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "PierrickPochelu",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/PierrickPochelu/word_tree_label/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 23 Dec 2021 08:39:53 GMT"
    },
    "technique": "GitHub API"
  }
}