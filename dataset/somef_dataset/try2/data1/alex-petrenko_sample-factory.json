{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2006.11751\n\n**Cite:** [BibTeX](https://github.com/alex-petrenko/sample-factory#citation"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you use this repository in your work or otherwise wish to cite it, please make reference to our ICML2020 paper.\n\n```\n@inproceedings{petrenko2020sf,\n  title={Sample Factory: Egocentric 3D Control from Pixels at 100000 FPS with Asynchronous Reinforcement Learning},\n  author={Petrenko, Aleksei and Huang, Zhehui and Kumar, Tushar and Sukhatme, Gaurav and Koltun, Vladlen},\n  booktitle={ICML},\n  year={2020}\n}\n```\n\nFor questions, issues, inquiries please email apetrenko1991@gmail.com. \nGithub issues and pull requests are welcome.\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{petrenko2020sf,\n  title={Sample Factory: Egocentric 3D Control from Pixels at 100000 FPS with Asynchronous Reinforcement Learning},\n  author={Petrenko, Aleksei and Huang, Zhehui and Kumar, Tushar and Sukhatme, Gaurav and Koltun, Vladlen},\n  booktitle={ICML},\n  year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8099084746018815,
        0.9977994744046882,
        0.9897432628886296
      ],
      "excerpt": "Codebase for high throughput asynchronous reinforcement learning. \nPaper: https://arxiv.org/abs/2006.11751 \nCite: BibTeX \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9668253207263927
      ],
      "excerpt": "Added fixed KL divergence penalty as in https://arxiv.org/pdf/1707.06347.pdf  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "    if hasattr(env, 'is_multiagent'): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8705769432999665,
        0.8244300823972747
      ],
      "excerpt": "This achieves 50K+ framerate on a 10-core machine (Intel Core i9-7900X): \npython -m sample_factory.algorithms.appo.train_appo --env=doom_benchmark --algo=APPO --env_frameskip=4 --use_rnn=True --num_workers=20 --num_envs_per_worker=32 --num_policies=1 --ppo_epochs=1 --rollout=32 --recurrence=32 --batch_size=4096 --experiment=doom_battle_appo_fps_20_32 --res_w=128 --res_h=72 --wide_aspect_ratio=False --policy_workers_per_policy=2 --worker_num_splits=2 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/alex-petrenko/sample-factory",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-06-20T00:59:01Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-24T18:59:20Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8317595835163475
      ],
      "excerpt": "Codebase for high throughput asynchronous reinforcement learning. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8549778689329213,
        0.8270418846476771
      ],
      "excerpt": "Fixed a small bug related to population-based training (a reward shaping dictionary was assumed to be a flat dict, \nwhile it could be a nested dict in some envs) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.896184459763429,
        0.9565039183012009,
        0.9077097148718286
      ],
      "excerpt": "Otherwise numerical instabilities can occur in certain environments, especially when the policy lag is high \nMore summaries related to the new loss \nMore improvements and fixes in runner interface, including support for NGC cluster \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9254992883811537,
        0.8654000036164773
      ],
      "excerpt": "Support inactive agents. Do deactivate an agent for a portion of the episode the environment should return info={'is_active': False} for the inactive agent. Useful for environments such as hide-n-seek. \nImproved memory consumption and performance with better shared memory management. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8942076368141727
      ],
      "excerpt": "DMLab-related bug fixes (courtesy of @donghoonlee04 and @sungwoong. Thank you!) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8212205168733568
      ],
      "excerpt": "Version 1.1.9 or above is recommended as it fixes bugs related to multi-agent training. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9482228896328685
      ],
      "excerpt": "of pre-generated environment layouts (see paper for details). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8247423098740628
      ],
      "excerpt": "ALE envs are supported out-of-the-box, although the existing wrappers and hyperparameters \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9207736068770146,
        0.8596853264842544,
        0.9367178394795734
      ],
      "excerpt": "Multi-agent environments are expected to return lists of observations/dones/rewards (one item for every agent). \nIt is expected that a multi-agent env exposes a property or a member variable num_agents that the algorithm uses \nto allocate the right amount of memory during startup. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8771335482574217,
        0.8885247104187685
      ],
      "excerpt": "the first observation of the next episode (because we have no use for the last observation of the previous \nepisode, we do not act based on it). See multi_agent_wrapper.py for example. For simplicity Sample Factory actually treats all \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9527688514782017
      ],
      "excerpt": "Some environments, such as VizDoom, DMLab, and Atari, are added to the env registry in the default installation, so training on these environments is as \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8457797821614241
      ],
      "excerpt": "(e.g. doom_, dmlab_ or atari_ for built-in Sample Factory envs). E.g. doom_battle or atari_breakout. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8190536485824287,
        0.9034248688816703
      ],
      "excerpt": "--train_dir location for all experiments folders, defaults to ./train_dir. \n--num_workers defaults to number of logical cores in the system, which will give the best throughput in \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8320666570082623
      ],
      "excerpt": "Must be even for the double-buffered sampling to work. Disable double-buffered sampling by setting --worker_num_splits=1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9730081343049689,
        0.818954216104255,
        0.9374637353604282,
        0.8345289635123204,
        0.9291383952880672
      ],
      "excerpt": "sample_factory/algorithms/algorithm.py contains parameters that allow users to customize the architectures of neural networks \ninvolved in the training process. Sample Factory includes a few popular NN architectures for RL, such as shallow \nconvnets for Atari and VizDoom, deeper ResNet models for DMLab, MLPs for continuous control tasks. \nCLI parameters allow users to choose between \nthese existing architectures, as well as specify the type of the policy core (LSTM/GRU/feed-forward), nonlinearities, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8430893997683147,
        0.8756679635665592
      ],
      "excerpt": "Train for 4B env steps (also can be stopped at any time with Ctrl+C and resumed by using the same cmd). \nThis is more or less optimal training setup for a 10-core machine. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9512494883121175
      ],
      "excerpt": "Doom \"battle\" and \"battle2\" environments, 36-core server (72 logical cores) with 4 GPUs: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.897179971273309
      ],
      "excerpt": "Duel and deathmatch versus bots, population-based training, 36-core server: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8630996058713069
      ],
      "excerpt": "Duel and deathmatch self-play, PBT, 36-core server: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8500367172385879
      ],
      "excerpt": "This achieves 100K+ framerate on a 36-core machine: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8627396498405498,
        0.820068920985699
      ],
      "excerpt": "benchmark like DMLab-30. On 36-core server generating enough environments for a 10B training session can take up to \na week. We provide a dataset of pre-generated levels to make training on DMLab-30 easier. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9494040772398018,
        0.8270431442650544
      ],
      "excerpt": "(or hyperparameter searches) with optimal distribution of work across GPUs. \nThe configuration of such experiments is done through Python scripts. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9386091898408839
      ],
      "excerpt": "Runner supports other backends for parallel execution: --runner=slurm and --runner=ngc for Slurm and NGC support respectively. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.808274215371622
      ],
      "excerpt": "See a separate trained_policies/README.md. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8838806234153114
      ],
      "excerpt": "a Doom executable. The file descriptors for these buffers tend to pile up. rm /dev/shm/ViZDoom* will take care of this issue. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8450805146627445
      ],
      "excerpt": "Async execution mode for the Doom environments, although the results are not always reproducible between sync and async modes. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9474620134969948,
        0.8376000593223435,
        0.8687346682933768
      ],
      "excerpt": "communication between the environment instances is required which results in a lot of syscalls. \nFor prototyping and testing consider single-player environments with bots instead. \nVectors of environments on rollout (actor) workers are instantiated on the same CPU thread. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "High throughput asynchronous reinforcement learning",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/alex-petrenko/sample-factory/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 42,
      "date": "Sat, 25 Dec 2021 11:21:42 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/alex-petrenko/sample-factory/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "alex-petrenko/sample-factory",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/alex-petrenko/sample-factory/master/kill_proc.sh",
      "https://raw.githubusercontent.com/alex-petrenko/sample-factory/master/all_tests.sh",
      "https://raw.githubusercontent.com/alex-petrenko/sample-factory/master/sample_factory_examples/command_line/train_dmlab30_server_pbt.sh",
      "https://raw.githubusercontent.com/alex-petrenko/sample-factory/master/sample_factory_examples/command_line/train_lunar_lander_continuous.sh",
      "https://raw.githubusercontent.com/alex-petrenko/sample-factory/master/sample_factory_examples/command_line/train_dmlab30.sh",
      "https://raw.githubusercontent.com/alex-petrenko/sample-factory/master/sample_factory/runner/slurm/sbatch_template.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "PyPI dependency resolution may result in suboptimal performance, i.e. some versions of MKL and Numpy are known to be slower.\nTo guarantee the maximum throughput (~10% faster than pip version) consider using our Conda environment with exact package versions:\n\n- Clone the repo: `git clone https://github.com/alex-petrenko/sample-factory.git`\n\n- Create and activate conda env:\n\n```\ncd sample-factory\nconda env create -f environment.yml\nconda activate sample-factory\n```\n\nSF is known to also work on macOS. There is no Windows support at this time.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Just install from PyPI:\n\n```pip install sample-factory```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9453127961979164
      ],
      "excerpt": "Fixed a bug that prevented Vizdoom .cfg and .wad files from being copied to site-packages during installation from PyPI \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9650452803878978,
        0.9896732233860246
      ],
      "excerpt": "To install VizDoom just follow system setup instructions from the original repository (VizDoom linux_deps), \nafter which the latest VizDoom can be installed from PyPI: pip install vizdoom. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9832722078746917,
        0.999746712887969,
        0.9090573989891695
      ],
      "excerpt": "- Follow installation instructions from DMLab Github. \n- pip install dm_env \n- To train on DMLab-30 you will need brady_konkle_oliva2008 dataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9705570785280946
      ],
      "excerpt": "--env (required) full name that uniquely identifies the environment, starting with the env family prefix \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8627430031904697
      ],
      "excerpt": "Train for 4B env steps (also can be stopped at any time with Ctrl+C and resumed by using the same cmd). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8372876401131443
      ],
      "excerpt": "Subsequent DMLab experiments on envs that require level generation will become faster since environment files from \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8389114357918929
      ],
      "excerpt": "python -m sample_factory.run_algorithm --algo=DUMMY_SAMPLER --env=doom_benchmark --num_workers=20 --num_envs_per_worker=1 --experiment=dummy_sampler --sample_env_frames=5000000 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8095931071211014
      ],
      "excerpt": "(e.g. OpenGL context). The solution should be an environment wrapper that starts the environment in a  \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.803150909361475
      ],
      "excerpt": "Experiment logs are now saved into the experiment folder as sf_log.txt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8005179906093722
      ],
      "excerpt": "aren't well optimized for sample efficiency in Atari. Tuned Atari training examples would be a welcome contribution. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8546096990752987
      ],
      "excerpt": "Sample Factory experiments are configured via command line parameters. The following command will print the help message \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.850527722068567
      ],
      "excerpt": "cfg.json file. If you want to start a new experiment, delete the old experiment folder or change the experiment name. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8140734326250975
      ],
      "excerpt": "to override _ActorCriticBase following examples in sample_factory/algorithms/appo/model.py. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8352639007889598
      ],
      "excerpt": "Train on one of the 6 \"basic\" VizDoom environments: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8208932483100051,
        0.8334573156571164
      ],
      "excerpt": "python -m sample_factory.algorithms.appo.train_appo --env=doom_duel_bots --train_for_seconds=360000 --algo=APPO --gamma=0.995 --env_frameskip=2 --use_rnn=True --reward_scale=0.5 --num_workers=72 --num_envs_per_worker=32 --num_policies=8 --ppo_epochs=1 --rollout=32 --recurrence=32 --batch_size=2048 --benchmark=False --res_w=128 --res_h=72 --wide_aspect_ratio=False --pbt_replace_reward_gap=0.2 --pbt_replace_reward_gap_absolute=3.0 --pbt_period_env_steps=5000000 --save_milestones_sec=1800 --with_pbt=True --experiment=doom_duel_bots \npython -m sample_factory.algorithms.appo.train_appo --env=doom_deathmatch_bots --train_for_seconds=3600000 --algo=APPO --use_rnn=True --gamma=0.995 --env_frameskip=2 --rollout=32 --num_workers=80 --num_envs_per_worker=24 --num_policies=8 --ppo_epochs=1 --rollout=32 --recurrence=32 --batch_size=2048 --res_w=128 --res_h=72 --wide_aspect_ratio=False --with_pbt=True --pbt_period_env_steps=5000000 --experiment=doom_deathmatch_bots \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8576888515637814,
        0.8565733245280581
      ],
      "excerpt": "python -m sample_factory.algorithms.appo.train_appo --env=doom_duel --train_for_seconds=360000 --algo=APPO --gamma=0.995 --env_frameskip=2 --use_rnn=True --num_workers=72 --num_envs_per_worker=16 --num_policies=8 --ppo_epochs=1 --rollout=32 --recurrence=32 --batch_size=2048 --res_w=128 --res_h=72 --wide_aspect_ratio=False --benchmark=False --pbt_replace_reward_gap=0.5 --pbt_replace_reward_gap_absolute=0.35 --pbt_period_env_steps=5000000 --with_pbt=True --pbt_start_mutation=100000000 --experiment=doom_duel_full \npython -m sample_factory.algorithms.appo.train_appo --env=doom_deathmatch_full --train_for_seconds=360000 --algo=APPO --gamma=0.995 --env_frameskip=2 --use_rnn=True --num_workers=72 --num_envs_per_worker=16 --num_policies=8 --ppo_epochs=1 --rollout=32 --recurrence=32 --batch_size=2048 --res_w=128 --res_h=72 --wide_aspect_ratio=False --benchmark=False --pbt_replace_reward_gap=0.1 --pbt_replace_reward_gap_absolute=0.1 --pbt_period_env_steps=5000000 --with_pbt=True --pbt_start_mutation=100000000 --experiment=doom_deathmatch_full \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8597746095482159
      ],
      "excerpt": "python -m sample_factory.algorithms.appo.train_appo --env=dmlab_30 --train_for_seconds=3600000 --algo=APPO --gamma=0.99 --use_rnn=True --num_workers=90 --num_envs_per_worker=12 --ppo_epochs=1 --rollout=32 --recurrence=32 --batch_size=2048 --benchmark=False --ppo_epochs=1 --max_grad_norm=0.0 --dmlab_renderer=software --decorrelate_experience_max_seconds=120 --reset_timeout_seconds=300 --encoder_custom=dmlab_instructions --encoder_type=resnet --encoder_subtype=resnet_impala --encoder_extra_fc_layers=1 --hidden_size=256 --nonlinearity=relu --rnn_type=lstm --dmlab_extended_action_set=True --num_policies=4 --pbt_replace_reward_gap=0.05 --pbt_replace_reward_gap_absolute=5.0 --pbt_period_env_steps=10000000 --pbt_start_mutation=100000000 --with_pbt=True --experiment=dmlab_30_resnet_4pbt_w90_v12 --dmlab_one_task_per_worker=True --set_workers_cpu_affinity=True --max_policy_lag=35 --pbt_target_objective=dmlab_target_objective --dmlab30_dataset=~/datasets/brady_konkle_oliva2008 --dmlab_use_level_cache=True --dmlab_level_cache_path=/home/user/dmlab_cache \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8488346306754657
      ],
      "excerpt": "Download here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8635042593346189
      ],
      "excerpt": "Here's an example runner script that we used to train agents for 6 basic VizDoom environments with 10 seeds each: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8364364855349268
      ],
      "excerpt": "from sample_factory.runner.run_description import RunDescription, Experiment, ParamGrid \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/alex-petrenko/sample-factory/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell",
      "Makefile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 Aleksei Petrenko\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Sample Factory",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "sample-factory",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "alex-petrenko",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/alex-petrenko/sample-factory/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "alex-petrenko",
        "body": "Version at the time of camera-ready ICML2020 submission",
        "dateCreated": "2020-06-24T00:42:52Z",
        "datePublished": "2020-06-24T00:47:05Z",
        "html_url": "https://github.com/alex-petrenko/sample-factory/releases/tag/1.0.0",
        "name": "ICML2020",
        "tag_name": "1.0.0",
        "tarball_url": "https://api.github.com/repos/alex-petrenko/sample-factory/tarball/1.0.0",
        "url": "https://api.github.com/repos/alex-petrenko/sample-factory/releases/27852979",
        "zipball_url": "https://api.github.com/repos/alex-petrenko/sample-factory/zipball/1.0.0"
      }
    ],
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Here we provide command lines that can be used to reproduce the experiments from the paper, which also serve as an example on how to configure large-scale RL experiments.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 287,
      "date": "Sat, 25 Dec 2021 11:21:42 GMT"
    },
    "technique": "GitHub API"
  },
  "support": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Sample Factory has a runtime environment registry for _families of environments_. A family of environments\nis defined by a name prefix (i.e. `atari_` or `doom_`) and a function that creates an instance of the environment\ngiven its full name, including the prefix (i.e. `atari_breakout`).\n\nRegistering families of environments allows the user to add\nand override configuration parameters (such as resolution, frameskip, default model type, etc.) for the whole family\nof environments, i.e. all VizDoom envs can share their basic configuration parameters that don't need to be specified for each experiment.\n\nCustom user-defined environment families and models can be added to the registry, see this example:\n`sample_factory_examples/train_custom_env_custom_model.py`\n\nScript `sample_factory_examples/train_gym_env.py` demonstrates how Sample Factory can be used with an environment defined in OpenAI Gym.\n\nSample Factory comes with a particularly comprehensive support for VizDoom and DMLab, see below.\n\n",
      "technique": "Header extraction"
    }
  ],
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Sample Factory is the fastest open source single-machine RL implementations (see paper for details).\nIf you plan to train RL agents on large amounts of experience, consider using it.\nSample Factory can significantly speed up\nthe experimentation or allow you to collect more samples in the same amount of time and achieve better performance.\n\n2. Consider using Sample Factory for your multi-agent and population-based training experiments. \nMulti-agent and PBT setups are really simple with Sample Factory.\n\n3. A lot of work went into our VizDoom and DMLab wrappers. For example, we include full support for\nconfigurable VizDoom multi-agent environments and their interop with RL algorithms, which can open new interesting research directions.\nConsider using Sample Factory if you train agents in these environments.\n\n4. Sample Factory can be a good choice as a prototype for a single node in a distributed RL system or as a reference\ncodebase for other types of async RL algorithms.\n\n",
      "technique": "Header extraction"
    }
  ]
}