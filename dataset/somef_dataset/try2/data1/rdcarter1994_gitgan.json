{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This is a port of [pix2pix](https://github.com/phillipi/pix2pix) from Torch to Tensorflow.  It also contains colorspace conversion code ported from Torch.  Thanks to the Tensorflow team for making such a quality library!  And special thanks to Phillip Isola for answering my questions about the pix2pix code.\n",
      "technique": "Header extraction"
    }
  ],
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you use this code for your research, please cite the paper this code is based on: <a href=\"https://arxiv.org/pdf/1611.07004v1.pdf\">Image-to-Image Translation Using Conditional Adversarial Networks</a>:\n\n```\n@article{pix2pix2016,\n  title={Image-to-Image Translation with Conditional Adversarial Networks},\n  author={Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},\n  journal={arxiv},\n  year={2016}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{pix2pix2016,\n  title={Image-to-Image Translation with Conditional Adversarial Networks},\n  author={Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},\n  journal={arxiv},\n  year={2016}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8095647066877186,
        0.8326585122547221
      ],
      "excerpt": "Based on pix2pix by Isola et al. \nArticle about this implemention \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rdcarter1994/gitgan",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-09-15T20:45:24Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-09-15T20:56:28Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9890188673288955
      ],
      "excerpt": "This port is based directly on the torch implementation, and not on an existing Tensorflow implementation.  It is meant to be a faithful implementation of the original work and so does not add anything.  The processing speed on a GPU with cuDNN was equivalent to the Torch implementation in testing. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.896466392580114
      ],
      "excerpt": "The data format used by this program is the same as the original pix2pix format, which consists of images of input and desired output side by side like: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9884531030638974
      ],
      "excerpt": "The facades dataset is the smallest and easiest to get started with. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.893962334658333
      ],
      "excerpt": "No other processing is required, the colorization mode (see Training section below) uses single images instead of image pairs. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9395535275740382
      ],
      "excerpt": "pix2pix.py includes special code to handle colorization with single images instead of pairs, using that looks like this: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9233588864669621
      ],
      "excerpt": "In this mode, image A is the black and white image (lightness only), and image B contains the color channels of that image (no lightness information). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9019602412088829
      ],
      "excerpt": "Validation of the code was performed on a Linux machine with a ~1.3 TFLOPS Nvidia GTX 750 Ti GPU and an Azure NC6 instance with a K80 GPU. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rdcarter1994/gitgan/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 29 Dec 2021 16:34:29 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/rdcarter1994/gitgan/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "rdcarter1994/gitgan",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/rdcarter1994/gitgan/master/docker/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/rdcarter1994/gitgan/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9738607088209219
      ],
      "excerpt": "Linux with Tensorflow GPU edition + cuDNN \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8447366828658196
      ],
      "excerpt": "For colorization, your images should ideally all be the same aspect ratio.  You can resize and crop them with the resize command: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8758777794521986
      ],
      "excerpt": "Testing is done with --mode test.  You should specify the checkpoint to use with --checkpoint, this should point to the output_dir that you created previously with --mode train: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8062211610784724,
        0.9876851288033279,
        0.9702717945002866
      ],
      "excerpt": "Validation of the code was performed on a Linux machine with a ~1.3 TFLOPS Nvidia GTX 750 Ti GPU and an Azure NC6 instance with a K80 GPU. \ngit clone https://github.com/affinelayer/pix2pix-tensorflow.git \ncd pix2pix-tensorflow \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9378529205150474
      ],
      "excerpt": "<img src=\"docs/examples.jpg\" width=\"900px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8739891000537701,
        0.8216270093103228,
        0.8739891000537701
      ],
      "excerpt": "<img src=\"docs/ab.png\" width=\"256px\"/> \nFor example: \n<img src=\"docs/418.png\" width=\"256px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8843337329198684
      ],
      "excerpt": "| dataset | example | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9525845436130999,
        0.9619001600264266,
        0.8857619587742089,
        0.9092639446674168,
        0.8732661801294349
      ],
      "excerpt": "| python tools/download-dataset.py facades <br> 400 images from CMP Facades dataset. (31MB) <br> Pre-trained: BtoA  | <img src=\"docs/facades.jpg\" width=\"256px\"/> | \n| python tools/download-dataset.py cityscapes <br> 2975 images from the Cityscapes training set. (113M) <br> Pre-trained: AtoB BtoA | <img src=\"docs/cityscapes.jpg\" width=\"256px\"/> | \n| python tools/download-dataset.py maps <br> 1096 training images scraped from Google Maps (246M) <br> Pre-trained: AtoB BtoA | <img src=\"docs/maps.jpg\" width=\"256px\"/> | \n| python tools/download-dataset.py edges2shoes <br> 50k training images from UT Zappos50K dataset. Edges are computed by HED edge detector + post-processing. (2.2GB) <br> Pre-trained: AtoB | <img src=\"docs/edges2shoes.jpg\" width=\"256px\"/>  | \n| python tools/download-dataset.py edges2handbags <br> 137K Amazon Handbag images from iGAN project. Edges are computed by HED edge detector + post-processing. (8.6GB) <br> Pre-trained: AtoB | <img src=\"docs/edges2handbags.jpg\" width=\"256px\"/> | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091,
        0.8414384768071093
      ],
      "excerpt": "python pix2pix.py \\ \n  --mode train \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589402744553264
      ],
      "excerpt": "  --input_dir facades/train \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091,
        0.8414384768071093
      ],
      "excerpt": "python pix2pix.py \\ \n  --mode train \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8583966466999511
      ],
      "excerpt": "<img src=\"docs/tensorboard-scalar.png\" width=\"250px\"/> <img src=\"docs/tensorboard-image.png\" width=\"250px\"/> <img src=\"docs/tensorboard-graph.png\" width=\"250px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8280406780420246
      ],
      "excerpt": "Testing is done with --mode test.  You should specify the checkpoint to use with --checkpoint, this should point to the output_dir that you created previously with --mode train: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091,
        0.8438769870689441
      ],
      "excerpt": "python pix2pix.py \\ \n  --mode test \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8098892235364082,
        0.8919411025644738
      ],
      "excerpt": "The test run will output an HTML file at facades_test/index.html that shows input/output/target image sets: \n<img src=\"docs/test-html.png\" width=\"300px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9096562735130456
      ],
      "excerpt": "python tools/download-dataset.py facades \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091,
        0.8414384768071093
      ],
      "excerpt": "    python pix2pix.py \\ \n      --mode train \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589402744553264
      ],
      "excerpt": "      --input_dir facades/train \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091,
        0.8438769870689441
      ],
      "excerpt": "    python pix2pix.py \\ \n      --mode test \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8787209803028618,
        0.8787209803028618,
        0.8835800773984891,
        0.8787209803028618
      ],
      "excerpt": "| <img src=\"docs/1-inputs.png\" width=\"256px\"> | <img src=\"docs/1-tensorflow.png\" width=\"256px\"> | <img src=\"docs/1-torch.jpg\" width=\"256px\"> | <img src=\"docs/1-targets.png\" width=\"256px\"> | \n| <img src=\"docs/5-inputs.png\" width=\"256px\"> | <img src=\"docs/5-tensorflow.png\" width=\"256px\"> | <img src=\"docs/5-torch.jpg\" width=\"256px\"> | <img src=\"docs/5-targets.png\" width=\"256px\"> | \n| <img src=\"docs/51-inputs.png\" width=\"256px\"> | <img src=\"docs/51-tensorflow.png\" width=\"256px\"> | <img src=\"docs/51-torch.jpg\" width=\"256px\"> | <img src=\"docs/51-targets.png\" width=\"256px\"> | \n| <img src=\"docs/95-inputs.png\" width=\"256px\"> | <img src=\"docs/95-tensorflow.png\" width=\"256px\"> | <img src=\"docs/95-torch.jpg\" width=\"256px\"> | <img src=\"docs/95-targets.png\" width=\"256px\"> | \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/rdcarter1994/gitgan/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "JavaScript",
      "Python",
      "HTML",
      "Dockerfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "pix2pix-tensorflow",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "gitgan",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "rdcarter1994",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rdcarter1994/gitgan/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Tensorflow 1.4.1\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 29 Dec 2021 16:34:29 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```sh\n#: clone this repo\ngit clone https://github.com/affinelayer/pix2pix-tensorflow.git\ncd pix2pix-tensorflow\n#: download the CMP Facades dataset (generated from http://cmp.felk.cvut.cz/~tylecr1/facade/)\npython tools/download-dataset.py facades\n#: train the model (this may take 1-8 hours depending on GPU, on CPU you will be waiting for a bit)\npython pix2pix.py \\\n  --mode train \\\n  --output_dir facades_train \\\n  --max_epochs 200 \\\n  --input_dir facades/train \\\n  --which_direction BtoA\n#: test the model\npython pix2pix.py \\\n  --mode test \\\n  --output_dir facades_test \\\n  --input_dir facades/val \\\n  --checkpoint facades_train\n```\n\nThe test run will output an HTML file at `facades_test/index.html` that shows input/output/target image sets.\n\nIf you have Docker installed, you can use the provided Docker image to run pix2pix without installing the correct version of Tensorflow:\n\n```sh\n#: train the model\npython tools/dockrun.py python pix2pix.py \\\n      --mode train \\\n      --output_dir facades_train \\\n      --max_epochs 200 \\\n      --input_dir facades/train \\\n      --which_direction BtoA\n#: test the model\npython tools/dockrun.py python pix2pix.py \\\n      --mode test \\\n      --output_dir facades_test \\\n      --input_dir facades/val \\\n      --checkpoint facades_train\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "<img src=\"docs/combine.png\" width=\"900px\"/>\n\n```sh\n#: Resize source images\npython tools/process.py \\\n  --input_dir photos/original \\\n  --operation resize \\\n  --output_dir photos/resized\n#: Create images with blank centers\npython tools/process.py \\\n  --input_dir photos/resized \\\n  --operation blank \\\n  --output_dir photos/blank\n#: Combine resized images with blanked images\npython tools/process.py \\\n  --input_dir photos/resized \\\n  --b_dir photos/blank \\\n  --operation combine \\\n  --output_dir photos/combined\n#: Split into train/val set\npython tools/split.py \\\n  --dir photos/combined\n```\n\nThe folder `photos/combined` will now have `train` and `val` subfolders that you can use for training and testing.\n\n",
      "technique": "Header extraction"
    }
  ]
}