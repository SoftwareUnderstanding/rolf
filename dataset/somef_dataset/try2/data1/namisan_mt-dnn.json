{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "BERT pytorch is from: https://github.com/huggingface/pytorch-pretrained-BERT <br/>\nBERT: https://github.com/google-research/bert <br/>\nWe also used some code from: https://github.com/kevinduh/san_mrc <br/>\n\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1904.09482",
      "https://arxiv.org/abs/1907.11983",
      "https://arxiv.org/abs/1908.03265",
      "https://arxiv.org/abs/1911.03437",
      "https://arxiv.org/abs/2002.07972",
      "https://arxiv.org/abs/2004.08994",
      "https://arxiv.org/abs/2010.12638",
      "https://arxiv.org/abs/1904.09482",
      "https://arxiv.org/abs/1907.11983",
      "https://arxiv.org/abs/1908.03265",
      "https://arxiv.org/abs/1911.03437",
      "https://arxiv.org/abs/2002.07972",
      "https://arxiv.org/abs/2004.08994",
      "https://arxiv.org/abs/2010.12638"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\n@inproceedings{liu2019mt-dnn,\n    title = \"Multi-Task Deep Neural Networks for Natural Language Understanding\",\n    author = \"Liu, Xiaodong and He, Pengcheng and Chen, Weizhu and Gao, Jianfeng\",\n    booktitle = \"Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics\",\n    month = jul,\n    year = \"2019\",\n    address = \"Florence, Italy\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/P19-1441\",\n    pages = \"4487--4496\"\n}\n\n\n@article{liu2019mt-dnn-kd,\n  title={Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding},\n  author={Liu, Xiaodong and He, Pengcheng and Chen, Weizhu and Gao, Jianfeng},\n  journal={arXiv preprint arXiv:1904.09482},\n  year={2019}\n}\n\n\n@article{he2019hnn,\n  title={A Hybrid Neural Network Model for Commonsense Reasoning},\n  author={He, Pengcheng and Liu, Xiaodong and Chen, Weizhu and Gao, Jianfeng},\n  journal={arXiv preprint arXiv:1907.11983},\n  year={2019}\n}\n\n\n@article{liu2019radam,\n  title={On the Variance of the Adaptive Learning Rate and Beyond},\n  author={Liu, Liyuan and Jiang, Haoming and He, Pengcheng and Chen, Weizhu and Liu, Xiaodong and Gao, Jianfeng and Han, Jiawei},\n  journal={arXiv preprint arXiv:1908.03265},\n  year={2019}\n}\n\n\n@article{jiang2019smart,\n  title={SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization},\n  author={Jiang, Haoming and He, Pengcheng and Chen, Weizhu and Liu, Xiaodong and Gao, Jianfeng and Zhao, Tuo},\n  journal={arXiv preprint arXiv:1911.03437},\n  year={2019}\n}\n\n\n@article{liu2020mtmtdnn,\n  title={The Microsoft Toolkit of Multi-Task Deep Neural Networks for Natural Language Understanding},\n  author={Liu, Xiaodong and Wang, Yu and Ji, Jianshu and Cheng, Hao and Zhu, Xueyun and Awa, Emmanuel and He, Pengcheng and Chen, Weizhu and Poon, Hoifung and Cao, Guihong and Jianfeng Gao},\n  journal={arXiv preprint arXiv:2002.07972},\n  year={2020}\n}\n\n\n@article{liu2020alum,\n  title={Adversarial Training for Large Neural Language Models},\n  author={Liu, Xiaodong and Cheng, Hao and He, Pengcheng and Chen, Weizhu and Wang, Yu and Poon, Hoifung and Gao, Jianfeng},\n  journal={arXiv preprint arXiv:2004.08994},\n  year={2020}\n}\n\n@article{cheng2020posterior,\n  title={Posterior Differential Regularization with f-divergence for Improving Model Robustness},\n  author={Cheng, Hao and Liu, Xiaodong and Pereira, Lis and Yu, Yaoliang and Gao, Jianfeng},\n  journal={arXiv preprint arXiv:2010.12638},\n  year={2020}\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{cheng2020posterior,\n  title={Posterior Differential Regularization with f-divergence for Improving Model Robustness},\n  author={Cheng, Hao and Liu, Xiaodong and Pereira, Lis and Yu, Yaoliang and Gao, Jianfeng},\n  journal={arXiv preprint arXiv:2010.12638},\n  year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{liu2020alum,\n  title={Adversarial Training for Large Neural Language Models},\n  author={Liu, Xiaodong and Cheng, Hao and He, Pengcheng and Chen, Weizhu and Wang, Yu and Poon, Hoifung and Gao, Jianfeng},\n  journal={arXiv preprint arXiv:2004.08994},\n  year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{liu2020mtmtdnn,\n  title={The Microsoft Toolkit of Multi-Task Deep Neural Networks for Natural Language Understanding},\n  author={Liu, Xiaodong and Wang, Yu and Ji, Jianshu and Cheng, Hao and Zhu, Xueyun and Awa, Emmanuel and He, Pengcheng and Chen, Weizhu and Poon, Hoifung and Cao, Guihong and Jianfeng Gao},\n  journal={arXiv preprint arXiv:2002.07972},\n  year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{jiang2019smart,\n  title={SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization},\n  author={Jiang, Haoming and He, Pengcheng and Chen, Weizhu and Liu, Xiaodong and Gao, Jianfeng and Zhao, Tuo},\n  journal={arXiv preprint arXiv:1911.03437},\n  year={2019}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{liu2019radam,\n  title={On the Variance of the Adaptive Learning Rate and Beyond},\n  author={Liu, Liyuan and Jiang, Haoming and He, Pengcheng and Chen, Weizhu and Liu, Xiaodong and Gao, Jianfeng and Han, Jiawei},\n  journal={arXiv preprint arXiv:1908.03265},\n  year={2019}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{he2019hnn,\n  title={A Hybrid Neural Network Model for Commonsense Reasoning},\n  author={He, Pengcheng and Liu, Xiaodong and Chen, Weizhu and Gao, Jianfeng},\n  journal={arXiv preprint arXiv:1907.11983},\n  year={2019}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{liu2019mt-dnn-kd,\n  title={Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding},\n  author={Liu, Xiaodong and He, Pengcheng and Chen, Weizhu and Gao, Jianfeng},\n  journal={arXiv preprint arXiv:1904.09482},\n  year={2019}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{liu2019mt-dnn,\n    title = \"Multi-Task Deep Neural Networks for Natural Language Understanding\",\n    author = \"Liu, Xiaodong and He, Pengcheng and Chen, Weizhu and Gao, Jianfeng\",\n    booktitle = \"Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics\",\n    month = jul,\n    year = \"2019\",\n    address = \"Florence, Italy\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/P19-1441\",\n    pages = \"4487--4496\"\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9993510466769998,
        0.8388711920974846,
        0.9222383658450612
      ],
      "excerpt": "Xiaodong Liu*, Pengcheng He*, Weizhu Chen and Jianfeng Gao<br/> \nMulti-Task Deep Neural Networks for Natural Language Understanding<br/> \nACL 2019 <br/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9993510466769998,
        0.8475604207426413,
        0.9402107675288106,
        0.9993510466769998
      ],
      "excerpt": "Xiaodong Liu, Pengcheng He, Weizhu Chen and Jianfeng Gao<br/> \nImproving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding <br/> \narXiv version <br/> \nPengcheng He, Xiaodong Liu, Weizhu Chen and Jianfeng Gao<br/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9402107675288106,
        0.9999594644859255
      ],
      "excerpt": "arXiv version <br/> \nLiyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao and Jiawei Han <br/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9402107675288106,
        0.9993510466769998
      ],
      "excerpt": "arXiv version <br/> \nHaoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao and Tuo Zhao <br/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9402107675288106,
        0.999998599486631
      ],
      "excerpt": "arXiv version <br/> \nXiaodong Liu, Yu Wang, Jianshu Ji, Hao Cheng, Xueyun Zhu, Emmanuel Awa, Pengcheng He, Weizhu Chen, Hoifung Poon, Guihong Cao, Jianfeng Gao<br/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9402107675288106,
        0.999994661998742
      ],
      "excerpt": "arXiv version <br/> \nXiaodong Liu, Hao Cheng, Pengcheng He, Weizhu Chen, Yu Wang, Hoifung Poon and Jianfeng Gao<br/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9402107675288106,
        0.9997872348762119
      ],
      "excerpt": "arXiv version <br/> \nHao Cheng and Xiaodong Liu and Lis Pereira and Yaoliang Yu and Jianfeng Gao<br/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9402107675288106
      ],
      "excerpt": "arXiv version <br/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8335025297201542
      ],
      "excerpt": "Please refer the script: scripts\\run_mt_dnn_gc_fp16.sh \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/namisan/mt-dnn",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "For help or issues using MT-DNN, please submit a GitHub issue.\n\nFor personal communication related to this package, please contact Xiaodong Liu (`xiaodl@microsoft.com`), Yu Wang (`yuwan@microsoft.com`), Pengcheng He (`penhe@microsoft.com`), Weizhu Chen (`wzchen@microsoft.com`), Jianshu Ji (`jianshuj@microsoft.com`), Hao Cheng (`chehao@microsoft.com`) or Jianfeng Gao (`jfgao@microsoft.com`).\n",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-02-19T22:58:26Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-24T04:07:50Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9578888452842247
      ],
      "excerpt": "This PyTorch package implements the Multi-Task Deep Neural Networks (MT-DNN) for Natural Language Understanding, as described in: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8577707596151322
      ],
      "excerpt": "Multi-Task Deep Neural Networks for Natural Language Understanding<br/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8022169834517647
      ],
      "excerpt": "Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding <br/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8293830178244861
      ],
      "excerpt": "Hybrid Neural Network Model for Commonsense Reasoning <br/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8603346247000166
      ],
      "excerpt": "On the Variance of the Adaptive Learning Rate and Beyond <br/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8043338969028725
      ],
      "excerpt": "SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization <br/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9137200099608279
      ],
      "excerpt": "The Microsoft Toolkit of Multi-Task Deep Neural Networks for Natural Language Understanding <br/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8184885419971552
      ],
      "excerpt": "Adversarial Training for Large Neural Language Models <br/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8142548267532593,
        0.9065436361714799
      ],
      "excerpt": "Hao Cheng and Xiaodong Liu and Lis Pereira and Yaoliang Yu and Jianfeng Gao<br/> \nPosterior Differential Regularization with f-divergence for Improving Model Robustness <br/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "Preprocess data </br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9270552489877517,
        0.9297821655313908
      ],
      "excerpt": "MTL refinement: refine MT-DNN (shared layers), initialized with the pre-trained BERT model, via MTL using all GLUE tasks excluding WNLI to learn a new shared representation. </br> \nNote that we ran this experiment on 8 V100 GPUs (32G) with a batch size of 32. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9638092951469075
      ],
      "excerpt": "Finetuning: finetune MT-DNN to each of the GLUE tasks to get task-specific models. </br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8458931811843686
      ],
      "excerpt": "Finetune on the STS-B task </br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.901270036543781,
        0.8458931811843686
      ],
      "excerpt": "   You should get about 90.5/90.4 on STS-B dev in terms of Pearson/Spearman correlation. </br> \nFinetune on the RTE task  </br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9067344713398077
      ],
      "excerpt": "   You should get about 83.8 on RTE dev in terms of accuracy. </br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "Preprocess data </br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "Preprocess data </br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9094021085997656
      ],
      "excerpt": "The code to reproduce HNN is under hnn folder, to reproduce the results of HNN, run  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8615480536199254
      ],
      "excerpt": "Here, we go through how to convert a Chinese Tensorflow BERT model into mt-dnn format. <br/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9427539812512333,
        0.8355025788318435
      ],
      "excerpt": "Yes, we released the pretrained shared embedings via MTL which are aligned to BERT base/large models: mt_dnn_base.pt and mt_dnn_large.pt. </br> \nTo obtain the similar models: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9967840874768388,
        0.9372790521094025
      ],
      "excerpt": "For SciTail/SNLI tasks, the purpose is to test generalization of the learned embedding and how easy it is adapted to a new domain instead of complicated model structures for a direct comparison with BERT. Thus, we use a linear projection on the all domain adaptation settings. \nThe difference is in the QNLI dataset. Please refere to the GLUE official homepage for more details. If you want to formulate QNLI as pair-wise ranking task as our paper, make sure that you use the old QNLI data. </br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.877601411872619,
        0.8670333488494621
      ],
      "excerpt": "If you have issues to access the old version of the data, please contact the GLUE team. \nWe can use the multi-task refinement model to run the prediction and produce a reasonable result. But to achieve a better result, it requires a fine-tuneing on each task. It is worthing noting the paper in arxiv is a littled out-dated and on the old GLUE dataset. We will update the paper as we mentioned below. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Multi-Task Deep Neural Networks for Natural Language Understanding",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/namisan/mt-dnn/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 376,
      "date": "Sat, 25 Dec 2021 13:29:07 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/namisan/mt-dnn/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "namisan/mt-dnn",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/namisan/mt-dnn/master/docker/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/namisan/mt-dnn/master/tutorials/Run_Your_Own_Task_in_MT-DNN.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/namisan/mt-dnn/master/download.sh",
      "https://raw.githubusercontent.com/namisan/mt-dnn/master/run_toy.sh",
      "https://raw.githubusercontent.com/namisan/mt-dnn/master/scripts/run_mt_dnn.sh",
      "https://raw.githubusercontent.com/namisan/mt-dnn/master/scripts/run_rte_mt_dnn_kd.sh",
      "https://raw.githubusercontent.com/namisan/mt-dnn/master/scripts/run_rte.sh",
      "https://raw.githubusercontent.com/namisan/mt-dnn/master/scripts/scitail_domain_adaptation_bash.sh",
      "https://raw.githubusercontent.com/namisan/mt-dnn/master/scripts/run_stsb.sh",
      "https://raw.githubusercontent.com/namisan/mt-dnn/master/scripts/run_mt_dnn_gc_fp16.sh",
      "https://raw.githubusercontent.com/namisan/mt-dnn/master/scripts/run_rte_roberta.sh",
      "https://raw.githubusercontent.com/namisan/mt-dnn/master/scripts/snli_domain_adaptation_bash.sh",
      "https://raw.githubusercontent.com/namisan/mt-dnn/master/scripts/domain_adaptation_run.sh",
      "https://raw.githubusercontent.com/namisan/mt-dnn/master/alum/alum_train.sh",
      "https://raw.githubusercontent.com/namisan/mt-dnn/master/experiments/glue/prepro.sh",
      "https://raw.githubusercontent.com/namisan/mt-dnn/master/hnn/script/hnn_train_large.sh",
      "https://raw.githubusercontent.com/namisan/mt-dnn/master/tests/test.sh",
      "https://raw.githubusercontent.com/namisan/mt-dnn/master/sample_data/input/my_head.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. python3.6 </br>\n   Reference to download and install : https://www.python.org/downloads/release/python-360/\n\n2. install requirements </br>\n   ```> pip install -r requirements.txt```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8994908652318586
      ],
      "excerpt": "   &gt; sh download.sh </br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.867931336984398
      ],
      "excerpt": "   &gt; sh experiments/glue/prepro.sh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8067071529356844
      ],
      "excerpt": "Run the following script for MT-DNN format</br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9256256881884376
      ],
      "excerpt": "Then run the prepro script with flags:   &gt; sh experiments/glue/prepro.sh --old_glue </br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8947626018990187
      ],
      "excerpt": "Pretrained UniLM: https://github.com/microsoft/unilm <br/> \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8634339304689512
      ],
      "excerpt": "Download data </br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8174540907975313,
        0.8517164634376088
      ],
      "excerpt": "Training </br> \n   &gt; python train.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8174540907975313
      ],
      "excerpt": "Training: </br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8946042064006514,
        0.880451935254173,
        0.8378512140531662,
        0.8174540907975313
      ],
      "excerpt": "   a) Download NER data to data/ner including: {train/valid/test}.txt </br> \n   b) Convert NER data to the canonical format: &gt; python experiments\\ner\\prepro.py --data data\\ner --output_dir data\\canonical_data </br> \n   c) Preprocess the canonical data to the MT-DNN format: &gt; python prepro_std.py --root_dir data\\canonical_data --task_def experiments\\ner\\ner_task_def.yml --model bert-base-uncased </br> \nTraining </br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9204454650770706,
        0.857816983505115,
        0.8174540907975313
      ],
      "excerpt": "   a) Download SQuAD data to data/squad including: {train/valid}.txt and then change file name to: {squad_train/squad_dev}.json</br> \n   b) Convert data to the MT-DNN format: &gt; python experiments\\squad\\squad_prepro.py  --root_dir data\\canonical_data --task_def experiments\\squad\\squad_task_def.yml --model bert-base-uncased </br> \nTraining </br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8265514985910797
      ],
      "excerpt": "   &gt;python extractor.py --do_lower_case --finput input_examples\\pair-input.txt --foutput input_examples\\pair-output.json --bert_model bert-base-uncased --checkpoint mt_dnn_models\\mt_dnn_base.pt </br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8212888235447114
      ],
      "excerpt": "   &gt;python extractor.py  --do_lower_case --finput input_examples\\single-input.txt --foutput input_examples\\single-output.json --bert_model bert-base-uncased --checkpoint mt_dnn_models\\mt_dnn_base.pt </br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8334882534580248
      ],
      "excerpt": "   For example, if you use the flag: --grad_accumulation_step 4 during the training, the actual batch size will be batch_size * 4. </br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8027361989108412
      ],
      "excerpt": "Here, we go through how to convert a Chinese Tensorflow BERT model into mt-dnn format. <br/> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/namisan/mt-dnn/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell",
      "Jupyter Notebook",
      "Dockerfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 Microsoft\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Multi-Task Deep Neural Networks for Natural Language Understanding",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "mt-dnn",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "namisan",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/namisan/mt-dnn/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1876,
      "date": "Sat, 25 Dec 2021 13:29:07 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "multi-task-learning",
      "natural-language-understanding",
      "deep-learning",
      "microsoft",
      "ranking",
      "named-entity-recognition",
      "bert"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Pull docker </br>\n   ```> docker pull allenlao/pytorch-mt-dnn:v0.5```\n\n2. Run docker </br>\n   ```> docker run -it --rm --runtime nvidia  allenlao/pytorch-mt-dnn:v1.2 bash``` </br>\n   Please refer to the following link if you first use docker: https://docs.docker.com/\n\n",
      "technique": "Header extraction"
    }
  ]
}