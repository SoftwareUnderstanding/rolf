{
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/AverageName/UI2IT",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-12-29T15:28:06Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-04-29T07:10:30Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9943260860830814
      ],
      "excerpt": "Image-to-image  translation  is  a  class  of  vision  and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks,paired training data will not be available.  We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G:X\u2192Y such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss.Because this mapping is highly under-constrained, we couple it with an inverse mapping F:Y\u2192X and introduce a cycle consistency loss to enforce F(G(X))\u2248X(and viceversa). Qualitative results are presented on several tasks where paired training data does not exist, including collection  style  transfer,  object  transfiguration, season transfer,photo enhancement, etc. Quantitative comparisons against several  prior  methods  demonstrate  the  superiority  of  ourapproach. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9977608153263243
      ],
      "excerpt": "We propose a novel method for unsupervised image-to-image translation, which incorporates  a  new  attention  module  and  a  new  learnable  normalization  function  in  an  end-to-end  manner. The  attention  module  guides  our  model  to  focus  on  more  important  regions  distinguishing  between  source  and  target  domains  based  on  the  attention  map  obtained  by  the  auxiliary  classifier.   Unlike previous attention-based method which cannot handle the geometric changes between domains, our model can translate both images requiring holistic changes and images requiring large shape changes.  Moreover, our new AdaLIN (Adap-tive Layer-Instance Normalization) function helps our attention-guided model to flexibly  control  the  amount  of  change  in  shape  and  texture  by  learned  parameters  depending  on  datasets. Experimental  results  show  the  superiority  of  theproposed  method  compared  to  the  existing  state-of-the-art  models  with a fixed network architecture and hyperparameters. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/AverageName/Cycle_gan_pytorch/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 23 Dec 2021 03:11:30 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/AverageName/UI2IT/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "AverageName/UI2IT",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.9306763785941483
      ],
      "excerpt": "You can get standard config files from repo or change them by looking at lightning models argparse arguments. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8345469661239152
      ],
      "excerpt": "If you want to run training or prediction on cpu, you should initialize variable gpus with value null in your config file.   \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/AverageName/UI2IT/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Zoo of Unsupervised Image-to-Image Translation Networks (PyTorch)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "UI2IT",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "AverageName",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/AverageName/UI2IT/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "  Firstly you need to install dependencies:  \n  `pip install -r requirements.txt`  \n  ",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "  ",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 5,
      "date": "Thu, 23 Dec 2021 03:11:30 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "  Now, when you've trained some model and have your best checkpoint, you can get translated images with this line of code:  \n  `python predict.py --yaml_path <your_config_file>`  \n  As in the case of training you can write your hyperparameters in CLI by hands.  \n  ",
      "technique": "Header extraction"
    }
  ]
}