{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1810.04805>`_  was published by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.\n\nCitation\n###################\nAs of now there is no published paper on FARM. If you want to use or cite our framework, please include\nthe link to this repository. If you are working with the German Bert model, you can link our\n`blog post <https://deepset.ai/german-bert>`_ describing its training details and performance."
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8781389718055871,
        0.890734504593722
      ],
      "excerpt": ".. image:: https://img.shields.io/github/release/deepset-ai/farm \n    :target: https://github.com/deepset-ai/FARM/releases \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.810250485749543
      ],
      "excerpt": ".. image:: https://img.shields.io/github/license/deepset-ai/farm \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9714692730288251,
        0.9626356225854676
      ],
      "excerpt": ".. image:: https://img.shields.io/badge/code%20style-black-000000.svg?style=flat-square \n    :target: https://github.com/ambv/black \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.895067659672329
      ],
      "excerpt": ".. image:: https://pepy.tech/badge/farm \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8437025776904489
      ],
      "excerpt": ".. image:: https://img.shields.io/twitter/follow/deepset_ai?style=social \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8178766371425032
      ],
      "excerpt": "Parallelized preprocessing, highly modular design, multi-task learning, experiment tracking, easy debugging and close integration with AWS SageMaker. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8209707758818588
      ],
      "excerpt": "Resources &lt;https://github.com/deepset-ai/FARM#resources&gt;_ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8209707758818588
      ],
      "excerpt": "FAQ &lt;https://github.com/deepset-ai/FARM#faq&gt;_ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8444342525991423
      ],
      "excerpt": "Powerful experiment tracking & execution \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8714162992508173
      ],
      "excerpt": "| Text Regression              | x                 |  x                |  x                |  x                |  x                |  x                |  x                |  x                | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9086180918996994
      ],
      "excerpt": "Open http://localhost:3000 in your browser \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999162574988953
      ],
      "excerpt": "The original BERT model and paper &lt;https://arxiv.org/abs/1810.04805&gt;_  was published by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/deepset-ai/FARM",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-07-17T14:51:12Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-29T00:56:39Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9394449182630016
      ],
      "excerpt": "(F\\ ramework for A\\ dapting R\\ epresentation M\\ odels) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9675136012093416
      ],
      "excerpt": "IMPORTANT: We migrated the core modeling parts of FARM into `Haystack &lt;https://github.com/deepset-ai/haystack/&gt;`_. All active development is happening there and this repo is not actively maintained anymore! Go over there to ask questions &amp; create issues! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9008662938021953,
        0.8133926615538735,
        0.904914173987548
      ],
      "excerpt": "What is it? \nFARM makes Transfer Learning with BERT & Co simple, fast and enterprise-ready. \nIt's built upon transformers &lt;https://github.com/huggingface/pytorch-transformers&gt;_ and provides additional features to simplify the life of developers: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8388125014784402
      ],
      "excerpt": "With FARM you can build fast proof-of-concepts for tasks like text classification, NER or question answering and transfer them easily into production. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9422316419285707,
        0.9707895544714038
      ],
      "excerpt": "Core features \nEasy fine-tuning of language models to your task and domain language \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9491349436748975
      ],
      "excerpt": "Modular design of language models and prediction heads \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9751058476596842,
        0.9316272904313976
      ],
      "excerpt": "Full Compatibility with HuggingFace Transformers' models and model hub \nSmooth upgrading to newer language models \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8824234589437305,
        0.9440839548828455
      ],
      "excerpt": "Checkpointing & Caching to resume training and reduce costs with spot instances \nSimple deployment and visualization to showcase your model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9028241729002858
      ],
      "excerpt": "| Language Model Fine-tuning   | x                 |                   |                   |                   |                   |                   |                   |                   | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356074087953868
      ],
      "excerpt": "* including CamemBERT and UmBERTo \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8164930319558203,
        0.9553914371033273
      ],
      "excerpt": "Usecases: Custom datasets, language models, prediction heads ... \nMetrics and parameters of your model training get automatically logged via MLflow. We provide a public MLflow server &lt;https://public-mlflow.deepset.ai/&gt;_ for testing and learning purposes. Check it out to see your own experiment results! Just be aware: We will start deleting all experiments on a regular schedule to ensure decent server performance for everybody! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9048938251168256
      ],
      "excerpt": "Once you got started with FARM, there's plenty of options to customize your pipeline and boost your models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8808523420640224
      ],
      "excerpt": "With early stopping, the run stops once a chosen metric is not improving any further and you take the best model up to this point. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9698632332663945
      ],
      "excerpt": "The checkpoints include the state of everything that matters (model, optimizer, lr_schedule ...) to resume training. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8653485881447984,
        0.9765239083477237
      ],
      "excerpt": "We are currently working a lot on simplifying large scale training and deployment. As a first step, we are adding support for training on AWS SageMaker. The interesting part \nhere is the option to use Managed Spot Instances and save about 70% on costs compared to the regular EC2 instances. This is particularly relevant for training models from scratch, which we \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9110714524910963
      ],
      "excerpt": "With this modular approach you can easily add prediction heads (multitask learning) and re-use them for different types of language models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8728592049761854,
        0.8788353920043569,
        0.938799842692507
      ],
      "excerpt": "Data Processing \nCustom Datasets can be loaded by customizing the Processor. It converts \"raw data\" into PyTorch Datasets. \nMuch of the heavy lifting is then handled behind the scenes to make it fast & simple to debug. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8214646476162296
      ],
      "excerpt": "Inference Time Benchmarks \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9491916363098762
      ],
      "excerpt": "which supports 100 different languages and shows surprisingly strong performance compared to single language models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9751139957648713
      ],
      "excerpt": "PredictionHeads are needed in order to adapt the general language understanding capabilities of the language model to a specific task. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9757593555880926,
        0.9045731355241255,
        0.805674493095292,
        0.9806013567186187,
        0.9103597491612104,
        0.9507590755347953,
        0.9981305289062385
      ],
      "excerpt": "Having separate PredictionHead classes means that it is a) very easy to re-use prediction heads on top of different language models \nand b) it simplifies multitask-learning. The latter allows you e.g. to add proxy tasks that facilitate learning of your \"true objective\". \nExample: You want to classify documents into classes and know that some document tags (e.g. author) already provide helpful information for this task. It might help to add additional tasks for classifying these meta tags. \n3. When is adaptation of a language model to a domain corpus useful? \nMostly when your domain language differs a lot to the one that the original model was trained on. \nExample: Your corpus is from the aerospace industry and contains a lot of engineering terminology. \nThis is very different to Wikipedia text on in terms of vocab and semantics. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8562864073121149
      ],
      "excerpt": "4. How can I adapt a language model to a domain corpus? \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9720989981749328
      ],
      "excerpt": "It's currently most common to put a fast \"retriever\" in front of the QA model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9026300458643949
      ],
      "excerpt": "Use gradient accumulation! It combines multiple batches before applying backprop. In FARM, just set the param :code:grad_acc_steps in :code:initialize_optimizer() and :code:Trainer() to the number of batches you want to combine (i.e. :code:grad_acc_steps=2 and :code:batch_size=16 results in an effective batch size of 32). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.953907222789215,
        0.9699609456466793
      ],
      "excerpt": "FARM is built upon parts of the great Transformers &lt;https://github.com/huggingface/pytorch-transformers&gt;_  repository from HuggingFace. It utilizes their implementations of models and tokenizers. \nFARM is a community effort! Essential pieces of it have been implemented by our FARMers out there. Thanks to all contributors! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8912746867853696
      ],
      "excerpt": "the link to this repository. If you are working with the German Bert model, you can link our \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": ":house_with_garden: Fast & easy transfer learning for NLP. Harvesting language models for the industry. Focus on Question Answering.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/deepset-ai/FARM/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 212,
      "date": "Thu, 30 Dec 2021 01:21:40 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/deepset-ai/FARM/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "deepset-ai/FARM",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/deepset-ai/FARM/master/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/deepset-ai/FARM/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/deepset-ai/FARM/master/tutorials/1_farm_building_blocks.ipynb",
      "https://raw.githubusercontent.com/deepset-ai/FARM/master/tutorials/2_Build_a_processor_for_your_own_dataset.ipynb",
      "https://raw.githubusercontent.com/deepset-ai/FARM/master/tutorials/sagemaker/3_train_with_sagemaker.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8390209154878375
      ],
      "excerpt": ".. image:: https://dev.azure.com/deepset/FARM/_apis/build/status/deepset-ai.FARM?branchName=master \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9096104964140866
      ],
      "excerpt": "    :alt: Build \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.933603124367836
      ],
      "excerpt": "Installation &lt;https://github.com/deepset-ai/FARM#installation&gt;_ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9755529771184988,
        0.9906248903846466,
        0.9979947896609701,
        0.9855251805776313
      ],
      "excerpt": "git clone https://github.com/deepset-ai/FARM.git \ncd FARM \npip install -r requirements.txt \npip install --editable . \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9409591197467996,
        0.999746712887969,
        0.9991125749443739
      ],
      "excerpt": "From PyPi:: \npip install farm \nNote: On windows you might need :code:pip install farm -f https://download.pytorch.org/whl/torch_stable.html to install PyTorch correctly \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9513475990367447,
        0.8473803391977704
      ],
      "excerpt": "any optimizer from PyTorch, Apex or Transformers \nany learning rate schedule from PyTorch or Transformers \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8359299706379749
      ],
      "excerpt": "| Multilabel Text classif.     | x                 |  x                |  x                |  x                |  x                |  x                |  x                |  x                | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.897176225947782,
        0.8737873112808705,
        0.8064386133854503
      ],
      "excerpt": "Basic Usage \nTrain a downstream model \nFARM offers two modes for model training: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8064454270243467
      ],
      "excerpt": "Advanced Usage \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8015521407194528,
        0.8209658350705685
      ],
      "excerpt": "Save time if you run similar pipelines (e.g. only experimenting with model params): Store your preprocessed dataset & load it next time from cache:: \ndata_silo = DataSilo(processor=processor, batch_size=batch_size, caching=True) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "            checkpoint_on_sigterm=True, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8163872888653051
      ],
      "excerpt": "8. My GPU runs out of memory. How can I train with decent batch sizes? \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/deepset-ai/FARM/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook",
      "HTML",
      "Dockerfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Apache License 2.0",
      "url": "https://api.github.com/licenses/apache-2.0"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'\\n                                 Apache License\\n                           Version 2.0, January 2004\\n                        http://www.apache.org/licenses/\\n\\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\\n\\n   1. Definitions.\\n\\n      \"License\" shall mean the terms and conditions for use, reproduction,\\n      and distribution as defined by Sections 1 through 9 of this document.\\n\\n      \"Licensor\" shall mean the copyright owner or entity authorized by\\n      the copyright owner that is granting the License.\\n\\n      \"Legal Entity\" shall mean the union of the acting entity and all\\n      other entities that control, are controlled by, or are under common\\n      control with that entity. For the purposes of this definition,\\n      \"control\" means (i) the power, direct or indirect, to cause the\\n      direction or management of such entity, whether by contract or\\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\\n      outstanding shares, or (iii) beneficial ownership of such entity.\\n\\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\\n      exercising permissions granted by this License.\\n\\n      \"Source\" form shall mean the preferred form for making modifications,\\n      including but not limited to software source code, documentation\\n      source, and configuration files.\\n\\n      \"Object\" form shall mean any form resulting from mechanical\\n      transformation or translation of a Source form, including but\\n      not limited to compiled object code, generated documentation,\\n      and conversions to other media types.\\n\\n      \"Work\" shall mean the work of authorship, whether in Source or\\n      Object form, made available under the License, as indicated by a\\n      copyright notice that is included in or attached to the work\\n      (an example is provided in the Appendix below).\\n\\n      \"Derivative Works\" shall mean any work, whether in Source or Object\\n      form, that is based on (or derived from) the Work and for which the\\n      editorial revisions, annotations, elaborations, or other modifications\\n      represent, as a whole, an original work of authorship. For the purposes\\n      of this License, Derivative Works shall not include works that remain\\n      separable from, or merely link (or bind by name) to the interfaces of,\\n      the Work and Derivative Works thereof.\\n\\n      \"Contribution\" shall mean any work of authorship, including\\n      the original version of the Work and any modifications or additions\\n      to that Work or Derivative Works thereof, that is intentionally\\n      submitted to Licensor for inclusion in the Work by the copyright owner\\n      or by an individual or Legal Entity authorized to submit on behalf of\\n      the copyright owner. For the purposes of this definition, \"submitted\"\\n      means any form of electronic, verbal, or written communication sent\\n      to the Licensor or its representatives, including but not limited to\\n      communication on electronic mailing lists, source code control systems,\\n      and issue tracking systems that are managed by, or on behalf of, the\\n      Licensor for the purpose of discussing and improving the Work, but\\n      excluding communication that is conspicuously marked or otherwise\\n      designated in writing by the copyright owner as \"Not a Contribution.\"\\n\\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\\n      on behalf of whom a Contribution has been received by Licensor and\\n      subsequently incorporated within the Work.\\n\\n   2. Grant of Copyright License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      copyright license to reproduce, prepare Derivative Works of,\\n      publicly display, publicly perform, sublicense, and distribute the\\n      Work and such Derivative Works in Source or Object form.\\n\\n   3. Grant of Patent License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      (except as stated in this section) patent license to make, have made,\\n      use, offer to sell, sell, import, and otherwise transfer the Work,\\n      where such license applies only to those patent claims licensable\\n      by such Contributor that are necessarily infringed by their\\n      Contribution(s) alone or by combination of their Contribution(s)\\n      with the Work to which such Contribution(s) was submitted. If You\\n      institute patent litigation against any entity (including a\\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\\n      or a Contribution incorporated within the Work constitutes direct\\n      or contributory patent infringement, then any patent licenses\\n      granted to You under this License for that Work shall terminate\\n      as of the date such litigation is filed.\\n\\n   4. Redistribution. You may reproduce and distribute copies of the\\n      Work or Derivative Works thereof in any medium, with or without\\n      modifications, and in Source or Object form, provided that You\\n      meet the following conditions:\\n\\n      (a) You must give any other recipients of the Work or\\n          Derivative Works a copy of this License; and\\n\\n      (b) You must cause any modified files to carry prominent notices\\n          stating that You changed the files; and\\n\\n      (c) You must retain, in the Source form of any Derivative Works\\n          that You distribute, all copyright, patent, trademark, and\\n          attribution notices from the Source form of the Work,\\n          excluding those notices that do not pertain to any part of\\n          the Derivative Works; and\\n\\n      (d) If the Work includes a \"NOTICE\" text file as part of its\\n          distribution, then any Derivative Works that You distribute must\\n          include a readable copy of the attribution notices contained\\n          within such NOTICE file, excluding those notices that do not\\n          pertain to any part of the Derivative Works, in at least one\\n          of the following places: within a NOTICE text file distributed\\n          as part of the Derivative Works; within the Source form or\\n          documentation, if provided along with the Derivative Works; or,\\n          within a display generated by the Derivative Works, if and\\n          wherever such third-party notices normally appear. The contents\\n          of the NOTICE file are for informational purposes only and\\n          do not modify the License. You may add Your own attribution\\n          notices within Derivative Works that You distribute, alongside\\n          or as an addendum to the NOTICE text from the Work, provided\\n          that such additional attribution notices cannot be construed\\n          as modifying the License.\\n\\n      You may add Your own copyright statement to Your modifications and\\n      may provide additional or different license terms and conditions\\n      for use, reproduction, or distribution of Your modifications, or\\n      for any such Derivative Works as a whole, provided Your use,\\n      reproduction, and distribution of the Work otherwise complies with\\n      the conditions stated in this License.\\n\\n   5. Submission of Contributions. Unless You explicitly state otherwise,\\n      any Contribution intentionally submitted for inclusion in the Work\\n      by You to the Licensor shall be under the terms and conditions of\\n      this License, without any additional terms or conditions.\\n      Notwithstanding the above, nothing herein shall supersede or modify\\n      the terms of any separate license agreement you may have executed\\n      with Licensor regarding such Contributions.\\n\\n   6. Trademarks. This License does not grant permission to use the trade\\n      names, trademarks, service marks, or product names of the Licensor,\\n      except as required for reasonable and customary use in describing the\\n      origin of the Work and reproducing the content of the NOTICE file.\\n\\n   7. Disclaimer of Warranty. Unless required by applicable law or\\n      agreed to in writing, Licensor provides the Work (and each\\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\\n      implied, including, without limitation, any warranties or conditions\\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\\n      PARTICULAR PURPOSE. You are solely responsible for determining the\\n      appropriateness of using or redistributing the Work and assume any\\n      risks associated with Your exercise of permissions under this License.\\n\\n   8. Limitation of Liability. In no event and under no legal theory,\\n      whether in tort (including negligence), contract, or otherwise,\\n      unless required by applicable law (such as deliberate and grossly\\n      negligent acts) or agreed to in writing, shall any Contributor be\\n      liable to You for damages, including any direct, indirect, special,\\n      incidental, or consequential damages of any character arising as a\\n      result of this License or out of the use or inability to use the\\n      Work (including but not limited to damages for loss of goodwill,\\n      work stoppage, computer failure or malfunction, or any and all\\n      other commercial damages or losses), even if such Contributor\\n      has been advised of the possibility of such damages.\\n\\n   9. Accepting Warranty or Additional Liability. While redistributing\\n      the Work or Derivative Works thereof, You may choose to offer,\\n      and charge a fee for, acceptance of support, warranty, indemnity,\\n      or other liability obligations and/or rights consistent with this\\n      License. However, in accepting such obligations, You may act only\\n      on Your own behalf and on Your sole responsibility, not on behalf\\n      of any other Contributor, and only if You agree to indemnify,\\n      defend, and hold each Contributor harmless for any liability\\n      incurred by, or claims asserted against, such Contributor by reason\\n      of your accepting any such warranty or additional liability.\\n\\n   END OF TERMS AND CONDITIONS\\n\\n   APPENDIX: How to apply the Apache License to your work.\\n\\n      To apply the Apache License to your work, attach the following\\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\\n      replaced with your own identifying information. (Don\\'t include\\n      the brackets!)  The text should be enclosed in the appropriate\\n      comment syntax for the file format. We also recommend that a\\n      file or class name and description of purpose be included on the\\n      same \"printed page\" as the copyright notice for easier\\n      identification within third-party archives.\\n\\n   Copyright [yyyy] [name of copyright owner]\\n\\n   Licensed under the Apache License, Version 2.0 (the \"License\");\\n   you may not use this file except in compliance with the License.\\n   You may obtain a copy of the License at\\n\\n       http://www.apache.org/licenses/LICENSE-2.0\\n\\n   Unless required by applicable law or agreed to in writing, software\\n   distributed under the License is distributed on an \"AS IS\" BASIS,\\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n   See the License for the specific language governing permissions and\\n   limitations under the License.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "###########",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "FARM",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "deepset-ai",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/deepset-ai/FARM/blob/master/readme.rst",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "julian-risch",
        "body": "### DPR Improvements\r\nDPR - improve loading of datasets #733 @voidful\r\nDPR - enable saving and loading of other model types, e.g., RoBERTa models #765 @Timoeller @julian-risch\r\nDPR - fix conversion of BiAdaptiveModel #753 @bogdankostic \r\n\r\n### torch 1.8.1 and transformers 4.6.1\r\nBump transformers version to 4.6.1 #787 @Timoeller @julian-risch\r\nBump torch version to 1.8.1 #767 @Timoeller @julian-risch\r\n\r\n### Multi-task Learning\r\nImplement Multi-task Learning and added example #778 @johann-petrak \r\n\r\n### List of Evaluation Metrics\r\nAllow list of metrics and add tests and pythondoc #777 @johann-petrak \r\n\r\n### Misc\r\nReduce number of logging messages by Processor about returning problematic ids #772 @johann-petrak\r\nAdd farm.\\_\\_version\\_\\_ tag #761 @johann-petrak \r\nAdd value of doc_stride, max_seq_len, max_query_length in error message #784 @ftesser \r\nConvert QACandidates with empty or whitespace answers to no_answers on doc level #756 @julian-risch \r\n\r\nString comparison: Should replace \"is\" with \"==\": #774 @johann-petrak \r\nFix reference before assignment in DataSilo #738 @bogdankostic\r\nChanging QA_input format in tutorial #735 @julian-risch \r\nFix TextPairClassificationProcessor example by adding metric #780 @julian-risch ",
        "dateCreated": "2021-06-10T09:36:26Z",
        "datePublished": "2021-06-10T09:45:12Z",
        "html_url": "https://github.com/deepset-ai/FARM/releases/tag/v0.8.0",
        "name": "v0.8.0 DPR Improvements",
        "tag_name": "v0.8.0",
        "tarball_url": "https://api.github.com/repos/deepset-ai/FARM/tarball/v0.8.0",
        "url": "https://api.github.com/repos/deepset-ai/FARM/releases/44332034",
        "zipball_url": "https://api.github.com/repos/deepset-ai/FARM/zipball/v0.8.0"
      },
      {
        "authorType": "User",
        "author_name": "Timoeller",
        "body": "A patch release focusing on bug fixes for Dense Passage Retrieval\r\nDPR\r\nFix saving and loading of DPR models and Processors in #746\r\nFix DPR tokenization statisticss in #738\r\nFix cosine similarity in DPR training #741\r\n\r\nMisc\r\nFix tuple input for TextPairClassification inference #723\r\n",
        "dateCreated": "2021-03-31T14:30:32Z",
        "datePublished": "2021-03-31T14:33:39Z",
        "html_url": "https://github.com/deepset-ai/FARM/releases/tag/v0.7.1",
        "name": "DPR patch release",
        "tag_name": "v0.7.1",
        "tarball_url": "https://api.github.com/repos/deepset-ai/FARM/tarball/v0.7.1",
        "url": "https://api.github.com/repos/deepset-ai/FARM/releases/40763796",
        "zipball_url": "https://api.github.com/repos/deepset-ai/FARM/zipball/v0.7.1"
      },
      {
        "authorType": "User",
        "author_name": "julian-risch",
        "body": "### QA Confidence Scores\r\nIn response to several requests from the community, we now provide more meaningful confidence scores for the predictions of extractive QA models. #690 #705 @julian-risch @timoeller @lalitpagaria \r\nTo this end, predicted answers got a new attribute called confidence, which is in the range [0,1] and can be calibrated with the probability that a prediction is an exact match. The intuition behind the scores is the following: After calibration, if the average confidence of 100 predictions is 70%, then on average 70 of the predictions will be correct.\r\nThe implementation of the calibration uses a technique called temperature scaling. The calibration can be executed on a dev set by running the eval() method in the Evaluator class and setting the parameter calibrate_conf_scores to true. This parameter is false by default as it is still an experimental feature and we continue working on it. The score attribute of predicted answers and their ranking remain unchanged so that the default behavior is unchanged.\r\nAn [example](https://github.com/deepset-ai/FARM/blob/master/examples/question_answering_confidence.py) shows how to calibrate and use the confidence scores. \r\n### Misc \r\nRefactor Text pair handling, that also add Text pair regression #713 @timoeller\r\nRefactor Textsimilarity processor #711 @timoeller\r\nRefactor Regression and inference processors #702 @timoeller\r\nFix NER probabilities #700 @brandenchan\r\nCalculate squad evaluation metrics overall and separately for text answers and no answers #698 @julian-risch\r\nRe-enable test_dpr_modules also for windows #697 @ftesser\r\nUse Path instead of String in ONNXAdaptiveModel #694 @skiran252\r\nBig thanks to all contributors!  ",
        "dateCreated": "2021-02-22T17:40:57Z",
        "datePublished": "2021-02-22T17:44:33Z",
        "html_url": "https://github.com/deepset-ai/FARM/releases/tag/v0.7.0",
        "name": "v0.7.0 QA Model Confidence",
        "tag_name": "v0.7.0",
        "tarball_url": "https://api.github.com/repos/deepset-ai/FARM/tarball/v0.7.0",
        "url": "https://api.github.com/repos/deepset-ai/FARM/releases/38438560",
        "zipball_url": "https://api.github.com/repos/deepset-ai/FARM/zipball/v0.7.0"
      },
      {
        "authorType": "User",
        "author_name": "Timoeller",
        "body": "This is just a small patch to change the return types of offsets in our QAInferencer, see #693 \r\n\r\nIt is needed to fix RestAPI related issues where int64 cannot decoded within JSONs.",
        "dateCreated": "2021-01-20T16:49:42Z",
        "datePublished": "2021-01-20T16:57:05Z",
        "html_url": "https://github.com/deepset-ai/FARM/releases/tag/v0.6.2",
        "name": "Patch Release - Bugfix QA Inference",
        "tag_name": "v0.6.2",
        "tarball_url": "https://api.github.com/repos/deepset-ai/FARM/tarball/v0.6.2",
        "url": "https://api.github.com/repos/deepset-ai/FARM/releases/36668471",
        "zipball_url": "https://api.github.com/repos/deepset-ai/FARM/zipball/v0.6.2"
      },
      {
        "authorType": "User",
        "author_name": "Timoeller",
        "body": "## Patch release\r\nThis is just a quick patch release to bugfix some input validation for Question Answering\r\n[**closed**] Fix/missing truncation bug [#679](https://github.com/deepset-ai/FARM/pull/679)\r\n\r\n## Additional feature for QA\r\nStill, another interesting feature slipped in: We can now filter QA predictions to not contain duplicate answers.\r\n[**closed**] Added filter_range parameter that allows to filter answers with similar start/end indices [#680](https://github.com/deepset-ai/FARM/pull/680)\r\n\r\n## Additional test\r\n[**part: tokenizer**][**task: QA**] Add integration test for QA processing [#683](https://github.com/deepset-ai/FARM/pull/683)\r\n\r\n## Misc\r\n[**closed**] Remove \"qas\" inference input wherever possible [#681](https://github.com/deepset-ai/FARM/pull/681)\r\n[**closed**] Added parameter names to convert_from_transformers call in question_answering_crossvalidation.py [#672](https://github.com/deepset-ai/FARM/pull/672)",
        "dateCreated": "2021-01-12T17:19:00Z",
        "datePublished": "2021-01-12T17:34:55Z",
        "html_url": "https://github.com/deepset-ai/FARM/releases/tag/v0.6.1",
        "name": "Patch Release - Bugfix QA Processing",
        "tag_name": "v0.6.1",
        "tarball_url": "https://api.github.com/repos/deepset-ai/FARM/tarball/v0.6.1",
        "url": "https://api.github.com/repos/deepset-ai/FARM/releases/36292847",
        "zipball_url": "https://api.github.com/repos/deepset-ai/FARM/zipball/v0.6.1"
      },
      {
        "authorType": "User",
        "author_name": "Timoeller",
        "body": "# Simplification of Preprocessing\r\nWe wanted to make preprocessing for all our tasks (e.g. QA, DPR, NER, classification) more understandable for FARM users, so that it is easier to adjust to specific use cases or extend the functionality to new tasks.\r\n\r\nTo achieve this we followed two design choices: \r\n1. Avoid deeply nested calls\r\n2. Keep all high-level descriptions in a single place\r\n\r\n## Question Answering Preprocessing\r\nWe especially focussed on making QA processing more sequential and divided the code into meaningful snippets #649 \r\n\r\nThe code snippets are (see related [method](https://github.com/deepset-ai/FARM/blob/master/farm/data_handler/processor.py#L1883)):\r\n- convert the input into FARM specific QA format\r\n- tokenize the questions and texts\r\n- split texts into passages to fit the sequence length constraint of Language Models\r\n- [optionally] convert labels (disabled during inference)\r\n- convert question, text, labels and additional information to PyTorch tensors\r\n\r\n# Breaking changes\r\n1. Switching to FastTokenizers (based on Huggingface tokenizer project written in Rust) as default Tokenizer. We changed the `use_fast=True` parameter in the [Tokenizer.load()](https://github.com/deepset-ai/FARM/blob/master/farm/modeling/tokenization.py#L58) method. Support for slow, python-based Tokenizers will be implemented for all tasks in the next release.\r\n2. The Processor.dataset_from_dicts method by default returns an additional parameter `problematic_sample_ids` that keeps track of which input sample caused problems during preprocessing:\r\n```\r\ndataset, tensor_names, problematic_sample_ids = processor.dataset_from_dicts(dicts=dicts)\r\n```\r\n\r\n# Update to transformers version 4.1.1 and torch version 1.7.0\r\nTransformers comes with many new features, including model versioning, that we do not want to miss out on. #665 \r\nModel versions can now be specified like:\r\n```\r\n    model = Inferencer.load(\r\n        model_name_or_path=\"deepset/roberta-base-squad2\",\r\n        revision=\"v2.0\",\r\n        task_type=\"question_answering\",\r\n    )\r\n```\r\n\r\n# DPR enhancements\r\n- MultiGPU support #619 \r\n- Added tests #643\r\n- Bugfixes and smaller enhancements #629 #655 #663 \r\n\r\n# Misc\r\n- Cleaner logging and error handling #639\r\n- Benchmark automation via CML #646\r\n- Disable DPR tests on Windows, since they do not work with PyTorch 1.6.1 #637\r\n- Option to disable MLflow logger #650\r\n- Fix to Earlystopping and custom head #617\r\n- Adding probability of masking a token parameter for LM task  #630\r\n\r\nBig thanks to all contributors!\r\n@ftesser @pashok3d @Timoeller @tanaysoni @brandenchan @bogdankostic @kolk @tholor",
        "dateCreated": "2020-12-29T18:30:12Z",
        "datePublished": "2020-12-30T11:45:23Z",
        "html_url": "https://github.com/deepset-ai/FARM/releases/tag/v0.6.0",
        "name": "",
        "tag_name": "v0.6.0",
        "tarball_url": "https://api.github.com/repos/deepset-ai/FARM/tarball/v0.6.0",
        "url": "https://api.github.com/repos/deepset-ai/FARM/releases/35820380",
        "zipball_url": "https://api.github.com/repos/deepset-ai/FARM/zipball/v0.6.0"
      },
      {
        "authorType": "User",
        "author_name": "tholor",
        "body": "## Add Dense Passage Retriever (DPR) incl. Training & Inference (#513, #601, #606)\r\nHappy to introduce a completely new task type to FARM: Text similarity with two separate transformer encoders\r\n\r\n**Why?** \r\nWe observe a big shift in Information Retrieval from sparse methods (BM25 etc.) towards dense methods that encode queries and docs as vectors and use vector similarity to retrieve the most similar docs for a certain query. This is not only helpful for document search but also for open-domain Question Answering. Dense methods outperform sparse methods already in many domains and are especially powerful if the matching between query and passage cannot happen via \"keywords\" but rather relies on semantics / synonyms / context.  \r\n\r\n**What?**\r\nOne of the most promising methods at the moment is \"Dense Passage Retrieval\" from Karphukin et al. (https://arxiv.org/abs/2004.04906). In a nutshell, DPR uses one transformer to encode the query and a second transformer to encode the passage. The two encoders project the different texts into the same vector space and are trained jointly on a similarity measure using in-batch-negatives.\r\n\r\n**How?** \r\nWe introduce a new class `BiAdaptiveModel` that has two language models plus a prediction head.\r\nIn the case of DPR, this will be one question encoder model and one passage encoder model.  \r\nSee the new example script [dpr_encoder.py](https://github.com/deepset-ai/FARM/blob/master/examples/dpr_encoder.py) for training / fine-tuning a DPR model.\r\nWe also have a tight integration in Haystack, where you can use it as a Retriever for open-domain Question Answering. \r\n\r\n## Refactor conversion from / to Transformers #576\r\nWe simplified conversion between FARM <-> Transformers. You can now run: \r\n\r\n```python\r\n# Transformers -> FARM\r\nmodel = Converter.convert_from_transformers(\"deepset/roberta-base-squad2\", device=\"cpu\")\r\n\r\n# FARM -> Transformers\r\ntransformer_models = Converter.convert_to_transformers(your_adaptive_model)\r\n```\r\nNote: In case your FARM AdaptiveModel has multiple prediction heads (e.g. 1x NER, 1x Text Classification), the conversion will return a list with two transformer models (both with one head respectively).\r\n\r\n## Upgrade to Transformers 3.3.1 #579\r\nTransformers 3.3.1 comes with a few new interesting features, incl. support for Retrieval-Augmented Generation (RAG) which can be used to **generate** answers rather than **extracting** answers. In contrast to GPT-3, the generation is conditioned on a set of retrieved documents, and is, therefore, more suitable for most QA applications in the industry that rely on a domain corpus.  \r\nThanks to @lalitpagaria, we'll support RAG also in Haystack soon (see https://github.com/deepset-ai/haystack/pull/484) \r\n\r\n\r\n----------------------------\r\n## Details\r\n\r\n### Question Answering\r\n- Improve Speed: Vectorize Question Answering Prediction Head [#603](https://github.com/deepset-ai/FARM/pull/603)\r\n- Fix removal of yes no answers [#540](https://github.com/deepset-ai/FARM/pull/540)\r\n- Fix QA bug that rejected spans at beginning of passage [#564](https://github.com/deepset-ai/FARM/pull/564)\r\n- Added warning about that Natural Questions Inference.  [#565](https://github.com/deepset-ai/FARM/pull/565)\r\n- Remove loss index from QA PH [#589](https://github.com/deepset-ai/FARM/pull/589)\r\n\r\n### Other\r\n-  Catch empty datasets in Inferencer [#605](https://github.com/deepset-ai/FARM/pull/605)\r\n-  Add option to set evaluation batch size [#607](https://github.com/deepset-ai/FARM/pull/607)\r\n-  Infer model type from config [#600](https://github.com/deepset-ai/FARM/pull/600)\r\n-  Fix random behavior when loading ELECTRA models [#599](https://github.com/deepset-ai/FARM/pull/599)\r\n-  Fix import for Python3.6 [#581](https://github.com/deepset-ai/FARM/pull/581)\r\n-  Fixed conversion of BertForMaskedLM to transformers [#555](https://github.com/deepset-ai/FARM/pull/555)\r\n-  Load correct config for DistilBert model [#562](https://github.com/deepset-ai/FARM/pull/562)\r\n-  Add passages per second calculation to benchmarks [#560](https://github.com/deepset-ai/FARM/pull/560)\r\n-  Fix batching in ONNX forward pass [#559](https://github.com/deepset-ai/FARM/pull/559)\r\n-  Add ONNX conversion & Inference [#557](https://github.com/deepset-ai/FARM/pull/557)\r\n\r\n\r\nBig thanks to all contributors!\r\n@ftesser @lalitpagaria @himanshurawlani @Timoeller @tanaysoni @brandenchan @bogdankostic @kolk @tholor \r\n\r\n",
        "dateCreated": "2020-10-30T16:12:12Z",
        "datePublished": "2020-10-30T16:51:53Z",
        "html_url": "https://github.com/deepset-ai/FARM/releases/tag/v0.5.0",
        "name": "v0.5.0",
        "tag_name": "v0.5.0",
        "tarball_url": "https://api.github.com/repos/deepset-ai/FARM/tarball/v0.5.0",
        "url": "https://api.github.com/repos/deepset-ai/FARM/releases/33279597",
        "zipball_url": "https://api.github.com/repos/deepset-ai/FARM/zipball/v0.5.0"
      },
      {
        "authorType": "User",
        "author_name": "tholor",
        "body": "## Minor patch: Relax PyTorch version requirements \r\n\r\nInstalling FARM in environments where torch's GPU version was already installed via pip (e.g. torch 1.6.0+cu101), caused version trouble. This is especially annoying in Google Colab environments.\r\nChange: Allow all torch 1.6.x versions incl 1.6.0+cu101 etc\r\n\r\n-------------------------------\r\nFurther changes: \r\n- Nested cross validation by @PhilipMay [#508](https://github.com/deepset-ai/FARM/pull/508)\r\n",
        "dateCreated": "2020-09-21T07:53:43Z",
        "datePublished": "2020-09-21T07:59:50Z",
        "html_url": "https://github.com/deepset-ai/FARM/releases/tag/v0.4.9",
        "name": "v0.4.9",
        "tag_name": "v0.4.9",
        "tarball_url": "https://api.github.com/repos/deepset-ai/FARM/tarball/v0.4.9",
        "url": "https://api.github.com/repos/deepset-ai/FARM/releases/31590191",
        "zipball_url": "https://api.github.com/repos/deepset-ai/FARM/zipball/v0.4.9"
      },
      {
        "authorType": "User",
        "author_name": "tholor",
        "body": "# Minor release \r\n\r\n### Experimental Support for fast Rust Tokenizers (#482)\r\nWhile preprocessing is usually not the bottleneck in our pipelines, there's still significant time spent on it (~ 20 % for QA inference). We saw substantial speed-ups with HuggingFace's \"FastTokenizers\" that are based on rust. We are therefore introducing a basic \"experimental\" implementation with this release. We are planning to stabilizing it and having a smoother fit into the FARM processor.\r\n\r\nUsage: \r\n```python\r\ntokenizer = Tokenizer.load(pretrained_model_name_or_path=\"\"bert-base-german-cased\"\",\r\n                           do_lower_case=False, \r\n                           use_fast=True)\r\n```\r\n### Upgrade to transformers 3.1.0 (#464)\r\nThe latest transformers release has quite interesting new features - one of them being basic support of a DPR model class (Dense Passage Retriever). This will simplify our dense passage retriever integration in Haystack and the upcoming DPR training which we plan to have in FARM.  \r\n\r\n----------------------------\r\n## Details\r\n\r\n### Question Answering\r\n- Add asserts on doc_stride and max_seq_len to prevent issues with sliding window [#538](https://github.com/deepset-ai/FARM/pull/538)\r\n- fix Natural Question inference processing [#521](https://github.com/deepset-ai/FARM/pull/521)\r\n\r\n### Other\r\n- Fix logging of error msg for FastTokenizer + QA [#541](https://github.com/deepset-ai/FARM/pull/541)\r\n- Fix truncation warnings in tokenizer [#528](https://github.com/deepset-ai/FARM/pull/528)\r\n- Evaluate model on best model when doing early stopping [#524](https://github.com/deepset-ai/FARM/pull/524)\r\n- Bump transformers version to 3.1.0 [#515](https://github.com/deepset-ai/FARM/pull/515)\r\n- Add warmup run to component benchmark [#504](https://github.com/deepset-ai/FARM/pull/504)\r\n- Add optional s3 auth via params [#511](https://github.com/deepset-ai/FARM/pull/511)\r\n- Add option to use fast HF tokenizer. [#482](https://github.com/deepset-ai/FARM/pull/482)\r\n- CodeBERT support for embeddings [#488](https://github.com/deepset-ai/FARM/pull/488)\r\n- Store test eval result in variable [#506](https://github.com/deepset-ai/FARM/pull/506)\r\n- Fix typo f1 micro vs. macro [#505](https://github.com/deepset-ai/FARM/pull/505)\r\n\r\nBig thanks to all contributors!\r\n@PhilipMay @lambdaofgod @Timoeller @tanaysoni @brandenchan @bogdankostic @kolk @tholor \r\n",
        "dateCreated": "2020-09-14T14:58:30Z",
        "datePublished": "2020-09-14T15:23:55Z",
        "html_url": "https://github.com/deepset-ai/FARM/releases/tag/0.4.8",
        "name": "0.4.8",
        "tag_name": "0.4.8",
        "tarball_url": "https://api.github.com/repos/deepset-ai/FARM/tarball/0.4.8",
        "url": "https://api.github.com/repos/deepset-ai/FARM/releases/31264125",
        "zipball_url": "https://api.github.com/repos/deepset-ai/FARM/zipball/0.4.8"
      },
      {
        "authorType": "User",
        "author_name": "tholor",
        "body": "## Main changes\r\n\r\n### Support for MiniLM Model (#464)\r\nInteresting model from Microsoft that is up to 2.7x faster than BERT, while showing similar or better performance on many tasks ([Paper](https://arxiv.org/abs/2002.10957)). We found it particularly interesting for QA and also published a fine-tuned model on SQuAD 2.0: [deepset/minilm-uncased-squad2](https://huggingface.co/deepset/minilm-uncased-squad2)\r\n\r\n### Benchmarks per component (#491)\r\nMeasuring the speed of individual components in the pipeline while respecting CUDA's async behaviour. We were especially interested in analyzing how much time we spend for QA in preprocessing, language model, and prediction head. Turns out it's on average about 20% : 50% : 30%. Interestingly, there's a high variance in the prediction head depending on the relevance of the question. We will use that information to further optimize performance in the prediction head. We'll share more detailed benchmarks soon.  \r\n\r\n### Support for PyTorch 1.6 (#502)\r\nWe now support 1.6 and 1.5.1\r\n\r\n\r\n----------------------------\r\n## Details\r\n\r\n### Question Answering\r\n- Pass max_answers param to processor [#503](https://github.com/deepset-ai/FARM/pull/503)\r\n- Deprecate QA input dicts with [context, qas] as keys [#472](https://github.com/deepset-ai/FARM/pull/472)\r\n- Squad processor verbose feature [#470](https://github.com/deepset-ai/FARM/pull/470)\r\n- Propagate QA ground truth in Inferencer [#469](https://github.com/deepset-ai/FARM/pull/469)\r\n- Ensure QAInferencer always has task_type \"question_answering\" [#460](https://github.com/deepset-ai/FARM/pull/460)\r\n\r\n### Other\r\n- Download models from (private) S3 [#500](https://github.com/deepset-ai/FARM/pull/500)\r\n- fix _initialize_data_loaders in data_silo [#476](https://github.com/deepset-ai/FARM/pull/476)\r\n- Remove torch version wildcard in requirements [#489](https://github.com/deepset-ai/FARM/pull/489)\r\n- Make num processes parameter consistent across inferencer and data silo [#480](https://github.com/deepset-ai/FARM/pull/480)\r\n- Remove rest_api_schema argument in inference_from_dicts() [#474](https://github.com/deepset-ai/FARM/pull/474)\r\n- farm.data_handler.utils: Add encoding to open write in split_file method [#466](https://github.com/deepset-ai/FARM/pull/466)\r\n- Fix and document Inferencer usage and pool handling [#429](https://github.com/deepset-ai/FARM/pull/429)\r\n- Remove assertions or replace with logging error [#468](https://github.com/deepset-ai/FARM/pull/468)\r\n- Remove baskets without features in _create_dataset. [#471](https://github.com/deepset-ai/FARM/pull/471)\r\n- fix bugs with regression label standardization [#456](https://github.com/deepset-ai/FARM/pull/456)\r\n\r\nBig thanks to all contributors!\r\n@PhilipMay @Timoeller @tanaysoni @brandenchan @bogdankostic @kolk @rohanag @lingsond @ftesser \r\n",
        "dateCreated": "2020-08-27T15:06:16Z",
        "datePublished": "2020-08-27T15:38:23Z",
        "html_url": "https://github.com/deepset-ai/FARM/releases/tag/0.4.7",
        "name": "0.4.7",
        "tag_name": "0.4.7",
        "tarball_url": "https://api.github.com/repos/deepset-ai/FARM/tarball/0.4.7",
        "url": "https://api.github.com/repos/deepset-ai/FARM/releases/30225736",
        "zipball_url": "https://api.github.com/repos/deepset-ai/FARM/zipball/0.4.7"
      },
      {
        "authorType": "User",
        "author_name": "tholor",
        "body": "## Main changes\r\n- Upgrading to Pytorch 1.5.1 and transformers 3.0.2\r\n- Important bug fix for language model training from scratch\r\n- Bug fixes and big refactorings for Question Answering, incl. a specialized QAInferencer with dedicated In- and Output objects to simplify usage and code completion:\r\n```\r\nfrom farm.infer import QAInferencer\r\nfrom farm.data_handler.inputs import QAInput, Question\r\n\r\nnlp = QAInferencer.load(\r\n    \"deepset/roberta-base-squad2\",\r\n    task_type=\"question_answering\",\r\n    batch_size=16,\r\n    num_processes=0)\r\n\r\ninput = QAInput(\r\n    doc_text=\"My name is Lucas and I live on Mars.\",\r\n    questions=Question(text=\"Who lives on Mars?\",\r\n                       uid=\"your-id\"))\r\n\r\nres = nlp.inference_from_objects([input], return_json=False)[0]\r\n\r\n# High level attributes for your query\r\nprint(res.question)\r\nprint(res.context)\r\nprint(res.no_answer_gap)\r\n# ...\r\n# Attributes for individual predictions (= answers)\r\npred = res.prediction[0]\r\nprint(pred.answer)\r\nprint(pred.answer_type)\r\nprint(pred.answer_support)\r\nprint(pred.offset_answer_start)\r\nprint(pred.offset_answer_end)\r\n# ...\r\n``` \r\n \r\n------------------------------------------------------\r\n## Details \r\n### Question Answering\r\n- Add meta attribute to QACandidate for Haystack [#455](https://github.com/deepset-ai/FARM/pull/455)\r\n- Fix start and end offset checks in QA [#450](https://github.com/deepset-ai/FARM/pull/450)\r\n- Fix offset_end character for QA [#449](https://github.com/deepset-ai/FARM/pull/449)\r\n- Dedicated Input Objects for QA [#445](https://github.com/deepset-ai/FARM/pull/445)\r\n- Question Answering improvements: cleaner code, more typed objects, better compatibility between SQuAD and Natural Questions [#411](https://github.com/deepset-ai/FARM/pull/411), [#438](https://github.com/deepset-ai/FARM/pull/438), [#419](https://github.com/deepset-ai/FARM/pull/419)\r\n\r\n### Other\r\n- Upgrade pytorch and python versions [#447](https://github.com/deepset-ai/FARM/pull/447)\r\n- Upgrade transformers version [#448](https://github.com/deepset-ai/FARM/pull/448)\r\n- Fix randomisation of train file for training from scratch [#427](https://github.com/deepset-ai/FARM/pull/427)\r\n- Fix loading of saved models with class weights [#431](https://github.com/deepset-ai/FARM/pull/431)\r\n- Remove raising exception errors in processor [#451](https://github.com/deepset-ai/FARM/pull/451)\r\n- Fix bug in benchmark tests with if statement [#430](https://github.com/deepset-ai/FARM/pull/430)\r\n- Remove hardcoded seeds from trainer [#424](https://github.com/deepset-ai/FARM/pull/424)\r\n- Conditional num_training_steps setting [#437](https://github.com/deepset-ai/FARM/pull/437)\r\n- Add badge with link to doc page [#432](https://github.com/deepset-ai/FARM/pull/432)\r\n- Fix for CI problem - closing multiprocessing.pool again [#403](https://github.com/deepset-ai/FARM/pull/403)\r\n\r\n:man_farmer:  :woman_farmer: Thanks to all contributors for making FARMer's life better!\r\n @PhilipMay, @tstadel,  @brandenchan, @tanaysoni, @Timoeller, @tholor, @bogdankostic",
        "dateCreated": "2020-07-10T12:39:54Z",
        "datePublished": "2020-07-10T13:28:35Z",
        "html_url": "https://github.com/deepset-ai/FARM/releases/tag/0.4.6",
        "name": "0.4.6",
        "tag_name": "0.4.6",
        "tarball_url": "https://api.github.com/repos/deepset-ai/FARM/tarball/0.4.6",
        "url": "https://api.github.com/repos/deepset-ai/FARM/releases/28440469",
        "zipball_url": "https://api.github.com/repos/deepset-ai/FARM/zipball/0.4.6"
      },
      {
        "authorType": "User",
        "author_name": "tanaysoni",
        "body": "Minor release including an important bug fix for Question Answering\r\n\r\n### Important Bug Fix QA \r\nFixing a bug that was introduced in 0.4.4 (#416 and #417) that resulted in returning only a single answer per document in certain situations. This caused particular trouble for open-domain QA settings like in [haystack](https://github.com/deepset-ai/haystack/). \r\n\r\n### Speed optimization training from scratch \r\nAdding multiple optimizations and bug fixes to improve training from scratch, incl.:\r\n- Enable usage of DistributedDataParallel\r\n- Enable Automatix Mixed Precision Training\r\n- Fix bugs in StreamingDataSilo\r\n- Fix bugs in Checkpointing (important for training via spot / on-demand instances)\r\n\r\nThis helped to boost training time in our benchmark from 616 hours down to 160 hours\r\nSee [#305](https://github.com/deepset-ai/FARM/pull/305) for details\r\n\r\n---------------------------------\r\n#### Other changes:\r\n- Add model optimization to inference loading [#415](https://github.com/deepset-ai/FARM/pull/415)\r\n- Proper attribute assignment in QA with yes / no answer [#414](https://github.com/deepset-ai/FARM/pull/414)\r\n\r\n",
        "dateCreated": "2020-06-24T09:06:42Z",
        "datePublished": "2020-06-24T09:42:36Z",
        "html_url": "https://github.com/deepset-ai/FARM/releases/tag/0.4.5",
        "name": "0.4.5",
        "tag_name": "0.4.5",
        "tarball_url": "https://api.github.com/repos/deepset-ai/FARM/tarball/0.4.5",
        "url": "https://api.github.com/repos/deepset-ai/FARM/releases/27863727",
        "zipball_url": "https://api.github.com/repos/deepset-ai/FARM/zipball/0.4.5"
      },
      {
        "authorType": "User",
        "author_name": "tanaysoni",
        "body": "## ELECTRA Model\r\nWe welcome a new language model to the FARM family that we found to be a really powerful alternative to the existing ones. ELECTRA is trained using a small generator network that replaces tokens with plausible alternatives and a discriminative model that predicts which learns to detect these replaced tokens (see the paper for details: https://arxiv.org/abs/2003.10555). This makes pretraining more efficient and improves down-stream performance for quite many tasks.\r\n\r\nYou can load it as usual via \r\n\r\n``` \r\nLanguageModel.load(\"google/electra-base-discriminator\")\r\n```\r\nSee HF's [model hub](https://huggingface.co/models?search=electra) for more model variants\r\n\r\n## Natural Questions Style QA\r\nWith QA being our favorite and focussed down-stream task, we are happy to support an additional style of QA in FARM ( [#334](https://github.com/deepset-ai/FARM/pull/334)). In contrast to the popular SQuAD-based models, these NQ models support binary answers, i.e. questions like \"Is Berlin the capital of Germany?\"  can be answered with \"Yes\" and an additional span that the model used as a \"supporting fact\" to give this answer.\r\n\r\nThe implementation leverages the option of prediction heads in FARM by having one `QuestionAnsweringHead` that predicts a span (like in SQuAD) and one `TextClassificationHead` that predicts what type of answer the model should give (current options: span, yes, no, is_impossible).\r\n \r\nExample: \r\n\r\n```\r\n    QA_input = [\r\n        {\r\n            \"qas\": [\"Is Berlin the capital of Germany?\"],\r\n            \"context\":  \"Berlin (/b\u025c\u02d0r\u02c8l\u026an/) is the capital and largest city of Germany by both area and population.\"\r\n        }\r\n    ]\r\n    model = Inferencer.load(model_name_or_path=\"../models/roberta-base-squad2-nq\", batch_size=batch_size, gpu=True)\r\n    result = model.inference_from_dicts(dicts=QA_input, return_json=False)\r\n    print(f\"Answer: {result[0].prediction[0].answer}\")\r\n\r\n   >> Answer: yes\r\n``` \r\n\r\nSee this [new example script](https://github.com/deepset-ai/FARM/blob/master/examples/natural_questions.py) for more details on training and inference. \r\n\r\n*Note: This release includes the initial version for NQ, but we are already working on some further simplifications and improvements in [#411 ](https://github.com/deepset-ai/FARM/pull/411).*\r\n\r\n## New speed benchmarking \r\nWith inference speed being crucial for many deployments - especially for QA, we introduce a new benchmarking tool in [#321](https://github.com/deepset-ai/FARM/pull/321). This allows us to easily compare the performance of different frameworks (e.g. ONNX vs. pytorch), parameters (e.g. batch size) and code optimizations of different FARM versions. \r\nSee the [readme](https://github.com/deepset-ai/FARM/blob/master/test/benchmarks/README.md) for usage details and this [spreadsheet](https://docs.google.com/spreadsheets/d/1ak9Cxj1zcNBDtjf7qn2j_ydKDDzpBgWiyJ7cO-7BPvA/edit?usp=sharing) for current results.\r\n\r\n\r\n-------------------------------------------------------------------\r\n\r\n## A few more changes  ...\r\n\r\n\r\n**Modeling**\r\n- Add support for Camembert-like models [#396](https://github.com/deepset-ai/FARM/pull/396)\r\n- Speed up in BERTLMHead by doing argmax on logits on GPU [#377](https://github.com/deepset-ai/FARM/pull/377)\r\n- Fix bug in BERT-style pretraining [#369](https://github.com/deepset-ai/FARM/pull/369)\r\n- Remove additional XLM-R tokens [#360](https://github.com/deepset-ai/FARM/pull/360)\r\n- ELECTRA: use gelu for pooled output of ELECTRA model [#364](https://github.com/deepset-ai/FARM/pull/364)\r\n\r\n**Data handling**\r\n- Option to specify text col name in `TextClassificationProcessor` and `RegressionProcessor` [#387](https://github.com/deepset-ai/FARM/pull/387)\r\n- Document magic data loading in `TextClassificationProcessor` PR [#383](https://github.com/deepset-ai/FARM/pull/383)\r\n- multilabel support for data_silo.calculate_class_weights [#389](https://github.com/deepset-ai/FARM/pull/389)\r\n- Implement Prediction Objects for Question Answering [#405](https://github.com/deepset-ai/FARM/pull/405)\r\n- Removing lambda function from AdaptiveModel so the class can be pickable [#345](https://github.com/deepset-ai/FARM/pull/345)\r\n- Add target device optimisations for ONNX export [#354](https://github.com/deepset-ai/FARM/pull/354)\r\n\r\n**Examples / Docs**\r\n- Add script to reproduce results from COVID-QA paper [#412](https://github.com/deepset-ai/FARM/pull/412)\r\n- Update tutorials [#348](https://github.com/deepset-ai/FARM/pull/348)\r\n- Docstring Format fix [#382](https://github.com/deepset-ai/FARM/pull/382)\r\n\r\n**Other**\r\n- Adjust code to squad inferencing [#367](https://github.com/deepset-ai/FARM/pull/367)\r\n- Convert pydantic objects to regular classes [#410](https://github.com/deepset-ai/FARM/pull/410)\r\n- Rename top n recall to top n accuracy [#409](https://github.com/deepset-ai/FARM/pull/409)\r\n- Add test for embedding extraction [#394](https://github.com/deepset-ai/FARM/pull/394)\r\n- Quick fix CI problems with OOM and unclosed worker pool [#406](https://github.com/deepset-ai/FARM/pull/406)\r\n- Update transformers version to 2.11 [#407](https://github.com/deepset-ai/FARM/pull/407)\r\n- Managing pytorch pip find-links directive [#393](https://github.com/deepset-ai/FARM/pull/393)\r\n- Zero based Epoch Display in Training Progress Bar [#398](https://github.com/deepset-ai/FARM/pull/398)\r\n- Add stalebot [#400](https://github.com/deepset-ai/FARM/pull/400)\r\n- Update pytorch to 1.5.0 [#392](https://github.com/deepset-ai/FARM/pull/392)\r\n- Question answering accuracy test [#357](https://github.com/deepset-ai/FARM/pull/357)\r\n- Add __init__.py files for `farm.conversion` module [#365](https://github.com/deepset-ai/FARM/pull/365)\r\n- Make onnx imports optional [#363](https://github.com/deepset-ai/FARM/pull/363)\r\n- Make ONNXRuntime dependency optional [#347](https://github.com/deepset-ai/FARM/pull/347)\r\n\r\n:man_farmer:  :woman_farmer: Thanks to all contributors for making FARMer's life better!\r\n @PhilipMay , @stefan-it, @ftesser , @tstadel,  @renaud, @skirdey, @brandenchan, @tanaysoni, @Timoeller, @tholor, @bogdankostic",
        "dateCreated": "2020-06-18T08:16:24Z",
        "datePublished": "2020-06-18T15:30:02Z",
        "html_url": "https://github.com/deepset-ai/FARM/releases/tag/0.4.4",
        "name": "0.4.4",
        "tag_name": "0.4.4",
        "tarball_url": "https://api.github.com/repos/deepset-ai/FARM/tarball/0.4.4",
        "url": "https://api.github.com/repos/deepset-ai/FARM/releases/27672930",
        "zipball_url": "https://api.github.com/repos/deepset-ai/FARM/zipball/0.4.4"
      },
      {
        "authorType": "User",
        "author_name": "tanaysoni",
        "body": "## :1234: Changed Multiprocessing in Inferencer\r\nThe Inferencer has now a fixed pool of processes instead of creating a new one for every inference call. \r\nThis accelerates the processing a bit and solves some problems when using it in combination with Frameworks like gunicorn/FastAPI etc ([#329](https://github.com/deepset-ai/FARM/pull/329))\r\n\r\nOld:\r\n```\r\n...\r\ninferencer.inference_from_dicts(dicts, num_processes=8)\r\n``` \r\n\r\nNew: \r\n```\r\nInferencer(dicts, num_processes=8)\r\n...\r\n```\r\n\r\n## :fast_forward: Streaming Inferencer \r\nYou can now also use the Inferencer in a \"streaming mode\". This is especially useful in production scenarios where the Inferencer is part of a bigger pipeline (e.g. consuming documents from elasticsearch) and you want to get predictions as soon as they are available ([#315](https://github.com/deepset-ai/FARM/pull/315))\r\n \r\n*Input:* Generator yielding dicts with your text\r\n*Output:* Generator yielding your predictions\r\n\r\n```\r\n    dicts = sample_dicts_generator()  # it can be a list of dicts or a generator object\r\n    results = inferencer.inference_from_dicts(dicts, streaming=True, multiprocessing_chunksize=20)\r\n    for prediction in results:  # results is a generator object that yields predictions\r\n        print(prediction)\r\n```\r\n\r\n## :older_woman: :older_man: \"Classic\" baseline models for benchmarking + S3E Pooling\r\nWhile Transformers are conquering many of the current NLP tasks, there are still quite some tasks (e.g. some document classification) where they are a complete overkill. Benchmarking Transformers with \"classic\" uncontextualized embedding models is a common, good practice and is now possible without switching frameworks. We added basic support for loading in embeddings models like GloVe, Word2vec and FastText and using them as a \"LanguageModels\" in FARM ([#285](https://github.com/deepset-ai/FARM/pull/285))\r\n\r\nSee the [example script](https://github.com/deepset-ai/FARM/blob/master/examples/doc_classification_word_embedding_LM.py)\r\n\r\nWe also added a new pooling method to get sentence or document embeddings from these models that can act as a strong baseline for transformer-based approaches (e.g Sentence-BERT). The method is called S3E and was recently introduced by Wang et al in [*\"Efficient Sentence Embedding via Semantic Subspace Analysis\"*](https://arxiv.org/abs/2002.0962) ([#286](https://github.com/deepset-ai/FARM/pull/286))\r\n\r\nSee the [example script](https://github.com/deepset-ai/FARM/blob/master/examples/embeddings_extraction_s3e_pooling.py)\r\n\r\n-------------------------------------------------------------------\r\n\r\n## A few more changes  ...\r\n\r\nModeling\r\n-  Cross-validation for Question-Answering [#335](https://github.com/deepset-ai/FARM/pull/335)\r\n- Add option to use max_seq_len tokens for LM Adaptation/Training-from-scratch instead of real sentences [#314](https://github.com/deepset-ai/FARM/pull/314)\r\n-  Add english glove models [#339](https://github.com/deepset-ai/FARM/pull/339)\r\n-  Implicitly connect heads with processor + check for connection [#337](https://github.com/deepset-ai/FARM/pull/337)\r\n\r\n\r\nEvaluation & Inference\r\n- Registration of custom evaluation reports [#331](https://github.com/deepset-ai/FARM/pull/331)\r\n- Standalone Evaluation with pretrained models [#330](https://github.com/deepset-ai/FARM/pull/330)\r\n- tqdm progress bar in inferencer [#338](https://github.com/deepset-ai/FARM/pull/338)\r\n- Group NER preds by sample [#327](https://github.com/deepset-ai/FARM/pull/327)\r\n-  Fix Processor configs when loading Inferencer [#318](https://github.com/deepset-ai/FARM/pull/318)\r\n\r\nOther\r\n- Fix the IOB2 to simple tags check [#324](https://github.com/deepset-ai/FARM/pull/324)\r\n- Update config when saving model to include changes of parameters [#323](https://github.com/deepset-ai/FARM/pull/323)\r\n-  Fix Issues with NER format Conversion [#322](https://github.com/deepset-ai/FARM/pull/322)\r\n-  Fix error message in loading of Tokenizer [#317](https://github.com/deepset-ai/FARM/pull/317)\r\n-  Less verbosity, Fix which Samples and Baskets being Thrown Away [#313](https://github.com/deepset-ai/FARM/pull/313)\r\n\r\n:man_farmer: :woman_farmer: Thanks to all contributors for making FARMer's life better!\r\n@brandenchan, @tanaysoni, @Timoeller, @tholor, @bogdankostic, @gsarti ",
        "dateCreated": "2020-04-29T14:52:08Z",
        "datePublished": "2020-04-29T15:52:55Z",
        "html_url": "https://github.com/deepset-ai/FARM/releases/tag/0.4.3",
        "name": "0.4.3",
        "tag_name": "0.4.3",
        "tarball_url": "https://api.github.com/repos/deepset-ai/FARM/tarball/0.4.3",
        "url": "https://api.github.com/repos/deepset-ai/FARM/releases/26009464",
        "zipball_url": "https://api.github.com/repos/deepset-ai/FARM/zipball/0.4.3"
      },
      {
        "authorType": "User",
        "author_name": "tanaysoni",
        "body": "## :fast_forward: Scalable preprocessing: StreamingDataSilo\r\nAllows you to load data lazily from disk and preprocess a batch on-the-fly when needed during training. \r\n\r\n```stream_data_silo = StreamingDataSilo(processor=processor, batch_size=batch_size)```\r\n\r\n=> Allows large datasets that don't fit in memory (e.g. for training from scratch)\r\n=> Training directly starts. No initial time for preprocessing needed.\r\n\r\n## :rocket: Better Inference: Speed, scalability, standardization\r\n### ONNX support:\r\n Microsoft recently added optimizations to the ONNX-runtime and [reported](https://cloudblogs.microsoft.com/opensource/2020/01/21/microsoft-onnx-open-source-optimizations-transformer-inference-gpu-cpu/) substantial speed-ups compared to PyTorch. Since these improvements can be particularly useful for inference-heavy tasks such as QA, we added a way to export your `AdaptiveModel` to the ONNX format and load it into the `Inferencer`: \r\n\r\n``` \r\nmodel = AdaptiveModel (...)\r\nmodel.convert_to_onnx(Path(\"./onnx_model\"))\r\ninferencer = Inferencer.load(model_name_or_path=Path(\"./onnx_model\"))\r\n```\r\n=> [See example](https://github.com/deepset-ai/FARM/blob/master/examples/onnx_question_answering.py)   \r\n=> Speed improvements depend on device and batch size. On a Tesla V100 we measured improvements between 30% - 260% for doing end-to-end QA inference on a large document and we still see more potential for optimizations.\r\n\r\n| Batch Size | PyTorch | ONNX | ONNX V100 optimizations | Speedup |\r\n|------------|---------|------|-----------|---------|\r\n| 1          | 27.5    | 12.8 | 10.6      | 2.59    |\r\n| 2          | 17.5    | 11.5 | 9.1       | 1.92    |\r\n| 4          | 12.5    | 10.7 | 8.3       | 1.50    |\r\n| 8          | 10.6    | 10.2 | 8.2       | 1.29    |\r\n| 16         | 10.5    | 10.1 | 7.8       | 1.38    |\r\n| 32         | 10.1    | 9.8  | 7.8       | 1.29    |\r\n| 64         | 9.9     | 9.8  | 7.8       | 1.26    |\r\n| 128        | 9.9     | 9.8  | 7.7       | 1.28    |\r\n| 256        | 10.0    | 9.8  | 7.9       | 1.26    |\r\n\r\n### Embedding extraction:\r\nExtracting embeddings from a model at inference time is now more similar to other inference modes.\r\n\r\n*Old*\r\n```\r\nmodel = Inferencer.load(lang_model, task_type=\"embeddings\", gpu=use_gpu, batch_size=batch_size)\r\nresult = model.extract_vectors(dicts=basic_texts, extraction_strategy=\"cls_token\", extraction_layer=-1)\r\n```\r\n*New*\r\n```\r\nmodel = Inferencer.load(lang_model, task_type=\"embeddings\", gpu=use_gpu, batch_size=batch_size,\r\n                            extraction_strategy=\"cls_token\", extraction_layer=-1)\r\nresult = model.inference_from_dicts(dicts=basic_texts, max_processes=1)\r\n``` \r\n\r\n=> The preprocessing can now also utilize multiprocessing \r\n=> It's easier to reuse other methods like `Inference.inference_from_file()`\r\n\r\n\r\n## :left_right_arrow: New tasks: TextPairClassification & Passage ranking\r\nAdded supprt for text pair classification and ranking. Both can be especially helpful in semantic search settings where you want to (re-)rank search results and will be incorporated in our [haystack](https://github.com/deepset-ai/haystack/) framework soon. \r\nExamples: \r\n- [MSMARCO passage ranking](https://github.com/deepset-ai/FARM/blob/master/examples/passage_ranking.py)\r\n- [ASNQ text pair classification](https://github.com/deepset-ai/FARM/blob/master/examples/text_pair_classification.py)\r\n\r\n\r\n-------------------------------------------------------------------\r\n\r\n## A few more changes  ...\r\n\r\n## Faster & simpler Inference\r\n-  Make extract_vectors more compatible to other inference types [#292](https://github.com/deepset-ai/FARM/pull/292)\r\n-  Add test for onnx qa inference. Fix bug in loading PHs for ONNX. [#297](https://github.com/deepset-ai/FARM/pull/297)\r\n-  Add ONNX Inference for Question Answering [#288](https://github.com/deepset-ai/FARM/pull/288)\r\n-  Improve inferencer for better multiprocessing with QA / haystack [#278](https://github.com/deepset-ai/FARM/pull/278)\r\n-  Scalable Qa aggregation [#268](https://github.com/deepset-ai/FARM/pull/268)\r\n-  Allow for multiple queries in QA inference when using rest_api format [#246](https://github.com/deepset-ai/FARM/pull/246)\r\n-  Decouple n_best in QA predictions [#269](https://github.com/deepset-ai/FARM/pull/269)\r\n-  Correct keyword argument for max_processes when used by calc_chunksize() [#255](https://github.com/deepset-ai/FARM/pull/255)\r\n-  Add document id to QA inference [#265](https://github.com/deepset-ai/FARM/pull/265)\r\n-  Refactor no answer handling in Question Answering [#258](https://github.com/deepset-ai/FARM/pull/258)\r\n\r\n## Streaming Data Silo / Training from scratch\r\n-  StreamingDataSilo for loading & preprocessing batches lazily during training [#239](https://github.com/deepset-ai/FARM/pull/239)\r\n-  Fix dict chunking in StreamingDataSilo for LMFinetuning [#284](https://github.com/deepset-ai/FARM/pull/284)\r\n-  Add example for training with AWS SageMaker [#283](https://github.com/deepset-ai/FARM/pull/283)\r\n-  Fix deletion of old training checkpoints [#282](https://github.com/deepset-ai/FARM/pull/282)\r\n-  Fix epoch number for saving a training checkpoint [#281](https://github.com/deepset-ai/FARM/pull/281)\r\n-  Fix Train Step calculations for Checkpointing [#279](https://github.com/deepset-ai/FARM/pull/279)\r\n-  Implement __len__()  for StreamingDataSilo [#274](https://github.com/deepset-ai/FARM/pull/274)\r\n-  Refactor StreamingDataSilo to support multiple train epochs [#266](https://github.com/deepset-ai/FARM/pull/266)\r\n-  Fix serialization for saving train checkpoints [#271](https://github.com/deepset-ai/FARM/pull/271)\r\n\r\n\r\n## Modeling\r\n-  Add support for text pair classification (ASNQ) and ranking (MSMarco) [#237](https://github.com/deepset-ai/FARM/pull/237)\r\n-  Add conversion of lm_finetuned to HF transformers [#290](https://github.com/deepset-ai/FARM/pull/290)\r\n-  Added `next_sentence_head` in `examples/lm_finetuning.py`. [#273](https://github.com/deepset-ai/FARM/pull/273)\r\n- Quickfix loading pred head [#256](https://github.com/deepset-ai/FARM/pull/256)\r\n-  Maked use of 'language' **kwargs if present in LanguageModel.load. [#262](https://github.com/deepset-ai/FARM/pull/262)\r\n- Add the option to define the language model class manually [#264](https://github.com/deepset-ai/FARM/pull/264)\r\n-  Fix XLMR Bug When Calculating Start of Second Sequence [#240](https://github.com/deepset-ai/FARM/pull/240)\r\n\r\n## Examples / Tutorials / Experiments\r\n-  Add data handling for GermEval14, add checks for correct data files [#259](https://github.com/deepset-ai/FARM/pull/259)\r\n-  Fix separator in CoNLL_de experiment config [#254](https://github.com/deepset-ai/FARM/pull/254)\r\n-  Use correct German conll03 data + conversion [#248](https://github.com/deepset-ai/FARM/pull/248)\r\n-  Bugfix parameter loading through experiment configs [#252](https://github.com/deepset-ai/FARM/pull/252)\r\n-  Add early stopping to experiment [#253](https://github.com/deepset-ai/FARM/pull/253)\r\n-  Fix Tutorial: Add missing param in initialize_optimizer [#245](https://github.com/deepset-ai/FARM/pull/245)\r\n\r\n## Other\r\n-  Add Azure test pipeline [#270](https://github.com/deepset-ai/FARM/pull/270)\r\n-  Fix progress bar in datasilo [#267](https://github.com/deepset-ai/FARM/pull/267)\r\n-  Turn off prints and logging during testing [#260](https://github.com/deepset-ai/FARM/pull/260)\r\n-  Pin Werkzeug version in requirements.txt [#250](https://github.com/deepset-ai/FARM/pull/250)\r\n-  Add ConnectionError handling for MLFlow logger [#236](https://github.com/deepset-ai/FARM/pull/236)\r\n-  Clearer message when DataSilo calculates Sequence Lengths [#293](https://github.com/deepset-ai/FARM/pull/293)\r\n-  Add metric to text_pair_classification example [#294](https://github.com/deepset-ai/FARM/pull/294)\r\n-  Add preprocessed CORD-19 dataset [#295](https://github.com/deepset-ai/FARM/pull/295)\r\n\r\n:man_farmer: :woman_farmer: Thanks to all contributors for making FARMer's life better!\r\n@brandenchan, @tanaysoni, @Timoeller, @tholor, @bogdankostic, @andra-pumnea, @PhilipMay, @ftesser, @guggio  \r\n",
        "dateCreated": "2020-04-02T12:17:53Z",
        "datePublished": "2020-04-02T12:18:56Z",
        "html_url": "https://github.com/deepset-ai/FARM/releases/tag/0.4.2",
        "name": "0.4.2",
        "tag_name": "0.4.2",
        "tarball_url": "https://api.github.com/repos/deepset-ai/FARM/tarball/0.4.2",
        "url": "https://api.github.com/repos/deepset-ai/FARM/releases/24997988",
        "zipball_url": "https://api.github.com/repos/deepset-ai/FARM/zipball/0.4.2"
      },
      {
        "authorType": "User",
        "author_name": "tanaysoni",
        "body": "## :man_farmer: :arrows_counterclockwise: :hugs:  Full compatibility with Transformers' models\r\nOpen-source is more than just public code. It's a mindset of sharing, being transparent and collaborating across organizations. It's about building on the shoulders of other projects and advancing together the state of technology. That's why we built on the top of the great [Transformers](https://github.com/huggingface/transformers) library by huggingface and are excited to release today an even deeper compatibility that simplifies the exchange & comparison of models.\r\n\r\n**1. Convert models from/to transformers**\r\n```\r\nmodel = AdaptiveModel.convert_from_transformers(\"deepset/bert-base-cased-squad2\", device=\"cpu\", task_type=\"question_answering\")\r\ntransformer_model = model.convert_to_transformers()\r\n```\r\n\r\n**2. Load models from their new [model hub](https://huggingface.co/models):**\r\n```\r\nLanguageModel.load(\"TurkuNLP/bert-base-finnish-cased-v1\")\r\nInferencer.load(\"deepset/bert-base-cased-squad2\",  task_type=\"question_answering\")\r\n...\r\n```\r\n\r\n## :rocket: Better & Faster Training\r\nThanks to @BramVanroy and @johann-petrak we got some really hot new features here:\r\n- **Automatic Mixed Precision (AMP) Training**: Speed up your training by ~ 35%! Model params are usually stored with FP32 precision. Some model layers don't need that precision and can be reduced to FP16, which speeds up training and reduces memory footprint. AMP is a smart way of figuring out, for which params we can reduce precision without sacrificing performance ([Read more](https://nvlabs.github.io/iccv2019-mixed-precision-tutorial/files/dusan_stosic_intro_to_mixed_precision_training.pdf)).\r\nTest it by installing [apex](https://github.com/NVIDIA/apex) and setting \"use_amp\" to \"O1\" in one of the FARM example scripts.\r\n\r\n- **More flexible Optimizers & Schedulers**:  Choose whatever optimizer you like from PyTorch, apex or Transformers. Take your preferred learning rate schedule from Transformers or PyTorch ([Read more](https://github.com/deepset-ai/FARM#1-optimizers--learning-rate-schedules))\r\n\r\n- **Cross-validation**: Get more reliable eval metrics on small datasets (see [example](https://github.com/deepset-ai/FARM/blob/master/examples/doc_classification_crossvalidation.py))\r\n\r\n- **Early Stopping**: With early stopping, the run stops once a chosen metric is not improving any further and you take the best model up to this point. This helps prevent overfitting on small datasets and reduces training time if your model doesn't improve any further (see [example](https://github.com/deepset-ai/FARM/blob/master/examples/doc_classification_with_earlystopping.py)).\r\n\r\n\r\n## :fast_forward: Caching & Checkpointing\r\nSave time if you run similar pipelines (e.g. only experimenting with model params): Store your preprocessed dataset & load it next time from cache:\r\n```\r\ndata_silo = DataSilo(processor=processor, batch_size=batch_size, caching=True)\r\n```\r\nStart & stop training by saving checkpoints of the trainer:\r\n```\r\ntrainer = Trainer.create_or_load_checkpoint(\r\n            ...\r\n            checkpoint_on_sigterm=True,\r\n            checkpoint_every=200,\r\n            checkpoint_root_dir=Path(\u201c/opt/ml/checkpoints/training\u201d),\r\n            resume_from_checkpoint=\u201clatest\u201d)\r\n```\r\nThe checkpoints include the state of everything that matters (model, optimizer, lr_schedule ...) to resume training. This is particularly useful, if your training crashes (e.g. because you are using spot cloud instances).\r\n\r\n## :cloud: Integration with AWS SageMaker & Training from scratch\r\nWe are currently working a lot on simplifying large scale training and deployment. As a first step, we are adding support for training on AWS SageMaker. The interesting part here is the option to use Spot Instances and save about 70% of costs compared to regular instances. This is particularly relevant for training models from scratch, which we introduce in a basic version in this release and will improve over the next weeks. See this [tutorial](https://github.com/deepset-ai/FARM/blob/master/tutorials/sagemaker/3_train_with_sagemaker.ipynb) to get started with using SageMaker for training on down-stream tasks.\r\n\r\n## :computer: Windows support\r\nFARM now also runs on Windows. This implies one breaking change:\r\nWe now use pathlib and therefore expect all directory paths to be of type `Path` instead of `str` [#172](https://github.com/deepset-ai/FARM/pull/172)\r\n\r\n-------------------------------------------------------------------\r\n\r\n## A few more changes ...\r\n### Modelling\r\n- [**enhancement**] ALBERT support [#169](https://github.com/deepset-ai/FARM/pull/169)\r\n- [**enhancement**] DistilBERT support [#187](https://github.com/deepset-ai/FARM/pull/187)\r\n- [**enhancement**] XLM-Roberta support [#181](https://github.com/deepset-ai/FARM/pull/181)\r\n- [**enhancement**] Automatically infer layer dims of prediction head [#195](https://github.com/deepset-ai/FARM/pull/195)\r\n- [**bug**] Implement next_sent_pred flag [#198](https://github.com/deepset-ai/FARM/pull/198)\r\n\r\n### QA\r\n- [**enhancement**] Encoding of QA IDs [#171](https://github.com/deepset-ai/FARM/pull/171)\r\n- [**enhancement**] Remove repeat QA preds from overlapping passages [#186](https://github.com/deepset-ai/FARM/pull/186)\r\n- [**enhancement**] More options to control predictions of Question Answering Head [#183](https://github.com/deepset-ai/FARM/pull/183)\r\n- [**bug**] Fix QA example [#203](https://github.com/deepset-ai/FARM/pull/203)\r\n\r\n### Training\r\n- [**enhancement**] Use AMP instead of naive fp16. More optimizers. More LR Schedules. [#133](https://github.com/deepset-ai/FARM/pull/133)\r\n- [**bug**] Fix for use AMP instead of naive fp16 (#133) [#180](https://github.com/deepset-ai/FARM/pull/180)\r\n- [**enhancement**] Add early stopping and custom metrics [#165](https://github.com/deepset-ai/FARM/pull/165)\r\n- [**enhancement**] Add checkpointing for training [#188](https://github.com/deepset-ai/FARM/pull/188)\r\n- [**enhancement**] Add train loss to tqdm. add desc for data preproc. log only 2 samples [#175](https://github.com/deepset-ai/FARM/pull/175)\r\n- [**enhancement**] Allow custom functions to aggregate loss of prediction heads  [#220](https://github.com/deepset-ai/FARM/pull/220)\r\n\r\n### Eval\r\n- [**bug**] Fixed micro f1 score [#179](https://github.com/deepset-ai/FARM/pull/179)\r\n- [**enhancement**] Rename classification_report to report [#173](https://github.com/deepset-ai/FARM/pull/173)\r\n\r\n### Data Handling\r\n- [**enhancement**] Add caching of datasets in DataSilo [#177](https://github.com/deepset-ai/FARM/pull/177)\r\n- [**enhancement**] Add option to limit number of processes in datasilo [#174](https://github.com/deepset-ai/FARM/pull/174)\r\n- [**enhancement**] Add max_multiprocessing_chunksize as a param for DataSilo [#168](https://github.com/deepset-ai/FARM/pull/168)\r\n- [**enhancement**] Issue59 - Add cross-validation for small datasets [#167](https://github.com/deepset-ai/FARM/pull/167)\r\n- [**enhancement**] Add max_samples argument to TextClassificationProcessor [#204](https://github.com/deepset-ai/FARM/pull/204)\r\n- [**bug**] Fix bug with added tokens [#197](https://github.com/deepset-ai/FARM/pull/197)\r\n\r\n### Other\r\n- [**other**] Disable multiprocessing in lm_finetuning tests to reduce memory footprint [#176](https://github.com/deepset-ai/FARM/pull/176)\r\n- [**bug**] Fix device arg in examples [#184](https://github.com/deepset-ai/FARM/pull/184)\r\n- [**other**] Add error message to train/dev split fn [#190](https://github.com/deepset-ai/FARM/pull/190)\r\n- [**enhancement**] Add more seeds [#192](https://github.com/deepset-ai/FARM/pull/192)\r\n\r\n:man_farmer: :woman_farmer: Thanks to all contributors for making FARMer's life better!\r\n@brandenchan, @tanaysoni, @Timoeller, @tholor, @maknotavailable, @johann-petrak, @BramVanroy\r\n",
        "dateCreated": "2020-02-03T09:53:16Z",
        "datePublished": "2020-02-03T10:19:59Z",
        "html_url": "https://github.com/deepset-ai/FARM/releases/tag/0.4.1",
        "name": "0.4.1",
        "tag_name": "0.4.1",
        "tarball_url": "https://api.github.com/repos/deepset-ai/FARM/tarball/0.4.1",
        "url": "https://api.github.com/repos/deepset-ai/FARM/releases/23333223",
        "zipball_url": "https://api.github.com/repos/deepset-ai/FARM/zipball/0.4.1"
      },
      {
        "authorType": "User",
        "author_name": "tanaysoni",
        "body": "## :paintbrush: Fundamental Re-design of Question Answering \r\nWe believe QA is one of the most exciting tasks for transfer learning. However, the complexity of the task lets  pipelines easily become messy, complicated and slow. This is unacceptable for production settings and creates a high barrier for developers to modify or improve them.\r\n\r\nWe put substantial effort in re-designing QA in FARM with two goals in mind: making it the simplest & fastest pipeline out there. \r\nResults: \r\n- :bulb: **Simplicity**: The pipeline is cleaner, more modular and easier to extend. \r\n- :rocket: **Speed**: Preprocessing of SQuAD 2.0 got **down to 42s** on a AWS p3.8xlarge (vs. ~ 20min in transformers and early versions of FARM). This will not only speed up training cycles and reduce GPU costs, but has also a big impact at inference time, where most time is actually spend on preprocessing.\r\n\r\nSee this [blog post](https://medium.com/deepset-ai/modern-question-answering-systems-explained-4d0913744097) for more details and to learn about the key steps in a QA pipeline.\r\n\r\n## :briefcase:  Support of proxy servers \r\nGood news for our corporate users: Many of you approached us that the automated downloads of datasets / models caused problem in environments with proxy servers. You can now pass the proxy details to Processor and LanguageModel in the format used by the [requests library](https://2.python-requests.org//en/latest/user/advanced/#proxies)  \r\n\r\n\r\nExample:  \r\n```\r\nproxies = {\"https\": \"http://user:pass@10.10.10.10:8000\"}\r\n\r\nlanguage_model = LanguageModel.load(pretrained_model_name_or_path = \"bert-base-cased\", \r\n                                    language = \"english\",\r\n                                    proxies=proxies\r\n                                    )\r\n...\r\nprocessor = BertStyleLMProcessor(data_dir=\"data/lm_finetune_nips\", \r\n                                 tokenizer=tokenizer,\r\n                                 max_seq_len=128, \r\n                                 max_docs=25,\r\n                                 next_sent_pred=True,\r\n                                 proxies = proxies,\r\n                                )\r\n``` \r\n\r\n--------------------------------\r\n### Modelling\r\n- [**enhancement**] QA redesign [#151](https://github.com/deepset-ai/FARM/pull/151)\r\n- [**enhancement**] Add backwards compatibility for loading prediction head [#159](https://github.com/deepset-ai/FARM/pull/159)\r\n- [**enhancement**] Raise an Exception when an invalid path is supplied for loading a saved model  [#137](https://github.com/deepset-ai/FARM/pull/137)\r\n- [**bug**] fix context in QA formatted preds [#163](https://github.com/deepset-ai/FARM/pull/163)\r\n- [**bug**] Fix loading custom vocab in transformers style for LM finetuning [#155](https://github.com/deepset-ai/FARM/pull/155)\r\n\r\n### Data Handling\r\n- [**enhancement**] Allow to load dataset from dicts in DataSilo [#127](https://github.com/deepset-ai/FARM/pull/127)\r\n- [**enhancement**] Option to supply proxy server [#136](https://github.com/deepset-ai/FARM/pull/136)\r\n- [**bug**] Fix tokenizer for multiple whitespaces [#156](https://github.com/deepset-ai/FARM/pull/156)\r\n\r\n## Inference\r\n- [**enhancement**] Change context in QA formatted preds to not split words [#138](https://github.com/deepset-ai/FARM/pull/138)\r\n\r\n## Other\r\n- [**enhancement**] Add test for output format of QA Inferencer [#149](https://github.com/deepset-ai/FARM/pull/149)\r\n- [**bug**] Fix classification report for multilabel [#150](https://github.com/deepset-ai/FARM/pull/150)\r\n- [**bug**] Fix inference in doc_classification_cola example [#147](https://github.com/deepset-ai/FARM/pull/147)\r\n\r\nThanks to all contributors for making FARMer's life better!\r\n@johann-petrak, @brandenchan, @tanaysoni, @Timoeller, @tholor, @cregouby \r\n",
        "dateCreated": "2019-11-27T17:45:20Z",
        "datePublished": "2019-11-28T11:08:30Z",
        "html_url": "https://github.com/deepset-ai/FARM/releases/tag/0.3.2",
        "name": "0.3.2",
        "tag_name": "0.3.2",
        "tarball_url": "https://api.github.com/repos/deepset-ai/FARM/tarball/0.3.2",
        "url": "https://api.github.com/repos/deepset-ai/FARM/releases/21809037",
        "zipball_url": "https://api.github.com/repos/deepset-ai/FARM/zipball/0.3.2"
      },
      {
        "authorType": "User",
        "author_name": "tanaysoni",
        "body": "# Improved Question Answering\r\n\r\n## Aggregation over multiple passages\r\nWhen asking questions on long documents, the underlying Language Model needs to cut the document in multiple passages and answer the question on each of them. The output needs to be aggregated.\r\n\r\n## Improved QA Inferencer\r\nThe QA Inferencer \r\n- projects model predictions back to character space \r\n- can be used in the FARM demos UI\r\n- writes predictions in SQuAD style format, so you can compare the model accuracy with other frameworks\r\n\r\n\r\n--------------------------------\r\n### Modelling\r\n- [**closed**] Refactor squad qa [#131](https://github.com/deepset-ai/FARM/pull/131)\r\n- [**enhancement**][**part: model**] Fix passing kwargs to LM loading (e.g. proxy) [#132](https://github.com/deepset-ai/FARM/pull/132)\r\n",
        "dateCreated": "2019-11-04T13:45:57Z",
        "datePublished": "2019-11-04T14:51:58Z",
        "html_url": "https://github.com/deepset-ai/FARM/releases/tag/0.3.1",
        "name": "0.3.1",
        "tag_name": "0.3.1",
        "tarball_url": "https://api.github.com/repos/deepset-ai/FARM/tarball/0.3.1",
        "url": "https://api.github.com/repos/deepset-ai/FARM/releases/21191072",
        "zipball_url": "https://api.github.com/repos/deepset-ai/FARM/zipball/0.3.1"
      },
      {
        "authorType": "User",
        "author_name": "tanaysoni",
        "body": "# Major Changes\r\n## Adding Roberta & XLNet\r\nWelcome RoBERTa and XLNet on the FARM :tada:! \r\nWe did some intense refactoring in FARM to make it easier to add more language models. However, we will only add models where we see some decent advantages. One of the next models to follow will very likely be ALBERT ... \r\n\r\nFor now, we support Roberta/XLNet on (Multilabel) Textclassification, Text Regression and NER. QA will follow soon. \r\n\r\n:warning:  **Breaking Change** - Loading of Language models has changed:\r\n`Bert.load(\"bert-base-cased\") -> LanguageModel.load(\"bert-base-cased\") `\r\n\r\n## Migrating to tokenizers from the [transformers repo](https://github.com/huggingface/transformers). \r\n\r\n**Pros:**\r\n- It's quite easy to add a tokenizer for any of the models implemented in transformers.\r\n- We rather support the development there than building something in parallel\r\n- The additional metadata during tokenization (offsets, start_of_word) is still created via tokenize_with_metadata\r\n- We can use encode_plus to add model specific special tokens (CLS, SEP ...)   \r\n\r\n**Cons:** \r\n-  We had to deprecate our attribute \"never_split_chars\" that allowed to adjust the BasicTokenizer of BERT.\r\n- Custom vocab is now realized by increasing vocab_size instead of replacing unused tokens \r\n\r\n:warning:  **Breaking Change** -  Loading of tokenizers has changed: \r\n`BertTokenizer.from_pretrained(\"bert-base-cased\") -> Tokenizer.load(\"bert-base-cased\")`\r\n\r\n:warning:  **Breaking Change** - never_split_chars:\r\n is no longer supported as an argument for the Tokenizer\r\n\r\n\r\n--------------------------------\r\n## Modelling:\r\n- [**enhancement**] Add Roberta, XLNet and redesign Tokenizer [#125](https://github.com/deepset-ai/FARM/pull/125)\r\n- [**bug**] fix loading of old tokenizer style [#129](https://github.com/deepset-ai/FARM/pull/129)\r\n\r\n## Data Handling:\r\n- [**bug**] Fix name of squad labels in experiment config [#121](https://github.com/deepset-ai/FARM/pull/121)\r\n- [**bug**] change arg in squadprocessor from labels to label_list  [#123](https://github.com/deepset-ai/FARM/pull/123)\r\n\r\n## Inference:\r\n- [**enhancement**] Add option to disable multiprocessing in Inferencer(#117) [#128](https://github.com/deepset-ai/FARM/pull/128)\r\n- [**bug**] Fix logging verbosity in Inferencer (#117) [#122](https://github.com/deepset-ai/FARM/pull/122)\r\n\r\n## Other\r\n- [**enhancement**] Tutorial update [#116](https://github.com/deepset-ai/FARM/pull/116)\r\n- [**enhancement**] Update docs for api/ui docker [#118](https://github.com/deepset-ai/FARM/pull/118)\r\n\r\n\r\n\r\n\r\n\r\n",
        "dateCreated": "2019-10-28T13:43:36Z",
        "datePublished": "2019-10-28T13:50:29Z",
        "html_url": "https://github.com/deepset-ai/FARM/releases/tag/0.3.0",
        "name": "0.3.0",
        "tag_name": "0.3.0",
        "tarball_url": "https://api.github.com/repos/deepset-ai/FARM/tarball/0.3.0",
        "url": "https://api.github.com/repos/deepset-ai/FARM/releases/21023262",
        "zipball_url": "https://api.github.com/repos/deepset-ai/FARM/zipball/0.3.0"
      },
      {
        "authorType": "User",
        "author_name": "tanaysoni",
        "body": "# Major Changes\r\n## Parallelization of Data Preprocessing :rocket: \r\nData preprocessing via the Processor is now **fast while maintaining a low memory footprint**. Before, the parallelization via multiprocessing was causing serious memory issues on larger data sets (e.g. for Language Model fine-tuning). Now, we are running a small chunk through the whole processor (-> Samples -> Featurization -> Dataset ...). The multiprocessing is handled by the DataSilo now which simplifies implementation. \r\n\r\nWith this new approach we can still easily inspect & debug all important transformations for a chunk, but only keep the resulting dataset in memory once a process has finished with a chunk. \r\n\r\n## Multilabel classification\r\nWe support now also multilabel classification. Prepare your data by simply setting `multilabel=true` in the `TextClassificationProcessor` and use the new `MultiLabelTextClassificationHead` for your model. \r\n=> See an example [here](https://github.com/deepset-ai/FARM/blob/master/examples/doc_classification_multilabel.py)\r\n\r\n## Concept of Tasks\r\nTo further simplify multi-task learning we added the concept of \"tasks\". With this you can now use one `TextClassificationProcessor` to preprocess data for multiple tasks (e.g. using two columns in your CSV for classification). \r\nExample: \r\n1. Add the tasks to the Processor:\r\n```\r\n    processor = TextClassificationProcessor(...)\r\n\r\n    news_categories = [\"Sports\", \"Tech\", \"Politics\", \"Business\", \"Society\"]\r\n    publisher = [\"cnn\", \"nytimes\",\"wsj\"]\r\n\r\n    processor.add_task(name=\"category\", label_list=news_categories, metric=\"acc\", label_column_name=\"category_label\")\r\n    processor.add_task(name=\"publisher\", label_list=publisher, metric=\"acc\", label_column_name=\"publisher_label\")\r\n```\r\n\r\n2. Link the data to right `PredictionHead` by supplying the task name at initialization:\r\n``` \r\ncategory_head = MultiLabelTextClassificationHead(layer_dims=[768,5)], task_name=\"action_type\")\r\npublisher_head = MultiLabelTextClassificationHead(layer_dims=[768, 3], task_name=\"parts\")\r\n```\r\n\r\n## Update to transformers 2.0\r\nWe are happy to see how huggingface's repository is growing and how they made another major step with the new 2.0 release. Since their collection of language models is awesome, we will continue building upon their  language models and tokenizers. However, we will keep following a different philosophy for all other components (dataprocessing, training, inference, deployment ...) to improve usability, allow multitask learning and simplify usage in the industry. \r\n\r\n\r\n--------------------------------\r\n## Modelling:\r\n- ['**enhancement**] Add Multilabel Classification (#89)\r\n - ['**enhancement**] Add PredictionHead for Regression task (#50)\r\n- [**enhancement**] Introduce concept of \"tasks\" to support of multitask training using multiple heads of the same type (e.g. for multiple text classification tasks) (#75)\r\n- [**enhancement**] Update dependency to transformers 2.0 (#106)\r\n- [**bug**] TypeError: classification_report() got an unexpected keyword argument 'target_names' [#93](https://github.com/deepset-ai/FARM/issues/93)\r\n- [**bug**] Fix issue with class weights (#82)\r\n\r\n \r\n## Data Handling:\r\n- [**enhancement**] Chunkwise multiprocessing to reduce memory footprint in preprocessing large datasets (#88)\r\n- [**bug**] Threading Error upon building Data Silo [#90](https://github.com/deepset-ai/FARM/issues/90)\r\n- [**bug**] Multiprocessing causes data preprocessing to crash [#110](https://github.com/deepset-ai/FARM/issues/110)\r\n(https://github.com/deepset-ai/FARM/issues/102)\r\n- [**bug**] Multiprocessing Error with PyTorch Version 1.2.0 [#97](https://github.com/deepset-ai/FARM/issues/97)\r\n- [**bug**] Windows fixes (#109)\r\n\r\n## Inference:\r\n- [**enhancement**] excessive uncalled-for warnings when using the inferencer [#104](https://github.com/deepset-ai/FARM/issues/104)\r\n- [**enhancement**] Get probability distribution over all classes in Inference mode (#102)\r\n- [**enhancement**] Add InferenceProcessor (#72)\r\n- [**bug**] Fix classifcation report bug with binary doc classification\r\n\r\n## Other:\r\n- [**enhancement**]  Add more tests (#108)\r\n-  [**enhancement**] do logging within run_experiment() (#37) \r\n-  [**enhancement**] Improved logging (#82, #87 #105)\r\n- [**bug**]  fix custom vocab for bert-base-cased (#108)\r\n\r\nThanks to all contributors: @tripl3a, @busyxin, @AhmedIdr, @jinnerbichler, @Timoeller, @tanaysoni, @brandenchan , @tholor\r\n\r\n\ud83d\udc69\u200d\ud83c\udf3e Happy FARMing!",
        "dateCreated": "2019-10-14T17:03:58Z",
        "datePublished": "2019-10-14T19:14:21Z",
        "html_url": "https://github.com/deepset-ai/FARM/releases/tag/0.2.2",
        "name": "0.2.2",
        "tag_name": "0.2.2",
        "tarball_url": "https://api.github.com/repos/deepset-ai/FARM/tarball/0.2.2",
        "url": "https://api.github.com/repos/deepset-ai/FARM/releases/20688579",
        "zipball_url": "https://api.github.com/repos/deepset-ai/FARM/zipball/0.2.2"
      },
      {
        "authorType": "User",
        "author_name": "tanaysoni",
        "body": "Besides fixing various smaller bugs, we focussed in this release on two major changes:\r\n\r\n**1. Speeding things up :rocket: :**  \r\n- By adding multiprocessing to the data preprocessing, we reduced the execution time for many tasks **from hours to minutes**. Since the functionality is mostly hidden in the parent class, the user doesn't have to implement anything on his own. However, this required changing the interface of the processor slightly. `_dict_to_samples` and `_sample_to_features` must now be `classmethods` and all objects accessed by them must be `class attributes`.\r\n- Multi-GPU support is now also available for the \"building blocks mode\"\r\n\r\n**2. Making the processor more user friendly :blush: :**\r\n- Instead of having one individual processor per dataset, we have implemented a more generic `TextClassificationProcessor` that you can instantiate easily for various predefined tasks (GNAD, GermEval ...) or your own dataset in CSV/TSV format\r\n```\r\nprocessor = TextClassificationProcessor(tokenizer=tokenizer,\r\n                                        max_seq_len=128,\r\n                                        data_dir=\"../data/germeval18\",\r\n                                        columns=[\"text\", \"label\", \"unused\"],\r\n                                        label_list=[\"OTHER\", \"OFFENSE\"],\r\n                                        metrics=[\"f1_macro\"]\r\n                                        ) \r\n```\r\nThanks for contributing @brandenchan @tanaysoni @tholor @Timoeller @tripl3a @Seb0 @waldemarhahn !\r\n________________\r\nModeling:\r\n- [**bug**] Accuracy metric in LM finetuning always zero [#30](https://github.com/deepset-ai/FARM/issues/30)\r\n- [**enhancement**] Multi-GPU only enabled in experiment mode [#57](https://github.com/deepset-ai/FARM/issues/57)\r\n- [**bug**] Wrong number of total steps for linear warmup schedule  [#46](https://github.com/deepset-ai/FARM/issues/46)\r\n\r\nData Handling:\r\n- [**enhancement**] Unify redundant `Processor`; add new `NERProcessor` and `TextClassificationProcessor`\r\n- [**enhancement**] Add parallel dataprocessing [#45](https://github.com/deepset-ai/FARM/pull/45)\r\n- [**bug**] `dev_size` param in run-by-config is being ignored [#49](https://github.com/deepset-ai/FARM/issues/49)\r\n- [**bug**] output_dir parameter in run by config is being ignored [#39](https://github.com/deepset-ai/FARM/issues/39)\r\n- [**bug**] Error when running by config with a list of batch sizes [#38](https://github.com/deepset-ai/FARM/issues/38)\r\n\r\nDocumentation:\r\n- [**bug**] LM finetuning example missing data [#47](https://github.com/deepset-ai/FARM/issues/47)\r\n- [**bug**] Colab Notebook referenced in readme does not work [#27](https://github.com/deepset-ai/FARM/issues/27)\r\n\r\nOther:\r\n- [**enhancement**] Proposition: improve dependency management with pipenv [#35](https://github.com/deepset-ai/FARM/issues/35)\r\n\r\n\r\n\r\n\r\n\r\n",
        "dateCreated": "2019-08-19T08:56:30Z",
        "datePublished": "2019-08-19T08:57:03Z",
        "html_url": "https://github.com/deepset-ai/FARM/releases/tag/0.2.0",
        "name": "Add multiprocessing to Datahandler, unify Processors",
        "tag_name": "0.2.0",
        "tarball_url": "https://api.github.com/repos/deepset-ai/FARM/tarball/0.2.0",
        "url": "https://api.github.com/repos/deepset-ai/FARM/releases/19371638",
        "zipball_url": "https://api.github.com/repos/deepset-ai/FARM/zipball/0.2.0"
      },
      {
        "authorType": "User",
        "author_name": "Timoeller",
        "body": "First release of FARM package\r\n\r\nContributor list: @brandenchan @tanaysoni @tholor @Timoeller @tripl3a ",
        "dateCreated": "2019-07-29T14:25:55Z",
        "datePublished": "2019-07-29T14:27:02Z",
        "html_url": "https://github.com/deepset-ai/FARM/releases/tag/0.1.2",
        "name": "Initial Release",
        "tag_name": "0.1.2",
        "tarball_url": "https://api.github.com/repos/deepset-ai/FARM/tarball/0.1.2",
        "url": "https://api.github.com/repos/deepset-ai/FARM/releases/18930474",
        "zipball_url": "https://api.github.com/repos/deepset-ai/FARM/zipball/0.1.2"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1444,
      "date": "Thu, 30 Dec 2021 01:21:40 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "language-models",
      "bert",
      "nlp",
      "deep-learning",
      "transfer-learning",
      "pytorch",
      "nlp-library",
      "nlp-framework",
      "xlnet-pytorch",
      "ner",
      "question-answering",
      "pretrained-models",
      "roberta",
      "germanbert"
    ],
    "technique": "GitHub API"
  }
}