{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1711.05101",
      "https://arxiv.org/abs/1705.07485"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9251255279142222
      ],
      "excerpt": "Table 1: Error rates (%) on CIFAR-10 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/loshchil/AdamW-and-SGDW",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "xgastaldi.mba2011 at london.edu  \nAny discussions, suggestions and questions are welcome!\n\n",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-11-07T08:59:51Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-06T08:18:26Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This method aims at helping computer vision practitioners faced with an overfit problem. The idea is to replace, in a 3-branch ResNet, the standard summation of residual branches by a stochastic affine combination. The largest tested model improves on the best single shot published result on CIFAR-10 by reaching 2.72% test error.\n\n![shake-shake](https://s3.eu-central-1.amazonaws.com/github-xg/architecture3.png)\n\nFigure 1: **Left:** Forward training pass. **Center:** Backward training pass. **Right:** At test time.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9917668636275605,
        0.810654900392797
      ],
      "excerpt": "This repository contains the code for the paper Shake-Shake regularization of 3-branch residual networks.  \nThe code is based on [fb.resnet.torch] (https://github.com/facebook/fb.resnet.torch). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9782560823046293
      ],
      "excerpt": "The base network is a 26 2x32d ResNet (i.e. the network has a depth of 26, 2 residual branches and the first residual block has a width of 32). \"Shake\" means that all scaling coefficients are overwritten with new random numbers before the pass. \"Even\" means that all scaling coefficients are set to 0.5 before the pass. \"Keep\" means that we keep, for the backward pass, the scaling coefficients used during the forward pass. \"Batch\" means that, for each residual block, we apply the same scaling coefficient for all the images in the mini-batch. \"Image\" means that, for each residual block, we apply a different scaling coefficient for each image in the mini-batch. The numbers in the Table below represent the average of 3 runs except for the 96d models which were run 5 times. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8488594358497039
      ],
      "excerpt": "Ln 17, 54-59, 81-88: Adds a log (courtesy of Sergey Zagoruyko)   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8837910584560875
      ],
      "excerpt": "Ln 88-89: Adds the learning rate to the elements printed on screen   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9764837721952218,
        0.8846445708788889
      ],
      "excerpt": "Ln 60-61: Avoids using the fb.renet.torch deepcopy (it doesn't seem to be compatible with the BN in shakeshakeblock) and replaces it with the deepcopy from stdlib \nLn 67-81: Saves only the best model   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Decoupled Weight Decay Regularization (ICLR 2019)",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/loshchil/AdamW-and-SGDW/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 24,
      "date": "Wed, 22 Dec 2021 03:10:48 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/loshchil/AdamW-and-SGDW/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "loshchil/AdamW-and-SGDW",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This repository contains the code for the paper Decoupled Weight Decay Regularization (old title: Fixing Weight Decay Regularization in Adam) by Ilya Loshchilov and Frank Hutter, ICLR 2019 [arXiv](https://arxiv.org/abs/1711.05101). \n\nThe code represents a tiny modification of the source code provided for the Shake-Shake regularization by Xavier Gastaldi [arXiv](https://arxiv.org/abs/1705.07485). Since the usage of both is very similar, the introduction and description of the original Shake-Shake code is given below. Please consider to  *first* run the Shake-Shake code and then our code. \n\nFind below a few examples to train a 26 2x96d \"Shake-Shake-Image\" ResNet on CIFAR-10 with 1 GPU.\nTo run on 4 GPUs, set `CUDA_VISIBLE_DEVICES=0,1,2,3` and `-nGPU 4`.\nFor test purposes you may reduce `-nEpochs` from 1500 to e.g. 150 and set `-widenFactor` to 4 to use a smaller network. \nTo run on ImageNet32x32, set `-dataset` to imagenet32 and reduce `-nEpochs` to 150.\nYou may consider to use `-weightDecay=0.05` for CIFAR-10. \n\nImportantly, please copy with replacement `adam.lua` and `sgd.lua` from `UPDATETORCHFILES` to `YOURTORCHFOLDER/install/share/lua/5.1/optim/`\n\nTo run AdamW for `nEpochs=1500` epochs without restarts with initial learning rate `LR=0.001`, normalized weight decay `weightDecay=0.025`   \n\n```\nCUDA_VISIBLE_DEVICES=0 th main.lua -algorithmType ADAMW -nEpochs 1500 -Te 1500 -Tmult 2 -widenFactor 6 -LR 0.001 -weightDecay 0.025 -dataset cifar10 -nGPU 1 -depth 26 -irun 1 -batchSize 128 -momentum 0.9 -shareGradInput false -optnet true -netType shakeshake -forwardShake true -backwardShake true -shakeImage true -lrShape cosine -LRdec true\n```\n\nTo run AdamW for `nEpochs=1500` epochs with restarts, where the first restart will happen after `Te=100` epochs and the second restart after 200 more epochs because `100*Tmult=200`. \n\n```\nCUDA_VISIBLE_DEVICES=0 th main.lua -algorithmType ADAMW -nEpochs 1500 -Te 100 -Tmult 2 -widenFactor 6 -LR 0.001 -weightDecay 0.025 -dataset cifar10 -nGPU 1 -depth 26 -irun 1 -batchSize 128 -momentum 0.9 -shareGradInput false -optnet true -netType shakeshake -forwardShake true -backwardShake true -shakeImage true -lrShape cosine -LRdec true\n```\n\nTo run SGDW for `nEpochs=150` epochs without restarts with initial learning rate `LR=0.05`, normalized weight decay `weightDecay=0.025`   \n\n```\nCUDA_VISIBLE_DEVICES=0 th main.lua -algorithmType SGDW -nEpochs 1500 -Te 1500 -Tmult 2 -widenFactor 6 -LR 0.05 -weightDecay 0.025 -dataset cifar10 -nGPU 1 -depth 26 -irun 1 -batchSize 128 -momentum 0.9 -shareGradInput false -optnet true -netType shakeshake -forwardShake true -backwardShake true -shakeImage true -lrShape cosine -LRdec true\n```\n\nTo run SGDW for `nEpochs=150` epochs with restarts, where the first restart will happen after `Te=100` epochs and the second restart after 200 more epochs because `100*Tmult=200`. \n\n```\nCUDA_VISIBLE_DEVICES=0 th main.lua -algorithmType SGDW -nEpochs 1500 -Te 100 -Tmult 2 -widenFactor 6 -LR 0.001 -weightDecay 0.025 -dataset cifar10 -nGPU 1 -depth 26 -irun 1 -batchSize 128 -momentum 0.9 -shareGradInput false -optnet true -netType shakeshake -forwardShake true -backwardShake true -shakeImage true -lrShape cosine -LRdec true\n```\n\nAcknowledgments: We thank Patryk Chrabaszcz for creating functions dealing with ImageNet32x32 dataset.\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8715280858109221
      ],
      "excerpt": "Shake   |Shake  |Batch  |3.54   |3.01   |- \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/loshchil/AdamW-and-SGDW/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Lua"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "BSD 3-Clause \"New\" or \"Revised\" License",
      "url": "https://api.github.com/licenses/bsd-3-clause"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'Copyright (c) 2017, Xavier Gastaldi. \\nAll rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without modification,\\nare permitted provided that the following conditions are met:\\n\\n * Redistributions of source code must retain the above copyright notice, this\\n   list of conditions and the following disclaimer.\\n\\n * Redistributions in binary form must reproduce the above copyright notice,\\n   this list of conditions and the following disclaimer in the documentation\\n   and/or other materials provided with the distribution.\\n\\n * Neither the name NetShake nor the names of its contributors may be used to\\n   endorse or promote products derived from this software without specific\\n   prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR\\nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\\nANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Decoupled Weight Decay Regularization (old title: Fixing Weight Decay Regularization in Adam)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "AdamW-and-SGDW",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "loshchil",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/loshchil/AdamW-and-SGDW/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 189,
      "date": "Wed, 22 Dec 2021 03:10:48 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "0. Install [fb.resnet.torch] (https://github.com/facebook/fb.resnet.torch), [optnet](https://github.com/fmassa/optimize-net) and [lua-stdlib](https://github.com/lua-stdlib/lua-stdlib).\n1. Download Shake-Shake\n```\ngit clone https://github.com/xgastaldi/shake-shake.git\n```\n2. Copy the elements in the shake-shake folder and paste them in the fb.resnet.torch folder. This will overwrite 5 files (*main.lua*, *train.lua*, *opts.lua*, *checkpoints.lua* and *models/init.lua*) and add 3 new files (*models/shakeshake.lua*, *models/shakeshakeblock.lua* and *models/mulconstantslices.lua*).\n3. You can train a 26 2x32d \"Shake-Shake-Image\" ResNet on CIFAR-10+ using\n\n```\nth main.lua -dataset cifar10 -nGPU 1 -batchSize 128 -depth 26 -shareGradInput false -optnet true -nEpochs 1800 -netType shakeshake -lrShape cosine -widenFactor 2 -LR 0.2 -forwardShake true -backwardShake true -shakeImage true\n``` \n\nYou can train a 26 2x96d \"Shake-Shake-Image\" ResNet on 2 GPUs using\n\n```\nCUDA_VISIBLE_DEVICES=0,1 th main.lua -dataset cifar10 -nGPU 2 -batchSize 128 -depth 26 -shareGradInput false -optnet true -nEpochs 1800 -netType shakeshake -lrShape cosine -widenFactor 6 -LR 0.2 -forwardShake true -backwardShake true -shakeImage true\n```\n\nA widenFactor of 2 corresponds to 32d, 4 to 64d, etc..\n\n",
      "technique": "Header extraction"
    }
  ]
}