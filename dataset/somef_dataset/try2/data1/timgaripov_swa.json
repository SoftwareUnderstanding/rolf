{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1803.05407",
      "https://arxiv.org/abs/1803.05407"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": " \n Provided model implementations were adapted from\n * VGG: [github.com/pytorch/vision/](https://github.com/pytorch/vision/)\n * PreResNet: [github.com/bearpaw/pytorch-classification](https://github.com/bearpaw/pytorch-classification)\n * WideResNet: [github.com/meliketoy/wide-resnet.pytorch](https://github.com/meliketoy/wide-resnet.pytorch)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{izmailov2018averaging,\n  title={Averaging Weights Leads to Wider Optima and Better Generalization},\n  author={Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},\n  journal={arXiv preprint arXiv:1803.05407},\n  year={2018}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9572487334023723,
        0.8629711733945958
      ],
      "excerpt": "| VGG16 (200)               | 72.55 \u00b1 0.10 | 73.91 \u00b1 0.12 | 74.17 \u00b1 0.15     | 74.27 \u00b1 0.25    | \n| PreResNet110 (150)        | 76.77 \u00b1 0.38 | 78.75 \u00b1 0.16 | 78.91 \u00b1 0.29     | 79.10 \u00b1 0.21    | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266,
        0.9086892148066392,
        0.9670492833072164
      ],
      "excerpt": "| PreResNet110 (150)        | 95.03 \u00b1 0.05 | 95.51 \u00b1 0.10 | 95.65 \u00b1 0.03     | 95.82 \u00b1 0.03    | \n| PreResNet164 (150)        | 95.28 \u00b1 0.10 | 95.56 \u00b1 0.11 | 95.77 \u00b1 0.04     | 95.83 \u00b1 0.03    | \n| WideResNet28x10 (200)     | 96.18 \u00b1 0.11 | 96.45 \u00b1 0.11 | 96.64 \u00b1 0.08     | 96.79 \u00b1 0.05    | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/timgaripov/swa",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-03-21T07:23:27Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-20T09:20:25Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "SWA is a simple DNN training method that can be used as a drop-in replacement for SGD with improved generalization, faster convergence, and essentially no overhead. The key idea of SWA is to average multiple samples produced by SGD with a modified learning rate schedule. We use a constant or cyclical learning rate schedule that causes SGD to _explore_ the set of points in the weight space corresponding to high-performing networks. We observe that SWA converges more quickly than SGD, and to wider optima that provide higher test accuracy. \n\nIn this repo we implement the constant learning rate schedule that we found to be most practical on CIFAR datasets.\n\n<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/14368801/37633888-89fdc05a-2bca-11e8-88aa-dd3661a44c3f.png\" width=250>\n  <img src=\"https://user-images.githubusercontent.com/14368801/37633885-89d809a0-2bca-11e8-8d57-3bd78734cea3.png\" width=250>\n  <img src=\"https://user-images.githubusercontent.com/14368801/37633887-89e93784-2bca-11e8-9d71-a385ea72ff7c.png\" width=250>\n</p>\n\nPlease cite our work if you find this approach useful in your research:\n```bibtex\n@article{izmailov2018averaging,\n  title={Averaging Weights Leads to Wider Optima and Better Generalization},\n  author={Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},\n  journal={arXiv preprint arXiv:1803.05407},\n  year={2018}\n}\n```\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9888612092264358
      ],
      "excerpt": "This repository contains a PyTorch implementation of the Stochastic Weight Averaging (SWA) training method for DNNs from the paper \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8192505441185383,
        0.9861909266767038,
        0.9826367306887556
      ],
      "excerpt": "by Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov and Andrew Gordon Wilson. \n<b>Note: as of August 2020, SWA is now a core optimizer in the PyTorch library, and can be immediately used by anyone with PyTorch, without needing an external repo, as easily SGD or Adam.</b> Please see this blog post introducing the native PyTorch implementation with examples. \nTest accuracy (%) of SGD and SWA on CIFAR-100 for different training budgets. For each model the Budget is defined as the number of epochs required to train the model with the conventional SGD procedure. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9402030572070177
      ],
      "excerpt": "Below we show the convergence plot for SWA and SGD with PreResNet164 on CIFAR-100 and the corresponding learning rates. The dashed line illustrates the accuracy of individual models averaged by SWA. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Stochastic Weight Averaging in PyTorch",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/timgaripov/swa/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 117,
      "date": "Sun, 26 Dec 2021 21:32:35 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/timgaripov/swa/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "timgaripov/swa",
    "technique": "GitHub API"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/timgaripov/swa/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "BSD 2-Clause \"Simplified\" License",
      "url": "https://api.github.com/licenses/bsd-2-clause"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'BSD 2-Clause License\\n\\nCopyright (c) 2018, Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, Andrew Gordon Wilson\\nAll rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met:\\n\\n Redistributions of source code must retain the above copyright notice, this\\n  list of conditions and the following disclaimer.\\n\\n Redistributions in binary form must reproduce the above copyright notice,\\n  this list of conditions and the following disclaimer in the documentation\\n  and/or other materials provided with the distribution.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Stochastic Weight Averaging (SWA)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "swa",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "timgaripov",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/timgaripov/swa/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* [PyTorch](http://pytorch.org/)\n* [torchvision](https://github.com/pytorch/vision/)\n* [tabulate](https://pypi.python.org/pypi/tabulate/)\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 827,
      "date": "Sun, 26 Dec 2021 21:32:35 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The code in this repository implements both SWA and conventional SGD training, with examples on the CIFAR-10 and CIFAR-100 datasets.\n\nTo run SWA use the following command:\n\n```bash\npython3 train.py --dir=<DIR> \\\n                 --dataset=<DATASET> \\\n                 --data_path=<PATH> \\\n                 --model=<MODEL> \\\n                 --epochs=<EPOCHS> \\\n                 --lr_init=<LR_INIT> \\\n                 --wd=<WD> \\\n                 --swa \\\n                 --swa_start=<SWA_START> \\\n                 --swa_lr=<SWA_LR>\n```\n\nParameters:\n\n* ```DIR``` &mdash; path to training directory where checkpoints will be stored\n* ```DATASET``` &mdash; dataset name [CIFAR10/CIFAR100] (default: CIFAR10)\n* ```PATH``` &mdash; path to the data directory\n* ```MODEL``` &mdash; DNN model name:\n    - VGG16/VGG16BN/VGG19/VGG19BN\n    - PreResNet110/PreResNet164\n    - WideResNet28x10\n* ```EPOCHS``` &mdash; number of training epochs (default: 200)\n* ```LR_INIT``` &mdash; initial learning rate (default: 0.1)\n* ```WD``` &mdash; weight decay (default: 1e-4)\n* ```SWA_START``` &mdash; the number of epoch after which SWA will start to average models (default: 161)\n* ```SWA_LR``` &mdash; SWA learning rate (default: 0.05)\n\n\nTo run conventional SGD training use the following command:\n```bash\npython3 train.py --dir=<DIR> \\\n                 --dataset=<DATASET> \\\n                 --data_path=<PATH> \\\n                 --model=<MODEL> \\\n                 --epochs=<EPOCHS> \\\n                 --lr_init=<LR_INIT> \\\n                 --wd=<WD> \n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "To reproduce the results from the paper run (we use same parameters for both CIFAR-10 and CIFAR-100 except for PreResNet):\n```bash\n#:VGG16\npython3 train.py --dir=<DIR> --dataset=CIFAR100 --data_path=<PATH> --model=VGG16 --epochs=200 --lr_init=0.05 --wd=5e-4 #: SGD\npython3 train.py --dir=<DIR> --dataset=CIFAR100 --data_path=<PATH> --model=VGG16 --epochs=300 --lr_init=0.05 --wd=5e-4 --swa --swa_start=161 --swa_lr=0.01 #: SWA 1.5 Budgets\n\n#:PreResNet\npython3 train.py --dir=<DIR> --dataset=CIFAR100 --data_path=<PATH>  --model=[PreResNet110 or PreResNet164] --epochs=150  --lr_init=0.1 --wd=3e-4 #: SGD\n#:CIFAR100\npython3 train.py --dir=<DIR> --dataset=CIFAR100 --data_path=<PATH>  --model=[PreResNet110 or PreResNet164] --epochs=225 --lr_init=0.1 --wd=3e-4 --swa --swa_start=126 --swa_lr=0.05 #: SWA 1.5 Budgets\n#:CIFAR10\npython3 train.py --dir=<DIR> --dataset=CIFAR10 --data_path=<PATH>  --model=[PreResNet110 or PreResNet164] --epochs=225 --lr_init=0.1 --wd=3e-4 --swa --swa_start=126 --swa_lr=0.01 #: SWA 1.5 Budgets\n\n#:WideResNet28x10 \npython3 train.py --dir=<DIR> --dataset=CIFAR100 --data_path=<PATH> --model=WideResNet28x10 --epochs=200 --lr_init=0.1 --wd=5e-4 #: SGD\npython3 train.py --dir=<DIR> --dataset=CIFAR100 --data_path=<PATH> --model=WideResNet28x10 --epochs=300 --lr_init=0.1 --wd=5e-4 --swa --swa_start=161 --swa_lr=0.05 #: SWA 1.5 Budgets\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}