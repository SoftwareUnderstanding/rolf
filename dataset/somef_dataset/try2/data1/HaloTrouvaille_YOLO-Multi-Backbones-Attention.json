{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1709.01507  \nCBAM Block paper : https://arxiv.org/abs/1807.06521  \nECA Block paper : https://arxiv.org/abs/1910.03151  \n| Model | Params | mAP |\n| ----- | ----- | ----- |\n| YOLOv3-tiny | 8.67M | 60.3 |\n| YOLOv3-tiny + SE | 8.933M | 62.3 |\n| YOLOv3-tiny + CBAM | 8.81M | 62.7 |\n| YOLOv3-tiny + ECA | 8.67M | 62.6 |\n\n \n# TODO\n- [x] ShuffleNetV2 backbone\n- [x] HuaWei GhostNet backbone \n- [x] ImageNet pretraining\n- [x] COCO datasets training\n- [ ] Other detection strategies\n- [ ] Other pruning strategies\n\n",
      "https://arxiv.org/abs/1807.06521  \nECA Block paper : https://arxiv.org/abs/1910.03151  \n| Model | Params | mAP |\n| ----- | ----- | ----- |\n| YOLOv3-tiny | 8.67M | 60.3 |\n| YOLOv3-tiny + SE | 8.933M | 62.3 |\n| YOLOv3-tiny + CBAM | 8.81M | 62.7 |\n| YOLOv3-tiny + ECA | 8.67M | 62.6 |\n\n \n# TODO\n- [x] ShuffleNetV2 backbone\n- [x] HuaWei GhostNet backbone \n- [x] ImageNet pretraining\n- [x] COCO datasets training\n- [ ] Other detection strategies\n- [ ] Other pruning strategies\n\n",
      "https://arxiv.org/abs/1910.03151  \n| Model | Params | mAP |\n| ----- | ----- | ----- |\n| YOLOv3-tiny | 8.67M | 60.3 |\n| YOLOv3-tiny + SE | 8.933M | 62.3 |\n| YOLOv3-tiny + CBAM | 8.81M | 62.7 |\n| YOLOv3-tiny + ECA | 8.67M | 62.6 |\n\n \n# TODO\n- [x] ShuffleNetV2 backbone\n- [x] HuaWei GhostNet backbone \n- [x] ImageNet pretraining\n- [x] COCO datasets training\n- [ ] Other detection strategies\n- [ ] Other pruning strategies\n\n"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "| ShuffleNetV2 1x | 3.59M | 13.99MB | 10.2 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.994799811898885,
        0.994799811898885,
        0.994799811898885
      ],
      "excerpt": "SE Block paper : https://arxiv.org/abs/1709.01507 \nCBAM Block paper : https://arxiv.org/abs/1807.06521 \nECA Block paper : https://arxiv.org/abs/1910.03151 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/HaloTrouvaille/YOLO-Multi-Backbones-Attention",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-11-25T10:52:43Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-25T05:58:16Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This Repository includes YOLOv3 with some lightweight backbones (***ShuffleNetV2, GhostNet, VoVNet***), some computer vision attention mechanism (***SE Block, CBAM Block, ECA Block***), pruning,quantization and distillation for GhostNet.\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9108725106018803,
        0.8139326900511574,
        0.8680909436978922
      ],
      "excerpt": "(1) The best lightweight model\u2014\u2014HuaWei GhostNet has been added as the YOLOv3 backbone! It is better than ShuffleNetV2. The result for visdrone dataset is as following. \n(2) Add Dorefa quantization method for arbitrary bit quantization! The result for visdrone dataset is as following. \n(3) And I delete the ShuffleNet and the attention mechanism.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8720741895353464
      ],
      "excerpt": "(1) Add pruning according to NetworkSlimming. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8759967608164038
      ],
      "excerpt": "(3) Add Imagenet pretraining model for GhostNet. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8043249690835851
      ],
      "excerpt": "(1) Add VoVNet as the backbone. The result is excellent. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8547899829850194
      ],
      "excerpt": "| Model | Params | Model Size | mAP | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8547899829850194
      ],
      "excerpt": "| Model | Params | Model Size | mAP | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8061811575056023
      ],
      "excerpt": "| Model | Params | mAP | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Model Compression\u2014YOLOv3 with multi lightweight backbones(ShuffleNetV2 HuaWei GhostNet), attention, prune and quantization",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/HaloTrouvaille/YOLO-Multi-Backbones-Attention/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 108,
      "date": "Sat, 25 Dec 2021 06:05:07 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/HaloTrouvaille/YOLO-Multi-Backbones-Attention/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "HaloTrouvaille/YOLO-Multi-Backbones-Attention",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/HaloTrouvaille/YOLO-Multi-Backbones-Attention/master/weights/download_yolov3_weights.sh",
      "https://raw.githubusercontent.com/HaloTrouvaille/YOLO-Multi-Backbones-Attention/master/utils/evolve.sh",
      "https://raw.githubusercontent.com/HaloTrouvaille/YOLO-Multi-Backbones-Attention/master/utils/gcp.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8375899557827347,
        0.8082178180051712,
        0.8837680365796365,
        0.9322609392449874
      ],
      "excerpt": "Attention : Single GPU will be better \nIf you need previous attention model or have any question, you can add my WeChat: AutrefoisLethe \npython 3.7   \npytorch >= 1.1.0   \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8265730133848285
      ],
      "excerpt": "| Pruned Model+INT8 | 5.81M | 75.1 | 34 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8913653591071321,
        0.9172644293645739
      ],
      "excerpt": "First of all, execute sparse training. \npython3 train.py --data data/visdrone.data --batch-size 4 --cfg cfg/ghost-yolov3-visdrone.cfg --img-size 640 --epochs 300  --device 3 -sr --s 0.0001 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python normal_prune.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9055109191057507
      ],
      "excerpt": "python3 train.py --data data/visdrone.data --batch-size 4 --cfg pruned.cfg --img-size 640 --epochs 300  --device 3 --weights weights/xxx.weighs \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9226444475160742
      ],
      "excerpt": "python3 train.py --data data/visdrone.data --batch-size 16 --cfg cfg/ghostnet-yolov3-visdrone.cfg --img-size 640 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/HaloTrouvaille/YOLO-Multi-Backbones-Attention/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Introduction",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "YOLO-Multi-Backbones-Attention",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "HaloTrouvaille",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/HaloTrouvaille/YOLO-Multi-Backbones-Attention/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 408,
      "date": "Sat, 25 Dec 2021 06:05:07 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "huawei-ghostnet",
      "object-detection-datasets",
      "attention-mechanism",
      "yolo",
      "se",
      "cbam",
      "eca",
      "shufflenet"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Download the datasets, place them in the ***data*** directory    \n2. Train the models by using following command (change the model structure by changing the cfg file)  \n```\n  python3 train.py --data data/visdrone.data --batch-size 16 --cfg cfg/ghost-yolov3-visdrone.cfg --img-size 640\n```\n3. Detect objects using the trained model (place the pictures or videos in the ***samples*** directory)    \n```\n  python3 detect.py --cfg cfg/ghostnet-yolov3-visdrone.cfg --weights weights/best.pt --data data/visdrone.data\n```\n4. Results:  \n![most](https://github.com/HaloTrouvaille/YOLO-Multi-Backbones-Attention/blob/master/output/most.png)  \n![car](https://github.com/HaloTrouvaille/YOLO-Multi-Backbones-Attention/blob/master/output/car.png)  \n![airplane](https://github.com/HaloTrouvaille/YOLO-Multi-Backbones-Attention/blob/master/output/airplane.png)  \n",
      "technique": "Header extraction"
    }
  ]
}