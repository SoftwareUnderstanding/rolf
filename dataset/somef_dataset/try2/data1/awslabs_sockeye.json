{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1712.05690",
      "https://arxiv.org/abs/2101.10035 (2021)\n* Briakou, Eleftheria, Marine Carpuat. \"Beyond Noise: Mitigating the Impact of Fine-grained Semantic Divergences on Neural Machine Translation\". arXiv preprint https://arxiv.org/abs/2105.15087 (2021)\n* Tang, Gongbo, Philipp R\u00f6nchen, Rico Sennrich, Joakim Nivre. \"Revisiting Negation in Neural Machine Translation\". Transactions of the Association for Computation Linguistics 9 (2021)\n* Vu, Thuy, Alessandro Moschitti. \"Machine Translation Customization via Automatic Training Data Selection from the Web\". arXiv preprint https://arxiv.org/abs/2102.1024 (2021)\n* Xu, Weijia, Marine Carpuat. \"EDITOR: An Edit-Based Transformer with Repositioning for Neural Machine Translation with Soft Lexical Constraints.\" Transactions of the Association for Computation Linguistics 9 (2021)\n* M\u00fcller, Mathias, Rico Sennrich. \"Understanding the Properties of Minimum Bayes Risk Decoding in Neural Machine Translation\". Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) (2021)\n* Popovi\u0107, Maja, Alberto Poncelas. \"On Machine Translation of User Reviews.\" Proceedings of RANLP (2021)\n* Popovi\u0107, Maja. \"On nature and causes of observed MT errors.\" Proceedings of the 18th MT Summit (Volume 1: Research Track) (2021)\n* Jain, Nishtha, Maja Popovi\u0107, Declan Groves, Eva Vanmassenhove. \"Generating Gender Augmented Data for NLP.\" Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing (2021)\n* Vilar, David, Marcello Federico. \"A Statistical Extension of Byte-Pair Encoding.\" Proceedings of IWSLT (2021)\n\n### 2020\n\n* Dinu, Georgiana, Prashant Mathur, Marcello Federico, Stanislas Lauly, Yaser Al-Onaizan. \"Joint translation and unit conversion for end-to-end localization.\" Proceedings of IWSLT (2020)\n* Exel, Miriam, Bianka Buschbeck, Lauritz Brandt, Simona Doneva. \"Terminology-Constrained Neural Machine Translation at SAP\". Proceedings of EAMT (2020).\n* Hisamoto, Sorami, Matt Post, Kevin Duh. \"Membership Inference Attacks on Sequence-to-Sequence Models: Is My Data In Your Machine Translation System?\" Transactions of the Association for Computational Linguistics, Volume 8 (2020)\n* Naradowsky, Jason, Xuan Zhan, Kevin Duh. \"Machine Translation System Selection from Bandit Feedback.\" arXiv preprint https://arxiv.org/abs/2002.09646 (2020)\n* Niu, Xing, Prashant Mathur, Georgiana Dinu, Yaser Al-Onaizan. \"Evaluating Robustness to Input Perturbations for Neural Machine Translation\". arXiv preprint \thttps://arxiv.org/abs/2005.00580 (2020)\n* Niu, Xing, Marine Carpuat. \"Controlling Neural Machine Translation Formality with Synthetic Supervision.\" Proceedings of AAAI (2020)\n* Keung, Phillip, Julian Salazar, Yichao Liu, Noah A. Smith. \"Unsupervised Bitext Mining and Translation\nvia Self-Trained Contextual Embeddings.\" arXiv preprint https://arxiv.org/abs/2010.07761 (2020).\n* Sokolov, Alex, Tracy Rohlin, Ariya Rastrow. \"Neural Machine Translation for Multilingual Grapheme-to-Phoneme Conversion.\" arXiv preprint https://arxiv.org/abs/2006.14194 (2020)\n* Stafanovi\u010ds, Art\u016brs, Toms Bergmanis, M\u0101rcis Pinnis. \"Mitigating Gender Bias in Machine Translation with Target Gender\nAnnotations.\" arXiv preprint https://arxiv.org/abs/2010.06203 (2020)\n* Stojanovski, Dario, Alexander Fraser. \"Addressing Zero-Resource Domains Using Document-Level Context in Neural Machine Translation.\" arXiv preprint arXiv preprint https://arxiv.org/abs/2004.14927 (2020)\n* Stojanovski, Dario, Benno Krojer, Denis Peskov, Alexander Fraser. \"ContraCAT: Contrastive Coreference Analytical Templates for Machine Translation\". Proceedings of COLING (2020)\n* Zhang, Xuan, Kevin Duh. \"Reproducible and Efficient Benchmarks for Hyperparameter Optimization of Neural Machine Translation Systems.\" Transactions of the Association for Computational Linguistics, Volume 8 (2020)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant, Nandar Win Min, and Thepchai Supnithi, \"Unsupervised Neural Machine Translation between Myanmar Sign Language and Myanmar Language\", Journal of Intelligent Informatics and Smart Technology, April 1st Issue, 2020, pp. 53-61. (Submitted December 21, 2019; accepted March 6, 2020; revised March 16, 2020; published online April 30, 2020)\n* Thazin Myint Oo, Ye Kyaw Thu, Khin Mar Soe and Thepchai Supnithi, \"Neural Machine Translation between Myanmar (Burmese) and Dawei (Tavoyan)\", In Proceedings of the 18th International Conference on Computer Applications (ICCA 2020), Feb 27-28, 2020, Yangon, Myanmar, pp. 219-227\n* M\u00fcller, Mathias, Annette Rios, Rico Sennrich. \"Domain Robustness in Neural Machine Translation.\" Proceedings of AMTA (2020)\n* Rios, Annette, Mathias M\u00fcller, Rico Sennrich. \"Subword Segmentation and a Single Bridge Language Affect Zero-Shot Neural Machine Translation.\" Proceedings of the 5th WMT: Research Papers (2020)\n* Popovi\u0107, Maja, Alberto Poncelas. \"Neural Machine Translation between similar South-Slavic languages.\" Proceedings of the 5th WMT: Research Papers (2020)\n* Popovi\u0107, Maja, Alberto Poncelas. \"Extracting correctly aligned segments from unclean parallel data using character n-gram matching.\" Proceedings of Conference on Language Technologies & Digital Humanities (JTDH 2020).\n* Popovi\u0107, Maja, Alberto Poncelas, Marija Brkic, Andy Way. \"Neural Machine Translation for translating into Croatian and Serbian.\" Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects (2020)\n\n### 2019\n\n* Agrawal, Sweta, Marine Carpuat. \"Controlling Text Complexity in Neural Machine Translation.\" Proceedings of EMNLP (2019)\n* Beck, Daniel, Trevor Cohn, Gholamreza Haffari. \"Neural Speech Translation using Lattice Transformations and Graph Networks.\" Proceedings of TextGraphs-13 (EMNLP 2019)\n* Currey, Anna, Kenneth Heafield. \"Zero-Resource Neural Machine Translation with Monolingual Pivot Data.\" Proceedings of EMNLP (2019)\n* Gupta, Prabhakar, Mayank Sharma. \"Unsupervised Translation Quality Estimation for Digital Entertainment Content Subtitles.\" IEEE International Journal of Semantic Computing (2019)\n* Hu, J. Edward, Huda Khayrallah, Ryan Culkin, Patrick Xia, Tongfei Chen, Matt Post, and Benjamin Van Durme. \"Improved Lexically Constrained Decoding for Translation and Monolingual Rewriting.\" Proceedings of NAACL-HLT (2019)\n* Rosendahl, Jan, Christian Herold, Yunsu Kim, Miguel Gra\u00e7a,Weiyue Wang, Parnia Bahar, Yingbo Gao and Hermann Ney \u201cThe RWTH Aachen University Machine Translation Systems for WMT 2019\u201d Proceedings of the 4th WMT: Research Papers (2019)\n* Thompson, Brian, Jeremy Gwinnup, Huda Khayrallah, Kevin Duh, and Philipp Koehn. \"Overcoming catastrophic forgetting during domain adaptation of neural machine translation.\" Proceedings of NAACL-HLT 2019 (2019)\n* T\u00e4ttar, Andre, Elizaveta Korotkova, Mark Fishel \u201cUniversity of Tartu\u2019s Multilingual Multi-domain WMT19 News Translation Shared Task Submission\u201d Proceedings of 4th WMT: Research Papers (2019)\n* Thazin Myint Oo, Ye Kyaw Thu and Khin Mar Soe, \"Neural Machine Translation between Myanmar (Burmese) and Rakhine (Arakanese)\", In Proceedings of the Sixth Workshop on NLP for Similar Languages, Varieties and Dialects, NAACL-2019, June 7th 2019, Minneapolis, United States, pp. 80-88\n\n### 2018\n\n* Domhan, Tobias. \"How Much Attention Do You Need? A Granular Analysis of Neural Machine Translation Architectures\". Proceedings of 56th ACL (2018)\n* Kim, Yunsu, Yingbo Gao, and Hermann Ney. \"Effective Cross-lingual Transfer of Neural Machine Translation Models without Shared Vocabularies.\" arXiv preprint https://arxiv.org/abs/1905.05475 (2019)\n* Korotkova, Elizaveta, Maksym Del, and Mark Fishel. \"Monolingual and Cross-lingual Zero-shot Style Transfer.\" arXiv preprint https://arxiv.org/abs/1808.00179 (2018)\n* Niu, Xing, Michael Denkowski, and Marine Carpuat. \"Bi-directional neural machine translation with synthetic parallel data.\" arXiv preprint https://arxiv.org/abs/1805.11213 (2018)\n* Niu, Xing, Sudha Rao, and Marine Carpuat. \"Multi-Task Neural Models for Translating Between Styles Within and Across Languages.\" COLING (2018)\n* Post, Matt and David Vilar. \"Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation.\" Proceedings of NAACL-HLT (2018)\n* Schamper, Julian, Jan Rosendahl, Parnia Bahar, Yunsu Kim, Arne Nix, and Hermann Ney. \"The RWTH Aachen University Supervised Machine Translation Systems for WMT 2018.\" Proceedings of the 3rd WMT: Shared Task Papers (2018)\n* Schulz, Philip, Wilker Aziz, and Trevor Cohn. \"A stochastic decoder for neural machine translation.\" arXiv preprint https://arxiv.org/abs/1805.10844 (2018)\n* Tamer, Alkouli, Gabriel Bretschner, and Hermann Ney. \"On The Alignment Problem In Multi-Head Attention-Based Neural Machine Translation.\" Proceedings of the 3rd WMT: Research Papers (2018)\n* Tang, Gongbo, Rico Sennrich, and Joakim Nivre. \"An Analysis of Attention Mechanisms: The Case of Word Sense Disambiguation in Neural Machine Translation.\" Proceedings of 3rd WMT: Research Papers (2018)\n* Thompson, Brian, Huda Khayrallah, Antonios Anastasopoulos, Arya McCarthy, Kevin Duh, Rebecca Marvin, Paul McNamee, Jeremy Gwinnup, Tim Anderson, and Philipp Koehn. \"Freezing Subnetworks to Analyze Domain Adaptation in Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1809.05218 (2018)\n* Vilar, David. \"Learning Hidden Unit Contribution for Adapting Neural Machine Translation Models.\" Proceedings of NAACL-HLT (2018)\n* Vyas, Yogarshi, Xing Niu and Marine Carpuat \u201cIdentifying Semantic Divergences in Parallel Text without Annotations\u201d. Proceedings of NAACL-HLT (2018)\n* Wang, Weiyue, Derui Zhu, Tamer Alkhouli, Zixuan Gan, and Hermann Ney. \"Neural Hidden Markov Model for Machine Translation\". Proceedings of 56th ACL (2018)\n* Zhang, Xuan, Gaurav Kumar, Huda Khayrallah, Kenton Murray, Jeremy Gwinnup, Marianna J Martindale, Paul McNamee, Kevin Duh, and Marine Carpuat. \"An Empirical Exploration of Curriculum Learning for Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1811.00739 (2018)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant and Nandar Win Min, \"Neural Machine Translation between Myanmar Sign Language and Myanmar Written Text\", In the second Regional Conference on Optical character recognition and Natural language processing technologies for ASEAN languages 2018 (ONA 2018), December 13-14, 2018, Phnom Penh, Cambodia.\n* Tang, Gongbo, Mathias M\u00fcller, Annette Rios and Rico Sennrich. \"Why Self-attention? A Targeted Evaluation of Neural Machine Translation Architectures.\" Proceedings of EMNLP (2018)\n\n### 2017\n\n* Domhan, Tobias and Felix Hieber. \"Using target-side monolingual data for neural machine translation through multi-task learning.\" Proceedings of EMNLP (2017).",
      "https://arxiv.org/abs/2105.15087 (2021)\n* Tang, Gongbo, Philipp R\u00f6nchen, Rico Sennrich, Joakim Nivre. \"Revisiting Negation in Neural Machine Translation\". Transactions of the Association for Computation Linguistics 9 (2021)\n* Vu, Thuy, Alessandro Moschitti. \"Machine Translation Customization via Automatic Training Data Selection from the Web\". arXiv preprint https://arxiv.org/abs/2102.1024 (2021)\n* Xu, Weijia, Marine Carpuat. \"EDITOR: An Edit-Based Transformer with Repositioning for Neural Machine Translation with Soft Lexical Constraints.\" Transactions of the Association for Computation Linguistics 9 (2021)\n* M\u00fcller, Mathias, Rico Sennrich. \"Understanding the Properties of Minimum Bayes Risk Decoding in Neural Machine Translation\". Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) (2021)\n* Popovi\u0107, Maja, Alberto Poncelas. \"On Machine Translation of User Reviews.\" Proceedings of RANLP (2021)\n* Popovi\u0107, Maja. \"On nature and causes of observed MT errors.\" Proceedings of the 18th MT Summit (Volume 1: Research Track) (2021)\n* Jain, Nishtha, Maja Popovi\u0107, Declan Groves, Eva Vanmassenhove. \"Generating Gender Augmented Data for NLP.\" Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing (2021)\n* Vilar, David, Marcello Federico. \"A Statistical Extension of Byte-Pair Encoding.\" Proceedings of IWSLT (2021)\n\n### 2020\n\n* Dinu, Georgiana, Prashant Mathur, Marcello Federico, Stanislas Lauly, Yaser Al-Onaizan. \"Joint translation and unit conversion for end-to-end localization.\" Proceedings of IWSLT (2020)\n* Exel, Miriam, Bianka Buschbeck, Lauritz Brandt, Simona Doneva. \"Terminology-Constrained Neural Machine Translation at SAP\". Proceedings of EAMT (2020).\n* Hisamoto, Sorami, Matt Post, Kevin Duh. \"Membership Inference Attacks on Sequence-to-Sequence Models: Is My Data In Your Machine Translation System?\" Transactions of the Association for Computational Linguistics, Volume 8 (2020)\n* Naradowsky, Jason, Xuan Zhan, Kevin Duh. \"Machine Translation System Selection from Bandit Feedback.\" arXiv preprint https://arxiv.org/abs/2002.09646 (2020)\n* Niu, Xing, Prashant Mathur, Georgiana Dinu, Yaser Al-Onaizan. \"Evaluating Robustness to Input Perturbations for Neural Machine Translation\". arXiv preprint \thttps://arxiv.org/abs/2005.00580 (2020)\n* Niu, Xing, Marine Carpuat. \"Controlling Neural Machine Translation Formality with Synthetic Supervision.\" Proceedings of AAAI (2020)\n* Keung, Phillip, Julian Salazar, Yichao Liu, Noah A. Smith. \"Unsupervised Bitext Mining and Translation\nvia Self-Trained Contextual Embeddings.\" arXiv preprint https://arxiv.org/abs/2010.07761 (2020).\n* Sokolov, Alex, Tracy Rohlin, Ariya Rastrow. \"Neural Machine Translation for Multilingual Grapheme-to-Phoneme Conversion.\" arXiv preprint https://arxiv.org/abs/2006.14194 (2020)\n* Stafanovi\u010ds, Art\u016brs, Toms Bergmanis, M\u0101rcis Pinnis. \"Mitigating Gender Bias in Machine Translation with Target Gender\nAnnotations.\" arXiv preprint https://arxiv.org/abs/2010.06203 (2020)\n* Stojanovski, Dario, Alexander Fraser. \"Addressing Zero-Resource Domains Using Document-Level Context in Neural Machine Translation.\" arXiv preprint arXiv preprint https://arxiv.org/abs/2004.14927 (2020)\n* Stojanovski, Dario, Benno Krojer, Denis Peskov, Alexander Fraser. \"ContraCAT: Contrastive Coreference Analytical Templates for Machine Translation\". Proceedings of COLING (2020)\n* Zhang, Xuan, Kevin Duh. \"Reproducible and Efficient Benchmarks for Hyperparameter Optimization of Neural Machine Translation Systems.\" Transactions of the Association for Computational Linguistics, Volume 8 (2020)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant, Nandar Win Min, and Thepchai Supnithi, \"Unsupervised Neural Machine Translation between Myanmar Sign Language and Myanmar Language\", Journal of Intelligent Informatics and Smart Technology, April 1st Issue, 2020, pp. 53-61. (Submitted December 21, 2019; accepted March 6, 2020; revised March 16, 2020; published online April 30, 2020)\n* Thazin Myint Oo, Ye Kyaw Thu, Khin Mar Soe and Thepchai Supnithi, \"Neural Machine Translation between Myanmar (Burmese) and Dawei (Tavoyan)\", In Proceedings of the 18th International Conference on Computer Applications (ICCA 2020), Feb 27-28, 2020, Yangon, Myanmar, pp. 219-227\n* M\u00fcller, Mathias, Annette Rios, Rico Sennrich. \"Domain Robustness in Neural Machine Translation.\" Proceedings of AMTA (2020)\n* Rios, Annette, Mathias M\u00fcller, Rico Sennrich. \"Subword Segmentation and a Single Bridge Language Affect Zero-Shot Neural Machine Translation.\" Proceedings of the 5th WMT: Research Papers (2020)\n* Popovi\u0107, Maja, Alberto Poncelas. \"Neural Machine Translation between similar South-Slavic languages.\" Proceedings of the 5th WMT: Research Papers (2020)\n* Popovi\u0107, Maja, Alberto Poncelas. \"Extracting correctly aligned segments from unclean parallel data using character n-gram matching.\" Proceedings of Conference on Language Technologies & Digital Humanities (JTDH 2020).\n* Popovi\u0107, Maja, Alberto Poncelas, Marija Brkic, Andy Way. \"Neural Machine Translation for translating into Croatian and Serbian.\" Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects (2020)\n\n### 2019\n\n* Agrawal, Sweta, Marine Carpuat. \"Controlling Text Complexity in Neural Machine Translation.\" Proceedings of EMNLP (2019)\n* Beck, Daniel, Trevor Cohn, Gholamreza Haffari. \"Neural Speech Translation using Lattice Transformations and Graph Networks.\" Proceedings of TextGraphs-13 (EMNLP 2019)\n* Currey, Anna, Kenneth Heafield. \"Zero-Resource Neural Machine Translation with Monolingual Pivot Data.\" Proceedings of EMNLP (2019)\n* Gupta, Prabhakar, Mayank Sharma. \"Unsupervised Translation Quality Estimation for Digital Entertainment Content Subtitles.\" IEEE International Journal of Semantic Computing (2019)\n* Hu, J. Edward, Huda Khayrallah, Ryan Culkin, Patrick Xia, Tongfei Chen, Matt Post, and Benjamin Van Durme. \"Improved Lexically Constrained Decoding for Translation and Monolingual Rewriting.\" Proceedings of NAACL-HLT (2019)\n* Rosendahl, Jan, Christian Herold, Yunsu Kim, Miguel Gra\u00e7a,Weiyue Wang, Parnia Bahar, Yingbo Gao and Hermann Ney \u201cThe RWTH Aachen University Machine Translation Systems for WMT 2019\u201d Proceedings of the 4th WMT: Research Papers (2019)\n* Thompson, Brian, Jeremy Gwinnup, Huda Khayrallah, Kevin Duh, and Philipp Koehn. \"Overcoming catastrophic forgetting during domain adaptation of neural machine translation.\" Proceedings of NAACL-HLT 2019 (2019)\n* T\u00e4ttar, Andre, Elizaveta Korotkova, Mark Fishel \u201cUniversity of Tartu\u2019s Multilingual Multi-domain WMT19 News Translation Shared Task Submission\u201d Proceedings of 4th WMT: Research Papers (2019)\n* Thazin Myint Oo, Ye Kyaw Thu and Khin Mar Soe, \"Neural Machine Translation between Myanmar (Burmese) and Rakhine (Arakanese)\", In Proceedings of the Sixth Workshop on NLP for Similar Languages, Varieties and Dialects, NAACL-2019, June 7th 2019, Minneapolis, United States, pp. 80-88\n\n### 2018\n\n* Domhan, Tobias. \"How Much Attention Do You Need? A Granular Analysis of Neural Machine Translation Architectures\". Proceedings of 56th ACL (2018)\n* Kim, Yunsu, Yingbo Gao, and Hermann Ney. \"Effective Cross-lingual Transfer of Neural Machine Translation Models without Shared Vocabularies.\" arXiv preprint https://arxiv.org/abs/1905.05475 (2019)\n* Korotkova, Elizaveta, Maksym Del, and Mark Fishel. \"Monolingual and Cross-lingual Zero-shot Style Transfer.\" arXiv preprint https://arxiv.org/abs/1808.00179 (2018)\n* Niu, Xing, Michael Denkowski, and Marine Carpuat. \"Bi-directional neural machine translation with synthetic parallel data.\" arXiv preprint https://arxiv.org/abs/1805.11213 (2018)\n* Niu, Xing, Sudha Rao, and Marine Carpuat. \"Multi-Task Neural Models for Translating Between Styles Within and Across Languages.\" COLING (2018)\n* Post, Matt and David Vilar. \"Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation.\" Proceedings of NAACL-HLT (2018)\n* Schamper, Julian, Jan Rosendahl, Parnia Bahar, Yunsu Kim, Arne Nix, and Hermann Ney. \"The RWTH Aachen University Supervised Machine Translation Systems for WMT 2018.\" Proceedings of the 3rd WMT: Shared Task Papers (2018)\n* Schulz, Philip, Wilker Aziz, and Trevor Cohn. \"A stochastic decoder for neural machine translation.\" arXiv preprint https://arxiv.org/abs/1805.10844 (2018)\n* Tamer, Alkouli, Gabriel Bretschner, and Hermann Ney. \"On The Alignment Problem In Multi-Head Attention-Based Neural Machine Translation.\" Proceedings of the 3rd WMT: Research Papers (2018)\n* Tang, Gongbo, Rico Sennrich, and Joakim Nivre. \"An Analysis of Attention Mechanisms: The Case of Word Sense Disambiguation in Neural Machine Translation.\" Proceedings of 3rd WMT: Research Papers (2018)\n* Thompson, Brian, Huda Khayrallah, Antonios Anastasopoulos, Arya McCarthy, Kevin Duh, Rebecca Marvin, Paul McNamee, Jeremy Gwinnup, Tim Anderson, and Philipp Koehn. \"Freezing Subnetworks to Analyze Domain Adaptation in Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1809.05218 (2018)\n* Vilar, David. \"Learning Hidden Unit Contribution for Adapting Neural Machine Translation Models.\" Proceedings of NAACL-HLT (2018)\n* Vyas, Yogarshi, Xing Niu and Marine Carpuat \u201cIdentifying Semantic Divergences in Parallel Text without Annotations\u201d. Proceedings of NAACL-HLT (2018)\n* Wang, Weiyue, Derui Zhu, Tamer Alkhouli, Zixuan Gan, and Hermann Ney. \"Neural Hidden Markov Model for Machine Translation\". Proceedings of 56th ACL (2018)\n* Zhang, Xuan, Gaurav Kumar, Huda Khayrallah, Kenton Murray, Jeremy Gwinnup, Marianna J Martindale, Paul McNamee, Kevin Duh, and Marine Carpuat. \"An Empirical Exploration of Curriculum Learning for Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1811.00739 (2018)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant and Nandar Win Min, \"Neural Machine Translation between Myanmar Sign Language and Myanmar Written Text\", In the second Regional Conference on Optical character recognition and Natural language processing technologies for ASEAN languages 2018 (ONA 2018), December 13-14, 2018, Phnom Penh, Cambodia.\n* Tang, Gongbo, Mathias M\u00fcller, Annette Rios and Rico Sennrich. \"Why Self-attention? A Targeted Evaluation of Neural Machine Translation Architectures.\" Proceedings of EMNLP (2018)\n\n### 2017\n\n* Domhan, Tobias and Felix Hieber. \"Using target-side monolingual data for neural machine translation through multi-task learning.\" Proceedings of EMNLP (2017).",
      "https://arxiv.org/abs/2102.1024 (2021)\n* Xu, Weijia, Marine Carpuat. \"EDITOR: An Edit-Based Transformer with Repositioning for Neural Machine Translation with Soft Lexical Constraints.\" Transactions of the Association for Computation Linguistics 9 (2021)\n* M\u00fcller, Mathias, Rico Sennrich. \"Understanding the Properties of Minimum Bayes Risk Decoding in Neural Machine Translation\". Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) (2021)\n* Popovi\u0107, Maja, Alberto Poncelas. \"On Machine Translation of User Reviews.\" Proceedings of RANLP (2021)\n* Popovi\u0107, Maja. \"On nature and causes of observed MT errors.\" Proceedings of the 18th MT Summit (Volume 1: Research Track) (2021)\n* Jain, Nishtha, Maja Popovi\u0107, Declan Groves, Eva Vanmassenhove. \"Generating Gender Augmented Data for NLP.\" Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing (2021)\n* Vilar, David, Marcello Federico. \"A Statistical Extension of Byte-Pair Encoding.\" Proceedings of IWSLT (2021)\n\n### 2020\n\n* Dinu, Georgiana, Prashant Mathur, Marcello Federico, Stanislas Lauly, Yaser Al-Onaizan. \"Joint translation and unit conversion for end-to-end localization.\" Proceedings of IWSLT (2020)\n* Exel, Miriam, Bianka Buschbeck, Lauritz Brandt, Simona Doneva. \"Terminology-Constrained Neural Machine Translation at SAP\". Proceedings of EAMT (2020).\n* Hisamoto, Sorami, Matt Post, Kevin Duh. \"Membership Inference Attacks on Sequence-to-Sequence Models: Is My Data In Your Machine Translation System?\" Transactions of the Association for Computational Linguistics, Volume 8 (2020)\n* Naradowsky, Jason, Xuan Zhan, Kevin Duh. \"Machine Translation System Selection from Bandit Feedback.\" arXiv preprint https://arxiv.org/abs/2002.09646 (2020)\n* Niu, Xing, Prashant Mathur, Georgiana Dinu, Yaser Al-Onaizan. \"Evaluating Robustness to Input Perturbations for Neural Machine Translation\". arXiv preprint \thttps://arxiv.org/abs/2005.00580 (2020)\n* Niu, Xing, Marine Carpuat. \"Controlling Neural Machine Translation Formality with Synthetic Supervision.\" Proceedings of AAAI (2020)\n* Keung, Phillip, Julian Salazar, Yichao Liu, Noah A. Smith. \"Unsupervised Bitext Mining and Translation\nvia Self-Trained Contextual Embeddings.\" arXiv preprint https://arxiv.org/abs/2010.07761 (2020).\n* Sokolov, Alex, Tracy Rohlin, Ariya Rastrow. \"Neural Machine Translation for Multilingual Grapheme-to-Phoneme Conversion.\" arXiv preprint https://arxiv.org/abs/2006.14194 (2020)\n* Stafanovi\u010ds, Art\u016brs, Toms Bergmanis, M\u0101rcis Pinnis. \"Mitigating Gender Bias in Machine Translation with Target Gender\nAnnotations.\" arXiv preprint https://arxiv.org/abs/2010.06203 (2020)\n* Stojanovski, Dario, Alexander Fraser. \"Addressing Zero-Resource Domains Using Document-Level Context in Neural Machine Translation.\" arXiv preprint arXiv preprint https://arxiv.org/abs/2004.14927 (2020)\n* Stojanovski, Dario, Benno Krojer, Denis Peskov, Alexander Fraser. \"ContraCAT: Contrastive Coreference Analytical Templates for Machine Translation\". Proceedings of COLING (2020)\n* Zhang, Xuan, Kevin Duh. \"Reproducible and Efficient Benchmarks for Hyperparameter Optimization of Neural Machine Translation Systems.\" Transactions of the Association for Computational Linguistics, Volume 8 (2020)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant, Nandar Win Min, and Thepchai Supnithi, \"Unsupervised Neural Machine Translation between Myanmar Sign Language and Myanmar Language\", Journal of Intelligent Informatics and Smart Technology, April 1st Issue, 2020, pp. 53-61. (Submitted December 21, 2019; accepted March 6, 2020; revised March 16, 2020; published online April 30, 2020)\n* Thazin Myint Oo, Ye Kyaw Thu, Khin Mar Soe and Thepchai Supnithi, \"Neural Machine Translation between Myanmar (Burmese) and Dawei (Tavoyan)\", In Proceedings of the 18th International Conference on Computer Applications (ICCA 2020), Feb 27-28, 2020, Yangon, Myanmar, pp. 219-227\n* M\u00fcller, Mathias, Annette Rios, Rico Sennrich. \"Domain Robustness in Neural Machine Translation.\" Proceedings of AMTA (2020)\n* Rios, Annette, Mathias M\u00fcller, Rico Sennrich. \"Subword Segmentation and a Single Bridge Language Affect Zero-Shot Neural Machine Translation.\" Proceedings of the 5th WMT: Research Papers (2020)\n* Popovi\u0107, Maja, Alberto Poncelas. \"Neural Machine Translation between similar South-Slavic languages.\" Proceedings of the 5th WMT: Research Papers (2020)\n* Popovi\u0107, Maja, Alberto Poncelas. \"Extracting correctly aligned segments from unclean parallel data using character n-gram matching.\" Proceedings of Conference on Language Technologies & Digital Humanities (JTDH 2020).\n* Popovi\u0107, Maja, Alberto Poncelas, Marija Brkic, Andy Way. \"Neural Machine Translation for translating into Croatian and Serbian.\" Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects (2020)\n\n### 2019\n\n* Agrawal, Sweta, Marine Carpuat. \"Controlling Text Complexity in Neural Machine Translation.\" Proceedings of EMNLP (2019)\n* Beck, Daniel, Trevor Cohn, Gholamreza Haffari. \"Neural Speech Translation using Lattice Transformations and Graph Networks.\" Proceedings of TextGraphs-13 (EMNLP 2019)\n* Currey, Anna, Kenneth Heafield. \"Zero-Resource Neural Machine Translation with Monolingual Pivot Data.\" Proceedings of EMNLP (2019)\n* Gupta, Prabhakar, Mayank Sharma. \"Unsupervised Translation Quality Estimation for Digital Entertainment Content Subtitles.\" IEEE International Journal of Semantic Computing (2019)\n* Hu, J. Edward, Huda Khayrallah, Ryan Culkin, Patrick Xia, Tongfei Chen, Matt Post, and Benjamin Van Durme. \"Improved Lexically Constrained Decoding for Translation and Monolingual Rewriting.\" Proceedings of NAACL-HLT (2019)\n* Rosendahl, Jan, Christian Herold, Yunsu Kim, Miguel Gra\u00e7a,Weiyue Wang, Parnia Bahar, Yingbo Gao and Hermann Ney \u201cThe RWTH Aachen University Machine Translation Systems for WMT 2019\u201d Proceedings of the 4th WMT: Research Papers (2019)\n* Thompson, Brian, Jeremy Gwinnup, Huda Khayrallah, Kevin Duh, and Philipp Koehn. \"Overcoming catastrophic forgetting during domain adaptation of neural machine translation.\" Proceedings of NAACL-HLT 2019 (2019)\n* T\u00e4ttar, Andre, Elizaveta Korotkova, Mark Fishel \u201cUniversity of Tartu\u2019s Multilingual Multi-domain WMT19 News Translation Shared Task Submission\u201d Proceedings of 4th WMT: Research Papers (2019)\n* Thazin Myint Oo, Ye Kyaw Thu and Khin Mar Soe, \"Neural Machine Translation between Myanmar (Burmese) and Rakhine (Arakanese)\", In Proceedings of the Sixth Workshop on NLP for Similar Languages, Varieties and Dialects, NAACL-2019, June 7th 2019, Minneapolis, United States, pp. 80-88\n\n### 2018\n\n* Domhan, Tobias. \"How Much Attention Do You Need? A Granular Analysis of Neural Machine Translation Architectures\". Proceedings of 56th ACL (2018)\n* Kim, Yunsu, Yingbo Gao, and Hermann Ney. \"Effective Cross-lingual Transfer of Neural Machine Translation Models without Shared Vocabularies.\" arXiv preprint https://arxiv.org/abs/1905.05475 (2019)\n* Korotkova, Elizaveta, Maksym Del, and Mark Fishel. \"Monolingual and Cross-lingual Zero-shot Style Transfer.\" arXiv preprint https://arxiv.org/abs/1808.00179 (2018)\n* Niu, Xing, Michael Denkowski, and Marine Carpuat. \"Bi-directional neural machine translation with synthetic parallel data.\" arXiv preprint https://arxiv.org/abs/1805.11213 (2018)\n* Niu, Xing, Sudha Rao, and Marine Carpuat. \"Multi-Task Neural Models for Translating Between Styles Within and Across Languages.\" COLING (2018)\n* Post, Matt and David Vilar. \"Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation.\" Proceedings of NAACL-HLT (2018)\n* Schamper, Julian, Jan Rosendahl, Parnia Bahar, Yunsu Kim, Arne Nix, and Hermann Ney. \"The RWTH Aachen University Supervised Machine Translation Systems for WMT 2018.\" Proceedings of the 3rd WMT: Shared Task Papers (2018)\n* Schulz, Philip, Wilker Aziz, and Trevor Cohn. \"A stochastic decoder for neural machine translation.\" arXiv preprint https://arxiv.org/abs/1805.10844 (2018)\n* Tamer, Alkouli, Gabriel Bretschner, and Hermann Ney. \"On The Alignment Problem In Multi-Head Attention-Based Neural Machine Translation.\" Proceedings of the 3rd WMT: Research Papers (2018)\n* Tang, Gongbo, Rico Sennrich, and Joakim Nivre. \"An Analysis of Attention Mechanisms: The Case of Word Sense Disambiguation in Neural Machine Translation.\" Proceedings of 3rd WMT: Research Papers (2018)\n* Thompson, Brian, Huda Khayrallah, Antonios Anastasopoulos, Arya McCarthy, Kevin Duh, Rebecca Marvin, Paul McNamee, Jeremy Gwinnup, Tim Anderson, and Philipp Koehn. \"Freezing Subnetworks to Analyze Domain Adaptation in Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1809.05218 (2018)\n* Vilar, David. \"Learning Hidden Unit Contribution for Adapting Neural Machine Translation Models.\" Proceedings of NAACL-HLT (2018)\n* Vyas, Yogarshi, Xing Niu and Marine Carpuat \u201cIdentifying Semantic Divergences in Parallel Text without Annotations\u201d. Proceedings of NAACL-HLT (2018)\n* Wang, Weiyue, Derui Zhu, Tamer Alkhouli, Zixuan Gan, and Hermann Ney. \"Neural Hidden Markov Model for Machine Translation\". Proceedings of 56th ACL (2018)\n* Zhang, Xuan, Gaurav Kumar, Huda Khayrallah, Kenton Murray, Jeremy Gwinnup, Marianna J Martindale, Paul McNamee, Kevin Duh, and Marine Carpuat. \"An Empirical Exploration of Curriculum Learning for Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1811.00739 (2018)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant and Nandar Win Min, \"Neural Machine Translation between Myanmar Sign Language and Myanmar Written Text\", In the second Regional Conference on Optical character recognition and Natural language processing technologies for ASEAN languages 2018 (ONA 2018), December 13-14, 2018, Phnom Penh, Cambodia.\n* Tang, Gongbo, Mathias M\u00fcller, Annette Rios and Rico Sennrich. \"Why Self-attention? A Targeted Evaluation of Neural Machine Translation Architectures.\" Proceedings of EMNLP (2018)\n\n### 2017\n\n* Domhan, Tobias and Felix Hieber. \"Using target-side monolingual data for neural machine translation through multi-task learning.\" Proceedings of EMNLP (2017).",
      "https://arxiv.org/abs/2002.09646 (2020)\n* Niu, Xing, Prashant Mathur, Georgiana Dinu, Yaser Al-Onaizan. \"Evaluating Robustness to Input Perturbations for Neural Machine Translation\". arXiv preprint \thttps://arxiv.org/abs/2005.00580 (2020)\n* Niu, Xing, Marine Carpuat. \"Controlling Neural Machine Translation Formality with Synthetic Supervision.\" Proceedings of AAAI (2020)\n* Keung, Phillip, Julian Salazar, Yichao Liu, Noah A. Smith. \"Unsupervised Bitext Mining and Translation\nvia Self-Trained Contextual Embeddings.\" arXiv preprint https://arxiv.org/abs/2010.07761 (2020).\n* Sokolov, Alex, Tracy Rohlin, Ariya Rastrow. \"Neural Machine Translation for Multilingual Grapheme-to-Phoneme Conversion.\" arXiv preprint https://arxiv.org/abs/2006.14194 (2020)\n* Stafanovi\u010ds, Art\u016brs, Toms Bergmanis, M\u0101rcis Pinnis. \"Mitigating Gender Bias in Machine Translation with Target Gender\nAnnotations.\" arXiv preprint https://arxiv.org/abs/2010.06203 (2020)\n* Stojanovski, Dario, Alexander Fraser. \"Addressing Zero-Resource Domains Using Document-Level Context in Neural Machine Translation.\" arXiv preprint arXiv preprint https://arxiv.org/abs/2004.14927 (2020)\n* Stojanovski, Dario, Benno Krojer, Denis Peskov, Alexander Fraser. \"ContraCAT: Contrastive Coreference Analytical Templates for Machine Translation\". Proceedings of COLING (2020)\n* Zhang, Xuan, Kevin Duh. \"Reproducible and Efficient Benchmarks for Hyperparameter Optimization of Neural Machine Translation Systems.\" Transactions of the Association for Computational Linguistics, Volume 8 (2020)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant, Nandar Win Min, and Thepchai Supnithi, \"Unsupervised Neural Machine Translation between Myanmar Sign Language and Myanmar Language\", Journal of Intelligent Informatics and Smart Technology, April 1st Issue, 2020, pp. 53-61. (Submitted December 21, 2019; accepted March 6, 2020; revised March 16, 2020; published online April 30, 2020)\n* Thazin Myint Oo, Ye Kyaw Thu, Khin Mar Soe and Thepchai Supnithi, \"Neural Machine Translation between Myanmar (Burmese) and Dawei (Tavoyan)\", In Proceedings of the 18th International Conference on Computer Applications (ICCA 2020), Feb 27-28, 2020, Yangon, Myanmar, pp. 219-227\n* M\u00fcller, Mathias, Annette Rios, Rico Sennrich. \"Domain Robustness in Neural Machine Translation.\" Proceedings of AMTA (2020)\n* Rios, Annette, Mathias M\u00fcller, Rico Sennrich. \"Subword Segmentation and a Single Bridge Language Affect Zero-Shot Neural Machine Translation.\" Proceedings of the 5th WMT: Research Papers (2020)\n* Popovi\u0107, Maja, Alberto Poncelas. \"Neural Machine Translation between similar South-Slavic languages.\" Proceedings of the 5th WMT: Research Papers (2020)\n* Popovi\u0107, Maja, Alberto Poncelas. \"Extracting correctly aligned segments from unclean parallel data using character n-gram matching.\" Proceedings of Conference on Language Technologies & Digital Humanities (JTDH 2020).\n* Popovi\u0107, Maja, Alberto Poncelas, Marija Brkic, Andy Way. \"Neural Machine Translation for translating into Croatian and Serbian.\" Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects (2020)\n\n### 2019\n\n* Agrawal, Sweta, Marine Carpuat. \"Controlling Text Complexity in Neural Machine Translation.\" Proceedings of EMNLP (2019)\n* Beck, Daniel, Trevor Cohn, Gholamreza Haffari. \"Neural Speech Translation using Lattice Transformations and Graph Networks.\" Proceedings of TextGraphs-13 (EMNLP 2019)\n* Currey, Anna, Kenneth Heafield. \"Zero-Resource Neural Machine Translation with Monolingual Pivot Data.\" Proceedings of EMNLP (2019)\n* Gupta, Prabhakar, Mayank Sharma. \"Unsupervised Translation Quality Estimation for Digital Entertainment Content Subtitles.\" IEEE International Journal of Semantic Computing (2019)\n* Hu, J. Edward, Huda Khayrallah, Ryan Culkin, Patrick Xia, Tongfei Chen, Matt Post, and Benjamin Van Durme. \"Improved Lexically Constrained Decoding for Translation and Monolingual Rewriting.\" Proceedings of NAACL-HLT (2019)\n* Rosendahl, Jan, Christian Herold, Yunsu Kim, Miguel Gra\u00e7a,Weiyue Wang, Parnia Bahar, Yingbo Gao and Hermann Ney \u201cThe RWTH Aachen University Machine Translation Systems for WMT 2019\u201d Proceedings of the 4th WMT: Research Papers (2019)\n* Thompson, Brian, Jeremy Gwinnup, Huda Khayrallah, Kevin Duh, and Philipp Koehn. \"Overcoming catastrophic forgetting during domain adaptation of neural machine translation.\" Proceedings of NAACL-HLT 2019 (2019)\n* T\u00e4ttar, Andre, Elizaveta Korotkova, Mark Fishel \u201cUniversity of Tartu\u2019s Multilingual Multi-domain WMT19 News Translation Shared Task Submission\u201d Proceedings of 4th WMT: Research Papers (2019)\n* Thazin Myint Oo, Ye Kyaw Thu and Khin Mar Soe, \"Neural Machine Translation between Myanmar (Burmese) and Rakhine (Arakanese)\", In Proceedings of the Sixth Workshop on NLP for Similar Languages, Varieties and Dialects, NAACL-2019, June 7th 2019, Minneapolis, United States, pp. 80-88\n\n### 2018\n\n* Domhan, Tobias. \"How Much Attention Do You Need? A Granular Analysis of Neural Machine Translation Architectures\". Proceedings of 56th ACL (2018)\n* Kim, Yunsu, Yingbo Gao, and Hermann Ney. \"Effective Cross-lingual Transfer of Neural Machine Translation Models without Shared Vocabularies.\" arXiv preprint https://arxiv.org/abs/1905.05475 (2019)\n* Korotkova, Elizaveta, Maksym Del, and Mark Fishel. \"Monolingual and Cross-lingual Zero-shot Style Transfer.\" arXiv preprint https://arxiv.org/abs/1808.00179 (2018)\n* Niu, Xing, Michael Denkowski, and Marine Carpuat. \"Bi-directional neural machine translation with synthetic parallel data.\" arXiv preprint https://arxiv.org/abs/1805.11213 (2018)\n* Niu, Xing, Sudha Rao, and Marine Carpuat. \"Multi-Task Neural Models for Translating Between Styles Within and Across Languages.\" COLING (2018)\n* Post, Matt and David Vilar. \"Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation.\" Proceedings of NAACL-HLT (2018)\n* Schamper, Julian, Jan Rosendahl, Parnia Bahar, Yunsu Kim, Arne Nix, and Hermann Ney. \"The RWTH Aachen University Supervised Machine Translation Systems for WMT 2018.\" Proceedings of the 3rd WMT: Shared Task Papers (2018)\n* Schulz, Philip, Wilker Aziz, and Trevor Cohn. \"A stochastic decoder for neural machine translation.\" arXiv preprint https://arxiv.org/abs/1805.10844 (2018)\n* Tamer, Alkouli, Gabriel Bretschner, and Hermann Ney. \"On The Alignment Problem In Multi-Head Attention-Based Neural Machine Translation.\" Proceedings of the 3rd WMT: Research Papers (2018)\n* Tang, Gongbo, Rico Sennrich, and Joakim Nivre. \"An Analysis of Attention Mechanisms: The Case of Word Sense Disambiguation in Neural Machine Translation.\" Proceedings of 3rd WMT: Research Papers (2018)\n* Thompson, Brian, Huda Khayrallah, Antonios Anastasopoulos, Arya McCarthy, Kevin Duh, Rebecca Marvin, Paul McNamee, Jeremy Gwinnup, Tim Anderson, and Philipp Koehn. \"Freezing Subnetworks to Analyze Domain Adaptation in Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1809.05218 (2018)\n* Vilar, David. \"Learning Hidden Unit Contribution for Adapting Neural Machine Translation Models.\" Proceedings of NAACL-HLT (2018)\n* Vyas, Yogarshi, Xing Niu and Marine Carpuat \u201cIdentifying Semantic Divergences in Parallel Text without Annotations\u201d. Proceedings of NAACL-HLT (2018)\n* Wang, Weiyue, Derui Zhu, Tamer Alkhouli, Zixuan Gan, and Hermann Ney. \"Neural Hidden Markov Model for Machine Translation\". Proceedings of 56th ACL (2018)\n* Zhang, Xuan, Gaurav Kumar, Huda Khayrallah, Kenton Murray, Jeremy Gwinnup, Marianna J Martindale, Paul McNamee, Kevin Duh, and Marine Carpuat. \"An Empirical Exploration of Curriculum Learning for Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1811.00739 (2018)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant and Nandar Win Min, \"Neural Machine Translation between Myanmar Sign Language and Myanmar Written Text\", In the second Regional Conference on Optical character recognition and Natural language processing technologies for ASEAN languages 2018 (ONA 2018), December 13-14, 2018, Phnom Penh, Cambodia.\n* Tang, Gongbo, Mathias M\u00fcller, Annette Rios and Rico Sennrich. \"Why Self-attention? A Targeted Evaluation of Neural Machine Translation Architectures.\" Proceedings of EMNLP (2018)\n\n### 2017\n\n* Domhan, Tobias and Felix Hieber. \"Using target-side monolingual data for neural machine translation through multi-task learning.\" Proceedings of EMNLP (2017).",
      "https://arxiv.org/abs/2005.00580 (2020)\n* Niu, Xing, Marine Carpuat. \"Controlling Neural Machine Translation Formality with Synthetic Supervision.\" Proceedings of AAAI (2020)\n* Keung, Phillip, Julian Salazar, Yichao Liu, Noah A. Smith. \"Unsupervised Bitext Mining and Translation\nvia Self-Trained Contextual Embeddings.\" arXiv preprint https://arxiv.org/abs/2010.07761 (2020).\n* Sokolov, Alex, Tracy Rohlin, Ariya Rastrow. \"Neural Machine Translation for Multilingual Grapheme-to-Phoneme Conversion.\" arXiv preprint https://arxiv.org/abs/2006.14194 (2020)\n* Stafanovi\u010ds, Art\u016brs, Toms Bergmanis, M\u0101rcis Pinnis. \"Mitigating Gender Bias in Machine Translation with Target Gender\nAnnotations.\" arXiv preprint https://arxiv.org/abs/2010.06203 (2020)\n* Stojanovski, Dario, Alexander Fraser. \"Addressing Zero-Resource Domains Using Document-Level Context in Neural Machine Translation.\" arXiv preprint arXiv preprint https://arxiv.org/abs/2004.14927 (2020)\n* Stojanovski, Dario, Benno Krojer, Denis Peskov, Alexander Fraser. \"ContraCAT: Contrastive Coreference Analytical Templates for Machine Translation\". Proceedings of COLING (2020)\n* Zhang, Xuan, Kevin Duh. \"Reproducible and Efficient Benchmarks for Hyperparameter Optimization of Neural Machine Translation Systems.\" Transactions of the Association for Computational Linguistics, Volume 8 (2020)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant, Nandar Win Min, and Thepchai Supnithi, \"Unsupervised Neural Machine Translation between Myanmar Sign Language and Myanmar Language\", Journal of Intelligent Informatics and Smart Technology, April 1st Issue, 2020, pp. 53-61. (Submitted December 21, 2019; accepted March 6, 2020; revised March 16, 2020; published online April 30, 2020)\n* Thazin Myint Oo, Ye Kyaw Thu, Khin Mar Soe and Thepchai Supnithi, \"Neural Machine Translation between Myanmar (Burmese) and Dawei (Tavoyan)\", In Proceedings of the 18th International Conference on Computer Applications (ICCA 2020), Feb 27-28, 2020, Yangon, Myanmar, pp. 219-227\n* M\u00fcller, Mathias, Annette Rios, Rico Sennrich. \"Domain Robustness in Neural Machine Translation.\" Proceedings of AMTA (2020)\n* Rios, Annette, Mathias M\u00fcller, Rico Sennrich. \"Subword Segmentation and a Single Bridge Language Affect Zero-Shot Neural Machine Translation.\" Proceedings of the 5th WMT: Research Papers (2020)\n* Popovi\u0107, Maja, Alberto Poncelas. \"Neural Machine Translation between similar South-Slavic languages.\" Proceedings of the 5th WMT: Research Papers (2020)\n* Popovi\u0107, Maja, Alberto Poncelas. \"Extracting correctly aligned segments from unclean parallel data using character n-gram matching.\" Proceedings of Conference on Language Technologies & Digital Humanities (JTDH 2020).\n* Popovi\u0107, Maja, Alberto Poncelas, Marija Brkic, Andy Way. \"Neural Machine Translation for translating into Croatian and Serbian.\" Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects (2020)\n\n### 2019\n\n* Agrawal, Sweta, Marine Carpuat. \"Controlling Text Complexity in Neural Machine Translation.\" Proceedings of EMNLP (2019)\n* Beck, Daniel, Trevor Cohn, Gholamreza Haffari. \"Neural Speech Translation using Lattice Transformations and Graph Networks.\" Proceedings of TextGraphs-13 (EMNLP 2019)\n* Currey, Anna, Kenneth Heafield. \"Zero-Resource Neural Machine Translation with Monolingual Pivot Data.\" Proceedings of EMNLP (2019)\n* Gupta, Prabhakar, Mayank Sharma. \"Unsupervised Translation Quality Estimation for Digital Entertainment Content Subtitles.\" IEEE International Journal of Semantic Computing (2019)\n* Hu, J. Edward, Huda Khayrallah, Ryan Culkin, Patrick Xia, Tongfei Chen, Matt Post, and Benjamin Van Durme. \"Improved Lexically Constrained Decoding for Translation and Monolingual Rewriting.\" Proceedings of NAACL-HLT (2019)\n* Rosendahl, Jan, Christian Herold, Yunsu Kim, Miguel Gra\u00e7a,Weiyue Wang, Parnia Bahar, Yingbo Gao and Hermann Ney \u201cThe RWTH Aachen University Machine Translation Systems for WMT 2019\u201d Proceedings of the 4th WMT: Research Papers (2019)\n* Thompson, Brian, Jeremy Gwinnup, Huda Khayrallah, Kevin Duh, and Philipp Koehn. \"Overcoming catastrophic forgetting during domain adaptation of neural machine translation.\" Proceedings of NAACL-HLT 2019 (2019)\n* T\u00e4ttar, Andre, Elizaveta Korotkova, Mark Fishel \u201cUniversity of Tartu\u2019s Multilingual Multi-domain WMT19 News Translation Shared Task Submission\u201d Proceedings of 4th WMT: Research Papers (2019)\n* Thazin Myint Oo, Ye Kyaw Thu and Khin Mar Soe, \"Neural Machine Translation between Myanmar (Burmese) and Rakhine (Arakanese)\", In Proceedings of the Sixth Workshop on NLP for Similar Languages, Varieties and Dialects, NAACL-2019, June 7th 2019, Minneapolis, United States, pp. 80-88\n\n### 2018\n\n* Domhan, Tobias. \"How Much Attention Do You Need? A Granular Analysis of Neural Machine Translation Architectures\". Proceedings of 56th ACL (2018)\n* Kim, Yunsu, Yingbo Gao, and Hermann Ney. \"Effective Cross-lingual Transfer of Neural Machine Translation Models without Shared Vocabularies.\" arXiv preprint https://arxiv.org/abs/1905.05475 (2019)\n* Korotkova, Elizaveta, Maksym Del, and Mark Fishel. \"Monolingual and Cross-lingual Zero-shot Style Transfer.\" arXiv preprint https://arxiv.org/abs/1808.00179 (2018)\n* Niu, Xing, Michael Denkowski, and Marine Carpuat. \"Bi-directional neural machine translation with synthetic parallel data.\" arXiv preprint https://arxiv.org/abs/1805.11213 (2018)\n* Niu, Xing, Sudha Rao, and Marine Carpuat. \"Multi-Task Neural Models for Translating Between Styles Within and Across Languages.\" COLING (2018)\n* Post, Matt and David Vilar. \"Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation.\" Proceedings of NAACL-HLT (2018)\n* Schamper, Julian, Jan Rosendahl, Parnia Bahar, Yunsu Kim, Arne Nix, and Hermann Ney. \"The RWTH Aachen University Supervised Machine Translation Systems for WMT 2018.\" Proceedings of the 3rd WMT: Shared Task Papers (2018)\n* Schulz, Philip, Wilker Aziz, and Trevor Cohn. \"A stochastic decoder for neural machine translation.\" arXiv preprint https://arxiv.org/abs/1805.10844 (2018)\n* Tamer, Alkouli, Gabriel Bretschner, and Hermann Ney. \"On The Alignment Problem In Multi-Head Attention-Based Neural Machine Translation.\" Proceedings of the 3rd WMT: Research Papers (2018)\n* Tang, Gongbo, Rico Sennrich, and Joakim Nivre. \"An Analysis of Attention Mechanisms: The Case of Word Sense Disambiguation in Neural Machine Translation.\" Proceedings of 3rd WMT: Research Papers (2018)\n* Thompson, Brian, Huda Khayrallah, Antonios Anastasopoulos, Arya McCarthy, Kevin Duh, Rebecca Marvin, Paul McNamee, Jeremy Gwinnup, Tim Anderson, and Philipp Koehn. \"Freezing Subnetworks to Analyze Domain Adaptation in Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1809.05218 (2018)\n* Vilar, David. \"Learning Hidden Unit Contribution for Adapting Neural Machine Translation Models.\" Proceedings of NAACL-HLT (2018)\n* Vyas, Yogarshi, Xing Niu and Marine Carpuat \u201cIdentifying Semantic Divergences in Parallel Text without Annotations\u201d. Proceedings of NAACL-HLT (2018)\n* Wang, Weiyue, Derui Zhu, Tamer Alkhouli, Zixuan Gan, and Hermann Ney. \"Neural Hidden Markov Model for Machine Translation\". Proceedings of 56th ACL (2018)\n* Zhang, Xuan, Gaurav Kumar, Huda Khayrallah, Kenton Murray, Jeremy Gwinnup, Marianna J Martindale, Paul McNamee, Kevin Duh, and Marine Carpuat. \"An Empirical Exploration of Curriculum Learning for Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1811.00739 (2018)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant and Nandar Win Min, \"Neural Machine Translation between Myanmar Sign Language and Myanmar Written Text\", In the second Regional Conference on Optical character recognition and Natural language processing technologies for ASEAN languages 2018 (ONA 2018), December 13-14, 2018, Phnom Penh, Cambodia.\n* Tang, Gongbo, Mathias M\u00fcller, Annette Rios and Rico Sennrich. \"Why Self-attention? A Targeted Evaluation of Neural Machine Translation Architectures.\" Proceedings of EMNLP (2018)\n\n### 2017\n\n* Domhan, Tobias and Felix Hieber. \"Using target-side monolingual data for neural machine translation through multi-task learning.\" Proceedings of EMNLP (2017).",
      "https://arxiv.org/abs/2010.07761 (2020).\n* Sokolov, Alex, Tracy Rohlin, Ariya Rastrow. \"Neural Machine Translation for Multilingual Grapheme-to-Phoneme Conversion.\" arXiv preprint https://arxiv.org/abs/2006.14194 (2020)\n* Stafanovi\u010ds, Art\u016brs, Toms Bergmanis, M\u0101rcis Pinnis. \"Mitigating Gender Bias in Machine Translation with Target Gender\nAnnotations.\" arXiv preprint https://arxiv.org/abs/2010.06203 (2020)\n* Stojanovski, Dario, Alexander Fraser. \"Addressing Zero-Resource Domains Using Document-Level Context in Neural Machine Translation.\" arXiv preprint arXiv preprint https://arxiv.org/abs/2004.14927 (2020)\n* Stojanovski, Dario, Benno Krojer, Denis Peskov, Alexander Fraser. \"ContraCAT: Contrastive Coreference Analytical Templates for Machine Translation\". Proceedings of COLING (2020)\n* Zhang, Xuan, Kevin Duh. \"Reproducible and Efficient Benchmarks for Hyperparameter Optimization of Neural Machine Translation Systems.\" Transactions of the Association for Computational Linguistics, Volume 8 (2020)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant, Nandar Win Min, and Thepchai Supnithi, \"Unsupervised Neural Machine Translation between Myanmar Sign Language and Myanmar Language\", Journal of Intelligent Informatics and Smart Technology, April 1st Issue, 2020, pp. 53-61. (Submitted December 21, 2019; accepted March 6, 2020; revised March 16, 2020; published online April 30, 2020)\n* Thazin Myint Oo, Ye Kyaw Thu, Khin Mar Soe and Thepchai Supnithi, \"Neural Machine Translation between Myanmar (Burmese) and Dawei (Tavoyan)\", In Proceedings of the 18th International Conference on Computer Applications (ICCA 2020), Feb 27-28, 2020, Yangon, Myanmar, pp. 219-227\n* M\u00fcller, Mathias, Annette Rios, Rico Sennrich. \"Domain Robustness in Neural Machine Translation.\" Proceedings of AMTA (2020)\n* Rios, Annette, Mathias M\u00fcller, Rico Sennrich. \"Subword Segmentation and a Single Bridge Language Affect Zero-Shot Neural Machine Translation.\" Proceedings of the 5th WMT: Research Papers (2020)\n* Popovi\u0107, Maja, Alberto Poncelas. \"Neural Machine Translation between similar South-Slavic languages.\" Proceedings of the 5th WMT: Research Papers (2020)\n* Popovi\u0107, Maja, Alberto Poncelas. \"Extracting correctly aligned segments from unclean parallel data using character n-gram matching.\" Proceedings of Conference on Language Technologies & Digital Humanities (JTDH 2020).\n* Popovi\u0107, Maja, Alberto Poncelas, Marija Brkic, Andy Way. \"Neural Machine Translation for translating into Croatian and Serbian.\" Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects (2020)\n\n### 2019\n\n* Agrawal, Sweta, Marine Carpuat. \"Controlling Text Complexity in Neural Machine Translation.\" Proceedings of EMNLP (2019)\n* Beck, Daniel, Trevor Cohn, Gholamreza Haffari. \"Neural Speech Translation using Lattice Transformations and Graph Networks.\" Proceedings of TextGraphs-13 (EMNLP 2019)\n* Currey, Anna, Kenneth Heafield. \"Zero-Resource Neural Machine Translation with Monolingual Pivot Data.\" Proceedings of EMNLP (2019)\n* Gupta, Prabhakar, Mayank Sharma. \"Unsupervised Translation Quality Estimation for Digital Entertainment Content Subtitles.\" IEEE International Journal of Semantic Computing (2019)\n* Hu, J. Edward, Huda Khayrallah, Ryan Culkin, Patrick Xia, Tongfei Chen, Matt Post, and Benjamin Van Durme. \"Improved Lexically Constrained Decoding for Translation and Monolingual Rewriting.\" Proceedings of NAACL-HLT (2019)\n* Rosendahl, Jan, Christian Herold, Yunsu Kim, Miguel Gra\u00e7a,Weiyue Wang, Parnia Bahar, Yingbo Gao and Hermann Ney \u201cThe RWTH Aachen University Machine Translation Systems for WMT 2019\u201d Proceedings of the 4th WMT: Research Papers (2019)\n* Thompson, Brian, Jeremy Gwinnup, Huda Khayrallah, Kevin Duh, and Philipp Koehn. \"Overcoming catastrophic forgetting during domain adaptation of neural machine translation.\" Proceedings of NAACL-HLT 2019 (2019)\n* T\u00e4ttar, Andre, Elizaveta Korotkova, Mark Fishel \u201cUniversity of Tartu\u2019s Multilingual Multi-domain WMT19 News Translation Shared Task Submission\u201d Proceedings of 4th WMT: Research Papers (2019)\n* Thazin Myint Oo, Ye Kyaw Thu and Khin Mar Soe, \"Neural Machine Translation between Myanmar (Burmese) and Rakhine (Arakanese)\", In Proceedings of the Sixth Workshop on NLP for Similar Languages, Varieties and Dialects, NAACL-2019, June 7th 2019, Minneapolis, United States, pp. 80-88\n\n### 2018\n\n* Domhan, Tobias. \"How Much Attention Do You Need? A Granular Analysis of Neural Machine Translation Architectures\". Proceedings of 56th ACL (2018)\n* Kim, Yunsu, Yingbo Gao, and Hermann Ney. \"Effective Cross-lingual Transfer of Neural Machine Translation Models without Shared Vocabularies.\" arXiv preprint https://arxiv.org/abs/1905.05475 (2019)\n* Korotkova, Elizaveta, Maksym Del, and Mark Fishel. \"Monolingual and Cross-lingual Zero-shot Style Transfer.\" arXiv preprint https://arxiv.org/abs/1808.00179 (2018)\n* Niu, Xing, Michael Denkowski, and Marine Carpuat. \"Bi-directional neural machine translation with synthetic parallel data.\" arXiv preprint https://arxiv.org/abs/1805.11213 (2018)\n* Niu, Xing, Sudha Rao, and Marine Carpuat. \"Multi-Task Neural Models for Translating Between Styles Within and Across Languages.\" COLING (2018)\n* Post, Matt and David Vilar. \"Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation.\" Proceedings of NAACL-HLT (2018)\n* Schamper, Julian, Jan Rosendahl, Parnia Bahar, Yunsu Kim, Arne Nix, and Hermann Ney. \"The RWTH Aachen University Supervised Machine Translation Systems for WMT 2018.\" Proceedings of the 3rd WMT: Shared Task Papers (2018)\n* Schulz, Philip, Wilker Aziz, and Trevor Cohn. \"A stochastic decoder for neural machine translation.\" arXiv preprint https://arxiv.org/abs/1805.10844 (2018)\n* Tamer, Alkouli, Gabriel Bretschner, and Hermann Ney. \"On The Alignment Problem In Multi-Head Attention-Based Neural Machine Translation.\" Proceedings of the 3rd WMT: Research Papers (2018)\n* Tang, Gongbo, Rico Sennrich, and Joakim Nivre. \"An Analysis of Attention Mechanisms: The Case of Word Sense Disambiguation in Neural Machine Translation.\" Proceedings of 3rd WMT: Research Papers (2018)\n* Thompson, Brian, Huda Khayrallah, Antonios Anastasopoulos, Arya McCarthy, Kevin Duh, Rebecca Marvin, Paul McNamee, Jeremy Gwinnup, Tim Anderson, and Philipp Koehn. \"Freezing Subnetworks to Analyze Domain Adaptation in Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1809.05218 (2018)\n* Vilar, David. \"Learning Hidden Unit Contribution for Adapting Neural Machine Translation Models.\" Proceedings of NAACL-HLT (2018)\n* Vyas, Yogarshi, Xing Niu and Marine Carpuat \u201cIdentifying Semantic Divergences in Parallel Text without Annotations\u201d. Proceedings of NAACL-HLT (2018)\n* Wang, Weiyue, Derui Zhu, Tamer Alkhouli, Zixuan Gan, and Hermann Ney. \"Neural Hidden Markov Model for Machine Translation\". Proceedings of 56th ACL (2018)\n* Zhang, Xuan, Gaurav Kumar, Huda Khayrallah, Kenton Murray, Jeremy Gwinnup, Marianna J Martindale, Paul McNamee, Kevin Duh, and Marine Carpuat. \"An Empirical Exploration of Curriculum Learning for Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1811.00739 (2018)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant and Nandar Win Min, \"Neural Machine Translation between Myanmar Sign Language and Myanmar Written Text\", In the second Regional Conference on Optical character recognition and Natural language processing technologies for ASEAN languages 2018 (ONA 2018), December 13-14, 2018, Phnom Penh, Cambodia.\n* Tang, Gongbo, Mathias M\u00fcller, Annette Rios and Rico Sennrich. \"Why Self-attention? A Targeted Evaluation of Neural Machine Translation Architectures.\" Proceedings of EMNLP (2018)\n\n### 2017\n\n* Domhan, Tobias and Felix Hieber. \"Using target-side monolingual data for neural machine translation through multi-task learning.\" Proceedings of EMNLP (2017).",
      "https://arxiv.org/abs/2006.14194 (2020)\n* Stafanovi\u010ds, Art\u016brs, Toms Bergmanis, M\u0101rcis Pinnis. \"Mitigating Gender Bias in Machine Translation with Target Gender\nAnnotations.\" arXiv preprint https://arxiv.org/abs/2010.06203 (2020)\n* Stojanovski, Dario, Alexander Fraser. \"Addressing Zero-Resource Domains Using Document-Level Context in Neural Machine Translation.\" arXiv preprint arXiv preprint https://arxiv.org/abs/2004.14927 (2020)\n* Stojanovski, Dario, Benno Krojer, Denis Peskov, Alexander Fraser. \"ContraCAT: Contrastive Coreference Analytical Templates for Machine Translation\". Proceedings of COLING (2020)\n* Zhang, Xuan, Kevin Duh. \"Reproducible and Efficient Benchmarks for Hyperparameter Optimization of Neural Machine Translation Systems.\" Transactions of the Association for Computational Linguistics, Volume 8 (2020)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant, Nandar Win Min, and Thepchai Supnithi, \"Unsupervised Neural Machine Translation between Myanmar Sign Language and Myanmar Language\", Journal of Intelligent Informatics and Smart Technology, April 1st Issue, 2020, pp. 53-61. (Submitted December 21, 2019; accepted March 6, 2020; revised March 16, 2020; published online April 30, 2020)\n* Thazin Myint Oo, Ye Kyaw Thu, Khin Mar Soe and Thepchai Supnithi, \"Neural Machine Translation between Myanmar (Burmese) and Dawei (Tavoyan)\", In Proceedings of the 18th International Conference on Computer Applications (ICCA 2020), Feb 27-28, 2020, Yangon, Myanmar, pp. 219-227\n* M\u00fcller, Mathias, Annette Rios, Rico Sennrich. \"Domain Robustness in Neural Machine Translation.\" Proceedings of AMTA (2020)\n* Rios, Annette, Mathias M\u00fcller, Rico Sennrich. \"Subword Segmentation and a Single Bridge Language Affect Zero-Shot Neural Machine Translation.\" Proceedings of the 5th WMT: Research Papers (2020)\n* Popovi\u0107, Maja, Alberto Poncelas. \"Neural Machine Translation between similar South-Slavic languages.\" Proceedings of the 5th WMT: Research Papers (2020)\n* Popovi\u0107, Maja, Alberto Poncelas. \"Extracting correctly aligned segments from unclean parallel data using character n-gram matching.\" Proceedings of Conference on Language Technologies & Digital Humanities (JTDH 2020).\n* Popovi\u0107, Maja, Alberto Poncelas, Marija Brkic, Andy Way. \"Neural Machine Translation for translating into Croatian and Serbian.\" Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects (2020)\n\n### 2019\n\n* Agrawal, Sweta, Marine Carpuat. \"Controlling Text Complexity in Neural Machine Translation.\" Proceedings of EMNLP (2019)\n* Beck, Daniel, Trevor Cohn, Gholamreza Haffari. \"Neural Speech Translation using Lattice Transformations and Graph Networks.\" Proceedings of TextGraphs-13 (EMNLP 2019)\n* Currey, Anna, Kenneth Heafield. \"Zero-Resource Neural Machine Translation with Monolingual Pivot Data.\" Proceedings of EMNLP (2019)\n* Gupta, Prabhakar, Mayank Sharma. \"Unsupervised Translation Quality Estimation for Digital Entertainment Content Subtitles.\" IEEE International Journal of Semantic Computing (2019)\n* Hu, J. Edward, Huda Khayrallah, Ryan Culkin, Patrick Xia, Tongfei Chen, Matt Post, and Benjamin Van Durme. \"Improved Lexically Constrained Decoding for Translation and Monolingual Rewriting.\" Proceedings of NAACL-HLT (2019)\n* Rosendahl, Jan, Christian Herold, Yunsu Kim, Miguel Gra\u00e7a,Weiyue Wang, Parnia Bahar, Yingbo Gao and Hermann Ney \u201cThe RWTH Aachen University Machine Translation Systems for WMT 2019\u201d Proceedings of the 4th WMT: Research Papers (2019)\n* Thompson, Brian, Jeremy Gwinnup, Huda Khayrallah, Kevin Duh, and Philipp Koehn. \"Overcoming catastrophic forgetting during domain adaptation of neural machine translation.\" Proceedings of NAACL-HLT 2019 (2019)\n* T\u00e4ttar, Andre, Elizaveta Korotkova, Mark Fishel \u201cUniversity of Tartu\u2019s Multilingual Multi-domain WMT19 News Translation Shared Task Submission\u201d Proceedings of 4th WMT: Research Papers (2019)\n* Thazin Myint Oo, Ye Kyaw Thu and Khin Mar Soe, \"Neural Machine Translation between Myanmar (Burmese) and Rakhine (Arakanese)\", In Proceedings of the Sixth Workshop on NLP for Similar Languages, Varieties and Dialects, NAACL-2019, June 7th 2019, Minneapolis, United States, pp. 80-88\n\n### 2018\n\n* Domhan, Tobias. \"How Much Attention Do You Need? A Granular Analysis of Neural Machine Translation Architectures\". Proceedings of 56th ACL (2018)\n* Kim, Yunsu, Yingbo Gao, and Hermann Ney. \"Effective Cross-lingual Transfer of Neural Machine Translation Models without Shared Vocabularies.\" arXiv preprint https://arxiv.org/abs/1905.05475 (2019)\n* Korotkova, Elizaveta, Maksym Del, and Mark Fishel. \"Monolingual and Cross-lingual Zero-shot Style Transfer.\" arXiv preprint https://arxiv.org/abs/1808.00179 (2018)\n* Niu, Xing, Michael Denkowski, and Marine Carpuat. \"Bi-directional neural machine translation with synthetic parallel data.\" arXiv preprint https://arxiv.org/abs/1805.11213 (2018)\n* Niu, Xing, Sudha Rao, and Marine Carpuat. \"Multi-Task Neural Models for Translating Between Styles Within and Across Languages.\" COLING (2018)\n* Post, Matt and David Vilar. \"Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation.\" Proceedings of NAACL-HLT (2018)\n* Schamper, Julian, Jan Rosendahl, Parnia Bahar, Yunsu Kim, Arne Nix, and Hermann Ney. \"The RWTH Aachen University Supervised Machine Translation Systems for WMT 2018.\" Proceedings of the 3rd WMT: Shared Task Papers (2018)\n* Schulz, Philip, Wilker Aziz, and Trevor Cohn. \"A stochastic decoder for neural machine translation.\" arXiv preprint https://arxiv.org/abs/1805.10844 (2018)\n* Tamer, Alkouli, Gabriel Bretschner, and Hermann Ney. \"On The Alignment Problem In Multi-Head Attention-Based Neural Machine Translation.\" Proceedings of the 3rd WMT: Research Papers (2018)\n* Tang, Gongbo, Rico Sennrich, and Joakim Nivre. \"An Analysis of Attention Mechanisms: The Case of Word Sense Disambiguation in Neural Machine Translation.\" Proceedings of 3rd WMT: Research Papers (2018)\n* Thompson, Brian, Huda Khayrallah, Antonios Anastasopoulos, Arya McCarthy, Kevin Duh, Rebecca Marvin, Paul McNamee, Jeremy Gwinnup, Tim Anderson, and Philipp Koehn. \"Freezing Subnetworks to Analyze Domain Adaptation in Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1809.05218 (2018)\n* Vilar, David. \"Learning Hidden Unit Contribution for Adapting Neural Machine Translation Models.\" Proceedings of NAACL-HLT (2018)\n* Vyas, Yogarshi, Xing Niu and Marine Carpuat \u201cIdentifying Semantic Divergences in Parallel Text without Annotations\u201d. Proceedings of NAACL-HLT (2018)\n* Wang, Weiyue, Derui Zhu, Tamer Alkhouli, Zixuan Gan, and Hermann Ney. \"Neural Hidden Markov Model for Machine Translation\". Proceedings of 56th ACL (2018)\n* Zhang, Xuan, Gaurav Kumar, Huda Khayrallah, Kenton Murray, Jeremy Gwinnup, Marianna J Martindale, Paul McNamee, Kevin Duh, and Marine Carpuat. \"An Empirical Exploration of Curriculum Learning for Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1811.00739 (2018)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant and Nandar Win Min, \"Neural Machine Translation between Myanmar Sign Language and Myanmar Written Text\", In the second Regional Conference on Optical character recognition and Natural language processing technologies for ASEAN languages 2018 (ONA 2018), December 13-14, 2018, Phnom Penh, Cambodia.\n* Tang, Gongbo, Mathias M\u00fcller, Annette Rios and Rico Sennrich. \"Why Self-attention? A Targeted Evaluation of Neural Machine Translation Architectures.\" Proceedings of EMNLP (2018)\n\n### 2017\n\n* Domhan, Tobias and Felix Hieber. \"Using target-side monolingual data for neural machine translation through multi-task learning.\" Proceedings of EMNLP (2017).",
      "https://arxiv.org/abs/2010.06203 (2020)\n* Stojanovski, Dario, Alexander Fraser. \"Addressing Zero-Resource Domains Using Document-Level Context in Neural Machine Translation.\" arXiv preprint arXiv preprint https://arxiv.org/abs/2004.14927 (2020)\n* Stojanovski, Dario, Benno Krojer, Denis Peskov, Alexander Fraser. \"ContraCAT: Contrastive Coreference Analytical Templates for Machine Translation\". Proceedings of COLING (2020)\n* Zhang, Xuan, Kevin Duh. \"Reproducible and Efficient Benchmarks for Hyperparameter Optimization of Neural Machine Translation Systems.\" Transactions of the Association for Computational Linguistics, Volume 8 (2020)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant, Nandar Win Min, and Thepchai Supnithi, \"Unsupervised Neural Machine Translation between Myanmar Sign Language and Myanmar Language\", Journal of Intelligent Informatics and Smart Technology, April 1st Issue, 2020, pp. 53-61. (Submitted December 21, 2019; accepted March 6, 2020; revised March 16, 2020; published online April 30, 2020)\n* Thazin Myint Oo, Ye Kyaw Thu, Khin Mar Soe and Thepchai Supnithi, \"Neural Machine Translation between Myanmar (Burmese) and Dawei (Tavoyan)\", In Proceedings of the 18th International Conference on Computer Applications (ICCA 2020), Feb 27-28, 2020, Yangon, Myanmar, pp. 219-227\n* M\u00fcller, Mathias, Annette Rios, Rico Sennrich. \"Domain Robustness in Neural Machine Translation.\" Proceedings of AMTA (2020)\n* Rios, Annette, Mathias M\u00fcller, Rico Sennrich. \"Subword Segmentation and a Single Bridge Language Affect Zero-Shot Neural Machine Translation.\" Proceedings of the 5th WMT: Research Papers (2020)\n* Popovi\u0107, Maja, Alberto Poncelas. \"Neural Machine Translation between similar South-Slavic languages.\" Proceedings of the 5th WMT: Research Papers (2020)\n* Popovi\u0107, Maja, Alberto Poncelas. \"Extracting correctly aligned segments from unclean parallel data using character n-gram matching.\" Proceedings of Conference on Language Technologies & Digital Humanities (JTDH 2020).\n* Popovi\u0107, Maja, Alberto Poncelas, Marija Brkic, Andy Way. \"Neural Machine Translation for translating into Croatian and Serbian.\" Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects (2020)\n\n### 2019\n\n* Agrawal, Sweta, Marine Carpuat. \"Controlling Text Complexity in Neural Machine Translation.\" Proceedings of EMNLP (2019)\n* Beck, Daniel, Trevor Cohn, Gholamreza Haffari. \"Neural Speech Translation using Lattice Transformations and Graph Networks.\" Proceedings of TextGraphs-13 (EMNLP 2019)\n* Currey, Anna, Kenneth Heafield. \"Zero-Resource Neural Machine Translation with Monolingual Pivot Data.\" Proceedings of EMNLP (2019)\n* Gupta, Prabhakar, Mayank Sharma. \"Unsupervised Translation Quality Estimation for Digital Entertainment Content Subtitles.\" IEEE International Journal of Semantic Computing (2019)\n* Hu, J. Edward, Huda Khayrallah, Ryan Culkin, Patrick Xia, Tongfei Chen, Matt Post, and Benjamin Van Durme. \"Improved Lexically Constrained Decoding for Translation and Monolingual Rewriting.\" Proceedings of NAACL-HLT (2019)\n* Rosendahl, Jan, Christian Herold, Yunsu Kim, Miguel Gra\u00e7a,Weiyue Wang, Parnia Bahar, Yingbo Gao and Hermann Ney \u201cThe RWTH Aachen University Machine Translation Systems for WMT 2019\u201d Proceedings of the 4th WMT: Research Papers (2019)\n* Thompson, Brian, Jeremy Gwinnup, Huda Khayrallah, Kevin Duh, and Philipp Koehn. \"Overcoming catastrophic forgetting during domain adaptation of neural machine translation.\" Proceedings of NAACL-HLT 2019 (2019)\n* T\u00e4ttar, Andre, Elizaveta Korotkova, Mark Fishel \u201cUniversity of Tartu\u2019s Multilingual Multi-domain WMT19 News Translation Shared Task Submission\u201d Proceedings of 4th WMT: Research Papers (2019)\n* Thazin Myint Oo, Ye Kyaw Thu and Khin Mar Soe, \"Neural Machine Translation between Myanmar (Burmese) and Rakhine (Arakanese)\", In Proceedings of the Sixth Workshop on NLP for Similar Languages, Varieties and Dialects, NAACL-2019, June 7th 2019, Minneapolis, United States, pp. 80-88\n\n### 2018\n\n* Domhan, Tobias. \"How Much Attention Do You Need? A Granular Analysis of Neural Machine Translation Architectures\". Proceedings of 56th ACL (2018)\n* Kim, Yunsu, Yingbo Gao, and Hermann Ney. \"Effective Cross-lingual Transfer of Neural Machine Translation Models without Shared Vocabularies.\" arXiv preprint https://arxiv.org/abs/1905.05475 (2019)\n* Korotkova, Elizaveta, Maksym Del, and Mark Fishel. \"Monolingual and Cross-lingual Zero-shot Style Transfer.\" arXiv preprint https://arxiv.org/abs/1808.00179 (2018)\n* Niu, Xing, Michael Denkowski, and Marine Carpuat. \"Bi-directional neural machine translation with synthetic parallel data.\" arXiv preprint https://arxiv.org/abs/1805.11213 (2018)\n* Niu, Xing, Sudha Rao, and Marine Carpuat. \"Multi-Task Neural Models for Translating Between Styles Within and Across Languages.\" COLING (2018)\n* Post, Matt and David Vilar. \"Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation.\" Proceedings of NAACL-HLT (2018)\n* Schamper, Julian, Jan Rosendahl, Parnia Bahar, Yunsu Kim, Arne Nix, and Hermann Ney. \"The RWTH Aachen University Supervised Machine Translation Systems for WMT 2018.\" Proceedings of the 3rd WMT: Shared Task Papers (2018)\n* Schulz, Philip, Wilker Aziz, and Trevor Cohn. \"A stochastic decoder for neural machine translation.\" arXiv preprint https://arxiv.org/abs/1805.10844 (2018)\n* Tamer, Alkouli, Gabriel Bretschner, and Hermann Ney. \"On The Alignment Problem In Multi-Head Attention-Based Neural Machine Translation.\" Proceedings of the 3rd WMT: Research Papers (2018)\n* Tang, Gongbo, Rico Sennrich, and Joakim Nivre. \"An Analysis of Attention Mechanisms: The Case of Word Sense Disambiguation in Neural Machine Translation.\" Proceedings of 3rd WMT: Research Papers (2018)\n* Thompson, Brian, Huda Khayrallah, Antonios Anastasopoulos, Arya McCarthy, Kevin Duh, Rebecca Marvin, Paul McNamee, Jeremy Gwinnup, Tim Anderson, and Philipp Koehn. \"Freezing Subnetworks to Analyze Domain Adaptation in Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1809.05218 (2018)\n* Vilar, David. \"Learning Hidden Unit Contribution for Adapting Neural Machine Translation Models.\" Proceedings of NAACL-HLT (2018)\n* Vyas, Yogarshi, Xing Niu and Marine Carpuat \u201cIdentifying Semantic Divergences in Parallel Text without Annotations\u201d. Proceedings of NAACL-HLT (2018)\n* Wang, Weiyue, Derui Zhu, Tamer Alkhouli, Zixuan Gan, and Hermann Ney. \"Neural Hidden Markov Model for Machine Translation\". Proceedings of 56th ACL (2018)\n* Zhang, Xuan, Gaurav Kumar, Huda Khayrallah, Kenton Murray, Jeremy Gwinnup, Marianna J Martindale, Paul McNamee, Kevin Duh, and Marine Carpuat. \"An Empirical Exploration of Curriculum Learning for Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1811.00739 (2018)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant and Nandar Win Min, \"Neural Machine Translation between Myanmar Sign Language and Myanmar Written Text\", In the second Regional Conference on Optical character recognition and Natural language processing technologies for ASEAN languages 2018 (ONA 2018), December 13-14, 2018, Phnom Penh, Cambodia.\n* Tang, Gongbo, Mathias M\u00fcller, Annette Rios and Rico Sennrich. \"Why Self-attention? A Targeted Evaluation of Neural Machine Translation Architectures.\" Proceedings of EMNLP (2018)\n\n### 2017\n\n* Domhan, Tobias and Felix Hieber. \"Using target-side monolingual data for neural machine translation through multi-task learning.\" Proceedings of EMNLP (2017).",
      "https://arxiv.org/abs/2004.14927 (2020)\n* Stojanovski, Dario, Benno Krojer, Denis Peskov, Alexander Fraser. \"ContraCAT: Contrastive Coreference Analytical Templates for Machine Translation\". Proceedings of COLING (2020)\n* Zhang, Xuan, Kevin Duh. \"Reproducible and Efficient Benchmarks for Hyperparameter Optimization of Neural Machine Translation Systems.\" Transactions of the Association for Computational Linguistics, Volume 8 (2020)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant, Nandar Win Min, and Thepchai Supnithi, \"Unsupervised Neural Machine Translation between Myanmar Sign Language and Myanmar Language\", Journal of Intelligent Informatics and Smart Technology, April 1st Issue, 2020, pp. 53-61. (Submitted December 21, 2019; accepted March 6, 2020; revised March 16, 2020; published online April 30, 2020)\n* Thazin Myint Oo, Ye Kyaw Thu, Khin Mar Soe and Thepchai Supnithi, \"Neural Machine Translation between Myanmar (Burmese) and Dawei (Tavoyan)\", In Proceedings of the 18th International Conference on Computer Applications (ICCA 2020), Feb 27-28, 2020, Yangon, Myanmar, pp. 219-227\n* M\u00fcller, Mathias, Annette Rios, Rico Sennrich. \"Domain Robustness in Neural Machine Translation.\" Proceedings of AMTA (2020)\n* Rios, Annette, Mathias M\u00fcller, Rico Sennrich. \"Subword Segmentation and a Single Bridge Language Affect Zero-Shot Neural Machine Translation.\" Proceedings of the 5th WMT: Research Papers (2020)\n* Popovi\u0107, Maja, Alberto Poncelas. \"Neural Machine Translation between similar South-Slavic languages.\" Proceedings of the 5th WMT: Research Papers (2020)\n* Popovi\u0107, Maja, Alberto Poncelas. \"Extracting correctly aligned segments from unclean parallel data using character n-gram matching.\" Proceedings of Conference on Language Technologies & Digital Humanities (JTDH 2020).\n* Popovi\u0107, Maja, Alberto Poncelas, Marija Brkic, Andy Way. \"Neural Machine Translation for translating into Croatian and Serbian.\" Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects (2020)\n\n### 2019\n\n* Agrawal, Sweta, Marine Carpuat. \"Controlling Text Complexity in Neural Machine Translation.\" Proceedings of EMNLP (2019)\n* Beck, Daniel, Trevor Cohn, Gholamreza Haffari. \"Neural Speech Translation using Lattice Transformations and Graph Networks.\" Proceedings of TextGraphs-13 (EMNLP 2019)\n* Currey, Anna, Kenneth Heafield. \"Zero-Resource Neural Machine Translation with Monolingual Pivot Data.\" Proceedings of EMNLP (2019)\n* Gupta, Prabhakar, Mayank Sharma. \"Unsupervised Translation Quality Estimation for Digital Entertainment Content Subtitles.\" IEEE International Journal of Semantic Computing (2019)\n* Hu, J. Edward, Huda Khayrallah, Ryan Culkin, Patrick Xia, Tongfei Chen, Matt Post, and Benjamin Van Durme. \"Improved Lexically Constrained Decoding for Translation and Monolingual Rewriting.\" Proceedings of NAACL-HLT (2019)\n* Rosendahl, Jan, Christian Herold, Yunsu Kim, Miguel Gra\u00e7a,Weiyue Wang, Parnia Bahar, Yingbo Gao and Hermann Ney \u201cThe RWTH Aachen University Machine Translation Systems for WMT 2019\u201d Proceedings of the 4th WMT: Research Papers (2019)\n* Thompson, Brian, Jeremy Gwinnup, Huda Khayrallah, Kevin Duh, and Philipp Koehn. \"Overcoming catastrophic forgetting during domain adaptation of neural machine translation.\" Proceedings of NAACL-HLT 2019 (2019)\n* T\u00e4ttar, Andre, Elizaveta Korotkova, Mark Fishel \u201cUniversity of Tartu\u2019s Multilingual Multi-domain WMT19 News Translation Shared Task Submission\u201d Proceedings of 4th WMT: Research Papers (2019)\n* Thazin Myint Oo, Ye Kyaw Thu and Khin Mar Soe, \"Neural Machine Translation between Myanmar (Burmese) and Rakhine (Arakanese)\", In Proceedings of the Sixth Workshop on NLP for Similar Languages, Varieties and Dialects, NAACL-2019, June 7th 2019, Minneapolis, United States, pp. 80-88\n\n### 2018\n\n* Domhan, Tobias. \"How Much Attention Do You Need? A Granular Analysis of Neural Machine Translation Architectures\". Proceedings of 56th ACL (2018)\n* Kim, Yunsu, Yingbo Gao, and Hermann Ney. \"Effective Cross-lingual Transfer of Neural Machine Translation Models without Shared Vocabularies.\" arXiv preprint https://arxiv.org/abs/1905.05475 (2019)\n* Korotkova, Elizaveta, Maksym Del, and Mark Fishel. \"Monolingual and Cross-lingual Zero-shot Style Transfer.\" arXiv preprint https://arxiv.org/abs/1808.00179 (2018)\n* Niu, Xing, Michael Denkowski, and Marine Carpuat. \"Bi-directional neural machine translation with synthetic parallel data.\" arXiv preprint https://arxiv.org/abs/1805.11213 (2018)\n* Niu, Xing, Sudha Rao, and Marine Carpuat. \"Multi-Task Neural Models for Translating Between Styles Within and Across Languages.\" COLING (2018)\n* Post, Matt and David Vilar. \"Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation.\" Proceedings of NAACL-HLT (2018)\n* Schamper, Julian, Jan Rosendahl, Parnia Bahar, Yunsu Kim, Arne Nix, and Hermann Ney. \"The RWTH Aachen University Supervised Machine Translation Systems for WMT 2018.\" Proceedings of the 3rd WMT: Shared Task Papers (2018)\n* Schulz, Philip, Wilker Aziz, and Trevor Cohn. \"A stochastic decoder for neural machine translation.\" arXiv preprint https://arxiv.org/abs/1805.10844 (2018)\n* Tamer, Alkouli, Gabriel Bretschner, and Hermann Ney. \"On The Alignment Problem In Multi-Head Attention-Based Neural Machine Translation.\" Proceedings of the 3rd WMT: Research Papers (2018)\n* Tang, Gongbo, Rico Sennrich, and Joakim Nivre. \"An Analysis of Attention Mechanisms: The Case of Word Sense Disambiguation in Neural Machine Translation.\" Proceedings of 3rd WMT: Research Papers (2018)\n* Thompson, Brian, Huda Khayrallah, Antonios Anastasopoulos, Arya McCarthy, Kevin Duh, Rebecca Marvin, Paul McNamee, Jeremy Gwinnup, Tim Anderson, and Philipp Koehn. \"Freezing Subnetworks to Analyze Domain Adaptation in Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1809.05218 (2018)\n* Vilar, David. \"Learning Hidden Unit Contribution for Adapting Neural Machine Translation Models.\" Proceedings of NAACL-HLT (2018)\n* Vyas, Yogarshi, Xing Niu and Marine Carpuat \u201cIdentifying Semantic Divergences in Parallel Text without Annotations\u201d. Proceedings of NAACL-HLT (2018)\n* Wang, Weiyue, Derui Zhu, Tamer Alkhouli, Zixuan Gan, and Hermann Ney. \"Neural Hidden Markov Model for Machine Translation\". Proceedings of 56th ACL (2018)\n* Zhang, Xuan, Gaurav Kumar, Huda Khayrallah, Kenton Murray, Jeremy Gwinnup, Marianna J Martindale, Paul McNamee, Kevin Duh, and Marine Carpuat. \"An Empirical Exploration of Curriculum Learning for Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1811.00739 (2018)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant and Nandar Win Min, \"Neural Machine Translation between Myanmar Sign Language and Myanmar Written Text\", In the second Regional Conference on Optical character recognition and Natural language processing technologies for ASEAN languages 2018 (ONA 2018), December 13-14, 2018, Phnom Penh, Cambodia.\n* Tang, Gongbo, Mathias M\u00fcller, Annette Rios and Rico Sennrich. \"Why Self-attention? A Targeted Evaluation of Neural Machine Translation Architectures.\" Proceedings of EMNLP (2018)\n\n### 2017\n\n* Domhan, Tobias and Felix Hieber. \"Using target-side monolingual data for neural machine translation through multi-task learning.\" Proceedings of EMNLP (2017).",
      "https://arxiv.org/abs/1905.05475 (2019)\n* Korotkova, Elizaveta, Maksym Del, and Mark Fishel. \"Monolingual and Cross-lingual Zero-shot Style Transfer.\" arXiv preprint https://arxiv.org/abs/1808.00179 (2018)\n* Niu, Xing, Michael Denkowski, and Marine Carpuat. \"Bi-directional neural machine translation with synthetic parallel data.\" arXiv preprint https://arxiv.org/abs/1805.11213 (2018)\n* Niu, Xing, Sudha Rao, and Marine Carpuat. \"Multi-Task Neural Models for Translating Between Styles Within and Across Languages.\" COLING (2018)\n* Post, Matt and David Vilar. \"Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation.\" Proceedings of NAACL-HLT (2018)\n* Schamper, Julian, Jan Rosendahl, Parnia Bahar, Yunsu Kim, Arne Nix, and Hermann Ney. \"The RWTH Aachen University Supervised Machine Translation Systems for WMT 2018.\" Proceedings of the 3rd WMT: Shared Task Papers (2018)\n* Schulz, Philip, Wilker Aziz, and Trevor Cohn. \"A stochastic decoder for neural machine translation.\" arXiv preprint https://arxiv.org/abs/1805.10844 (2018)\n* Tamer, Alkouli, Gabriel Bretschner, and Hermann Ney. \"On The Alignment Problem In Multi-Head Attention-Based Neural Machine Translation.\" Proceedings of the 3rd WMT: Research Papers (2018)\n* Tang, Gongbo, Rico Sennrich, and Joakim Nivre. \"An Analysis of Attention Mechanisms: The Case of Word Sense Disambiguation in Neural Machine Translation.\" Proceedings of 3rd WMT: Research Papers (2018)\n* Thompson, Brian, Huda Khayrallah, Antonios Anastasopoulos, Arya McCarthy, Kevin Duh, Rebecca Marvin, Paul McNamee, Jeremy Gwinnup, Tim Anderson, and Philipp Koehn. \"Freezing Subnetworks to Analyze Domain Adaptation in Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1809.05218 (2018)\n* Vilar, David. \"Learning Hidden Unit Contribution for Adapting Neural Machine Translation Models.\" Proceedings of NAACL-HLT (2018)\n* Vyas, Yogarshi, Xing Niu and Marine Carpuat \u201cIdentifying Semantic Divergences in Parallel Text without Annotations\u201d. Proceedings of NAACL-HLT (2018)\n* Wang, Weiyue, Derui Zhu, Tamer Alkhouli, Zixuan Gan, and Hermann Ney. \"Neural Hidden Markov Model for Machine Translation\". Proceedings of 56th ACL (2018)\n* Zhang, Xuan, Gaurav Kumar, Huda Khayrallah, Kenton Murray, Jeremy Gwinnup, Marianna J Martindale, Paul McNamee, Kevin Duh, and Marine Carpuat. \"An Empirical Exploration of Curriculum Learning for Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1811.00739 (2018)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant and Nandar Win Min, \"Neural Machine Translation between Myanmar Sign Language and Myanmar Written Text\", In the second Regional Conference on Optical character recognition and Natural language processing technologies for ASEAN languages 2018 (ONA 2018), December 13-14, 2018, Phnom Penh, Cambodia.\n* Tang, Gongbo, Mathias M\u00fcller, Annette Rios and Rico Sennrich. \"Why Self-attention? A Targeted Evaluation of Neural Machine Translation Architectures.\" Proceedings of EMNLP (2018)\n\n### 2017\n\n* Domhan, Tobias and Felix Hieber. \"Using target-side monolingual data for neural machine translation through multi-task learning.\" Proceedings of EMNLP (2017).",
      "https://arxiv.org/abs/1808.00179 (2018)\n* Niu, Xing, Michael Denkowski, and Marine Carpuat. \"Bi-directional neural machine translation with synthetic parallel data.\" arXiv preprint https://arxiv.org/abs/1805.11213 (2018)\n* Niu, Xing, Sudha Rao, and Marine Carpuat. \"Multi-Task Neural Models for Translating Between Styles Within and Across Languages.\" COLING (2018)\n* Post, Matt and David Vilar. \"Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation.\" Proceedings of NAACL-HLT (2018)\n* Schamper, Julian, Jan Rosendahl, Parnia Bahar, Yunsu Kim, Arne Nix, and Hermann Ney. \"The RWTH Aachen University Supervised Machine Translation Systems for WMT 2018.\" Proceedings of the 3rd WMT: Shared Task Papers (2018)\n* Schulz, Philip, Wilker Aziz, and Trevor Cohn. \"A stochastic decoder for neural machine translation.\" arXiv preprint https://arxiv.org/abs/1805.10844 (2018)\n* Tamer, Alkouli, Gabriel Bretschner, and Hermann Ney. \"On The Alignment Problem In Multi-Head Attention-Based Neural Machine Translation.\" Proceedings of the 3rd WMT: Research Papers (2018)\n* Tang, Gongbo, Rico Sennrich, and Joakim Nivre. \"An Analysis of Attention Mechanisms: The Case of Word Sense Disambiguation in Neural Machine Translation.\" Proceedings of 3rd WMT: Research Papers (2018)\n* Thompson, Brian, Huda Khayrallah, Antonios Anastasopoulos, Arya McCarthy, Kevin Duh, Rebecca Marvin, Paul McNamee, Jeremy Gwinnup, Tim Anderson, and Philipp Koehn. \"Freezing Subnetworks to Analyze Domain Adaptation in Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1809.05218 (2018)\n* Vilar, David. \"Learning Hidden Unit Contribution for Adapting Neural Machine Translation Models.\" Proceedings of NAACL-HLT (2018)\n* Vyas, Yogarshi, Xing Niu and Marine Carpuat \u201cIdentifying Semantic Divergences in Parallel Text without Annotations\u201d. Proceedings of NAACL-HLT (2018)\n* Wang, Weiyue, Derui Zhu, Tamer Alkhouli, Zixuan Gan, and Hermann Ney. \"Neural Hidden Markov Model for Machine Translation\". Proceedings of 56th ACL (2018)\n* Zhang, Xuan, Gaurav Kumar, Huda Khayrallah, Kenton Murray, Jeremy Gwinnup, Marianna J Martindale, Paul McNamee, Kevin Duh, and Marine Carpuat. \"An Empirical Exploration of Curriculum Learning for Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1811.00739 (2018)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant and Nandar Win Min, \"Neural Machine Translation between Myanmar Sign Language and Myanmar Written Text\", In the second Regional Conference on Optical character recognition and Natural language processing technologies for ASEAN languages 2018 (ONA 2018), December 13-14, 2018, Phnom Penh, Cambodia.\n* Tang, Gongbo, Mathias M\u00fcller, Annette Rios and Rico Sennrich. \"Why Self-attention? A Targeted Evaluation of Neural Machine Translation Architectures.\" Proceedings of EMNLP (2018)\n\n### 2017\n\n* Domhan, Tobias and Felix Hieber. \"Using target-side monolingual data for neural machine translation through multi-task learning.\" Proceedings of EMNLP (2017).",
      "https://arxiv.org/abs/1805.11213 (2018)\n* Niu, Xing, Sudha Rao, and Marine Carpuat. \"Multi-Task Neural Models for Translating Between Styles Within and Across Languages.\" COLING (2018)\n* Post, Matt and David Vilar. \"Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation.\" Proceedings of NAACL-HLT (2018)\n* Schamper, Julian, Jan Rosendahl, Parnia Bahar, Yunsu Kim, Arne Nix, and Hermann Ney. \"The RWTH Aachen University Supervised Machine Translation Systems for WMT 2018.\" Proceedings of the 3rd WMT: Shared Task Papers (2018)\n* Schulz, Philip, Wilker Aziz, and Trevor Cohn. \"A stochastic decoder for neural machine translation.\" arXiv preprint https://arxiv.org/abs/1805.10844 (2018)\n* Tamer, Alkouli, Gabriel Bretschner, and Hermann Ney. \"On The Alignment Problem In Multi-Head Attention-Based Neural Machine Translation.\" Proceedings of the 3rd WMT: Research Papers (2018)\n* Tang, Gongbo, Rico Sennrich, and Joakim Nivre. \"An Analysis of Attention Mechanisms: The Case of Word Sense Disambiguation in Neural Machine Translation.\" Proceedings of 3rd WMT: Research Papers (2018)\n* Thompson, Brian, Huda Khayrallah, Antonios Anastasopoulos, Arya McCarthy, Kevin Duh, Rebecca Marvin, Paul McNamee, Jeremy Gwinnup, Tim Anderson, and Philipp Koehn. \"Freezing Subnetworks to Analyze Domain Adaptation in Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1809.05218 (2018)\n* Vilar, David. \"Learning Hidden Unit Contribution for Adapting Neural Machine Translation Models.\" Proceedings of NAACL-HLT (2018)\n* Vyas, Yogarshi, Xing Niu and Marine Carpuat \u201cIdentifying Semantic Divergences in Parallel Text without Annotations\u201d. Proceedings of NAACL-HLT (2018)\n* Wang, Weiyue, Derui Zhu, Tamer Alkhouli, Zixuan Gan, and Hermann Ney. \"Neural Hidden Markov Model for Machine Translation\". Proceedings of 56th ACL (2018)\n* Zhang, Xuan, Gaurav Kumar, Huda Khayrallah, Kenton Murray, Jeremy Gwinnup, Marianna J Martindale, Paul McNamee, Kevin Duh, and Marine Carpuat. \"An Empirical Exploration of Curriculum Learning for Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1811.00739 (2018)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant and Nandar Win Min, \"Neural Machine Translation between Myanmar Sign Language and Myanmar Written Text\", In the second Regional Conference on Optical character recognition and Natural language processing technologies for ASEAN languages 2018 (ONA 2018), December 13-14, 2018, Phnom Penh, Cambodia.\n* Tang, Gongbo, Mathias M\u00fcller, Annette Rios and Rico Sennrich. \"Why Self-attention? A Targeted Evaluation of Neural Machine Translation Architectures.\" Proceedings of EMNLP (2018)\n\n### 2017\n\n* Domhan, Tobias and Felix Hieber. \"Using target-side monolingual data for neural machine translation through multi-task learning.\" Proceedings of EMNLP (2017).",
      "https://arxiv.org/abs/1805.10844 (2018)\n* Tamer, Alkouli, Gabriel Bretschner, and Hermann Ney. \"On The Alignment Problem In Multi-Head Attention-Based Neural Machine Translation.\" Proceedings of the 3rd WMT: Research Papers (2018)\n* Tang, Gongbo, Rico Sennrich, and Joakim Nivre. \"An Analysis of Attention Mechanisms: The Case of Word Sense Disambiguation in Neural Machine Translation.\" Proceedings of 3rd WMT: Research Papers (2018)\n* Thompson, Brian, Huda Khayrallah, Antonios Anastasopoulos, Arya McCarthy, Kevin Duh, Rebecca Marvin, Paul McNamee, Jeremy Gwinnup, Tim Anderson, and Philipp Koehn. \"Freezing Subnetworks to Analyze Domain Adaptation in Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1809.05218 (2018)\n* Vilar, David. \"Learning Hidden Unit Contribution for Adapting Neural Machine Translation Models.\" Proceedings of NAACL-HLT (2018)\n* Vyas, Yogarshi, Xing Niu and Marine Carpuat \u201cIdentifying Semantic Divergences in Parallel Text without Annotations\u201d. Proceedings of NAACL-HLT (2018)\n* Wang, Weiyue, Derui Zhu, Tamer Alkhouli, Zixuan Gan, and Hermann Ney. \"Neural Hidden Markov Model for Machine Translation\". Proceedings of 56th ACL (2018)\n* Zhang, Xuan, Gaurav Kumar, Huda Khayrallah, Kenton Murray, Jeremy Gwinnup, Marianna J Martindale, Paul McNamee, Kevin Duh, and Marine Carpuat. \"An Empirical Exploration of Curriculum Learning for Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1811.00739 (2018)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant and Nandar Win Min, \"Neural Machine Translation between Myanmar Sign Language and Myanmar Written Text\", In the second Regional Conference on Optical character recognition and Natural language processing technologies for ASEAN languages 2018 (ONA 2018), December 13-14, 2018, Phnom Penh, Cambodia.\n* Tang, Gongbo, Mathias M\u00fcller, Annette Rios and Rico Sennrich. \"Why Self-attention? A Targeted Evaluation of Neural Machine Translation Architectures.\" Proceedings of EMNLP (2018)\n\n### 2017\n\n* Domhan, Tobias and Felix Hieber. \"Using target-side monolingual data for neural machine translation through multi-task learning.\" Proceedings of EMNLP (2017).",
      "https://arxiv.org/abs/1809.05218 (2018)\n* Vilar, David. \"Learning Hidden Unit Contribution for Adapting Neural Machine Translation Models.\" Proceedings of NAACL-HLT (2018)\n* Vyas, Yogarshi, Xing Niu and Marine Carpuat \u201cIdentifying Semantic Divergences in Parallel Text without Annotations\u201d. Proceedings of NAACL-HLT (2018)\n* Wang, Weiyue, Derui Zhu, Tamer Alkhouli, Zixuan Gan, and Hermann Ney. \"Neural Hidden Markov Model for Machine Translation\". Proceedings of 56th ACL (2018)\n* Zhang, Xuan, Gaurav Kumar, Huda Khayrallah, Kenton Murray, Jeremy Gwinnup, Marianna J Martindale, Paul McNamee, Kevin Duh, and Marine Carpuat. \"An Empirical Exploration of Curriculum Learning for Neural Machine Translation.\" arXiv preprint https://arxiv.org/abs/1811.00739 (2018)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant and Nandar Win Min, \"Neural Machine Translation between Myanmar Sign Language and Myanmar Written Text\", In the second Regional Conference on Optical character recognition and Natural language processing technologies for ASEAN languages 2018 (ONA 2018), December 13-14, 2018, Phnom Penh, Cambodia.\n* Tang, Gongbo, Mathias M\u00fcller, Annette Rios and Rico Sennrich. \"Why Self-attention? A Targeted Evaluation of Neural Machine Translation Architectures.\" Proceedings of EMNLP (2018)\n\n### 2017\n\n* Domhan, Tobias and Felix Hieber. \"Using target-side monolingual data for neural machine translation through multi-task learning.\" Proceedings of EMNLP (2017).",
      "https://arxiv.org/abs/1811.00739 (2018)\n* Swe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant and Nandar Win Min, \"Neural Machine Translation between Myanmar Sign Language and Myanmar Written Text\", In the second Regional Conference on Optical character recognition and Natural language processing technologies for ASEAN languages 2018 (ONA 2018), December 13-14, 2018, Phnom Penh, Cambodia.\n* Tang, Gongbo, Mathias M\u00fcller, Annette Rios and Rico Sennrich. \"Why Self-attention? A Targeted Evaluation of Neural Machine Translation Architectures.\" Proceedings of EMNLP (2018)\n\n### 2017\n\n* Domhan, Tobias and Felix Hieber. \"Using target-side monolingual data for neural machine translation through multi-task learning.\" Proceedings of EMNLP (2017)."
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "For more information about Sockeye, see our papers ([BibTeX](sockeye.bib)).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8109194328925066,
        0.9874442356791788,
        0.8109194328925066,
        0.9992509669732584,
        0.8109194328925066,
        0.9994050351980508,
        0.9843964928347042,
        0.9933018585123049
      ],
      "excerpt": "Tobias Domhan, Michael Denkowski, David Vilar, Xing Niu, Felix Hieber, Kenneth Heafield. \nThe Sockeye 2 Neural Machine Translation Toolkit at AMTA 2020. Proceedings of the 14th Conference of the Association for Machine Translation in the Americas (AMTA'20). \nFelix Hieber, Tobias Domhan, Michael Denkowski, David Vilar. \nSockeye 2: A Toolkit for Neural Machine Translation. Proceedings of the 22nd Annual Conference of the European Association for Machine Translation, Project Track (EAMT'20). \nFelix Hieber, Tobias Domhan, Michael Denkowski, David Vilar, Artem Sokolov, Ann Clifton, Matt Post. \nThe Sockeye Neural Machine Translation Toolkit at AMTA 2018. Proceedings of the 13th Conference of the Association for Machine Translation in the Americas  (AMTA'18). \nFelix Hieber, Tobias Domhan, Michael Denkowski, David Vilar, Artem Sokolov, Ann Clifton and Matt Post. 2017. \nSockeye: A Toolkit for Neural Machine Translation. ArXiv e-prints. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9064557279119131,
        0.995062399057115,
        0.9996269564132113
      ],
      "excerpt": "If you know more, please let us know or submit a pull request (last updated: August 2021). \nBergmanis, Toms, M\u0101rcis Pinnis. \"Facilitating Terminology Translation with Target Lemma Annotations\". arXiv preprint arXiv:2101.10035 (2021) \nBriakou, Eleftheria, Marine Carpuat. \"Beyond Noise: Mitigating the Impact of Fine-grained Semantic Divergences on Neural Machine Translation\". arXiv preprint arXiv:2105.15087 (2021) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.958555146126918
      ],
      "excerpt": "Vu, Thuy, Alessandro Moschitti. \"Machine Translation Customization via Automatic Training Data Selection from the Web\". arXiv preprint arXiv:2102.1024 (2021) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9978189506957168,
        0.8787202727663734,
        0.9648712073454949
      ],
      "excerpt": "M\u00fcller, Mathias, Rico Sennrich. \"Understanding the Properties of Minimum Bayes Risk Decoding in Neural Machine Translation\". Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) (2021) \nPopovi\u0107, Maja, Alberto Poncelas. \"On Machine Translation of User Reviews.\" Proceedings of RANLP (2021) \nPopovi\u0107, Maja. \"On nature and causes of observed MT errors.\" Proceedings of the 18th MT Summit (Volume 1: Research Track) (2021) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8119362885011652,
        0.9976439145923457,
        0.977265581187069
      ],
      "excerpt": "Vilar, David, Marcello Federico. \"A Statistical Extension of Byte-Pair Encoding.\" Proceedings of IWSLT (2021) \nDinu, Georgiana, Prashant Mathur, Marcello Federico, Stanislas Lauly, Yaser Al-Onaizan. \"Joint translation and unit conversion for end-to-end localization.\" Proceedings of IWSLT (2020) \nExel, Miriam, Bianka Buschbeck, Lauritz Brandt, Simona Doneva. \"Terminology-Constrained Neural Machine Translation at SAP\". Proceedings of EAMT (2020). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9997219652463041,
        0.9993844933316152,
        0.869747438243687,
        0.9726805786299247,
        0.9985956723239058,
        0.9992332308094932
      ],
      "excerpt": "Naradowsky, Jason, Xuan Zhan, Kevin Duh. \"Machine Translation System Selection from Bandit Feedback.\" arXiv preprint arXiv:2002.09646 (2020) \nNiu, Xing, Prashant Mathur, Georgiana Dinu, Yaser Al-Onaizan. \"Evaluating Robustness to Input Perturbations for Neural Machine Translation\". arXiv preprint   arXiv:2005.00580 (2020) \nNiu, Xing, Marine Carpuat. \"Controlling Neural Machine Translation Formality with Synthetic Supervision.\" Proceedings of AAAI (2020) \nKeung, Phillip, Julian Salazar, Yichao Liu, Noah A. Smith. \"Unsupervised Bitext Mining and Translation \nvia Self-Trained Contextual Embeddings.\" arXiv preprint arXiv:2010.07761 (2020). \nSokolov, Alex, Tracy Rohlin, Ariya Rastrow. \"Neural Machine Translation for Multilingual Grapheme-to-Phoneme Conversion.\" arXiv preprint arXiv:2006.14194 (2020) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9996993569696869,
        0.9999689010098674,
        0.9862970228278511,
        0.9982309773155749,
        0.999986273687415,
        0.9998671098710694,
        0.9332019866014951,
        0.9961792811018745,
        0.8674284279971992,
        0.8465684738103354,
        0.8740893527995671,
        0.9873942599616754,
        0.9624786146553362
      ],
      "excerpt": "Annotations.\" arXiv preprint arXiv:2010.06203 (2020) \nStojanovski, Dario, Alexander Fraser. \"Addressing Zero-Resource Domains Using Document-Level Context in Neural Machine Translation.\" arXiv preprint arXiv preprint arXiv:2004.14927 (2020) \nStojanovski, Dario, Benno Krojer, Denis Peskov, Alexander Fraser. \"ContraCAT: Contrastive Coreference Analytical Templates for Machine Translation\". Proceedings of COLING (2020) \nZhang, Xuan, Kevin Duh. \"Reproducible and Efficient Benchmarks for Hyperparameter Optimization of Neural Machine Translation Systems.\" Transactions of the Association for Computational Linguistics, Volume 8 (2020) \nSwe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant, Nandar Win Min, and Thepchai Supnithi, \"Unsupervised Neural Machine Translation between Myanmar Sign Language and Myanmar Language\", Journal of Intelligent Informatics and Smart Technology, April 1st Issue, 2020, pp. 53-61. (Submitted December 21, 2019; accepted March 6, 2020; revised March 16, 2020; published online April 30, 2020) \nThazin Myint Oo, Ye Kyaw Thu, Khin Mar Soe and Thepchai Supnithi, \"Neural Machine Translation between Myanmar (Burmese) and Dawei (Tavoyan)\", In Proceedings of the 18th International Conference on Computer Applications (ICCA 2020), Feb 27-28, 2020, Yangon, Myanmar, pp. 219-227 \nM\u00fcller, Mathias, Annette Rios, Rico Sennrich. \"Domain Robustness in Neural Machine Translation.\" Proceedings of AMTA (2020) \nRios, Annette, Mathias M\u00fcller, Rico Sennrich. \"Subword Segmentation and a Single Bridge Language Affect Zero-Shot Neural Machine Translation.\" Proceedings of the 5th WMT: Research Papers (2020) \nPopovi\u0107, Maja, Alberto Poncelas. \"Neural Machine Translation between similar South-Slavic languages.\" Proceedings of the 5th WMT: Research Papers (2020) \nPopovi\u0107, Maja, Alberto Poncelas. \"Extracting correctly aligned segments from unclean parallel data using character n-gram matching.\" Proceedings of Conference on Language Technologies & Digital Humanities (JTDH 2020). \nPopovi\u0107, Maja, Alberto Poncelas, Marija Brkic, Andy Way. \"Neural Machine Translation for translating into Croatian and Serbian.\" Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects (2020) \nAgrawal, Sweta, Marine Carpuat. \"Controlling Text Complexity in Neural Machine Translation.\" Proceedings of EMNLP (2019) \nBeck, Daniel, Trevor Cohn, Gholamreza Haffari. \"Neural Speech Translation using Lattice Transformations and Graph Networks.\" Proceedings of TextGraphs-13 (EMNLP 2019) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999869856035828,
        0.9999168773783726,
        0.9999998392494107,
        0.9997920227745765,
        0.9992514589095117,
        0.999875039322344,
        0.9942123213556888,
        0.9999743572132611,
        0.9999870714309658,
        0.9994073824279496
      ],
      "excerpt": "Gupta, Prabhakar, Mayank Sharma. \"Unsupervised Translation Quality Estimation for Digital Entertainment Content Subtitles.\" IEEE International Journal of Semantic Computing (2019) \nHu, J. Edward, Huda Khayrallah, Ryan Culkin, Patrick Xia, Tongfei Chen, Matt Post, and Benjamin Van Durme. \"Improved Lexically Constrained Decoding for Translation and Monolingual Rewriting.\" Proceedings of NAACL-HLT (2019) \nRosendahl, Jan, Christian Herold, Yunsu Kim, Miguel Gra\u00e7a,Weiyue Wang, Parnia Bahar, Yingbo Gao and Hermann Ney \u201cThe RWTH Aachen University Machine Translation Systems for WMT 2019\u201d Proceedings of the 4th WMT: Research Papers (2019) \nThompson, Brian, Jeremy Gwinnup, Huda Khayrallah, Kevin Duh, and Philipp Koehn. \"Overcoming catastrophic forgetting during domain adaptation of neural machine translation.\" Proceedings of NAACL-HLT 2019 (2019) \nT\u00e4ttar, Andre, Elizaveta Korotkova, Mark Fishel \u201cUniversity of Tartu\u2019s Multilingual Multi-domain WMT19 News Translation Shared Task Submission\u201d Proceedings of 4th WMT: Research Papers (2019) \nThazin Myint Oo, Ye Kyaw Thu and Khin Mar Soe, \"Neural Machine Translation between Myanmar (Burmese) and Rakhine (Arakanese)\", In Proceedings of the Sixth Workshop on NLP for Similar Languages, Varieties and Dialects, NAACL-2019, June 7th 2019, Minneapolis, United States, pp. 80-88 \nDomhan, Tobias. \"How Much Attention Do You Need? A Granular Analysis of Neural Machine Translation Architectures\". Proceedings of 56th ACL (2018) \nKim, Yunsu, Yingbo Gao, and Hermann Ney. \"Effective Cross-lingual Transfer of Neural Machine Translation Models without Shared Vocabularies.\" arXiv preprint arXiv:1905.05475 (2019) \nKorotkova, Elizaveta, Maksym Del, and Mark Fishel. \"Monolingual and Cross-lingual Zero-shot Style Transfer.\" arXiv preprint arXiv:1808.00179 (2018) \nNiu, Xing, Michael Denkowski, and Marine Carpuat. \"Bi-directional neural machine translation with synthetic parallel data.\" arXiv preprint arXiv:1805.11213 (2018) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/awslabs/sockeye",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "CONTRIBUTING",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-06-08T07:44:30Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-22T18:14:04Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.992468850726366,
        0.9728384285847066
      ],
      "excerpt": "Sockeye is an open-source sequence-to-sequence framework for Neural Machine Translation built on PyTorch. It implements distributed training and optimized inference for state-of-the-art models, powering Amazon Translate and other MT applications. Recent developments and changes are tracked in our CHANGELOG. \nFor a quickstart guide to training a standard NMT model on any size of data, see the WMT 2014 English-German tutorial. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9816330697496715
      ],
      "excerpt": "With version 3.0.0, Sockeye is based on PyTorch. We maintain backwards compatibility with \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8146690258999753
      ],
      "excerpt": "can be converted to models running with PyTorch using the converter CLI (sockeye.mx_to_pt). This will \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356944176323139,
        0.9881349715738458
      ],
      "excerpt": "file to &lt;model&gt;/params.best.mx. Note that this only applies to fully-trained models that are to be used \nfor inference. Continued training of an MXNet model with PyTorch is not supported \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9512149597493866,
        0.939756502496341
      ],
      "excerpt": "Sockeye 2.x, based on the MXNet Gluon API, is available in the sockeye_2 branch. \nSockeye 1.x, based on the MXNet Module API, is available in the sockeye_1 branch. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8862313285837988
      ],
      "excerpt": "The Sockeye 2 Neural Machine Translation Toolkit at AMTA 2020. Proceedings of the 14th Conference of the Association for Machine Translation in the Americas (AMTA'20). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9090513537984101
      ],
      "excerpt": "Sockeye 2: A Toolkit for Neural Machine Translation. Proceedings of the 22nd Annual Conference of the European Association for Machine Translation, Project Track (EAMT'20). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8283096190611764
      ],
      "excerpt": "The Sockeye Neural Machine Translation Toolkit at AMTA 2018. Proceedings of the 13th Conference of the Association for Machine Translation in the Americas  (AMTA'18). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9461470372200336
      ],
      "excerpt": "Tang, Gongbo, Philipp R\u00f6nchen, Rico Sennrich, Joakim Nivre. \"Revisiting Negation in Neural Machine Translation\". Transactions of the Association for Computation Linguistics 9 (2021) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9891101861574861,
        0.9601155081082985,
        0.9305639882098518
      ],
      "excerpt": "Xu, Weijia, Marine Carpuat. \"EDITOR: An Edit-Based Transformer with Repositioning for Neural Machine Translation with Soft Lexical Constraints.\" Transactions of the Association for Computation Linguistics 9 (2021) \nM\u00fcller, Mathias, Rico Sennrich. \"Understanding the Properties of Minimum Bayes Risk Decoding in Neural Machine Translation\". Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) (2021) \nPopovi\u0107, Maja, Alberto Poncelas. \"On Machine Translation of User Reviews.\" Proceedings of RANLP (2021) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9622781962663473
      ],
      "excerpt": "Jain, Nishtha, Maja Popovi\u0107, Declan Groves, Eva Vanmassenhove. \"Generating Gender Augmented Data for NLP.\" Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing (2021) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.850819077145495
      ],
      "excerpt": "Dinu, Georgiana, Prashant Mathur, Marcello Federico, Stanislas Lauly, Yaser Al-Onaizan. \"Joint translation and unit conversion for end-to-end localization.\" Proceedings of IWSLT (2020) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9589130071997564
      ],
      "excerpt": "Hisamoto, Sorami, Matt Post, Kevin Duh. \"Membership Inference Attacks on Sequence-to-Sequence Models: Is My Data In Your Machine Translation System?\" Transactions of the Association for Computational Linguistics, Volume 8 (2020) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8444814044283137
      ],
      "excerpt": "Niu, Xing, Marine Carpuat. \"Controlling Neural Machine Translation Formality with Synthetic Supervision.\" Proceedings of AAAI (2020) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8231714289961519,
        0.8787819969639867,
        0.8029169769454282
      ],
      "excerpt": "Stojanovski, Dario, Benno Krojer, Denis Peskov, Alexander Fraser. \"ContraCAT: Contrastive Coreference Analytical Templates for Machine Translation\". Proceedings of COLING (2020) \nZhang, Xuan, Kevin Duh. \"Reproducible and Efficient Benchmarks for Hyperparameter Optimization of Neural Machine Translation Systems.\" Transactions of the Association for Computational Linguistics, Volume 8 (2020) \nSwe Zin Moe, Ye Kyaw Thu, Hnin Aye Thant, Nandar Win Min, and Thepchai Supnithi, \"Unsupervised Neural Machine Translation between Myanmar Sign Language and Myanmar Language\", Journal of Intelligent Informatics and Smart Technology, April 1st Issue, 2020, pp. 53-61. (Submitted December 21, 2019; accepted March 6, 2020; revised March 16, 2020; published online April 30, 2020) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8564264701792517
      ],
      "excerpt": "Rios, Annette, Mathias M\u00fcller, Rico Sennrich. \"Subword Segmentation and a Single Bridge Language Affect Zero-Shot Neural Machine Translation.\" Proceedings of the 5th WMT: Research Papers (2020) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8440148173393602,
        0.9590885729237851
      ],
      "excerpt": "Popovi\u0107, Maja, Alberto Poncelas. \"Extracting correctly aligned segments from unclean parallel data using character n-gram matching.\" Proceedings of Conference on Language Technologies & Digital Humanities (JTDH 2020). \nPopovi\u0107, Maja, Alberto Poncelas, Marija Brkic, Andy Way. \"Neural Machine Translation for translating into Croatian and Serbian.\" Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects (2020) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8776375729447835,
        0.8416713600943883
      ],
      "excerpt": "Beck, Daniel, Trevor Cohn, Gholamreza Haffari. \"Neural Speech Translation using Lattice Transformations and Graph Networks.\" Proceedings of TextGraphs-13 (EMNLP 2019) \nCurrey, Anna, Kenneth Heafield. \"Zero-Resource Neural Machine Translation with Monolingual Pivot Data.\" Proceedings of EMNLP (2019) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8128274288768271
      ],
      "excerpt": "Thazin Myint Oo, Ye Kyaw Thu and Khin Mar Soe, \"Neural Machine Translation between Myanmar (Burmese) and Rakhine (Arakanese)\", In Proceedings of the Sixth Workshop on NLP for Similar Languages, Varieties and Dialects, NAACL-2019, June 7th 2019, Minneapolis, United States, pp. 80-88 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9486101148130944
      ],
      "excerpt": "Tang, Gongbo, Rico Sennrich, and Joakim Nivre. \"An Analysis of Attention Mechanisms: The Case of Word Sense Disambiguation in Neural Machine Translation.\" Proceedings of 3rd WMT: Research Papers (2018) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Sequence-to-sequence framework with a focus on Neural Machine Translation based on PyTorch",
      "technique": "GitHub API"
    }
  ],
  "documentation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- For information on how to use Sockeye, please visit [our documentation](https://awslabs.github.io/sockeye/).\n- Developers may be interested in our [developer guidelines](https://awslabs.github.io/sockeye/development.html).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "http://sockeye.readthedocs.io/",
      "technique": "Regular expression"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/awslabs/sockeye/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 303,
      "date": "Sun, 26 Dec 2021 21:27:45 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/awslabs/sockeye/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "awslabs/sockeye",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/awslabs/sockeye/main/sockeye_contrib/fast_align/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/awslabs/sockeye/tree/main/docs"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/awslabs/sockeye/main/style-check.sh",
      "https://raw.githubusercontent.com/awslabs/sockeye/main/sockeye_contrib/fast_align/lex_table.sh",
      "https://raw.githubusercontent.com/awslabs/sockeye/main/sockeye_contrib/fast_align/build.sh",
      "https://raw.githubusercontent.com/awslabs/sockeye/main/docs/tutorials/multilingual/prepare-iwslt17-multilingual.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Download the current version of Sockeye:\n```bash\ngit clone https://github.com/awslabs/sockeye.git\n```\n\nInstall the sockeye module and its dependencies:\n```bash\ncd sockeye && pip3 install --editable .\n```\n\nFor faster GPU training, install [NVIDIA Apex](https://github.com/NVIDIA/apex). NVIDIA also provides [PyTorch Docker containers](https://ngc.nvidia.com/catalog/containers/nvidia:pytorch) that include Apex.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.841544028326253,
        0.9713215244673717,
        0.8149671461781824,
        0.8174715585480736
      ],
      "excerpt": "With version 3.0.0, Sockeye is based on PyTorch. We maintain backwards compatibility with \nMXNet models in version 2.3.x a little bit longer. If MXNet 2.x is installed, Sockeye can run both with PyTorch or MXNet. \nAll models trained with 2.3.x (using MXNet) \ncan be converted to models running with PyTorch using the converter CLI (sockeye.mx_to_pt). This will \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9782434799078226,
        0.9319306507511096
      ],
      "excerpt": "sockeye.mx_to_pt requires MXNet to be installed into the environment. \nAll CLIs of Version 3.0.0 now use PyTorch by default, e.g. sockeye-{train,translate,score}. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9517584937887421,
        0.9854242685291102
      ],
      "excerpt": "equivalence between PyTorch and MXNet models. Note that running Sockeye 3.0.0 with MXNet requires MXNet 2.x to be \ninstalled (pip install --pre -f https://dist.mxnet.io/python 'mxnet&gt;=2.0.0b2021') \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/awslabs/sockeye/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "JavaScript",
      "Shell",
      "TeX",
      "Dockerfile",
      "CSS"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Apache License 2.0",
      "url": "https://api.github.com/licenses/apache-2.0"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'                                 Apache License\\n                           Version 2.0, January 2004\\n                        http://www.apache.org/licenses/\\n\\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\\n\\n   1. Definitions.\\n\\n      \"License\" shall mean the terms and conditions for use, reproduction,\\n      and distribution as defined by Sections 1 through 9 of this document.\\n\\n      \"Licensor\" shall mean the copyright owner or entity authorized by\\n      the copyright owner that is granting the License.\\n\\n      \"Legal Entity\" shall mean the union of the acting entity and all\\n      other entities that control, are controlled by, or are under common\\n      control with that entity. For the purposes of this definition,\\n      \"control\" means (i) the power, direct or indirect, to cause the\\n      direction or management of such entity, whether by contract or\\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\\n      outstanding shares, or (iii) beneficial ownership of such entity.\\n\\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\\n      exercising permissions granted by this License.\\n\\n      \"Source\" form shall mean the preferred form for making modifications,\\n      including but not limited to software source code, documentation\\n      source, and configuration files.\\n\\n      \"Object\" form shall mean any form resulting from mechanical\\n      transformation or translation of a Source form, including but\\n      not limited to compiled object code, generated documentation,\\n      and conversions to other media types.\\n\\n      \"Work\" shall mean the work of authorship, whether in Source or\\n      Object form, made available under the License, as indicated by a\\n      copyright notice that is included in or attached to the work\\n      (an example is provided in the Appendix below).\\n\\n      \"Derivative Works\" shall mean any work, whether in Source or Object\\n      form, that is based on (or derived from) the Work and for which the\\n      editorial revisions, annotations, elaborations, or other modifications\\n      represent, as a whole, an original work of authorship. For the purposes\\n      of this License, Derivative Works shall not include works that remain\\n      separable from, or merely link (or bind by name) to the interfaces of,\\n      the Work and Derivative Works thereof.\\n\\n      \"Contribution\" shall mean any work of authorship, including\\n      the original version of the Work and any modifications or additions\\n      to that Work or Derivative Works thereof, that is intentionally\\n      submitted to Licensor for inclusion in the Work by the copyright owner\\n      or by an individual or Legal Entity authorized to submit on behalf of\\n      the copyright owner. For the purposes of this definition, \"submitted\"\\n      means any form of electronic, verbal, or written communication sent\\n      to the Licensor or its representatives, including but not limited to\\n      communication on electronic mailing lists, source code control systems,\\n      and issue tracking systems that are managed by, or on behalf of, the\\n      Licensor for the purpose of discussing and improving the Work, but\\n      excluding communication that is conspicuously marked or otherwise\\n      designated in writing by the copyright owner as \"Not a Contribution.\"\\n\\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\\n      on behalf of whom a Contribution has been received by Licensor and\\n      subsequently incorporated within the Work.\\n\\n   2. Grant of Copyright License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      copyright license to reproduce, prepare Derivative Works of,\\n      publicly display, publicly perform, sublicense, and distribute the\\n      Work and such Derivative Works in Source or Object form.\\n\\n   3. Grant of Patent License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      (except as stated in this section) patent license to make, have made,\\n      use, offer to sell, sell, import, and otherwise transfer the Work,\\n      where such license applies only to those patent claims licensable\\n      by such Contributor that are necessarily infringed by their\\n      Contribution(s) alone or by combination of their Contribution(s)\\n      with the Work to which such Contribution(s) was submitted. If You\\n      institute patent litigation against any entity (including a\\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\\n      or a Contribution incorporated within the Work constitutes direct\\n      or contributory patent infringement, then any patent licenses\\n      granted to You under this License for that Work shall terminate\\n      as of the date such litigation is filed.\\n\\n   4. Redistribution. You may reproduce and distribute copies of the\\n      Work or Derivative Works thereof in any medium, with or without\\n      modifications, and in Source or Object form, provided that You\\n      meet the following conditions:\\n\\n      (a) You must give any other recipients of the Work or\\n          Derivative Works a copy of this License; and\\n\\n      (b) You must cause any modified files to carry prominent notices\\n          stating that You changed the files; and\\n\\n      (c) You must retain, in the Source form of any Derivative Works\\n          that You distribute, all copyright, patent, trademark, and\\n          attribution notices from the Source form of the Work,\\n          excluding those notices that do not pertain to any part of\\n          the Derivative Works; and\\n\\n      (d) If the Work includes a \"NOTICE\" text file as part of its\\n          distribution, then any Derivative Works that You distribute must\\n          include a readable copy of the attribution notices contained\\n          within such NOTICE file, excluding those notices that do not\\n          pertain to any part of the Derivative Works, in at least one\\n          of the following places: within a NOTICE text file distributed\\n          as part of the Derivative Works; within the Source form or\\n          documentation, if provided along with the Derivative Works; or,\\n          within a display generated by the Derivative Works, if and\\n          wherever such third-party notices normally appear. The contents\\n          of the NOTICE file are for informational purposes only and\\n          do not modify the License. You may add Your own attribution\\n          notices within Derivative Works that You distribute, alongside\\n          or as an addendum to the NOTICE text from the Work, provided\\n          that such additional attribution notices cannot be construed\\n          as modifying the License.\\n\\n      You may add Your own copyright statement to Your modifications and\\n      may provide additional or different license terms and conditions\\n      for use, reproduction, or distribution of Your modifications, or\\n      for any such Derivative Works as a whole, provided Your use,\\n      reproduction, and distribution of the Work otherwise complies with\\n      the conditions stated in this License.\\n\\n   5. Submission of Contributions. Unless You explicitly state otherwise,\\n      any Contribution intentionally submitted for inclusion in the Work\\n      by You to the Licensor shall be under the terms and conditions of\\n      this License, without any additional terms or conditions.\\n      Notwithstanding the above, nothing herein shall supersede or modify\\n      the terms of any separate license agreement you may have executed\\n      with Licensor regarding such Contributions.\\n\\n   6. Trademarks. This License does not grant permission to use the trade\\n      names, trademarks, service marks, or product names of the Licensor,\\n      except as required for reasonable and customary use in describing the\\n      origin of the Work and reproducing the content of the NOTICE file.\\n\\n   7. Disclaimer of Warranty. Unless required by applicable law or\\n      agreed to in writing, Licensor provides the Work (and each\\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\\n      implied, including, without limitation, any warranties or conditions\\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\\n      PARTICULAR PURPOSE. You are solely responsible for determining the\\n      appropriateness of using or redistributing the Work and assume any\\n      risks associated with Your exercise of permissions under this License.\\n\\n   8. Limitation of Liability. In no event and under no legal theory,\\n      whether in tort (including negligence), contract, or otherwise,\\n      unless required by applicable law (such as deliberate and grossly\\n      negligent acts) or agreed to in writing, shall any Contributor be\\n      liable to You for damages, including any direct, indirect, special,\\n      incidental, or consequential damages of any character arising as a\\n      result of this License or out of the use or inability to use the\\n      Work (including but not limited to damages for loss of goodwill,\\n      work stoppage, computer failure or malfunction, or any and all\\n      other commercial damages or losses), even if such Contributor\\n      has been advised of the possibility of such damages.\\n\\n   9. Accepting Warranty or Additional Liability. While redistributing\\n      the Work or Derivative Works thereof, You may choose to offer,\\n      and charge a fee for, acceptance of support, warranty, indemnity,\\n      or other liability obligations and/or rights consistent with this\\n      License. However, in accepting such obligations, You may act only\\n      on Your own behalf and on Your sole responsibility, not on behalf\\n      of any other Contributor, and only if You agree to indemnify,\\n      defend, and hold each Contributor harmless for any liability\\n      incurred by, or claims asserted against, such Contributor by reason\\n      of your accepting any such warranty or additional liability.\\n\\n   END OF TERMS AND CONDITIONS\\n\\n   APPENDIX: How to apply the Apache License to your work.\\n\\n      To apply the Apache License to your work, attach the following\\n      boilerplate notice, with the fields enclosed by brackets \"{}\"\\n      replaced with your own identifying information. (Don\\'t include\\n      the brackets!)  The text should be enclosed in the appropriate\\n      comment syntax for the file format. We also recommend that a\\n      file or class name and description of purpose be included on the\\n      same \"printed page\" as the copyright notice for easier\\n      identification within third-party archives.\\n\\n   Copyright {yyyy} {name of copyright owner}\\n\\n   Licensed under the Apache License, Version 2.0 (the \"License\");\\n   you may not use this file except in compliance with the License.\\n   You may obtain a copy of the License at\\n\\n       http://www.apache.org/licenses/LICENSE-2.0\\n\\n   Unless required by applicable law or agreed to in writing, software\\n   distributed under the License is distributed on an \"AS IS\" BASIS,\\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n   See the License for the specific language governing permissions and\\n   limitations under the License.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Sockeye",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "sockeye",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "awslabs",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/awslabs/sockeye/blob/main/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "fhieber",
        "body": "## [3.0.7]\r\n\r\n## Changed\r\n\r\n- Improve training speed by using`torch.nn.functional.multi_head_attention_forward` for self- and encoder-attention\r\n  during training. Requires reorganization of the parameter layout of the key-value input projections,\r\n  as the current Sockeye attention interleaves for faster inference.\r\n  Attention masks (both for source masking and autoregressive masks need some shape adjustments as requirements\r\n  for the fused MHA op differ slightly).\r\n  - Non-interleaved format for joint key-value input projection parameters:\r\n    `in_features=hidden, out_features=2*hidden -> Shape: (2*hidden, hidden)`\r\n  - Interleaved format for joint-key-value input projection stores key and value parameters, grouped by heads:\r\n    `Shape: ((num_heads * 2 * hidden_per_head), hidden)`\r\n  - Models save and load key-value projection parameters in interleaved format.\r\n  - When `model.training == True` key-value projection parameters are put into\r\n    non-interleaved format for `torch.nn.functional.multi_head_attention_forward`\r\n  - When `model.training == False`, i.e. model.eval() is called, key-value projection\r\n    parameters are again converted into interleaved format in place.\r\n\r\n## [3.0.6]\r\n\r\n### Fixed\r\n\r\n- Fixed checkpoint decoder issue that prevented using `bleu` as `--optimized-metric` for distributed training ([#995](https://github.com/awslabs/sockeye/issues/995)).\r\n\r\n## [3.0.5]\r\n\r\n### Fixed\r\n\r\n- Fixed data download in multilingual tutorial.",
        "dateCreated": "2021-12-19T16:49:18Z",
        "datePublished": "2021-12-20T09:59:18Z",
        "html_url": "https://github.com/awslabs/sockeye/releases/tag/3.0.7",
        "name": "3.0.7",
        "tag_name": "3.0.7",
        "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/3.0.7",
        "url": "https://api.github.com/repos/awslabs/sockeye/releases/55656008",
        "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/3.0.7"
      },
      {
        "authorType": "User",
        "author_name": "fhieber",
        "body": "## [3.0.4]\r\n\r\n###\r\n\r\n- Make sure data permutation indices are in int64 format (doesn't seem to be the case by default on all platforms).\r\n\r\n## [3.0.3]\r\n\r\n### Fixed\r\n\r\n- Fixed ensemble decoding for models without target factors.\r\n\r\n## [3.0.2]\r\n\r\n### Changed\r\n\r\n- `sockeye-translate`: Beam search now computes and returns secondary target factor scores. Secondary target factors\r\n  do not participate in beam search, but are greedily chosen at every time step. Accumulated scores for secondary factors\r\n  are not normalized by length. Factor scores are included in JSON output (``--output-type json``).\r\n- `sockeye-score` now returns tab-separated scores for each target factor. Users can decide how to combine factor scores\r\n  depending on the downstream application. Score for the first, primary factor (i.e. output words) are normalized,\r\n  other factors are not.\r\n\r\n## [3.0.1]\r\n\r\n### Fixed\r\n\r\n- Parameter averaging (`sockeye-average`) now always uses the CPU, which enables averaging parameters from GPU-trained models on CPU-only hosts.",
        "dateCreated": "2021-12-13T17:22:22Z",
        "datePublished": "2021-12-13T17:39:31Z",
        "html_url": "https://github.com/awslabs/sockeye/releases/tag/3.0.4",
        "name": "3.0.4",
        "tag_name": "3.0.4",
        "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/3.0.4",
        "url": "https://api.github.com/repos/awslabs/sockeye/releases/55207161",
        "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/3.0.4"
      },
      {
        "authorType": "User",
        "author_name": "fhieber",
        "body": "## [3.0.0] Sockeye 3: Fast Neural Machine Translation with PyTorch\r\n\r\nSockeye is now based on PyTorch.\r\nWe maintain backwards compatibility with MXNet models in version 2.3.x until 3.1.0.\r\nIf MXNet 2.x is installed, Sockeye can run both with PyTorch or MXNet but MXNet is no longer strictly required.\r\n\r\n### Added\r\n\r\n- Added model converter CLI `sockeye.mx_to_pt` that converts MXNet models to PyTorch models.\r\n- Added `--apex-amp` training argument that runs entire model in FP16 mode, replaces `--dtype float16` (requires [Apex](https://github.com/NVIDIA/apex)).\r\n- Training automatically uses Apex fused optimizers if available (requires [Apex](https://github.com/NVIDIA/apex)).\r\n- Added training argument `--label-smoothing-impl` to choose label smoothing implementation (default of `mxnet` uses the same logic as MXNet Sockeye 2).\r\n\r\n### Changed\r\n\r\n- CLI names point to the PyTorch code base (e.g. `sockeye-train` etc.).\r\n- MXNet-based CLIs are now accessible via `sockeye-<name>-mx`.\r\n- MXNet code requires MXNet >= 2.0 since we adopted the new numpy interface.\r\n- `sockeye-train` now uses PyTorch's distributed data-parallel mode for multi-process (multi-GPU) training. Launch with: `torchrun --no_python --nproc_per_node N sockeye-train --dist ...`\r\n- Updated the [quickstart tutorial](docs/tutorials/wmt_large.md) to cover multi-device training with PyTorch Sockeye.\r\n- Changed `--device-ids` argument (plural) to `--device-id` (singular). For multi-GPU training, see distributed mode noted above.\r\n- Updated default value: `--pad-vocab-to-multiple-of 8`\r\n- Removed `--horovod` argument used with `horovodrun` (use `--dist` with `torchrun`).\r\n- Removed `--optimizer-params` argument (use `--optimizer-betas`, `--optimizer-eps`).\r\n- Removed `--no-hybridization` argument (use `PYTORCH_JIT=0`, see [Disable JIT for Debugging](https://pytorch.org/docs/stable/jit.html#disable-jit-for-debugging)).\r\n- Removed `--omp-num-threads` argument (use `--env=OMP_NUM_THREADS=N`).\r\n\r\n### Removed\r\n\r\n- Removed support for constrained decoding (both positive and negative lexical constraints)\r\n- Removed support for beam histories\r\n- Removed `--amp-scale-interval` argument.\r\n- Removed `--kvstore` argument.\r\n- Removed arguments: `--weight-init`, `--weight-init-scale` `--weight-init-xavier-factor-type`, `--weight-init-xavier-rand-type`\r\n- Removed `--decode-and-evaluate-device-id` argument.\r\n- Removed arguments: `--monitor-pattern'`, `--monitor-stat-func`\r\n- Removed CUDA-specific requirements files in `requirements/`",
        "dateCreated": "2021-11-30T09:42:15Z",
        "datePublished": "2021-11-30T09:48:34Z",
        "html_url": "https://github.com/awslabs/sockeye/releases/tag/3.0.0",
        "name": "3.0.0",
        "tag_name": "3.0.0",
        "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/3.0.0",
        "url": "https://api.github.com/repos/awslabs/sockeye/releases/54306072",
        "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/3.0.0"
      },
      {
        "authorType": "User",
        "author_name": "fhieber",
        "body": "## [2.3.24]\r\n### Added\r\n\r\n- Use of the safe yaml loader for the model configuration files.\r\n\r\n## [2.3.23]\r\n### Changed\r\n\r\n- Do not sort BIAS_STATE in beam search. It is constant across decoder steps.",
        "dateCreated": "2021-10-21T15:53:54Z",
        "datePublished": "2021-11-05T09:28:33Z",
        "html_url": "https://github.com/awslabs/sockeye/releases/tag/2.3.24",
        "name": "2.3.24",
        "tag_name": "2.3.24",
        "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/2.3.24",
        "url": "https://api.github.com/repos/awslabs/sockeye/releases/52769304",
        "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/2.3.24"
      },
      {
        "authorType": "User",
        "author_name": "fhieber",
        "body": "## [2.3.22]\r\n### Fixed\r\n\r\n- The previous commit introduced a regression for vocab creation. The results was that the vocabulary was created on the input characters rather than on tokens.\r\n\r\n\r\n## [2.3.21]\r\n### Added\r\n\r\n- Extended parallelization of data preparation to vocabulary and statistics creation while minimizing the overhead of sharding.\r\n\r\n## [2.3.20]\r\n### Added\r\n\r\n- Added debug logging for restrict_lexicon lookups\r\n\r\n## [2.3.19]\r\n### Changed\r\n\r\n- When training only the decoder (`--fixed-param-strategy all_except_decoder`), disable autograd for the encoder and embeddings to save memory.\r\n\r\n## [2.3.18]\r\n### Changed\r\n\r\n- Updated Docker builds and documentation.  See [sockeye_contrib/docker](sockeye_contrib/docker).",
        "dateCreated": "2021-09-23T12:50:13Z",
        "datePublished": "2021-09-30T09:41:22Z",
        "html_url": "https://github.com/awslabs/sockeye/releases/tag/2.3.22",
        "name": "2.3.22",
        "tag_name": "2.3.22",
        "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/2.3.22",
        "url": "https://api.github.com/repos/awslabs/sockeye/releases/50535944",
        "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/2.3.22"
      },
      {
        "authorType": "User",
        "author_name": "fhieber",
        "body": "## [2.3.17]\r\n### Added\r\n- Added an alternative, faster implementation of greedy search. The '--greedy' flag to `sockeye.translate` will enable it. This implementation does not support hypothesis scores, batch decoding, or lexical constraints.\"\r\n\r\n## [2.3.16]\r\n\r\n### Added\r\n- Added option `--transformer-feed-forward-use-glu` to use Gated Linear Units in transformer feed forward networks ([Dauphin et al., 2016](https://arxiv.org/abs/1612.08083); [Shazeer, 2020](https://arxiv.org/abs/2002.05202)).\r\n\r\n## [2.3.15]\r\n\r\n### Changed\r\n- Optimization: Decoder class is now a complete HybridBlock (no forward method).",
        "dateCreated": "2021-05-30T20:04:06Z",
        "datePublished": "2021-06-17T10:50:20Z",
        "html_url": "https://github.com/awslabs/sockeye/releases/tag/2.3.17",
        "name": "2.3.17",
        "tag_name": "2.3.17",
        "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/2.3.17",
        "url": "https://api.github.com/repos/awslabs/sockeye/releases/44786693",
        "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/2.3.17"
      },
      {
        "authorType": "User",
        "author_name": "fhieber",
        "body": "## [2.3.14]\r\n\r\n### Changed\r\n- Updated to [MXNet 1.8.0](https://github.com/apache/incubator-mxnet/tree/1.8.0)\r\n- Removed dependency support for Cuda 9.2 (no longer supported by MXNet 1.8).\r\n- Added dependency support for Cuda 11.0 and 11.2.\r\n- Updated Python requirement to 3.7 and later. (Removed backporting `dataclasses` requirement)\r\n\r\n## [2.3.13]\r\n\r\n### Added\r\n- Target factors are now also collected for nbest translations (and stored in the JSON output handler).\r\n\r\n## [2.3.12]\r\n\r\n### Added\r\n- Added `--config` option to `prepare_data` CLI to allow setting commandline flags via a yaml config. \r\n- Flags for the `prepare_data` CLI are now stored in the output folder under `args.yaml`\r\n  (equivalent to the behavior of `sockeye_train`)\r\n\r\n## [2.3.11]\r\n\r\n### Added\r\n- Added option `prevent_unk` to avoid generating `<unk>` token in beam search.",
        "dateCreated": "2021-04-06T12:21:35Z",
        "datePublished": "2021-04-07T11:50:08Z",
        "html_url": "https://github.com/awslabs/sockeye/releases/tag/2.3.14",
        "name": "2.3.14",
        "tag_name": "2.3.14",
        "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/2.3.14",
        "url": "https://api.github.com/repos/awslabs/sockeye/releases/41053485",
        "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/2.3.14"
      },
      {
        "authorType": "User",
        "author_name": "fhieber",
        "body": "## [2.3.10]\r\n\r\n### Changed\r\n\r\n- Make sure that the top N best params files retained, even if N > --keep-last-params. This ensures that model\r\n  averaging will not be crippled when keeping only a few params files during training. This can result in a\r\n  significant savings of disk space during training.\r\n\r\n## [2.3.9]\r\n\r\n### Added\r\n\r\n- Added scripts for processing Sockeye benchmark output (`--output-type benchmark`):\r\n  - [benchmark_to_output.py](sockeye_contrib/benchmark/benchmark_to_output.py) extracts translations\r\n  - [benchmark_to_percentiles.py](sockeye_contrib/benchmark/benchmark_to_percentiles.py) computes percentiles",
        "dateCreated": "2021-02-05T10:25:14Z",
        "datePublished": "2021-02-08T10:00:44Z",
        "html_url": "https://github.com/awslabs/sockeye/releases/tag/2.3.10",
        "name": "2.3.10",
        "tag_name": "2.3.10",
        "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/2.3.10",
        "url": "https://api.github.com/repos/awslabs/sockeye/releases/37730556",
        "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/2.3.10"
      },
      {
        "authorType": "User",
        "author_name": "fhieber",
        "body": "## [2.3.8]\r\n\r\n### Fixed\r\n\r\n- Fix problem identified in issue #925 that caused learning rate\r\n  warmup to fail in some instances when doing continued training\r\n\r\n## [2.3.7]\r\n\r\n### Changed\r\n\r\n- Use dataclass module to simplify Config classes. No functional change.\r\n\r\n## [2.3.6]\r\n\r\n### Fixed\r\n\r\n- Fixes the problem identified in issue #890, where the lr_scheduler\r\n  does not behave as expected when continuing training. The problem is\r\n  that the lr_scheduler is kept as part of the optimizer, but the\r\n  optimizer is not saved when saving state. Therefore, every time\r\n  training is restarted, a new lr_scheduler is created with initial\r\n  parameter settings. Fix by saving and restoring the lr_scheduling\r\n  separately.\r\n\r\n## [2.3.5]\r\n\r\n### Fixed\r\n\r\n- Fixed issue with LearningRateSchedulerPlateauReduce.__repr__ printing\r\n    out num_not_improved instead of reduce_num_not_improved.\r\n\r\n## [2.3.4]\r\n\r\n### Fixed\r\n\r\n- Fixed issue with dtype mismatch in beam search when translating with `--dtype float16`.\r\n\r\n## [2.3.3]\r\n\r\n### Changed\r\n\r\n- Upgraded `SacreBLEU` dependency of Sockeye to a newer version (`1.4.14`).\r\n",
        "dateCreated": "2021-01-08T08:07:29Z",
        "datePublished": "2021-01-08T08:10:37Z",
        "html_url": "https://github.com/awslabs/sockeye/releases/tag/2.3.8",
        "name": "2.3.8",
        "tag_name": "2.3.8",
        "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/2.3.8",
        "url": "https://api.github.com/repos/awslabs/sockeye/releases/36130087",
        "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/2.3.8"
      },
      {
        "authorType": "User",
        "author_name": "fhieber",
        "body": "## [2.3.2]\r\n### Fixed\r\n\r\n- Fixed edge case that unintentionally skips softmax for sampling if beam size is 1.\r\n\r\n## [2.3.1]\r\n### Fixed\r\n\r\n- Optimizing for BLEU/CHRF with horovod required the secondary workers to also create checkpoint decoders.\r\n\r\n## [2.3.0]\r\n\r\n### Added\r\n\r\n- Added support for target factors.\r\n  If provided with additional target-side tokens/features (token-parallel to the regular target-side) at training time,\r\n  the model can now learn to predict these in a multi-task setting. You can provide target factor data similar to source\r\n  factors: `--target-factors <factor_file1> [<factor_fileN>]`. During training, Sockeye optimizes one loss per factor\r\n  in a multi-task setting. The weight of the losses can be controlled by `--target-factors-weight`.\r\n  At inference, target factors are decoded greedily, they do not participate in beam search.\r\n  The predicted factor at each time step is the argmax over its separate output\r\n  layer distribution. To receive the target factor predictions at inference time, use\r\n  `--output-type translation_with_factors`. \r\n  \r\n### Changed\r\n\r\n- `load_model(s)` now returns a list of target vocabs.\r\n- Default source factor combination changed to `sum` (was `concat` before).\r\n- `SockeyeModel` class has three new properties: `num_target_factors`, `target_factor_configs`,\r\n  and `factor_output_layers`.",
        "dateCreated": "2020-11-10T16:02:33Z",
        "datePublished": "2020-11-18T13:41:36Z",
        "html_url": "https://github.com/awslabs/sockeye/releases/tag/2.3.2",
        "name": "2.3.2",
        "tag_name": "2.3.2",
        "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/2.3.2",
        "url": "https://api.github.com/repos/awslabs/sockeye/releases/34117464",
        "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/2.3.2"
      },
      {
        "authorType": "User",
        "author_name": "fhieber",
        "body": "## [2.2.8]\r\n\r\n### Changed\r\n- Make source/target data parameters required for the scoring CLI to avoid cryptic error messages.\r\n\r\n\r\n## [2.2.7]\r\n\r\n### Added\r\n- Added an argument to specify the log level of secondary workers. Defaults to ERROR to hide any logs except for exceptions.\r\n\r\n\r\n## [2.2.6]\r\n\r\n### Fixed\r\n- Avoid a crash due to an edge case when no model improvement has been observed by the time the learning rate gets reduced for the first time.\r\n\r\n## [2.2.5]\r\n\r\n### Fixed\r\n- Enforce sentence batching for sockeye score tool, set default batch size to 56\r\n\r\n## [2.2.4]\r\n\r\n### Changed\r\n- Use softmax with length in DotAttentionCell.\r\n- Use `contrib.arange_like` in AutoRegressiveBias block to reduce number of ops.\r\n\r\n## [2.2.3]\r\n\r\n### Added\r\n\r\n- Log the absolute number of `<unk>` tokens in source and target data\r\n\r\n## [2.2.2]\r\n\r\n### Fixed\r\n\r\n- Fix: Guard against null division for small batch sizes.\r\n\r\n## [2.2.1]\r\n\r\n## Fixed\r\n\r\n- Fixes a corner case bug by which the beam decoder can wrongly return a best hypothesis with -infinite score.\r\n",
        "dateCreated": "2020-11-03T16:25:59Z",
        "datePublished": "2020-11-05T14:06:49Z",
        "html_url": "https://github.com/awslabs/sockeye/releases/tag/2.2.8",
        "name": "2.2.8",
        "tag_name": "2.2.8",
        "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/2.2.8",
        "url": "https://api.github.com/repos/awslabs/sockeye/releases/33504760",
        "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/2.2.8"
      },
      {
        "authorType": "User",
        "author_name": "fhieber",
        "body": "## [2.2.0]\r\n\r\n### Changed\r\n\r\n- Replaced multi-head attention with [interleaved_matmul_encdec](https://github.com/apache/incubator-mxnet/pull/16408) operators, which removes previously needed transposes and improves performance.\r\n\r\n- Beam search states and model layers now assume time-major format.\r\n\r\n## [2.1.26]\r\n\r\n### Fixed\r\n\r\n- Fixes a backwards incompatibility introduced in 2.1.17, which would prevent models trained with prior versions to be used for inference.\r\n\r\n## [2.1.25]\r\n\r\n### Changed\r\n\r\n- Reverting PR #772 as it causes issues with `amp`.\r\n\r\n## [2.1.24]\r\n\r\n### Changed\r\n\r\n- Make sure to write a final checkpoint when stopping with `--max-updates`, `--max-samples` or `--max-num-epochs`.\r\n\r\n## [2.1.23]\r\n\r\n### Changed\r\n\r\n- Updated to [MXNet 1.7.0](https://github.com/apache/incubator-mxnet/tree/1.7.0).\r\n- Re-introduced use of softmax with length parameter in DotAttentionCell (see PR #772).\r\n\r\n## [2.1.22]\r\n\r\n### Added\r\n\r\n- Re-introduced `--softmax-temperature` flag for `sockeye.score` and `sockeye.translate`.",
        "dateCreated": "2020-10-02T14:18:33Z",
        "datePublished": "2020-10-04T17:22:34Z",
        "html_url": "https://github.com/awslabs/sockeye/releases/tag/2.2.0",
        "name": "2.2.0",
        "tag_name": "2.2.0",
        "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/2.2.0",
        "url": "https://api.github.com/repos/awslabs/sockeye/releases/32141318",
        "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/2.2.0"
      },
      {
        "authorType": "User",
        "author_name": "fhieber",
        "body": "## [2.1.21]\r\n\r\n### Added\r\n\r\n- Added an optional ability to cache encoder outputs of model.\r\n\r\n## [2.1.20]\r\n\r\n### Fixed\r\n\r\n- Fixed a bug where the training state object was saved to disk before training metrics were added to it, leading to an inconsistency between the training state object and the metrics file (see #859).\r\n\r\n## [2.1.19]\r\n\r\n### Fixed\r\n\r\n- When loading a shard in Horovod mode, there is now a check that each non-empty bucket contains enough sentences to cover each worker's slice. If not, the bucket's sentences are replicated to guarantee coverage.\r\n\r\n## [2.1.18]\r\n\r\n### Fixed\r\n\r\n- Fixed a bug where sampling translation fails because an array is created in the wrong context.",
        "dateCreated": "2020-08-27T13:30:01Z",
        "datePublished": "2020-08-27T13:31:17Z",
        "html_url": "https://github.com/awslabs/sockeye/releases/tag/2.1.21",
        "name": "2.1.21",
        "tag_name": "2.1.21",
        "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/2.1.21",
        "url": "https://api.github.com/repos/awslabs/sockeye/releases/30220141",
        "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/2.1.21"
      },
      {
        "authorType": "User",
        "author_name": "fhieber",
        "body": "## [2.1.17]\r\n\r\n### Added\r\n\r\n- Added `layers.SSRU`, which implements a Simpler Simple Recurrent Unit as described in\r\nKim et al, \"From Research to Production and Back: Ludicrously Fast Neural Machine Translation\" WNGT 2019.\r\n\r\n- Added `ssru_transformer` option to `--decoder`, which enables the usage of SSRUs as a replacement for the decoder-side self-attention layers.\r\n\r\n### Changed\r\n\r\n- Reduced the number of arguments for `MultiHeadSelfAttention.hybrid_forward()`.\r\n `previous_keys` and `previous_values` should now be input together as `previous_states`, a list containing two symbols.",
        "dateCreated": "2020-08-20T18:20:06Z",
        "datePublished": "2020-08-20T18:25:24Z",
        "html_url": "https://github.com/awslabs/sockeye/releases/tag/2.1.17",
        "name": "2.1.17",
        "tag_name": "2.1.17",
        "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/2.1.17",
        "url": "https://api.github.com/repos/awslabs/sockeye/releases/29943526",
        "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/2.1.17"
      },
      {
        "authorType": "User",
        "author_name": "fhieber",
        "body": "## [2.1.16]\r\n\r\n### Fixed\r\n\r\n- Fixed batch sizing error introduced in version 2.1.12 (c00da52) that caused batch sizes to be multiplied by the number of devices. Batch sizing now works as documented (same as pre-2.1.12 versions).\r\n- Fixed `max-word` batching to properly size batches to a multiple of both `--batch-sentences-multiple-of` and the number of devices.\r\n\r\n## [2.1.15]\r\n\r\n### Added\r\n\r\n- Inference option `--mc-dropout` to use dropout during inference, leading to non-deterministic output. This option uses the same dropout parameters present in the model config file.\r\n\r\n## [2.1.14]\r\n\r\n### Added\r\n\r\n- Added `sockeye.rerank` option `--output` to specify output file.\r\n- Added `sockeye.rerank` option `--output-reference-instead-of-blank` to output reference line instead of best hypothesis when best hypothesis is blank.",
        "dateCreated": "2020-07-30T19:04:28Z",
        "datePublished": "2020-07-31T09:34:06Z",
        "html_url": "https://github.com/awslabs/sockeye/releases/tag/2.1.16",
        "name": "2.1.16",
        "tag_name": "2.1.16",
        "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/2.1.16",
        "url": "https://api.github.com/repos/awslabs/sockeye/releases/29156235",
        "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/2.1.16"
      },
      {
        "authorType": "User",
        "author_name": "fhieber",
        "body": "## [2.1.13]\r\n\r\n### Added\r\n\r\n- Training option `--quiet-secondary-workers` that suppresses console output for secondary workers when training with Horovod/MPI.\r\n- Set version of isort to `<5.0.0` in requirements.dev.txt to avoid incompatibility between newer versions of isort and pylint.\r\n\r\n## [2.1.12]\r\n\r\n### Added\r\n\r\n- Batch type option `max-word` for max number of words including padding tokens (more predictable memory usage than `word`).\r\n- Batching option `--batch-sentences-multiple-of` that is similar to `--round-batch-sizes-to-multiple-of` but always rounds down (more predictable memory usage).\r\n\r\n### Changed\r\n\r\n- Default bucketing settings changed to width 8, max sequence length 95 (96 including BOS/EOS tokens), and no bucket scaling.\r\n- Argument `--no-bucket-scaling` replaced with `--bucket-scaling` which is False by default.\r\n\r\n## [2.1.11]\r\n\r\n### Changed\r\n\r\n- Updated `sockeye.rerank` module to use \"add-k\" smoothing for sentence-level BLEU.\r\n\r\n### Fixed\r\n\r\n- Updated `sockeye.rerank` module to use current N-best format.",
        "dateCreated": "2020-07-06T14:14:40Z",
        "datePublished": "2020-07-07T14:11:56Z",
        "html_url": "https://github.com/awslabs/sockeye/releases/tag/2.1.13",
        "name": "2.1.13",
        "tag_name": "2.1.13",
        "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/2.1.13",
        "url": "https://api.github.com/repos/awslabs/sockeye/releases/28315373",
        "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/2.1.13"
      },
      {
        "authorType": "User",
        "author_name": "fhieber",
        "body": "## [2.1.10]\r\n\r\n### Changed\r\n\r\n- Changed to a cross-entropy loss implementation that avoids the use of SoftmaxOutput.\r\n\r\n## [2.1.9]\r\n\r\n### Added\r\n\r\n- Added training argument `--ignore-extra-params` to ignore extra parameters when loading models.  The primary use case is continuing training with a model that has already been annotated with scaling factors (`sockeye.quantize`).\r\n\r\n### Fixed\r\n\r\n- Properly pass `allow_missing` flag to `model.load_parameters()`\r\n\r\n## [2.1.8]\r\n\r\n### Changed\r\n\r\n- Update to sacrebleu=1.4.10",
        "dateCreated": "2020-06-23T15:41:12Z",
        "datePublished": "2020-06-23T15:44:37Z",
        "html_url": "https://github.com/awslabs/sockeye/releases/tag/2.1.10",
        "name": "2.1.10",
        "tag_name": "2.1.10",
        "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/2.1.10",
        "url": "https://api.github.com/repos/awslabs/sockeye/releases/27835569",
        "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/2.1.10"
      },
      {
        "authorType": "User",
        "author_name": "fhieber",
        "body": "## [2.1.7]\r\n\r\n### Changed\r\n\r\n- Optimize prepare_data by saving the shards in parallel. The prepare_data script accepts a new parameter `--max-processes` to control the level of parallelism with which shards are written to disk.\r\n\r\n## [2.1.6]\r\n\r\n### Changed\r\n\r\n- Updated Dockerfiles optimized for CPU (intgemm int8 inference, full MKL support) and GPU (distributed training with Horovod).  See [sockeye_contrib/docker](sockeye_contrib/docker).\r\n\r\n### Added\r\n\r\n- Official support for int8 quantization with [intgemm](https://github.com/kpu/intgemm):\r\n  - This requires the \"intgemm\" fork of MXNet ([kpuatamazon/incubator-mxnet/intgemm](https://github.com/kpuatamazon/incubator-mxnet/tree/intgemm)).  This is the version of MXNet used in the Sockeye CPU docker image (see [sockeye_contrib/docker](sockeye_contrib/docker)).\r\n  - Use `sockeye.translate --dtype int8` to quantize a trained float32 model at runtime.\r\n  - Use the `sockeye.quantize` CLI to annotate a float32 model with int8 scaling factors for fast runtime quantization.\r\n\r\n## [2.1.5]\r\n\r\n### Changed\r\n\r\n- Changed state caching for transformer models during beam search to cache states with attention heads already separated out. This avoids repeated transpose operations during decoding, leading to faster inference.\r\n\r\n## [2.1.4]\r\n\r\n### Added\r\n\r\n- Added Dockerfiles that build an experimental CPU-optimized Sockeye image:\r\n  - Uses the latest versions of [kpuatamazon/incubator-mxnet](https://github.com/kpuatamazon/incubator-mxnet) (supports [intgemm](https://github.com/kpu/intgemm) and makes full use of Intel MKL) and [kpuatamazon/sockeye](https://github.com/kpuatamazon/sockeye) (supports int8 quantization for inference).\r\n  - See [sockeye_contrib/docker](sockeye_contrib/docker).\r\n\r\n## [2.1.3]\r\n\r\n### Changed\r\n\r\n- Performance optimizations to beam search inference\r\n  - Remove unneeded take ops on encoder states\r\n  - Gathering input data before sending to GPU, rather than sending each batch element individually\r\n  - All of beam search can be done in fp16, if specified by the model\r\n  - Other small miscellaneous optimizations\r\n- Model states are now a flat list in ensemble inference, structure of states provided by `state_structure()`\r\n\r\n## [2.1.2]\r\n\r\n### Changed\r\n\r\n- Updated to [MXNet 1.6.0](https://github.com/apache/incubator-mxnet/tree/1.6.0)\r\n\r\n### Added\r\n\r\n- Added support for CUDA 10.2\r\n\r\n### Removed\r\n\r\n- Removed support for CUDA<9.1 / CUDNN<7.5\r\n\r\n## [2.1.1]\r\n\r\n### Added\r\n- Ability to set environment variables from training/translate CLIs before MXNet is imported. For example, users can\r\n  configure MXNet as such: `--env \"OMP_NUM_THREADS=1;MXNET_ENGINE_TYPE=NaiveEngine\"`\r\n\r\n## [2.1.0]\r\n\r\n### Changed\r\n\r\n- Version bump, which should have been included in commit b0461b due to incompatible models.\r\n\r\n## [2.0.1]\r\n\r\n### Changed\r\n\r\n- Inference defaults to using the max input length observed in training (versus scaling down based on mean length ratio and standard deviations).\r\n\r\n### Added\r\n\r\n- Additional parameter fixing strategies:\r\n  - `all_except_feed_forward`: Only train feed forward layers.\r\n  - `encoder_and_source_embeddings`: Only train the decoder (decoder layers, output layer, and target embeddings).\r\n  - `encoder_half_and_source_embeddings`: Train the latter half of encoder layers and the decoder.\r\n- Option to specify the number of CPU threads without using an environment variable (`--omp-num-threads`).\r\n- More flexibility for source factors combination\r\n\r\n## [2.0.0]\r\n\r\n### Changed\r\n\r\n- Update to [MXNet 1.5.0](https://github.com/apache/incubator-mxnet/tree/1.5.0)\r\n- Moved `SockeyeModel` implementation and all layers to [Gluon API](http://mxnet.incubator.apache.org/versions/master/gluon/index.html)\r\n- Removed support for Python 3.4.\r\n- Removed image captioning module\r\n- Removed outdated Autopilot module\r\n- Removed unused training options: Eve, Nadam, RMSProp, Nag, Adagrad, and Adadelta optimizers, `fixed-step` and `fixed-rate-inv-t` learning rate schedulers\r\n- Updated and renamed learning rate scheduler `fixed-rate-inv-sqrt-t` -> `inv-sqrt-decay`\r\n- Added script for plotting metrics files: [sockeye_contrib/plot_metrics.py](sockeye_contrib/plot_metrics.py)\r\n- Removed option `--weight-tying`.  Weight tying is enabled by default, disable with `--weight-tying-type none`.\r\n\r\n### Added\r\n\r\n- Added distributed training support with Horovod/OpenMPI.  Use `horovodrun` and the `--horovod` training flag.\r\n- Added Dockerfiles that build a Sockeye image with all features enabled.  See [sockeye_contrib/docker](sockeye_contrib/docker).\r\n- Added `none` learning rate scheduler (use a fixed rate throughout training)\r\n- Added `linear-decay` learning rate scheduler\r\n- Added training option `--learning-rate-t-scale` for time-based decay schedulers\r\n- Added support for MXNet's [Automatic Mixed Precision](https://mxnet.incubator.apache.org/versions/master/tutorials/amp/amp_tutorial.html).  Activate with the `--amp` training flag.  For best results, make sure as many model dimensions are possible are multiples of 8.\r\n- Added options for making various model dimensions multiples of a given value.  For example, use `--pad-vocab-to-multiple-of 8`, `--bucket-width 8 --no-bucket-scaling`, and `--round-batch-sizes-to-multiple-of 8` with AMP training.\r\n- Added [GluonNLP](http://gluon-nlp.mxnet.io/)'s BERTAdam optimizer, an implementation of the Adam variant used by Devlin et al. ([2018](https://arxiv.org/pdf/1810.04805.pdf)).  Use `--optimizer bertadam`.\r\n- Added training option `--checkpoint-improvement-threshold` to set the amount of metric improvement required over the window of previous checkpoints to be considered actual model improvement (used with `--max-num-checkpoint-not-improved`).",
        "dateCreated": "2020-06-03T09:30:48Z",
        "datePublished": "2020-06-03T09:40:57Z",
        "html_url": "https://github.com/awslabs/sockeye/releases/tag/2.1.7",
        "name": "2.1.7",
        "tag_name": "2.1.7",
        "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/2.1.7",
        "url": "https://api.github.com/repos/awslabs/sockeye/releases/27170373",
        "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/2.1.7"
      },
      {
        "authorType": "User",
        "author_name": "fhieber",
        "body": "## [1.18.115]\r\n### Added\r\n- Added requirements for MXnet compatible with cuda 10.1.\r\n\r\n## [1.18.114]\r\n### Fixed\r\n- Fix bug in prepare_train_data arguments.\r\n\r\n## [1.18.113]\r\n### Fixed\r\n- Added logging arguments for prepare_data CLI.\r\n\r\n## [1.18.112]\r\n### Added\r\n- Option to suppress creation of logfiles for CLIs (`--no-logfile`).\r\n\r\n## [1.18.111]\r\n### Added\r\n- Added an optional checkpoint callback for the train function.\r\n\r\n### Changed\r\n- Excluded gradients from pickled fields of TrainState\r\n\r\n## [1.18.110]\r\n### Changed\r\n- We now guard against failures to run `nvidia-smi` for GPU memory monitoring.\r\n\r\n## [1.18.109]\r\n### Fixed\r\n- Fixed the metric names by prefixing training metrics with 'train-' and validation metrics with 'val-'. Also restricted the custom logging function to accept only a dictionary and a compulsory global_step parameter.\r\n\r\n## [1.18.108]\r\n### Changed\r\n- More verbose log messages about target token counts.\r\n\r\n## [1.18.107]\r\n### Changed\r\n- Updated to [MXNet 1.5.0](https://github.com/apache/incubator-mxnet/tree/1.5.0)",
        "dateCreated": "2020-05-04T09:41:58Z",
        "datePublished": "2020-06-03T07:41:07Z",
        "html_url": "https://github.com/awslabs/sockeye/releases/tag/1.18.115",
        "name": "1.18.115",
        "tag_name": "1.18.115",
        "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/1.18.115",
        "url": "https://api.github.com/repos/awslabs/sockeye/releases/27166436",
        "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/1.18.115"
      },
      {
        "authorType": "User",
        "author_name": "fhieber",
        "body": "## [1.18.106]\r\n### Added\r\n- Added an optional time limit for stopping training. The training will stop at the next checkpoint after reaching the time limit.\r\n\r\n## [1.18.105]\r\n### Added\r\n- Added support for a possibility to have a custom metrics logger - a function passed as an extra parameter. If supplied, the logger is called during training.\r\n\r\n## [1.18.104]\r\n### Changed\r\n- Implemented an attention-based copy mechanism as described in [Jia, Robin, and Percy Liang. \"Data recombination for neural semantic parsing.\" (2016)](https://arxiv.org/abs/1606.03622).\r\n- Added a <ptr\\d+> special symbol to explicitly point at an input token in the target sequence\r\n- Changed the decoder interface to pass both the decoder data and the pointer data.\r\n- Changed the AttentionState named tuple to add the raw attention scores.\r\n\r\n## [1.18.103]\r\n### Added\r\n- Added ability to score image-sentence pairs by extending the scoring feature originally implemented for machine \r\n  translation to the image captioning module.\r\n\r\n## [1.18.102]\r\n### Fixed\r\n- Fixed loading of more than 10 source vocabulary files to be in the right, numerical order.\r\n\r\n## [1.18.101]\r\n### Changed\r\n- Update to Sacrebleu 1.3.6\r\n\r\n## [1.18.100]\r\n### Fixed\r\n- Always initializing the multiprocessing context. This should fix issues observed when running `sockeye-train`.\r\n\r\n## [1.18.99]\r\n### Changed\r\n- Updated to [MXNet 1.4.1](https://github.com/apache/incubator-mxnet/tree/1.4.1)\r\n\r\n## [1.18.98]\r\n### Changed\r\n- Converted several transformer-related layer implementations to Gluon HybridBlocks. No functional change.",
        "dateCreated": "2019-08-09T12:53:03Z",
        "datePublished": "2019-08-18T08:56:04Z",
        "html_url": "https://github.com/awslabs/sockeye/releases/tag/1.18.106",
        "name": "1.18.106",
        "tag_name": "1.18.106",
        "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/1.18.106",
        "url": "https://api.github.com/repos/awslabs/sockeye/releases/19360256",
        "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/1.18.106"
      },
      {
        "authorType": "User",
        "author_name": "fhieber",
        "body": "## [1.18.97]\r\n### Changed\r\n- Updated to PyYAML 5.1\r\n\r\n## [1.18.96]\r\n### Changed\r\n- Extracted prepare vocab functionality in the build vocab step into its own function. This matches the pattern in prepare data and train where the main() function only has argparsing, and it invokes a separate function to do the work. This is to allow modules that import this one to circumvent the command line.\r\n\r\n## [1.18.95]\r\n### Changed\r\n- Removed custom operators from transformer models and replaced them with symbolic operators.\r\n  Improves Performance.\r\n\r\n## [1.18.94]\r\n### Added\r\n- Added ability to accumulate gradients over multiple batches (--update-interval). This allows simulation of large\r\n  batch sizes on environments with limited memory. For example: training with `--batch-size 4096 --update-interval 2`\r\n  should be close to training with `--batch-size 8192` at smaller memory footprint.\r\n\r\n## [1.18.93]\r\n### Fixed\r\n- Made `brevity_penalty` argument in `Translator` class optional to ensure backwards compatibility.",
        "dateCreated": "2019-05-06T11:12:16Z",
        "datePublished": "2019-05-07T14:07:04Z",
        "html_url": "https://github.com/awslabs/sockeye/releases/tag/1.18.97",
        "name": "1.18.97",
        "tag_name": "1.18.97",
        "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/1.18.97",
        "url": "https://api.github.com/repos/awslabs/sockeye/releases/17196876",
        "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/1.18.97"
      },
      {
        "authorType": "User",
        "author_name": "fhieber",
        "body": "## [1.18.92]\r\n### Added\r\n- Added sentence length (and length ratio) prediction to be able to discourage hypotheses that are too short at inference time. Can be enabled for training with `--length-task` and with `--brevity-penalty-type` during inference.\r\n\r\n## [1.18.91]\r\n### Changed\r\n- Multiple lexicons can now be specified with the `--restrict-lexicon` option:\r\n  - For a single lexicon: `--restrict-lexicon /path/to/lexicon`.\r\n  - For multiple lexicons: `--restrict-lexicon key1:/path/to/lexicon1 key2:/path/to/lexicon2 ...`.\r\n  - Use `--json-input` to specify the lexicon to use for each input, ex: `{\"text\": \"some input string\", \"restrict_lexicon\": \"key1\"}`.\r\n\r\n## [1.18.90]\r\n### Changed\r\n- Updated to [MXNet 1.4.0](https://github.com/apache/incubator-mxnet/tree/1.4.0)\r\n- Integration tests no longer check for equivalence of outputs with batch size 2\r\n\r\n## [1.18.89]\r\n### Fixed\r\n- Made the length ratios per bucket change backwards compatible.\r\n\r\n## [1.18.88]\r\n### Changed\r\n- Made sacrebleu a pip dependency and removed it from `sockeye_contrib`.\r\n\r\n## [1.18.87]\r\n### Added\r\n- Data statistics at training time now compute mean and standard deviation of length ratios per bucket.\r\n  This information is stored in the model's config, but not used at the moment.\r\n\r\n## [1.18.86]\r\n### Added\r\n- Added the `--fixed-param-strategy` option that allows fixing various model parameters during training via named strategies.\r\n  These include some of the simpler combinations from [Wuebker et al. (2018)](https://arxiv.org/abs/1811.01990) such as fixing everything except the first and last layers of the encoder and decoder (`all_except_outer_layers`).  See the help message for a full list of strategies.",
        "dateCreated": "2019-04-16T13:48:51Z",
        "datePublished": "2019-04-16T14:03:00Z",
        "html_url": "https://github.com/awslabs/sockeye/releases/tag/1.18.92",
        "name": "1.18.92",
        "tag_name": "1.18.92",
        "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/1.18.92",
        "url": "https://api.github.com/repos/awslabs/sockeye/releases/16796268",
        "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/1.18.92"
      },
      {
        "authorType": "User",
        "author_name": "fhieber",
        "body": "## [1.18.85]\r\n### Changed\r\n- Disabled dynamic batching for `Translator.translate()` by default due to increased memory usage. The default is to\r\n  fill-up batches to `Translator.max_batch_size`.\r\n  Dynamic batching can still be enabled if `fill_up_batches` is set to False.\r\n### Added\r\n- Added parameter to force training to stop after a given number of checkpoints. Useful when forced to share limited GPU resources.\r\n\r\n## [1.18.84]\r\n### Fixed\r\n- Fixed lexical constraints bugs that broke batching and caused large drop in BLEU.\r\n  These were introduced with sampling (1.18.64).\r\n\r\n## [1.18.83]\r\n### Changed\r\n - The embedding size is automatically adjusted to the Transformer model size in case it is not specified on the command line.\r\n\r\n## [1.18.82]\r\n### Fixed\r\n- Fixed type conversion in metrics file reading introduced in 1.18.79.\r\n\r\n## [1.18.81]\r\n### Fixed\r\n- Making sure the training pickled training state contains the checkpoint decoder's BLEU score of the last checkpoint.\r\n\r\n## [1.18.80]\r\n### Fixed\r\n- Fixed a bug introduced in 1.18.77 where blank lines in the training data resulted in failure.\r\n\r\n## [1.18.79]\r\n### Added\r\n- Writing of the convergence/divergence status to the metrics file and guarding against numpy.histogram's errors for NaNs during divergent behaviour.",
        "dateCreated": "2019-03-14T16:21:11Z",
        "datePublished": "2019-03-15T14:07:08Z",
        "html_url": "https://github.com/awslabs/sockeye/releases/tag/1.18.85",
        "name": "1.18.85",
        "tag_name": "1.18.85",
        "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/1.18.85",
        "url": "https://api.github.com/repos/awslabs/sockeye/releases/16141294",
        "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/1.18.85"
      },
      {
        "authorType": "User",
        "author_name": "fhieber",
        "body": "## [1.18.78]\r\n### Changed\r\n- Dynamic batch sizes: `Translator.translate()` will adjust batch size in beam search to the actual number of inputs without using padding.\r\n\r\n## [1.18.77]\r\n### Added\r\n- `sockeye.score` now loads data on demand and doesn't skip any input lines\r\n\r\n## [1.18.76]\r\n### Changed\r\n- Do not compare scores from translation and scoring in integration tests.\r\n\r\n### Added\r\n- Adding the option via the flag `--stop-training-on-decoder-failure` to stop training in case the checkpoint decoder dies (e.g. because there is not enough memory).\r\nIn case this is turned on a checkpoint decoder is launched right when training starts in order to fail as early as possible.\r\n\r\n## [1.18.75]\r\n### Changed\r\n- Do not create dropout layers for inference models for performance reasons.\r\n\r\n## [1.18.74]\r\n### Changed\r\n- Revert change in 1.18.72 as no memory saving could be observed.\r\n\r\n## [1.18.73]\r\n### Fixed\r\n- Fixed a bug where `source-factors-num-embed` was not correctly adjusted to `num-embed`\r\n  when using prepared data & `source-factor-combine` sum.",
        "dateCreated": "2019-02-24T14:43:21Z",
        "datePublished": "2019-02-24T14:44:40Z",
        "html_url": "https://github.com/awslabs/sockeye/releases/tag/1.18.78",
        "name": "1.18.78",
        "tag_name": "1.18.78",
        "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/1.18.78",
        "url": "https://api.github.com/repos/awslabs/sockeye/releases/15741313",
        "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/1.18.78"
      },
      {
        "authorType": "User",
        "author_name": "fhieber",
        "body": "## [1.18.72]\r\n### Changed\r\n- Removed use of `expand_dims` in favor of `reshape` to save memory.\r\n\r\n\r\n## [1.18.71]\r\n### Fixed\r\n- Fixed default setting of source factor combination to be 'concat' for backwards compatibility.\r\n\r\n## [1.18.70]\r\n### Added\r\n- Sockeye now outputs fields found in a JSON input object, if they are not overwritten by Sockeye. This behavior can be enabled by selecting `--json-input` (to read input as a JSON object) and `--output-type json` (to write a JSON object to output).\r\n\r\n## [1.18.69]\r\n### Added\r\n- Source factors can now be added to the embeddings instead of concatenated with `--source-factors-combine sum` (default: concat)\r\n\r\n## [1.18.68]\r\n- Fixed training crashes with `--learning-rate-decay-optimizer-states-reset initial` option.",
        "dateCreated": "2019-01-28T16:43:50Z",
        "datePublished": "2019-01-28T16:45:22Z",
        "html_url": "https://github.com/awslabs/sockeye/releases/tag/1.18.72",
        "name": "1.18.72",
        "tag_name": "1.18.72",
        "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/1.18.72",
        "url": "https://api.github.com/repos/awslabs/sockeye/releases/15220367",
        "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/1.18.72"
      },
      {
        "authorType": "User",
        "author_name": "fhieber",
        "body": "## [1.18.67]\r\n### Added\r\n- Added `fertility` as a further type of attention coverage.\r\n- Added an option for training to keep the initializations of the model via `--keep-initializations`. When set, the trainer will avoid deleting the params file for the first checkpoint, no matter what `--keep-last-params` is set to.\r\n\r\n## [1.18.66]\r\n### Fixed\r\n- Fix to argument names that are allowed to differ for resuming training.\r\n\r\n## [1.18.65]\r\n### Changed\r\n- More informative error message about inconsistent --shared-vocab setting.\r\n\r\n## [1.18.64]\r\n### Added\r\n- Adding translation sampling via `--sample [N]`. This causes the decoder to sample each next step from the target distribution probabilities at each\r\n  timestep. An optional value of `N` causes the decoder to sample only from the top `N` vocabulary items for each hypothesis at each timestep (the\r\n  default is 0, meaning to sample from the entire vocabulary).\r\n\r\n## [1.18.63]\r\n### Changed\r\n- The checkpoint decoder and nvidia-smi subprocess are now launched from a forkserver, allowing for a better separation between processes.\r\n\r\n## [1.18.62]\r\n### Added\r\n- Add option to make `TranslatorInputs` directly from a dict.",
        "dateCreated": "2018-12-21T10:52:11Z",
        "datePublished": "2018-12-21T10:55:03Z",
        "html_url": "https://github.com/awslabs/sockeye/releases/tag/1.18.67",
        "name": "1.18.67",
        "tag_name": "1.18.67",
        "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/1.18.67",
        "url": "https://api.github.com/repos/awslabs/sockeye/releases/14656823",
        "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/1.18.67"
      },
      {
        "authorType": "User",
        "author_name": "fhieber",
        "body": "## [1.18.61]\r\n### Changed\r\n- Update to MXNet 1.3.1. Removed requirements/requirements.gpu-cu{75,91}.txt as CUDA 7.5 and 9.1 are deprecated.\r\n\r\n## [1.18.60]\r\n### Fixed\r\n- Performance optimization to skip the softmax operation for single model greedy decoding is now only applied if no translation scores are required in the output.\r\n\r\n## [1.18.59]\r\n### Added\r\n- Full training state is now returned from EarlyStoppingTrainer's fit().\r\n### Changed\r\n- Training state cleanup will not be performed for training runs that did not converge yet.\r\n- Switched to portalocker for locking files (Windows compatibility).\r\n\r\n## [1.18.58]\r\n### Added\r\n- Added nbest translation, exposed as `--nbest-size`. Nbest translation means to not only output the most probable translation according to a model, but the top n most probable hypotheses. If `--nbest-size > 1` and the option `--output-type` is not explicitly specified, the output type will be changed to one JSON list of nbest translations per line. `--nbest-size` can never be larger than `--beam-size`.\r\n\r\n### Changed\r\n- Changed `sockeye.rerank` CLI to be compatible with nbest translation JSON output format.",
        "dateCreated": "2018-11-29T17:21:04Z",
        "datePublished": "2018-11-29T19:47:04Z",
        "html_url": "https://github.com/awslabs/sockeye/releases/tag/1.18.61",
        "name": "1.18.61",
        "tag_name": "1.18.61",
        "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/1.18.61",
        "url": "https://api.github.com/repos/awslabs/sockeye/releases/14266088",
        "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/1.18.61"
      },
      {
        "authorType": "User",
        "author_name": "tdomhan",
        "body": "## [1.18.57]\r\n### Added\r\n- Added `sockeye.score` CLI for quickly scoring existing translations ([documentation](tutorials/scoring.md)).\r\n### Fixed\r\n- Entry-point clean-up after the contrib/ rename",
        "dateCreated": "2018-10-16T16:54:14Z",
        "datePublished": "2018-10-26T13:34:02Z",
        "html_url": "https://github.com/awslabs/sockeye/releases/tag/1.18.57",
        "name": "1.18.57",
        "tag_name": "1.18.57",
        "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/1.18.57",
        "url": "https://api.github.com/repos/awslabs/sockeye/releases/13677512",
        "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/1.18.57"
      },
      {
        "authorType": "User",
        "author_name": "fhieber",
        "body": "## [1.18.56]\r\n### Changed\r\n- Update to MXNet 1.3.0.post0\r\n\r\n## [1.18.55]\r\n- Renamed `contrib` to less-generic `sockeye_contrib`",
        "dateCreated": "2018-09-20T07:14:26Z",
        "datePublished": "2018-09-20T07:17:05Z",
        "html_url": "https://github.com/awslabs/sockeye/releases/tag/1.18.56",
        "name": "1.18.56",
        "tag_name": "1.18.56",
        "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/1.18.56",
        "url": "https://api.github.com/repos/awslabs/sockeye/releases/12995837",
        "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/1.18.56"
      },
      {
        "authorType": "User",
        "author_name": "fhieber",
        "body": "## [1.18.54]\r\n### Added\r\n- `--source-factor-vocabs` can be set to provide source factor vocabularies.\r\n\r\n## [1.18.53]\r\n### Added\r\n- Always skipping softmax for greedy decoding by default, only for single models.\r\n- Added option `--skip-topk` for greedy decoding.\r\n\r\n## [1.18.52]\r\n### Fixed\r\n- Fixed bug in constrained decoding to make sure best hypothesis satifies all constraints.\r\n\r\n## [1.18.51]\r\n### Added\r\n- Added a CLI for reranking of an nbest list of translations.\r\n\r\n## [1.18.50]\r\n### Fixed\r\n- Check for equivalency of training and validation source factors was incorrectly indented.\r\n\r\n## [1.18.49]\r\n### Changed\r\n- Removed dependence on the nvidia-smi tool. The number of GPUs is now determined programatically.\r\n\r\n## [1.18.48]\r\n### Changed\r\n- Translator.max_input_length now reports correct maximum input length for TranslatorInput objects, independent of the internal representation, where an additional EOS gets added.",
        "dateCreated": "2018-09-16T13:27:45Z",
        "datePublished": "2018-09-16T13:30:54Z",
        "html_url": "https://github.com/awslabs/sockeye/releases/tag/1.18.54",
        "name": "1.18.54",
        "tag_name": "1.18.54",
        "tarball_url": "https://api.github.com/repos/awslabs/sockeye/tarball/1.18.54",
        "url": "https://api.github.com/repos/awslabs/sockeye/releases/12922413",
        "zipball_url": "https://api.github.com/repos/awslabs/sockeye/zipball/1.18.54"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1035,
      "date": "Sun, 26 Dec 2021 21:27:45 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "deep-learning",
      "deep-neural-networks",
      "machine-learning",
      "machine-translation",
      "neural-machine-translation",
      "encoder-decoder",
      "attention-mechanism",
      "sequence-to-sequence",
      "sequence-to-sequence-models",
      "sockeye",
      "attention-is-all-you-need",
      "attention-model",
      "seq2seq",
      "translation",
      "transformer-architecture",
      "transformer",
      "transformer-network",
      "pytorch"
    ],
    "technique": "GitHub API"
  }
}