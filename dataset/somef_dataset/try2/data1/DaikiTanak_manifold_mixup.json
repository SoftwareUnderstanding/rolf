{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1806.05236\n\nThis repo includes DenseNet (https://arxiv.org/pdf/1608.06993.pdf",
      "https://arxiv.org/abs/1512.03385"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9681501911997673,
        0.9999933029934741
      ],
      "excerpt": "pytorch implementation of manifold-mixup : https://arxiv.org/abs/1806.05236 \nThis repo includes DenseNet (https://arxiv.org/pdf/1608.06993.pdf), ResNet (https://arxiv.org/abs/1512.03385), and Dual Path Networks (https://arxiv.org/pdf/1707.01629.pdf). \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/DaikiTanak/manifold_mixup",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-01-23T04:51:28Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-17T16:58:44Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "pytorch implementation of manifold-mixup",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/DaikiTanak/manifold_mixup/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 28 Dec 2021 05:15:58 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/DaikiTanak/manifold_mixup/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "DaikiTanak/manifold_mixup",
    "technique": "GitHub API"
  },
  "invocation": [
    {
      "confidence": [
        0.8733450332610028,
        0.9014425823694966
      ],
      "excerpt": "train = Dataset(X, y) \ntrain_loader = torch.utils.data.DataLoader(train, batch_size=BATCH_SIZE, shuffle=True) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8779795841842539
      ],
      "excerpt": "model = densenet121(if_mixup=True) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8779795841842539
      ],
      "excerpt": "model = se_resnet18(if_mixup=True) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8779795841842539
      ],
      "excerpt": "model = dpn98(if_mixup=True) \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/DaikiTanak/manifold_mixup/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "manifold_mixup",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "manifold_mixup",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "DaikiTanak",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/DaikiTanak/manifold_mixup/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 14,
      "date": "Tue, 28 Dec 2021 05:15:58 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "``` python\noptimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True, dampening=0, weight_decay=0.0005)\nscheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[40, 60, 80], gamma=0.1)\n\n#: Define beta distribution\ndef mixup_data(alpha=1.0):\n    '''Return lambda'''\n    if alpha > 0.:\n        lam = np.random.beta(alpha, alpha)\n    else:\n        lam = 1.\n    return lam\n\nloss_function = nn.CrossEntropyLoss()\nbce_loss = torch.nn.BCELoss()\n\nfor epoch in range(EPOCH):\n  scheduler.step()\n  #: Training Phase\n  model.train()\n  train_loss = 0\n  for i, train_data in enumerate(tqdm(train_loader)):\n      inputs, labels = train_data\n      inputs = inputs.to(device)\n      labels = labels.to(device)\n      if not args.mixup:\n          #: if you don't use manifold mixup\n          outputs = model(inputs)\n          loss = loss_function(outputs, labels)\n\n      elif args.mixup:\n          #: if you use manifold mixup\n          lam = mixup_data(alpha=args.mixup_alpha)\n          lam = torch.from_numpy(np.array([lam]).astype('float32')).to(device)\n          output, reweighted_target = model(inputs, lam=lam, target=labels)\n          loss = bce_loss(softmax(output), reweighted_target)\n\n      train_loss += loss.item()\n      optimizer.zero_grad()\n      loss.backward()\n      optimizer.step()\n```\n",
      "technique": "Header extraction"
    }
  ]
}