{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1409.1556",
      "https://arxiv.org/abs/1608.06993",
      "https://arxiv.org/abs/1512.03385",
      "https://arxiv.org/abs/1505.04597"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.952482020494086
      ],
      "excerpt": "3. ResNet code - Deep Residual Learning for Image Recognition \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "    if len(y) == 1: \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/hycis/TensorGraph",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2016-01-06T04:59:00Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-06-30T04:42:56Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9664102902062383,
        0.9866635174552937,
        0.9576008431613313,
        0.9456733918180092,
        0.9619061467206461,
        0.9631166300485383
      ],
      "excerpt": "TensorGraph is a simple, lean, and clean framework on TensorFlow for building any imaginable models. \nAs deep learning becomes more and more common and the architectures becoming more \nand more complicated, it seems that we need some easy to use framework to quickly \nbuild these models and that's what TensorGraph is designed for. It's a very simple \nframework that adds a very thin layer above tensorflow. It is for more advanced \nusers who want to have more control and flexibility over his model building and \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9764201441936116
      ],
      "excerpt": "TensorGraph is targeted more at intermediate to advance users who feel keras or \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9473378721421895
      ],
      "excerpt": "constantly. Also for enterprise users who want to share deep learning models \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8451740901930999
      ],
      "excerpt": "Everything in TensorGraph is about layers. A model such as VGG or Resnet can be a layer. An identity block from Resnet or a dense block from Densenet can be a layer as well. Building models in TensorGraph is same as building a toy with lego. For example you can create a new model (layer) by subclass the BaseModel layer and use DenseBlock layer inside your ModelA layer. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9813348231091052
      ],
      "excerpt": "creating a layer only created all the Variables. To connect the Variables into a graph, you can do a train_fprop(X) or test_fprop(X) to create the tensorflow graph. By abstracting Variable creation away from linking the Variable nodes into graph prevent the problem of certain tensorflow layers that always reinitialise its weights when it's called, example the tf.nn.batch_normalization layer. Also having a separate channel for training and testing is to cater to layers with different training and testing behaviours such as batchnorm and dropout. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9261145390218889
      ],
      "excerpt": "1. VGG16 code and VGG19 code - Very Deep Convolutional Networks for Large-Scale Image Recognition \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8249511460213679
      ],
      "excerpt": "3. ResNet code - Deep Residual Learning for Image Recognition \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.876061149028644,
        0.9368741331342161
      ],
      "excerpt": "There are three types of layers, BaseLayer, BaseModel and Merge. \nBaseLayer is a low lying layer that wraps tensorflow codes directly, and define \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9002065213274221
      ],
      "excerpt": "When implementing BaseLayer we need to implement _train_fprop() and _test_fprop(), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8367126783955209
      ],
      "excerpt": "    ''' place all your variables and variables initialization here. ''' \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8215364242650981
      ],
      "excerpt": "       this is called during forward propagation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8689148833213769,
        0.9540477878476037
      ],
      "excerpt": "BaseModel is a higher level layer that can be made up of BaseLayers and \nBaseModels. For BaseModel, a default implementation of _train_fprop \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8821130259286946
      ],
      "excerpt": "Graph, to use this default implementation, we have to define self.startnode \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9333834769663193
      ],
      "excerpt": "Another way to customize your own inputs and outputs is to redefine _train_fprop \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8464078551808342
      ],
      "excerpt": "The default _train_fprop and _test_fprop in BaseModel looks like this \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8128222134901443
      ],
      "excerpt": "        raise Exception('{} is empty or not a list'.format(y)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9099493332194205,
        0.8092766598829512
      ],
      "excerpt": "for the MyLayerFork Model, for two inputs and two outputs, we can redefine it \nwith multiple StartNodes and EndNodes within _train_fprop and _test_fprop. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877,
        0.860059181823877
      ],
      "excerpt": "    model = MyLayerFork() \n    y1, y2 = model.train_fprop(X1, X2) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9856667411477337
      ],
      "excerpt": "        Concat which is a Merge layer is used to concat the list of states from \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9609872309779411
      ],
      "excerpt": "into graph. When we create nodes and layers, we also initializes all the tensorflow \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9975260469843916
      ],
      "excerpt": "The initialization of Variables and the linking of Variables into a computational \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9897448621297914,
        0.9966474696551649,
        0.8439370582997212,
        0.9508638459879365
      ],
      "excerpt": "the flexibility of building our computational graph without the worry of accidental \nreinitialization of the Variables. \nWe defined three types of nodes \nStartNode : for inputs to the graph \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8896252338225701
      ],
      "excerpt": "EndNode : for getting outputs from the model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8568952790563679,
        0.9530892747874549
      ],
      "excerpt": "to another HiddenNode or StartNode, the nodes are connected together to form \nan architecture. The graph always starts with StartNode and ends with EndNode. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8000756013199848
      ],
      "excerpt": "example of building a tensor graph. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9567588029116127
      ],
      "excerpt": "with ops.control_dependencies(update_ops): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9009614607129621
      ],
      "excerpt": "init_op = tf.group(tf.global_variables_initializer(), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "    for X,y in train_data: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A tensorflow library for building all kinds of models",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/hycis/TensorGraph/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 20,
      "date": "Mon, 27 Dec 2021 11:24:13 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/hycis/TensorGraph/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "hycis/TensorGraph",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/hycis/TensorGraph/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/hycis/TensorGraph/master/pipupdate.sh",
      "https://raw.githubusercontent.com/hycis/TensorGraph/master/test/models_zoo/aibraintumormodel/resources/run_mpi.sh",
      "https://raw.githubusercontent.com/hycis/TensorGraph/master/tensorgraph/models_zoo/aibraintumormodel/run_mpi.sh",
      "https://raw.githubusercontent.com/hycis/TensorGraph/master/tensorgraph/models_zoo/aibraintumormodel/run_nonmpi.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "First you need to install [tensorflow](https://www.tensorflow.org/versions/r0.9/get_started/os_setup.html)\n\nTo install tensorgraph for bleeding edge version via pip\n```bash\nsudo pip install --upgrade git+https://github.com/hycis/TensorGraph.git@master\n```\nor simply clone and add to `PYTHONPATH`.\n```bash\ngit clone https://github.com/hycis/TensorGraph.git\nexport PYTHONPATH=/path/to/TensorGraph:$PYTHONPATH\n```\nin order for the install to persist via export `PYTHONPATH`. Add `PYTHONPATH=/path/to/TensorGraph:$PYTHONPATH` to your `.bashrc` for linux or\n`.bash_profile` for mac. While this method works, you will have to ensure that\nall the dependencies in [setup.py](setup.py) are installed.\n\n-----\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8983557191619819
      ],
      "excerpt": ": Pin GPU to be used to process local rank (one GPU per process) \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9012248701992861
      ],
      "excerpt": "import tensorgraph as tg \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8778487586960795
      ],
      "excerpt": "         example: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8474168762440003
      ],
      "excerpt": "   #: first fork output \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8458751354831934
      ],
      "excerpt": "if name == 'main': \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8452442223327638
      ],
      "excerpt": "    return tf.concat(axis=self.axis, values=state_list) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9010195194286076
      ],
      "excerpt": "import horovod.tensorflow as hvd \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.925671696398174
      ],
      "excerpt": "import tensorflow as tf \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8712595579064647
      ],
      "excerpt": "opt = tf.train.RMSPropOptimizer(0.001) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123763140827432
      ],
      "excerpt": "                   tf.local_variables_initializer()) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8132145333878185,
        0.8415328298313424
      ],
      "excerpt": "config = tf.ConfigProto() \nconfig.gpu_options.allow_growth = True \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8008331685760428,
        0.8008331685760428,
        0.8483819940059046,
        0.8072303782655743
      ],
      "excerpt": "    sess.run(init_op) \n    bcast.run() \n#: training model \nfor epoch in range(100): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8008331685760428
      ],
      "excerpt": "        _, loss_train = sess.run([train_op, train_cost], feed_dict={X_ph:X, y_ph:y}) \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/hycis/TensorGraph/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Apache License 2.0",
      "url": "https://api.github.com/licenses/apache-2.0"
    },
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "TensorGraph",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "TensorGraph",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "hycis",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/hycis/TensorGraph/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "hycis",
        "body": "remove the need for keeping dimension after each layer",
        "dateCreated": "2021-06-30T04:43:05Z",
        "datePublished": "2021-06-30T04:48:40Z",
        "html_url": "https://github.com/hycis/TensorGraph/releases/tag/7.0.2",
        "name": "major reformat",
        "tag_name": "7.0.2",
        "tarball_url": "https://api.github.com/repos/hycis/TensorGraph/tarball/7.0.2",
        "url": "https://api.github.com/repos/hycis/TensorGraph/releases/45465462",
        "zipball_url": "https://api.github.com/repos/hycis/TensorGraph/zipball/7.0.2"
      },
      {
        "authorType": "User",
        "author_name": "hycis",
        "body": "",
        "dateCreated": "2016-08-31T02:30:03Z",
        "datePublished": "2016-09-01T05:43:42Z",
        "html_url": "https://github.com/hycis/TensorGraph/releases/tag/v1.4.2",
        "name": "first release",
        "tag_name": "v1.4.2",
        "tarball_url": "https://api.github.com/repos/hycis/TensorGraph/tarball/v1.4.2",
        "url": "https://api.github.com/repos/hycis/TensorGraph/releases/4025474",
        "zipball_url": "https://api.github.com/repos/hycis/TensorGraph/zipball/v1.4.2"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 69,
      "date": "Mon, 27 Dec 2021 11:24:13 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "tensorflow",
      "deep-learning"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "<img src=\"draw/graph.png\" height=\"250\">\n\nFirst define the `StartNode` for putting the input placeholder\n```python\ny1_dim = 50\ny2_dim = 100\nbatchsize = 32\nlearning_rate = 0.01\n\ny1 = tf.placeholder('float32', [None, y1_dim])\ny2 = tf.placeholder('float32', [None, y2_dim])\ns1 = StartNode(input_vars=[y1])\ns2 = StartNode(input_vars=[y2])\n```\nThen define the `HiddenNode` for putting the sequential layers in each `HiddenNode`\n```python\nh1 = HiddenNode(prev=[s1, s2],\n                input_merge_mode=Concat(),\n                layers=[Linear(y2_dim), RELU()])\nh2 = HiddenNode(prev=[s2],\n                layers=[Linear(y2_dim), RELU()])\nh3 = HiddenNode(prev=[h1, h2],\n                input_merge_mode=Sum(),\n                layers=[Linear(y1_dim), RELU()])\n                layers=[Linear(y1_dim+y2_dim, y2_dim), RELU()])\nh2 = HiddenNode(prev=[s2],\n                layers=[Linear(y2_dim, y2_dim), RELU()])\nh3 = HiddenNode(prev=[h1, h2],\n                input_merge_mode=Sum(),\n                layers=[Linear(y2_dim, y1_dim), RELU()])\n```\nThen define the `EndNode`. `EndNode` is used to back-trace the graph to connect\nthe nodes together.\n```python\ne1 = EndNode(prev=[h3])\ne2 = EndNode(prev=[h2])\n```\nFinally build the graph by putting `StartNodes` and `EndNodes` into `Graph`, we\ncan choose to use the entire architecture by using all the `StartNodes` and `EndNodes`\nand run the forward propagation to get symbolic output from train mode. The number\nof outputs from `graph.train_fprop` is the same as the number of `EndNodes` put\ninto `Graph`\n```python\ngraph = Graph(start=[s1, s2], end=[e1, e2])\no1, o2 = graph.train_fprop()\n```\nor we can choose which node to start and which node to end, example\n```python\ngraph = Graph(start=[s2], end=[e1])\no1, = graph.train_fprop()\n```\nFinally build an optimizer to optimize the objective function\n```python\no1_mse = tf.reduce_mean((y1 - o1)**2)\no2_mse = tf.reduce_mean((y2 - o2)**2)\nmse = o1_mse + o2_mse\noptimizer = tf.train.AdamOptimizer(learning_rate).minimize(mse)\n```\n\n-----\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Below is another example for building a more powerful [hierachical softmax](examples/hierachical_softmax.py)\nwhereby the lower hierachical softmax layer can be conditioned on all the upper\nhierachical softmax layers.\n\n<img src=\"draw/hsoftmax.png\" height=\"250\">\n\n```python\n#:#: params\nx_dim = 50\ncomponent_dim = 100\nbatchsize = 32\nlearning_rate = 0.01\n\n\nx_ph = tf.placeholder('float32', [None, x_dim])\n#: the three hierachical level\ny1_ph = tf.placeholder('float32', [None, component_dim])\ny2_ph = tf.placeholder('float32', [None, component_dim])\ny3_ph = tf.placeholder('float32', [None, component_dim])\n\n#: define the graph model structure\nstart = StartNode(input_vars=[x_ph])\n\nh1 = HiddenNode(prev=[start], layers=[Linear(component_dim), Softmax()])\nh2 = HiddenNode(prev=[h1], layers=[Linear(component_dim), Softmax()])\nh3 = HiddenNode(prev=[h2], layers=[Linear(component_dim), Softmax()])\nh1 = HiddenNode(prev=[start], layers=[Linear(x_dim, component_dim), Softmax()])\nh2 = HiddenNode(prev=[h1], layers=[Linear(component_dim, component_dim), Softmax()])\nh3 = HiddenNode(prev=[h2], layers=[Linear(component_dim, component_dim), Softmax()])\n\n\ne1 = EndNode(prev=[h1], input_merge_mode=Sum())\ne2 = EndNode(prev=[h1, h2], input_merge_mode=Sum())\ne3 = EndNode(prev=[h1, h2, h3], input_merge_mode=Sum())\n\ngraph = Graph(start=[start], end=[e1, e2, e3])\n\no1, o2, o3 = graph.train_fprop()\n\no1_mse = tf.reduce_mean((y1_ph - o1)**2)\no2_mse = tf.reduce_mean((y2_ph - o2)**2)\no3_mse = tf.reduce_mean((y3_ph - o3)**2)\nmse = o1_mse + o2_mse + o3_mse\noptimizer = tf.train.AdamOptimizer(learning_rate).minimize(mse)\n```\n\n-----\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Below is an example on transfer learning with bi-modality inputs and merge at\nthe middle layer with shared representation, in fact, TensorGraph can be used\nto build any number of modalities for transfer learning.\n\n<img src=\"draw/transferlearn.png\" height=\"250\">\n\n```python\n#:#: params\nx1_dim = 50\nx2_dim = 100\nshared_dim = 200\ny_dim = 100\nbatchsize = 32\nlearning_rate = 0.01\n\n\nx1_ph = tf.placeholder('float32', [None, x1_dim])\nx2_ph = tf.placeholder('float32', [None, x2_dim])\ny_ph = tf.placeholder('float32', [None, y_dim])\n\n#: define the graph model structure\ns1 = StartNode(input_vars=[x1_ph])\ns2 = StartNode(input_vars=[x2_ph])\n\nh1 = HiddenNode(prev=[s1], layers=[Linear(shared_dim), RELU()])\nh2 = HiddenNode(prev=[s2], layers=[Linear(shared_dim), RELU()])\nh3 = HiddenNode(prev=[h1,h2], input_merge_mode=Sum(),\n                layers=[Linear(y_dim), Softmax()])\nh1 = HiddenNode(prev=[s1], layers=[Linear(x1_dim, shared_dim), RELU()])\nh2 = HiddenNode(prev=[s2], layers=[Linear(x2_dim, shared_dim), RELU()])\nh3 = HiddenNode(prev=[h1,h2], input_merge_mode=Sum(),\n                layers=[Linear(shared_dim, y_dim), Softmax()])\n\ne1 = EndNode(prev=[h3])\n\ngraph = Graph(start=[s1, s2], end=[e1])\no1, = graph.train_fprop()\n\nmse = tf.reduce_mean((y_ph - o1)**2)\noptimizer = tf.train.AdamOptimizer(learning_rate).minimize(mse)\n```\n",
      "technique": "Header extraction"
    }
  ]
}