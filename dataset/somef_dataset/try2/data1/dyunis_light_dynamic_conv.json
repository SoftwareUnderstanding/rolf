{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1901.10430"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8260022124206593
      ],
      "excerpt": "  convolutions and improve performance \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "    - renormalize gradients if they exceed 0.1 norm \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8714162992508173
      ],
      "excerpt": "Machine translation (Table 1 + 2): \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dyunis/light_dynamic_conv",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-04-27T20:39:37Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-04-29T21:45:37Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.87231495793691
      ],
      "excerpt": "given that \\(W \\in \\mathbb{R}^{d \\times k}\\) for a kernel width \\(k\\), and an  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8027054364117208
      ],
      "excerpt": "  that they're convolving in \\(n\\), the time dimension? \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9206226328700434,
        0.8380087813803518
      ],
      "excerpt": "to reduce the number of parameters in the convolution, tie matrix \\(W\\) into  \n  chunks of size \\(\\frac{d}{H}\\) in the first dimension of size \\(d\\). This  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8348631060320385
      ],
      "excerpt": "  dimension (the one of size \\(k\\)) using a softmax \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9529867326134754
      ],
      "excerpt": "So the full expression for Light Convolutions is: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.807874034883729
      ],
      "excerpt": "linear one using \\(W^Q \\in \\mathbb{R}^{H \\times k \\times d}\\) such that at some \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8428397242663921
      ],
      "excerpt": "and we have the whole expression for dynamic convolutions: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8663141851474838
      ],
      "excerpt": "but, unlike self-attention, the changing weights depend only on the current  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8147301179863328
      ],
      "excerpt": "upscaling the input from \\(d \\to 2d\\), \\(W_I \\in \\mathbb{R}^{d \\times 2d}\\) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9297576866037767
      ],
      "excerpt": "the block that they actually replace self attention with is in figure 2: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8151569335605653,
        0.9834248445307517
      ],
      "excerpt": "fewer parameters, so to have a \"fair\" comparison, increase the number of blocks \n  to 7 instead of 6 for the LightConv and DynamicConv versions \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8476203880697271
      ],
      "excerpt": "kernel size: for encoder \\(k \\in [3,7,15,31,31,31,31]\\), for decoder \\(k \\in [3, 7, 15, 31, 31, 31]\\) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9693233393948762,
        0.9693233393948762
      ],
      "excerpt": "WMT En-Fr (English to French) \nWMT Zh-En (English to Chinese) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8717645604416024,
        0.8912042609170074
      ],
      "excerpt": "  test accuracy of the seed which resulted in the highest validation BLEU. \n  Ablations are conducted on the validation set and we report the mean BLEU \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9588250518052309,
        0.8721534496752483
      ],
      "excerpt": "\"For all datasets, we tune a length penalty as well as the number of  \n  checkpoints to average on the validation set.\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9561978473748322
      ],
      "excerpt": "Summarization: CNN-DailyMail summarization, evaluate on F1-Rouge, Rouge-1,  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9827194533594307
      ],
      "excerpt": "The gist is that it seems like everything imaginable is tuned \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8678664432052576
      ],
      "excerpt": "- also per task they have different kernel sizes \\(k\\) and number of heads \\(H\\) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9677044145690714,
        0.8497902063431805
      ],
      "excerpt": "Ablation of all the bells and whistles (Table 3): \nthey proceed strictly adding features, but one would probably like to see \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8991050019417348,
        0.9387942679521953
      ],
      "excerpt": "outperforming baselines except for an RL method \nis this a standard list of baselines? \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dyunis/light_dynamic_conv/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 21 Dec 2021 19:05:26 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/dyunis/light_dynamic_conv/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "dyunis/light_dynamic_conv",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.905424343665109
      ],
      "excerpt": "note that, like self-attention, the weights are changing per timestep \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8359299706379749
      ],
      "excerpt": "  \\text{Attention}(Q, K, V) = \\text{softmax} ( \\frac{QK^T}{\\sqrt{d_k}} ) V \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8359299706379749
      ],
      "excerpt": "  O_{i,c} = \\text{DepthwiseConv}(X,W_{c,:},i,c) = \\sum_{j=1}^k W_{c,j} X_{(i+j- \\lceil \\frac{k+1}{2} \\rceil), c} \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8359299706379749,
        0.8359299706379749
      ],
      "excerpt": "\\text{LightConv}(X,W_{\\lceil \\frac{cH}{d} \\rceil,:}, i, c) = \\text{DepthwiseConv} \n(X,\\text{DropConnect}(\\text{softmax}(W_{\\lceil \\frac{cH}{d} \\rceil,:},:)), i, c) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8359299706379749
      ],
      "excerpt": "\\text{DynamicConv}(X,i,c) = \\text{LightConv}(X, f(X_i)_{h,:}, i, c) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8088083039150537
      ],
      "excerpt": "        - first 60K have dim 1024 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8370843899288677
      ],
      "excerpt": "  test the transformer?) \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/dyunis/light_dynamic_conv/issues{/number}",
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "## Pay Less Attention with Lightweight and Dynamic Convolutions - Wu et al. 2019",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "light_dynamic_conv",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "dyunis",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dyunis/light_dynamic_conv/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 21 Dec 2021 19:05:26 GMT"
    },
    "technique": "GitHub API"
  }
}