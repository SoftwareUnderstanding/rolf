{
  "citation": [
    {
      "confidence": [
        0.826390740590465
      ],
      "excerpt": "2018 paper, uses information from strokes in the characters, cool paper \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8283216015784888,
        0.9415278611747312
      ],
      "excerpt": "http://aclweb.org/anthology/D17-1080  \nC++ implementation at https://github.com/oshikiri/w2v-sembei , seems buggy? \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9136407644925094
      ],
      "excerpt": "Jieba segmenter (https://github.com/fxsjy/jieba) claims 1.5 MB/s, which means whole indexed zhtw factiva would be segmented in a single day on one machine. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8451619309498795
      ],
      "excerpt": " - rust implementation of jieba:  https://github.com/messense/jieba-rs \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/merlon/nlp-chinese-experiments",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-09-13T11:19:30Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-07-06T15:18:25Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9947770301046956,
        0.936520359375493,
        0.8381247354440396,
        0.8059874555851874
      ],
      "excerpt": "In this repo are code and notes related to experimentation with chinese embeddings. We would like to use embeddings of chinese words as input to our disambiguation and involvement models. \nHowever, since chinese is not naturally segmented language (there are no spaces) this is problematic. \nFor any questions about this repo, contact Jan Bogar. \nThis README is organized as follows: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9637521386604071
      ],
      "excerpt": "4. For the future: where to get data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.911510754297319
      ],
      "excerpt": "\u201cWe used the Stanford word segmenter for Chinese\u201d \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.809016930469521
      ],
      "excerpt": "No mention of segmentation, but they obviously used it  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9449270987705582,
        0.8793643231341297
      ],
      "excerpt": "About ngrams of words, so might be turned into ngrams of characters \nContains some chinese analogy datasets \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.873874835437148
      ],
      "excerpt": "In this notebook I compared accuracy of jieba segmenter and Stanford NLP segmenter against annotated dataset: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8685290123545322
      ],
      "excerpt": "Number of sentences without error: 4622 for jieba, 4843 for stanfordNLP \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9657585908559713,
        0.9630178861774533,
        0.9410149809074673
      ],
      "excerpt": "Majority of approaches to chinese NLP (including embeddings) assumes segmentation of chinese sentences as a first step. In light of that, I researched two tools for chinese language segmentation: Stanford NLP Segmenter and Jieba . \nOn my human annotated dataset, only about 10% of sentences are segmented without any error. Even if the segmentation rules are not clear, it is alarmingly low success ratio. Accuracy on individual boundaries is for both jieba and stanford sentencer are below 90 %. \nJieba would be capable of segmenting  whole factiva in a matter of 1-2 days on a single machine. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9040944097781292,
        0.9839628750852456
      ],
      "excerpt": " - Fasttext embeddings for chinese words are freely available and would be easy to use . Also training the embeddings on our own  datasets would be relatively easy. \n - Fasttext might also be partially immune to effects of erroneous segmentation, since it uses subword information for learning of embeddings, and therefore might assign approximately correct vector also to word that is incomplete or has some characters added. This is however untested hypothesis. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8708787679919443,
        0.9417273660869286
      ],
      "excerpt": " - segmentation is unreliable and introduces another source of error early on in the pipeline. \n - For segmentation-free approaches, implementations are few. Training of our own embeddings would require a lot more effort (about a week of research and coding for working prototype at best, unless ngram2vec proves viable option). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8155855618347391
      ],
      "excerpt": "Use fasttext on unsegmented text ( e.g. use all ngrams in the text instead of words) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8555890588851409
      ],
      "excerpt": " - Likely worse than both of the above \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "experiments with chinese segmenters and embeddings",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/oapio/nlp-chinese-experiments/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sat, 25 Dec 2021 12:45:19 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/merlon/nlp-chinese-experiments/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "merlon/nlp-chinese-experiments",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/oapio/nlp-chinese-experiments/master/word_lengths.ipynb",
      "https://raw.githubusercontent.com/oapio/nlp-chinese-experiments/master/segmenters%20test.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8235364196061509
      ],
      "excerpt": "Segmentation with https://github.com/fxsjy/jieba \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8235364196061509
      ],
      "excerpt": "Segmentation with https://github.com/NLPchina/ansj_seg \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8045184183036755
      ],
      "excerpt": "C++ implementation at https://github.com/oshikiri/w2v-sembei , seems buggy? \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/merlon/nlp-chinese-experiments/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Chinese embeddings and segmentation research",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "nlp-chinese-experiments",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "merlon",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/merlon/nlp-chinese-experiments/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sat, 25 Dec 2021 12:45:19 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "For any future chinese embeddigns research, we will need huge corpus of raw chinese text.\nLuckily, we have whole factiva clone in jsonl format in google storage (also indexed in elasticsearch).\n\nData is described in the beggining of this document: https://docs.google.com/document/d/1j_5AYKNEM0tbRgixkmM1OzGjLbUHWIB52kYPRzCdVbY/edit#heading=h.xtuqoz5uvrzr\n\nLink to the data is: https://console.cloud.google.com/storage/browser/factiva-snapshots-processed/snapshot-full?project=merlon-182319&authuser=0&pli=1&angularJsUrl=%2Fstorage%2Fbrowser%2Ffactiva-snapshots-processed%3Fproject%3Dmerlon-182319%26authuser%3D1%26pli%3D1\n\nTo download the data and other operations, I strongly reccomend use of gsutil tool.\nIf you will train embeddings in google cloud, you don't have to download the data, so just download one month or one year for experiments.\nIt is really just a bunch of jsonl files with one article per line, sorted into directories by language, year and month.\n\nPerson of contact for the data is Michal Nanasi.\n",
      "technique": "Header extraction"
    }
  ]
}