{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1712.05884",
      "https://arxiv.org/abs/1710.08969",
      "https://arxiv.org/abs/1910.10838"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- [NVIDIA's tacotron2](https://github.com/NVIDIA/tacotron2)\n- [keonlee9420's tacotron2_MMI](https://github.com/keonlee9420/tacotron2_MMI)\n- [keonlee9420's STYLER](https://github.com/keonlee9420/STYLER)\n- [philipperemy's DeepSpeaker](https://github.com/philipperemy/deep-speaker)\n- [espnet's diagonal guided attention loss](https://github.com/espnet/espnet/blob/e962a3c609ad535cd7fb9649f9f9e9e0a2a27291/espnet/nets/pytorch_backend/e2e_tts_tacotron2.py#L25)\n- [Zero-Shot Multi-Speaker Text-To-Speech with State-of-the-art Neural Speaker Embeddings](https://arxiv.org/abs/1910.10838)\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```\n@misc{lee2021comprehensive-tacotron2,\n  author = {Lee, Keon},\n  title = {Comprehensive-Tacotron2},\n  year = {2021},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/keonlee9420/Comprehensive-Tacotron2}}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{lee2021comprehensive-tacotron2,\n  author = {Lee, Keon},\n  title = {Comprehensive-Tacotron2},\n  year = {2021},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/keonlee9420/Comprehensive-Tacotron2}}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/keonlee9420/Comprehensive-Tacotron2",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-07-24T03:36:08Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-18T15:39:44Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9960262358606998
      ],
      "excerpt": "PyTorch Implementation of Google's Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions. Unlike many previous implementations, this is kind of a Comprehensive Tacotron2 where the model supports both single-, multi-speaker TTS and several techniques such as reduction factor to enforce the robustness of the decoder alignment. The model can learn alignment only in 5k. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9678777675683804
      ],
      "excerpt": "The validation logs up to 70K of synthesized mel and alignment are shown below (LJSpeech_val_LJ038-0050 and VCTK_val_p323_008 from top to bottom). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9243717878618372
      ],
      "excerpt": "Batch inference is also supported, try \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9579992662465059,
        0.8605590358331859
      ],
      "excerpt": "to synthesize all utterances in preprocessed_data/LJSpeech/val.txt. You can replace LJSpeech with VCTK. Note that only 1 batch size is supported currently due to the autoregressive model architecture. \nThe supported datasets are \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9095368652286329
      ],
      "excerpt": "VCTK: The CSTR VCTK Corpus includes speech data uttered by 110 English speakers (multi-speaker TTS ) with various accents. Each speaker reads out about 400 sentences, which were selected from a newspaper, the rainbow passage and an elicitation paragraph used for the speech accent archive. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8816330491236615
      ],
      "excerpt": "For a multi-speaker TTS with external speaker embedder, download ResCNN Softmax+Triplet pretrained model of philipperemy's DeepSpeaker for the speaker embedding and locate it in ./deepspeaker/pretrained_models/. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9823795354674532,
        0.95536790042888
      ],
      "excerpt": "Support n_frames_per_step&gt;1 mode (which is not supported by NVIDIA's tacotron2). This is the key factor to get the robustness of the decoder alignment as described in the paper. Also, it reduces the training & inference time by the factor time. \nThe current implementation provides pre-trained model of n_frames_per_step==2, but it should also work for any number greater than 2. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "PyTorch Implementation of Google's Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions. This implementation supports both single-, multi-speaker TTS and several techniques to enforce the robustness and efficiency of the model.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/keonlee9420/Comprehensive-Tacotron2/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 6,
      "date": "Sun, 26 Dec 2021 17:45:54 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/keonlee9420/Comprehensive-Tacotron2/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "keonlee9420/Comprehensive-Tacotron2",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8005652726390801
      ],
      "excerpt": "You have to download the pretrained models and put them in output/ckpt/LJSpeech/ or output/ckpt/VCTK/. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.882394915165871
      ],
      "excerpt": "    <img src=\"img/model.png\" width=\"80%\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.847000395032459
      ],
      "excerpt": "    <img src=\"./img/LJSpeech_val_LJ038-0050.gif\" width=\"80%\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.847000395032459
      ],
      "excerpt": "    <img src=\"./img/VCTK_val_p323_008.gif\" width=\"80%\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.869069506622991
      ],
      "excerpt": "You have to download the pretrained models and put them in output/ckpt/LJSpeech/ or output/ckpt/VCTK/. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9125808348480132
      ],
      "excerpt": "python3 synthesize.py --text \"YOUR_DESIRED_TEXT\" --restore_step RESTORE_STEP --mode single --dataset LJSpeech \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9125808348480132,
        0.8511167213741988
      ],
      "excerpt": "python3 synthesize.py --text \"YOUR_DESIRED_TEXT\" --speaker_id SPEAKER_ID --restore_step RESTORE_STEP --mode single --dataset VCTK \nThe generated utterances will be put in output/result/. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8817082648442314
      ],
      "excerpt": "python3 synthesize.py --source preprocessed_data/LJSpeech/val.txt --restore_step RESTORE_STEP --mode batch --dataset LJSpeech \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8705832100376315,
        0.8287974015137436,
        0.9150949912405066
      ],
      "excerpt": "  python3 preprocess.py --dataset DATASET \nTrain your model with \npython3 train.py --dataset DATASET \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.825361263358686
      ],
      "excerpt": "tensorboard --logdir output/log \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8999813819370864
      ],
      "excerpt": "    <img src=\"./preprocessed_data/VCTK/spker_embed_tsne.png\" width=\"80%\"> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/keonlee9420/Comprehensive-Tacotron2/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 Philippe R\\xc3\\xa9my\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Comprehensive Tacotron2 - PyTorch Implementation",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Comprehensive-Tacotron2",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "keonlee9420",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/keonlee9420/Comprehensive-Tacotron2/blob/main/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "keonlee9420",
        "body": "",
        "dateCreated": "2021-07-29T11:30:40Z",
        "datePublished": "2021-08-11T04:12:27Z",
        "html_url": "https://github.com/keonlee9420/Comprehensive-Tacotron2/releases/tag/v0.1.0",
        "name": "First Release",
        "tag_name": "v0.1.0",
        "tarball_url": "https://api.github.com/repos/keonlee9420/Comprehensive-Tacotron2/tarball/v0.1.0",
        "url": "https://api.github.com/repos/keonlee9420/Comprehensive-Tacotron2/releases/47643403",
        "zipball_url": "https://api.github.com/repos/keonlee9420/Comprehensive-Tacotron2/zipball/v0.1.0"
      },
      {
        "authorType": "User",
        "author_name": "keonlee9420",
        "body": "First Official Release",
        "dateCreated": "2021-07-29T11:30:40Z",
        "datePublished": "2021-08-03T01:45:21Z",
        "html_url": "https://github.com/keonlee9420/Comprehensive-Tacotron2/releases/tag/v1.0.0",
        "name": "v1.0.0",
        "tag_name": "v1.0.0",
        "tarball_url": "https://api.github.com/repos/keonlee9420/Comprehensive-Tacotron2/tarball/v1.0.0",
        "url": "https://api.github.com/repos/keonlee9420/Comprehensive-Tacotron2/releases/47194205",
        "zipball_url": "https://api.github.com/repos/keonlee9420/Comprehensive-Tacotron2/zipball/v1.0.0"
      }
    ],
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You can install the Python dependencies with\n```\npip3 install -r requirements.txt\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 21,
      "date": "Sun, 26 Dec 2021 17:45:54 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "text-to-speech",
      "tts",
      "tacotron",
      "tacotron2",
      "pytorch",
      "speech-synthesis",
      "autoregressive",
      "single-speaker",
      "multi-speaker",
      "robustness",
      "efficiency",
      "comprehensive",
      "neural-tts",
      "mel-gan",
      "hifi-gan",
      "reduction-factor",
      "diagonal-guided-attention",
      "deep-learning"
    ],
    "technique": "GitHub API"
  }
}