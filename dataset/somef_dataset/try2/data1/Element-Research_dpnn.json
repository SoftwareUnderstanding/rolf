{
  "citation": [
    {
      "confidence": [
        0.9712438395444837
      ],
      "excerpt": "(Hinton et al. 2012, p. 2) . \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9853315135509472
      ],
      "excerpt": "Ref: http://arxiv.org/pdf/1602.07360v1.pdf \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "attempts = 10 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "   end \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "d ln(f(x,p))     1/p[i]    if i = x \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Element-Research/dpnn",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2015-04-06T15:58:58Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-01T02:54:24Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.886207401260495
      ],
      "excerpt": "This package provides many useful features that aren't part of the main nn package. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8777381539621864,
        0.8697305956432995
      ],
      "excerpt": "parameters or gradParameters with the original module, without incuring any memory overhead. \nWe also redefined type such that the type-cast preserves Tensor sharing within a structure of modules. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9092308942518406
      ],
      "excerpt": "ReverseTable : reverse the order of elements in a table; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8231200452245283,
        0.9064633339386062,
        0.9474311219638604,
        0.8999982587432711,
        0.8283146174552133,
        0.8540349901009908,
        0.9394798065059199
      ],
      "excerpt": "SpatialGlimpse : takes a fovead glimpse of an image at a given location; \nKmeans : Kmeans clustering layer. Forward computes distances with respect to centroids and returns index of closest centroid. Centroids can be updated using gradient descent. Centroids could be initialized randomly or by using kmeans++ algoirthm; \nSpatialRegionDropout : Randomly dropouts a region (top, bottom, leftmost, rightmost) of the input image. Works with batch and any number of channels; \nFireModule : FireModule as mentioned in the SqueezeNet; \nNCEModule : optimized placeholder for a Linear + SoftMax using noise-contrastive estimation. \nSpatialFeatNormalization : Module for widely used preprocessing step of mean zeroing and standardization for images. \nSpatialBinaryConvolution : Module for binary spatial convolution (Binary weights) as mentioned in XNOR-Net. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8558470795518546
      ],
      "excerpt": "PCAColorTransform : Module for adding noise to input image using Principal Components Analysis. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8350089705348115,
        0.9379108021133385
      ],
      "excerpt": "VRClassReward : criterion for variance-reduced classification-based reward; \nBinaryClassReward : criterion for variance-reduced binary classification reward (like VRClassReward, but for binary classes); \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8716956809399634,
        0.8060150781171693,
        0.834318309732498
      ],
      "excerpt": " * BinaryLogisticRegression : criterion for binary logistic regression; \n * SpatialBinaryLogisticRegression : criterion for pixel wise binary logistic regression; \n * NCECriterion : criterion exclusively used with NCEModule. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8716956809399634,
        0.8060150781171693,
        0.8052781780445771,
        0.8105359094226414
      ],
      "excerpt": " * BinaryLogisticRegression : criterion for binary logistic regression. \n * SpatialBinaryLogisticRegression : criterion for pixel wise binary logistic regression. \nA lot of the functionality implemented here was pulled from \ndp, which makes heavy use of this package. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8869774810143517
      ],
      "excerpt": "which is one of the main reasons why we made it. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9192233658572296
      ],
      "excerpt": "weightDecay, maxParamNorm (for regularization), and so on. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8632340819874873
      ],
      "excerpt": "Defaults to {'weight', 'bias'}, which is a static variable (i.e. table exists in class namespace). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8632340819874873
      ],
      "excerpt": "Defaults to {'gradWeight', 'gradBias'}, which is a static variable (i.e. table exists in class namespace). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9261547645415676
      ],
      "excerpt": "This function converts all the parameters of a module to the given type_str. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9966474696551649
      ],
      "excerpt": "maintain the sharing of storage \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8506678336083956
      ],
      "excerpt": "with the original module. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8360381409930925,
        0.9348485402538272
      ],
      "excerpt": "the gradients w.r.t. parameters with the original module. \nThis is equivalent to : \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9875805019343794
      ],
      "excerpt": "yet it is much more efficient, especially for modules with lots of parameters, as these \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8302836887631456,
        0.8364239267752633
      ],
      "excerpt": "This is particularly useful for Recurrent neural networks \nwhich require efficient copies with shared parameters and gradient w.r.t. parameters for each time-step. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9731837334485816
      ],
      "excerpt": "This method implements a hard constraint on the upper bound of the norm of output and/or input neuron weights \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9537142398278886,
        0.8704514824910765
      ],
      "excerpt": "In a weight matrix, this is a contraint on rows (maxOutNorm) and/or columns (maxInNorm), respectively. \nHas a regularization effect analogous to weightDecay, but with easier to optimize hyper-parameters. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8617542474707612
      ],
      "excerpt": "Only affects parameters with more than one dimension. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9097357972051064
      ],
      "excerpt": "Returns a table of Tensors (momGradParams). For each element in the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9055324892424731,
        0.9427686922188242
      ],
      "excerpt": "(gradParams) is returned by a call to parameters. \nThis method is used internally by updateGradParameters. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9438182211197719
      ],
      "excerpt": "When using momentum learning, another Tensor is added for each parameter Tensor (momGradParams). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8462406380791223
      ],
      "excerpt": "The default is to use classic momentum (momNesterov = false). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.979515358303127,
        0.954005800439481
      ],
      "excerpt": "Decays the weight of the parameterized models. \nImplements an L2 norm loss on parameters with dimensions greater or equal to wdMinDim (default is 2). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8722781779296513,
        0.970122413033299,
        0.9342820172095258
      ],
      "excerpt": "Implements a contrainst on the norm of gradients w.r.t. parameters (Pascanu et al. 2012). \nWhen moduleLocal = false (the default), the norm is calculated globally to Module for which this is called. \nSo if you call it on an MLP, the norm is computed on the concatenation of all parameter Tensors. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.927700921640369,
        0.9245186528038692
      ],
      "excerpt": "to the norm of all parameters in each component (non-container) module. \nThis method is useful to prevent the exploding gradient in \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9691573162974947
      ],
      "excerpt": "This method is used by Criterions that implement the REINFORCE algorithm like VRClassReward. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.942233545696172,
        0.9004988106261894
      ],
      "excerpt": "REINFORCE Criterions broadcast a reward to all REINFORCE modules between the forward and the backward. \nIn this way, when the following call to backward reaches the REINFORCE modules, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9816049470941441
      ],
      "excerpt": "The reward is broadcast to all REINFORCE modules contained \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8100777338516852
      ],
      "excerpt": "Note that the reward should be a 1D tensor of size batchSize, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.912118838665183,
        0.8805247669145795,
        0.8445508690320558
      ],
      "excerpt": "This module is a decorator that can be used to control the serialization/deserialization \nbehavior of the encapsulated module. Basically, making the resulting string or \nfile heavy (the default), medium or light in terms of size. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8072146434315397,
        0.9480713868679597,
        0.813014094801899,
        0.9668509010896795
      ],
      "excerpt": "Note that this will also be the type of the deserialized object. \nThe default serialization tensortype is nil, i.e. the module is serialized as is. \nThe heavySerial() has the serialization process serialize every attribute in the module graph, \nwhich is the default behavior of nn. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8682550472808229
      ],
      "excerpt": "Some modules overwrite the default Module.dpnn_mediumEmpty static attribute with their own. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8929013942235748,
        0.9092530825914178,
        0.9475185631668794
      ],
      "excerpt": "But also empties all the parameter gradients specified by the \nattribute dpnn_gradParameters, which defaults to {gradWeight, gradBias}. \nWe recomment using mediumSerial() for training, and lightSerial() for \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8401509303374116
      ],
      "excerpt": "The original paper uses 2+2 where the first two are (but there could be more than two): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8642599348793603,
        0.8559607020725433
      ],
      "excerpt": "and where the other two are : \n3x3 maxpool -> 1x1 conv (reduce/project) -> relu \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9650526976409415,
        0.9048884933313375,
        0.8902402650744403,
        0.9703146564183247
      ],
      "excerpt": "This module allows the first group of columns to be of any \nnumber while the last group consist of exactly two columns. \nThe 1x1 convoluations are used to reduce the number of input channels \n(or filters) such that the capacity of the network doesn't explode. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8197170433060633
      ],
      "excerpt": "configuration options are specified in lists of n+2 elements. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8737689043313828,
        0.9487897351518665
      ],
      "excerpt": "outputSize : numbers of filters in the non-1x1 convolution kernel sizes, e.g. {32,48} \nreduceSize : numbers of filters in the 1x1 convolutions (reduction) used in each column, e.g. {48,64,32,32}. The last 2 are used respectively for the max pooling (projection) column (the last column in the paper) and the column that has nothing but a 1x1 conv (the first column in the paper). This table should have two elements more than the outputSize \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9989355781379824
      ],
      "excerpt": "reduceStride : strides of the 1x1 (reduction) convolutions. Defaults to {1,1,...}. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9488986909802875,
        0.9127085501038632,
        0.9775937492690902,
        0.9745706787367668,
        0.9548691908803201,
        0.8138429718326321
      ],
      "excerpt": "padding : set this to true to add padding to the input of the convolutions such that output width and height are same as that of the original non-padded input. Defaults to true. \nkernelSize : size (height = width) of the non-1x1 convolution kernels. Defaults to {5,3}. \nkernelStride : stride of the kernels (height = width) of the convolution. Defaults to {1,1} \npoolSize: size (height = width) of the spatial max pooling used in the next-to-last column. Defaults to 3. \npoolStride : stride (height = width) of the spatial max pooling. Defaults to 1. \nFor a complete example using this module, refer to the following : \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9092308942518406
      ],
      "excerpt": "Reverses the order of elements in a table. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8314680942494376
      ],
      "excerpt": "This module clips input values such that the output is between minval and maxval. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.848797572079307
      ],
      "excerpt": "within the boundaries of the input image. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8732828588969164,
        0.947731071281522
      ],
      "excerpt": "This module is commonly used at the input layer to artificially \naugment the size of the dataset to prevent overfitting. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8216567707461434
      ],
      "excerpt": "Ref. A. Recurrent Model for Visual Attention \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9651961111799601
      ],
      "excerpt": "A glimpse is the concatenation of down-scaled cropped images of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9006387740052055,
        0.9651732076862566,
        0.8787050038662197
      ],
      "excerpt": "The input is a pair of Tensors: {image, location} \nlocation are (y,x) coordinates of the center of the different scales \nof patches to be cropped from image image. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9010907916783906,
        0.8121078662058051,
        0.9640117237994775
      ],
      "excerpt": "The output is a batch of glimpses taken in image at location (y,x). \nsize can be either a scalar which specifies the width = height of glimpses, \nor a table of {height, width} to support a rectangular shape of glimpses. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.916311662854034,
        0.9161723611434984,
        0.984499854469879
      ],
      "excerpt": "So basically, this module can be used to focus the attention of the model \non a region of the input image. \nIt is commonly used with the RecurrentAttention \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9488146879167939
      ],
      "excerpt": "Following is an example of SpatialRegionDropout outputs on the famous lena image. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.991038211057757,
        0.9499628316001265,
        0.9501971385553466
      ],
      "excerpt": "FireModule is comprised of two submodules 1) A squeeze convolution module comprised of 1x1 filters followed by 2) an expand module that is comprised of a mix of 1x1 and 3x3 convolution filters. \nArguments: s1x1: number of 1x1 filters in the squeeze submodule, e1x1: number of 1x1 filters in the expand submodule, e3x3: number of 3x3 filters in the expand submodule. It is recommended that s1x1 be less than (e1x1+e3x3) if you want to limit the number of input channels to the 3x3 filters in the expand submodule. \nFireModule works only with batches, for single sample convert the sample to a batch of size 1. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.823112818176006
      ],
      "excerpt": "This module normalizies each feature channel of input image based on its corresponding mean and standard deviation scalar values. This module does not learn the mean and std, they are provided as arguments. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9935698062622835
      ],
      "excerpt": "Functioning of SpatialBinaryConvolution is similar to nn/SpatialConvolution. Only difference is that Binary weights are used for forward/backward and floating point weights are used for weight updates. Check Binary-Weight-Network section of XNOR-net. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9272913260627716
      ],
      "excerpt": "This module performs a simple data augmentation technique. SimpleColorTransform module adds random noise to each color channel independently. In more advanced data augmentation technique noise is added using principal components of color channels. For that please check PCAColorTransform \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9259448024271412
      ],
      "excerpt": "std = 0.1 -- Std deviation of normal distribution with mean zero for noise. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.95695391088993
      ],
      "excerpt": "This module performs a data augmentation using Principal Component analysis of pixel values. When in training mode, mulitples of principal components are added to input image pixels. Magnitude of value added (noise) is dependent upon the corresponding eigen value and a random value sampled from a Gaussian distribution with mean zero and std (default 0.1) standard deviation. This technique was used in the famous AlexNet paper. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9971760296077524
      ],
      "excerpt": "k is the number of centroids and dim is the dimensionality of samples. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9891785691729262
      ],
      "excerpt": "      km:backward(samples, gradOutput) -- gradOutput is ignored \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8306089718479601
      ],
      "excerpt": "This criterion decorates a criterion by allowing the input and target to be \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8897492932032536,
        0.8490037945672047
      ],
      "excerpt": "When castTarget = true (the default), the targetModule is cast along with the inputModule and \ncriterion. Otherwise, the targetModule isn't. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9380823291367479
      ],
      "excerpt": "When used in conjunction with NCECriterion, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9853937035796649
      ],
      "excerpt": "The point of the NCE is to speedup computation for large Linear + SoftMax layers. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9918869780706232
      ],
      "excerpt": "This is common when implementing language models having with large vocabularies of a million words. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9254878057467103,
        0.891604363568047
      ],
      "excerpt": "The inputSize and outputSize are the same as for the Linear module. \nThe number of noise samples to be drawn per example is k. A value of 25 should work well. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9765087015621915,
        0.8901936911327308,
        0.9819971548714602,
        0.8392286885950004,
        0.9239564391507934
      ],
      "excerpt": "The unigrams is a tensor of size outputSize that contains the frequencies or probability distribution over classes. \nIt is used to sample noise samples via a fast implementation of torch.multinomial. \nThe Z is the normalization constant of the approximated SoftMax. \nThe default is math.exp(9) as specified in Ref. A. \nFor inference, or measuring perplexity, the full Linear + SoftMax will need to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8510656543945491
      ],
      "excerpt": "Furthermore, to simulate Linear + LogSoftMax instead, one need only add the following to the above: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8326663835551541
      ],
      "excerpt": "This criterion only works with an NCEModule on the output layer. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9613864372399693,
        0.8972805536918962
      ],
      "excerpt": "The reinforce(reward) method is called by a special Reward Criterion (e.g. VRClassReward). \nAfter which, when backward is called, the reward will be used to generate gradInputs. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8458135146950733
      ],
      "excerpt": "When stochastic=false (the default), the module is only stochastic during training. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8803634203893199
      ],
      "excerpt": "where the reward is what is provided by a Reward criterion like \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.963046074357867,
        0.893152018989909,
        0.9229435847393357
      ],
      "excerpt": "where a is the alpha of the original paper, i.e. a reward scale, \nR is the raw reward (usually 0 or 1), and b is the baseline reward, \nwhich is often taken to be the expected raw reward R. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.936717281002758,
        0.8578962816241775
      ],
      "excerpt": "See ReinforceBernoulli for a concrete derivation. \nAlso, as you can see, the gradOutput is ignored. So within a backpropagation graph, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8225962953806188
      ],
      "excerpt": "with their own obtained from the broadcasted reward. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8085237724988705
      ],
      "excerpt": "A Reinforce subclass that implements the REINFORCE algorithm \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9340346476353791
      ],
      "excerpt": "Uses the REINFORCE algorithm (ref. A p.230-236) which is \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8738911665035191,
        0.9679526154935032
      ],
      "excerpt": "p : probability of sampling a 1 \nthe derivative of the log bernoulli w.r.t. probability p is : \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8085237724988705
      ],
      "excerpt": "A Reinforce subclass that implements the REINFORCE algorithm \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8667657245688573,
        0.8650737564487609
      ],
      "excerpt": "Inputs are the means of the normal distribution. \nThe stdev argument specifies the standard deviation of the distribution. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9340346476353791
      ],
      "excerpt": "Uses the REINFORCE algorithm (ref. A p.238-239) which is \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.950273197571371
      ],
      "excerpt": "the derivative of log normal w.r.t. mean u is : \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9144250795235553
      ],
      "excerpt": "As an example, it is used to sample locations for the RecurrentAttention \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8085237724988705
      ],
      "excerpt": "A Reinforce subclass that implements the REINFORCE algorithm \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9097775923742042
      ],
      "excerpt": "Inputs are the shapes of the gamma distribution. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9500636405798015
      ],
      "excerpt": "During evaluation, when stochastic=false, outputs are equal to the mean, defined as the product of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9340346476353791
      ],
      "excerpt": "Uses the REINFORCE algorithm (ref. A) which is \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9414321286068565
      ],
      "excerpt": "the derivative of log gamma w.r.t. shape k is : \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8085237724988705
      ],
      "excerpt": "A Reinforce subclass that implements the REINFORCE algorithm \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8990902610633408
      ],
      "excerpt": "Inputs are the categorical probabilities of the distribution : p[1], p[2], ..., p[k]. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8167000410858386
      ],
      "excerpt": "For n categories, both the input and output ares of size batchSize x n. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9340346476353791
      ],
      "excerpt": "Uses the REINFORCE algorithm (ref. A) which is \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.959059510188034
      ],
      "excerpt": "the derivative of log categorical w.r.t. probability vector p is : \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8406252253359919,
        0.9134866975579525
      ],
      "excerpt": "This Reward criterion implements the REINFORCE algoritm (ref. A) for classification models. \nSpecifically, it is a Variance Reduces (VR) classification reinforcement leanring (reward-based) criterion. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8928387239921527
      ],
      "excerpt": "While it conforms to the Criterion interface (which it inherits), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9505000626939046
      ],
      "excerpt": "Instead, a reward is broadcast to the module via the reinforce method. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9574356937128496,
        0.9407873489474585,
        0.962730914255955,
        0.9691926079887127,
        0.9809983015045362,
        0.8249100922785236,
        0.8988998336403671
      ],
      "excerpt": "where a is the alpha described in Ref. A, i.e. a reward scale (defaults to 1), \nR is the raw reward (0 for incorrect and 1 for correct classification), \nand b is the baseline reward, which is often taken to be the expected raw reward R. \nThe target of the criterion is a tensor of class indices. \nThe input to the criterion is a table {y,b} where y is the probability \n(or log-probability) of classes (usually the output of a SoftMax), \nand b is the baseline reward discussed above. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9301242486967586,
        0.8744854354899104,
        0.9425458893488535
      ],
      "excerpt": "As for b, its gradInputs are obtained from the criterion, which defaults to MSECriterion. \nThe criterion's target is the commensurate raw reward R. \nUsing a*(R-b) instead of a*R to obtain a reward is what makes this class variance reduced (VR). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9202025716563332
      ],
      "excerpt": "Note : for RNNs with R = 1 for last step in sequence, encapsulate it \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8959106350153186
      ],
      "excerpt": "For an example, this criterion is used along with the RecurrentAttention \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8881867513292965,
        0.8129739960232296,
        0.8550705752668607
      ],
      "excerpt": "So basically, the input is still a table of two tensors. \nThe first input tensor is of size batchsize containing Bernoulli probabilities. \nThe second input tensor is the baseline prediction described in VRClassReward. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8249767149866616
      ],
      "excerpt": "This criterion implements the score criterion mentioned in (ref. A). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9784916087976733
      ],
      "excerpt": "This criterion implements the spatial component of the criterion mentioned in  (ref. A). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "deep extensions to nn",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Element-Research/dpnn/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 81,
      "date": "Tue, 28 Dec 2021 18:50:54 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Element-Research/dpnn/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Element-Research/dpnn",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.9316869655325504
      ],
      "excerpt": "The package provides the following Modules: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8112695073980559
      ],
      "excerpt": "The following modules and criterions can be used to implement the REINFORCE algorithm : \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8004686696163884
      ],
      "excerpt": "However, dpnn can be used without dp (for e.g. you can use it with optim), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.933538006273495
      ],
      "excerpt": "clone = mlp:clone() \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8661176197453521
      ],
      "excerpt": "<a name='nn.Serial'></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8661176197453521
      ],
      "excerpt": "<a name='nn.Inception'></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8661176197453521
      ],
      "excerpt": "<a name='nn.ReverseTable'></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8661176197453521
      ],
      "excerpt": "<a name='nn.Clip'></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8661176197453521
      ],
      "excerpt": "<a name='nn.SpatialUniformCrop'></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8661176197453521
      ],
      "excerpt": "<a name='nn.SpatialGlimpse'></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8661176197453521
      ],
      "excerpt": "<a name='nn.SpatialRegionDropout'></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8661176197453521
      ],
      "excerpt": "<a name='nn.FireModule'></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8661176197453521
      ],
      "excerpt": "<a name='nn.SpatialFeatNormalization'></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8661176197453521
      ],
      "excerpt": "<a name='nn.SpatialBinaryConvolution'></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8661176197453521
      ],
      "excerpt": "<a name='nn.SimpleColorTransform'></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8661176197453521
      ],
      "excerpt": "<a name='nn.PCAColorTransform'></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8661176197453521
      ],
      "excerpt": "<a name='nn.Kmeans'></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.933538006273495
      ],
      "excerpt": "      bestKm = km:clone() \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8661176197453521
      ],
      "excerpt": "<a name='nn.ModuleCriterion'></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8661176197453521
      ],
      "excerpt": "<a name='nn.NCEModule'></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9036972081503227
      ],
      "excerpt": "be computed. The NCEModule can do this by switching on the following : \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.896469777115865
      ],
      "excerpt": "Furthermore, to simulate Linear + LogSoftMax instead, one need only add the following to the above: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8661176197453521
      ],
      "excerpt": "<a name='nn.NCECriterion'></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8661176197453521
      ],
      "excerpt": "<a name='nn.Reinforce'></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8259178517358924
      ],
      "excerpt": "The criterion will normally be responsible for the following formula : \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8661176197453521
      ],
      "excerpt": "<a name='nn.ReinforceBernoulli'></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8661176197453521
      ],
      "excerpt": "<a name='nn.ReinforceNormal'></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8661176197453521
      ],
      "excerpt": "<a name='nn.ReinforceGamma'></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8661176197453521
      ],
      "excerpt": "<a name='nn.ReinforceCategorical'></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8058859924462193
      ],
      "excerpt": "(ref. A) for a Categorical (i.e. Multinomial with one sample) probability distribution. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8661176197453521
      ],
      "excerpt": "<a name='nn.VRClassReward'></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8661176197453521
      ],
      "excerpt": "<a name='nn.BinaryClassReward'></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8661176197453521
      ],
      "excerpt": "<a name='nn.BLR'></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8661176197453521
      ],
      "excerpt": "<a name='nn.SpatialBLR'></a> \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8514240067010168
      ],
      "excerpt": "Nesterov momentum (momNesterov = true) is computed as follows (the first line is the same as classic momentum): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8421074476017179
      ],
      "excerpt": "<a name='nn.Serial'></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8421074476017179
      ],
      "excerpt": "<a name='nn.ReverseTable'></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8421074476017179
      ],
      "excerpt": "<a name='nn.Clip'></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8421074476017179
      ],
      "excerpt": "<a name='nn.SpatialUniformCrop'></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8598383869473318
      ],
      "excerpt": "During training, this module will output a cropped patch of size oheight, owidth \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8421074476017179
      ],
      "excerpt": "<a name='nn.SpatialGlimpse'></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8991091073079236
      ],
      "excerpt": "depth is number of patches to crop per glimpse (one patch per depth). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8421074476017179
      ],
      "excerpt": "<a name='nn.SpatialRegionDropout'></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8421074476017179
      ],
      "excerpt": "<a name='nn.FireModule'></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8421074476017179
      ],
      "excerpt": "<a name='nn.SpatialFeatNormalization'></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8421074476017179
      ],
      "excerpt": "<a name='nn.SpatialBinaryConvolution'></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8421074476017179
      ],
      "excerpt": "<a name='nn.SimpleColorTransform'></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8421074476017179
      ],
      "excerpt": "<a name='nn.PCAColorTransform'></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8421074476017179
      ],
      "excerpt": "<a name='nn.Kmeans'></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8158177619030575
      ],
      "excerpt": "km:initRandom(samples) -- Randomly initialize centroids from input samples. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8246241868790175
      ],
      "excerpt": "A call to forward will generate an output containing the index of the nearest cluster for each sample in the batch. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8421074476017179
      ],
      "excerpt": "<a name='nn.ModuleCriterion'></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8421074476017179
      ],
      "excerpt": "<a name='nn.NCEModule'></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8324604710457705
      ],
      "excerpt": "The number of noise samples to be drawn per example is k. A value of 25 should work well. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "ncem.logsoftmax = true \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8421074476017179
      ],
      "excerpt": "<a name='nn.NCECriterion'></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8421074476017179
      ],
      "excerpt": "<a name='nn.Reinforce'></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8471632383263955
      ],
      "excerpt": "            d ln(f(output,input)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8421074476017179
      ],
      "excerpt": "<a name='nn.ReinforceBernoulli'></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8471632383263955
      ],
      "excerpt": "d ln(f(output,input))   d ln(f(x,p))    (x - p) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8421074476017179
      ],
      "excerpt": "<a name='nn.ReinforceNormal'></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8421074476017179
      ],
      "excerpt": "<a name='nn.ReinforceGamma'></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8144338362021298
      ],
      "excerpt": "k : shape (input) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8421074476017179
      ],
      "excerpt": "<a name='nn.ReinforceCategorical'></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8421074476017179
      ],
      "excerpt": "<a name='nn.VRClassReward'></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8421074476017179
      ],
      "excerpt": "<a name='nn.BinaryClassReward'></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8430858212076273
      ],
      "excerpt": "The first input tensor is of size batchsize containing Bernoulli probabilities. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8421074476017179
      ],
      "excerpt": "<a name='nn.BLR'></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8421074476017179
      ],
      "excerpt": "<a name='nn.SpatialBLR'></a> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Element-Research/dpnn/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Lua",
      "CMake"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other"
    },
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "dpnn : deep extensions to nn",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "dpnn",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Element-Research",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Element-Research/dpnn/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 190,
      "date": "Tue, 28 Dec 2021 18:50:54 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[Sagar Waghmare](https://github.com/sagarwaghmare69) wrote a nice [tutorial](tutorials/ladder.md)\non how to use dpnn with nngraph to reproduce the\n[Lateral Connections in Denoising Autoencoders Support Supervised Learning](http://arxiv.org/pdf/1504.08215.pdf).\n\nA brief (1 hours) overview of Torch7, which includes some details about __dpnn__,\nis available via this [NVIDIA GTC Webinar video](http://on-demand.gputechconf.com/gtc/2015/webinar/torch7-applied-deep-learning-for-vision-natural-language.mp4). In any case, this presentation gives a nice overview of Logistic Regression, Multi-Layer Perceptrons, Convolutional Neural Networks and Recurrent Neural Networks using Torch7.\n\n\n\n<a name='nn.Module'></a>\n",
      "technique": "Header extraction"
    }
  ]
}