{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Our code is based on the work of Xiaowei Hu (xhujoy) who shared his implementation of A3C for pysc2.\n\nSpecial thanks to Professor Iddo Drori, our instructor at Columbia University, as well as Niels Justesen for their expertise and guidance.\n\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1710.09767v2, 2017.](https://arxiv.org/pdf/1710.09767.pdf)<a name=\"MLSH\"></a>\n4. [Xiaowei Hu's PySC2 Agents](https://github.com/xhujoy/pysc2-agents) \n"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. [O. Vinyals, T. Ewalds, S. Bartunov, P. Georgiev. et al. StarCraft II: A New Challenge for Reinforcement Learning. Google DeepMind, 2017.](https://deepmind.com/documents/110/sc2le.pdf)\n2. [V. Mnih, A. Badia, M. Mirza1, A. Graves, T. Harley, T. Lillicrap, D. Silver, K. Kavukcuoglu. Asynchronous Methods for Deep Reinforcement Learning, 2016.](https://arxiv.org/pdf/1602.01783.pdf)\n3. [K. Frans, J. Ho, X. Chen, P. Abbeel, J. Schulman. Meta Learning Shared Hierarchies. arXiv preprint arXiv:1710.09767v2, 2017.](https://arxiv.org/pdf/1710.09767.pdf)<a name=\"MLSH\"></a>\n4. [Xiaowei Hu's PySC2 Agents](https://github.com/xhujoy/pysc2-agents) \n\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/roop-pal/Meta-Learning-for-StarCraft-II-Minigames",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-03-09T20:57:31Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-03T03:54:44Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Deep reinforcement learning has made significant strides in recent years, with results achieved in board games such as Go. However, there are a number of obstacles preventing such methods from being applied to more real-world situations. For instance, more realistic strategic situations often involve much larger spaces of possible states and actions, an environment state which is only partially observed, multiple agents to control, and a necessity for long-term strategies involving not hundreds but thousands or tens of thousands of steps. It has thus been suggested that creating learning algorithms which outperform humans in playing real-time strategy (RTS) video games would signal a more generalizable result about the ability of a computer to make decisions in the real world.\n\nOf the current RTS games on the market, StarCraft II is one of the most popular. The recent release by Google\u2019s DeepMind of SC2LE (StarCraft II Learning Environment) presents an interface with which to train deep reinforcement learners to compete in the game, both in smaller \u201cminigames\u201d and on full matches. The SC2LE environment is described on [DeepMind's github repo.](https://github.com/deepmind/pysc2) \n\nIn this project, we focus on solving a variety of minigames, which capture various aspects of the full StarCraft II game. These minigames focus on tasks such as gathering resources, moving to waypoints, finding enemies, or skirmishing with units. In each case the player is given a homogeneous set of units (marines), and a reward is based off the minigame (+5 for defeating each enemy roach in DefeatRoaches, for example).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9945008707396813,
        0.9686012840917947,
        0.9197579727832841,
        0.9628264872089326,
        0.9591982161824768,
        0.9973964720967647
      ],
      "excerpt": "For this project we implemented and tested deep reinforcement learning methods for five SC2LE minigames involving moving units to target locations as well as battles between groups of agents. This project was developed for the course COMS 4995 Deep Learning taught by Prof. Iddo Drori at Columbia University, in Spring 2018. This work is done by Connor Hargus, Jerome Kafrouni and Roop Pal who have contributed equally. \nWe started our project by partially reproducing the results obtained by DeepMind in their SC2LE publication, as shown by the table above. Then, we implemented a meta-learning strategy showing how an agent's skills can be transferred between minigames. \nA draft of our paper can be found here. \nWe first implemented and tested \"baseline\" agents that will let us evaluate more complex reinforcement learning agents. We compare our results with \"random\" agents that choose any random action at each step, and simple scripted agents that intend to solve the minigame with a simple deterministic policy. The scripted agents can be found in the folder scripted_agents. \nWe then implemented a \"smarter\" baseline agent using a Q-table. For this to be possible, we reduced the action space to a few basic actions (mainly selecting units and attacking points), and also reduced the state space (a 4 by 4 grid indicating where the roaches are along with the number of marines left). \nWe then made a review of the current architectures used to solve these minigames. In their paper, DeepMind use the A3C algorithm (Asynchronous Advantage Actor Critic) with several architectures (Atari-Net, FullyConv, FullyConv LSTM) that are described in section 4.3 of the SC2LE paper. DeepMind did not include open source implementations of the architectures used in their paper, yet a few research teams shared implementations, and our work relies on theirs. Useful github resources can be found in the readme of the docs folder of this repo. All agents based on different reinforcement learning ideas (MLSH, A3C) will be in the rl_agents folder. Our A3C agent is mainly based on the work of  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9828314819350257,
        0.9917608831270234,
        0.9207769664276019
      ],
      "excerpt": "The main contribution is an implementation of a MLSH (Meta-Learning Shared Hierarchies) agent, which can be trained on multiple minigames, sharing sub-policies. A master policy selects which sub-policy to use given observations. This allows the agent to generalize to previously unseen minigames by just training a master policy. A more detailed explanation of the algorithm can be found in the paper. \nWe trained our agents on 5 of the 7 minigames: MoveToBeacon, CollectMineralShards, FindAndDefeatZerglings, DefeatRoaches and DefeatZerglingsAndBanelings. We also tried a simpler approach: we wrote scripted bots to solve these games, and implemented a simple Q-Learning agent with simpler action and state spaces. We implemented our MLSH algorithm from scratch as an adaptation of the A3C using an AtariNet architecture produced by Xiaowei Hu. The results presented in the table above describe test results after 5 million game steps of training. \nThe videos below show (1) our A3C agent trained with Atarinet architecture, on 25,000 episodes, playing DefeatRoaches, (2) our simple Q-Learning agent trained on MoveToBeacon, and (3) our MLSH agent trained on 4 minigames, playing DefeatRoaches. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "We reproduced DeepMind's results and implement a meta-learning (MLSH) agent which can generalize across minigames.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/roop-pal/Meta-Learning-for-StarCraft-II-Minigames/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 6,
      "date": "Tue, 21 Dec 2021 01:17:31 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/roop-pal/Meta-Learning-for-StarCraft-II-Minigames/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "roop-pal/Meta-Learning-for-StarCraft-II-Minigames",
    "technique": "GitHub API"
  },
  "invocation": [
    {
      "confidence": [
        0.8246982354761949
      ],
      "excerpt": "         width=\"240\" height=\"180\" border=\"10\" /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8246982354761949
      ],
      "excerpt": "         width=\"240\" height=\"180\" border=\"10\" /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8246982354761949
      ],
      "excerpt": "         width=\"240\" height=\"180\" border=\"10\" /> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/roop-pal/Meta-Learning-for-StarCraft-II-Minigames/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Meta-Learning for StarCraft II Minigame Strategy",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Meta-Learning-for-StarCraft-II-Minigames",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "roop-pal",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/roop-pal/Meta-Learning-for-StarCraft-II-Minigames/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To run an agent, instead of calling pysc2 directly as in the instructions from DeepMind, run the main.py script of our project, with the agent class passed as a flag. For example, to run the q table agent or the MLSH agent:\n\n```\n$ python -m main --agent=rl_agents.qtable_agent.QTableAgent --map=DefeatRoaches\n$ python -m main --agent=rl_agents.mlsh_agent.MLSHAgent --num_subpol=3 --subpol_steps=5 --training\n```\n\nIf no agent is specified, the A3C agent is run by default:\n\n```\n$ python -m main --map=DefeatRoaches\n```\nA full list of the flags that can be used along with their descriptions is available in the main.py of script. The most important and useful flags are:\n\n- map: the map on which to run the agent. Should not be used with MLSHAgent which uses a list of maps to use, since MLSH trains on multiple maps.\n- max_agent_steps: the number of steps to perform per episode (after which, episode is stopped). This is used to speed up training by focusing on early states of episodes\n- parallel: number of threads to run, defaults at 1.\n\nFlags specific to the MLSHAgent:\n\n- num_subpol: number of subpolicies to train and use\n- subpol_steps: periodicity of subpolicy choices done by the master policy (in game steps)\n- warmup_len: number of episodes during which only the master subpolicy is trained\n- join_len: number of episodes during which both master and subpolicies are trained\n\n\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 23,
      "date": "Tue, 21 Dec 2021 01:17:31 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "sc2le",
      "deep-reinforcement-learning",
      "reinforcement-learning",
      "starcraft2-ai",
      "starcraft2",
      "minigames",
      "multi-agent-learning"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To get started, follow the instructions on the [pysc2 repository](https://github.com/deepmind/pysc2). As described in their instructions, make sure that the environment is set up correctly by running:\n\n```\n$ python -m pysc2.bin.agent --map Simple64\n```\n\nOur project relies on a few more packages, that can be installed by running:\n\n```\n$ pip install -r requirements.txt\n```\n\nWe have tested our project using python 3 and pysc2 version 1.2, which is the main version currently available.\n\nWe are currently training our agents on a google cloud instance with a 4 core CPU and two Tesla K80 GPUs. This configuration might evolve during the project.\n\n",
      "technique": "Header extraction"
    }
  ]
}