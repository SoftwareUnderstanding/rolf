{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1409.3215>`_ - Original Seq2Seq Paper\n* `Neural Machine Translation by Jointly Learning to Align and Translate <https://arxiv.org/abs/1409.0473>`_ - Sequence to Sequence with Attention\n* `Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation <https://arxiv.org/abs/1406.1078>`_\n\n\n***************\nReferences\n***************\n.. [jurafsky2000speech] Jurafsky, D., 2000. Speech and language processing: An introduction to natural language processing. Computational linguistics, and speech recognition.\n.. [goldberg2017neural] Goldberg, Yoav. \"Neural network methods for natural language processing.\" Synthesis Lectures on Human Language Technologies 10.1 (2017",
      "https://arxiv.org/abs/1409.0473>`_ - Sequence to Sequence with Attention\n* `Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation <https://arxiv.org/abs/1406.1078>`_\n\n\n***************\nReferences\n***************\n.. [jurafsky2000speech] Jurafsky, D., 2000. Speech and language processing: An introduction to natural language processing. Computational linguistics, and speech recognition.\n.. [goldberg2017neural] Goldberg, Yoav. \"Neural network methods for natural language processing.\" Synthesis Lectures on Human Language Technologies 10.1 (2017",
      "https://arxiv.org/abs/1406.1078>`_\n\n\n***************\nReferences\n***************\n.. [jurafsky2000speech] Jurafsky, D., 2000. Speech and language processing: An introduction to natural language processing. Computational linguistics, and speech recognition.\n.. [goldberg2017neural] Goldberg, Yoav. \"Neural network methods for natural language processing.\" Synthesis Lectures on Human Language Technologies 10.1 (2017",
      "https://arxiv.org/abs/1409.0473."
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9206848443805595
      ],
      "excerpt": "[Understanding LSTM Netwroks &lt;http://colah.github.io/posts/2015-08-Understanding-LSTMs/&gt;_]. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8043073075947367
      ],
      "excerpt": "if self.bidirectional: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8043073075947367
      ],
      "excerpt": "    if self.connection_possibility_status: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8144481826143558
      ],
      "excerpt": "      :param transform: Post processing if necessary. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "      if auto_encoder: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9252605693360756
      ],
      "excerpt": "Sequence to Sequence Learning with Neural Networks &lt;https://arxiv.org/abs/1409.3215&gt;_ - Original Seq2Seq Paper \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9978694782138345
      ],
      "excerpt": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation &lt;https://arxiv.org/abs/1406.1078&gt;_ \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/astorfi/sequence-to-sequence-from-scratch",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-09-30T02:56:40Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-20T05:18:13Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8315275704403727,
        0.9722104783308271,
        0.8139820047380468,
        0.9764940610810512,
        0.9121296574947011,
        0.8609046402899991
      ],
      "excerpt": "In this project we explain the sequence to sequence modeling using [Pytorch &lt;https://pytorch.org/&gt;_]. \nHere, we tried to achieve some primary goals as we hope to make this work unique compared to the many other available tutorials: \nWe called this repo \"from scratch\" due to the fact that we do NOT consider \n  any background for the reader in terms of implementation. \nInstead of using high-level package modules, \n  simple RNN architectures are used for demonstration purposes. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8465437608407567
      ],
      "excerpt": "  The downside, however, is the relatively low speed of training. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9399356561569124,
        0.8847511516492386
      ],
      "excerpt": "to start from scratch and learn everything spoon-by-spoon. The goal is to \ngive as much detail as possible so the others do NOT have to spend the time to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9853883699876146
      ],
      "excerpt": "The goal here is to create a sequence-to-sequence mapping model which is going to be built on an \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9739924546634948
      ],
      "excerpt": "for the model to understand it. Clearly, it should be a sequence of words in the input and the equivalent \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9438739371792675,
        0.8898466238937633,
        0.9111882577582923,
        0.9232108258202323,
        0.9892615791184091,
        0.9210106207479617
      ],
      "excerpt": "A learned representation for context elements is called word embedding in which the words with similar meaning, ideally, \nbecome highly correlated in the representation space as well. One of the main incentives behind word embedding representations \nis the high generalization power as opposed to sparse higher dimensional representation [goldberg2017neural]. Unlike the traditional \nbag-of-word representation in which different words have quite different representation regardless of their usage, \nin learning the distributed representation, the usage of words in the context is of great importance which lead to \nsimilar representation for correlated words in meaning. The are different approaches for creating word embedding. Please \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9589040312974123,
        0.852377929196601
      ],
      "excerpt": "for more details. \nThe encoder generates a single output vector that embodies the input sequence meaning. The general procedure is as follows: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8808623752246936,
        0.830054438885179,
        0.9426813723402677
      ],
      "excerpt": "2. For the next step, the hidden step and the next word will be fed to the same network (W) for updating the weights. \n3. In the end, the last output will be the representative of the input sentence (called the \"context vector\"). \nThe EncoderRNN attribute is dedicated to the encoder structure. The Encoder in our code, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9260731030109943
      ],
      "excerpt": "will be fed with the input sequence in the reverse time order. The outputs of the two \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8141671593148034,
        0.9171150422823885
      ],
      "excerpt": "and returned). The created feature vector will represents the initial hidden states of the decoder. The \narchitecture of a bi-lstm is as below: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.842154188208771
      ],
      "excerpt": "   :alt: map to buried treasure \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8857404000033772,
        0.9312532557229404,
        0.9610343904469876,
        0.9254763018252364
      ],
      "excerpt": "     * For nn.LSTM, same input_size & hidden_size is chosen. \n     :param input_size: The size of the input vocabulary \n     :param hidden_size: The hidden size of the RNN. \n     :param batch_size: The batch_size for mini-batch optimization. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9876140330948138,
        0.9719987496970947
      ],
      "excerpt": "the LSTM cells to represent what exactly is going on in the encoding/decoding phases! \nThe initialization of the LSTM is a little bit different compared to the LSTM \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9467170263437739,
        0.978027802044922
      ],
      "excerpt": "As it can be seen in the above code, for the Bidirectional LSTM, we have separate and independent \nstates for forwards and backward directions. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9183017086943762,
        0.9260265335071566
      ],
      "excerpt": "be used as the initial hidden state of the decoder. Decoding is as follows: \n1. At each step, an input token and a hidden state is fed to the decoder. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8882665697766319,
        0.8184948354309182,
        0.9315513190693757,
        0.882053156390854
      ],
      "excerpt": "After the first decoder step, for the following steps, the input is going to be the previous word prediction of the RNN. \nSo the output generation will be upon the network sequence prediction. In case of using teacher_forcing, the input is going to be the actual \ntargeted output word. It provides better guidance for the training but it is inconsistent with the evaluation stage as \ntargeted outputs do not exists! In order to handle the issue with this approach, new approaches have been proposed [lamb2016professor]_. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9442413143446493
      ],
      "excerpt": "    The spesific type of the hidden layer for the RNN type that is used (LSTM). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8483545200513991,
        0.8497096240009787,
        0.911048909126989
      ],
      "excerpt": "The context vector, generated by the encoder, will be used as the initial hidden state of the decoder. \nIn case that their dimension is not matched, a linear layer should be employed to transformed the context vector \nto a suitable input (shape-wise) for the decoder cell state (including the memory(Cn) and hidden(hn) states). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9060735113703133,
        0.8487479445277892
      ],
      "excerpt": "1. The hidden sizes of encoder and decoder are the same BUT we have a bidirectional LSTM as the Encoder. \n2. The hidden sizes of encoder and decoder are NOT same. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9773982480956349
      ],
      "excerpt": "At the first state we have to define word indexing for further processing. The word2index is the dictionary of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "    for word in sentence.split(' '): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8849287813843965,
        0.8593442505954433
      ],
      "excerpt": "the indexing from 1 by SOS_token = 1 to have the zero reserved! \nIn the end, we define a dataset class to handle the processing: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9426318689958203
      ],
      "excerpt": "      :param max_input_length: The maximum enforced length of the sentences. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9145281469021935
      ],
      "excerpt": "      # Skip and eliminate the sentences with a length larger than max_input_length! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9424275716816649,
        0.8954751506663121
      ],
      "excerpt": "The training/evaluation of this model is done in a not very optimized way deliberately!! The reasons are as follows: \nI followed the principle of running with one click that I personnal have for all my open source projects. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9823896345565541,
        0.9915203439688887,
        0.9004876248465599,
        0.9795790124741467,
        0.9732638801308657
      ],
      "excerpt": "Instead of using ready-to-use RNN objects which process mini-batches of data, we input the sequence word-by-word to help \n  the readers having a better sense of what is happening behind the doors of seq-to-seq modeling scheme. \nFor the evaluation, we simply generate the outputs of \n  the system based on the built model to see if the model is good enough! \nFor mini-batch optimization, we input batches of sequences. There is a very important note for the batch feeding. After \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8526510724664901
      ],
      "excerpt": "      # the new input sequence as a continuation of the previous sequence. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8989374734999913
      ],
      "excerpt": "Input:  she is more wise than clever  EOS \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9516860713378575
      ],
      "excerpt": ".. [jurafsky2000speech] Jurafsky, D., 2000. Speech and language processing: An introduction to natural language processing. Computational linguistics, and speech recognition. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": ":speech_balloon: Sequence to Sequence from Scratch Using Pytorch",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/astorfi/neural-machine-translation-from-scratch/releases",
    "technique": "GitHub API"
  },
  "faq": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Machine Translation(MT) is one of the areas of NLP that has been profoundly affected by advances in deep learning.\nIn fact, progress in MT can be categorized into pre-deep learning and deep learning era. Confirmation of this could\nbe some of the reference books in NLP community such as \u201dSpeech and Language Processing\u201d [jurafsky2000speech]_. Second version of\nthis book was published in 2008 and chapter 25 is dedicated to machine translation but there is not a single mention of\ndeep learning usage for MT. However, today we know that the top performing machine translation systems are solely\nbased on neural networks which led to the term Neural Machine Translation (NMT).\n\nWhen we use the term neural machine translation, we are talking about applying different deep learning tech-\nniques for the task of machine translation. It was after success of neural network in image classification tasks\nthat researchers started to use neural networks in machine translation. Around 2013 research groups started to achieve\nbreakthrough results in NMT and boosted state of the art performance. Unlike traditional statistical machine transla-\ntion, NMT is based on an end-to-end neural network that increases the performance of machine translation systems\n[bahdanau2014neural]_.\n\nWe dedicate this project to a core deep learning based model for sequence-to-sequence modeling and in particular machine translation: An Encoder-Decoder architecture\nbased on Long-Short Term Memory (LSTM) networks.\n\n------------------------------------------------------------\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Although sequence to sequence modeling scope is broader than just the machine translation task,\nthe main focus on seq-2-seq research has been dedicated to MT due to its great importance in real-world\nproblems. Furthermore, machine translation is the bridge for a universal human-machine conversation.\n\n------------------------------------------------------------\n",
      "technique": "Header extraction"
    }
  ],
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 18,
      "date": "Tue, 21 Dec 2021 09:03:38 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/astorfi/sequence-to-sequence-from-scratch/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "astorfi/sequence-to-sequence-from-scratch",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/astorfi/neural-machine-translation-from-scratch/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8263211337530757
      ],
      "excerpt": "NOTE: We do NOT generate the whole LSTM/Bi-LSTM architecture using Pytorch. Instead, we just use \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8418426356390782
      ],
      "excerpt": "        self.name = name \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8822244972688988
      ],
      "excerpt": "  The principle says: \"Everyone must be able to run everything by one click!\". So you see pretty much everything in one \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8613165950393644
      ],
      "excerpt": "the previously processed sequence. It can be seen in the following Python script: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8709017446873079
      ],
      "excerpt": "Output:  i m glad i invited you  EOS \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8366327080755738
      ],
      "excerpt": "2. The first output, should be the first word of the output sequence and so on. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8158687721849047,
        0.8113404775112109
      ],
      "excerpt": "    output = self.out(output[0]) \n    return output, (h_n, c_n) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8190340842636501
      ],
      "excerpt": "The dataset is prepaired using the data_loader.py script. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.812998327792582
      ],
      "excerpt": "        self.name = name \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8747152940615591
      ],
      "excerpt": "      :param phase: train/test. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984,
        0.8967380512134904
      ],
      "excerpt": "      input_lang, output_lang, pairs = prepareData(lang_in, lang_out, max_input_length, auto_encoder=auto_encoder, reverse=True) \n      print(random.choice(pairs)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8089117413629329
      ],
      "excerpt": "          selected_pairs = pairs[0:int(0.8 * len(pairs))] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8089117413629329
      ],
      "excerpt": "          selected_pairs = pairs[int(0.8 * len(pairs)):] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8719103181000751
      ],
      "excerpt": "  Python file! \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/astorfi/sequence-to-sequence-from-scratch/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "##################\nTable of Contents\n##################\n.. contents::\n  :local:\n  :depth: 4\n\n***************\nDocumentation\n***************\n.. image:: https://badges.frapsoft.com/os/v2/open-source.png?v=103\n    :target: https://github.com/ellerbrock/open-source-badge/\n.. image:: https://img.shields.io/twitter/follow/amirsinatorfi.svg?label=Follow&style=social\n      :target: https://twitter.com/amirsinatorfi",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "sequence-to-sequence-from-scratch",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "astorfi",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/astorfi/sequence-to-sequence-from-scratch/blob/master/README.rst",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 105,
      "date": "Tue, 21 Dec 2021 09:03:38 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "pytorch",
      "python",
      "machine-learning",
      "deep-learning"
    ],
    "technique": "GitHub API"
  }
}