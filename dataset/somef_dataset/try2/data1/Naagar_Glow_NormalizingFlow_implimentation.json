{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1904.04676",
      "https://arxiv.org/abs/1807.03039",
      "https://arxiv.org/abs/1807.03039\n\nImplementation of Glow on CelebA and MNIST datasets.\n\n#### Results\nI trained two models:\n- Model A with 3 levels, 32 depth, 512 width (~74M parameters"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* Official implementation in Tensorflow: https://github.com/openai/glow\n\n\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Naagar/Glow_NormalizingFlow_Implementation",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-12-27T11:36:05Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-05-28T09:47:20Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8134142558978211
      ],
      "excerpt": "Reimplementations of density estimation algorithms from: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9863163992720678
      ],
      "excerpt": "Implementation of Glow on CelebA and MNIST datasets. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "| Model A | Model B | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "| Model A | Model B | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "pyTorch implimentation of the Glow paper and Reimplementations of density estimation algorithms",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Naagar/Glow_NormalizingFlow_implimentation/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sun, 26 Dec 2021 02:19:30 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Naagar/Glow_NormalizingFlow_Implementation/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Naagar/Glow_NormalizingFlow_Implementation",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Naagar/Glow_NormalizingFlow_implimentation/main/batch_gm.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9382134360300461
      ],
      "excerpt": "To download CelebA follow the instructions here. A nice script that simplifies downloading and extracting can be found here: https://github.com/nperraud/download-celebA-HQ/ \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8860000327613594,
        0.8837400581832529
      ],
      "excerpt": "- Model A with 3 levels, 32 depth, 512 width (~74M parameters). Trained on 5 bit images, batch size of 16 per GPU over 100K iterations. \n- Model B with 3 levels, 24 depth, 256 width (~22M parameters). Trained on 4 bit images, batch size of 32 per GPU over 100K iterations. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8013669323213567
      ],
      "excerpt": "Embedding vectors were calculated for the first 30K training images and positive / negative attributes were averaged then subtracting. The resulting dz was ranged and applied on a test set image (middle image represents the unchanged / actual data point). \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Naagar/Glow_NormalizingFlow_Implementation/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Normalizing flows",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Glow_NormalizingFlow_Implementation",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Naagar",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Naagar/Glow_NormalizingFlow_Implementation/blob/main/readme.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* python 3.6\n* pytorch 1.0\n* numpy\n* matplotlib\n* tensorboardX\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sun, 26 Dec 2021 02:19:30 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To train a model using pytorch distributed package:\n```\npython -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE \\\n       glow.py --train \\\n               --distributed \\\n               --dataset=celeba \\\n               --data_dir=[path to data source] \\\n               --n_levels=3 \\\n               --depth=32 \\\n               --width=512 \\\n               --batch_size=16 [this is per GPU]\n```\nFor larger models or image sizes add `--checkpoint_grads` to checkpoint gradients using pytorch's library. I trained a 3 layer / 32 depth / 512 width model with batch size of 16 without gradient checkpointing and a 4 layer / 48 depth / 512 width model with batch size of 16 which had ~190M params so required gradient checkpointing (and was painfully slow on 8 GPUs).\n\n\nTo evaluate model:\n```\npython glow.py --evaluate \\\n               --restore_file=[path to .pt checkpoint] \\\n               --dataset=celeba \\\n               --data_dir=[path to data source] \\\n               --[options of the saved model: n_levels, depth, width, batch_size]\n```\n\nTo generate samples from a trained model:\n```\npython glow.py --generate \\\n               --restore_file=[path to .pt checkpoint] \\\n               --dataset=celeba \\\n               --data_dir=[path to data source] \\\n               --[options of the saved model: n_levels, depth, width, batch_size] \\\n               --z_std=[temperature parameter; if blank, generates range]\n```\n\nTo visualize manipulations on specific image given a trained model:\n```\npython glow.py --visualize \\\n               --restore_file=[path to .pt checkpoint] \\\n               --dataset=celeba \\\n               --data_dir=[path to data source] \\\n               --[options of the saved model: n_levels, depth, width, batch_size] \\\n               --z_std=[temperature parameter; if blank, uses default] \\\n               --vis_attrs=[list of indices of attribute to be manipulated, if blank, manipulates every attribute] \\\n               --vis_alphas=[list of values by which `dz` should be multiplied, defaults [-2,2]] \\\n               --vis_img=[path to image to manipulate (note: size needs to match dataset); if blank uses example from test dataset]\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}