{
  "citation": [
    {
      "confidence": [
        0.8903858689638849
      ],
      "excerpt": "+-- preprocess_data.py (preprocessing of CIFAR-10/100 and STL-10) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/DD2412-Final-Projects/swag-reproduced",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-10-11T09:00:19Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-07-24T15:29:03Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9936956672628973,
        0.9555110743174914
      ],
      "excerpt": "This repository contains the code used in our attempt at the NeurIPS 2019 Reproducibility Challenge, in reproducing the methods proposed in the paper \"A Simple Baseline for Bayesian Uncertainty in Deep Learning\" by W. Maddox et al. This project also served as the final project of the course DD2412 at KTH Royal Institute of Technology. \nThe code is implemented in TensorFlow 1.14 for Python3. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.895889835582498
      ],
      "excerpt": "    |   +-- vgg16.py (the vgg16 implementation) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8675822423246797,
        0.8333989023086074,
        0.8751747652656585
      ],
      "excerpt": "The reproduced paper - \"A Simple Baseline for Bayesian Uncertainty in Deep Learning\"    \nA vital paper referenced by the authors - \"Averaging Weights Leads to Wider Optima and Better Generalization\" \nTensorFlow implementation of VGG-16: https://www.cs.toronto.edu/~frossard/post/vgg16/.  \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/NajibYavari/DD2412/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Fri, 24 Dec 2021 17:59:07 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/DD2412-Final-Projects/swag-reproduced/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "DD2412-Final-Projects/swag-reproduced",
    "technique": "GitHub API"
  },
  "invocation": [
    {
      "confidence": [
        0.877350667834488,
        0.8615811834034244,
        0.9586232994076559
      ],
      "excerpt": "+-- test.py (loads learned weights, runs the a standard test procedure and reports resulting metrics and plot data) \n+-- test_swag,py (loads learned SWAG parameters, runs the SWAG test procedure and reports resulting metrics and plot data) \n+-- utils.py (utility functions) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9446597289932327
      ],
      "excerpt": "    |   +-- reliability_diagram.py (takes plot data output by test.py or test_swag.py and produces reliability diagrams) \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/DD2412-Final-Projects/swag-reproduced/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Reproducibility Project - \"A Simple Baseline for Bayesian Uncertainty in Deep Learning\"",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "swag-reproduced",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "DD2412-Final-Projects",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/DD2412-Final-Projects/swag-reproduced/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Fri, 24 Dec 2021 17:59:07 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To install all necessary dependencies for the implementation, run the command\n```\npip install -r requirements.txt\n```\nTo preprocess a dataset to the required format, use \u00b4preprocess_data.py\u00b4. Example:   \n```\npython preprocess_data.py --data_path data/cifar-10-raw/ --train_frac 0.9 --valid_frac 0.1 --save_path data/cifar-10/ --data_set cifar10\n```\nTo train a VGG-16 model using regular SGD, use `train.py`. Example:   \n```\npython train.py --data_path data/cifar-10/ --save_weight_path weights/ --save_checkpoint_path checkpoints/ --save_plots_path plots/\n```\nTo train a VGG-16 model using SWA/SWAG-dIag/SWAG, use \u00b4train_swag.py\u00b4. Example:   \n```\npython train_swag.py --data_path data/cifar-10/ --save_param_path weights/ --save_checkpoint_path checkpoints/ --save_plots_path plots/\n```\nTo test a model trained with SGD, use `test.py`. Example:   \n```\npython test.py --data_path data/cifar-10/ --load_weight_file weights/sgd_weights.npz\n```\nTo test a model trained with SWA/SWAG-Diag/SWAG, use `test_swag.py`. Example:   \n```\npython test_swag.py --data_path data/cifar-10/ --load_patam_file weights/swag_params.npz --mode swag\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}