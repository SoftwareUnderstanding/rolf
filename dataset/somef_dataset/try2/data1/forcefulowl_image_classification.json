{
  "citation": [
    {
      "confidence": [
        0.8275296219550469
      ],
      "excerpt": "    im = Image.open(img_name).convert('RGB') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "    if fn.endswith('.jpg'): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "    rotation_range=30, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9156566588472104
      ],
      "excerpt": "| Conv2D/s=1 | 1 * 1 * 32 * 64 | 112 * 112 * 32 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8444342525991423
      ],
      "excerpt": "| batch_size=32, lr=1, epochs=100 | 2:02:35.984546 | loss: 0.0496 - acc: 0.9844 - val_loss: 0.9146 - val_acc: 0.8437 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9846982194102245
      ],
      "excerpt": "Deep Residual Learning for Image Recognition \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9559715772848645
      ],
      "excerpt": "| inverted_residual | 2.2M | 77.28% | 2:32:11 | 00:03:35/0.142s | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "| inverted_residual | 2.2M | 77.35% | 2:32:25 | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/forcefulowl/image_classification",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-03-31T03:01:24Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-04-25T20:12:41Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Convolutional neural networks ahve become famous in cmoputer vision ever since *AlexNet* popularized deep convolutional neural networks by winning IamgeNet Challenge: ILSVRC 2012. The general trend has been to make deeper and more complicated networks in roder to achieve higher accuracy. However, these advances to improve accuracy are not necessarily making networks more efficient with respect to size and spped. In many real world applications such as robotics, self-driving, the recognition tasks need to be carried out in a timely fashion on a computationally limited platform. That's inspire me to build some effient convolutional neural networks which can be used on Mobile/portable device. \n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9570265957609417
      ],
      "excerpt": "The format of raw data is as follows: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9524857976817878
      ],
      "excerpt": "And the format of label is as follows: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.957662971030376
      ],
      "excerpt": "Directly loading all of the data into memory. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "    return data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9383311475122647
      ],
      "excerpt": "for fn in os.listdir('C:\\Users\\gavin\\Desktop\\ISIC2018_Task3_Training_Input'): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9947094600123172,
        0.9085437079700814
      ],
      "excerpt": "That is so memory consuming, even the most state-of-the art configuration won't have enough memory space to process the data the way I used to do it. Meanwhile, the number of training data is not large enough, Data Augumentation is the next step to achieve. \nFirstly, chaning the format of the raw data using reformat_data.py. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8614537859849672,
        0.9509676263232164,
        0.9917100360086288,
        0.8738455095815225,
        0.8757599429137632,
        0.9786790143489522,
        0.9216899014059043
      ],
      "excerpt": "Depthwise Separable Convolution is a form of factorized convolutions which factorize a standard convolution into a depthwise convolution and a  convolution called a pointwise convolution. The depthwise convolution applies a single filter to each input channel, the pointwise convolution then applies a  convolution to combine the outputs the depthwise convolution. A standard convolution both filters and combines inputs into a new set of outputs in one step. The depthwise separable convolution splits this into two layers, a separate layer for filtering and a separate layer for combining. This factorization has the effect of drastically reducing computation and model size.  \nA standard convolutional layer takes as input a  feature map F and produces a  feature map G where  is a spatial width and height of a square input feature map, M is the number of input channels,  is the spatial width and height of a square output feature map and N is the number of output channel. \nThe standard convolutional layer is parameterized by convolution kernel K of size  where  is the spatial dimension of the kernel assumed to be square and M is number of input channels and N is the number of output channels as defined previously.  \nStandard convolutions have the computational cost of: \nwhere the computational cost depends multiplicatively on the number of input channels M, the number of output channels N, the kernel size  and the feature map size . \nThe standard convolution operation has the effect of filtering features based on the convolutional kernels and combining features in order to produce a new representation. The filtering and combination steps can be split into two steps via the use of factorized convolutions called depthwise separable convolutions for substantial reduction in computational cost. \nDepthwise separable convolution are made up of two layers: depthwise convolutions and pointwise convolutions. Using depthwise convolutions to apply a single filter per input channel (input depth). Pointwise convolution, a simple  convolution, is then used to create a linear combination of the output of the depthwise layer. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9742538526611836,
        0.8574045593892323
      ],
      "excerpt": "Depthwise convolution is extremely efficient relative to standard convolution. However it only filters input channels, it does not combine them to create new features. So an additional layer that computes a linear combination of the output of depthwise convolution via  convolution is needed in order to generate these new features. \nThe combination of depthwise convolution and  (pointwise) convolution is called depthwise separable convolution which was originally introduced in. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9983870066101618,
        0.9681181942154262,
        0.9734802410540832,
        0.9742567039708868,
        0.8785468640948145,
        0.9675958547732146
      ],
      "excerpt": "which is the sum of the depthwise and  pointwise convolutions. \nBy expressing convolution as two step process of filtering and combining, there's a reduction in computation of \nAlthough using Depthwise Separable Convolution make the model already small and low latency, many times a specific use case or application may require the model to be smaller and faster. In order to construct these smaller and less computationally expensive models, I also set two hyper-parameters: width multiplier and resolution multiplier. The role of the width multiplier  is to thin a network uniformly at each layer. For a give layer and width multiplier , the number of input channels M becomes M and the number of output channels N becomes N. \nThe computational cost of a depthwise separable convolution with width multiplier  is: \nwhere  witth typical settings of 1, 0.75, 0.5 and 0.25. Width multiplier has the effect of reducing computational cost and the number of parameters quadratically by roughly .  \nThe structure of the model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9944768649477298,
        0.9890999191705508
      ],
      "excerpt": "Deep convolutional neural networks have led to a series of breakthroughs for image classification. Deep networks naturally integrate low/mid/high-level features and classifiers in an end-to-end multilayer fashion, and the 'levels' of features can be enriched by the number of stacked layers(depth). Evidence reveals that network depth is of crucial importance. \nDriven by the significance of depth, a question arises: Is learning better networks as easy as stacking more layers?  An obstacle to answering this question was the notorious problem of vanishing/exploding gradients, which hamper convergence from the beginning. When deeper networks are able to start converging, a degradation problem has been exposed: with the network depth increasing ,accuracy gets saturated ( which might be unsurprising) and then degrades rapidly. Unexpectedly, such degradation is not caused by overfitting, and adding more layers to a suitably deep model leads to higher training error. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9578322041173596
      ],
      "excerpt": "Formally, denoting the desired underlying mapping as , it let the stacked nonlinear layers fit another mapping of . The original mapping is recast into . The formulation of  can be realized by feedforward neural networks with 'shortcut connections' are those skipping one or more layers, the shortcut connections simply perform identity mapping, and their outputs are added to the outputs of the stacked layers. Identity shortcut connections add neither extra parameter nor computational complexity. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8103050422810089
      ],
      "excerpt": "For each residual function , using a stack of 3 layers instead of 2. The three layers are , , and  convolutions, where the  layers are responsible for reducing and then increasing(restoring) dimensions, leaving the  layer a bottleneck with smaller input\\output dimensions. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9270860001424026,
        0.8247087716404848
      ],
      "excerpt": "Comparing of Depthwise Separable Convolution and Linear Bottleneck. \n<img src='/img/comparing of mobilenet v1_v2.png'> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9588650792106729
      ],
      "excerpt": "ReLU is capable of preserving complete information about the input manifold, but only if the input manifold lies in a low-dimensional subspace of the input space. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9819983416254678
      ],
      "excerpt": "Examples of ReLU transformations of low-dimensional manifolds embedded in higher-dimensional spaces. In these examples the initial spiral is embedded into an n-dimensional space using random matrix T followed by ReLU, and then projected back to the 2D space using $T^{-1}$. In examples above n = 2,3 result in information loss where certain points of the manifold collapse into each other, while for n=15 to 30 the transformation is highly non-convex. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.922412673687793
      ],
      "excerpt": "The inverted design is considerably more memory efficient. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9977245508412412,
        0.8247087716404848
      ],
      "excerpt": "Comparing of bottleneck and inverted residuals. \n<img src='/img/comparing of bottleneck.png'> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8892535323673445
      ],
      "excerpt": "Bottleneck residual block transforming from k to k' channels, with stride = s and expansion factor t. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9607387963257055
      ],
      "excerpt": "Structure of the model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9713301356952433,
        0.9734797160731634,
        0.9006273206829105
      ],
      "excerpt": "Modern convolutional neural networks usually consist of repeated building blocks with the same structure, such as Xception and ResNeXt introduce efficient depthwise separable convolutions or group convolutions into the building blocks to strike an excellent trade-off between representation capability and computational cost. However, both designs do not fully take the  convolutions into account, which require considerable complexity. For example, in ResNeXt only  layers are equipped with group convolutions. As a result, for each residual unit in ResNeXt the pointwise convolutions occupy 93.4% multiplication-adds( cardinality = 32 as suggested in). In tiny networks, expensive pointwise convolutions result in limited number of channels to meet the complexity constraint, which might significantly damage the accuracy. \nTo address the issue, a straightforward solution is to apply channel sparse connections, for example group convolutions, also on  layers.By ensuring that each convolution operates only on the corresponding input channel group, group convolution significantly reduces computation cost. However, if multiple group convolutions stack together, there is one side effect: outputs from a certain channel are only derived from a small fraction of input channels. It is clear that outputs from a certain group only relate to the inputs within the group. This property blocks information flow between channel groups and weakens representation \nIf we allow group convolution to obtain input data from different groups , the input and output channels will be fully related. Specifically, for the feature map generated from the previous group layer, we can first divide the channels in each group into several subgroups, then feed each group in the next layer with different subgroups.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9261020539736146
      ],
      "excerpt": "Starting from the design principle of bottleneck unit. In its residual brach, for the  layers, applying a computational economical  depthwise convolution on the bottleneck feature map. Then, replacing the first  layer with pointwise group convolution followed by a channel shuffle operation. The purpose of the second pointwise group convolution is to recover the channel dimension to match the shortcut path. For simplicity, no apply an extra channel shuffle operation after the second pointwise layer as it results in comparable scores. The usage of batch normalization(BN) and nonlinearity is similar, except not use ReLU after depthwise convolution as suggested by Xception.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9910516134918242
      ],
      "excerpt": "Sturcture of Model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9002318914490006
      ],
      "excerpt": "Validation experiment for Guideline 1. Four different ratios of number of input/output channels(c1 and c2) are tested, while the total FLOPs under the four ratios is fixed by carying the number of channels. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9351050083912421
      ],
      "excerpt": "3. Network fragmentation reduces degree of parallelism \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9194902904889667
      ],
      "excerpt": "Validation experiment for Guideline 3. c denotes the number of channels for 1-fragment. The channel number in other fragmented structures is adjusted so taht the FLOPs is the same as 1-fragment. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.908925214220865
      ],
      "excerpt": "Conclusion and Discussions \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9808729689927425,
        0.9109996641714411
      ],
      "excerpt": "be aware of the cost of using group convolution; \nreduce the degree of fragmentation; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9850233035207455,
        0.9241634590042133,
        0.9449004451982802
      ],
      "excerpt": "At the begining of each unit, the input of c feature channels are split into two braches with c - c' and c' channels, respectively. Following G3, one branch remains as identity. The other branch consist of three convolutions with the same input and output channels to satisfy G1. The two  convolutions are no longer group-wise. This is partially to follow G2, and partially because the split operation already produces two groups. \nAfter convolution ,the two branches are concatenated. So, the number of channels keeps the same(G1). The same 'channel shuffle' operation is then used to enable information communication between the two branches. After the shuffling, the next unit begins. Note that the 'Add' operation no longer exists. Element-wise operations like ReLU and depthwise convolutions exist only in one branch. Also, the three successive elementwise operations, 'Concat', 'Channel Shuffle' and 'Channel Split', are merged into a single element-wise operation. These changes are beneficial according to G4. \nAll models with \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9449004451982802
      ],
      "excerpt": "All models with \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/forcefulowl/image_classification/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 23 Dec 2021 07:30:04 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/forcefulowl/image_classification/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "forcefulowl/image_classification",
    "technique": "GitHub API"
  },
  "invocation": [
    {
      "confidence": [
        0.8277234265367381
      ],
      "excerpt": "The input data are dermoscopic lesion images in JPEG format. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8314823419435533
      ],
      "excerpt": "<img src='/img/raw_data.png'> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8314823419435533
      ],
      "excerpt": "<img src='/img/raw_label.png'> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9389031781205087
      ],
      "excerpt": "    data = np.array(im) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8626865865857928,
        0.8314823419435533
      ],
      "excerpt": "Firstly, chaning the format of the raw data using reformat_data.py. \n<img src='/img/new_data.png'> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "    horizontal_flip=True, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8224608874658778,
        0.8398336512870205
      ],
      "excerpt": "width_shift_range: fraction of total width. \nheight_shift_range: fraction of total height. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8064576403969749
      ],
      "excerpt": "validation_split: Split the dataset into 70% train and 30% val. It will shuffle the data. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8174540907975313,
        0.8594142235991984
      ],
      "excerpt": "    subset='training', \n    shuffle=True) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "    shuffle=True) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8067604896880841
      ],
      "excerpt": "<img src = '/img/depthwise separable convolution.png'> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.841302263682951
      ],
      "excerpt": "| Type/Strike |  Filter shape | Input Size | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8228632974204363
      ],
      "excerpt": "|  values of parameters | training time | result | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8521489417489208
      ],
      "excerpt": "| batch_size=32, lr=0.01, epochs=100 | 2:00:44.432700 | loss: 0.1524 - acc: 0.9481 - val_loss: 0.5204 - val_acc: 0.8420 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8318192242599911
      ],
      "excerpt": "| batch_size=16, lr=0.01, epochs=100 | 2:13:38.293478 | loss: 0.1412 - acc: 0.9524 - val_loss: 0.5066 - val_acc: 0.8372 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8518018224731596
      ],
      "excerpt": "| batch_size=32, lr=0.1, epochs=100, init_w=xaveir | 2:01:36.719090 | loss: 0.3441 - acc: 0.8718 - val_loss: 0.7564 - val_acc: 0.7605 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8314823419435533
      ],
      "excerpt": "<img src='/img/gradient_vanishing.png'> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8228632974204363
      ],
      "excerpt": "|  values of parameters | training time | result | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8133077797546561,
        0.8239088818313542
      ],
      "excerpt": "| batch_size=8, lr=1, epochs=100 | 2:47:20.486248 |loss: 0.3449 - acc: 0.8718 - val_loss: 1.1156 - val_acc: 0.7741| \n| batch_size=8, lr=0.01, epochs=200 | 5:41:32.741626 |loss: 0.0190 - acc: 0.9937 - val_loss: 1.1468 - val_acc: 0.8460| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8315090463735514
      ],
      "excerpt": "<img src='/img/bottleneck.png'> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8187085532064932
      ],
      "excerpt": "<img src='/img/comparing of mobilenet v1_v2.png'> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8058400379608219
      ],
      "excerpt": "<img src='/img/inverted block.png'> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8200627565967624
      ],
      "excerpt": "<img src='/img/comparing of bottleneck.png'> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8471632383263955
      ],
      "excerpt": "|  Input | Operator | Output | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8471632383263955
      ],
      "excerpt": "| Input | Operator | expansion | output channels | t | s | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8228632974204363
      ],
      "excerpt": "|  values of parameters | training time | result | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8518018224731596,
        0.8518018224731596,
        0.8864749502652319
      ],
      "excerpt": "| batch_size=32, lr=1, epochs=100 | 2:01:26.619140 |loss: 0.0795 - acc: 0.9733 - val_loss: 1.6232 - val_acc: 0.7831| \n| batch_size=32, lr=0.1, epochs=100 | 2:01:06.484069 |loss: 0.0183 - acc: 0.9941 - val_loss: 1.1009 - val_acc: 0.8404| \n| batch_size=32, lr=0.01, epochs=100 | 2:01:09.871235 |loss: 0.1427 - acc: 0.9521 - val_loss: 0.5747 - val_acc: 0.8397| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8314823419435533
      ],
      "excerpt": "<img src='/img/channel_shuffle.png'> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8314823419435533
      ],
      "excerpt": "<img src='/img/shufflenet_unit.png'> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8309741275285016
      ],
      "excerpt": "|  Layer | Output size | Ksize | Stride | Repeat | Output channels(8 groups)| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8228632974204363
      ],
      "excerpt": "|  values of parameters | training time | result | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8535742254486868
      ],
      "excerpt": "<img src='/img/G1.png'> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8408593770872321
      ],
      "excerpt": "<img src='/img/G2.png'> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8535742254486868
      ],
      "excerpt": "<img src='/img/G3.png'> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8314823419435533
      ],
      "excerpt": "<img src='/img/G4.png'> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8314823419435533
      ],
      "excerpt": "<img src='/img/shufflenet_v2.png'> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8721706385144611
      ],
      "excerpt": "|  model | Params | result | training time | prediction time(1512 images) | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030199575248096
      ],
      "excerpt": "|  model | Params | result | training time | \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/forcefulowl/image_classification/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Image_classification",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "image_classification",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "forcefulowl",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/forcefulowl/image_classification/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 23 Dec 2021 07:30:04 GMT"
    },
    "technique": "GitHub API"
  }
}