{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1708.02002\n* Keras implementation of RetinaNet: https://github.com/fizyr/keras-retinane"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* Video data is from [Berkeley DeepDrive](https://bdd-data.berkeley.edu/)\n* RetinaNet paper: https://arxiv.org/abs/1708.02002\n* Keras implementation of RetinaNet: https://github.com/fizyr/keras-retinanet",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/RichardMathewsII/YOLBO",
    "technique": "GitHub API"
  },
  "contributors": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contributors\nThis is a list of people who contributed patches to keras-retinanet.\nIf you feel you should be listed here or if you have any other questions/comments on your listing here,\nplease create an issue or pull request at https://github.com/fizyr/keras-retinanet/\n\nHans Gaiser &#104;&#46;&#103;&#97;&#105;&#115;&#101;&#114;&#64;&#102;&#105;&#122;&#121;&#114;&#46;&#99;&#111;&#109;\nMaarten de Vries &#109;&#97;&#97;&#114;&#116;&#101;&#110;&#64;&#100;&#101;&#45;&#118;&#114;&#105;&#46;&#101;&#115;\nValerio Carpani\nAshley Williamson\nYann Henon\nValeriu Lacatusu\nAndr\u00e1s Vidosits\nCristian Gratie\njjiunlin\nSorin Panduru\nRodrigo Meira de Andrade\nEnrico Liscio &#101;&#46;&#108;&#105;&#115;&#99;&#105;&#111;&#64;&#102;&#105;&#122;&#121;&#114;&#46;&#99;&#111;&#109;\nMihai Morariu\npedroconceicao\njjiun\nWudi Fang\nMike Clark\nhannesedvartsen\nMax Van Sande\nPierre D\u00e9rian\nori\nmxvs\nmwilder\nMuhammed Kocabas\nKoen Vijverberg\niver56\nhnsywangxin\nGuillaume Erhard\nEduardo Ramos\nDiegoAgher\nAlexander Pacha\nAgastya Kalra\nJiri BOROVEC\nntsagko\ncharlie / tianqi\njsemric\nMartin Zlocha\nRaghav Bhardwaj\nbw4sz\nMorten Back Nielsen\ndshahrokhian\nAlex / adreo00\nsimone.merello\nMatt Wilder\nJinwoo Baek\nEtienne Meunier\nDenis Dowling\ncclauss\nAndrew Grigorev\nZFTurbo\nUgoLouche\nRichard Higgins\nRajat /  rajat.goel\nphilipp.marquardt\npeacherwu\nPaul / pauldesigaud\nMartin Genet\nLeo / leonardvandriel\nLaurens Hagendoorn\nJulius / juliussimonelli\nHolyGuacamole\nFausto Morales\nborakrc\nBen Weinstein\nAnil Karaka\nAndrea Panizza\nBruno Santos",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-04-07T19:29:19Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-08-21T17:58:19Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Most of the progress in computer vision has centered around object detection and \nsemantic segmentation in images. For image classification, popular networks have \nbeen ResNet, VGG Network, and GoogleNet. We have seen strong image \nsegmentation architectures such as FCN, SegNet, UNet, and PSPNet.\nWhen it has come to video data, the most common approach has been to deploy fast \nobject detection algorithms on each frame of the video, such as YOLO and \nRetinaNet. While this approach is effective, there is certainly room for \nimprovement. By performing fast object detection frame-by-frame, all of the previous\n timestep information is lost, and each timestep is just a brand-new image to the \n object detection algorithm. The goal of this project was to investigate the \n incorporation of previous timestep information to increase object detection \n in video data. This project also provides code for performing object detection\n on video data.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9690338525905733
      ],
      "excerpt": "The approach of this algorithm is to consider the results of an object detection  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9689906932441623
      ],
      "excerpt": "account. The core idea behind YOLBO is if RetinaNet is unsure about a detection in the  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.940554837883108
      ],
      "excerpt": "the detection is most likely valid. For every frame, RetinaNet makes a significant  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.989985654051016
      ],
      "excerpt": "YOLBO is able to effectively identify which of the many detections scored less than  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9700161083738165,
        0.960071124189782,
        0.9827696172424006
      ],
      "excerpt": "more about this project in detail, visit the associated paper. \nThe data structure used in the YOLBO algorithm is the detection matrix, a set of  \nspatial layers that the RetinaNet detections are mapped on to. There are N detections,  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9702655515407878
      ],
      "excerpt": "The centers of the bounding boxes, c, are calculated and the indices of the spatial  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8892888606085949
      ],
      "excerpt": "to map the scores to a spatial layer, where l = C, at the location where the detection  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8661152090436671
      ],
      "excerpt": "the previous timestep. For each detection of the current frame scoring less than the  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8336504933140886,
        0.8273302675613865
      ],
      "excerpt": "box in the previous detection matrix of the corresponding spatial layer and gathers  \nall the scores into a list of scores, S. The max value from S will replace the  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "You Only Look Back Once - algorithm using spatio-temporal information to improve RetinaNet object detection in video",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/RichardMathewsII/YOLBO/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sat, 25 Dec 2021 08:40:14 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/RichardMathewsII/YOLBO/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "RichardMathewsII/YOLBO",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/RichardMathewsII/YOLBO/master/demo.ipynb"
    ],
    "technique": "File Exploration"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/RichardMathewsII/YOLBO/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "YOLBO - An extension of RetinaNet for video object detection",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "YOLBO",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "RichardMathewsII",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/RichardMathewsII/YOLBO/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "RichardMathewsII",
        "body": "Weights pretrained on COCO dataset for the ResNet50 backbone.\r\nDemo video from Berkeley DeepDrive",
        "dateCreated": "2020-05-18T05:38:30Z",
        "datePublished": "2020-05-18T16:05:34Z",
        "html_url": "https://github.com/RichardMathewsII/YOLBO/releases/tag/v2.0",
        "name": "RetinaNet Weights and Demo Video",
        "tag_name": "v2.0",
        "tarball_url": "https://api.github.com/repos/RichardMathewsII/YOLBO/tarball/v2.0",
        "url": "https://api.github.com/repos/RichardMathewsII/YOLBO/releases/26634809",
        "zipball_url": "https://api.github.com/repos/RichardMathewsII/YOLBO/zipball/v2.0"
      },
      {
        "authorType": "User",
        "author_name": "RichardMathewsII",
        "body": "Pretrained RetinaNet model with ResNet50 backbone on COCO dataset.\r\nDemo video",
        "dateCreated": "2020-05-18T03:20:34Z",
        "datePublished": "2020-05-18T03:24:37Z",
        "html_url": "https://github.com/RichardMathewsII/YOLBO/releases/tag/v1.0",
        "name": "RetinaNet Weights",
        "tag_name": "v1.0",
        "tarball_url": "https://api.github.com/repos/RichardMathewsII/YOLBO/tarball/v1.0",
        "url": "https://api.github.com/repos/RichardMathewsII/YOLBO/releases/26610406",
        "zipball_url": "https://api.github.com/repos/RichardMathewsII/YOLBO/zipball/v1.0"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sat, 25 Dec 2021 08:40:14 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To run an pretrained RetinaNet-Resnet model on video data, visit [demo](demo.ipynb)\nand follow the instructions. To train a RetinaNet-Resnet model, go to \n[keras-retinanet](https://github.com/fizyr/keras-retinanet).\n\n",
      "technique": "Header extraction"
    }
  ]
}