{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2004.10934, 2020.\n\n# [Model Architecture](#contents)\n\nYOLOv4 choose CSPDarknet53 backbone, SPP additional module, PANet path-aggregation neck, and YOLOv4 (anchor based) head as the architecture of YOLOv4.\n\n# [Dataset](#contents)\n\nDataset support: [MS COCO] or datasetd with the same format as MS COCO\nAnnotation support: [MS COCO] or annotation as the same format as MS COCO\n\n- The directory structure is as follows, the name of directory and file is user define:\n\n    ```text\n        \u251c\u2500\u2500 dataset\n            \u251c\u2500\u2500 YOLOv4\n                \u251c\u2500\u2500 annotations\n                \u2502   \u251c\u2500 train.json\n                \u2502   \u2514\u2500 val.json\n                \u251c\u2500train\n                \u2502   \u251c\u2500picture1.jpg\n                \u2502   \u251c\u2500 ...\n                \u2502   \u2514\u2500picturen.jpg\n                \u251c\u2500 val\n                    \u251c\u2500picture1.jpg\n                    \u251c\u2500 ...\n                    \u2514\u2500picturen.jpg\n    ```\n\nwe suggest user to use MS COCO dataset to experience our model,\nother datasets need to use the same format as MS COCO.\n\n# [Environment Requirements](#contents)\n\n- Hardware\uff08Ascend\uff09\n    - Prepare hardware environment with Ascend processor. If you want to try Ascend, please send the [application form](https://obs-9be7.obs.cn-east-2.myhuaweicloud.com/file/other/Ascend%20Model%20Zoo%E4%BD%93%E9%AA%8C%E8%B5%84%E6%BA%90%E7%94%B3%E8%AF%B7%E8%A1%A8.docx) to ascend@huawei.com. Once approved, you can get the resources.\n- Framework\n    - [MindSpore](https://www.mindspore.cn/install/en)\n- For more information, please check the resources below\uff1a\n    - [MindSpore tutorials](https://www.mindspore.cn/tutorial/training/en/master/index.html)\n    - [MindSpore Python API](https://www.mindspore.cn/doc/api_python/en/master/index.html)\n\n# [Quick Start](#contents)\n\nAfter installing MindSpore via the official website, you can start training and evaluation as follows:\n\n```text\n# The cspdarknet53_backbone.ckpt in the follow script is got from cspdarknet53 training like paper.\n# The parameter of training_shape define image shape for network, default is\n                   [416, 416],\n                   [448, 448],\n                   [480, 480],\n                   [512, 512],\n                   [544, 544],\n                   [576, 576],\n                   [608, 608],\n                   [640, 640],\n                   [672, 672],\n                   [704, 704],\n                   [736, 736].\n# It means use 11 kinds of shape as input shape, or it can be set some kind of shape.\n```\n\n```bash\n#run training example(1p) by python command\npython train.py \\\n    --data_dir=./dataset/xxx \\\n    --pretrained_backbone=cspdarknet53_backbone.ckpt \\\n    --is_distributed=0 \\\n    --lr=0.1 \\\n    --t_max=320 \\\n    --max_epoch=320 \\\n    --warmup_epochs=4 \\\n    --training_shape=416 \\\n    --lr_scheduler=cosine_annealing > log.txt 2>&1 &\n```\n\n```bash\n# standalone training example(1p) by shell script\nsh run_standalone_train.sh dataset/xxx cspdarknet53_backbone.ckpt\n```\n\n```bash\n# For Ascend device, distributed training example(8p) by shell script\nsh run_distribute_train.sh dataset/xxx cspdarknet53_backbone.ckpt rank_table_8p.json\n```\n\n```bash\n# run evaluation by python command\npython eval.py \\\n    --data_dir=./dataset/xxx \\\n    --pretrained=yolov4.ckpt \\\n    --testing_shape=416 > log.txt 2>&1 &\n```\n\n```bash\n# run evaluation by shell script\nsh run_eval.sh dataset/xxx checkpoint/xxx.ckpt\n```\n\n# [Script Description](#contents)\n\n## [Script and Sample Code](#contents)\n\n```text\n\u2514\u2500yolov4\n  \u251c\u2500README.md\n  \u251c\u2500mindspore_hub_conf.py             # config for mindspore hub\n  \u251c\u2500scripts\n    \u251c\u2500run_standalone_train.sh         # launch standalone training(1p) in ascend\n    \u251c\u2500run_distribute_train.sh         # launch distributed training(8p) in ascend\n    \u2514\u2500run_eval.sh                     # launch evaluating in ascend\n    \u251c\u2500run_test.sh                     # launch testing in ascend\n  \u251c\u2500src\n    \u251c\u2500__init__.py                     # python init file\n    \u251c\u2500config.py                       # parameter configuration\n    \u251c\u2500cspdarknet53.py                 # backbone of network\n    \u251c\u2500distributed_sampler.py          # iterator of dataset\n    \u251c\u2500export.py                       # convert mindspore model to air model\n    \u251c\u2500initializer.py                  # initializer of parameters\n    \u251c\u2500logger.py                       # log function\n    \u251c\u2500loss.py                         # loss function\n    \u251c\u2500lr_scheduler.py                 # generate learning rate\n    \u251c\u2500transforms.py                   # Preprocess data\n    \u251c\u2500util.py                         # util function\n    \u251c\u2500yolo.py                         # yolov4 network\n    \u251c\u2500yolo_dataset.py                 # create dataset for YOLOV4\n\n  \u251c\u2500eval.py                           # evaluate val results\n  \u251c\u2500test.py#                          # evaluate test results\n  \u2514\u2500train.py                          # train net\n```\n\n## [Script Parameters](#contents)\n\nMajor parameters train.py as follows:\n\n```text\noptional arguments:\n  -h, --help            show this help message and exit\n  --device_target       device where the code will be implemented: \"Ascend\", default is \"Ascend\"\n  --data_dir DATA_DIR   Train dataset directory.\n  --per_batch_size PER_BATCH_SIZE\n                        Batch size for Training. Default: 8.\n  --pretrained_backbone PRETRAINED_BACKBONE\n                        The ckpt file of CspDarkNet53. Default: \"\".\n  --resume_yolov4 RESUME_YOLOV4\n                        The ckpt file of YOLOv4, which used to fine tune.\n                        Default: \"\"\n  --lr_scheduler LR_SCHEDULER\n                        Learning rate scheduler, options: exponential,\n                        cosine_annealing. Default: exponential\n  --lr LR               Learning rate. Default: 0.001\n  --lr_epochs LR_EPOCHS\n                        Epoch of changing of lr changing, split with \",\".\n                        Default: 220,250\n  --lr_gamma LR_GAMMA   Decrease lr by a factor of exponential lr_scheduler.\n                        Default: 0.1\n  --eta_min ETA_MIN     Eta_min in cosine_annealing scheduler. Default: 0\n  --t_max T_MAX         T-max in cosine_annealing scheduler. Default: 320\n  --max_epoch MAX_EPOCH\n                        Max epoch num to train the model. Default: 320\n  --warmup_epochs WARMUP_EPOCHS\n                        Warmup epochs. Default: 0\n  --weight_decay WEIGHT_DECAY\n                        Weight decay factor. Default: 0.0005\n  --momentum MOMENTUM   Momentum. Default: 0.9\n  --loss_scale LOSS_SCALE\n                        Static loss scale. Default: 64\n  --label_smooth LABEL_SMOOTH\n                        Whether to use label smooth in CE. Default:0\n  --label_smooth_factor LABEL_SMOOTH_FACTOR\n                        Smooth strength of original one-hot. Default: 0.1\n  --log_interval LOG_INTERVAL\n                        Logging interval steps. Default: 100\n  --ckpt_path CKPT_PATH\n                        Checkpoint save location. Default: outputs/\n  --ckpt_interval CKPT_INTERVAL\n                        Save checkpoint interval. Default: None\n  --is_save_on_master IS_SAVE_ON_MASTER\n                        Save ckpt on master or all rank, 1 for master, 0 for\n                        all ranks. Default: 1\n  --is_distributed IS_DISTRIBUTED\n                        Distribute train or not, 1 for yes, 0 for no. Default:\n                        1\n  --rank RANK           Local rank of distributed. Default: 0\n  --group_size GROUP_SIZE\n                        World size of device. Default: 1\n  --need_profiler NEED_PROFILER\n                        Whether use profiler. 0 for no, 1 for yes. Default: 0\n  --training_shape TRAINING_SHAPE\n                        Fix training shape. Default: \"\"\n  --resize_rate RESIZE_RATE\n                        Resize rate for multi-scale training. Default: 10\n```\n\n## [Training Process](#contents)\n\nYOLOv4 can be trained from the scratch or with the backbone named cspdarknet53.\nCspdarknet53 is a classifier which can be trained on some dataset like ImageNet(ILSVRC2012).\nIt is easy for users to train Cspdarknet53. Just replace the backbone of Classifier Resnet50 with cspdarknet53.\nResnet50 is easy to get in mindspore model zoo.\n\n### Training\n\nFor Ascend device, standalone training example(1p) by shell script\n\n```bash\nsh run_standalone_train.sh dataset/coco2017 cspdarknet53_backbone.ckpt\n```\n\n```text\npython train.py \\\n    --data_dir=/dataset/xxx \\\n    --pretrained_backbone=cspdarknet53_backbone.ckpt \\\n    --is_distributed=0 \\\n    --lr=0.1 \\\n    --t_max=320 \\\n    --max_epoch=320 \\\n    --warmup_epochs=4 \\\n    --training_shape=416 \\\n    --lr_scheduler=cosine_annealing > log.txt 2>&1 &\n```\n\nThe python command above will run in the background, you can view the results through the file log.txt.\n\nAfter training, you'll get some checkpoint files under the outputs folder by default. The loss value will be achieved as follows:\n\n```text\n\n# grep \"loss:\" train/log.txt\n2020-10-16 15:00:37,483:INFO:epoch[0], iter[0], loss:8248.610352, 0.03 imgs/sec, lr:2.0466639227834094e-07\n2020-10-16 15:00:52,897:INFO:epoch[0], iter[100], loss:5058.681709, 51.91 imgs/sec, lr:2.067130662908312e-05\n2020-10-16 15:01:08,286:INFO:epoch[0], iter[200], loss:1583.772806, 51.99 imgs/sec, lr:4.1137944208458066e-05\n2020-10-16 15:01:23,457:INFO:epoch[0], iter[300], loss:1229.840823, 52.75 imgs/sec, lr:6.160458724480122e-05\n2020-10-16 15:01:39,046:INFO:epoch[0], iter[400], loss:1155.170310, 51.32 imgs/sec, lr:8.207122300518677e-05\n2020-10-16 15:01:54,138:INFO:epoch[0], iter[500], loss:920.922433, 53.02 imgs/sec, lr:0.00010253786604152992\n2020-10-16 15:02:09,209:INFO:epoch[0], iter[600], loss:808.610681, 53.09 imgs/sec, lr:0.00012300450180191547\n2020-10-16 15:02:24,240:INFO:epoch[0], iter[700], loss:621.931513, 53.23 imgs/sec, lr:0.00014347114483825862\n2020-10-16 15:02:39,280:INFO:epoch[0], iter[800], loss:527.155985, 53.20 imgs/sec, lr:0.00016393778787460178\n...\n```\n\n### Distributed Training\n\nFor Ascend device, distributed training example(8p) by shell script\n\n```bash\nsh run_distribute_train.sh dataset/coco2017 cspdarknet53_backbone.ckpt rank_table_8p.json\n```\n\nThe above shell script will run distribute training in the background. You can view the results through the file train_parallel[X]/log.txt. The loss value will be achieved as follows:\n\n```text\n# distribute training result(8p, shape=416)\n...\n2020-10-16 14:58:25,142:INFO:epoch[0], iter[1000], loss:242.509259, 388.73 imgs/sec, lr:0.00032783843926154077\n2020-10-16 14:58:41,320:INFO:epoch[0], iter[1100], loss:228.137516, 395.61 imgs/sec, lr:0.0003605895326472819\n2020-10-16 14:58:57,607:INFO:epoch[0], iter[1200], loss:219.689884, 392.94 imgs/sec, lr:0.00039334059692919254\n2020-10-16 14:59:13,787:INFO:epoch[0], iter[1300], loss:216.173309, 395.56 imgs/sec, lr:0.00042609169031493366\n2020-10-16 14:59:29,969:INFO:epoch[0], iter[1400], loss:234.500610, 395.54 imgs/sec, lr:0.00045884278370067477\n2020-10-16 14:59:46,132:INFO:epoch[0], iter[1500], loss:209.420913, 396.00 imgs/sec, lr:0.0004915939061902463\n2020-10-16 15:00:02,416:INFO:epoch[0], iter[1600], loss:210.953930, 393.04 imgs/sec, lr:0.000524344970472157\n2020-10-16 15:00:18,651:INFO:epoch[0], iter[1700], loss:197.171296, 394.20 imgs/sec, lr:0.0005570960929617286\n2020-10-16 15:00:34,056:INFO:epoch[0], iter[1800], loss:203.928903, 415.47 imgs/sec, lr:0.0005898471572436392\n2020-10-16 15:00:53,680:INFO:epoch[1], iter[1900], loss:191.693561, 326.14 imgs/sec, lr:0.0006225982797332108\n2020-10-16 15:01:10,442:INFO:epoch[1], iter[2000], loss:196.632004, 381.82 imgs/sec, lr:0.0006553493440151215\n2020-10-16 15:01:27,180:INFO:epoch[1], iter[2100], loss:193.813570, 382.43 imgs/sec, lr:0.0006881004082970321\n2020-10-16 15:01:43,736:INFO:epoch[1], iter[2200], loss:176.996778, 386.59 imgs/sec, lr:0.0007208515307866037\n2020-10-16 15:02:00,294:INFO:epoch[1], iter[2300], loss:185.858901, 386.55 imgs/sec, lr:0.0007536025950685143\n...\n\n```\n\n```text\n# distribute training result(8p, dynamic shape)\n...\n2020-10-16 20:40:17,148:INFO:epoch[0], iter[800], loss:283.765033, 248.93 imgs/sec, lr:0.00026233625249005854\n2020-10-16 20:40:43,576:INFO:epoch[0], iter[900], loss:257.549973, 242.18 imgs/sec, lr:0.00029508734587579966\n2020-10-16 20:41:12,743:INFO:epoch[0], iter[1000], loss:252.426355, 219.43 imgs/sec, lr:0.00032783843926154077\n2020-10-16 20:41:43,153:INFO:epoch[0], iter[1100], loss:232.104760, 210.46 imgs/sec, lr:0.0003605895326472819\n2020-10-16 20:42:12,583:INFO:epoch[0], iter[1200], loss:236.973975, 217.47 imgs/sec, lr:0.00039334059692919254\n2020-10-16 20:42:39,004:INFO:epoch[0], iter[1300], loss:228.881298, 242.24 imgs/sec, lr:0.00042609169031493366\n2020-10-16 20:43:07,811:INFO:epoch[0], iter[1400], loss:255.025714, 222.19 imgs/sec, lr:0.00045884278370067477\n2020-10-16 20:43:38,177:INFO:epoch[0], iter[1500], loss:223.847151, 210.76 imgs/sec, lr:0.0004915939061902463\n2020-10-16 20:44:07,766:INFO:epoch[0], iter[1600], loss:222.302487, 216.30 imgs/sec, lr:0.000524344970472157\n2020-10-16 20:44:37,411:INFO:epoch[0], iter[1700], loss:211.063779, 215.89 imgs/sec, lr:0.0005570960929617286\n2020-10-16 20:45:03,092:INFO:epoch[0], iter[1800], loss:210.425542, 249.21 imgs/sec, lr:0.0005898471572436392\n2020-10-16 20:45:32,767:INFO:epoch[1], iter[1900], loss:208.449521, 215.67 imgs/sec, lr:0.0006225982797332108\n2020-10-16 20:45:59,163:INFO:epoch[1], iter[2000], loss:209.700071, 242.48 imgs/sec, lr:0.0006553493440151215\n...\n```\n\n### Transfer Training\n\nYou can train your own model based on either pretrained classification model or pretrained detection model. You can perform transfer training by following steps.\n\n1. Convert your own dataset to COCO style. Otherwise you have to add your own data preprocess code.\n2. Change config.py according to your own dataset, especially the `num_classes`.\n3. Set argument `filter_weight` to `True` and `pretrained_checkpoint` to pretrained checkpoint while calling `train.py`, this will filter the final detection box weight from the pretrained model.\n4. Build your own bash scripts using new config and arguments for further convenient.\n\n## [Evaluation Process](#contents)\n\n### Valid\n\n```bash\npython eval.py \\\n    --data_dir=./dataset/coco2017 \\\n    --pretrained=yolov4.ckpt \\\n    --testing_shape=608 > log.txt 2>&1 &\nOR\nsh run_eval.sh dataset/coco2017 checkpoint/yolov4.ckpt\n```\n\nThe above python command will run in the background. You can view the results through the file \"log.txt\". The mAP of the test dataset will be as follows:\n\n```text\n# log.txt\n=============coco eval reulst=========\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.442\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.635\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.479\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.274\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.485\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.567\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.331\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.545\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.590\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.418\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.638\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.717\n```\n\n### Test-dev\n\n```bash\npython test.py \\\n    --data_dir=./dataset/coco2017 \\\n    --pretrained=yolov4.ckpt \\\n    --testing_shape=608 > log.txt 2>&1 &\nOR\nsh run_test.sh dataset/coco2017 checkpoint/yolov4.ckpt\n```\n\nThe predict_xxx.json will be found in test/outputs/%Y-%m-%d_time_%H_%M_%S/.\nRename the file predict_xxx.json to detections_test-dev2017_yolov4_results.json and compress it to detections_test-dev2017_yolov4_results.zip\nSubmit file detections_test-dev2017_yolov4_results.zip to the MS COCO evaluation server for the test-dev2019 (bbox) <https://competitions.codalab.org/competitions/20794#participate>\nYou will get such results in the end of file View scoring output log.\n\n```text\noverall performance\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.447\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.642\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.487\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.267\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.485\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.549\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.335\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.547\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.584\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.392\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.627\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.711\n```\n\n## [Convert Process](#contents)\n\n### Convert\n\nIf you want to infer the network on Ascend 310, you should convert the model to MINDIR:\n\n```python\npython export.py --ckpt_file [CKPT_PATH] --file_name [FILE_NAME] --file_format [FILE_FORMAT]\n```\n\nThe ckpt_file parameter is required,\n`EXPORT_FORMAT` should be in [\"AIR\", \"ONNX\", \"MINDIR\"]\n\n## [Inference Process](#contents)\n\n### Usage\n\nBefore performing inference, the mindir file must be exported by export script on the 910 environment.\nCurrent batch_Size can only be set to 1. The precision calculation process needs about 70G+ memory space.\n\n```shell\n# Ascend310 inference\nsh run_infer_310.sh [MINDIR_PATH] [DATA_PATH] [DEVICE_ID] [ANN_FILE]\n```\n\n`DEVICE_ID` is optional, default value is 0.\n\n### result\n\nInference result is saved in current path, you can find result like this in acc.log file.\n\n```text\n=============coco eval reulst=========\nAverage Precision (AP) @[ IoU=0.50:0.95 | area= all   | maxDets=100 ] = 0.438\nAverage Precision (AP) @[ IoU=0.50      | area= all   | maxDets=100 ] = 0.630\nAverage Precision (AP) @[ IoU=0.75      | area= all   | maxDets=100 ] = 0.475\nAverage Precision (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.272\nAverage Precision (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.481\nAverage Precision (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.567\nAverage Recall    (AR) @[ IoU=0.50:0.95 | area= all   | maxDets=  1 ] = 0.330\nAverage Recall    (AR) @[ IoU=0.50:0.95 | area= all   | maxDets= 10 ] = 0.542\nAverage Recall    (AR) @[ IoU=0.50:0.95 | area= all   | maxDets=100 ] = 0.588\nAverage Recall    (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.410\nAverage Recall    (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.636\nAverage Recall    (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.716\n```\n\n# [Model Description](#contents)\n\n## [Performance](#contents)\n\n### Evaluation Performance\n\nYOLOv4 on 118K images(The annotation and data format must be the same as coco2017)\n\n| Parameters                 | YOLOv4                                                      |\n| -------------------------- | ----------------------------------------------------------- |\n| Resource                   | Ascend 910; CPU 2.60GHz, 192cores; Memory, 755G             |\n| uploaded Date              | 10/16/2020 (month/day/year)                                 |\n| MindSpore Version          | 1.0.0-alpha                                                 |\n| Dataset                    | 118K images                                                 |\n| Training Parameters        | epoch=320, batch_size=8, lr=0.012,momentum=0.9              |\n| Optimizer                  | Momentum                                                    |\n| Loss Function              | Sigmoid Cross Entropy with logits, Giou Loss                |\n| outputs                    | boxes and label                                             |\n| Loss                       | 50                                                          |\n| Speed                      | 1p 53FPS 8p 390FPS(shape=416) 220FPS(dynamic shape)         |\n| Total time                 | 48h(dynamic shape)                                          |\n| Checkpoint for Fine tuning | about 500M (.ckpt file)                                     |\n| Scripts                    | <https://gitee.com/mindspore/mindspore/tree/master/model_zoo/> |\n\n### Inference Performance\n\nYOLOv4 on 20K images(The annotation and data format must be the same as coco test2017 )\n\n| Parameters                 | YOLOv4                                                      |\n| -------------------------- | ----------------------------------------------------------- |\n| Resource                   | Ascend 910; CPU 2.60GHz, 192cores; Memory, 755G             |\n| uploaded Date              | 10/16/2020 (month/day/year)                                 |\n| MindSpore Version          | 1.0.0-alpha                                                 |\n| Dataset                    | 20K images                                                  |\n| batch_size                 | 1                                                           |\n| outputs                    | box position and sorces, and probability                    |\n| Accuracy                   | map >= 44.7%(shape=608)                                     |\n| Model for inference        | about 500M (.ckpt file)                                     |\n\n# [Description of Random Situation](#contents)\n\nIn dataset.py, we set the seed inside ```create_dataset``` function.\nIn var_init.py, we set seed for weight initialization\n\n# [ModelZoo Homepage](#contents)\n\n Please check the official [homepage](https://gitee.com/mindspore/mindspore/tree/master/model_zoo)."
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9999831338750962
      ],
      "excerpt": "Bochkovskiy A, Wang C Y, Liao H Y M. YOLOv4: Optimal Speed and Accuracy of Object Detection[J]. arXiv preprint arXiv:2004.10934, 2020. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8558857979955563
      ],
      "excerpt": "                        Resize rate for multi-scale training. Default: 10 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9598596565459202,
        0.8629711733945958,
        0.9409741438873975
      ],
      "excerpt": "2020-10-16 15:00:37,483:INFO:epoch[0], iter[0], loss:8248.610352, 0.03 imgs/sec, lr:2.0466639227834094e-07 \n2020-10-16 15:00:52,897:INFO:epoch[0], iter[100], loss:5058.681709, 51.91 imgs/sec, lr:2.067130662908312e-05 \n2020-10-16 15:01:08,286:INFO:epoch[0], iter[200], loss:1583.772806, 51.99 imgs/sec, lr:4.1137944208458066e-05 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9409741438873975,
        0.8629711733945958,
        0.8629711733945958
      ],
      "excerpt": "2020-10-16 15:01:39,046:INFO:epoch[0], iter[400], loss:1155.170310, 51.32 imgs/sec, lr:8.207122300518677e-05 \n2020-10-16 15:01:54,138:INFO:epoch[0], iter[500], loss:920.922433, 53.02 imgs/sec, lr:0.00010253786604152992 \n2020-10-16 15:02:09,209:INFO:epoch[0], iter[600], loss:808.610681, 53.09 imgs/sec, lr:0.00012300450180191547 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8629711733945958
      ],
      "excerpt": "2020-10-16 15:02:39,280:INFO:epoch[0], iter[800], loss:527.155985, 53.20 imgs/sec, lr:0.00016393778787460178 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9098164483377704
      ],
      "excerpt": "2020-10-16 14:59:29,969:INFO:epoch[0], iter[1400], loss:234.500610, 395.54 imgs/sec, lr:0.00045884278370067477 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9409741438873975
      ],
      "excerpt": "2020-10-16 15:00:18,651:INFO:epoch[0], iter[1700], loss:197.171296, 394.20 imgs/sec, lr:0.0005570960929617286 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9776983601694141,
        0.8629711733945958,
        0.8629711733945958,
        0.8629711733945958
      ],
      "excerpt": "2020-10-16 15:01:10,442:INFO:epoch[1], iter[2000], loss:196.632004, 381.82 imgs/sec, lr:0.0006553493440151215 \n2020-10-16 15:01:27,180:INFO:epoch[1], iter[2100], loss:193.813570, 382.43 imgs/sec, lr:0.0006881004082970321 \n2020-10-16 15:01:43,736:INFO:epoch[1], iter[2200], loss:176.996778, 386.59 imgs/sec, lr:0.0007208515307866037 \n2020-10-16 15:02:00,294:INFO:epoch[1], iter[2300], loss:185.858901, 386.55 imgs/sec, lr:0.0007536025950685143 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9409741438873975,
        0.9598596565459202,
        0.8629711733945958,
        0.9598596565459202
      ],
      "excerpt": "2020-10-16 20:40:43,576:INFO:epoch[0], iter[900], loss:257.549973, 242.18 imgs/sec, lr:0.00029508734587579966 \n2020-10-16 20:41:12,743:INFO:epoch[0], iter[1000], loss:252.426355, 219.43 imgs/sec, lr:0.00032783843926154077 \n2020-10-16 20:41:43,153:INFO:epoch[0], iter[1100], loss:232.104760, 210.46 imgs/sec, lr:0.0003605895326472819 \n2020-10-16 20:42:12,583:INFO:epoch[0], iter[1200], loss:236.973975, 217.47 imgs/sec, lr:0.00039334059692919254 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8629711733945958,
        0.8629711733945958,
        0.9598596565459202,
        0.9598596565459202
      ],
      "excerpt": "2020-10-16 20:43:07,811:INFO:epoch[0], iter[1400], loss:255.025714, 222.19 imgs/sec, lr:0.00045884278370067477 \n2020-10-16 20:43:38,177:INFO:epoch[0], iter[1500], loss:223.847151, 210.76 imgs/sec, lr:0.0004915939061902463 \n2020-10-16 20:44:07,766:INFO:epoch[0], iter[1600], loss:222.302487, 216.30 imgs/sec, lr:0.000524344970472157 \n2020-10-16 20:44:37,411:INFO:epoch[0], iter[1700], loss:211.063779, 215.89 imgs/sec, lr:0.0005570960929617286 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.870561293637803
      ],
      "excerpt": "2020-10-16 20:45:32,767:INFO:epoch[1], iter[1900], loss:208.449521, 215.67 imgs/sec, lr:0.0006225982797332108 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9926742442780817
      ],
      "excerpt": "| uploaded Date              | 10/16/2020 (month/day/year)                                 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9926742442780817
      ],
      "excerpt": "| uploaded Date              | 10/16/2020 (month/day/year)                                 | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Moonforeva/mindspore_yolov4_modified",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-04-17T16:13:17Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-12T04:13:09Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "In dataset.py, we set the seed inside ```create_dataset``` function.\nIn var_init.py, we set seed for weight initialization\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8112146660077283
      ],
      "excerpt": "Model Description \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9756292949940919,
        0.9727146410408217,
        0.8287112600564575
      ],
      "excerpt": "YOLOv4 is a state-of-the-art detector which is faster (FPS) and more accurate (MS COCO AP50...95 and AP50) than all available alternative detectors. \nYOLOv4 has verified a large number of features, and selected for use such of them for improving the accuracy of both the classifier and the detector. \nThese features can be used as best-practice for future studies and developments. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8888499143090783
      ],
      "excerpt": "we suggest user to use MS COCO dataset to experience our model, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.895352591117532
      ],
      "excerpt": "For more information, please check the resources below\uff1a \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8076392524539409,
        0.9758273337078931
      ],
      "excerpt": ": The cspdarknet53_backbone.ckpt in the follow script is got from cspdarknet53 training like paper. \n: The parameter of training_shape define image shape for network, default is \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.849292068391993
      ],
      "excerpt": "    \u251c\u2500cspdarknet53.py                 #: backbone of network \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9487243783121371
      ],
      "excerpt": "                        The ckpt file of YOLOv4, which used to fine tune. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9857057985632042
      ],
      "excerpt": "                        Epoch of changing of lr changing, split with \",\". \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9608510733453137
      ],
      "excerpt": "                        Save ckpt on master or all rank, 1 for master, 0 for \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9137990829994085
      ],
      "excerpt": "  --rank RANK           Local rank of distributed. Default: 0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8435443566776053
      ],
      "excerpt": "                        World size of device. Default: 1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9892775160913749,
        0.9889506626567547
      ],
      "excerpt": "It is easy for users to train Cspdarknet53. Just replace the backbone of Classifier Resnet50 with cspdarknet53. \nResnet50 is easy to get in mindspore model zoo. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9251495230823084
      ],
      "excerpt": "The ckpt_file parameter is required, \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Moonforeva/mindspore_yolov4_modified/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sat, 25 Dec 2021 11:43:47 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Moonforeva/mindspore_yolov4_modified/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Moonforeva/mindspore_yolov4_modified",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Moonforeva/mindspore_yolov4_modified/master/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Moonforeva/mindspore_yolov4_modified/master/scripts/run_test.sh",
      "https://raw.githubusercontent.com/Moonforeva/mindspore_yolov4_modified/master/scripts/run_standalone_train.sh",
      "https://raw.githubusercontent.com/Moonforeva/mindspore_yolov4_modified/master/scripts/run_eval.sh",
      "https://raw.githubusercontent.com/Moonforeva/mindspore_yolov4_modified/master/scripts/run_infer_310.sh",
      "https://raw.githubusercontent.com/Moonforeva/mindspore_yolov4_modified/master/scripts/run_distribute_train.sh",
      "https://raw.githubusercontent.com/Moonforeva/mindspore_yolov4_modified/master/ascend310_infer/src/build.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9886068399243466
      ],
      "excerpt": "Environment Requirements \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.877813560196573
      ],
      "excerpt": "Prepare hardware environment with Ascend processor. If you want to try Ascend, please send the application form to ascend@huawei.com. Once approved, you can get the resources. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639022768319172
      ],
      "excerpt": "sh run_standalone_train.sh dataset/xxx cspdarknet53_backbone.ckpt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8430665775485048
      ],
      "excerpt": "sh run_distribute_train.sh dataset/xxx cspdarknet53_backbone.ckpt rank_table_8p.json \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8178483569271014
      ],
      "excerpt": "  --data_dir DATA_DIR   Train dataset directory. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8621178970438443
      ],
      "excerpt": "                        Save ckpt on master or all rank, 1 for master, 0 for \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9153604431029827
      ],
      "excerpt": "sh run_standalone_train.sh dataset/coco2017 cspdarknet53_backbone.ckpt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8933866340514772
      ],
      "excerpt": "sh run_distribute_train.sh dataset/coco2017 cspdarknet53_backbone.ckpt rank_table_8p.json \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8135745390226773
      ],
      "excerpt": "Build your own bash scripts using new config and arguments for further convenient. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9153604431029827
      ],
      "excerpt": "sh run_eval.sh dataset/coco2017 checkpoint/yolov4.ckpt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9153604431029827
      ],
      "excerpt": "sh run_test.sh dataset/coco2017 checkpoint/yolov4.ckpt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.801741321210649
      ],
      "excerpt": "| MindSpore Version          | 1.0.0-alpha                                                 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8905967494062484
      ],
      "excerpt": "| Scripts                    | https://gitee.com/mindspore/mindspore/tree/master/model_zoo/ | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.801741321210649
      ],
      "excerpt": "| MindSpore Version          | 1.0.0-alpha                                                 | \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8606280910157142
      ],
      "excerpt": "Quick Start \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8890390350607155
      ],
      "excerpt": "            \u2502   \u251c\u2500 train.json \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137,
        0.8639986685036579
      ],
      "excerpt": "            \u251c\u2500train \n            \u2502   \u251c\u2500picture1.jpg \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639986685036579
      ],
      "excerpt": "            \u2502   \u2514\u2500picturen.jpg \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639986685036579
      ],
      "excerpt": "                \u251c\u2500picture1.jpg \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639986685036579
      ],
      "excerpt": "                \u2514\u2500picturen.jpg \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9024969691147869,
        0.9503189345333785
      ],
      "excerpt": ":run training example(1p) by python command \npython train.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8943096850060217
      ],
      "excerpt": "python eval.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9151270764930737
      ],
      "excerpt": "  \u251c\u2500src \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8815763798329048,
        0.8311976687681194
      ],
      "excerpt": "    \u251c\u2500distributed_sampler.py          #: iterator of dataset \n    \u251c\u2500export.py                       #: convert mindspore model to air model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8996298282120481
      ],
      "excerpt": "    \u251c\u2500logger.py                       #: log function \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8007169859666832,
        0.9132566544672166
      ],
      "excerpt": "    \u251c\u2500transforms.py                   #: Preprocess data \n    \u251c\u2500util.py                         #: util function \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8696478423327958,
        0.8603868796522177,
        0.919765970937171,
        0.9041705675966061
      ],
      "excerpt": "    \u251c\u2500yolo_dataset.py                 #: create dataset for YOLOV4 \n\u251c\u2500eval.py                           #: evaluate val results \n  \u251c\u2500test.py#:                          #: evaluate test results \n  \u2514\u2500train.py                          #: train net \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8180460762754344
      ],
      "excerpt": "Major parameters train.py as follows: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8910781021929108
      ],
      "excerpt": "  --data_dir DATA_DIR   Train dataset directory. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8419938855337222
      ],
      "excerpt": "                        Batch size for Training. Default: 8. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9011679364826327
      ],
      "excerpt": "                        Max epoch num to train the model. Default: 320 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8249064470805756
      ],
      "excerpt": "                        Save checkpoint interval. Default: None \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9503189345333785
      ],
      "excerpt": "python train.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8306999800527426,
        0.8077884693236151,
        0.8102170393874704,
        0.8356180650325328,
        0.9052802624936067,
        0.9268283243433245,
        0.9362480270498343,
        0.9225596931052947,
        0.9263437319654542,
        0.887322280220754,
        0.9108547989111869,
        0.8961662935036081
      ],
      "excerpt": "The python command above will run in the background, you can view the results through the file log.txt. \nAfter training, you'll get some checkpoint files under the outputs folder by default. The loss value will be achieved as follows: \n: grep \"loss:\" train/log.txt \n2020-10-16 15:00:37,483:INFO:epoch[0], iter[0], loss:8248.610352, 0.03 imgs/sec, lr:2.0466639227834094e-07 \n2020-10-16 15:00:52,897:INFO:epoch[0], iter[100], loss:5058.681709, 51.91 imgs/sec, lr:2.067130662908312e-05 \n2020-10-16 15:01:08,286:INFO:epoch[0], iter[200], loss:1583.772806, 51.99 imgs/sec, lr:4.1137944208458066e-05 \n2020-10-16 15:01:23,457:INFO:epoch[0], iter[300], loss:1229.840823, 52.75 imgs/sec, lr:6.160458724480122e-05 \n2020-10-16 15:01:39,046:INFO:epoch[0], iter[400], loss:1155.170310, 51.32 imgs/sec, lr:8.207122300518677e-05 \n2020-10-16 15:01:54,138:INFO:epoch[0], iter[500], loss:920.922433, 53.02 imgs/sec, lr:0.00010253786604152992 \n2020-10-16 15:02:09,209:INFO:epoch[0], iter[600], loss:808.610681, 53.09 imgs/sec, lr:0.00012300450180191547 \n2020-10-16 15:02:24,240:INFO:epoch[0], iter[700], loss:621.931513, 53.23 imgs/sec, lr:0.00014347114483825862 \n2020-10-16 15:02:39,280:INFO:epoch[0], iter[800], loss:527.155985, 53.20 imgs/sec, lr:0.00016393778787460178 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8677337573588683,
        0.9007320156712417
      ],
      "excerpt": "The above shell script will run distribute training in the background. You can view the results through the file train_parallel[X]/log.txt. The loss value will be achieved as follows: \n: distribute training result(8p, shape=416) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8261452405343381
      ],
      "excerpt": "2020-10-16 14:58:25,142:INFO:epoch[0], iter[1000], loss:242.509259, 388.73 imgs/sec, lr:0.00032783843926154077 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8136324382910735,
        0.8030330199446223,
        0.8465726378961036,
        0.8786448574791287,
        0.9042229387125903,
        0.846490295204943,
        0.9122098326634972,
        0.9211868902204224,
        0.9263437319654542,
        0.8661293749074762
      ],
      "excerpt": "2020-10-16 14:59:29,969:INFO:epoch[0], iter[1400], loss:234.500610, 395.54 imgs/sec, lr:0.00045884278370067477 \n2020-10-16 14:59:46,132:INFO:epoch[0], iter[1500], loss:209.420913, 396.00 imgs/sec, lr:0.0004915939061902463 \n2020-10-16 15:00:02,416:INFO:epoch[0], iter[1600], loss:210.953930, 393.04 imgs/sec, lr:0.000524344970472157 \n2020-10-16 15:00:18,651:INFO:epoch[0], iter[1700], loss:197.171296, 394.20 imgs/sec, lr:0.0005570960929617286 \n2020-10-16 15:00:34,056:INFO:epoch[0], iter[1800], loss:203.928903, 415.47 imgs/sec, lr:0.0005898471572436392 \n2020-10-16 15:00:53,680:INFO:epoch[1], iter[1900], loss:191.693561, 326.14 imgs/sec, lr:0.0006225982797332108 \n2020-10-16 15:01:10,442:INFO:epoch[1], iter[2000], loss:196.632004, 381.82 imgs/sec, lr:0.0006553493440151215 \n2020-10-16 15:01:27,180:INFO:epoch[1], iter[2100], loss:193.813570, 382.43 imgs/sec, lr:0.0006881004082970321 \n2020-10-16 15:01:43,736:INFO:epoch[1], iter[2200], loss:176.996778, 386.59 imgs/sec, lr:0.0007208515307866037 \n2020-10-16 15:02:00,294:INFO:epoch[1], iter[2300], loss:185.858901, 386.55 imgs/sec, lr:0.0007536025950685143 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9117813821660473
      ],
      "excerpt": ": distribute training result(8p, dynamic shape) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8654635576054491,
        0.8440461244300711,
        0.8378379176879374,
        0.854498718221547,
        0.8470290276339444,
        0.8730643275904907,
        0.8598934540828201,
        0.8654635576054491,
        0.8750645006378481,
        0.8403053761435337,
        0.8828375130848551,
        0.9151855754079956,
        0.908125236927998
      ],
      "excerpt": "2020-10-16 20:40:17,148:INFO:epoch[0], iter[800], loss:283.765033, 248.93 imgs/sec, lr:0.00026233625249005854 \n2020-10-16 20:40:43,576:INFO:epoch[0], iter[900], loss:257.549973, 242.18 imgs/sec, lr:0.00029508734587579966 \n2020-10-16 20:41:12,743:INFO:epoch[0], iter[1000], loss:252.426355, 219.43 imgs/sec, lr:0.00032783843926154077 \n2020-10-16 20:41:43,153:INFO:epoch[0], iter[1100], loss:232.104760, 210.46 imgs/sec, lr:0.0003605895326472819 \n2020-10-16 20:42:12,583:INFO:epoch[0], iter[1200], loss:236.973975, 217.47 imgs/sec, lr:0.00039334059692919254 \n2020-10-16 20:42:39,004:INFO:epoch[0], iter[1300], loss:228.881298, 242.24 imgs/sec, lr:0.00042609169031493366 \n2020-10-16 20:43:07,811:INFO:epoch[0], iter[1400], loss:255.025714, 222.19 imgs/sec, lr:0.00045884278370067477 \n2020-10-16 20:43:38,177:INFO:epoch[0], iter[1500], loss:223.847151, 210.76 imgs/sec, lr:0.0004915939061902463 \n2020-10-16 20:44:07,766:INFO:epoch[0], iter[1600], loss:222.302487, 216.30 imgs/sec, lr:0.000524344970472157 \n2020-10-16 20:44:37,411:INFO:epoch[0], iter[1700], loss:211.063779, 215.89 imgs/sec, lr:0.0005570960929617286 \n2020-10-16 20:45:03,092:INFO:epoch[0], iter[1800], loss:210.425542, 249.21 imgs/sec, lr:0.0005898471572436392 \n2020-10-16 20:45:32,767:INFO:epoch[1], iter[1900], loss:208.449521, 215.67 imgs/sec, lr:0.0006225982797332108 \n2020-10-16 20:45:59,163:INFO:epoch[1], iter[2000], loss:209.700071, 242.48 imgs/sec, lr:0.0006553493440151215 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8714981136313872
      ],
      "excerpt": "Set argument filter_weight to True and pretrained_checkpoint to pretrained checkpoint while calling train.py, this will filter the final detection box weight from the pretrained model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8943096850060217
      ],
      "excerpt": "python eval.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.840299769413005
      ],
      "excerpt": "The above python command will run in the background. You can view the results through the file \"log.txt\". The mAP of the test dataset will be as follows: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8024269194255869
      ],
      "excerpt": " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.479 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9515752551715031
      ],
      "excerpt": "python test.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8486349214928024
      ],
      "excerpt": "Rename the file predict_xxx.json to detections_test-dev2017_yolov4_results.json and compress it to detections_test-dev2017_yolov4_results.zip \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8024269194255869
      ],
      "excerpt": " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.487 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8024269194255869
      ],
      "excerpt": "Average Precision (AP) @[ IoU=0.75      | area= all   | maxDets=100 ] = 0.475 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8212149302893826
      ],
      "excerpt": "YOLOv4 on 118K images(The annotation and data format must be the same as coco2017) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8076713752689532,
        0.848333937641539
      ],
      "excerpt": "| Dataset                    | 118K images                                                 | \n| Training Parameters        | epoch=320, batch_size=8, lr=0.012,momentum=0.9              | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8160674778763787,
        0.8775336685400213
      ],
      "excerpt": "| Speed                      | 1p 53FPS 8p 390FPS(shape=416) 220FPS(dynamic shape)         | \n| Total time                 | 48h(dynamic shape)                                          | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8076713752689532
      ],
      "excerpt": "| Dataset                    | 20K images                                                  | \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Moonforeva/mindspore_yolov4_modified/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell",
      "C++",
      "Dockerfile",
      "CMake"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contents",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "mindspore_yolov4_modified",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Moonforeva",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Moonforeva/mindspore_yolov4_modified/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Sat, 25 Dec 2021 11:43:47 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Before performing inference, the mindir file must be exported by export script on the 910 environment.\nCurrent batch_Size can only be set to 1. The precision calculation process needs about 70G+ memory space.\n\n```shell\n#: Ascend310 inference\nsh run_infer_310.sh [MINDIR_PATH] [DATA_PATH] [DEVICE_ID] [ANN_FILE]\n```\n\n`DEVICE_ID` is optional, default value is 0.\n\n",
      "technique": "Header extraction"
    }
  ]
}