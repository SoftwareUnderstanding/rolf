{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "MMAction2 is an open-source project that is contributed by researchers and engineers from various colleges and companies.\nWe appreciate all the contributors who implement their methods or add new features and users who give valuable feedback.\nWe wish that the toolbox and benchmark could serve the growing research community by providing a flexible toolkit to reimplement existing methods and develop their new models.\n\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2106.13230",
      "https://arxiv.org/abs/2107.10161",
      "https://arxiv.org/abs/2103.17263"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find this project useful in your research, please consider cite:\n\n```BibTeX\n@misc{2020mmaction2,\n    title={OpenMMLab's Next Generation Video Understanding Toolbox and Benchmark},\n    author={MMAction2 Contributors},\n    howpublished = {\\url{https://github.com/open-mmlab/mmaction2}},\n    year={2020}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "cff-version: 1.2.0\nmessage: \"If you use this software, please cite it as below.\"\nauthors:\n  - name: \"MMAction2 Contributors\"\ntitle: \"OpenMMLab's Next Generation Video Understanding Toolbox and Benchmark\"\ndate-released: 2020-07-21\nurl: \"https://github.com/open-mmlab/mmaction2\"\nlicense: Apache-2.0",
      "technique": "File Exploration"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{2020mmaction2,\n    title={OpenMMLab's Next Generation Video Understanding Toolbox and Benchmark},\n    author={MMAction2 Contributors},\n    howpublished = {\\url{https://github.com/open-mmlab/mmaction2}},\n    year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.97812821469553
      ],
      "excerpt": "(2021-10-29) We provide a demo for skeleton-based and rgb-based spatio-temporal detection and action recognition (demo/demo_video_structuralize.py). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.997670315963766,
        0.8462204750624114
      ],
      "excerpt": "Release: v0.20.0 was released in 30/10/2021. Please refer to changelog.md for details and release history. \n<table style=\"margin-left:auto;margin-right:auto;font-size:1.3vw;padding:3px 5px;text-align:center;vertical-align:center;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.986327158429457
      ],
      "excerpt": "    <td colspan=\"5\" style=\"font-weight:bold;\">Action Recognition</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999279786846396,
        0.999959986877335,
        0.9999588436923104,
        0.9996763749322151,
        0.9999567859658505
      ],
      "excerpt": "    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/recognition/c3d/README.md\">C3D</a> (CVPR'2014)</td> \n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/recognition/tsn/README.md\">TSN</a> (ECCV'2016)</td> \n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/recognition/i3d/README.md\">I3D</a> (CVPR'2017)</td> \n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/recognition/i3d/README.md\">I3D Non-Local</a> (CVPR'2018)</td> \n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/recognition/r2plus1d/README.md\">R(2+1)D</a> (CVPR'2018)</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999639880455053,
        0.9999314080358421,
        0.9994864070325202,
        0.9999314080358421,
        0.9999314080358421
      ],
      "excerpt": "    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/recognition/trn/README.md\">TRN</a> (ECCV'2018)</td> \n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/recognition/tsm/README.md\">TSM</a> (ICCV'2019)</td> \n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/recognition/tsm/README.md\">TSM Non-Local</a> (ICCV'2019)</td> \n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/recognition/slowonly/README.md\">SlowOnly</a> (ICCV'2019)</td> \n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/recognition/slowfast/README.md\">SlowFast</a> (ICCV'2019)</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999314080358421,
        0.9990892112717679,
        0.9996354853111827,
        0.9996354853111827,
        0.9996962193038992
      ],
      "excerpt": "    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/recognition/csn/README.md\">CSN</a> (ICCV'2019)</td> \n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/recognition/tin/README.md\">TIN</a> (AAAI'2020)</td> \n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/recognition/tpn/README.md\">TPN</a> (CVPR'2020)</td> \n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/recognition/x3d/README.md\">X3D</a> (CVPR'2020)</td> \n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/recognition/omnisource/README.md\">OmniSource</a> (ECCV'2020)</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.990803422949846,
        0.9998342785553219,
        0.9990892112717679
      ],
      "excerpt": "    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/recognition_audio/resnet/README.md\">MultiModality: Audio</a> (ArXiv'2020)</td> \n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/recognition/tanet/README.md\">TANet</a> (ArXiv'2020)</td> \n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/recognition/timesformer/README.md\">TimeSformer</a> (ICML'2021)</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9115465676107753
      ],
      "excerpt": "    <td colspan=\"5\" style=\"font-weight:bold;\">Action Localization</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9995200537736332,
        0.9997479707749294,
        0.9995200537736332
      ],
      "excerpt": "    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/localization/ssn/README.md\">SSN</a> (ICCV'2017)</td> \n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/localization/bsn/README.md\">BSN</a> (ECCV'2018)</td> \n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/localization/bmn/README.md\">BMN</a> (ICCV'2019)</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9850821515827285
      ],
      "excerpt": "    <td colspan=\"5\" style=\"font-weight:bold;\">Spatio-Temporal Action Detection</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9997479707749294,
        0.9994008594903259,
        0.9994008594903259,
        0.9997119769699535
      ],
      "excerpt": "    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/detection/acrn/README.md\">ACRN</a> (ECCV'2018)</td> \n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/detection/ava/README.md\">SlowOnly+Fast R-CNN</a> (ICCV'2019)</td> \n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/detection/ava/README.md\">SlowFast+Fast R-CNN</a> (ICCV'2019)</td> \n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/detection/lfb/README.md\">LFB</a> (CVPR'2019)</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9785643756558006
      ],
      "excerpt": "    <td colspan=\"5\" style=\"font-weight:bold;\">Skeleton-based Action Recognition</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.998089257981238,
        0.9970716287629291
      ],
      "excerpt": "    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/skeleton/stgcn/README.md\">ST-GCN</a> (AAAI'2018)</td> \n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/configs/skeleton/posec3d/README.md\">PoseC3D</a> (ArXiv'2021)</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8462204750624114
      ],
      "excerpt": "<table style=\"margin-left:auto;margin-right:auto;font-size:1.3vw;padding:3px 5px;text-align:center;vertical-align:center;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.986327158429457
      ],
      "excerpt": "    <td colspan=\"4\" style=\"font-weight:bold;\">Action Recognition</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9547229455066405
      ],
      "excerpt": "    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/tools/data/hmdb51/README.md\">HMDB51</a> (<a href=\"https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/\">Homepage</a>) (ICCV'2011)</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9901069411288598,
        0.9994826033283807
      ],
      "excerpt": "    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/tools/data/activitynet/README.md\">ActivityNet</a> (<a href=\"http://activity-net.org/\">Homepage</a>) (CVPR'2015)</td> \n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/tools/data/kinetics/README.md\">Kinetics-[400/600/700]</a> (<a href=\"https://deepmind.com/research/open-source/kinetics/\">Homepage</a>) (CVPR'2017)</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9966656497009679,
        0.9966656497009679,
        0.9569711397466016,
        0.9790129091640919
      ],
      "excerpt": "    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/tools/data/sthv1/README.md\">SthV1</a> (<a href=\"https://20bn.com/datasets/something-something/v1/\">Homepage</a>) (ICCV'2017)</td> \n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/tools/data/sthv2/README.md\">SthV2</a> (<a href=\"https://20bn.com/datasets/something-something/\">Homepage</a>) (ICCV'2017)</td> \n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/tools/data/diving48/README.md\">Diving48</a> (<a href=\"http://www.svcl.ucsd.edu/projects/resound/dataset.html\">Homepage</a>) (ECCV'2018)</td> \n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/tools/data/jester/README.md\">Jester</a> (<a href=\"https://20bn.com/datasets/jester/v1\">Homepage</a>) (ICCV'2019)</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9940536105150153,
        0.9910874297577638,
        0.97771681610941
      ],
      "excerpt": "    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/tools/data/mmit/README.md\">Multi-Moments in Time</a> (<a href=\"http://moments.csail.mit.edu/challenge_iccv_2019.html\">Homepage</a>) (ArXiv'2019)</td> \n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/tools/data/hvu/README.md\">HVU</a> (<a href=\"https://github.com/holistic-video-understanding/HVU-Dataset\">Homepage</a>) (ECCV'2020)</td> \n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/tools/data/omnisource/README.md\">OmniSource</a> (<a href=\"https://kennymckormick.github.io/omnisource/\">Homepage</a>) (ECCV'2020)</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9733788202599862
      ],
      "excerpt": "    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/tools/data/gym/README.md\">FineGYM</a> (<a href=\"https://sdolivia.github.io/FineGym/\">Homepage</a>) (CVPR'2020)</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9115465676107753
      ],
      "excerpt": "    <td colspan=\"4\" style=\"font-weight:bold;\">Action Localization</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8767997868153117,
        0.9901069411288598
      ],
      "excerpt": "    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/tools/data/thumos14/README.md\">THUMOS14</a> (<a href=\"https://www.crcv.ucf.edu/THUMOS14/download.html\">Homepage</a>) (THUMOS Challenge 2014)</td> \n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/tools/data/activitynet/README.md\">ActivityNet</a> (<a href=\"http://activity-net.org/\">Homepage</a>) (CVPR'2015)</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9850821515827285
      ],
      "excerpt": "    <td colspan=\"4\" style=\"font-weight:bold;\">Spatio-Temporal Action Detection</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8445430266171893,
        0.9906838041892104
      ],
      "excerpt": "    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/tools/data/jhmdb/README.md\">JHMDB*</a> (<a href=\"http://jhmdb.is.tue.mpg.de/\">Homepage</a>) (ICCV'2015)</td> \n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/tools/data/ava/README.md\">AVA</a> (<a href=\"https://research.google.com/ava/index.html\">Homepage</a>) (CVPR'2018)</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9785643756558006
      ],
      "excerpt": "    <td colspan=\"4\" style=\"font-weight:bold;\">Skeleton-based Action Recognition</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9877211672009342,
        0.9877211672009342,
        0.9877211672009342,
        0.9877211672009342
      ],
      "excerpt": "    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/tools/data/skeleton/README.md\">PoseC3D-FineGYM</a> (<a href=\"https://kennymckormick.github.io/posec3d/\">Homepage</a>) (ArXiv'2021)</td> \n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/tools/data/skeleton/README.md\">PoseC3D-NTURGB+D</a> (<a href=\"https://kennymckormick.github.io/posec3d/\">Homepage</a>) (ArXiv'2021)</td> \n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/tools/data/skeleton/README.md\">PoseC3D-UCF101</a> (<a href=\"https://kennymckormick.github.io/posec3d/\">Homepage</a>) (ArXiv'2021)</td> \n    <td><a href=\"https://github.com/open-mmlab/mmaction2/blob/master/tools/data/skeleton/README.md\">PoseC3D-HMDB51</a> (<a href=\"https://kennymckormick.github.io/posec3d/\">Homepage</a>) (ArXiv'2021)</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.985476067974435,
        0.99974943861901,
        0.9858452345425824
      ],
      "excerpt": "Video Swin Transformer. [paper][github] \nEvidential Deep Learning for Open Set Action Recognition, ICCV 2021 Oral. [paper][github] \nRethinking Self-supervised Correspondence Learning: A Video Frame-level Similarity Perspective, ICCV 2021 Oral. [paper][github] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9485358500871338
      ],
      "excerpt": "MMCV: OpenMMLab foundational library for computer vision. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9712620773557584
      ],
      "excerpt": "MMAction2: OpenMMLab's next-generation video understanding toolbox and benchmark. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9829315454255052,
        0.8550956278676971
      ],
      "excerpt": "MMDetection3D: OpenMMLab's next-generation platform for general 3D object detection. \nMMEditing: OpenMMLab image and video editing toolbox. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9642443437219912,
        0.9887678291751212,
        0.9530038435323631
      ],
      "excerpt": "MMHuman3D: OpenMMLab human pose and shape estimation toolbox and benchmark. \nMMOCR: A Comprehensive Toolbox for Text Detection, Recognition and Understanding. \nMMPose: OpenMMLab pose estimation toolbox and benchmark. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeOfConduct": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/CODE_OF_CONDUCT.md",
    "technique": "File Exploration"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/open-mmlab/mmaction2",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "We appreciate all contributions to improve MMAction2. Please refer to CONTRIBUTING.md in MMCV for more details about the contributing guideline.",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-07-11T07:19:10Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-27T09:19:35Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "English | [\u7b80\u4f53\u4e2d\u6587](/README_zh-CN.md)\n\n[![Documentation](https://readthedocs.org/projects/mmaction2/badge/?version=latest)](https://mmaction2.readthedocs.io/en/latest/)\n[![actions](https://github.com/open-mmlab/mmaction2/workflows/build/badge.svg)](https://github.com/open-mmlab/mmaction2/actions)\n[![codecov](https://codecov.io/gh/open-mmlab/mmaction2/branch/master/graph/badge.svg)](https://codecov.io/gh/open-mmlab/mmaction2)\n[![PyPI](https://img.shields.io/pypi/v/mmaction2)](https://pypi.org/project/mmaction2/)\n[![LICENSE](https://img.shields.io/github/license/open-mmlab/mmaction2.svg)](https://github.com/open-mmlab/mmaction2/blob/master/LICENSE)\n[![Average time to resolve an issue](https://isitmaintained.com/badge/resolution/open-mmlab/mmaction2.svg)](https://github.com/open-mmlab/mmaction2/issues)\n[![Percentage of issues still open](https://isitmaintained.com/badge/open/open-mmlab/mmaction2.svg)](https://github.com/open-mmlab/mmaction2/issues)\n\nMMAction2 is an open-source toolbox for video understanding based on PyTorch.\nIt is a part of the [OpenMMLab](http://openmmlab.org/) project.\n\nThe master branch works with **PyTorch 1.3+**.\n\n<div align=\"center\">\n  <div style=\"float:left;margin-right:10px;\">\n  <img src=\"https://github.com/open-mmlab/mmaction2/raw/master/resources/mmaction2_overview.gif\" width=\"380px\"><br>\n    <p style=\"font-size:1.5vw;\">Action Recognition Results on Kinetics-400</p>\n  </div>\n  <div style=\"float:right;margin-right:0px;\">\n  <img src=\"https://user-images.githubusercontent.com/34324155/123989146-2ecae680-d9fb-11eb-916b-b9db5563a9e5.gif\" width=\"380px\"><br>\n    <p style=\"font-size:1.5vw;\">Skeleton-base Action Recognition Results on NTU-RGB+D-120</p>\n  </div>\n</div>\n<div align=\"center\">\n  <img src=\"https://github.com/open-mmlab/mmaction2/raw/master/resources/spatio-temporal-det.gif\" width=\"800px\"/><br>\n    <p style=\"font-size:1.5vw;\">Spatio-Temporal Action Detection Results on AVA-2.1</p>\n</div>\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9736214879020163
      ],
      "excerpt": "(2021-10-25) We provide a script(tools/data/skeleton/gen_ntu_rgbd_raw.py) to convert the NTU60 and NTU120 3D raw skeleton data to our format. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.926502571889532,
        0.805635738050114
      ],
      "excerpt": "(2021-10-16) We support PoseC3D on UCF101 and HMDB51, achieves 87.0% and 69.3% Top-1 accuracy with 2D skeletons only. Pre-extracted 2D skeletons are also available. \nRelease: v0.20.0 was released in 30/10/2021. Please refer to changelog.md for details and release history. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8188193618396273,
        0.8855837060014499
      ],
      "excerpt": "A summary can be found on the model zoo page. \nWe will keep up with the latest progress of the community and support more popular algorithms and frameworks. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8230144243246138,
        0.9951354873601783,
        0.9493487973233552
      ],
      "excerpt": "Datasets marked with * are not fully supported yet, but related dataset preparation steps are provided. A summary can be found on the Supported Datasets page. \nTo demonstrate the efficacy and efficiency of our framework, we compare MMAction2 with some other popular frameworks and official releases in terms of speed. Details can be found in benchmark. \nCurrently, there are many research works and projects built on MMAction2 by users from community, such as: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8263017574802516,
        0.9849447025035514
      ],
      "excerpt": "etc., check projects.md to see all related projects. \nWe appreciate all contributions to improve MMAction2. Please refer to CONTRIBUTING.md in MMCV for more details about the contributing guideline. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "OpenMMLab's Next Generation Video Understanding Toolbox and Benchmark",
      "technique": "GitHub API"
    }
  ],
  "documentation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "https://mmaction2.readthedocs.io/",
      "technique": "Regular expression"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/open-mmlab/mmaction2/releases",
    "technique": "GitHub API"
  },
  "faq": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please refer to [FAQ](docs/en/faq.md) for frequently asked questions.\n\n",
      "technique": "Header extraction"
    }
  ],
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 476,
      "date": "Tue, 28 Dec 2021 02:00:44 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/open-mmlab/mmaction2/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "open-mmlab/mmaction2",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/docker/Dockerfile",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/docker/serve/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/open-mmlab/mmaction2/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/demo/visualize_heatmap_volume.ipynb",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/demo/mmaction2_tutorial_zh-CN.ipynb",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/demo/mmaction2_tutorial.ipynb",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/demo/demo.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/docker/serve/entrypoint.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/slurm_train.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/slurm_test.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/dist_test.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/dist_train.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/activitynet/extract_frames.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/activitynet/download_feature_annotations.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/activitynet/download_videos.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/activitynet/download_bsn_videos.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/activitynet/download_features.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/activitynet/download_annotations.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/mmit/generate_videos_filelist.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/mmit/extract_frames.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/mmit/extract_rgb_frames.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/mmit/generate_rawframes_filelist.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/mmit/preprocess_data.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/mmit/extract_rgb_frames_opencv.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/gym/extract_frames.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/gym/download_videos.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/gym/download_annotations.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/jester/extract_flow.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/jester/generate_videos_filelist.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/jester/encode_videos.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/jester/generate_rawframes_filelist.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/sthv1/extract_flow.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/sthv1/generate_videos_filelist.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/sthv1/encode_videos.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/sthv1/generate_rawframes_filelist.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/diving48/generate_videos_filelist.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/diving48/extract_frames.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/diving48/extract_rgb_frames.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/diving48/generate_rawframes_filelist.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/diving48/download_videos.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/diving48/extract_rgb_frames_opencv.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/diving48/download_annotations.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/mit/generate_videos_filelist.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/mit/extract_frames.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/mit/extract_rgb_frames.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/mit/generate_rawframes_filelist.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/mit/preprocess_data.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/mit/extract_rgb_frames_opencv.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/hmdb51/generate_videos_filelist.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/hmdb51/extract_frames.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/hmdb51/extract_rgb_frames.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/hmdb51/generate_rawframes_filelist.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/hmdb51/download_videos.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/hmdb51/extract_rgb_frames_opencv.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/hmdb51/download_annotations.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/ucf101/generate_videos_filelist.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/ucf101/extract_frames.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/ucf101/extract_rgb_frames.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/ucf101/generate_rawframes_filelist.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/ucf101/download_videos.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/ucf101/extract_rgb_frames_opencv.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/ucf101/download_annotations.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/kinetics/generate_videos_filelist.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/kinetics/extract_frames.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/kinetics/extract_rgb_frames.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/kinetics/generate_rawframes_filelist.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/kinetics/download_videos.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/kinetics/download_backup_annotations.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/kinetics/extract_rgb_frames_opencv.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/kinetics/download_annotations.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/kinetics/rename_classnames.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/sthv2/generate_videos_filelist.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/sthv2/extract_frames.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/sthv2/extract_rgb_frames.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/sthv2/generate_rawframes_filelist.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/sthv2/extract_rgb_frames_opencv.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/skeleton/download_annotations.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/hvu/generate_videos_filelist.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/hvu/extract_frames.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/hvu/generate_rawframes_filelist.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/hvu/download_videos.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/hvu/download_annotations.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/thumos14/extract_frames.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/thumos14/extract_rgb_frames.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/thumos14/denormalize_proposal_file.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/thumos14/download_videos.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/thumos14/fetch_tag_proposals.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/thumos14/extract_rgb_frames_opencv.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/thumos14/download_annotations.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/ava/extract_frames.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/ava/extract_rgb_frames.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/ava/download_videos_parallel.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/ava/fetch_ava_proposals.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/ava/cut_videos.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/ava/extract_rgb_frames_ffmpeg.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/ava/download_videos.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/ava/download_videos_gnu_parallel.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/data/ava/download_annotations.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/tools/misc/dist_clip_feature_extraction.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/docs/zh_cn/merge_docs.sh",
      "https://raw.githubusercontent.com/open-mmlab/mmaction2/master/docs/en/merge_docs.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please refer to [data_preparation.md](docs/en/data_preparation.md) for a general knowledge of data preparation.\nThe supported datasets are listed in [supported_datasets.md](docs/en/supported_datasets.md)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Please refer to [install.md](docs/en/install.md) for installation.\n\n",
      "technique": "Header extraction"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/open-mmlab/mmaction2/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell",
      "Dockerfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Apache License 2.0",
      "url": "https://api.github.com/licenses/apache-2.0"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'Copyright 2018-2019 Open-MMLab. All rights reserved.\\n\\n                                 Apache License\\n                           Version 2.0, January 2004\\n                        http://www.apache.org/licenses/\\n\\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\\n\\n   1. Definitions.\\n\\n      \"License\" shall mean the terms and conditions for use, reproduction,\\n      and distribution as defined by Sections 1 through 9 of this document.\\n\\n      \"Licensor\" shall mean the copyright owner or entity authorized by\\n      the copyright owner that is granting the License.\\n\\n      \"Legal Entity\" shall mean the union of the acting entity and all\\n      other entities that control, are controlled by, or are under common\\n      control with that entity. For the purposes of this definition,\\n      \"control\" means (i) the power, direct or indirect, to cause the\\n      direction or management of such entity, whether by contract or\\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\\n      outstanding shares, or (iii) beneficial ownership of such entity.\\n\\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\\n      exercising permissions granted by this License.\\n\\n      \"Source\" form shall mean the preferred form for making modifications,\\n      including but not limited to software source code, documentation\\n      source, and configuration files.\\n\\n      \"Object\" form shall mean any form resulting from mechanical\\n      transformation or translation of a Source form, including but\\n      not limited to compiled object code, generated documentation,\\n      and conversions to other media types.\\n\\n      \"Work\" shall mean the work of authorship, whether in Source or\\n      Object form, made available under the License, as indicated by a\\n      copyright notice that is included in or attached to the work\\n      (an example is provided in the Appendix below).\\n\\n      \"Derivative Works\" shall mean any work, whether in Source or Object\\n      form, that is based on (or derived from) the Work and for which the\\n      editorial revisions, annotations, elaborations, or other modifications\\n      represent, as a whole, an original work of authorship. For the purposes\\n      of this License, Derivative Works shall not include works that remain\\n      separable from, or merely link (or bind by name) to the interfaces of,\\n      the Work and Derivative Works thereof.\\n\\n      \"Contribution\" shall mean any work of authorship, including\\n      the original version of the Work and any modifications or additions\\n      to that Work or Derivative Works thereof, that is intentionally\\n      submitted to Licensor for inclusion in the Work by the copyright owner\\n      or by an individual or Legal Entity authorized to submit on behalf of\\n      the copyright owner. For the purposes of this definition, \"submitted\"\\n      means any form of electronic, verbal, or written communication sent\\n      to the Licensor or its representatives, including but not limited to\\n      communication on electronic mailing lists, source code control systems,\\n      and issue tracking systems that are managed by, or on behalf of, the\\n      Licensor for the purpose of discussing and improving the Work, but\\n      excluding communication that is conspicuously marked or otherwise\\n      designated in writing by the copyright owner as \"Not a Contribution.\"\\n\\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\\n      on behalf of whom a Contribution has been received by Licensor and\\n      subsequently incorporated within the Work.\\n\\n   2. Grant of Copyright License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      copyright license to reproduce, prepare Derivative Works of,\\n      publicly display, publicly perform, sublicense, and distribute the\\n      Work and such Derivative Works in Source or Object form.\\n\\n   3. Grant of Patent License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      (except as stated in this section) patent license to make, have made,\\n      use, offer to sell, sell, import, and otherwise transfer the Work,\\n      where such license applies only to those patent claims licensable\\n      by such Contributor that are necessarily infringed by their\\n      Contribution(s) alone or by combination of their Contribution(s)\\n      with the Work to which such Contribution(s) was submitted. If You\\n      institute patent litigation against any entity (including a\\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\\n      or a Contribution incorporated within the Work constitutes direct\\n      or contributory patent infringement, then any patent licenses\\n      granted to You under this License for that Work shall terminate\\n      as of the date such litigation is filed.\\n\\n   4. Redistribution. You may reproduce and distribute copies of the\\n      Work or Derivative Works thereof in any medium, with or without\\n      modifications, and in Source or Object form, provided that You\\n      meet the following conditions:\\n\\n      (a) You must give any other recipients of the Work or\\n          Derivative Works a copy of this License; and\\n\\n      (b) You must cause any modified files to carry prominent notices\\n          stating that You changed the files; and\\n\\n      (c) You must retain, in the Source form of any Derivative Works\\n          that You distribute, all copyright, patent, trademark, and\\n          attribution notices from the Source form of the Work,\\n          excluding those notices that do not pertain to any part of\\n          the Derivative Works; and\\n\\n      (d) If the Work includes a \"NOTICE\" text file as part of its\\n          distribution, then any Derivative Works that You distribute must\\n          include a readable copy of the attribution notices contained\\n          within such NOTICE file, excluding those notices that do not\\n          pertain to any part of the Derivative Works, in at least one\\n          of the following places: within a NOTICE text file distributed\\n          as part of the Derivative Works; within the Source form or\\n          documentation, if provided along with the Derivative Works; or,\\n          within a display generated by the Derivative Works, if and\\n          wherever such third-party notices normally appear. The contents\\n          of the NOTICE file are for informational purposes only and\\n          do not modify the License. You may add Your own attribution\\n          notices within Derivative Works that You distribute, alongside\\n          or as an addendum to the NOTICE text from the Work, provided\\n          that such additional attribution notices cannot be construed\\n          as modifying the License.\\n\\n      You may add Your own copyright statement to Your modifications and\\n      may provide additional or different license terms and conditions\\n      for use, reproduction, or distribution of Your modifications, or\\n      for any such Derivative Works as a whole, provided Your use,\\n      reproduction, and distribution of the Work otherwise complies with\\n      the conditions stated in this License.\\n\\n   5. Submission of Contributions. Unless You explicitly state otherwise,\\n      any Contribution intentionally submitted for inclusion in the Work\\n      by You to the Licensor shall be under the terms and conditions of\\n      this License, without any additional terms or conditions.\\n      Notwithstanding the above, nothing herein shall supersede or modify\\n      the terms of any separate license agreement you may have executed\\n      with Licensor regarding such Contributions.\\n\\n   6. Trademarks. This License does not grant permission to use the trade\\n      names, trademarks, service marks, or product names of the Licensor,\\n      except as required for reasonable and customary use in describing the\\n      origin of the Work and reproducing the content of the NOTICE file.\\n\\n   7. Disclaimer of Warranty. Unless required by applicable law or\\n      agreed to in writing, Licensor provides the Work (and each\\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\\n      implied, including, without limitation, any warranties or conditions\\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\\n      PARTICULAR PURPOSE. You are solely responsible for determining the\\n      appropriateness of using or redistributing the Work and assume any\\n      risks associated with Your exercise of permissions under this License.\\n\\n   8. Limitation of Liability. In no event and under no legal theory,\\n      whether in tort (including negligence), contract, or otherwise,\\n      unless required by applicable law (such as deliberate and grossly\\n      negligent acts) or agreed to in writing, shall any Contributor be\\n      liable to You for damages, including any direct, indirect, special,\\n      incidental, or consequential damages of any character arising as a\\n      result of this License or out of the use or inability to use the\\n      Work (including but not limited to damages for loss of goodwill,\\n      work stoppage, computer failure or malfunction, or any and all\\n      other commercial damages or losses), even if such Contributor\\n      has been advised of the possibility of such damages.\\n\\n   9. Accepting Warranty or Additional Liability. While redistributing\\n      the Work or Derivative Works thereof, You may choose to offer,\\n      and charge a fee for, acceptance of support, warranty, indemnity,\\n      or other liability obligations and/or rights consistent with this\\n      License. However, in accepting such obligations, You may act only\\n      on Your own behalf and on Your sole responsibility, not on behalf\\n      of any other Contributor, and only if You agree to indemnify,\\n      defend, and hold each Contributor harmless for any liability\\n      incurred by, or claims asserted against, such Contributor by reason\\n      of your accepting any such warranty or additional liability.\\n\\n   END OF TERMS AND CONDITIONS\\n\\n   APPENDIX: How to apply the Apache License to your work.\\n\\n      To apply the Apache License to your work, attach the following\\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\\n      replaced with your own identifying information. (Don\\'t include\\n      the brackets!)  The text should be enclosed in the appropriate\\n      comment syntax for the file format. We also recommend that a\\n      file or class name and description of purpose be included on the\\n      same \"printed page\" as the copyright notice for easier\\n      identification within third-party archives.\\n\\n   Copyright 2018-2019 Open-MMLab.\\n\\n   Licensed under the Apache License, Version 2.0 (the \"License\");\\n   you may not use this file except in compliance with the License.\\n   You may obtain a copy of the License at\\n\\n       http://www.apache.org/licenses/LICENSE-2.0\\n\\n   Unless required by applicable law or agreed to in writing, software\\n   distributed under the License is distributed on an \"AS IS\" BASIS,\\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n   See the License for the specific language governing permissions and\\n   limitations under the License.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Introduction",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "mmaction2",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "open-mmlab",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/open-mmlab/mmaction2/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "kennymckormick",
        "body": "**Highlights**\r\n\r\n- Support TorchServe\r\n- Add video structuralize demo\r\n- Support using 3D skeletons for skeleton-based action recognition\r\n- Benchmark PoseC3D on UCF and HMDB\r\n\r\n**New Features**\r\n\r\n- Support TorchServe ([#1212](https://github.com/open-mmlab/mmaction2/pull/1212))\r\n- Support 3D skeletons pre-processing ([#1218](https://github.com/open-mmlab/mmaction2/pull/1218))\r\n- Support video structuralize demo ([#1197](https://github.com/open-mmlab/mmaction2/pull/1197))\r\n\r\n**Documentations**\r\n\r\n- Revise README.md and add projects.md ([#1214](https://github.com/open-mmlab/mmaction2/pull/1214))\r\n- Add CN docs for Skeleton dataset, PoseC3D and ST-GCN ([#1228](https://github.com/open-mmlab/mmaction2/pull/1228), [#1237](https://github.com/open-mmlab/mmaction2/pull/1237), [#1236](https://github.com/open-mmlab/mmaction2/pull/1236))\r\n- Add tutorial for custom dataset training for skeleton-based action recognition ([#1234](https://github.com/open-mmlab/mmaction2/pull/1234))\r\n\r\n**Bug and Typo Fixes**\r\n\r\n- Fix tutorial link ([#1219](https://github.com/open-mmlab/mmaction2/pull/1219))\r\n- Fix GYM links ([#1224](https://github.com/open-mmlab/mmaction2/pull/1224))\r\n\r\n**ModelZoo**\r\n\r\n- Benchmark PoseC3D on UCF and HMDB ([#1223](https://github.com/open-mmlab/mmaction2/pull/1223))\r\n- Add ST-GCN + 3D skeleton model for NTU60-XSub ([#1236](https://github.com/open-mmlab/mmaction2/pull/1236))\r\n\r\n## New Contributors\r\n* @bit-scientist made their first contribution in https://github.com/open-mmlab/mmaction2/pull/1234\r\n\r\n**Full Changelog**: https://github.com/open-mmlab/mmaction2/compare/v0.19.0...v0.20.0",
        "dateCreated": "2021-10-30T03:57:25Z",
        "datePublished": "2021-10-30T03:59:31Z",
        "html_url": "https://github.com/open-mmlab/mmaction2/releases/tag/v0.20.0",
        "name": "MMAction2 V0.20.0 Release",
        "tag_name": "v0.20.0",
        "tarball_url": "https://api.github.com/repos/open-mmlab/mmaction2/tarball/v0.20.0",
        "url": "https://api.github.com/repos/open-mmlab/mmaction2/releases/52358719",
        "zipball_url": "https://api.github.com/repos/open-mmlab/mmaction2/zipball/v0.20.0"
      },
      {
        "authorType": "User",
        "author_name": "kennymckormick",
        "body": "**Highlights**\r\n\r\n- Support ST-GCN\r\n- Refactor the inference API\r\n- Add code spell check hook\r\n\r\n**New Features**\r\n\r\n- Support ST-GCN ([#1123](https://github.com/open-mmlab/mmaction2/pull/1123))\r\n\r\n**Improvement**\r\n\r\n- Add label maps for every dataset ([#1127](https://github.com/open-mmlab/mmaction2/pull/1127))\r\n- Remove useless code MultiGroupCrop ([#1180](https://github.com/open-mmlab/mmaction2/pull/1180))\r\n- Refactor Inference API ([#1191](https://github.com/open-mmlab/mmaction2/pull/1191))\r\n- Add code spell check hook ([#1208](https://github.com/open-mmlab/mmaction2/pull/1208))\r\n- Use docker in CI ([#1159](https://github.com/open-mmlab/mmaction2/pull/1159))\r\n\r\n**Documentations**\r\n\r\n- Update metafiles to new OpenMMLAB protocols ([#1134](https://github.com/open-mmlab/mmaction2/pull/1134))\r\n- Switch to new doc style ([#1160](https://github.com/open-mmlab/mmaction2/pull/1160))\r\n- Improve the ERROR message ([#1203](https://github.com/open-mmlab/mmaction2/pull/1203))\r\n- Fix invalid URL in getting_started ([#1169](https://github.com/open-mmlab/mmaction2/pull/1169))\r\n\r\n**Bug and Typo Fixes**\r\n\r\n- Compatible with new MMClassification ([#1139](https://github.com/open-mmlab/mmaction2/pull/1139))\r\n- Add missing runtime dependencies ([#1144](https://github.com/open-mmlab/mmaction2/pull/1144))\r\n- Fix THUMOS tag proposals path ([#1156](https://github.com/open-mmlab/mmaction2/pull/1156))\r\n- Fix LoadHVULabel ([#1194](https://github.com/open-mmlab/mmaction2/pull/1194))\r\n- Switch the default value of `persistent_workers` to False ([#1202](https://github.com/open-mmlab/mmaction2/pull/1202))\r\n- Fix `_freeze_stages` for MobileNetV2 ([#1193](https://github.com/open-mmlab/mmaction2/pull/1193))\r\n- Fix resume when building rawframes ([#1150](https://github.com/open-mmlab/mmaction2/pull/1150))\r\n- Fix device bug for class weight ([#1188](https://github.com/open-mmlab/mmaction2/pull/1188))\r\n- Correct Arg names in extract_audio.py ([#1148](https://github.com/open-mmlab/mmaction2/pull/1148))\r\n\r\n**ModelZoo**\r\n\r\n- Add TSM-MobileNetV2 ported from TSM ([#1163](https://github.com/open-mmlab/mmaction2/pull/1163))\r\n- Add ST-GCN for NTURGB+D-XSub-60 ([#1123](https://github.com/open-mmlab/mmaction2/pull/1123))",
        "dateCreated": "2021-10-07T16:12:53Z",
        "datePublished": "2021-10-07T16:13:47Z",
        "html_url": "https://github.com/open-mmlab/mmaction2/releases/tag/v0.19.0",
        "name": "MMAction2 V0.19.0 Release",
        "tag_name": "v0.19.0",
        "tarball_url": "https://api.github.com/repos/open-mmlab/mmaction2/tarball/v0.19.0",
        "url": "https://api.github.com/repos/open-mmlab/mmaction2/releases/50978509",
        "zipball_url": "https://api.github.com/repos/open-mmlab/mmaction2/zipball/v0.19.0"
      },
      {
        "authorType": "User",
        "author_name": "kennymckormick",
        "body": "**Improvement**\r\n\r\n- Add CopyRight ([#1099](https://github.com/open-mmlab/mmaction2/pull/1099))\r\n- Support NTU Pose Extraction ([#1076](https://github.com/open-mmlab/mmaction2/pull/1076))\r\n- Support Caching in RawFrameDecode ([#1078](https://github.com/open-mmlab/mmaction2/pull/1078))\r\n- Add citations & Support python3.9 CI & Use fixed-version sphinx ([#1125](https://github.com/open-mmlab/mmaction2/pull/1125))\r\n\r\n**Documentations**\r\n\r\n- Add Descriptions of PoseC3D dataset ([#1053](https://github.com/open-mmlab/mmaction2/pull/1053))\r\n\r\n**Bug and Typo Fixes**\r\n\r\n- Fix SSV2 checkpoints ([#1101](https://github.com/open-mmlab/mmaction2/pull/1101))\r\n- Fix CSN normalization ([#1116](https://github.com/open-mmlab/mmaction2/pull/1116))\r\n- Fix typo ([#1121](https://github.com/open-mmlab/mmaction2/pull/1121))\r\n- Fix new_crop_quadruple bug ([#1108](https://github.com/open-mmlab/mmaction2/pull/1108))",
        "dateCreated": "2021-09-02T07:54:52Z",
        "datePublished": "2021-09-02T07:56:54Z",
        "html_url": "https://github.com/open-mmlab/mmaction2/releases/tag/v0.18.0",
        "name": "MMAction2 V0.18.0 Release",
        "tag_name": "v0.18.0",
        "tarball_url": "https://api.github.com/repos/open-mmlab/mmaction2/tarball/v0.18.0",
        "url": "https://api.github.com/repos/open-mmlab/mmaction2/releases/48870542",
        "zipball_url": "https://api.github.com/repos/open-mmlab/mmaction2/zipball/v0.18.0"
      },
      {
        "authorType": "User",
        "author_name": "kennymckormick",
        "body": "**Highlights**\r\n\r\n- Support PyTorch 1.9\r\n- Support Pytorchvideo Transforms\r\n- Support PreciseBN\r\n\r\n**New Features**\r\n\r\n- Support Pytorchvideo Transforms ([#1008](https://github.com/open-mmlab/mmaction2/pull/1008))\r\n- Support PreciseBN ([#1038](https://github.com/open-mmlab/mmaction2/pull/1038))\r\n\r\n**Improvements**\r\n\r\n- Remove redundant augmentations in config files ([#996](https://github.com/open-mmlab/mmaction2/pull/996))\r\n- Make resource directory to hold common resource pictures ([#1011](https://github.com/open-mmlab/mmaction2/pull/1011))\r\n- Remove deperecated FrameSelector ([#1010](https://github.com/open-mmlab/mmaction2/pull/1010))\r\n- Support Concat Dataset ([#1000](https://github.com/open-mmlab/mmaction2/pull/1000))\r\n- Add `to-mp4` option to resize_videos.py ([#1021](https://github.com/open-mmlab/mmaction2/pull/1021))\r\n- Add option to keep tail frames ([#1050](https://github.com/open-mmlab/mmaction2/pull/1050))\r\n- Update MIM support ([#1061](https://github.com/open-mmlab/mmaction2/pull/1061))\r\n- Calculate Top-K accurate and inaccurate classes ([#1047](https://github.com/open-mmlab/mmaction2/pull/1047))\r\n\r\n**Bug and Typo Fixes**\r\n\r\n- Fix bug in PoseC3D demo ([#1009](https://github.com/open-mmlab/mmaction2/pull/1009))\r\n- Fix some problems in resize_videos.py ([#1012](https://github.com/open-mmlab/mmaction2/pull/1012))\r\n- Support torch1.9 ([#1015](https://github.com/open-mmlab/mmaction2/pull/1015))\r\n- Remove redundant code in CI ([#1046](https://github.com/open-mmlab/mmaction2/pull/1046))\r\n- Fix bug about persistent_workers ([#1044](https://github.com/open-mmlab/mmaction2/pull/1044))\r\n- Support TimeSformer feature extraction ([#1035](https://github.com/open-mmlab/mmaction2/pull/1035))\r\n- Fix ColorJitter ([#1025](https://github.com/open-mmlab/mmaction2/pull/1025))\r\n\r\n**ModelZoo**\r\n\r\n- Add TSM-R50 sthv1 models trained by PytorchVideo RandAugment and AugMix ([#1008](https://github.com/open-mmlab/mmaction2/pull/1008))\r\n- Update SlowOnly SthV1 checkpoints ([#1034](https://github.com/open-mmlab/mmaction2/pull/1034))\r\n- Add SlowOnly Kinetics400 checkpoints trained with Precise-BN ([#1038](https://github.com/open-mmlab/mmaction2/pull/1038))\r\n- Add CSN-R50 from scratch checkpoints ([#1045](https://github.com/open-mmlab/mmaction2/pull/1045))\r\n- TPN Kinetics-400 Checkpoints trained with the new ColorJitter ([#1025](https://github.com/open-mmlab/mmaction2/pull/1025))\r\n\r\n**Documentation**\r\n\r\n- Add Chinese translation of feature_extraction.md ([#1020](https://github.com/open-mmlab/mmaction2/pull/1020))\r\n- Fix the code snippet in getting_started.md ([#1023](https://github.com/open-mmlab/mmaction2/pull/1023))\r\n- Fix TANet config table ([#1028](https://github.com/open-mmlab/mmaction2/pull/1028))\r\n- Add description to PoseC3D dataset ([#1053](https://github.com/open-mmlab/mmaction2/pull/1053))",
        "dateCreated": "2021-08-03T04:49:56Z",
        "datePublished": "2021-08-03T04:56:47Z",
        "html_url": "https://github.com/open-mmlab/mmaction2/releases/tag/v0.17.0",
        "name": "MMAction2 V0.17.0 Release",
        "tag_name": "v0.17.0",
        "tarball_url": "https://api.github.com/repos/open-mmlab/mmaction2/tarball/v0.17.0",
        "url": "https://api.github.com/repos/open-mmlab/mmaction2/releases/47199367",
        "zipball_url": "https://api.github.com/repos/open-mmlab/mmaction2/zipball/v0.17.0"
      },
      {
        "authorType": "User",
        "author_name": "kennymckormick",
        "body": "**Highlights**\r\n\r\n- Support using backbone from pytorch-image-models(timm)\r\n- Support PIMS Decoder\r\n- Demo for skeleton-based action recognition\r\n- Support Timesformer\r\n\r\n**New Features**\r\n\r\n- Support using backbones from pytorch-image-models(timm) for TSN ([#880](https://github.com/open-mmlab/mmaction2/pull/880))\r\n- Support torchvision transformations in preprocessing pipelines ([#972](https://github.com/open-mmlab/mmaction2/pull/972))\r\n- Demo for skeleton-based action recognition ([#972](https://github.com/open-mmlab/mmaction2/pull/972))\r\n- Support Timesformer ([#839](https://github.com/open-mmlab/mmaction2/pull/839))\r\n\r\n**Improvements**\r\n\r\n- Add a tool to find invalid videos ([#907](https://github.com/open-mmlab/mmaction2/pull/907), [#950](https://github.com/open-mmlab/mmaction2/pull/950))\r\n- Add an option to specify spectrogram_type ([#909](https://github.com/open-mmlab/mmaction2/pull/909))\r\n- Add json output to video demo ([#906](https://github.com/open-mmlab/mmaction2/pull/906))\r\n- Add MIM related docs ([#918](https://github.com/open-mmlab/mmaction2/pull/918))\r\n- Rename lr to scheduler ([#916](https://github.com/open-mmlab/mmaction2/pull/916))\r\n- Support `--cfg-options` for demos ([#911](https://github.com/open-mmlab/mmaction2/pull/911))\r\n- Support number counting for flow-wise filename template ([#922](https://github.com/open-mmlab/mmaction2/pull/922))\r\n- Add Chinese tutorial ([#941](https://github.com/open-mmlab/mmaction2/pull/941))\r\n- Change ResNet3D default values ([#939](https://github.com/open-mmlab/mmaction2/pull/939))\r\n- Adjust script structure ([#935](https://github.com/open-mmlab/mmaction2/pull/935))\r\n- Add font color to args in long_video_demo ([#947](https://github.com/open-mmlab/mmaction2/pull/947))\r\n- Polish code style with Pylint ([#908](https://github.com/open-mmlab/mmaction2/pull/908))\r\n- Support PIMS Decoder ([#946](https://github.com/open-mmlab/mmaction2/pull/946))\r\n- Improve Metafiles ([#956](https://github.com/open-mmlab/mmaction2/pull/956), [#979](https://github.com/open-mmlab/mmaction2/pull/979), [#966](https://github.com/open-mmlab/mmaction2/pull/966))\r\n- Add links to download Kinetics400 validation ([#920](https://github.com/open-mmlab/mmaction2/pull/920))\r\n- Audit the usage of shutil.rmtree ([#943](https://github.com/open-mmlab/mmaction2/pull/943))\r\n- Polish localizer related codes([#913](https://github.com/open-mmlab/mmaction2/pull/913))\r\n\r\n**Bug and Typo Fixes**\r\n\r\n- Fix spatiotemporal detection demo ([#899](https://github.com/open-mmlab/mmaction2/pull/899))\r\n- Fix docstring for 3D inflate ([#925](https://github.com/open-mmlab/mmaction2/pull/925))\r\n- Fix bug of writing text to video with TextClip ([#952](https://github.com/open-mmlab/mmaction2/pull/952))\r\n- Fix mmcv install in CI ([#977](https://github.com/open-mmlab/mmaction2/pull/977))\r\n\r\n**ModelZoo**\r\n\r\n- Add TSN with Swin Transformer backbone as an example for using pytorch-image-models(timm) backbones ([#880](https://github.com/open-mmlab/mmaction2/pull/880))\r\n- Port CSN checkpoints from VMZ ([#945](https://github.com/open-mmlab/mmaction2/pull/945))\r\n- Release various checkpoints for UCF101, HMDB51 and Sthv1 ([#938](https://github.com/open-mmlab/mmaction2/pull/938))\r\n- Support Timesformer ([#839](https://github.com/open-mmlab/mmaction2/pull/839))\r\n- Update TSM modelzoo ([#981](https://github.com/open-mmlab/mmaction2/pull/981))",
        "dateCreated": "2021-07-01T09:54:43Z",
        "datePublished": "2021-07-01T09:56:21Z",
        "html_url": "https://github.com/open-mmlab/mmaction2/releases/tag/v0.16.0",
        "name": "MMAction2 V0.16.0 Release",
        "tag_name": "v0.16.0",
        "tarball_url": "https://api.github.com/repos/open-mmlab/mmaction2/tarball/v0.16.0",
        "url": "https://api.github.com/repos/open-mmlab/mmaction2/releases/45548033",
        "zipball_url": "https://api.github.com/repos/open-mmlab/mmaction2/zipball/v0.16.0"
      },
      {
        "authorType": "User",
        "author_name": "dreamerlin",
        "body": "**Highlights**\r\n\r\n- Support PoseC3D\r\n- Support ACRN\r\n- Support MIM\r\n\r\n**New Features**\r\n\r\n- Support PoseC3D ([#786](https://github.com/open-mmlab/mmaction2/pull/786), [#890](https://github.com/open-mmlab/mmaction2/pull/890))\r\n- Support MIM ([#870](https://github.com/open-mmlab/mmaction2/pull/870))\r\n- Support ACRN and Focal Loss ([#891](https://github.com/open-mmlab/mmaction2/pull/891))\r\n- Support Jester dataset ([#864](https://github.com/open-mmlab/mmaction2/pull/864))\r\n\r\n**Improvements**\r\n\r\n- Add `metric_options` for evaluation to docs ([#873](https://github.com/open-mmlab/mmaction2/pull/873))\r\n- Support creating a new label map based on custom classes for demos about spatio temporal demo ([#879](https://github.com/open-mmlab/mmaction2/pull/879))\r\n- Improve document about AVA dataset preparation ([#878](https://github.com/open-mmlab/mmaction2/pull/878))\r\n- Provide a script to extract clip-level feature ([#856](https://github.com/open-mmlab/mmaction2/pull/856))\r\n\r\n**Bug and Typo Fixes**\r\n\r\n- Fix issues about resume ([#877](https://github.com/open-mmlab/mmaction2/pull/877), [#878](https://github.com/open-mmlab/mmaction2/pull/878))\r\n- Correct the key name of `eval_results` dictionary for metric 'mmit_mean_average_precision' ([#885](https://github.com/open-mmlab/mmaction2/pull/885))\r\n\r\n**ModelZoo**\r\n\r\n- Support Jester dataset ([#864](https://github.com/open-mmlab/mmaction2/pull/864))\r\n- Support ACRN and Focal Loss ([#891](https://github.com/open-mmlab/mmaction2/pull/891))",
        "dateCreated": "2021-05-31T15:12:00Z",
        "datePublished": "2021-05-31T15:13:53Z",
        "html_url": "https://github.com/open-mmlab/mmaction2/releases/tag/v0.15.0",
        "name": "MMAction2 V0.15.0 Release",
        "tag_name": "v0.15.0",
        "tarball_url": "https://api.github.com/repos/open-mmlab/mmaction2/tarball/v0.15.0",
        "url": "https://api.github.com/repos/open-mmlab/mmaction2/releases/43861526",
        "zipball_url": "https://api.github.com/repos/open-mmlab/mmaction2/zipball/v0.15.0"
      },
      {
        "authorType": "User",
        "author_name": "dreamerlin",
        "body": "**Highlights**\r\n\r\n- Support TRN\r\n- Support Diving48\r\n\r\n**New Features**\r\n\r\n- Support TRN ([#755](https://github.com/open-mmlab/mmaction2/pull/755))\r\n- Support Diving48 ([#835](https://github.com/open-mmlab/mmaction2/pull/835))\r\n- Support Webcam Demo for Spatio-temporal Action Detection Models ([#795](https://github.com/open-mmlab/mmaction2/pull/795))\r\n\r\n**Improvements**\r\n\r\n- Add softmax option for pytorch2onnx tool ([#781](https://github.com/open-mmlab/mmaction2/pull/781))\r\n- Support TRN ([#755](https://github.com/open-mmlab/mmaction2/pull/755))\r\n- Test with onnx models and TensorRT engines ([#758](https://github.com/open-mmlab/mmaction2/pull/758))\r\n- Speed up AVA Testing ([#784](https://github.com/open-mmlab/mmaction2/pull/784))\r\n- Add `self.with_neck` attribute ([#796](https://github.com/open-mmlab/mmaction2/pull/796))\r\n- Update installation document ([#798](https://github.com/open-mmlab/mmaction2/pull/798))\r\n- Use a random master port ([#809](https://github.com/open-mmlab/mmaction2/pull/8098))\r\n- Update AVA processing data document ([#801](https://github.com/open-mmlab/mmaction2/pull/801))\r\n- Refactor spatio-temporal augmentation ([#782](https://github.com/open-mmlab/mmaction2/pull/782))\r\n- Add QR code in CN README ([#812](https://github.com/open-mmlab/mmaction2/pull/812))\r\n- Add Alternative way to download Kinetics ([#817](https://github.com/open-mmlab/mmaction2/pull/817), [#822](https://github.com/open-mmlab/mmaction2/pull/822))\r\n- Refactor Sampler ([#790](https://github.com/open-mmlab/mmaction2/pull/790))\r\n- Use EvalHook in MMCV with backward compatibility ([#793](https://github.com/open-mmlab/mmaction2/pull/793))\r\n- Use MMCV Model Registry ([#843](https://github.com/open-mmlab/mmaction2/pull/843))\r\n\r\n**Bug and Typo Fixes**\r\n\r\n- Fix a bug in pytorch2onnx.py when `num_classes <= 4` ([#800](https://github.com/open-mmlab/mmaction2/pull/800), [#824](https://github.com/open-mmlab/mmaction2/pull/824))\r\n- Fix `demo_spatiotemporal_det.py` error ([#803](https://github.com/open-mmlab/mmaction2/pull/803), [#805](https://github.com/open-mmlab/mmaction2/pull/805))\r\n- Fix loading config bugs when resume ([#820](https://github.com/open-mmlab/mmaction2/pull/820))\r\n- Make HMDB51 annotation generation more robust ([#811](https://github.com/open-mmlab/mmaction2/pull/811))\r\n\r\n**ModelZoo**\r\n\r\n- Update checkpoint for 256 height in something-V2 ([#789](https://github.com/open-mmlab/mmaction2/pull/789))\r\n- Support Diving48 ([#835](https://github.com/open-mmlab/mmaction2/pull/835))\r\n",
        "dateCreated": "2021-05-06T03:42:01Z",
        "datePublished": "2021-05-03T06:43:55Z",
        "html_url": "https://github.com/open-mmlab/mmaction2/releases/tag/v0.14.0",
        "name": "MMAction2 V0.14.0 Release",
        "tag_name": "v0.14.0",
        "tarball_url": "https://api.github.com/repos/open-mmlab/mmaction2/tarball/v0.14.0",
        "url": "https://api.github.com/repos/open-mmlab/mmaction2/releases/42343799",
        "zipball_url": "https://api.github.com/repos/open-mmlab/mmaction2/zipball/v0.14.0"
      },
      {
        "authorType": "User",
        "author_name": "dreamerlin",
        "body": "**Highlights**\r\n\r\n- Support LFB\r\n- Support using backbone from MMCls/TorchVision\r\n- Add Chinese documentation\r\n\r\n**New Features**\r\n\r\n- Support LFB ([#553](https://github.com/open-mmlab/mmaction2/pull/553))\r\n- Support using backbones from MMCls for TSN ([#679](https://github.com/open-mmlab/mmaction2/pull/679))\r\n- Support using backbones from TorchVision for TSN ([#720](https://github.com/open-mmlab/mmaction2/pull/720))\r\n- Support Mixup and Cutmix for recognizers ([#681](https://github.com/open-mmlab/mmaction2/pull/681))\r\n- Support Chinese documentation ([#665](https://github.com/open-mmlab/mmaction2/pull/665), [#680](https://github.com/open-mmlab/mmaction2/pull/680), [#689](https://github.com/open-mmlab/mmaction2/pull/689), [#701](https://github.com/open-mmlab/mmaction2/pull/701), [#702](https://github.com/open-mmlab/mmaction2/pull/702), [#703](https://github.com/open-mmlab/mmaction2/pull/703), [#706](https://github.com/open-mmlab/mmaction2/pull/706), [#716](https://github.com/open-mmlab/mmaction2/pull/716), [#717](https://github.com/open-mmlab/mmaction2/pull/717), [#731](https://github.com/open-mmlab/mmaction2/pull/731), [#733](https://github.com/open-mmlab/mmaction2/pull/733), [#735](https://github.com/open-mmlab/mmaction2/pull/735), [#736](https://github.com/open-mmlab/mmaction2/pull/736), [#737](https://github.com/open-mmlab/mmaction2/pull/737), [#738](https://github.com/open-mmlab/mmaction2/pull/738), [#739](https://github.com/open-mmlab/mmaction2/pull/739), [#740](https://github.com/open-mmlab/mmaction2/pull/740), [#742](https://github.com/open-mmlab/mmaction2/pull/742), [#752](https://github.com/open-mmlab/mmaction2/pull/752), [#759](https://github.com/open-mmlab/mmaction2/pull/759), [#761](https://github.com/open-mmlab/mmaction2/pull/761), [#772](https://github.com/open-mmlab/mmaction2/pull/772), [#775](https://github.com/open-mmlab/mmaction2/pull/775))\r\n\r\n**Improvements**\r\n\r\n- Add slowfast config/json/log/ckpt for training custom classes of AVA ([#678](https://github.com/open-mmlab/mmaction2/pull/678))\r\n- Set RandAugment as Imgaug default transforms ([#585](https://github.com/open-mmlab/mmaction2/pull/585))\r\n- Add `--test-last` & `--test-best` for `tools/train.py` to test checkpoints after training ([#608](https://github.com/open-mmlab/mmaction2/pull/608))\r\n- Add fcn_testing in TPN ([#684](https://github.com/open-mmlab/mmaction2/pull/684))\r\n- Remove redundant recall functions ([#741](https://github.com/open-mmlab/mmaction2/pull/741))\r\n- Recursively remove pretrained step for testing ([#695](https://github.com/open-mmlab/mmaction2/pull/695))\r\n- Improve demo by limiting inference fps ([#668](https://github.com/open-mmlab/mmaction2/pull/668))\r\n\r\n**Bug and Typo Fixes**\r\n\r\n- Fix a bug about multi-class in VideoDataset ([#723](https://github.com/open-mmlab/mmaction2/pull/678))\r\n- Reverse key-value in anet filelist generation ([#686](https://github.com/open-mmlab/mmaction2/pull/686))\r\n- Fix flow norm cfg typo ([#693](https://github.com/open-mmlab/mmaction2/pull/693))\r\n\r\n**ModelZoo**\r\n\r\n- Add LFB for AVA2.1 ([#553](https://github.com/open-mmlab/mmaction2/pull/553))\r\n- Add TSN with ResNeXt-101-32x4d backbone as an example for using MMCls backbones ([#679](https://github.com/open-mmlab/mmaction2/pull/679))\r\n- Add TSN with Densenet161 backbone as an example for using TorchVision backbones ([#720](https://github.com/open-mmlab/mmaction2/pull/720))\r\n- Add slowonly_nl_embedded_gaussian_r50_4x16x1_150e_kinetics400_rgb ([#690](https://github.com/open-mmlab/mmaction2/pull/690))\r\n- Add slowonly_nl_embedded_gaussian_r50_8x8x1_150e_kinetics400_rgb ([#704](https://github.com/open-mmlab/mmaction2/pull/704))\r\n- Add slowonly_nl_kinetics_pretrained_r50_4x16x1(8x8x1)_20e_ava_rgb ([#730](https://github.com/open-mmlab/mmaction2/pull/730))",
        "dateCreated": "2021-04-01T08:09:38Z",
        "datePublished": "2021-04-01T11:02:20Z",
        "html_url": "https://github.com/open-mmlab/mmaction2/releases/tag/v0.13.0",
        "name": "MMAction2 V0.13.0 Release",
        "tag_name": "v0.13.0",
        "tarball_url": "https://api.github.com/repos/open-mmlab/mmaction2/tarball/v0.13.0",
        "url": "https://api.github.com/repos/open-mmlab/mmaction2/releases/40813079",
        "zipball_url": "https://api.github.com/repos/open-mmlab/mmaction2/zipball/v0.13.0"
      },
      {
        "authorType": "User",
        "author_name": "innerlee",
        "body": "**Highlights**\r\n\r\n- Support TSM-MobileNetV2\r\n- Support TANet\r\n- Support GPU Normalize\r\n\r\n**New Features**\r\n\r\n- Support TSM-MobileNetV2 ([#415](https://github.com/open-mmlab/mmaction2/pull/415))\r\n- Support flip with label mapping ([#591](https://github.com/open-mmlab/mmaction2/pull/591))\r\n- Add seed option for sampler ([#642](https://github.com/open-mmlab/mmaction2/pull/642))\r\n- Support GPU Normalize ([#586](https://github.com/open-mmlab/mmaction2/pull/586))\r\n- Support TANet ([#595](https://github.com/open-mmlab/mmaction2/pull/595))\r\n\r\n**Improvements**\r\n\r\n- Training custom classes of ava dataset ([#555](https://github.com/open-mmlab/mmaction2/pull/555))\r\n- Add CN README in homepage ([#592](https://github.com/open-mmlab/mmaction2/pull/592), [#594](https://github.com/open-mmlab/mmaction2/pull/594))\r\n- Support soft label for CrossEntropyLoss ([#625](https://github.com/open-mmlab/mmaction2/pull/625))\r\n- Refactor config: Specify `train_cfg` and `test_cfg` in `model` ([#629](https://github.com/open-mmlab/mmaction2/pull/629))\r\n- Provide an alternative way to download older kinetics annotations ([#597](https://github.com/open-mmlab/mmaction2/pull/597))\r\n- Update FAQ for\r\n  - 1). data pipeline about video and frames ([#598](https://github.com/open-mmlab/mmaction2/pull/598))\r\n  - 2). how to show results ([#598](https://github.com/open-mmlab/mmaction2/pull/598))\r\n  - 3). batch size setting for batchnorm ([#657](https://github.com/open-mmlab/mmaction2/pull/657))\r\n  - 4). how to fix stages of backbone when finetuning models ([#658](https://github.com/open-mmlab/mmaction2/pull/658))\r\n- Modify default value of `save_best` ([#600](https://github.com/open-mmlab/mmaction2/pull/600))\r\n- Use BibTex rather than latex in markdown ([#607](https://github.com/open-mmlab/mmaction2/pull/607))\r\n- Add warnings of uninstalling mmdet and supplementary documents ([#624](https://github.com/open-mmlab/mmaction2/pull/624))\r\n- Support soft label for CrossEntropyLoss ([#625](https://github.com/open-mmlab/mmaction2/pull/625))\r\n\r\n**Bug and Typo Fixes**\r\n\r\n- Fix value of `pem_low_temporal_iou_threshold` in BSN ([#556](https://github.com/open-mmlab/mmaction2/pull/556))\r\n- Fix ActivityNet download script ([#601](https://github.com/open-mmlab/mmaction2/pull/601))\r\n\r\n**ModelZoo**\r\n\r\n- Add TSM-MobileNetV2 for Kinetics400 ([#415](https://github.com/open-mmlab/mmaction2/pull/415))\r\n- Add deeper SlowFast models ([#605](https://github.com/open-mmlab/mmaction2/pull/605))",
        "dateCreated": "2021-02-28T16:12:19Z",
        "datePublished": "2021-03-01T02:00:15Z",
        "html_url": "https://github.com/open-mmlab/mmaction2/releases/tag/v0.12.0",
        "name": "MMAction2 V0.12.0 Release",
        "tag_name": "v0.12.0",
        "tarball_url": "https://api.github.com/repos/open-mmlab/mmaction2/tarball/v0.12.0",
        "url": "https://api.github.com/repos/open-mmlab/mmaction2/releases/39030796",
        "zipball_url": "https://api.github.com/repos/open-mmlab/mmaction2/zipball/v0.12.0"
      },
      {
        "authorType": "User",
        "author_name": "innerlee",
        "body": "**Highlights**\r\n\r\n- Support imgaug\r\n- Support spatial temporal demo\r\n- Refactor EvalHook, config structure, unittest structure\r\n\r\n**New Features**\r\n\r\n- Support [imgaug](https://imgaug.readthedocs.io/en/latest/index.html) for augmentations in the data pipeline ([#492](https://github.com/open-mmlab/mmaction2/pull/492))\r\n- Support setting `max_testing_views` for extremely large models to save GPU memory used ([#511](https://github.com/open-mmlab/mmaction2/pull/511))\r\n- Add spatial temporal demo ([#547](https://github.com/open-mmlab/mmaction2/pull/547), [#566](https://github.com/open-mmlab/mmaction2/pull/566))\r\n\r\n**Improvements**\r\n\r\n- Refactor EvalHook ([#395](https://github.com/open-mmlab/mmaction2/pull/395))\r\n- Refactor AVA hook ([#567](https://github.com/open-mmlab/mmaction2/pull/567))\r\n- Add repo citation ([#545](https://github.com/open-mmlab/mmaction2/pull/545))\r\n- Add dataset size of Kinetics400 ([#503](https://github.com/open-mmlab/mmaction2/pull/503))\r\n- Add lazy operation docs ([#504](https://github.com/open-mmlab/mmaction2/pull/504))\r\n- Add class_weight for CrossEntropyLoss and BCELossWithLogits ([#509](https://github.com/open-mmlab/mmaction2/pull/509))\r\n- add some explanation about the resampling in slowfast ([#502](https://github.com/open-mmlab/mmaction2/pull/502))\r\n- Modify paper title in README.md ([#512](https://github.com/open-mmlab/mmaction2/pull/512))\r\n- Add alternative ways to download Kinetics ([#521](https://github.com/open-mmlab/mmaction2/pull/521))\r\n- Add OpenMMLab projects link in README ([#530](https://github.com/open-mmlab/mmaction2/pull/530))\r\n- Change default preprocessing to shortedge to 256 ([#538](https://github.com/open-mmlab/mmaction2/pull/538))\r\n- Add config tag in dataset README ([#540](https://github.com/open-mmlab/mmaction2/pull/540))\r\n- Add solution for markdownlint installation issue ([#497](https://github.com/open-mmlab/mmaction2/pull/497))\r\n- Add dataset overview in readthedocs ([#548](https://github.com/open-mmlab/mmaction2/pull/548))\r\n- Modify the trigger mode of the warnings of missing mmdet ([583](https://github.com/open-mmlab/mmaction2/pull/583))\r\n- Refactor config structure ([#488](https://github.com/open-mmlab/mmaction2/pull/488), [#572](https://github.com/open-mmlab/mmaction2/pull/572))\r\n- Refactor unittest structure ([#433](https://github.com/open-mmlab/mmaction2/pull/433))\r\n\r\n**Bug and Typo Fixes**\r\n\r\n- Fix a bug about ava dataset validation ([#527](https://github.com/open-mmlab/mmaction2/pull/527))\r\n- Fix a bug about ResNet pretrain weight initialization ([#582](https://github.com/open-mmlab/mmaction2/pull/582))\r\n- Fix a bug in CI due to MMCV index ([#495](https://github.com/open-mmlab/mmaction2/pull/495))\r\n- Remove invalid links of MiT and MMiT ([#516](https://github.com/open-mmlab/mmaction2/pull/516))\r\n- Fix frame rate bug for AVA preparation ([#576](https://github.com/open-mmlab/mmaction2/pull/576))",
        "dateCreated": "2021-02-01T10:19:23Z",
        "datePublished": "2021-02-01T10:22:30Z",
        "html_url": "https://github.com/open-mmlab/mmaction2/releases/tag/v0.11.0",
        "name": "MMAction2 V0.11.0 Release",
        "tag_name": "v0.11.0",
        "tarball_url": "https://api.github.com/repos/open-mmlab/mmaction2/tarball/v0.11.0",
        "url": "https://api.github.com/repos/open-mmlab/mmaction2/releases/37168474",
        "zipball_url": "https://api.github.com/repos/open-mmlab/mmaction2/zipball/v0.11.0"
      },
      {
        "authorType": "User",
        "author_name": "kennymckormick",
        "body": "**Highlights**\r\n\r\n- Support Spatio-Temporal Action Detection (AVA)\r\n- Support precise BN\r\n\r\n**New Features**\r\n\r\n- Support precise BN ([#501](https://github.com/open-mmlab/mmaction2/pull/501/))\r\n- Support Spatio-Temporal Action Detection (AVA) ([#351](https://github.com/open-mmlab/mmaction2/pull/351))\r\n- Support to return feature maps in `inference_recognizer` ([#458](https://github.com/open-mmlab/mmaction2/pull/458))\r\n\r\n**Improvements**\r\n\r\n- Add arg `stride` to long_video_demo.py, to make inference faster ([#468](https://github.com/open-mmlab/mmaction2/pull/468))\r\n- Support training and testing for Spatio-Temporal Action Detection ([#351](https://github.com/open-mmlab/mmaction2/pull/351))\r\n- Fix CI due to pip upgrade ([#454](https://github.com/open-mmlab/mmaction2/pull/454))\r\n- Add markdown lint in pre-commit hook ([#255](https://github.com/open-mmlab/mmaction2/pull/225))\r\n- Speed up confusion matrix calculation ([#465](https://github.com/open-mmlab/mmaction2/pull/465))\r\n- Use title case in modelzoo statistics ([#456](https://github.com/open-mmlab/mmaction2/pull/456))\r\n- Add FAQ documents for easy troubleshooting. ([#413](https://github.com/open-mmlab/mmaction2/pull/413), [#420](https://github.com/open-mmlab/mmaction2/pull/420), [#439](https://github.com/open-mmlab/mmaction2/pull/439))\r\n- Support Spatio-Temporal Action Detection with context ([#471](https://github.com/open-mmlab/mmaction2/pull/471))\r\n- Add class weight for CrossEntropyLoss and BCELossWithLogits ([#509](https://github.com/open-mmlab/mmaction2/pull/509))\r\n- Add Lazy OPs docs ([#504](https://github.com/open-mmlab/mmaction2/pull/504))\r\n\r\n**Bug and Typo Fixes**\r\n\r\n- Fix typo in default argument of BaseHead ([#446](https://github.com/open-mmlab/mmaction2/pull/446))\r\n- Fix potential bug about `output_config` overwrite ([#463](https://github.com/open-mmlab/mmaction2/pull/463))\r\n\r\n**ModelZoo**\r\n\r\n- Add SlowOnly, SlowFast for AVA2.1 ([#351](https://github.com/open-mmlab/mmaction2/pull/351))\r\n",
        "dateCreated": "2021-01-05T04:56:53Z",
        "datePublished": "2021-01-05T05:26:23Z",
        "html_url": "https://github.com/open-mmlab/mmaction2/releases/tag/v0.10.0",
        "name": "MMAction2 V0.10.0 Release",
        "tag_name": "v0.10.0",
        "tarball_url": "https://api.github.com/repos/open-mmlab/mmaction2/tarball/v0.10.0",
        "url": "https://api.github.com/repos/open-mmlab/mmaction2/releases/35980323",
        "zipball_url": "https://api.github.com/repos/open-mmlab/mmaction2/zipball/v0.10.0"
      },
      {
        "authorType": "User",
        "author_name": "innerlee",
        "body": "**Highlights**\r\n- Support GradCAM utils for recognizers\r\n- Support ResNet Audio model\r\n\r\n**New Features**\r\n- Automatically add modelzoo statistics to readthedocs ([#327](https://github.com/open-mmlab/mmaction2/pull/327))\r\n- Support GYM99 data preparation ([#331](https://github.com/open-mmlab/mmaction2/pull/331))\r\n- Add AudioOnly Pathway from AVSlowFast. ([#355](https://github.com/open-mmlab/mmaction2/pull/355))\r\n- Add GradCAM utils for recognizer ([#324](https://github.com/open-mmlab/mmaction2/pull/324))\r\n- Add print config script ([#345](https://github.com/open-mmlab/mmaction2/pull/345))\r\n- Add online motion vector decoder ([#291](https://github.com/open-mmlab/mmaction2/pull/291))\r\n\r\n**Improvements**\r\n- Support PyTorch 1.7 in CI ([#312](https://github.com/open-mmlab/mmaction2/pull/312))\r\n- Support to predict different labels in a long video ([#274](https://github.com/open-mmlab/mmaction2/pull/274))\r\n- Update docs bout test crops ([#359](https://github.com/open-mmlab/mmaction2/pull/359))\r\n- Polish code format using pylint manually ([#338](https://github.com/open-mmlab/mmaction2/pull/338))\r\n- Update unittest coverage ([#358](https://github.com/open-mmlab/mmaction2/pull/358), [#322](https://github.com/open-mmlab/mmaction2/pull/322), [#325](https://github.com/open-mmlab/mmaction2/pull/325))\r\n- Add random seed for building filelists ([#323](https://github.com/open-mmlab/mmaction2/pull/323))\r\n- Update colab tutorial ([#367](https://github.com/open-mmlab/mmaction2/pull/367))\r\n- set default batch_size of evaluation and testing to 1 ([#250](https://github.com/open-mmlab/mmaction2/pull/250))\r\n- Rename the preparation docs to `README.md` ([#388](https://github.com/open-mmlab/mmaction2/pull/388))\r\n- Move docs about demo to `demo/README.md` ([#329](https://github.com/open-mmlab/mmaction2/pull/329))\r\n- Remove redundant code in `tools/test.py` ([#310](https://github.com/open-mmlab/mmaction2/pull/310))\r\n- Automatically calculate number of test clips for Recognizer2D ([#359](https://github.com/open-mmlab/mmaction2/pull/359))\r\n\r\n**Bug and Typo Fixes**\r\n- Fix rename Kinetics classnames bug ([#384](https://github.com/open-mmlab/mmaction2/pull/384))\r\n- Fix a bug in BaseDataset when `data_prefix` is None ([#314](https://github.com/open-mmlab/mmaction2/pull/314))\r\n- Fix a bug about `tmp_folder` in `OpenCVInit` ([#357](https://github.com/open-mmlab/mmaction2/pull/357))\r\n- Fix `get_thread_id` when not using disk as backend ([#354](https://github.com/open-mmlab/mmaction2/pull/354), [#357](https://github.com/open-mmlab/mmaction2/pull/357))\r\n- Fix the bug of HVU object `num_classes` from 1679 to 1678 ([#307](https://github.com/open-mmlab/mmaction2/pull/307))\r\n- Fix typo in `export_model.md` ([#399](https://github.com/open-mmlab/mmaction2/pull/399))\r\n- Fix OmniSource training configs ([#321](https://github.com/open-mmlab/mmaction2/pull/321))\r\n- Fix Issue #306: Bug of SampleAVAFrames ([#317](https://github.com/open-mmlab/mmaction2/pull/317))\r\n\r\n**ModelZoo**\r\n- Add SlowOnly model for GYM99, both RGB and Flow ([#336](https://github.com/open-mmlab/mmaction2/pull/336))\r\n- Add auto modelzoo statistics in readthedocs ([#327](https://github.com/open-mmlab/mmaction2/pull/327))\r\n- Add TSN for HMDB51 pretrained on Kinetics400, Moments in Time and ImageNet. ([#372](https://github.com/open-mmlab/mmaction2/pull/372))\r\n",
        "dateCreated": "2020-12-01T14:03:19Z",
        "datePublished": "2020-12-01T14:04:44Z",
        "html_url": "https://github.com/open-mmlab/mmaction2/releases/tag/v0.9.0",
        "name": "MMAction2 V0.9.0 Release",
        "tag_name": "v0.9.0",
        "tarball_url": "https://api.github.com/repos/open-mmlab/mmaction2/tarball/v0.9.0",
        "url": "https://api.github.com/repos/open-mmlab/mmaction2/releases/34636355",
        "zipball_url": "https://api.github.com/repos/open-mmlab/mmaction2/zipball/v0.9.0"
      },
      {
        "authorType": "User",
        "author_name": "innerlee",
        "body": "### v0.8.0 (31/10/2020)\r\n\r\n**Highlights**\r\n- Support [OmniSource](https://arxiv.org/abs/2003.13042)\r\n- Support C3D\r\n- Support video recognition with audio modality\r\n- Support HVU\r\n- Support X3D\r\n\r\n**New Features**\r\n- Support AVA dataset preparation ([#266](https://github.com/open-mmlab/mmaction2/pull/266))\r\n- Support the training of video recognition dataset with multiple tag categories ([#235](https://github.com/open-mmlab/mmaction2/pull/235))\r\n- Support joint training with multiple training datasets of multiple formats, including images, untrimmed videos, etc. ([#242](https://github.com/open-mmlab/mmaction2/pull/242))\r\n- Support to specify a start epoch to conduct evaluation ([#216](https://github.com/open-mmlab/mmaction2/pull/216))\r\n- Implement X3D models, support testing with model weights converted from SlowFast ([#288](https://github.com/open-mmlab/mmaction2/pull/288))\r\n\r\n**Improvements**\r\n- Set default values of 'average_clips' in each config file so that there is no need to set it explicitly during testing in most cases ([#232](https://github.com/open-mmlab/mmaction2/pull/232))\r\n- Extend HVU datatools to generate individual file list for each tag category ([#258](https://github.com/open-mmlab/mmaction2/pull/258))\r\n- Support data preparation for Kinetics-600 and Kinetics-700 ([#254](https://github.com/open-mmlab/mmaction2/pull/254))\r\n- Add `cfg-options` in arguments to override some settings in the used config for convenience ([#212](https://github.com/open-mmlab/mmaction2/pull/212))\r\n- Rename the old evaluating protocol `mean_average_precision` as `mmit_mean_average_precision` since it is only used on MMIT and is not the `mAP` we usually talk about. Add `mean_average_precision`, which is the real `mAP` ([#235](https://github.com/open-mmlab/mmaction2/pull/235))\r\n- Add accurate setting (Three crop * 2 clip) and report corresponding performance for TSM model ([#241](https://github.com/open-mmlab/mmaction2/pull/241))\r\n- Add citations in each preparing_dataset.md in `tools/data/dataset` ([#289](https://github.com/open-mmlab/mmaction2/pull/289))\r\n- Update the performance of audio-visual fusion on Kinetics-400 ([#281](https://github.com/open-mmlab/mmaction2/pull/281))\r\n- Support data preparation of OmniSource web datasets, including GoogleImage, InsImage, InsVideo and KineticsRawVideo ([#294](https://github.com/open-mmlab/mmaction2/pull/294))\r\n- Use `metric_options` dict to provide metric args in `evaluate` ([#286](https://github.com/open-mmlab/mmaction2/pull/286))\r\n\r\n**Bug Fixes**\r\n- Register `FrameSelector` in `PIPELINES` ([#268](https://github.com/open-mmlab/mmaction2/pull/268))\r\n- Fix the potential bug for default value in dataset_setting ([#245](https://github.com/open-mmlab/mmaction2/pull/245))\r\n- Fix the data preparation bug for `something-something` dataset ([#278](https://github.com/open-mmlab/mmaction2/pull/278))\r\n- Fix the invalid config url in slowonly README data benchmark ([#249](https://github.com/open-mmlab/mmaction2/pull/249))\r\n- Validate that the performance of models trained with videos have no significant difference comparing to the performance of models trained with rawframes ([#256](https://github.com/open-mmlab/mmaction2/pull/256))\r\n- Correct the `img_norm_cfg` used by TSN-3seg-R50 UCF-101 model, improve the Top-1 accuracy by 3% ([#273](https://github.com/open-mmlab/mmaction2/pull/273))\r\n\r\n**ModelZoo**\r\n- Add Baselines for Kinetics-600 and Kinetics-700, including TSN-R50-8seg and SlowOnly-R50-8x8 ([#259](https://github.com/open-mmlab/mmaction2/pull/259))\r\n- Add OmniSource benchmark on MiniKineitcs ([#296](https://github.com/open-mmlab/mmaction2/pull/296))\r\n- Add Baselines for HVU, including TSN-R18-8seg on 6 tag categories of HVU ([#287](https://github.com/open-mmlab/mmaction2/pull/287))\r\n- Add X3D models ported from [SlowFast](https://github.com/facebookresearch/SlowFast/) ([#288](https://github.com/open-mmlab/mmaction2/pull/288))",
        "dateCreated": "2020-10-31T14:02:53Z",
        "datePublished": "2020-10-31T14:37:34Z",
        "html_url": "https://github.com/open-mmlab/mmaction2/releases/tag/v0.8.0",
        "name": "MMAction2 V0.8.0 Release",
        "tag_name": "v0.8.0",
        "tarball_url": "https://api.github.com/repos/open-mmlab/mmaction2/tarball/v0.8.0",
        "url": "https://api.github.com/repos/open-mmlab/mmaction2/releases/33305534",
        "zipball_url": "https://api.github.com/repos/open-mmlab/mmaction2/zipball/v0.8.0"
      },
      {
        "authorType": "User",
        "author_name": "innerlee",
        "body": "\r\n# **Highlights**\r\n- Support TPN\r\n- Support JHMDB, UCF101-24, HVU dataset preparation\r\n- support onnx model conversion\r\n\r\n# **New Features**\r\n- Support the data pre-processing pipeline for the HVU Dataset ([#277](https://github.com/open-mmlab/mmaction2/pull/227/))\r\n- Support real-time action recognition from web camera ([#171](https://github.com/open-mmlab/mmaction2/pull/171))\r\n- Support onnx ([#160](https://github.com/open-mmlab/mmaction2/pull/160))\r\n- Support UCF101-24 preparation ([#219](https://github.com/open-mmlab/mmaction2/pull/219))\r\n- Support evaluating mAP for ActivityNet with [CUHK17_activitynet_pred](http://activity-net.org/challenges/2017/evaluation.html) ([#176](https://github.com/open-mmlab/mmaction2/pull/176))\r\n- Add the data pipeline for ActivityNet, including downloading videos, extracting RGB and Flow frames, finetuning TSN and extracting feature ([#190](https://github.com/open-mmlab/mmaction2/pull/190))\r\n- Support JHMDB preparation ([#220](https://github.com/open-mmlab/mmaction2/pull/220))\r\n\r\n# **ModelZoo**\r\n- Add finetuning setting for SlowOnly ([#173](https://github.com/open-mmlab/mmaction2/pull/173))\r\n- Add TSN and SlowOnly models trained with [OmniSource](https://arxiv.org/abs/2003.13042), which achieve 75.7% Top-1 with TSN-R50-3seg and 80.4% Top-1 with SlowOnly-R101-8x8 ([#215](https://github.com/open-mmlab/mmaction2/pull/215))\r\n\r\n# **Improvements**\r\n- Support demo with video url ([#165](https://github.com/open-mmlab/mmaction2/pull/165))\r\n- Support multi-batch when testing ([#184](https://github.com/open-mmlab/mmaction2/pull/184))\r\n- Add tutorial for adding a new learning rate updater ([#181](https://github.com/open-mmlab/mmaction2/pull/181))\r\n- Add config name in meta info ([#183](https://github.com/open-mmlab/mmaction2/pull/183))\r\n- Remove git hash in `__version__` ([#189](https://github.com/open-mmlab/mmaction2/pull/189))\r\n- Check mmcv version ([#189](https://github.com/open-mmlab/mmaction2/pull/189))\r\n- Update url with 'https://download.openmmlab.com' ([#208](https://github.com/open-mmlab/mmaction2/pull/208))\r\n- Update Docker file to support PyTorch 1.6 and update `install.md` ([#209](https://github.com/open-mmlab/mmaction2/pull/209))\r\n- Polish readsthedocs display ([#217](https://github.com/open-mmlab/mmaction2/pull/217), [#229](https://github.com/open-mmlab/mmaction2/pull/229))\r\n\r\n# **Bug Fixes**\r\n- Fix the bug when using OpenCV to extract only RGB frames with original shape ([#184](https://github.com/open-mmlab/mmaction2/pull/187))\r\n- Fix the bug of sthv2 `num_classes` from 339 to 174 ([#174](https://github.com/open-mmlab/mmaction2/pull/174), [#207](https://github.com/open-mmlab/mmaction2/pull/207))\r\n",
        "dateCreated": "2020-10-08T14:35:57Z",
        "datePublished": "2020-10-03T15:06:16Z",
        "html_url": "https://github.com/open-mmlab/mmaction2/releases/tag/v0.7.0",
        "name": "MMAction2 V0.7.0 Release",
        "tag_name": "v0.7.0",
        "tarball_url": "https://api.github.com/repos/open-mmlab/mmaction2/tarball/v0.7.0",
        "url": "https://api.github.com/repos/open-mmlab/mmaction2/releases/32124937",
        "zipball_url": "https://api.github.com/repos/open-mmlab/mmaction2/zipball/v0.7.0"
      },
      {
        "authorType": "User",
        "author_name": "hellock",
        "body": "## Highlights\r\n- Support TIN, CSN, SSN, NonLocal\r\n- Support FP16 training\r\n\r\n## New Features\r\n- Support NonLocal module and provide ckpt in TSM and I3D ([#41](https://github.com/open-mmlab/mmaction2/pull/41))\r\n- Support SSN ([#33](https://github.com/open-mmlab/mmaction2/pull/33), [#37](https://github.com/open-mmlab/mmaction2/pull/37), [#52](https://github.com/open-mmlab/mmaction2/pull/52), [#55](https://github.com/open-mmlab/mmaction2/pull/55))\r\n- Support CSN ([#87](https://github.com/open-mmlab/mmaction2/pull/87))\r\n- Support TIN ([#53](https://github.com/open-mmlab/mmaction2/pull/53))\r\n- Support HMDB51 dataset preparation ([#60](https://github.com/open-mmlab/mmaction2/pull/60))\r\n- Support encoding videos from frames ([#84](https://github.com/open-mmlab/mmaction2/pull/84))\r\n- Support FP16 training ([#25](https://github.com/open-mmlab/mmaction2/pull/25))\r\n- Enhance demo by supporting rawframe inference ([#59](https://github.com/open-mmlab/mmaction2/pull/59)), output video/gif ([#72](https://github.com/open-mmlab/mmaction2/pull/72))\r\n\r\n## ModelZoo\r\n- Update Slowfast modelzoo ([#51](https://github.com/open-mmlab/mmaction2/pull/51))\r\n- Update TSN, TSM video checkpoints ([#50](https://github.com/open-mmlab/mmaction2/pull/50))\r\n- Add data benchmark for TSN ([#57](https://github.com/open-mmlab/mmaction2/pull/57))\r\n- Add data benchmark for SlowOnly ([#77](https://github.com/open-mmlab/mmaction2/pull/77))\r\n- Add BSN/BMN performance results with feature extracted by our codebase ([#99](https://github.com/open-mmlab/mmaction2/pull/99))\r\n\r\n## Improvements\r\n- Polish data preparation codes ([#70](https://github.com/open-mmlab/mmaction2/pull/70))\r\n- Improve data preparation scripts ([#58](https://github.com/open-mmlab/mmaction2/pull/58))\r\n- Improve unittest coverage and minor fix ([#62](https://github.com/open-mmlab/mmaction2/pull/62))\r\n- Support PyTorch 1.6 in CI ([#117](https://github.com/open-mmlab/mmaction2/pull/117))\r\n- Support `with_offset` for rawframe dataset ([#48](https://github.com/open-mmlab/mmaction2/pull/48))\r\n- Support json annotation files ([#119](https://github.com/open-mmlab/mmaction2/pull/119))\r\n- Support `multi-class` in TSMHead ([#104](https://github.com/open-mmlab/mmaction2/pull/104))\r\n- Support using `val_step()` to validate data for each `val` workflow ([#123](https://github.com/open-mmlab/mmaction2/pull/123))\r\n- Use `xxInit()` method to get `total_frames` and make `total_frames` a required key ([#90](https://github.com/open-mmlab/mmaction2/pull/90))\r\n- Add paper introduction in model readme ([#140](https://github.com/open-mmlab/mmaction2/pull/140))\r\n- Adjust the directory structure of `tools/` and rename some scripts files ([#142](https://github.com/open-mmlab/mmaction2/pull/142))\r\n\r\n## Bug Fixes\r\n- Fix configs for localization test ([#67](https://github.com/open-mmlab/mmaction2/pull/67))\r\n- Fix configs of SlowOnly by fixing lr to 8 gpus ([#136](https://github.com/open-mmlab/mmaction2/pull/136))\r\n- Fix the bug in analyze_log ([#54](https://github.com/open-mmlab/mmaction2/pull/54))\r\n- Fix the bug of generating HMDB51 class index file ([#69](https://github.com/open-mmlab/mmaction2/pull/69))\r\n- Fix the bug of using `load_checkpoint()` in ResNet ([#93](https://github.com/open-mmlab/mmaction2/pull/93))\r\n- Fix the bug of `--work-dir` when using slurm training script ([#110](https://github.com/open-mmlab/mmaction2/pull/110))\r\n- Correct the sthv1/sthv2 rawframes filelist generate command ([#71](https://github.com/open-mmlab/mmaction2/pull/71))\r\n- `CosineAnnealing` typo ([#47](https://github.com/open-mmlab/mmaction2/pull/47))",
        "dateCreated": "2020-09-02T16:22:35Z",
        "datePublished": "2020-09-02T16:26:57Z",
        "html_url": "https://github.com/open-mmlab/mmaction2/releases/tag/v0.6.0",
        "name": "MMAction2 V0.6.0 Release",
        "tag_name": "v0.6.0",
        "tarball_url": "https://api.github.com/repos/open-mmlab/mmaction2/tarball/v0.6.0",
        "url": "https://api.github.com/repos/open-mmlab/mmaction2/releases/30597154",
        "zipball_url": "https://api.github.com/repos/open-mmlab/mmaction2/zipball/v0.6.0"
      },
      {
        "authorType": "User",
        "author_name": "hellock",
        "body": "The first release of MMAction2.",
        "dateCreated": "2020-07-21T15:38:53Z",
        "datePublished": "2020-07-21T15:47:35Z",
        "html_url": "https://github.com/open-mmlab/mmaction2/releases/tag/v0.5.0",
        "name": "MMAction2 V0.5.0 Release",
        "tag_name": "v0.5.0",
        "tarball_url": "https://api.github.com/repos/open-mmlab/mmaction2/tarball/v0.5.0",
        "url": "https://api.github.com/repos/open-mmlab/mmaction2/releases/28789883",
        "zipball_url": "https://api.github.com/repos/open-mmlab/mmaction2/zipball/v0.5.0"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1558,
      "date": "Tue, 28 Dec 2021 02:00:44 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "action-recognition",
      "temporal-action-localization",
      "pytorch",
      "video-understanding",
      "tsn",
      "i3d",
      "slowfast",
      "ava",
      "spatial-temporal-action-detection",
      "benchmark",
      "tsm",
      "x3d",
      "non-local",
      "deep-learning",
      "openmmlab",
      "posec3d",
      "video-classification"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please see [getting_started.md](docs/en/getting_started.md) for the basic usage of MMAction2.\nThere are also tutorials:\n\n- [learn about configs](docs/en/tutorials/1_config.md)\n- [finetuning models](docs/en/tutorials/2_finetune.md)\n- [adding new dataset](docs/en/tutorials/3_new_dataset.md)\n- [designing data pipeline](docs/en/tutorials/4_data_pipeline.md)\n- [adding new modules](docs/en/tutorials/5_new_modules.md)\n- [exporting model to onnx](docs/en/tutorials/6_export_model.md)\n- [customizing runtime settings](docs/en/tutorials/7_customize_runtime.md)\n\nA Colab tutorial is also provided. You may preview the notebook [here](demo/mmaction2_tutorial.ipynb) or directly [run](https://colab.research.google.com/github/open-mmlab/mmaction2/blob/master/demo/mmaction2_tutorial.ipynb) on Colab.\n\n",
      "technique": "Header extraction"
    }
  ]
}