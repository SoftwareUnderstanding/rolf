{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1409.1556\n    #This is going to take some time...\n    base_model = VGG16(weights='imagenet'",
      "https://arxiv.org/abs/1409.1556\n    #This is going to take some time...\n    base_model = VGG16(weights='imagenet'"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9962717187710581,
        0.9999654690096298,
        0.9611841524374782
      ],
      "excerpt": "Dans l'introduction au projet, il est conseill\u00e9 de d\u00e9marrer la construction de l'application par la classification 1 vs 1, ce qui a \u00e9t\u00e9 fait ici, cependant j'ai pr\u00e9f\u00e9r\u00e9 simplifier le code de l'application finale en n'y laissant que ce qui \u00e9tait relatif \u00e0 la classification 1 vs All \u00e9tant donn\u00e9 que c'est ce qui a \u00e9t\u00e9 d\u00e9ploy\u00e9 et distribu\u00e9. \nEn commen\u00e7ant par la classification 1 vs 1, j'ai pu plus facilement it\u00e9rer sur la conception de l'application, j'ai aussi limit\u00e9 dans un premier temps le volume d'images et donc de features \u00e0 exploiter pour acc\u00e9l\u00e9rer le traitement \u00e9tant donn\u00e9 qu'il s'agissait de tester la logique dans un premier temps. J'ai ensuite adapt\u00e9 le fonctionnement pour faire de la classification 1 vs All en appliquant la m\u00eame strat\u00e9gie de limitation des volumes. \nUne fois l'application prototyp\u00e9 en local, il a \u00e9t\u00e9 possible de la d\u00e9ployer sur un cluster de calcul AWS afin de r\u00e9aliser l'apprentissage complet des mod\u00e8les pour ensuite passer \u00e0 une phase d'analyse des r\u00e9sultats puis d'optimisation des performances. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8906174419333412
      ],
      "excerpt": "D\u00e9finition de variable dans le cadre de ces instructions: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8806285587786947
      ],
      "excerpt": "    #:Load model VGG16 as described in https://arxiv.org/abs/1409.1556 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "            if __name__ == \"__main__\": \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.967090817984898,
        0.9939049268959973,
        0.9856386183121458
      ],
      "excerpt": "Pour chaque image pr\u00e9sente dans le dossier en argument, le script va produire un fichier JSON contenant une repr\u00e9sentation de l'image sous la forme d\u2019un tableau de valeurs flottantes, ce qui permettra de r\u00e9aliser des calculs. \nDans le cadre du projet, j'ai r\u00e9alis\u00e9 l'extraction des features une seule fois pour \u00e9viter de l'int\u00e9grer \u00e0 l'application et donc r\u00e9aliser ce traitement \u00e0 chaque it\u00e9ration de la phase de d\u00e9veloppement. Au moment du d\u00e9ploiement sur le cluster, j'ai upload\u00e9 les features dans le bucket S3 en m'assurant que les permissions ad\u00e9quates \u00e9taient en place. Je consid\u00e8re cela comme un pr\u00e9-requis dans le cadre de ce readme de la m\u00eame fa\u00e7on qu'il sera n\u00e9cessaire d'uploader l'application. \nPour l'extraction des features, j'ai utilis\u00e9 la version 3.6.5 de Python et voici la version des d\u00e9pendances utilis\u00e9es pendant la phase de d\u00e9veloppement : \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9998444113968797
      ],
      "excerpt": "Dans le cadre de la configuration du cluster de calculs, il m'a \u00e9t\u00e9 n\u00e9cessaire d'uploader un script bash sur AWS S3 qui est ex\u00e9cut\u00e9 au moment de l'initialisation du cluster de fa\u00e7on \u00e0 installer la librairie boto3 qui permet de lire dans le bucket S3.    \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9995084465273778
      ],
      "excerpt": "Un second fichier est aussi n\u00e9cessaire en local, il s'agit d'un fichier de configuration Spark utilis\u00e9 lui aussi au moment de l'initiatlisation du cluster de fa\u00e7on \u00e0 allouer 2 Go de memoire par executor \u00e9tant donn\u00e9 que j'ai rencontr\u00e9 de nombreux probl\u00e8me de m\u00e9moire avec erreur OOM, cette configuration a permis de r\u00e9soudre le soucis : \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9983201864389021
      ],
      "excerpt": "Pour cr\u00e9er le cluster, j'utilise le CLI mis \u00e0 disposition de fa\u00e7on \u00e0 simplifier l'it\u00e9ration et la phase de d\u00e9ploiement, voici la commande finale: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "  --region eu-west-3 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9860180493649525
      ],
      "excerpt": "Apr\u00e8s avoir suivi les instructions de connexion en SSH au cluster fournies par AWS il est possible de se connecter au cluster EMR qui a pr\u00e9alablement \u00e9t\u00e9 configur\u00e9 en accord avec nos besoins et de lancer notre application Spark via la commande suivante: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8868220341378448
      ],
      "excerpt": "Une fois le script en cours, les logs apparaitront dans la console et il est aussi possible d'aller observer l'ex\u00e9cution de l'application Spark \u00e0 partir des interfaces d\u00e9di\u00e9es qu'AWS rend disponible qui seront aussi n\u00e9cessaire \u00e0 l'optimisation de l'application. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/elliott-iadvize/ocr-aws-distributed",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-03-27T14:09:05Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-03-30T08:40:25Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The purpose of the project is to create a Spark application to separate images into learning and test sets, extract features from images from two different classes, learn a model using the learning data and measure the model's performance on the test data.\n\n> For example, we are told that with 100 learning images in each class, it is possible to obtain 98% good classifications in the Wheaten Terrier vs Yorkshire Terrier task.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "L'objectif du projet est de cr\u00e9er une application Spark permettant de s\u00e9parer les images en jeux d'apprentissage et de test, d'extraire les features des images en provenance de deux classes diff\u00e9rentes, d'apprendre un mod\u00e8le sur les donn\u00e9es d'apprentissage et de mesurer les performances du mod\u00e8le sur les donn\u00e9es de test.\n\n> \u00c0 titre d'exemple, il nous est indiqu\u00e9 qu'avec 100 images d'apprentissage dans chaque classe, il est possible d'obtenir 98% de bonnes classifications dans la t\u00e2che Wheaten Terrier vs Yorkshire Terrier.\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9546916514775146
      ],
      "excerpt": "    #:This is going to take some time... \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8632320137746209,
        0.8665907610677593
      ],
      "excerpt": "    #:Model will produce the output of the 'fc2'layer which is the penultimate neural network layer \n    #:(see the paper above for mode details) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076,
        0.9530556550744498,
        0.8126656590778484
      ],
      "excerpt": "for image_path in sys.argv[1:]: \n    features = extract_features(model, image_path) \n    with open(image_path + \".json\", \"w\") as out: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "        def extract_features(model, image_path): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8745153602789364,
        0.8930901044020226
      ],
      "excerpt": "            features = model.predict(x) \n            return features.tolist()[0] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9878011521667285,
        0.9656326216344647,
        0.9863568894364471,
        0.9911214675366926
      ],
      "excerpt": "In the introduction to the project, it is recommended to start building the application with 1 vs 1 classification, which has been done here, however I preferred to simplify the code of the final application by leaving only what was related to 1 vs All classification since it is what has been deployed and distributed. \nStarting with 1 vs 1 classification, I was able to iterate more easily on the design of the application, I also limited the volume of images and therefore features to use to speed up processing since it was a matter of testing the logic in the first place. I then adapted the operation to make the 1 vs All classification by applying the same volume limitation strategy. \nOnce the application was prototyped locally, it was possible to deploy it on an AWS computing cluster in order to complete the full learning of the models and then move on to a phase of results analysis and performance optimization. \nAs part of the configuration of the EMR cluster, it was necessary for me to upload a bash script on AWS S3 which is executed at the time of the initialization of the cluster in order to install the boto3 library which allows to read in the S3 bucket. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9747115731020219
      ],
      "excerpt": "A second file is also required locally, it is a Spark configuration file also used at the time of the cluster initialization in order to allocate 2 GB of memory per executor since I have encountered many memory problems with OOM error, this configuration has solved the problem: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8115605853455142
      ],
      "excerpt": "To create the cluster, I use the CLI provided to simplify the iteration and deployment phase, here is the final command: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9630051807267551
      ],
      "excerpt": "After following the instructions provided by AWS to connect in SSH to the cluster it is possible to connect to the EMR cluster that has previously been configured according to our needs and launch our Spark application using the following command: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Projet 1: R\u00e9aliser un apprentissage distribu\u00e9",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/elliott-iadvize/ocr-aws-distributed/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Mon, 27 Dec 2021 23:04:08 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/elliott-iadvize/ocr-aws-distributed/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "elliott-iadvize/ocr-aws-distributed",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/elliott-iadvize/ocr-aws-distributed/master/configs/install_dependencies.sh",
      "https://raw.githubusercontent.com/elliott-iadvize/ocr-aws-distributed/master/configs/create_cluster.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8746706635703053,
        0.9950849319650424
      ],
      "excerpt": ":Dependencies can be installed by running: \n:pip install keras tensorflow h5py pillow \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9974357497005517
      ],
      "excerpt": "sudo pip install -U 'boto3==1.7.4' --force-reinstall \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8244343035009378
      ],
      "excerpt": "  --applications Name=Ganglia Name=Spark Name=Zeppelin \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8661176197453521
      ],
      "excerpt": "  --name 'Cluster OCR' \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9411437817793031
      ],
      "excerpt": "  --bootstrap-actions Path=${Bucket Name}/install_dependencies.sh   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9974357497005517
      ],
      "excerpt": "sudo pip install -U'boto3===1.7.4' --force-reinstall \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8396959085238944
      ],
      "excerpt": "aws emr create-cluster --applications Name=Ganglia Name=Spark Name=Zeppelin --ec2-attributes '{\"KeyName\":\"iadvize_eau_ocr\",\"InstanceProfile\":\"EMR_EC2_DefaultRole\",\"SubnetId\":\"subnet-278fcb4e\",\"EmrManagedSlaveSecurityGroup\":\"sg-02b2a819721044d5d\",\"EmrManagedMasterSecurityGroup\":\"sg-016b7b7113298da7f\"}' --service-role EMR_DefaultRole --enable-debugging --release-label emr-5.20.0 --log-uri 's3n://aws-logs-370616375808-eu-west-3/elasticmapreduce/' --name 'Cluster OCR' --instance-groups '[{\"InstanceCount\":1,\"EbsConfiguration\":{\"EbsBlockDeviceConfigs\":[{\"VolumeSpecification\":{\"SizeInGB\":32,\"VolumeType\":\"gp2\"},\"VolumesPerInstance\":1}]},\"InstanceGroupType\":\"MASTER\",\"InstanceType\":\"m5.xlarge\",\"Name\":\"Master Instance Group\"},{\"InstanceCount\":2,\"EbsConfiguration\":{\"EbsBlockDeviceConfigs\":[{\"VolumeSpecification\":{\"SizeInGB\":32,\"VolumeType\":\"gp2\"},\"VolumesPerInstance\":1}]},\"InstanceGroupType\":\"CORE\",\"InstanceType\":\"m5.xlarge\",\"Name\":\"Core Instance Group\"}]' --configurations '[{\"Classification\":\"spark\",\"Properties\":{},\"Configurations\":[]}]' --scale-down-behavior TERMINATE_AT_TASK_COMPLETION --region eu-west-3 --bootstrap-actions Path=${Bucket Name}/install_dependencies.sh  --configurations file://./spark_config.json \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9180062578030207
      ],
      "excerpt": "import json \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8044029904524085,
        0.9074045923204996
      ],
      "excerpt": ":Run script as: \n:./extract-features.py images/*.jpg \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9040368155137037
      ],
      "excerpt": "from keras.models import Model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9457175861910134
      ],
      "excerpt": "import numpy as np \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8719491278656056
      ],
      "excerpt": "    model = Model(input=base_model.input, output=base_model.get_layer('fc2').output) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8471643361862832
      ],
      "excerpt": "    with open(image_path + \".json\", \"w\") as out: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8973933083440926
      ],
      "excerpt": "            x = np.expand_dims(x, axis=0) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8142835995138061
      ],
      "excerpt": "                main() \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "      \"maximizeResourceAllocation\": \"true\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8421074476017179
      ],
      "excerpt": "  --name 'Cluster OCR' \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8867280275402708
      ],
      "excerpt": "  --configurations file://./spark_config.json \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8850277726585748
      ],
      "excerpt": "sudo spark-submit --deploy-mode cluster ${Bucket Name}/classifier.py ${Bucket Name} \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "      \"maximizeResourceAllocation\": \"true\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8850277726585748
      ],
      "excerpt": "sudo spark-submit --deploy-mode cluster ${Bucket Name}/classifier.py ${Bucket Name} \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/elliott-iadvize/ocr-aws-distributed/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "OCR#1 : R\u00e9alisez un apprentissage distribu\u00e9 - [~](https://github.com/elliott-iadvize/ocr-aws-distributed \"~\")",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "ocr-aws-distributed",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "elliott-iadvize",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/elliott-iadvize/ocr-aws-distributed/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Variable definition in the context of these instructions:\n`${Bucket Name} -> name of the bucket on which features are stored, for instance: s3://hodor-loves-data`\n\nA script to extract features from the images retrieved here [Oxford IIIT-Pet Dataset](http://www.robots.ox.ac.uk/~vgg/data/pets/ \"Oxford IIIT-Pet Dataset\") is provided as part of the project:\n\n```python\n#:!/usr/bin/python\n#: -*- coding: utf-8 -*-\nimport json\nimport sys\n\n#:Dependencies can be installed by running:\n#:pip install keras tensorflow h5py pillow\n\n#:Run script as:\n#:./extract-features.py images/*.jpg\n\nfrom keras.applications.vgg16 import VGG16\nfrom keras.models import Model\nfrom keras.preprocessing import image\nfrom keras.applications.vgg16 import preprocess_input\nimport numpy as np\n\ndef main():\n    #:Load model VGG16 as described in https://arxiv.org/abs/1409.1556\n    #:This is going to take some time...\n    base_model = VGG16(weights='imagenet')\n    #:Model will produce the output of the 'fc2'layer which is the penultimate neural network layer\n    #:(see the paper above for mode details)\n    model = Model(input=base_model.input, output=base_model.get_layer('fc2').output)\n\n    #:For each image, extract the representation\n    for image_path in sys.argv[1:]:\n        features = extract_features(model, image_path)\n        with open(image_path + \".json\", \"w\") as out:\n            json.dump(features, out)\n\n            def extract_features(model, image_path):\n                img = image.load_img(image_path, target_size=(224, 224))\n                x = image.img_to_array(img)\n                x = np.expand_dims(x, axis=0)\n                x = preprocess_input(x)\n\n                features = model.predict(x)\n                return features.tolist()[0]\n\n                if __name__ == \"__main__\":\n                    main()\n  ```\n\nFor each image in the argument folder, the script will produce a JSON file containing a representation of the image in the form of a floating value table, which will allow calculations to be performed.\n\nAs part of the project, I extracted the features only once to avoid integrating them into the application and therefore performing this processing at each iteration of the development phase. When I deployed on the cluster, I uploaded the features into the S3 bucket making sure that the appropriate permissions were in place. I consider this a prerequisite for this readme.\n\nFor feature extraction, I used the `3.6.5` version of Python and here is the version of the dependencies used during the development phase:\n```python\n#:requirements.txt\n\nkeras==2.2.4\ntensorflow==1.12.0\nh5py==2.8.0\npillow==5.3.0\nboto3==1.7.4\n```\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Mon, 27 Dec 2021 23:04:08 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "These instructions will get you a copy of the project up and running on your local machine for development and testing purposes. See deployment for notes on how to deploy the project on a live system.\n\n",
      "technique": "Header extraction"
    }
  ]
}