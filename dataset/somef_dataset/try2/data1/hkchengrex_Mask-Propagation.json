{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2103.07941",
      "https://arxiv.org/abs/1904.00607",
      "https://arxiv.org/abs/2007.08270",
      "https://arxiv.org/abs/1911.01911",
      "https://arxiv.org/abs/1512.03012"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please cite our paper if you find this repo useful!\n\n```bibtex\n@inproceedings{cheng2021mivos,\n  title={Modular Interactive Video Object Segmentation: Interaction-to-Mask, Propagation and Difference-Aware Fusion},\n  author={Cheng, Ho Kei and Tai, Yu-Wing and Tang, Chi-Keung},\n  booktitle={CVPR},\n  year={2021}\n}\n```\n\nAnd if you want to cite the datasets:\n\n<details> \n<summary>\n\nbibtex\n\n</summary>\n\n```bibtex\n@inproceedings{shi2015hierarchicalECSSD,\n  title={Hierarchical image saliency detection on extended CSSD},\n  author={Shi, Jianping and Yan, Qiong and Xu, Li and Jia, Jiaya},\n  booktitle={TPAMI},\n  year={2015},\n}\n\n@inproceedings{wang2017DUTS,\n  title={Learning to Detect Salient Objects with Image-level Supervision},\n  author={Wang, Lijun and Lu, Huchuan and Wang, Yifan and Feng, Mengyang \n  and Wang, Dong, and Yin, Baocai and Ruan, Xiang}, \n  booktitle={CVPR},\n  year={2017}\n}\n\n@inproceedings{FSS1000,\n  title = {FSS-1000: A 1000-Class Dataset for Few-Shot Segmentation},\n  author = {Li, Xiang and Wei, Tianhan and Chen, Yau Pun and Tai, Yu-Wing and Tang, Chi-Keung},\n  booktitle={CVPR},\n  year={2020}\n}\n\n@inproceedings{zeng2019towardsHRSOD,\n  title = {Towards High-Resolution Salient Object Detection},\n  author = {Zeng, Yi and Zhang, Pingping and Zhang, Jianming and Lin, Zhe and Lu, Huchuan},\n  booktitle = {ICCV},\n  year = {2019}\n}\n\n@inproceedings{cheng2020cascadepsp,\n  title={{CascadePSP}: Toward Class-Agnostic and Very High-Resolution Segmentation via Global and Local Refinement},\n  author={Cheng, Ho Kei and Chung, Jihoon and Tai, Yu-Wing and Tang, Chi-Keung},\n  booktitle={CVPR},\n  year={2020}\n}\n\n@inproceedings{xu2018youtubeVOS,\n  title={Youtube-vos: A large-scale video object segmentation benchmark},\n  author={Xu, Ning and Yang, Linjie and Fan, Yuchen and Yue, Dingcheng and Liang, Yuchen and Yang, Jianchao and Huang, Thomas},\n  booktitle = {ECCV},\n  year={2018}\n}\n\n@inproceedings{perazzi2016benchmark,\n  title={A benchmark dataset and evaluation methodology for video object segmentation},\n  author={Perazzi, Federico and Pont-Tuset, Jordi and McWilliams, Brian and Van Gool, Luc and Gross, Markus and Sorkine-Hornung, Alexander},\n  booktitle={CVPR},\n  year={2016}\n}\n\n@inproceedings{denninger2019blenderproc,\n  title={BlenderProc},\n  author={Denninger, Maximilian and Sundermeyer, Martin and Winkelbauer, Dominik and Zidan, Youssef and Olefir, Dmitry and Elbadrawy, Mohamad and Lodhi, Ahsan and Katam, Harinandan},\n  booktitle={arXiv:1911.01911},\n  year={2019}\n}\n\n@inproceedings{shapenet2015,\n  title       = {{ShapeNet: An Information-Rich 3D Model Repository}},\n  author      = {Chang, Angel Xuan and Funkhouser, Thomas and Guibas, Leonidas and Hanrahan, Pat and Huang, Qixing and Li, Zimo and Savarese, Silvio and Savva, Manolis and Song, Shuran and Su, Hao and Xiao, Jianxiong and Yi, Li and Yu, Fisher},\n  booktitle   = {arXiv:1512.03012},\n  year        = {2015}\n}\n```\n\n</details>\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{shapenet2015,\n  title       = {{ShapeNet: An Information-Rich 3D Model Repository}},\n  author      = {Chang, Angel Xuan and Funkhouser, Thomas and Guibas, Leonidas and Hanrahan, Pat and Huang, Qixing and Li, Zimo and Savarese, Silvio and Savva, Manolis and Song, Shuran and Su, Hao and Xiao, Jianxiong and Yi, Li and Yu, Fisher},\n  booktitle   = {arXiv:1512.03012},\n  year        = {2015}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{denninger2019blenderproc,\n  title={BlenderProc},\n  author={Denninger, Maximilian and Sundermeyer, Martin and Winkelbauer, Dominik and Zidan, Youssef and Olefir, Dmitry and Elbadrawy, Mohamad and Lodhi, Ahsan and Katam, Harinandan},\n  booktitle={arXiv:1911.01911},\n  year={2019}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{perazzi2016benchmark,\n  title={A benchmark dataset and evaluation methodology for video object segmentation},\n  author={Perazzi, Federico and Pont-Tuset, Jordi and McWilliams, Brian and Van Gool, Luc and Gross, Markus and Sorkine-Hornung, Alexander},\n  booktitle={CVPR},\n  year={2016}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{xu2018youtubeVOS,\n  title={Youtube-vos: A large-scale video object segmentation benchmark},\n  author={Xu, Ning and Yang, Linjie and Fan, Yuchen and Yue, Dingcheng and Liang, Yuchen and Yang, Jianchao and Huang, Thomas},\n  booktitle = {ECCV},\n  year={2018}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{cheng2020cascadepsp,\n  title={{CascadePSP}: Toward Class-Agnostic and Very High-Resolution Segmentation via Global and Local Refinement},\n  author={Cheng, Ho Kei and Chung, Jihoon and Tai, Yu-Wing and Tang, Chi-Keung},\n  booktitle={CVPR},\n  year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{zeng2019towardsHRSOD,\n  title = {Towards High-Resolution Salient Object Detection},\n  author = {Zeng, Yi and Zhang, Pingping and Zhang, Jianming and Lin, Zhe and Lu, Huchuan},\n  booktitle = {ICCV},\n  year = {2019}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{FSS1000,\n  title = {FSS-1000: A 1000-Class Dataset for Few-Shot Segmentation},\n  author = {Li, Xiang and Wei, Tianhan and Chen, Yau Pun and Tai, Yu-Wing and Tang, Chi-Keung},\n  booktitle={CVPR},\n  year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{wang2017DUTS,\n  title={Learning to Detect Salient Objects with Image-level Supervision},\n  author={Wang, Lijun and Lu, Huchuan and Wang, Yifan and Feng, Mengyang \n  and Wang, Dong, and Yin, Baocai and Ruan, Xiang}, \n  booktitle={CVPR},\n  year={2017}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{shi2015hierarchicalECSSD,\n  title={Hierarchical image saliency detection on extended CSSD},\n  author={Shi, Jianping and Yan, Qiong and Xu, Li and Jia, Jiaya},\n  booktitle={TPAMI},\n  year={2015},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{cheng2021mivos,\n  title={Modular Interactive Video Object Segmentation: Interaction-to-Mask, Propagation and Difference-Aware Fusion},\n  author={Cheng, Ho Kei and Tai, Yu-Wing and Tang, Chi-Keung},\n  booktitle={CVPR},\n  year={2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9747466821846071
      ],
      "excerpt": "Ho Kei Cheng, Yu-Wing Tai, Chi-Keung Tang \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9676976614759849
      ],
      "excerpt": "A tool for propagating masks across video frames. Results \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9104388306336967
      ],
      "excerpt": "DAVIS 2016 val: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9222383658450612
      ],
      "excerpt": "DAVIS 2017 val: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8654671031158477
      ],
      "excerpt": "| Without BL pretraining | :heavy_check_mark: | 80.5 | 85.8 | 83.1 | 11.2 | D17_s02 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9186665153711271
      ],
      "excerpt": "YouTubeVOS val (2018): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9222383658450612
      ],
      "excerpt": "YouTubeVOS val (2019): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.808498626019349
      ],
      "excerpt": "DAVIS 2017 test-dev: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559,
        0.8955886365383559
      ],
      "excerpt": "| s02 | link |  link | \n| s012 | link |  link | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/hkchengrex/Mask-Propagation",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-03-09T07:45:03Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-20T21:20:14Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8722135897667622,
        0.983453675082494
      ],
      "excerpt": "[arXiv] [Paper PDF] [Project Page] [Papers with Code] \nThis repo implements an improved version of the Space-Time Memory Network (STM) and is part of the accompanying code of Modular Interactive Video Object Segmentation: Interaction-to-Mask, Propagation and Difference-Aware Fusion (MiVOS). It can be used as: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9098383597709057
      ],
      "excerpt": "An integral component for reproducing and/or improving the performance in MiVOS. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9082502686363026,
        0.8273101935920342,
        0.9510096445123098
      ],
      "excerpt": "| Generate more synthetic data | :heavy_check_mark: | :x: | :x: | \nFPS is amortized, computed as total processing time / total number of frames irrespective of the number of objects, aka multi-object FPS. All times are measured on an RTX 2080 Ti with IO time excluded. Pre-computed results and evaluation outputs (either from local evaluation or CodaLab output log) are also provided. All evaluations are done in 480p resolution. \n(Note: This implementation is not optimal in speed. There are ways to speed it up but we wanted to keep it in its simplest PyTorch form.) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9567588029116127
      ],
      "excerpt": "| With BL pretraining | :x: | 89.9 | 92.2 | 91.0 | 15.5 | D16_s012_notop | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9567588029116127,
        0.947253525205545
      ],
      "excerpt": "| With BL pretraining | :heavy_check_mark: | 81.7 | 87.4 | 84.5 | 11.2 | D17_s012 | \nFor YouTubeVOS val and DAVIS test-dev we also tried the kernelized memory (called KM in our code) technique described in Kernelized Memory Network for Video Object Segmentation. It works nicely with our top-k filtering. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9567588029116127
      ],
      "excerpt": "| With BL30K, k=20 | :x: | 82.6 | 81.1 | 77.7 | 85.6 | 86.2 | YV_2018_val_s012 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9500664185058448,
        0.9500664185058448
      ],
      "excerpt": "| Full model with top-k | :x: | 82.0 | 80.6 | 77.3 | 84.7 | 85.5 | YV_val_s012 | \n| Full model with top-k | :heavy_check_mark: | 82.8 | 81.6 | 77.7 | 85.8 | 85.9 | YV_val_s012_km | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9500664185058448,
        0.9500664185058448,
        0.9660127238873896
      ],
      "excerpt": "| Full model with top-k | :x: | 72.7 | 80.2 | 76.5 | D17_testdev_s012 | \n| Full model with top-k | :heavy_check_mark: | 74.9 | 82.2 | 78.6 | D17_testdev_s012_km | \nThe W matrix can be considered as a dense correspondence (affinity) matrix. This is in fact how we used it in the fusion module. See try_correspondence.py for details. We have included a small GUI there to show the correspondences (a point source is used, but a mask/tensor can be used in general). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8464625893523539
      ],
      "excerpt": "Here we provide two pretrained models. One is pretrained on static images and transferred to main training (we call it s02: stage 0 -> stage 2); the other is pretrained on both static images and BL30K then transferred to main training (we call it s012). For the s02 model, we train it for 300K (instead of 150K) iterations in the main training stage to offset the extra training. More iterations do not help/help very little. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8754435553669339
      ],
      "excerpt": "| Model | Google Drive | OneDrive | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9876933634317248
      ],
      "excerpt": "BL30K is a synthetic dataset rendered using ShapeNet data and Blender. For details, see MiVOS. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8678166575281885
      ],
      "excerpt": "Google Drive is much faster in my experience. Your mileage might vary. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9273556005439357
      ],
      "excerpt": "We implemented training with Distributed Data Parallel (DDP) with two 11GB GPUs. Replace a, b with the GPU ids, cccc with an unused port number,  defg with a unique experiment identifier, and h with the training stage (0/1/2). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9762238097363893
      ],
      "excerpt": "While I did start building this from STM's official evaluation code, the official training code is not available and therefore a lot of details are missing. My own judgments are used in the engineering of this work. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8134097581454998,
        0.9703991755583085,
        0.9819680157495835
      ],
      "excerpt": "Top-k filtering (proposed by us) is included here \nOur raw performance (without BL30K or top-k) is slightly worse than the original STM model but I believe we train with fewer resources. \nCongratulations to the \u8def\u8fc7 team for winning a TianChi VOS competition with our framework and additional data augmentation. Their code is available here: https://github.com/mrgjbd/copy_paste_for_video_seg \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "[CVPR 2021] MiVOS - Mask Propagation module. Reproduced STM (and better) with training code :star2:. Semi-supervised video object segmentation evaluation.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/hkchengrex/Mask-Propagation/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 11,
      "date": "Thu, 30 Dec 2021 07:45:25 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/hkchengrex/Mask-Propagation/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "hkchengrex/Mask-Propagation",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "I recommend either softlinking (`ln -s`) existing data or use the provided `download_datasets.py` to structure the datasets as our format. `download_datasets.py` might download more than what you need -- just comment out things that you don't like. The script does not download BL30K because it is huge (>600GB) and we don't want to crash your harddisks. See below.\n\n```bash\n\u251c\u2500\u2500 BL30K\n\u251c\u2500\u2500 DAVIS\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 2016\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 Annotations\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 ...\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 2017\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 test-dev\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 Annotations\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 ...\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 trainval\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 Annotations\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 Mask-Propagation\n\u251c\u2500\u2500 static\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 BIG_small\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 YouTube\n    \u251c\u2500\u2500 all_frames\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 valid_all_frames\n    \u251c\u2500\u2500 train\n    \u251c\u2500\u2500 train_480p\n    \u2514\u2500\u2500 valid\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8187236509318547
      ],
      "excerpt": "One concrete example is: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8679519711125578
      ],
      "excerpt": "Produced using eval_davis_2016.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8679519711125578
      ],
      "excerpt": "Produced using eval_davis.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8679519711125578
      ],
      "excerpt": "Produced using eval_youtube.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8679519711125578
      ],
      "excerpt": "Produced using eval_davis.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8311741045194964
      ],
      "excerpt": "Try it yourself: python try_correspondence.py. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8881573616584233
      ],
      "excerpt": "The script download_model.py automatically downloads the s012 model. Put all pretrained models in Mask-Propagation/saves/. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.80513335864747,
        0.8443423844923084,
        0.8730066742762854,
        0.8966411003170377
      ],
      "excerpt": "One concrete example is: \nPre-training on static images: CUDA_VISIBLE_DEVICES=0,1 OMP_NUM_THREADS=4 python -m torch.distributed.launch --master_port 9842 --nproc_per_node=2 train.py --id retrain_s0 --stage 0 \nPre-training on the BL30K dataset: CUDA_VISIBLE_DEVICES=0,1 OMP_NUM_THREADS=4 python -m torch.distributed.launch --master_port 9842 --nproc_per_node=2 train.py --id retrain_s01 --load_network [path_to_trained_s0.pth]  --stage 1 \nMain training: CUDA_VISIBLE_DEVICES=0,1 OMP_NUM_THREADS=4 python -m torch.distributed.launch --master_port 9842 --nproc_per_node=2 train.py --id retrain_s012 --load_network [path_to_trained_s01.pth]  --stage 2 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9106931496915549
      ],
      "excerpt": "model/model.py - Training procedure. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/hkchengrex/Mask-Propagation/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "MiVOS (CVPR 2021) - Mask Propagation",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Mask-Propagation",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "hkchengrex",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/hkchengrex/Mask-Propagation/blob/main/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "hkchengrex",
        "body": "Pretrained model",
        "dateCreated": "2021-03-14T14:54:31Z",
        "datePublished": "2021-03-14T15:06:14Z",
        "html_url": "https://github.com/hkchengrex/Mask-Propagation/releases/tag/1.0",
        "name": "v1.0",
        "tag_name": "1.0",
        "tarball_url": "https://api.github.com/repos/hkchengrex/Mask-Propagation/tarball/1.0",
        "url": "https://api.github.com/repos/hkchengrex/Mask-Propagation/releases/39783100",
        "zipball_url": "https://api.github.com/repos/hkchengrex/Mask-Propagation/zipball/1.0"
      }
    ],
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We used these packages/versions in the development of this project. It is likely that higher versions of the same package will also work. This is not an exhaustive list -- other common python packages (e.g. pillow) are expected and not listed.\n\n- PyTorch `1.7.1`\n- torchvision `0.8.2`\n- OpenCV `4.2.0`\n- progressbar\n- [thinspline](https://github.com/cheind/py-thin-plate-spline) for training (`pip install git+https://github.com/cheind/py-thin-plate-spline`)\n- gitpython for training\n- gdown for downloading pretrained models\n\nRefer to the official [PyTorch guide](<https://pytorch.org/>) for installing PyTorch/torchvision. The rest can be installed by:\n\n`pip install progressbar2 opencv-python gitpython gdown git+https://github.com/cheind/py-thin-plate-spline`\n\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You can look at the corresponding scripts (`eval_davis.py`, `eval_youtube.py`, etc.). The arguments tooltip should give you a rough idea of how to use them. For example, if you have downloaded the datasets and pretrained models using our scripts, you only need to specify the output path: `python eval_davis.py --output [somewhere]` for DAVIS 2017 validation set evaluation.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 64,
      "date": "Thu, 30 Dec 2021 07:45:25 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "computer-vision",
      "deep-learning",
      "pytorch",
      "segmentation",
      "video-segmentation",
      "video-object-segmentation",
      "cvpr2021"
    ],
    "technique": "GitHub API"
  }
}