{
  "citation": [
    {
      "confidence": [
        0.9856363140359321
      ],
      "excerpt": "Zihao Ye, Qipeng Guo, Quan Gan, Xipeng Qiu, Zheng Zhang \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9278824608274014
      ],
      "excerpt": "BPT+GloVe: 92.12(\u00b10.11) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9625923568277878
      ],
      "excerpt": "- Machine Translation(WMT14 En-De, metric: BLEU), base setting. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/yzh119/BPT",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-08-13T15:56:20Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-21T15:21:33Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.983083058191796
      ],
      "excerpt": "This repo contains the code for our paper \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9831959594122849,
        0.9939496354355282
      ],
      "excerpt": "The code is written in DGL with PyTorch as backend. \nFollowing is the visualization of the sparse matrix of BPT underlying graph when sequence length is 8192 and k is 4. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8399455360964693,
        0.8793464108992509,
        0.9880949764800572
      ],
      "excerpt": "Note that our CUDA kernel uses atomic operations which may result in non-determinism, we report the mean and std of accuracy in multiple(10) runs. \nThe IMDB dataset has not official train/dev split, we follow the setting of Bryan et al., 2017 and hold out 10% samples for validation. We report the test accuracy of model with best valid loss. \nFor sentence level modeling, we show that BPT models better inductive bias than vanilla transformer by attending fine-grained features of neighbors and coarse-grained features of far-away tokens. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9424604597535
      ],
      "excerpt": "    - Transformer-base(our implementation): 27.2 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.820557272176312
      ],
      "excerpt": "- Natural Language Inference(SNLI, metric: accuracy), ESIM-like structure, 3 layers for self-attention and 3 layers for cross-sentence attention. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9849053448844326
      ],
      "excerpt": "        - Like Text Classification, the result on NLI is also not stable because of randomness in our CUDA kernel, we report the mean and std of accuracy in multiple(7) runs. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9567588029116127
      ],
      "excerpt": "Integrate kernels with dgl 0.5 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Source code of paper \"BP-Transformer: Modelling Long-Range Context via Binary Partitioning\"",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/yzh119/BPT/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 17,
      "date": "Tue, 28 Dec 2021 21:52:29 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/yzh119/BPT/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "yzh119/BPT",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/yzh119/BPT/master/get_text8.sh",
      "https://raw.githubusercontent.com/yzh119/BPT/master/get_wmt.sh",
      "https://raw.githubusercontent.com/yzh119/BPT/master/get_iwslt.sh",
      "https://raw.githubusercontent.com/yzh119/BPT/master/get_enwik8.sh",
      "https://raw.githubusercontent.com/yzh119/BPT/master/get_multi30k.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8623475411607111
      ],
      "excerpt": "To reproduce: python lm.py --config configs/enwik8-8192.yml --gpu 0,1,2,3,4,5,6,7 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8623475411607111
      ],
      "excerpt": "To reproduce: python text_classification.py --config configs/imdb-4.yml --gpu 0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8623475411607111
      ],
      "excerpt": "    - To reproduce: python mt.py --config configs/wmt-*.yml --gpu 0,1,2,3,4,5,6,7 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8623475411607111
      ],
      "excerpt": "    - To reproduce: python nli.py --config configs/snli.yml --gpu 0  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8623475411607111
      ],
      "excerpt": "    - To reproduce: python text_classification.py --config configs/sst5-2.yml --gpu 0 \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8418026326092306
      ],
      "excerpt": "To reproduce: python lm.py --config configs/enwik8-8192.yml --gpu 0,1,2,3,4,5,6,7 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8480597884698422
      ],
      "excerpt": "To reproduce: python mt.py --config configs/iwslt-4-64.yml --gpu 0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8502080156921068
      ],
      "excerpt": "To reproduce: python text_classification.py --config configs/imdb-4.yml --gpu 0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8418026326092306
      ],
      "excerpt": "    - To reproduce: python mt.py --config configs/wmt-*.yml --gpu 0,1,2,3,4,5,6,7 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8418026326092306
      ],
      "excerpt": "    - To reproduce: python nli.py --config configs/snli.yml --gpu 0  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8418026326092306
      ],
      "excerpt": "    - To reproduce: python text_classification.py --config configs/sst5-2.yml --gpu 0 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/yzh119/BPT/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Cuda",
      "C++",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 Zihao Ye\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "BP-Transformer",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "BPT",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "yzh119",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/yzh119/BPT/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- torchtext 0.4\n- dgl 0.4 (the code on master branch is not compatible with dgl 0.5, please checkout `develop` branch for dgl 0.5 compatible version).\n- yaml\n- spacy\n- PyTorch 1.1+\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 111,
      "date": "Tue, 28 Dec 2021 21:52:29 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "For Multi-GPU training, please `export NCCL_LL_THRESHOLD=0` before running scripts because of a PyTorch bug mentioned [here](https://github.com/pytorch/pytorch/issues/20630).\n\nThe codebase has two dependencies: `graph_kernel` and `graph_builder`, the first one is for efficient graph attention on GPU with node parallel strategy written in CUDA, the second one is for efficient graph construction written in Cython. To install them:\n```\ncd graph_builder\npython setup.py install\ncd ..\ncd graph_kernel\npython setup.py install\ncd ..\n``` \n\nWe support the following tasks with BPT as backbone:\n- Text Classification: `text_classification.py`\n- Language Modeling: `lm.py`\n- Machine Translation: `mt.py`\n- Natural Language Inference: `nli.py`\n\nAll experiment settings mentioned in our paper are available at `configs/`.\n\n```\npython *.py --config configs/*.yml --gpu [GPUs]\n```\n\nNote that this repo does not contain any data files, to get dataset required for experiments, run `. get_*.sh` and the corresponding dataset would be downloaded and preprocessed.\n\nFor machine translation, we have another script `mt_infer.py` for decoding:\n```\npython mt_infer.py --config configs/*.yml --gpu [GPU]\n``` \n\nBefore decoding, please make sure you have finished the training using `mt.py` with the same config file.\n\n**NOTE**:\nCurrently we do not support CPU training/inference.\n\n",
      "technique": "Header extraction"
    }
  ]
}