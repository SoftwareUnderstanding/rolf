{
  "citation": [
    {
      "confidence": [
        0.8199748500934674
      ],
      "excerpt": "This comes from Machine Perception class @ETH during 2020 spring semester. <br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9061005469315193,
        0.8356013927728488
      ],
      "excerpt": "  non li oche giaco, se Cercoresto daicra<br> \n  feruto alcundon par de' Fuorchi sonno,<br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "  mi fu maca scricciat\u00e0 il focose<br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488,
        0.8111036989382164
      ],
      "excerpt": "  de l'un cheggio, e a le piaggi traddume.<br> \n<img src=\"https://github.com/GiuliaLanzillotta/exercises/blob/master/compressions.jpg\"> \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/GiuliaLanzillotta/My-ML-exercises",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-03-13T17:57:16Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-21T10:47:49Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9173889116725589,
        0.967063489213573,
        0.9950049735629369,
        0.9375513106810864
      ],
      "excerpt": "The idea is to train an LSTM on character-level text prediction using a single .txt file. I chose a text from the Gutenberg repository, namely la Divina Commedia , by Dante Alighieri.  \n<br> Maybe what is most exciting about this work is perfectly synthesized by Andrej Karpathy (the author of the blog post):  \nThere\u2019s something magical about Recurrent Neural Networks (RNNs). [...] Sometimes the ratio of how simple your model is to the quality of the results you get out of it blows past your expectations, and this was one of those times.  \nHere's a preview of the results:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9737771165383996,
        0.8751025491644577
      ],
      "excerpt": "The goal of this notebook was to explore different clustering algorithms in the setting of Image Compression. <br> \nAbove you can see the result obtained with K-means for different values of k (the number of clusters). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9673789600846879,
        0.8813301035070621,
        0.9432967781407455,
        0.9248008543390261,
        0.8405306918369074
      ],
      "excerpt": "Implementation of Generating sentences from a continuous space paper<br> \nHere's a link to the paper. \nWhat I want to explore here is the expression of sentiment in generative models. <br> \nThe dataset consists of two different samples of tweets, one with positive sentiment and one with negative sentiment. <br> \nThe goal is to train two generators on the two sets separetly and analyse the qualitative differences. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8413527395367097,
        0.9721867086059184,
        0.9466585656341012
      ],
      "excerpt": "This notebook will explore the magic of GANs. <br> \nWe are going to refer to a particular GAN architecture : the Cycle GAN. Here's a link to the paper for the more curious.  \nThe goal? Taking a picture of my beautiful sister and turn it into a painting, to see how she would have looked like a few centuries ago. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9795255383626384,
        0.8705767807497179,
        0.8684911688615964
      ],
      "excerpt": "This exercise draws inspiration from a pair of lectures in the ```Computational Intelligence Lab``` class ```@ETH``` during 2020 spring semester. The topic is *signal processing* in the realm of *lossy data compression*. <br> \nThe notebook goes through some maths and applies it to 1D signal first, and to images in the end.  \nIn the plot below you can see the results of image compression using FFT transform (center) and DCT transform (right). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A few exercises on machine learning-related problems. ",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/GiuliaLanzillotta/exercises/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 22 Dec 2021 21:01:38 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/GiuliaLanzillotta/My-ML-exercises/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "GiuliaLanzillotta/My-ML-exercises",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/GiuliaLanzillotta/exercises/master/PLSA.ipynb",
      "https://raw.githubusercontent.com/GiuliaLanzillotta/exercises/master/Writing_like_Dante.ipynb",
      "https://raw.githubusercontent.com/GiuliaLanzillotta/exercises/master/Turning_my_sister_into_an_old_painting.ipynb",
      "https://raw.githubusercontent.com/GiuliaLanzillotta/exercises/master/Intro_to_quantum_computing.ipynb",
      "https://raw.githubusercontent.com/GiuliaLanzillotta/exercises/master/Sentiment_analysis.ipynb",
      "https://raw.githubusercontent.com/GiuliaLanzillotta/exercises/master/FGSM.ipynb",
      "https://raw.githubusercontent.com/GiuliaLanzillotta/exercises/master/Adversarial_attacks_on_MNIST.ipynb",
      "https://raw.githubusercontent.com/GiuliaLanzillotta/exercises/master/Color_compression.ipynb",
      "https://raw.githubusercontent.com/GiuliaLanzillotta/exercises/master/Adversarial_defense.ipynb",
      "https://raw.githubusercontent.com/GiuliaLanzillotta/exercises/master/Tweet_Generator.ipynb",
      "https://raw.githubusercontent.com/GiuliaLanzillotta/exercises/master/shapley_values.ipynb",
      "https://raw.githubusercontent.com/GiuliaLanzillotta/exercises/master/Signal_processing.ipynb",
      "https://raw.githubusercontent.com/GiuliaLanzillotta/exercises/master/Collaborative%20filtering.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8514963447188377
      ],
      "excerpt": "Note: to be able run this notebook you need to have the .zip datasets in your current working file system, or in your Google Drive folder. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/GiuliaLanzillotta/My-ML-exercises/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "C++"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Exercises :)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "My-ML-exercises",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "GiuliaLanzillotta",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/GiuliaLanzillotta/My-ML-exercises/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Wed, 22 Dec 2021 21:01:38 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This repo gathers various exercises on ML problems. Some of them are still active projects, others are completed. \n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "This comes from ```Computational Intelligence Lab``` class ```@ETH``` during 2020 spring semester. <br>\nThe exercise explores basic matrix completion/factorization techniques and exposes their limitation. \nAlso, you'll find an hand-made implementation of SGD.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "This comes from ```Computational Intelligence Lab``` class ```@ETH``` during 2020 spring semester. <br>\nThe exercise explores Probabilistic latent semantic analysis as presented in the paper https://arxiv.org/pdf/1301.6705.pdf. \nThe goal of the exercise is to implement PLSA by using the EM algorithm, and to apply it to the Associated Press corpus dataset.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "This comes from ```Computational Intelligence Lab``` class ```@ETH``` during 2020 spring semester. <br>\nThis is an exploratory notebook used for the (ongoing) *Kaggle Competition* https://www.kaggle.com/c/cil-text-classification-2020/data.\n\nUp until now in the notebook I have implemented:\n- vocabulary extraction \n- **GloVe** embedding training with **Stochastic Gradient Descent**\n- training pipeline (using the learned embeddings to train a sentiment classifier)\n- predictions pipeline (using the learned embeddings and the trained classifier to predict tweets sentiment)\n\n**Update 12.04.20**:<br>\nThe project moved to a dedicated repo. You can see all the changes there. \n\n*Note*: to be able run this notebook you need to have the *.zip* datasets in your current working file system, or in your *Google Drive* folder.\n\n",
      "technique": "Header extraction"
    }
  ]
}