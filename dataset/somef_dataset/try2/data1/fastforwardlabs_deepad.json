{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1901.03407",
      "https://arxiv.org/abs/1606.03498"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9999878864718271
      ],
      "excerpt": "| GAN (BiGAN)                | <ul><li>Supports variational inference (probabilistic measure of uncertainty) </li><li>Use of discriminator signal allows better learning of data manifold Mihaela Rosca (2018), Issues with VAEs and GANs, CVPR18(useful for high dimensional image data).</li><li>GANs trained in semi-supervised learning mode have shown great promise, even with very few labeled data Raghavendra Chalapathy et al. (2019) \"Deep Learning for Anomaly Detection: A Survey\"</li></ul> | <ul><li>Requires a large amount of training data, and longer training time (epochs) to arrive at stable results Tim Salimans et al. (2016) \"Improved Techniques for Training GANs\", Neurips 2016</li><li>Training can be unstable (GAN mode collapse)</li></ul> | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/fastforwardlabs/deepad",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-01-31T22:04:31Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-16T09:10:03Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "|                              |                          |                            |\n| :--------------------------: | :----------------------: | :------------------------: |\n|         AutoEncoder          | Variational AutoEncoder  |           BiGAN            |\n|   ![](metrics/ae/roc.png)    | ![](metrics/vae/roc.png) | ![](metrics/bigan/roc.png) |\n|           Seq2Seq            |           PCA            |           OCSVM            |\n| ![](metrics/seq2seq/roc.png) | ![](metrics/pca/roc.png) | ![](metrics/ocsvm/roc.png) |\n\nFor each model, we use labeled test data to first select a threshold that yields the best accuracy and then report on metrics such as f1, f2, precision, and recall at that threshold. We also report on ROC (area under the curve) to evaluate the overall skill of each model. Given that the dataset we use is not extremely complex (18 features), we see that most models perform relatively well. Deep models (BiGAN, AE) are more robust (precision, recall, ROC AUC), compared to PCA and OCSVM. The sequence-to-sequence model is not particularly competitive, given the data is not temporal. On a more complex dataset (e.g., images), we expect to see (similar to existing research), more pronounced advantages in using a deep learning model.\n\nFor additional details on each model, see our [report](https://ff12.fastforwardlabs.com/). Note that models implemented here are optimized for tabular data. For example, extending this to work with image data will usually require the use of convolutional layers (as opposed to dense layers) within the neural network models to achieve performant results.\n\n|                                  |                              |                                |\n| :------------------------------: | :--------------------------: | :----------------------------: |\n|           AutoEncoder            |   Variational AutoEncoder    |             BiGAN              |\n|   ![](metrics/ae/metrics.png)    | ![](metrics/vae/metrics.png) | ![](metrics/bigan/metrics.png) |\n|             Seq2Seq              |             PCA              |             OCSVM              |\n| ![](metrics/seq2seq/metrics.png) | ![](metrics/pca/metrics.png) | ![](metrics/ocsvm/metrics.png) |\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9936991362974561,
        0.9582538567264357
      ],
      "excerpt": "This repo contains experimental code used to implement deep learning techniques for the task of anomaly detection and launches an interactive dashboard to visualize model results applied to a network intrusion use case. We include implementations of several neural networks (Autoencoder, Variational Autoencoder, Bidirectional GAN, Sequence Models) in Tensorflow 2.0 and two other baselines (One Class SVM, PCA).  \nFor an in-depth review of the concepts presented here, please consult the Cloudera Fast Forward report Deep Learning for Anomaly Detection. Additionally, two related prototypes are available for reference - Blip & Anomagram \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9924587443977252,
        0.9115575362285298,
        0.9761932930528336,
        0.9669045190506964,
        0.9436346117928655,
        0.9076327045763576
      ],
      "excerpt": "Anomalies - often referred to as outliers, abnormalities, rare events, or deviants - are data points or patterns in data that do not conform to a notion of normal behavior. Anomaly detection, then, is the task of finding those patterns in data that do not adhere to expected norms, given previous observations. The capability to recognize or detect anomalous behavior can provide highly useful insights across industries. Flagging unusual cases or enacting a planned response when they occur can save businesses time, costs, and customers. Hence, anomaly detection has found diverse applications in a variety of domains, including IT analytics, network intrusion analytics, medical diagnostics, financial fraud protection, manufacturing quality control, marketing and social media analytics, and more. \nThe underlying strategy for most approaches to anomaly detection is to first model normal behavior, and then exploit this knowledge to identify deviations (anomalies). In this repo, the process includes the following steps: \nBuild a model of normal behavior using available data. Typically the model is trained on normal behavior data or assumes a small amount of abnormal data. \nBased on this model, assign an anomaly score to each data point that represents a measure of deviation from normal behavior. The models in this repo use a reconstruction error approach, where the model attempts to reconstruct a sample at test time, and uses the reconstruction error as an anomaly score. The intuition is that normal samples will be reconstructed with almost no error, while abnormal/unusual samples will be reconstructed with larger error margins. \nApply a threshold on the anomaly score to determine which samples are anomalies. \nAs an illustrative example, an autoencoder model is trained on normal samples where the task is to reconstruct the input. At test time, we can use the reconstruction error (mean squared error) for each sample as anomaly scores. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "\u251c\u2500\u2500 data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9442575497391057,
        0.9537082619567245
      ],
      "excerpt": "The cml folder contains the artifacts needed to configure and launch the project on Cloudera Machine Learning (CML). \nThe models directory contains modules for each of the model implementations. Each module comes with code to specify parameters and methods for training and computing an anomaly score. It also serves as the holding location of saved models after training. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9329336962960141
      ],
      "excerpt": "This script contains code to train and then evaluate each model by generating a histogram of anomaly scores assigned by each model, and ROC curve to assess model skill on the anomaly detection task. The general steps taken in the script are: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.92998344837666,
        0.9227024162453319,
        0.9823257451930152,
        0.9984191923464204,
        0.860059181823877
      ],
      "excerpt": "Trains all of the models (Autoencoder, Variational Autoencoder, Bidirectional GAN, Sequence Models, PCA, OCSVM) \nEvaluates the models on a test split (8000 inliers, 2000 outliers). Generates charts on model performance: histogram of anomaly scores, ROC, general metrics (f1,f2, precision, recall, accuracy) \nGiven the differences between the deep learning methods discussed above (and their variants), it can be challenging to decide on the right model. When data contains sequences with temporal dependencies, a \nsequence-to-sequence model (or architectures with LSTM layers) can model these relationships well, yielding better results. For scenarios requiring principled estimates of uncertainty, generative models such as a VAE and GAN based approaches are suitable. For scenarios where the data is images, AEs, VAEs and GANs designed with convolution layers are suitable. The following table highlights the pros and cons of the different types of models, to provide guidance on when they are a good fit. \n| Model                      | Pros                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | Cons                                                                                                                                                                                                                                                                                                | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8864208836988493
      ],
      "excerpt": "| Sequence-to-Sequence Model | <ul><li>Well suited for data with temporal components (e.g., discretized time series data)</li></ul>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | <ul><li>Slow inference (compute scales with sequence length which needs to be fixed)</li><li>Training can be slow</li><li>Limited accuracy when data contains features with no temporal dependence</li><li>Supports variational inference (probabilistic measure of uncertainty)</li></ul>          | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9819427982677009,
        0.9784199666672277,
        0.940419160217277,
        0.834543718463456,
        0.8539936076070572
      ],
      "excerpt": "For users interested in deploying this application on Cloudera Machine Learning, there are three ways to launch the project: \nFrom Prototype Catalog - Navigate to the Prototype Catalog on a CML workspace, select the \"Deep Learning for Anomaly Detection\" tile, click \"Launch as Project\", click \"Configure Project\" \nAs ML Prototype - In a CML workspace, click \"New Project\", add a Project Name, select \"ML Prototype\" as the Initial Setup option, copy in the repo URL, click \"Create Project\", click \"Configure Project\" \nManual Setup - In a CML workspace, click \"New Project\", add a Project Name, select \"Git\" as the Initial Setup option. Launch a Python3 Workbench Session with at least 4GB of memory and run the cml/cml_build.py script which will create a CML Application and provide a link to the UI.  \nRegardless of the launch method, the following steps are performed for you: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9571158797199792
      ],
      "excerpt": "Model Training - A CML job is executed which consists of a call to train.py. This in turn trains a model and saves the model to the models/savedmodel folder. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Deep Learning for Anomaly Deteection",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/fastforwardlabs/deepad/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 15,
      "date": "Wed, 22 Dec 2021 21:00:20 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/fastforwardlabs/deepad/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "fastforwardlabs/deepad",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.805828519031474
      ],
      "excerpt": "Download the KDD dataset if not already downloaded \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9455558911430924
      ],
      "excerpt": "Install Dependencies - All necessary package dependencies are installed into the new project. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8360669713900878
      ],
      "excerpt": "As an illustrative example, an autoencoder model is trained on normal samples where the task is to reconstruct the input. At test time, we can use the reconstruction error (mean squared error) for each sample as anomaly scores. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991
      ],
      "excerpt": "\u2502   \u251c\u2500\u2500 kdd_data_gen.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991,
        0.9336801098518991,
        0.9336801098518991
      ],
      "excerpt": "\u2502   \u251c\u2500\u2500 cml_build.py \n\u2502   \u251c\u2500\u2500 cml_servemodel.py \n\u2502   \u251c\u2500\u2500 install_deps.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991,
        0.9336801098518991,
        0.9336801098518991,
        0.9336801098518991,
        0.9336801098518991,
        0.9336801098518991,
        0.8924976426181745,
        0.9336801098518991,
        0.9336801098518991,
        0.9336801098518991,
        0.950563948951535,
        0.9521174821235511
      ],
      "excerpt": "\u2502   \u251c\u2500\u2500 ae.py \n\u2502   \u251c\u2500\u2500 bigan.py \n\u2502   \u251c\u2500\u2500 ocsvm.py \n\u2502   \u251c\u2500\u2500 pca.py \n\u2502   \u251c\u2500\u2500 seq2seq.py \n\u2502   \u251c\u2500\u2500 vae.py \n\u251c\u2500\u2500 utils \n\u2502   \u251c\u2500\u2500 data_utils.py \n\u2502   \u251c\u2500\u2500 eval_utils.py \n\u2502   \u251c\u2500\u2500 train_utils.py \n\u251c\u2500\u2500 train.py \n\u251c\u2500\u2500 test.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8249016075410863
      ],
      "excerpt": "Model Training - A CML job is executed which consists of a call to train.py. This in turn trains a model and saves the model to the models/savedmodel folder. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/fastforwardlabs/deepad/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "JavaScript",
      "CSS",
      "HTML",
      "SCSS"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Apache License 2.0",
      "url": "https://api.github.com/licenses/apache-2.0"
    },
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Deep Learning for Anomaly Detection",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "deepad",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "fastforwardlabs",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/fastforwardlabs/deepad/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 40,
      "date": "Wed, 22 Dec 2021 21:00:20 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "anomaly-detection",
      "deep-learning",
      "outliers"
    ],
    "technique": "GitHub API"
  }
}