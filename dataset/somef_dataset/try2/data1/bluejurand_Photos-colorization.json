{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1712.03400",
      "https://arxiv.org/abs/1603.08511",
      "https://arxiv.org/abs/1603.06668"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9309341280294594,
        0.834695500396296
      ],
      "excerpt": "[2] Richard Zhang, Phillip Isola, Alexei A. Efros, Colorful Image Colorization, \n[3] Gustav Larsson, Michael Maire, Gregory Shakhnarovich, Learning Representations for Automatic Colorization, \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/bluejurand/Photos-colorization",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-05-20T17:29:18Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-12T08:21:17Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Created model is working correctly, suprisingly well taking into acount the fact that it was trained only\non computer with only one graphics processing unit. Analyzing image results one can conclude that model\nhas tendency to leave image with faded colors. Another problem is leaving small object not colored.\nPossible solutions to that drawbacks could be to train model on computer with more powerfull GPUs,\nlike Amazon EC2 p3.16xlarge. Furthermore good option could be addition of class rebalancing basing on\nprobability like in [2] paper.  \n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8608726680654488,
        0.9746410087045827
      ],
      "excerpt": "Presented algorithm is able to colorize black-white photographies. Graph above shows model architecture.  \nCode is implemented in keras API with tensorflow backend. Resources which helped to establish this code  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9419577908793865
      ],
      "excerpt": "Main change in the structure of the model was the swap of feature extractor model from inception-resnet-v2 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.955633193449745,
        0.9252688368040194
      ],
      "excerpt": "To practice deep learning in keras enviroment, transfer learning and image processing. \nTest capabilities of modern algorithms in face of demanding task of image colorization. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8503829381739865
      ],
      "excerpt": "Deep Learning \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9086844611681346,
        0.9501260171915216,
        0.9409181438334504,
        0.9456447980294932
      ],
      "excerpt": "Image beneth shows implemented model architecture. It is basing on encoder-decoder network mixed with \ntransfer learning model, which is used as a feature extractor. Encoder-decoder structure is called \nautoencoder. For images it bases on convolutions and subsequent convolutions with upscaling. \nConvolutions when reducing the size of the image learns the latent representation of the grayscale image. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.904194768115752,
        0.94951696816067,
        0.9152384363167041,
        0.981158596102786
      ],
      "excerpt": "In that case the transfer learning model is xception. Name xception comes from extreme version of inception. \nWhen in inception 1x1 convolutions were used to project the original input into several separate, \nsmaller input spaces, and from each of those input spaces were used a different type of filter to transform \nthose smaller 3D blocks of data. Xception goes further and instead of partitioning input data into several \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.980695548994058,
        0.9007866751769263,
        0.8214712461003553,
        0.8889130025693758,
        0.9769783480215418,
        0.9711316781272328
      ],
      "excerpt": "The output of Xception model is a 1000 feature vector which is replicated and added to the output of encoder. \nThis operation is followed by decoder, which restores the input image size. \nBelow are presented in order from left to right: \n- original image, which was an input to the algorithm; \n- grayscale image, it is only the luminance component, one of the outcomes to CIELab transformation; \n- resut image, which is the output of the presented model.   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.952999848547738,
        0.9857135051255395,
        0.9087794437998546
      ],
      "excerpt": "Implementation of algorithm to the historical photo: \nCouple of photographies which shows the drawbacks of the model. Colorized only part of the images, \nleaving a large parts black and white. Mistakes in color sleection for some of the elements. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8656725617077721
      ],
      "excerpt": "[1] Federico Baldassarre, Diego Gonz\u00e1lez Morin, Lucas Rod\u00e9s-Guirao, Deep Koalarization: Image Colorization using CNNs and Inception-Resnet-v2, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9458469575324786,
        0.8917530883276923
      ],
      "excerpt": "[4] Satoshi Iizuka, Edgar Simo-Serra, Hiroshi Ishikawa, Let there be Color!: Joint End-to-end Learning of Global and Local Image Priors  \nfor Automatic Image Colorization with Simultaneous Classification, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9606277698624849
      ],
      "excerpt": "[5] Dipanjan Sarkar, Raghav Bali, Tamoghna Ghosh, Hands-On Transfer Learning with Python: Implement advanced deep learning and neural network models using TensorFlow and Keras \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Keras repository which colorize black-white images.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/bluejurand/Photos-colorization/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sun, 26 Dec 2021 13:45:01 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/bluejurand/Photos-colorization/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "bluejurand/Photos-colorization",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Python is a requirement (Python 3.3 or greater, or Python 2.7). Recommended enviroment is Anaconda distribution to install Python and Spyder (https://www.anaconda.com/download/).\n\n__Installing dependencies__  \nTo install can be used pip command in command line.  \n  \n\tpip install -r requirements.txt\n\n__Installing python libraries__  \nExemplary commands to install python libraries:\n \n\tpip install numpy  \n\tpip install pandas  \n\tpip install xgboost  \n\tpip install seaborn \n\n__Running code__  \nEverything is executed from file main.py. Go to directory where code is downoladed and run a command: \n\n\tpy main.py\n\t\nAdditional requirement is Tensorflow GPU support. Process of configuiring it is described [here](https://www.tensorflow.org/install/gpu).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8233516307540644
      ],
      "excerpt": "Following upscaling done by decoder learns to reconstruct the color version of photography. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/bluejurand/Photos-colorization/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Photos colorization",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Photos-colorization",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "bluejurand",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/bluejurand/Photos-colorization/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sun, 26 Dec 2021 13:45:01 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\tdef image_a_b_gen(generator, transfer_learning_generator, transfer_learning_model):\n\t\tfor rgb_image, rgb_tl_image in zip(generator, transfer_learning_generator):\n\t\t\tlab_image = rgb2lab(rgb_image[0])\n\t\t\tluminance = lab_image[:, :, :, [0]]\n\t\t\tab_components = lab_image[:, :, :, 1:] / 128\n\t\t\ttl_model_features = []\n\t\t\tlab_image_tl = rgb2lab(rgb_tl_image[0])\n\t\t\tluminance_tl = lab_image_tl[:, :, :, [0]]\n\n\t\t\tfor i, sample in enumerate(luminance_tl):\n\t\t\t\tsample = gray2rgb(sample)\n\t\t\t\tsample = sample.reshape((1, 331, 331, 3))\n\t\t\t\tembedding = transfer_learning_model.predict(sample)\n\t\t\t\ttl_model_features.append(embedding)\n\n\t\t\ttl_model_features = np.array(tl_model_features)\n\t\t\ttl_model_features_shape_2d = backend.int_shape(Lambda(lambda x: x[:, 0, :], dtype='float32')(tl_model_features))\n\t\t\ttl_model_features = tl_model_features.reshape(tl_model_features_shape_2d)\n\t\t\tyield ([tl_model_features, luminance], ab_components) \n<!-- -->\n\tdef build_encoder(encoder_input):\n\t\tencoder_output = Conv2D(64, (3, 3), activation='relu', padding='same', strides=2)(encoder_input)\n\t\tencoder_output = Conv2D(128, (3, 3), activation='relu', padding='same')(encoder_output)\n\t\tencoder_output = Conv2D(128, (3, 3), activation='relu', padding='same', strides=2)(encoder_output)\n\t\tencoder_output = Conv2D(256, (3, 3), activation='relu', padding='same')(encoder_output)\n\t\tencoder_output = Conv2D(256, (3, 3), activation='relu', padding='same', strides=2)(encoder_output)\n\t\tencoder_output = Conv2D(512, (3, 3), activation='relu', padding='same')(encoder_output)\n\t\tencoder_output = Conv2D(512, (3, 3), activation='relu', padding='same')(encoder_output)\n\t\tencoder_output = Conv2D(256, (3, 3), activation='relu', padding='same')(encoder_output)\n\t\treturn encoder_output\n\n",
      "technique": "Header extraction"
    }
  ]
}