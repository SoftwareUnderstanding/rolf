{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\n@article{xiong2021nystromformer,\n  title={Nystr{\\\"o}mformer: A Nystr{\\\"o}m-based Algorithm for Approximating Self-Attention},\n  author={Xiong, Yunyang and Zeng, Zhanpeng and Chakraborty, Rudrasis and Tan, Mingxing and Fung, Glenn and Li, Yin and Singh, Vikas},\n  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},\n  year={2021}\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{xiong2021nystromformer,\n  title={Nystr{\\\"o}mformer: A Nystr{\\\"o}m-based Algorithm for Approximating Self-Attention},\n  author={Xiong, Yunyang and Zeng, Zhanpeng and Chakraborty, Rudrasis and Tan, Mingxing and Fung, Glenn and Li, Yin and Singh, Vikas},\n  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},\n  year={2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8090016440670298
      ],
      "excerpt": "  /bin/bash -c \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8090016440670298
      ],
      "excerpt": "  /bin/bash -c \\ \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mlpen/Nystromformer",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-12-11T02:20:47Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-22T05:39:40Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8910281160637226,
        0.9502865874548753,
        0.9456071661834996,
        0.8774873239822306,
        0.9978906823070279
      ],
      "excerpt": "We extended segment-means to compute landmarks without requiring the sequence length divisible by the number of landmarks. Then we used this Nystromformer to  \nperform deployment of T2T-Vit_t-14 for image classification without retraining. Our T2T-ViT-Nys-14 achieves 78% top-1 accuracy, outperforming performer/Linformer +4.3%/+12.7% for the direct deployment. \nWe fixed the coefficient computation of initial Z_0, which can lead to faster convergence to pseudoinverse. The original implementation has a scale difference.  \nWe leave the original as a default option. The added initialization is recommended. Thanks @sbodenstein for pointing out the difference. \nWe have released the source code of PyTorch reimplementation of Long Range Arena (LRA) benchmark, which is to evaluate the generalization ability of models on diverse longer sequence tasks. Our codes are based on the official Jax LRA implementation. Reformer PyTorch implementation is from huggingface and Performer PyTorch implementation is from lucidrains. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9972213658614143,
        0.9595310359178567
      ],
      "excerpt": "Transformers have emerged as a powerful workhorse for a broad range of natural language processing tasks. A key component that drives the impressive performance of Transformers is their self-attention mechanism that identifies/encodes the influence or dependence of other tokens for each specific token. Its benefits notwithstanding, the quadratic complexity of self-attention on the input sequence length has limited its application to longer sequences \u2013 a topic being actively studied in the community. To address this limitation, we propose Nystromformer \u2013 a model that exhibits excellent scalability as a function of sequence length. Our idea is based on adapting the Nystrom method to approximate the standard self-attention with an efficient O(n) complexity. \nThe pretraining dataset consists of English Wikipedia and BookCorpus. For pretraining on long sequence, we added one third Stories and one third Realnews. All downloaded data files should be placed in the corresponding folder under data-preprocessing. The original format of English Wikipedia dump is preprocessed using \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "  -v \"$PWD:/model\" \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "  -v \"$PWD:/model\" \\ \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mlpen/Nystromformer/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 24,
      "date": "Sun, 26 Dec 2021 16:09:04 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/mlpen/Nystromformer/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "mlpen/Nystromformer",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/mlpen/Nystromformer/main/reorganized_code/start_docker.sh",
      "https://raw.githubusercontent.com/mlpen/Nystromformer/main/reorganized_code/LRA/start_lra.sh",
      "https://raw.githubusercontent.com/mlpen/Nystromformer/main/code/bash_scripts/glue_search.sh",
      "https://raw.githubusercontent.com/mlpen/Nystromformer/main/LRA/datasets/create_datasets.sh",
      "https://raw.githubusercontent.com/mlpen/Nystromformer/main/LRA/code/run_tasks.sh"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.8314823419435533
      ],
      "excerpt": "<img src=\"img/LRA.png\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8314823419435533
      ],
      "excerpt": "<img src=\"img/avg_LRA.png\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8425822579863308
      ],
      "excerpt": "docker run --rm --name=pretrain \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.881173410293818
      ],
      "excerpt": "docker run --rm --name=glue \\ \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/mlpen/Nystromformer/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Nystromformer: A Nystrom-based Algorithm for Approximating Self-Attention",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Nystromformer",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "mlpen",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mlpen/Nystromformer/blob/main/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\ndocker, nvidia-docker\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 248,
      "date": "Sun, 26 Dec 2021 16:09:04 GMT"
    },
    "technique": "GitHub API"
  }
}