{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "generative design\nGeodesign\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.873420782039883
      ],
      "excerpt": "- Higher order theory of consciousness \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8944178096468923
      ],
      "excerpt": "Good video explanation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9195926162616405
      ],
      "excerpt": "Category : unsupervised machine learning \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9195926162616405
      ],
      "excerpt": "Category : unsupervised machine learning \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/basile-bron/Artificial-Intelligence-Bible",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-08-16T08:47:26Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-03-11T07:38:59Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.990952226023931,
        0.9375133114083641,
        0.8704112699032779
      ],
      "excerpt": "This repository is aiming to regroup most of the definition in the field of AI. Since there is a lot of things to remember, I would advice the use of flashcard if you are new to the field. \nWeak artificial intelligence (weak AI), also known as narrow AI, is artificial intelligence that is focused on one narrow task. Weak AI is defined in contrast to strong AI, a machine with the ability to apply intelligence to any problem, rather than just one specific problem, sometimes considered to require consciousness, sentience and mind). Many currently existing systems that claim to use \"artificial intelligence\" are likely operating as a weak AI focused on a narrowly defined specific problem. \nSiri is a good example of narrow intelligence. Siri operates within a limited pre-defined range of functions. There is no genuine intelligence or no self-awareness despite being a sophisticated example of weak AI. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9879868740878568,
        0.9405233987496834
      ],
      "excerpt": "Artificial general intelligence (AGI) is the intelligence of a machine that has the capacity to understand or learn any intellectual task that a human being can. It is a primary goal of some artificial intelligence research and a common topic in science fiction and future studies. Some researchers refer to Artificial general intelligence as \"strong AI\", \"full AI\" or as the ability of a machine to perform \"general intelligent action\"; others reserve \"strong AI\" for machines capable of experiencing consciousness. \nSome references emphasize a distinction between strong AI and \"applied AI\" (also called \"narrow AI\" or \"weak AI\"): the use of software to study or accomplish specific problem solving or reasoning tasks. Weak AI, in contrast to strong AI, does not attempt to perform the full range of human cognitive abilities. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9752980395612063
      ],
      "excerpt": "On a side note, here is the 3 theory of consciousness that are taken seriously by the neurology community: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9968029537584643,
        0.8822822109793921
      ],
      "excerpt": "- biological theory of consciousness \n- Higher order theory of consciousness \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9383732902122189
      ],
      "excerpt": "The activation function of a node defines the output of that node given an input or set of inputs. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.978261328341983
      ],
      "excerpt": "i.e Dropout is a method that allow you to \"turn off\" some of the neurones randomly. the most common dropout probability for a layer is 0.5 but it can be tune at will. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8364859981881485
      ],
      "excerpt": "While in GD, you have to run through ALL the samples in your training set to do a single update for a parameter in a particular iteration, in SGD, on the other hand, you use ONLY ONE or SUBSET of training sample from your training set to do the update for a parameter in a particular iteration. If you use SUBSET, it is called Minibatch Stochastic gradient Descent. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9719688380910874
      ],
      "excerpt": "SGD often converges much faster compared to GD but the error function is not as well minimized as in the case of GD. Often in most cases, the close approximation that you get in SGD for the parameter values are enough because they reach the optimal values and keep oscillating there. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9856701230634042
      ],
      "excerpt": "Unsupervised learning is a type of self-organized Hebbian learning that helps find previously unknown patterns in data set without pre-existing labels. It is also known as self-organization and allows modelling probability densities of given inputs. It is one of the main three categories of machine learning, along with supervised and reinforcement learning. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9037116844379766
      ],
      "excerpt": "the four main task of unsupervised learning are: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8814258894474294
      ],
      "excerpt": "Group similar instance together into cluster \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8446407140594459
      ],
      "excerpt": "Learn what a \"normal\" data look like, and use it to detect abnormal instances. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.985097056066915
      ],
      "excerpt": "Estimating how isolated a data is. i.g instance located in low density region are likely to be Anomaly. it is usefull for data analysis, data visualisation and anomaly detection. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8795556503480305
      ],
      "excerpt": "- Data analysis \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9394643255625444
      ],
      "excerpt": "K-means is generally linear with regard to m, k, n. However if the structure does not have a clustering structure it can increase exponentially but it is rarely the case, generally speaking K-Means is one of the fastest clustering algorithms. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9048059159686598,
        0.9045438160169788,
        0.9828987133251804,
        0.8599776990139394
      ],
      "excerpt": "1) Kmeans tries to create same sized cluster no matter how the data is scattered \n2) Kmeans doesnt work well for non-globular structures \n3) Kmeans doesnt care about how dense the data is present \n4) Curse of dimensionality affects kmeans at high dimension since it uses distance measure \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9056537592890719,
        0.9968029537584643,
        0.8474010374126262
      ],
      "excerpt": "- Spatial \n- Clustering of \n- Application with \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.904655932941579
      ],
      "excerpt": "- Set of points \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8701917466230847,
        0.8898417676665318,
        0.9747699022462425,
        0.9495269864652703
      ],
      "excerpt": "1) For each instance, the algorithm count how many instances are located within a small distance \"epsilon\". this region is called the epsilon-neighbourhood. \n2) If an instance has at least min_sample instances in its epsilon-neighbourhood (including itself then it is considered a core instance) in other word the instance is considered to be in a dense regions. \n3) All instances in the neighbourhood of a core instance belong to the same cluster. this neighbourhood may include other core instances; therefore, a long sequence of neighbouring core instances form a single cluster. \n4) Any instance that is not a core instance and does not have one in it's neighbourhood is considered an anomaly. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8007696939111447
      ],
      "excerpt": "- How to choose the density level (i.e min_neighbors) ? \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9751615022972041
      ],
      "excerpt": "In layman\u2019s terms, we find a suitable value for epsilon by calculating the distance to the nearest n points for each point, sorting and plotting the results. Then we look to see where the change is most pronounced (think of the angle between your arm and forearm) and select that as epsilon. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9673083984190803
      ],
      "excerpt": "Reduce the dimension of a dataset visualisation, to have a better understanding of the data. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9897662640228564
      ],
      "excerpt": "the variance ratio is regarded as the amount of information kept during the dimensionality reduction, you generally don't want the variance to go lower than 95% unless it is for data visualisation. In that case use 2 or 3 dimension to have a clear visual of your data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9916567179316859
      ],
      "excerpt": "\"An autoencoder is a type of artificial neural network used to learn efficient data coding in an unsupervised manner. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore signal \u201cnoise\u201d. Along with the reduction side, a reconstructing side is learnt, where the autoencoder tries to generate from the reduced encoding a representation as close as possible to its original input, hence its name.\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9196130857746437,
        0.931885458591953
      ],
      "excerpt": "Auto encoder are just neural network where the target output is the input. and with a bottleneck in the middle. \nBasicaly it is an \"auto compression algorithms\" but it goes further than that: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8008561970704818,
        0.9475532854695479,
        0.9841002579288386
      ],
      "excerpt": "Semi-supervised is a hybridization of supervised and unsupervised techniques. \nSupervised learning is the machine learning task of learning a function that maps an input to an output based on example input-output pairs. It infers a function from labelled training data consisting of a set of training examples. In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyses the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a \"reasonable\" way (see inductive bias). \nThe parallel task in human and animal psychology is often referred to as concept learning. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9355162300934644,
        0.9602254268274945,
        0.9352761618009462
      ],
      "excerpt": "Artificial neural networks (ANN) or connectionist systems are computing systems that are inspired by, but not necessarily identical to, the biological neural networks that constitute animal brains. Such systems \"learn\" to perform tasks by considering examples, generally without being programmed with any task-specific rules. For example, in image recognition, they might learn to identify images that contain cats by analysing example images that have been manually labelled as \"cat\" or \"no cat\" and using the results to identify cats in other images. They do this without any prior knowledge about cats, for example, that they have fur, tails, whiskers and cat-like faces. Instead, they automatically generate identifying characteristics from the learning material that they process. \nAn ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it. \nThe perceptron is one of the very first algorithms of machine learning, and the most simple artificial neural network. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9862255522299723
      ],
      "excerpt": "In a CNN the filters contain the weight \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9843830912903376
      ],
      "excerpt": "- Padding is a technique that add border of x number of pixel on an image \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8867360058189546
      ],
      "excerpt": "- To allowed to do convolution without loosing pixel on the image itself. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8245379531836012
      ],
      "excerpt": "Applying max pooling on a matrix will reduce the size of the given image/matrix. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9915730266177147,
        0.9229794342941458
      ],
      "excerpt": "This is done to in part to help over-fitting by providing an abstracted form of the representation. As well, it reduces the computational cost by reducing the number of parameters to learn and provides basic translation invariance to the internal representation. \nit is quicker than convoluting \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9124552651245452,
        0.9582012101461612
      ],
      "excerpt": "Now the question is \"ok, but how do I remember their place of I only have the max number?\" \nIn practice, this is achieved by creating a mask that remembers the position of the values used in the first phase, which we can later utilize to transfer the gradients. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8717633669284067,
        0.857559132841841,
        0.9136475926373603
      ],
      "excerpt": "- Max pooling: The maximum pixel value of the batch is selected. \n- Min pooling: The minimum pixel value of the batch is selected. \n- Average pooling: The average value of all the pixels in the batch is selected. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8232680344797968
      ],
      "excerpt": "What is a recurrent neural network ? \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8467274529797315
      ],
      "excerpt": "The Hopfield network is an Auto-associative Memory Network \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9443292446783146
      ],
      "excerpt": "Linear regression allow us to put a straight line that match the trend of a graph with 2 output (or more with multiple linear regression). It is used to then make prediction. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9860190446176182
      ],
      "excerpt": "Logistic regression is a binary classifier. It is different from linear regression, in the sense that it is used to predict a boolean. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8980279620923272
      ],
      "excerpt": "| Require very little data preparation. (no need for scalling or normalization)       | A small change in the data can cause a large change in the structure of the decision tree causing instability.      | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9757871396622286,
        0.8606974167894562
      ],
      "excerpt": "Now at this point it kind of feel like magic. but take a look a the following graph on the picture to have a better intuition of what the algorithm do. \nto clarify a the equation itself here is a video that worked for me (brave yourself for the Indian accent). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9760276209561869,
        0.8367230187474699,
        0.8186374126194227,
        0.9484974947688446,
        0.8222297320165052,
        0.9105816021419029
      ],
      "excerpt": "The transformer is composed of multiple parts and subparts. \nIt firstly consists of the encoder, decoder and a final linear layer. \nThen in the encoder the first concept is the attention mechanism or \"Scaled Dot-Product Attention\" as stated in the paper. \nThe second concept is the \"Multi-Head Attention\" which is simply the parallelized version of the former \nIn a nutshell self attention is basically getting the vectors after the word embedding layer and mixing/influencing each vector to one another. \nThe output is then the same amount of vector/word but each one is now semantically related to the context of the sentence. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8314840041085997
      ],
      "excerpt": "What is the difference between attention and self attention mechanism ? \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.880017208135507
      ],
      "excerpt": "Visual of what the mechanism is doing. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9501101767944186
      ],
      "excerpt": "Whet is the difference between Artificial Neural Network and deep learning? \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9333017873958236
      ],
      "excerpt": "What are the pros and cons of supervised vs unsupervised learning ? \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9134819464492666
      ],
      "excerpt": "Is running more epoch really a direct cause of Overfitting ? \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Artificial Intelligence Bible",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/basile-bron/Artificial-Intelligence-Bible/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Fri, 24 Dec 2021 04:57:43 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/basile-bron/Artificial-Intelligence-Bible/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "basile-bron/Artificial-Intelligence-Bible",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8534211533666184
      ],
      "excerpt": "This thread made me realise that if you want to know if you tuned your hyperparameter properly you should run as much epoch as possible. if it does not overfit then you won. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8778487586960795
      ],
      "excerpt": "example ? \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8241358140383946
      ],
      "excerpt": "| Missing value does not affect noticeably the result| Often require higher time to train the model | \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/basile-bron/Artificial-Intelligence-Bible/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Artificial-Intelligence-Lexical",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Artificial-Intelligence-Bible",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "basile-bron",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/basile-bron/Artificial-Intelligence-Bible/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Fri, 24 Dec 2021 04:57:43 GMT"
    },
    "technique": "GitHub API"
  }
}