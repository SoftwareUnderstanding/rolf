{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2108.03348, which augments the Transformer architecture with residual edge channels. The resultant architecture can directly process graph-structured data and acheives good results on supervised graph-learning tasks as presented by [Dwivedi et al.](https://arxiv.org/abs/2003.00982",
      "https://arxiv.org/abs/2003.00982",
      "https://arxiv.org/abs/2103.09430",
      "https://arxiv.org/abs/2108.03348"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please cite the following paper if you find the code useful:\n```\n@article{hussain2021edge,\n  title={Edge-augmented Graph Transformers: Global Self-attention is Enough for Graphs},\n  author={Hussain, Md Shamim and Zaki, Mohammed J and Subramanian, Dharmashankar},\n  journal={arXiv preprint arXiv:2108.03348},\n  year={2021}\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{hussain2021edge,\n  title={Edge-augmented Graph Transformers: Global Self-attention is Enough for Graphs},\n  author={Hussain, Md Shamim and Zaki, Mohammed J and Subramanian, Dharmashankar},\n  journal={arXiv preprint arXiv:2108.03348},\n  year={2021}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/shamim-hussain/egt",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-09-13T03:50:55Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-08T10:27:13Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We included two Jupyter notebooks to demonstrate how the HDF5 datasets are created\n* For the medium scale datasets view `create_hdf_benchmarking_datasets.ipynb`. You will need `pytorch`, `ogb==1.1.1` and `dgl==0.4.2` libraries to run the notebook. The notebook is also runnable on Google Colaboratory.\n* For the large scale pcqm4m dataset view `create_hdf_pcqm4m.ipynb`. You will need `pytorch`, `ogb>=1.3.0` and `rdkit>=2019.03.1` to run the notebook.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "This is the official implementation of the **Edge-augmented Graph Transformer (EGT)** as described in https://arxiv.org/abs/2108.03348, which augments the Transformer architecture with residual edge channels. The resultant architecture can directly process graph-structured data and acheives good results on supervised graph-learning tasks as presented by [Dwivedi et al.](https://arxiv.org/abs/2003.00982). It also achieves good performance on the large-scale [PCQM4M-LSC](https://arxiv.org/abs/2103.09430) (`0.1263 MAE` on val) dataset. EGT beats convolutional/message-passing graph neural networks on a wide range of supervised tasks and thus demonstrates that convolutional aggregation is not an essential inductive bias for graphs.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8501020417438102
      ],
      "excerpt": "The config file can contain many different configurations, however, the only required configuration is scheme, which specifies the training scheme. If the other configurations are not specified, a default value will be assumed for them. Here are some of the commonly used configurations: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8951745612276321
      ],
      "excerpt": "initial_lr: Initial learning rate. In case of warmup it is the maximum learning rate. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9368013325506174
      ],
      "excerpt": "min_lr_factor: The factor by which the minimum LR is smaller, of the initial LR. Default is 0.01. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9713560676966503,
        0.9383264853351184,
        0.9181755782011435,
        0.8194639830398168
      ],
      "excerpt": "model_width: The dimensionality of the node channels d_h. \nedge_width: The dimensionality of the edge channels d_e. \nnum_heads: The number of attention heads. Default is 8. \nffn_multiplier: FFN multiplier for both channels. Default is 2.0 . \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9787729905103795,
        0.9371296746616603
      ],
      "excerpt": "mlp_layers: Dimensionality of the final MLP layers, specified as a list of factors with respect to d_h. Default is [0.5, 0.25]. \ngate_attention: Set this to False to get the ungated EGT variant (EGT-U). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9001975728312612
      ],
      "excerpt": "edge_channel_type: Used to create ablated variants of EGT. A value of \"residual\" (default) implies pure/full EGT. \"constrained\" implies EGT-constrained. \"bias\" implies EGT-simple. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9469422401298087
      ],
      "excerpt": "[For SVD-based encodings]: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9966474696551649
      ],
      "excerpt": "sel_svd_features: Rank of the SVD encodings r. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9394449182630016
      ],
      "excerpt": "[For Eigenvectors encodings]: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Edge-Augmented Graph Transformer",
      "technique": "GitHub API"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "For our experiments, we converted the datasets to HDF5 format for the convenience of using them without any specific library. Only the `h5py` library is required. The datasets can be downloaded from - \n* Medium-scale datasets (GNN Benchmarking Datasets by Dwivedi et al.) : https://zenodo.org/record/5500978\n* Large-scale dataset (PCQM4M by Hu et al.) : https://zenodo.org/record/5501020\n\nOr you can simply run the provided bash scripts `download_medium_scale_datasets.sh`, `download_large_scale_datasets.sh`. The default location of the datasets is the *datasets* directory.\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/shamim-hussain/egt/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Thu, 23 Dec 2021 17:56:31 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/shamim-hussain/egt/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "shamim-hussain/egt",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/shamim-hussain/egt/master/create_hdf_pcqm4m.ipynb",
      "https://raw.githubusercontent.com/shamim-hussain/egt/master/create_hdf_benchmarking_datasets.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/shamim-hussain/egt/master/download_medium_scale_datasets.sh",
      "https://raw.githubusercontent.com/shamim-hussain/egt/master/download_large_scale_datasets.sh"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.9460398540955819,
        0.9134407059978614,
        0.8863114046997479
      ],
      "excerpt": "Once the training is started a model folder will be created in the models directory, under the specified dataset name. This folder will contain a copy of the input config file, for the convenience of resuming training/evaluation. Also, it will contain a config.json which will contain all configs, including unspecified default values, used for the training. Training will be checkpointed per epoch. In case of any interruption you can resume training by running the run_training.py with the config.json file again. \nIn case you wish to finalize training midway, just stop training and run end_training.py script with the config.json file to save the model weights. \nAfter training, you can run the do_evaluations.py script with the same config file to perform evaluations. Alongside being printed to stdout, results will be saved in the predictions directory, under the model directory. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8251243364082053,
        0.8219397488031032
      ],
      "excerpt": "save_path: The training process will create a model directory containing the logs, checkpoints, configs, model summary and predictions/evaluations. By default it creates a folder at models/<dataset_name>/<model_name> but it can be changed via this config. \ncache_dir: During first time of training/evaluation the data will be cached to a tensorflow cache format. Default path is data_cache/<dataset_name>/<positional_encoding>. But it can be changed via this config. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.806231503790735
      ],
      "excerpt": "batch_size: Batch size. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/shamim-hussain/egt/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2021 Md Shamim Hussain\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Edge-augmented Graph Transformer",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "egt",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "shamim-hussain",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/shamim-hussain/egt/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* `python >= 3.7`\n* `tensorflow >= 2.1.0`\n* `h5py >= 2.8.0`\n* `numpy >= 1.18.4`\n* `scikit-learn >= 0.22.1`\n\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You must create a `JSON` config file containing the configuration of a model, its training and evaluation configs (configurations). The same config file is used to do both training and evaluations.\n\n* To run training: ```python run_training.py <config_file.json>```\n* To end training (prematurely): ```python end_training.py <config_file.json>```\n* To perform evaluations: ```python do_evaluations.py <config_file.json>```\n\nConfig files for the main results presented in the paper are contained in the *configs/main* directory, whereas configurations for the ablation study are contained in the *configs/ablation* directory. The paths and names of the files are self-explanatory.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3,
      "date": "Thu, 23 Dec 2021 17:56:31 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "transformer",
      "graphs",
      "attention",
      "gnns",
      "graph-learning",
      "self-attention"
    ],
    "technique": "GitHub API"
  }
}