{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "I thank the support from [Bloomberg Data Science Ph.D. Fellowship](https://www.techatbloomberg.com/bloomberg-data-science-ph-d-fellowship/).\nWe thank the reviewers and [Yixin Nie](https://easonnie.github.io/) \nand [Jie Lei](https://www.cs.unc.edu/~jielei/)\nfor their helpful discussions.\nPart of the code are built based on huggingface [transformers](https://github.com/huggingface/transformers) and \nfacebook [xlm](https://github.com/facebookresearch/XLM) and [faiss](https://github.com/facebookresearch/faiss).\n\n4K3.\n",
      "technique": "Header extraction"
    }
  ],
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find our project useful, please cite this paper:\n```\n@inproceedings{tan2020vokenization,\n  title={Vokenization: Improving Language Understanding with Contextualized, \nVisual-Grounded Supervision},\n  author={Tan, Hao and Bansal, Mohit},\n  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing},\n  year={2020}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{tan2020vokenization,\n  title={Vokenization: Improving Language Understanding with Contextualized, \nVisual-Grounded Supervision},\n  author={Tan, Hao and Bansal, Mohit},\n  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing},\n  year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9726805786299247
      ],
      "excerpt": "Visual-Grounded Supervision\" (Hao Tan and Mohit Bansal). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.842790493796475
      ],
      "excerpt": "    * Extracting Visual Feature \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "1. Wiki103 (around 10 min) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/airsplay/vokenization",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-10-12T22:58:40Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-02T02:49:05Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9521719160278395
      ],
      "excerpt": "PyTorch code for the EMNLP 2020 paper \"Vokenization: Improving Language Understanding with Contextualized,  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8642726892620535
      ],
      "excerpt": "    * Downloading Image and Captioning Data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8669499845061871
      ],
      "excerpt": "    * Downloading Pure-Language Data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8214601497835897
      ],
      "excerpt": "ingore the code blocks related to \"English Wikipedia\". \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9637696718545037,
        0.9178859640160374
      ],
      "excerpt": "In this module (corresponding to Sec 3.2 of the paper),  \nwe want to learn a token-image matching model from sentence-image aligned data (i.e., image captioning data). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9827690710376555
      ],
      "excerpt": "The model is trained on MS COCO with pairwise hinge loss (details in Sec. 3.2 of the paper). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8883167946690528
      ],
      "excerpt": ": \"0,1\" indicates using the GPUs 0 and 1. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9360049494801472
      ],
      "excerpt": ": \"--lang bert\" is the langaugae backbone \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8419433816083766,
        0.9816071115820924,
        0.9624558571665929,
        0.9512745765186755
      ],
      "excerpt": "The language backbones are initialized from huggingface transformers. \nWe found that the results with XLNet is pretty low but have not identified  \nthe reason. Results of other backbones are similar. \nThe vokenization is a bridge between the cross-modality (words-and-image) matching models (xmatching) and  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9287590538789227
      ],
      "excerpt": "The final goal is to convert the language tokens to related images  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9622518575664345,
        0.8846996061659639,
        0.8381402790019441,
        0.8801752798735896
      ],
      "excerpt": "These vokens enable the visual supervision of the language model. \nWe mainly provide pr-eprocessing tools (i.e., feature extraction, tokenization, and vokenization) and \nevaluation tools of previous cross-modal matching models here. \nHere is a diagram of these processes and we next discuss them one-by-one: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9152977101704372
      ],
      "excerpt": "We next tokenize the language corpus. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "data  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9064103871485357
      ],
      "excerpt": "The image pre-processing extracts the image features to build the keys in the vokenization retrieval process. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9611843351973686
      ],
      "excerpt": "It is used to unify the image ids in different experiments  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8036451950461622,
        0.8191410946630464
      ],
      "excerpt": "so that we would not get different indexing in different retrieval experiments. \nNote: The ids created by create_image_ids.py are only the order of the images. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9109644530506509
      ],
      "excerpt": "Extract image features regarding the list built above, using code  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8871852068188951,
        0.8322367234735498
      ],
      "excerpt": "We benchmark the performance of cross-modal matching models from large scale. \nThe evaluation includes two different metrics: diversity and the retrieval performance. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9201359869876435
      ],
      "excerpt": "measures the correspondence of the token and the retrieved images. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8953300957652736
      ],
      "excerpt": "After all these steps, we could start to vokenize the language corpus. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9087637595831598
      ],
      "excerpt": "The code is optimized and could be continued by just rerunning it. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.854810922316412
      ],
      "excerpt": "e.g., the image id vg_nococo/8 corresponds to 8-th feature \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.883076852447474
      ],
      "excerpt": "    #: Note: mp is the abbreviation for \"multi-processing\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9693233393948762
      ],
      "excerpt": "to vokenize a corpus.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9159850700596901,
        0.9838710958027822
      ],
      "excerpt": "(based on faiss). \nAs discussed in Sec. 2 of the paper, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.864480121855841
      ],
      "excerpt": "After the vokenization process of wiki103, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8901885858440062
      ],
      "excerpt": "dataset with the support of voken supervisions. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9394449182630016
      ],
      "excerpt": "for comparisons. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.930013490909484,
        0.864480121855841
      ],
      "excerpt": "the max per-gpu-batch-size is 32 with O1 but 64 with O2. \nAfter the vokenization process of wiki103, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8901885858440062
      ],
      "excerpt": "dataset with the support of voken supervisions. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8835576208958956
      ],
      "excerpt": "and around 5-7 days to finish in 4 Titan Pascal/T4 cards. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8329227999259182
      ],
      "excerpt": "Titan Pascal would also save some memory with the --fp16 option. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8875533684437961
      ],
      "excerpt": "We also provide pure language-model pre-training as baselines. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9579016041213193
      ],
      "excerpt": "dataset with the masked language model only. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "PyTorch code for EMNLP 2020 Paper \"Vokenization: Improving Language Understanding with Visual Supervision\"",
      "technique": "GitHub API"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Download MS COCO images:\n    ```shell script\n    #: MS COCO (Train 13G, Valid 6G)\n    mkdir -p data/mscoco\n    wget http://images.cocodataset.org/zips/train2014.zip -P data/mscoco\n    wget http://images.cocodataset.org/zips/val2014.zip -P data/mscoco\n    unzip data/mscoco/train2014.zip -d data/mscoco/images/ && rm data/mscoco/train2014.zip\n    unzip data/mscoco/val2014.zip -d data/mscoco/images/ && rm data/mscoco/val2014.zip\n    ```\n   If you already have COCO image on disk. Save them as \n    ```\n    data\n      |-- mscoco\n            |-- images\n                 |-- train2014\n                         |-- COCO_train2014_000000000009.jpg\n                         |-- COCO_train2014_000000000025.jpg\n                         |-- ......\n                 |-- val2014\n                         |-- COCO_val2014_000000000042.jpg\n                         |-- ......\n    ```\n\n2. Download captions (split following the LXMERT project):\n    ```shell script\n    mkdir -p data/lxmert\n    wget https://nlp.cs.unc.edu/data/lxmert_data/lxmert/mscoco_train.json -P data/lxmert/\n    wget https://nlp.cs.unc.edu/data/lxmert_data/lxmert/mscoco_nominival.json -P data/lxmert/\n    wget https://nlp.cs.unc.edu/data/lxmert_data/lxmert/vgnococo.json -P data/lxmert/\n    wget https://nlp.cs.unc.edu/data/lxmert_data/lxmert/mscoco_minival.json -P data/lxmert/\n    ```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "We provide scripts to get the datasets \"wiki103\" and \"wiki\".\nWe would note them as \"XX-cased\" or \"XX-uncased\" where the suffix \"cased\" / \"uncased\" only indicates\nthe property of the raw text.\n1. **Wiki103**. The [wiki103](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/) dataset\nis a seleted subset of English Wikipedia, containing around 100M tokens.\n    ```shell script\n    bash data/wiki103/get_data_cased.sh\n    ```\n2. **English Wikipedia**. \nThe script to download and process wiki data are modified from [XLM](https://github.com/facebookresearch/XLM).\nIt will download a 17G file. \nThe speed depends on the networking and it usually takes several hours to filter the data.\nThe process ends with around 2.8B tokens.\n    ```shell script\n    bash data/wiki/get_data_cased.bash en\n    ```\n    Note: For *RoBERTa*, it requires an untokenized version of wiki (o.w. the results would be much lower), \n    so please use the following command:\n    ```shell script\n    bash data/wiki/get_data_cased_untokenized.bash en\n    ```\n   \n> Note: I recommend to focus on \"Wiki103\" first and \n> ingore the code blocks related to \"English Wikipedia\".\n> \"Eng Wiki\" might take too long to complete.\n   \n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Since MS COCO images are used in training the cross-modal matching model\nas in [xmatching](#contextualized-cross-modal-matching-xmatching).\nWe will use the [Visual Genome](https://visualgenome.org/) images as \ncandidate vokens for retrievel.\nWe here download the images first.\n```shell script\nwget https://cs.stanford.edu/people/rak248/VG_100K_2/images.zip -P data/vg/\nwget https://cs.stanford.edu/people/rak248/VG_100K_2/images2.zip -P data/vg/\nunzip data/vg/images.zip -d data/vg/images && rm data/vg/images.zip\nunzip data/vg/images2.zip -d data/vg/images && rm data/vg/images2.zip\ncd data/vg/images\nmv VG_100K/* .\nmv VG_100K_2/* .\nrm -rf VG_100K VG_100K_2\ncd ../../../\n```\nIf you already have Visual Genome image on disk. Save them as \n```\ndata\n|-- vg\n    |-- images\n         |-- 1000.jpg\n         |-- 1001.jpg\n         |-- ......\n```\n    \n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "This downloaindg scrip is copied from [huggingface transformers](https://github.com/huggingface/transformers/tree/master/examples/text-classification)\nproject.\nSince the [transformers](https://github.com/huggingface/transformers) is still under dense\ndevelopment, the change of APIs might affect the code. \nI have upgraded the code compaticability to transformers==3.3.\n```shell script\nwget https://raw.githubusercontent.com/huggingface/transformers/master/utils/download_glue_data.py\npython download_glue_data.py --data_dir data/glue --tasks all\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/airsplay/vokenization/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 17,
      "date": "Sat, 25 Dec 2021 13:26:48 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/airsplay/vokenization/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "airsplay/vokenization",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/airsplay/vokenization/master/data/wiki103/get_data_uncased.sh",
      "https://raw.githubusercontent.com/airsplay/vokenization/master/data/wiki103/get_data_cased.sh",
      "https://raw.githubusercontent.com/airsplay/vokenization/master/data/wiki/install-tools.sh",
      "https://raw.githubusercontent.com/airsplay/vokenization/master/data/wiki/tools/tokenize.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```shell script\npip install -r requirements.txt\n```\n\nRequire python 3.6 + (to support huggingface [transformers](https://github.com/huggingface/transformers)).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.833114516308531
      ],
      "excerpt": "    bash tokenization/tokenize_wiki103_bert.bash \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.833114516308531
      ],
      "excerpt": "    bash tokenization/tokenize_wiki_bert.bash \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.825255480014364,
        0.8567125915659521
      ],
      "excerpt": ": bash scripts/extract_keys.bash $GPU_ID $MODEL_NAME \nbash scripts/extract_keys.bash 0 bert_resnext  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8567125915659521
      ],
      "excerpt": "bash scripts/xmatching_benchmark.bash 0 bert_resnext \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8627668017712228
      ],
      "excerpt": "Note: --tokenizer-name must be provided in the script. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8567125915659521,
        0.8567125915659521
      ],
      "excerpt": "    #: bash scripts/mpvokenize_wiki103.bash $USE_GPUS $SNAP_NAME \n    bash scripts/mpvokenize_wiki103.bash 0,1,2,3 bert_resnext \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8567125915659521,
        0.8567125915659521
      ],
      "excerpt": "    #: bash scripts/mpvokenize_wiki.bash $USE_GPUS $SNAP_NAME \n    bash scripts/mpvokenize_wiki.bash 0,1,2,3 bert_resnext \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8567125915659521
      ],
      "excerpt": "bash scripts/small_vlm_wiki103.bash 0,1,2,3 wiki103_bert_small \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8916055175911868
      ],
      "excerpt": "please install the nvidia/apex library with command: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9876636780183828,
        0.9906248903846466
      ],
      "excerpt": "git clone https://github.com/NVIDIA/apex \ncd apex \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8448888739279247
      ],
      "excerpt": "the script scripts/small_vlm_wiki103.bash. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8567125915659521
      ],
      "excerpt": "bash scripts/base_vlm_wiki.bash 0,1,2,3 wiki_bert_base \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9642739075518953
      ],
      "excerpt": "installing apex). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8132069836281186
      ],
      "excerpt": "Other tasks could be evaluated following the setup here \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8567125915659521
      ],
      "excerpt": "bash scripts/run_glue_epochs.bash 0,1,2,3 snap/vlm/wiki103_bert_small --snaps 7                           \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8567125915659521
      ],
      "excerpt": "bash scripts/run_glue_epochs.bash 0 snap/vlm/wiki103_bert_small --snaps 1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8567125915659521
      ],
      "excerpt": "bash scripts/small_wiki103.bash 0,1,2,3 bert_small \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8500511993155683
      ],
      "excerpt": "Or you could directly using the script small_wiki103_glue.bash to  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8567125915659521
      ],
      "excerpt": "bash scripts/small_wiki103_glue.bash 0,1,2,3 bert_small \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8567125915659521
      ],
      "excerpt": "bash scripts/base_wiki.bash 0,1,2,3 bert_wiki \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8567125915659521
      ],
      "excerpt": "bash scripts/base_wiki_glue.bash 0,1,2,3 bert_wiki \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8483819940059046
      ],
      "excerpt": "    * Model Training \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8031395565064957
      ],
      "excerpt": ": Speed: 20 min ~ 30 min / 1 Epoch, 20 Epochs by default. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8206333918867882,
        0.8152308343893572
      ],
      "excerpt": "Taking the wiki103 dataset and BERT tokenizer as an example,  \nwe convert the training file into \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.829692766188175,
        0.8603166186777739
      ],
      "excerpt": "        |-- wiki.train.raw.bert-base-uncased.line \nThe txt file wiki.train.raw.bert-base-uncased saves the tokens and each line in this file is the tokens of a line  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8256446405646283
      ],
      "excerpt": "wiki.train.raw.bert-base-uncased.line to index the starting token index of each line. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8433661752020046
      ],
      "excerpt": "Each line has a range \"line[i]\" to \"line[i+1]\" in the hdf5 file. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991
      ],
      "excerpt": "vokenization/create_image_ids.py.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python vokenization/create_image_ids.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991,
        0.8181783310633743
      ],
      "excerpt": "vokenization/extract_vision_keys.py.  \nThe code will first read the image ids saved in data/vokenization/images/{IMAGE_SET}_ids.txt and locate the images. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8880636280088927
      ],
      "excerpt": "(in vokenization/evaluate_diversity.py) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8880636280088927
      ],
      "excerpt": "(in vokenization/evaluate_retrieval.py)  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8253721852349334
      ],
      "excerpt": "We gather these two utils into one script and the command here: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8056220637023023
      ],
      "excerpt": "It will run a BERT-12Layers-768Hiddens (same as BERT_BASE) model on the English Wikipedia \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/airsplay/vokenization/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 Hao Tan\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Vokenization",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "vokenization",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "airsplay",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/airsplay/vokenization/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 160,
      "date": "Sat, 25 Dec 2021 13:26:48 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "For all results saved under `snap/` (whatever the dir names),\nrunning the folloing command will print out all the results.\n```bash\npython vlm/show_glue_results_epochs.py \n```\n\nIt will print results like\n```\nsnap/vlm/test_finetune/glueepoch_checkpoint-epoch0019\n     RTE    MRPC   STS-B    CoLA   SST-2    QNLI     QQP    MNLI MNLI-MM    GLUE\n   54.51   84.72   87.18   52.32   90.02   88.36   87.16   81.92   82.57   78.75\nsnap/vlm/bert_6L_512H_wiki103_sharedheadctr_noshuffle/glueepoch_checkpoint-epoch0029\n     RTE    MRPC   STS-B    CoLA   SST-2    QNLI     QQP    MNLI MNLI-MM    GLUE\n   58.12   82.76   84.45   26.74   89.56   84.40   86.52   77.56   77.99   74.23\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}