{
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Dongcf/Pytorch_Bert_Text_Classification",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-08-28T09:52:03Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-08-28T10:02:32Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9572011217221116
      ],
      "excerpt": "This repo contains a PyTorch implementation of a pretrained BERT model  for multi-label text classification. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9652771208610523
      ],
      "excerpt": "At the root of the project, you will see: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "|  \u2514\u2500\u2500 model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8596982069667689
      ],
      "excerpt": "|  \u2514\u2500\u2500 output #:save the ouput of model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.974376041305329,
        0.9528725823047942
      ],
      "excerpt": "When converting the tensorflow checkpoint into the pytorch, it's expected to choice the \"bert_model.ckpt\", instead of \"bert_model.ckpt.index\", as the input file. Otherwise, you will see that the model can learn nothing and give almost same random outputs for any inputs. This means, in fact, you have not loaded the true ckpt for your model \nWhen using multiple GPUs, the non-tensor calculations, such as accuracy and f1_score, are not supported by DataParallel instance \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "\u8fd8\u5728\u8c03\u8bd5",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Dongcf/Pytorch_Bert_Text_Classification/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 28 Dec 2021 10:46:41 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Dongcf/Pytorch_Bert_Text_Classification/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Dongcf/Pytorch_Bert_Text_Classification",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8498444657937956
      ],
      "excerpt": "note: for the new pytorch-pretrained-bert package . use comd from pytorch_pretrained_bert.modeling import BertPreTrainedModel \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9336801098518991,
        0.9336801098518991
      ],
      "excerpt": "|  |  \u2514\u2500\u2500 lrscheduler.py\u3000\u3000 \n|  |  \u2514\u2500\u2500 trainingmonitor.py\u3000 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8369245672576114
      ],
      "excerpt": "|  |  \u2514\u2500\u2500 basic_config.py #:a configuration file for storing model parameters \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9152750849795331,
        0.9336801098518991
      ],
      "excerpt": "|  |  \u2514\u2500\u2500 dataset.py\u3000\u3000 \n|  |  \u2514\u2500\u2500 data_transformer.py\u3000\u3000 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8277380791614312
      ],
      "excerpt": "|  \u2514\u2500\u2500 output #:save the ouput of model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8653523522410159,
        0.828520898273279
      ],
      "excerpt": "|  \u2514\u2500\u2500 train #:used for training a model \n|  |  \u2514\u2500\u2500 trainer.py  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8197328191806175,
        0.9336801098518991,
        0.9336801098518991
      ],
      "excerpt": "|  \u2514\u2500\u2500 utils #: a set of utility functions \n\u251c\u2500\u2500 convert_tf_checkpoint_to_pytorch.py \n\u251c\u2500\u2500 train_bert_multi_label.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8768003128815295
      ],
      "excerpt": "training result: \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Dongcf/Pytorch_Bert_Text_Classification/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Bert multi-label text classification by PyTorch",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Pytorch_Bert_Text_Classification",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Dongcf",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Dongcf/Pytorch_Bert_Text_Classification/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- csv\n- tqdm\n- numpy\n- pickle\n- scikit-learn\n- PyTorch 1.0\n- matplotlib\n- pandas\n- pytorch_pretrained_bert (load bert model)\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 28 Dec 2021 10:46:41 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "you need download pretrained bert model (`uncased_L-12_H-768_A-12`)\n\n1. Download the Bert pretrained model from [Google](https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip) and place it into the `/pybert/model/pretrain` directory.\n2. `pip install pytorch-pretrained-bert` from [github](https://github.com/huggingface/pytorch-pretrained-BERT).\n3. Run `python convert_tf_checkpoint_to_pytorch.py` to transfer the pretrained model(tensorflow version)  into pytorch form .\n4. Prepare [kaggle data](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data), you can modify the `io.data_transformer.py` to adapt your data.\n5. Modify configuration information in `pybert/config/basic_config.py`(the path of data,...).\n6. Run `python train_bert_multi_label.py` to fine tuning bert model.\n7. Run `python inference.py` to predict new data.\n\n\n",
      "technique": "Header extraction"
    }
  ]
}