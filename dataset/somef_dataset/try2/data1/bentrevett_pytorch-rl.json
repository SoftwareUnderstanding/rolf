{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1602.01783",
      "https://arxiv.org/abs/1506.02438",
      "https://arxiv.org/abs/1707.06347"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* 'Reinforcement Learning: An Introduction' - http://incompleteideas.net/sutton/book/the-book-2nd.html\n* 'Algorithms for Reinforcement Learning' - https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf\n* List of key papers in deep reinforcement learning - https://spinningup.openai.com/en/latest/spinningup/keypapers.html",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/bentrevett/pytorch-rl",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-03-18T17:24:34Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-24T02:26:50Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Tutorials for reinforcement learning in PyTorch and Gym by implementing a few of the popular algorithms. [IN PROGRESS]",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/bentrevett/pytorch-rl/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 48,
      "date": "Sat, 25 Dec 2021 17:25:25 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/bentrevett/pytorch-rl/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "bentrevett/pytorch-rl",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/bentrevett/pytorch-rl/master/2%20-%20Actor%20Critic%20%5BCartPole%5D.ipynb",
      "https://raw.githubusercontent.com/bentrevett/pytorch-rl/master/dqn_working.ipynb",
      "https://raw.githubusercontent.com/bentrevett/pytorch-rl/master/3%20-%20Advantage%20Actor%20Critic%20%28A2C%29%20%5BCartPole%5D.ipynb",
      "https://raw.githubusercontent.com/bentrevett/pytorch-rl/master/1%20-%20Vanilla%20Policy%20Gradient%20%28REINFORCE%29%20%5BCartPole%5D.ipynb",
      "https://raw.githubusercontent.com/bentrevett/pytorch-rl/master/3_advantage_actor_critic.ipynb",
      "https://raw.githubusercontent.com/bentrevett/pytorch-rl/master/3a%20-%20Advantage%20Actor%20Critic%20%28A2C%29%20%5BLunarLander%5D.ipynb",
      "https://raw.githubusercontent.com/bentrevett/pytorch-rl/master/4%20-%20Generalized%20Advantage%20Estimation%20%28GAE%29%20%5BCartPole%5D.ipynb",
      "https://raw.githubusercontent.com/bentrevett/pytorch-rl/master/8%20-%20n%20step%20A2C.ipynb",
      "https://raw.githubusercontent.com/bentrevett/pytorch-rl/master/5a%20-%20Proximal%20Policy%20Optimization%20%28PPO%29%20%5BLunarLander%5D.ipynb",
      "https://raw.githubusercontent.com/bentrevett/pytorch-rl/master/4a%20-%20Generalized%20Advantage%20Estimation%20%28GAE%29%20%5BLunarLander%5D.ipynb",
      "https://raw.githubusercontent.com/bentrevett/pytorch-rl/master/checkpoint_viz.ipynb",
      "https://raw.githubusercontent.com/bentrevett/pytorch-rl/master/0%20-%20Introduction%20to%20Gym.ipynb",
      "https://raw.githubusercontent.com/bentrevett/pytorch-rl/master/5%20-%20Proximal%20Policy%20Optimization%20%28PPO%29%20%5BCartPole%5D.ipynb",
      "https://raw.githubusercontent.com/bentrevett/pytorch-rl/master/1_policy_gradient.ipynb",
      "https://raw.githubusercontent.com/bentrevett/pytorch-rl/master/2_q_learning.ipynb"
    ],
    "technique": "File Exploration"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/bentrevett/pytorch-rl/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 Ben Trevett\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "PyTorch Reinforcement Learning",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "pytorch-rl",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "bentrevett",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/bentrevett/pytorch-rl/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 170,
      "date": "Sat, 25 Dec 2021 17:25:25 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "pytorch",
      "pytorch-tutorial",
      "pytorch-implmention",
      "pytorch-implementation",
      "reinforcement-learning",
      "reinforcement-learning-algorithms",
      "rl",
      "pytorch-tutorials",
      "pytorch-rl",
      "policy-gradient",
      "actor-critic",
      "a2c",
      "advantage-actor-critic",
      "generalized-advantage-estimation"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To install PyTorch, see installation instructions on the [PyTorch website](pytorch.org).\n\nTo install Gym, see installation instructions on the [Gym GitHub repo](https://github.com/openai/gym).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "All tutorials use Monte Carlo methods to train the CartPole-v1 environment with the goal of reaching a total episode reward of 475 averaged over the last 25 episodes. There are also alternate versions of some algorithms to show how to use those algorithms with other environments.\n\n* 0 - [Introduction to Gym](https://github.com/bentrevett/pytorch-rl/blob/master/0%20-%20Introduction%20to%20Gym.ipynb)\n\n* 1 - [Vanilla Policy Gradient (REINFORCE)](https://github.com/bentrevett/pytorch-rl/blob/master/1%20-%20Vanilla%20Policy%20Gradient%20(REINFORCE)%20[CartPole].ipynb)\n\n    This tutorial covers the workflow of a reinforcement learning project. We'll learn how to: create an environment, initialize a model to act as our policy, create a state/action/reward loop and update our policy. We update our policy with the [vanilla policy gradient algorithm](https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf), also known as [REINFORCE](http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf).\n    \n* 2 - [Actor Critic](https://github.com/bentrevett/pytorch-rl/blob/master/2%20-%20Actor%20Critic.ipynb)\n\n    This tutorial introduces the family of [actor-critic](https://papers.nips.cc/paper/1786-actor-critic-algorithms.pdf) algorithms, which we will use for the next few tutorials.\n\n* 3 - [Advantage Actor Critic (A2C)](https://github.com/bentrevett/pytorch-rl/blob/master/3%20-%20Advantage%20Actor%20Critic%20(A2C)%20[CartPole].ipynb)\n\n    We cover an improvement to the actor-critic framework, the [A2C](https://arxiv.org/abs/1602.01783) (advantage actor-critic) algorithm.\n    \n* 4 - [Generalized Advantage Estimation (GAE)](https://github.com/bentrevett/pytorch-rl/blob/master/4%20-%20Generalized%20Advantage%20Estimation%20(GAE)%20[CartPole].ipynb)\n\n    We improve on A2C by adding [GAE](https://arxiv.org/abs/1506.02438) (generalized advantage estimation). \n\n* 5 - [Proximal Policy Evaluation](https://github.com/bentrevett/pytorch-rl/blob/master/5%20-%20Proximal%20Policy%20Optimization%20(PPO)%20[CartPole].ipynb)\n\n    We cover another improvement on A2C, [PPO](https://arxiv.org/abs/1707.06347) (proximal policy optimization).\n\nPotential algorithms covered in future tutorials: DQN, ACER, ACKTR.\n\n",
      "technique": "Header extraction"
    }
  ]
}