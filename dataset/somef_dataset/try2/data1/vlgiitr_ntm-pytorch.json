{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- We have used the following codebase as a reference for our implementation : **[loudinthecloud/pytorch-ntm][2]**  \n\n[2]:https://github.com/loudinthecloud/pytorch-ntm\n\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1410.5401\nNeural Turing Machines (NTMs"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/vlgiitr/ntm-pytorch",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-09-25T22:17:12Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-20T15:58:19Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9065356207190821
      ],
      "excerpt": "Code for the paper \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9874204999563104,
        0.9727975225635306
      ],
      "excerpt": "Neural Turing Machines (NTMs) contain a recurrent network coupled with an external memory resource, which it can interact with by attentional processes. Therefore NTMs can be called Memory Augmented Neural Networks. They are end-to-end differentiable and thus are hypothesised at being able to learn simple algorithms. They outperform LSTMs in learning several algorithmic tasks due to the presence of external memory without an increase in parameters and computation. \nThis repository is a stable Pytorch implementation of a Neural Turing Machine and contains the code for training, evaluating and visualizing results for the Copy, Repeat Copy, Associative Recall and Priority Sort tasks. The code has been tested for all 4 tasks and the results obtained are in accordance with the results mentioned in the paper. The training and evaluation code for N-Gram task has been provided however the results would be uploaded after testing. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8398945686423919
      ],
      "excerpt": "The script runs with all arguments set to default value. If you wish to changes any of these, run the script with -h to see available arguments and change them as per need be. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9054681039890263,
        0.9648499489207446,
        0.8828510631828527,
        0.8083679322195574,
        0.9004136458540933,
        0.9236858552596241,
        0.8607772790354249
      ],
      "excerpt": "Both RMSprop and Adam optimizers have been provided. -momentum and -alpha are parameters for RMSprop and -beta1 and -beta2 are parameters for Adam. All these arguments are initialized to their default values. \nThe smoothing factor for all curves is 0.6 \n- Training for copy task is carried out with sequence length ranging from 1-20. The curve for bits per sequence error vs iterations for this task is shown below : \nTraining for repeat copy task is carried out with sequence length ranging from 1-10 and repeat number in the range 1-10. The curve for bits per sequence error vs iterations for this task is shown below : \nTraining for associative recall task is carried out the number of items ranging from 2-6.The curve for bits per sequence error vs iterations for this task is shown below : \nTraining for priority sort task is carried outwith an input sequence length of 20 and target sequence length of 16. The curve for bits per sequence error vs iterations for this task is shown below : \nThe model was trained and was evaluated as mentioned in the paper. The results were in accordance with the paper. Saved models for all the tasks are available in the saved_models folder. The model for copy task has been trained upto 500k iterations and those for repeat copy, associative recall and priority sort have been trained upto 100k iterations. The code for saving and loading the model has been incorporated in train.py and evaluate.py respectively. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9191880908202181,
        0.95038931602771,
        0.9563875997623138,
        0.9781955006739665
      ],
      "excerpt": "- Results for copy task shows that the NTM generalizes well for sequence length upto 120. The target and output for copy task is shown below : \nResults for the repeat copy task shows that the NTM generalizes well for maximum sequence length of 20 and repeat number     upto 20. The target and output for repeat copy task is shown below : \nResults for associative recall task shows that the NTM generalizes well for number of items upto 20. The target and output for associative recall task is shown below : \nResults for the priority sort task also show the better generalization capability of the NTM. The target and output for priority sort task is shown below : \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Neural Turing Machines in Pytorch.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/vlgiitr/ntm-pytorch/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3,
      "date": "Wed, 29 Dec 2021 10:40:35 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/vlgiitr/ntm-pytorch/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "vlgiitr/ntm-pytorch",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Our code is implemented in Pytorch 0.4.0 and Python >=3.5. To setup, proceed as follows :\n\nTo install Pytorch head over to ```https://pytorch.org/``` or install using miniconda or anaconda package by running \n```conda install -c soumith pytorch ```.\n\nClone this repository :\n\n```\ngit clone https://www.github.com/kdexd/ntm-pytorch\n```\n\nThe other python libraries that you'll need to run the code :\n```\npip install numpy \npip install tensorboard_logger\npip install matplotlib\npip install tqdm\npip install Pillow\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.999746712887969
      ],
      "excerpt": "pip install tensorboard_logger \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9503189345333785
      ],
      "excerpt": "python train.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9209441574835889
      ],
      "excerpt": "usage : train.py [-h] [-task_json TASK_JSON] [-batch_size BATCH_SIZE] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8708148446064828
      ],
      "excerpt": "python evaluate.py \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/vlgiitr/ntm-pytorch/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'The MIT License (MIT)\\nCopyright \\xc2\\xa9 2018 Karan Desai &#107;&#100;&#100;&#101;&#115;&#107;&#97;&#114;&#64;&#103;&#109;&#97;&#105;&#108;&#46;&#99;&#111;&#109;\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \\xe2\\x80\\x9cSoftware\\xe2\\x80\\x9d), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in\\nall copies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \\xe2\\x80\\x9cAS IS\\xe2\\x80\\x9d, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\\nTHE SOFTWARE.'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Neural Turing Machines (Pytorch)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "ntm-pytorch",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "vlgiitr",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/vlgiitr/ntm-pytorch/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 39,
      "date": "Wed, 29 Dec 2021 10:40:35 GMT"
    },
    "technique": "GitHub API"
  }
}