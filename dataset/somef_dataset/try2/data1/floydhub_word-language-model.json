{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1608.05859"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8654671031158477
      ],
      "excerpt": "Oxford Deep NLP 2017 course \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/floydhub/word-language-model",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-10-05T12:54:01Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-04T15:16:16Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9565632143306932
      ],
      "excerpt": "This is a porting of pytorch/examples/word_language_model making it usables on FloydHub. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8385084245304949
      ],
      "excerpt": ": Train a tied LSTM on PTB with CUDA for 40 epochs, reaching perplexity of 87.17 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8759304845304278
      ],
      "excerpt": "The model uses the nn.RNN module (and its sister modules nn.GRU and nn.LSTM) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8128937923427383
      ],
      "excerpt": "training is stopped and the current model is evaluated against the test dataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9847131274818297
      ],
      "excerpt": "It's time to evaluate our model generating some text: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.803181101509165
      ],
      "excerpt": "FloydHub supports seving mode for demo and testing purpose. Before serving your model through REST API, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8958078893934639
      ],
      "excerpt": "The service endpoint will take a couple minutes to become ready. Once it's up, you can interact with the model by sending a POST request wih the number of words and the temperature that the model will use to generate text: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8592638691416131,
        0.9158560630085512,
        0.9049527516097404,
        0.9476422750887281
      ],
      "excerpt": "Note that this feature is in preview mode and is not production ready yet \nSome useful resources on NLP for Deep Learning and language modeling task: \nThe Unreasonable Effectiveness of Recurrent Neural Networks \nNatural Language Processing with Deep Learning - Stanford \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Pytorch world language model (text generation) for PTB dataset example",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/floydhub/word-language-model/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 7,
      "date": "Sun, 26 Dec 2021 07:27:39 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/floydhub/word-language-model/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "floydhub/word-language-model",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Before you start, log in on FloydHub with the [floyd login](http://docs.floydhub.com/commands/login/) command, then fork and init the project:\n\n```bash\n$ git clone https://github.com/floydhub/word-language-model.git\n$ cd word-language-model\n$ floyd init word-language-model\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9086485614739517
      ],
      "excerpt": "Before you start, you need to upload the Penn Treebank-3 dataset as a FloydHub Dataset following this guide: create and upload a dataset. Then you will be ready to play with different language models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9199118571064199
      ],
      "excerpt": "floyd run --gpu --env pytorch-0.2 --data <USERNAME>/dataset/<PENN-TB3>/<VERSION>:input \"python main.py --cuda --epochs 7\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9199118571064199
      ],
      "excerpt": "floyd run --gpu --env pytorch-0.2 --data <USERNAME>/dataset/<PENN-TB3>/<VERSION>:input \"python main.py --cuda --epochs 7 --tied\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9199118571064199
      ],
      "excerpt": "floyd run --gpu --env pytorch-0.2 --data <USERNAME>/dataset/<PENN-TB3>/<VERSION>:input \"python main.py --cuda --tied\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9898776722834727,
        0.9864827865697483
      ],
      "excerpt": "--gpu run your job on a FloydHub GPU instance. \n--env pytorch-0.2 prepares a pytorch environment for python 3. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9151160589521125
      ],
      "excerpt": "which will automatically use the cuDNN backend if run on CUDA with cuDNN installed. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9083759626284401
      ],
      "excerpt": "You can follow along the progress by using the logs command. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8012828257497907,
        0.832114876170992
      ],
      "excerpt": "By default, the training script uses the PTB dataset, provided. \nThe trained model can then be used by the generate script to generate new text. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8825409279166986
      ],
      "excerpt": "floyd run --gpu --env pytorch-0.2 --data <USERNAME>/dataset/<PENN-TB3>/<VERSION>:input \"python main.py --cuda --epochs 7\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8825409279166986
      ],
      "excerpt": "floyd run --gpu --env pytorch-0.2 --data <USERNAME>/dataset/<PENN-TB3>/<VERSION>:input \"python main.py --cuda --epochs 7 --tied\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8861595228259087
      ],
      "excerpt": "floyd run --gpu --env pytorch-0.2 --data <USERNAME>/dataset/<PENN-TB3>/<VERSION>:input \"python main.py --cuda --tied\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8267090972822392,
        0.8962213553710014
      ],
      "excerpt": ": Generate samples from the trained LSTM model. \nfloyd run --gpu --env pytorch-0.2 --data <USERNAME>/dataset/<PENN-TB3>/<VERSION>:input --data <REPLACE_WITH_JOB_OUTPUT_NAME>:model \"python generate.py --cuda\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8267090972822392,
        0.8962213553710014
      ],
      "excerpt": ": Generate samples from the trained LSTM model. \nfloyd run --gpu --env pytorch-0.2 --data <USERNAME>/dataset/<PENN-TB3>/<VERSION>:input --data <REPLACE_WITH_JOB_OUTPUT_NAME>:model \"python generate.py --cuda\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8740487281916979
      ],
      "excerpt": "with --mode serve flag, FloydHub will run the app.py file in your project \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/floydhub/word-language-model/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "BSD 3-Clause \"New\" or \"Revised\" License",
      "url": "https://api.github.com/licenses/bsd-3-clause"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'BSD 3-Clause License\\n\\nCopyright (c) 2017,\\nAll rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met:\\n\\n Redistributions of source code must retain the above copyright notice, this\\n  list of conditions and the following disclaimer.\\n\\n Redistributions in binary form must reproduce the above copyright notice,\\n  this list of conditions and the following disclaimer in the documentation\\n  and/or other materials provided with the distribution.\\n\\n* Neither the name of the copyright holder nor the names of its\\n  contributors may be used to endorse or promote products derived from\\n  this software without specific prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Word-level language modeling RNN",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "word-language-model",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "floydhub",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/floydhub/word-language-model/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Here's the commands to training, evaluating and serving your language modeling task on FloydHub.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 30,
      "date": "Sun, 26 Dec 2021 07:27:39 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "language-model",
      "floydhub",
      "pytorch",
      "text-generation",
      "deep-learning",
      "rnn",
      "recurrent-neural-networks"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The `main.py` script accepts the following arguments:\n\n```bash\noptional arguments:\n  -h, --help         show this help message and exit\n  --data DATA        location of the data corpus\n  --model MODEL      type of recurrent net (RNN_TANH, RNN_RELU, LSTM, GRU)\n  --emsize EMSIZE    size of word embeddings\n  --nhid NHID        number of hidden units per layer\n  --nlayers NLAYERS  number of layers\n  --lr LR            initial learning rate\n  --clip CLIP        gradient clipping\n  --epochs EPOCHS    upper epoch limit\n  --batch-size N     batch size\n  --bptt BPTT        sequence length\n  --dropout DROPOUT  dropout applied to layers (0 = no dropout)\n  --decay DECAY      learning rate decay per epoch\n  --tied             tie the word embedding and softmax weights\n  --seed SEED        random seed\n  --cuda             use CUDA\n  --log-interval N   report interval\n  --save SAVE        path to save the final model\n```\n\nWith these arguments, a variety of models can be tested.\nAs an example, the following arguments produce slower but better models:\n\n```bash\npython main.py --cuda --emsize 650 --nhid 650 --dropout 0.5 --epochs 40           #: Test perplexity of 80.97\npython main.py --cuda --emsize 650 --nhid 650 --dropout 0.5 --epochs 40 --tied    #: Test perplexity of 75.96\npython main.py --cuda --emsize 1500 --nhid 1500 --dropout 0.65 --epochs 40        #: Test perplexity of 77.42\npython main.py --cuda --emsize 1500 --nhid 1500 --dropout 0.65 --epochs 40 --tied #: Test perplexity of 72.30\n```\n\nThese perplexities are equal or better than\n[Recurrent Neural Network Regularization (Zaremba et al. 2014)](https://arxiv.org/pdf/1409.2329.pdf)\nand are similar to [Using the Output Embedding to Improve Language Models (Press & Wolf 2016](https://arxiv.org/abs/1608.05859) and [Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling (Inan et al. 2016)](https://arxiv.org/pdf/1611.01462.pdf), though both of these papers have improved perplexities by using a form of recurrent dropout [(variational dropout)](http://papers.nips.cc/paper/6241-a-theoretically-grounded-application-of-dropout-in-recurrent-neural-networks).\n\n\n",
      "technique": "Header extraction"
    }
  ]
}