{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](docs/an_image_is_worth_16x16_words_transformers_for_image_recognition_at_scale.pdf) (Paper)\n- Eunkwang Jeon, ViT-pytorch (2020), [GitHub repository](https://github.com/jeonsworld/ViT-pytorch)\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9532964465413193
      ],
      "excerpt": "Explore Transformer-based architectures for Computer Vision Tasks. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "- CIFAR 10 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "| Vishal Mittal       | 2017A7PS0080P | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ra1ph2/Vision-Transformer",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-11-07T19:11:41Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-28T18:43:00Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9820190150445136
      ],
      "excerpt": "Implementation of the ViT model in Pytorch from the paper 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale' by Google Research. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9335525110657695,
        0.9236890989330343,
        0.9753943221319227
      ],
      "excerpt": "Transformers have been the de-facto for NLP tasks, and CNN/Resnet-like architectures have been the state of the art for Computer Vision. \nTill date, researchers have tried using attention for Vision, but used them in conjunction with CNN. \nThis paper mainly discusses the strength and versatility of vision transformers, as it kind of approves that they can be used in recognition and can even beat the state-of-the-art CNN. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9554838551469381,
        0.9142627655242724
      ],
      "excerpt": "The results of Vision Transformer have been compared with results of other architectures as well \u2013 BiT (Resnet 152x4), and EfficientNet, on same conditions. \nThe models have also been evaluated on VTAB classification suite consisting of 19 tasks divided into groups as Natural, Specialized and Structured Tasks. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9482927338964166
      ],
      "excerpt": "Due to non-availability of powerful compute on Google Colab, we chose to train and test on these 2 datasets \u2013  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.914826422671444
      ],
      "excerpt": "Hybrid Variant of Vision Transformer with Pretrained ResNet features as input to the Transformer (VisionTransformer.ipynb) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8130427604914866,
        0.9868346741227544,
        0.9969051880357543,
        0.8710386680996262,
        0.9525395377893997
      ],
      "excerpt": "Patch size in the Vision Transformer decides the length of the sequence. Lower patch size leads to higher information exchange during the self attention mechanism. This is verified by the better results using lower patch-size 4 over 8 on a 32x32 image \nIncreasing the number of layers of the Vision Transformer should ideally lead to better results but the results on the 8 Layer model are marginally better than the 12 Layer model which can be attributed to the small datasets used to train the models. Models with higher complexity require more data to capture the image features \nAs noted in the paper, Hybrid Vision Transformer performs better on small datasets compared to ViT as the initial ResNet features are able to capture the lower level features due to the locality property of Convolutions which normal ViT is not able to capture with the limited data available for training. \nResNets trained from scratch are able to outperform both ViT and Hybrid-ViT trained from scratch due to its inherent inductive bias of locality and translation invariance. These biases can not learned by the ViT on small datasets. \nPreTrained ViT performs much better than the other methods due to being trained on huge datasets and thus having learned the better representations than even ResNet since it can access much further information right from the very beginning unlike CNN. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9866061625358206,
        0.8884073929577349,
        0.8758210718402643
      ],
      "excerpt": "Due to non-availability of better computing resources, the model could not be trained on large datasets which is the first and the foremost requirement of this architecture to produce very high accuracies. Due to this limitation, we could not produce accuracies as mentioned in the paper in implementation from scratch.  \nEvaluating the model on VTAB classification suite. \nDifferent Attention mechanisms could be explored that take the 2D structure of images into account. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Implementation of Vision Transformer from scratch and performance compared to standard CNNs (ResNets) and pre-trained ViT on CIFAR10 and CIFAR100.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ra1ph2/Vision-Transformer/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 8,
      "date": "Wed, 29 Dec 2021 02:48:23 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ra1ph2/Vision-Transformer/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "ra1ph2/Vision-Transformer",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/ra1ph2/Vision-Transformer/tree/main/docs"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/ra1ph2/Vision-Transformer/main/VisionTransformer.ipynb",
      "https://raw.githubusercontent.com/ra1ph2/Vision-Transformer/main/Pretrained_ViT.ipynb"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.8910295653542505
      ],
      "excerpt": "<img src=\"docs/images/ViT_Model_Architecture.png\" alt=\"arch\" width=\"800\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8910295653542505,
        0.8910295653542505
      ],
      "excerpt": "<img src=\"docs/images/Methodology.png\" alt=\"Methodology\" width=\"800\"> \n<img src=\"docs/images/Transformer_Encoder.png\" alt=\"Transformer_Encoder\" width=\"250\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8910295653542505,
        0.8910295653542505,
        0.8910295653542505
      ],
      "excerpt": "<img src=\"docs/images/Attention.png\" alt=\"Attention\" width=\"500\"> \n<img src=\"docs/images/Attention_Mechanism.png\" alt=\"Attention_Mechanism\" width=\"800\"> \n<img src=\"docs/images/Multi_Head_Attention.png\" alt=\"Multi_Head_Attention\" width=\"800\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8910295653542505,
        0.8910295653542505,
        0.8910295653542505,
        0.8910295653542505
      ],
      "excerpt": "<img src=\"docs/images/Attention_Map_Visualization.png\" alt=\"Attention_Map_Visualization\" width=\"800\"> \n<img src=\"docs/images/Patch_Embedding.png\" alt=\"Patch_Embedding\" width=\"500\"> \n<img src=\"docs/images/Position_Embedding.png\" alt=\"Position_Embedding\" width=\"500\"> \n<img src=\"docs/images/Resut_Table.png\" alt=\"Resut_Table\" width=\"800\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8910295653542505,
        0.8910295653542505
      ],
      "excerpt": "<img src=\"docs/images/CIFAR10_Acc.png\" alt=\"CIFAR10_Acc\" width=\"800\"> \n<img src=\"docs/images/CIFAR100_Acc.png\" alt=\"CIFAR100_Acc\" width=\"800\"> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ra1ph2/Vision-Transformer/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Vision-Transformer",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Vision-Transformer",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "ra1ph2",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ra1ph2/Vision-Transformer/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 45,
      "date": "Wed, 29 Dec 2021 02:48:23 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "computer-vision",
      "transformer-architecture",
      "convolutional-neural-networks",
      "attention-mechanism",
      "pytorch",
      "jupyter-notebook"
    ],
    "technique": "GitHub API"
  }
}