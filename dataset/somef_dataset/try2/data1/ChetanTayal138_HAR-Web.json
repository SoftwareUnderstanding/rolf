{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Human Activity Recognition is a domain in Computer Vision that deals with identifying what action is being performed by a human entity in a video feed. \nDeep Learning approaches to carry out human activity recognition has been typically been tackled using 3D-CNNs, LRCNNs and also the widely adopted 2-Stream Architecture https://github.com/jeffreyyihuang/two-stream-action-recognition that uses both RGB-images and optical flow. \n\nHAR-Web is based on the project by https://github.com/felixchenfy/Realtime-Action-Recognition that utilizes Human Pose Estimation to generate 2D-Skeletons and use skeleton coordinates to classify actions. A big advantage of this approach is the reduced computation needed to carrying out action recognition making it a much viable approach when it comes to identifying human actions in a real time basis.\n\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1703.10667 | \n| Human activity recognition from skeleton poses  | https://arxiv.org/pdf/1908.08928v1.pdf |\n| tf-pose-estimation | https://github.com/ildoonet/tf-pose-estimation | \n| Realtime-Action-Recognition | https://github.com/felixchenfy/Realtime-Action-Recognition | "
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "| Title | Link | \n| ------ | ----- |\n| 2 Stream Convolution | http://papers.nips.cc/paper/5353-two-stream-convolutional | \n| Temporal Segment Networks | https://link.springer.com/chapter/10.1007/978-3-319-46484-8_2 |\n| TS-LSTM  | https://arxiv.org/abs/1703.10667 | \n| Human activity recognition from skeleton poses  | https://arxiv.org/pdf/1908.08928v1.pdf |\n| tf-pose-estimation | https://github.com/ildoonet/tf-pose-estimation | \n| Realtime-Action-Recognition | https://github.com/felixchenfy/Realtime-Action-Recognition | \n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ChetanTayal138/HAR-Web",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-05-10T16:42:09Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-07-29T05:43:04Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "HAR-Web is a web application that can be utilized to carry out the task of Human Activity Recognition in real time on the web using GPU-enabled devices. The web application is based on a micro service architecture where the project has been divided into 4 basic services, each running on a different port and deployed using Docker containers. I would like to add support for Kubernetes but as of now it doesn't support native hardware access like Docker-Swarm. One way of solving this would be to write a host device plugin like https://github.com/honkiko/k8s-hostdev-plugin but specifically for webcam access. If anyone has an idea on how to enable local webcam access on K8S, I would love to hear about it. \n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9121596983972644,
        0.8580424147903744,
        0.9079689173200417,
        0.9493005241258711
      ],
      "excerpt": "Initial page for the app that gives the user the option to train their own model or use existing model and routes to the appropriate service accordingly. (plan to add presets) \nVideo recording service written in NodeJS that allows user to record videos and then store it for training. By default the videos are recorded at 10 FPS and a total of 300 frames are recorded to generate a good enough amount of training data for each action. \nContains serialized model deployed using Flask and streams the predictions in real time after capturing video feed from a webcam in real time. \nModifies the config file with the appropriate labels and model of choice and then starts the training process in the background which consists of generating heatmaps. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9053268626482123
      ],
      "excerpt": "Ports for the different services in the docker-compose file are listed below:   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9796658623969368,
        0.9715521357568202,
        0.9795975881076768,
        0.8972587341014818,
        0.8541917334907291,
        0.886373534335615,
        0.9489152693731988,
        0.9671426943517845,
        0.9968029537584643,
        0.9592761833359557,
        0.9693321924471353,
        0.9977037853330379,
        0.9891424312565266,
        0.9857799987732996,
        0.9774565521282009,
        0.9726894945507046,
        0.9546213527101272
      ],
      "excerpt": "Human 2D pose estimation deals with localization of different key human parts and using these localized points to construct a pose for the human. HAR-Web uses https://github.com/ildoonet/tf-pose-estimation to generate the 2D-Poses which is  \nan implementation of OpenPose(written in Caffe) in TensorFlow.  \nThe model developed by CMU uses Part-Affinity Fields ( https://arxiv.org/pdf/1611.08050.pdf ) and has a t stage, two branch process of generating the predictions for poses. In the first stage, a neural network is used to carry out two simultaneous predictions : A set 'S' of 2D confidence maps for body part locations and a set 'L' of 2D vector fields of part affinities, which encode the degree of association between different body parts.  \nThe two branches use feature maps F generated using the first 10 layers of VGG-19 as their inputs, with the first branch predicting the set S and the second branch predicting the set L. Subsequent stages use these predictions and concatenate with the original feature map F and used iteratively to produce refined productions. \nThe skeleton generated by OpenPose has 18 joints and each joint has 2 coordinates(x,y) associated with it. Preprocessing is then done to :- \n  1. Scale the x and y coordinates as OpenPose has different scales for these. \n  2. Removal of joints on the head.  \n  3. Get rid of frames with no necks or thigh detected. \n  4. Filling of missing joints  \nFeatures are then extracted by concatenating the skeleton data from a window of 5 frames at a time. The exact feature extraction has been described in the original report that also talks about specific feature selection that were the most effective for training.  \nA total feature vector of dimension 314 is created and reduced to 50 dimensions using PCA. This 50 dimension network is finally used to classify different actions using a neural network with 3 hidden layers of 100 nodes each. \nBuilt using opencv4nodejs, which is an API for native OpenCV for NodeJS.   this service deals with recording the video of a person and stores it as frames for our training data. The base image that I used for creating the docker container that had a working and compatible version of opencv4nodejs is available here on DockerHub. \nA big advantage of using opencv4nodejs over using simply Flask for a task like this was because it provides an asynchronous API that allows built in multithreading and doesn't have to rely on something like Flask-Threads to avoid non-blocking calls. \nThis is important in our application because it allows us to save record and save the frames on two seperate threads and leads to performance gains. \nThe Trainer microservice uses a config file to carry out a 5-step training process which goes from generating heatmaps to extracting specific features mentioned in 1.3, carrying out PCA to reduce dimensionality of the feature vector and training a neural network on it. The config file is modified once the user selects to train their own model and generates a pickle file for the trained classifier that serves our predictions on the Flask server.  \nI have hardcoded HAR-Web to utilize 20-280 frames from the 300 frames that are recorded as training data but I plan on adding giving the user freedom to give exactly what number of frames to use and how many frames to record in the first place. \nThis is a very basic service that serves the front page for the web app and routes to the particular service requested by the user accordingly.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Human Activity Recognition on the web",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ChetanTayal138/HAR-Web/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3,
      "date": "Sun, 26 Dec 2021 13:43:00 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ChetanTayal138/HAR-Web/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "ChetanTayal138/HAR-Web",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/ChetanTayal138/HAR-Web/master/Frontend/Dockerfile",
      "https://raw.githubusercontent.com/ChetanTayal138/HAR-Web/master/Recorder/Dockerfile",
      "https://raw.githubusercontent.com/ChetanTayal138/HAR-Web/master/Recognizer/Dockerfile",
      "https://raw.githubusercontent.com/ChetanTayal138/HAR-Web/master/Trainer/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/ChetanTayal138/HAR-Web/master/Frontend/run_docker.sh",
      "https://raw.githubusercontent.com/ChetanTayal138/HAR-Web/master/Recorder/run_docker.sh",
      "https://raw.githubusercontent.com/ChetanTayal138/HAR-Web/master/Recognizer/run_files.sh",
      "https://raw.githubusercontent.com/ChetanTayal138/HAR-Web/master/Recognizer/run_docker.sh",
      "https://raw.githubusercontent.com/ChetanTayal138/HAR-Web/master/Trainer/run_files.sh",
      "https://raw.githubusercontent.com/ChetanTayal138/HAR-Web/master/Trainer/run_docker.sh"
    ],
    "technique": "File Exploration"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ChetanTayal138/HAR-Web/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "HTML",
      "JavaScript",
      "Dockerfile",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'\\nThe MIT License (MIT)\\n\\nCopyright (c) 2014 Jonathan Ong me@jongleberry.com\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in\\nall copies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\\nTHE SOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "1. Introduction",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "HAR-Web",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "ChetanTayal138",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ChetanTayal138/HAR-Web/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Sun, 26 Dec 2021 13:43:00 GMT"
    },
    "technique": "GitHub API"
  }
}