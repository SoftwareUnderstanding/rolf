{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2105.02358",
      "https://arxiv.org/abs/1709.01507",
      "https://arxiv.org/abs/2105.13677",
      "https://arxiv.org/abs/1911.09483",
      "https://arxiv.org/abs/2106.13112",
      "https://arxiv.org/abs/2106.12368",
      "https://arxiv.org/abs/2106.04803",
      "https://arxiv.org/abs/2107.00782",
      "https://arxiv.org/abs/2107.12292",
      "https://arxiv.org/abs/2108.02456",
      "https://arxiv.org/abs/2108.01072",
      "https://arxiv.org/abs/2107.00645",
      "https://arxiv.org/abs/2010.03045",
      "https://arxiv.org/abs/2103.02907",
      "https://arxiv.org/abs/2103.02907",
      "https://arxiv.org/abs/2110.07641",
      "https://arxiv.org/abs/2109.14382",
      "https://arxiv.org/abs/2105.02358",
      "https://arxiv.org/abs/1709.01507",
      "https://arxiv.org/abs/2105.13677",
      "https://arxiv.org/abs/1911.09483",
      "https://arxiv.org/abs/2106.13112",
      "https://arxiv.org/abs/2106.12368",
      "https://arxiv.org/abs/2106.04803",
      "https://arxiv.org/abs/2107.00782",
      "https://arxiv.org/abs/2107.12292",
      "https://arxiv.org/abs/2108.02456",
      "https://arxiv.org/abs/2108.01072",
      "https://arxiv.org/abs/2107.00645",
      "https://arxiv.org/abs/2010.03045",
      "https://arxiv.org/abs/2103.02907",
      "https://arxiv.org/abs/2103.02907",
      "https://arxiv.org/abs/2110.07641",
      "https://arxiv.org/abs/2110.07641",
      "https://arxiv.org/abs/1611.05431v2",
      "https://arxiv.org/abs/2103.02907",
      "https://arxiv.org/abs/1611.05431v2",
      "https://arxiv.org/abs/2103.02907",
      "https://arxiv.org/abs/2105.08050",
      "https://arxiv.org/abs/2109.05422",
      "https://arxiv.org/abs/2105.08050",
      "https://arxiv.org/abs/2109.05422",
      "https://arxiv.org/abs/2101.03697",
      "https://arxiv.org/abs/1908.03930",
      "https://arxiv.org/abs/2103.13425",
      "https://arxiv.org/abs/2101.03697",
      "https://arxiv.org/abs/1908.03930",
      "https://arxiv.org/abs/2103.13425",
      "https://arxiv.org/abs/1704.04861",
      "https://arxiv.org/abs/2103.06255",
      "https://arxiv.org/abs/1912.03458",
      "https://arxiv.org/abs/1904.04971",
      "https://arxiv.org/abs/1704.04861",
      "https://arxiv.org/abs/2103.06255",
      "https://arxiv.org/abs/1912.03458",
      "https://arxiv.org/abs/1904.04971"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.94562550681252
      ],
      "excerpt": "11. Efficient Multi-Head Self-Attention(EMSA) Usage \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9852580844366388
      ],
      "excerpt": "Pytorch implementation of \"ResT: An Efficient Transformer for Visual Recognition---arXiv 2021.05.28\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9740747029102247,
        0.8965379412703154
      ],
      "excerpt": "Pytorch implementation of \"MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning---arXiv 2019.11.17\" \nPytorch implementation of \"Spatial Group-wise Enhance: Improving Semantic Feature Learning in Convolutional Networks---arXiv 2019.05.23\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9992969305310266
      ],
      "excerpt": "Pytorch implementation of VOLO: Vision Outlooker for Visual Recognition---arXiv 2021.06.24\"  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9984459491448996
      ],
      "excerpt": "Pytorch implementation of Vision Permutator: A Permutable MLP-Like Architecture for Visual Recognition---arXiv 2021.06.23  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8952832796748892,
        0.996820649814941
      ],
      "excerpt": "Pytorch implementation of Polarized Self-Attention: Towards High-quality Pixel-wise Regression---arXiv 2021.07.02  \u3010\u8bba\u6587\u89e3\u6790\u3011  \nPytorch implementation of Contextual Transformer Networks for Visual Recognition---arXiv 2021.07.26  \u3010\u8bba\u6587\u89e3\u6790\u3011  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9312077423103171
      ],
      "excerpt": "Pytorch implementation of S\u00b2-MLPv2: Improved Spatial-Shift MLP Architecture for Vision---arXiv 2021.08.02 \u3010\u8bba\u6587\u89e3\u6790\u3011  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9655546226044326,
        0.9228167941548804,
        0.9703639279865677
      ],
      "excerpt": "Pytorch implementation of MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer---ArXiv 2021.10.05 \nPytorch implementation of Non-deep Networks---ArXiv 2021.10.20 \nPytorch implementation of UFO-ViT: High Performance Linear Vision Transformer without Softmax---ArXiv 2021.09.29 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9658570831144664
      ],
      "excerpt": "\"ResT: An Efficient Transformer for Visual Recognition\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8893977543919693
      ],
      "excerpt": "A2-Nets: Double Attention Networks \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.998949195363509,
        0.9963362112764189
      ],
      "excerpt": "VOLO: Vision Outlooker for Visual Recognition\" \nVision Permutator: A Permutable MLP-Like Architecture for Visual Recognition\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.999459050841918,
        0.9425293339222982,
        0.9876190900346457,
        0.8736224643973708
      ],
      "excerpt": "Contextual Transformer Networks for Visual Recognition---arXiv 2021.07.26 \nResidual Attention: A Simple but Effective Method for Multi-Label Recognition---ICCV2021 \nS\u00b2-MLPv2: Improved Spatial-Shift MLP Architecture for Vision---arXiv 2021.08.02 \nGlobal Filter Networks for Image Classification---arXiv 2021.07.01 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9939826811848347,
        0.9860055197142586,
        0.994844021510752,
        0.9459265559142548
      ],
      "excerpt": "MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer---ArXiv 2021.10.05 \nNon-deep Networks---ArXiv 2021.10.20 \nUFO-ViT: High Performance Linear Vision Transformer without Softmax---ArXiv 2021.09.29 \nPytorch implementation of \"Deep Residual Learning for Image Recognition---CVPR2016 Best Paper\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9655546226044326
      ],
      "excerpt": "Pytorch implementation of MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer---ArXiv 2020.10.05 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9903926387212362
      ],
      "excerpt": "\"Deep Residual Learning for Image Recognition---CVPR2016 Best Paper\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9939826811848347
      ],
      "excerpt": "MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer---ArXiv 2020.10.05 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9383547099930265
      ],
      "excerpt": "Pytorch implementation of \"RepMLP: Re-parameterizing Convolutions into Fully-connected Layers for Image Recognition---arXiv 2021.05.05\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8669289381661682,
        0.8656442788361102
      ],
      "excerpt": "Pytorch implementation of \"Sparse MLP for Image Recognition: Is Self-Attention Really Necessary?---arXiv 2021.09.12\" \n\"RepMLP: Re-parameterizing Convolutions into Fully-connected Layers for Image Recognition\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.834896575052709
      ],
      "excerpt": "Pytorch implementation of \"Involution: Inverting the Inherence of Convolution for Visual Recognition---CVPR2021\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9675317607050109
      ],
      "excerpt": "\"Involution: Inverting the Inherence of Convolution for Visual Recognition\" \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/xmu-xiaoma666/External-Attention-pytorch",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-05-08T13:11:46Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-26T23:06:04Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.911340466869147,
        0.9679002978662066
      ],
      "excerpt": "If this project is helpful to you, welcome to give a star.  \nDon't forget to follow me to learn about project updates. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9391767134256334
      ],
      "excerpt": "For All\uff1a \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8309307333381428,
        0.9287024266616845,
        0.9533070089446747,
        0.9565765838529157,
        0.8217932129852996,
        0.9225052186708819,
        0.9310311670563326,
        0.8810218734588655,
        0.8761232229608736
      ],
      "excerpt": "Pytorch implementation of \"Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks---arXiv 2021.05.05\" \nPytorch implementation of \"Attention Is All You Need---NIPS2017\" \nPytorch implementation of \"Squeeze-and-Excitation Networks---CVPR2018\" \nPytorch implementation of \"Selective Kernel Networks---CVPR2019\" \nPytorch implementation of \"CBAM: Convolutional Block Attention Module---ECCV2018\" \nPytorch implementation of \"BAM: Bottleneck Attention Module---BMCV2018\" \nPytorch implementation of \"ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks---CVPR2020\" \nPytorch implementation of \"Dual Attention Network for Scene Segmentation---CVPR2019\" \nPytorch implementation of \"EPSANet: An Efficient Pyramid Split Attention Block on Convolutional Neural Network---arXiv 2021.05.30\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9668697666785104,
        0.8157908871341325,
        0.9234703509436222,
        0.9398743446028988,
        0.9847425802705592
      ],
      "excerpt": "Pytorch implementation of \"SA-NET: SHUFFLE ATTENTION FOR DEEP CONVOLUTIONAL NEURAL NETWORKS---ICASSP 2021\" \nPytorch implementation of \"MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning---arXiv 2019.11.17\" \nPytorch implementation of \"Spatial Group-wise Enhance: Improving Semantic Feature Learning in Convolutional Networks---arXiv 2019.05.23\" \nPytorch implementation of \"A2-Nets: Double Attention Networks---NIPS2018\" \nPytorch implementation of \"An Attention Free Transformer---ICLR2021 (Apple New Work)\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9583120363995844
      ],
      "excerpt": "Pytorch implementation of CoAtNet: Marrying Convolution and Attention for All Data Sizes---arXiv 2021.06.09  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9489868599178242
      ],
      "excerpt": "Pytorch implementation of Scaling Local Self-Attention for Parameter Efficient Visual Backbones---CVPR2021 Oral  \u3010\u8bba\u6587\u89e3\u6790\u3011 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.888498996636368
      ],
      "excerpt": "Pytorch implementation of Residual Attention: A Simple but Effective Method for Multi-Label Recognition---ICCV2021  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8556102613492028,
        0.935531158106533,
        0.8976617636440929
      ],
      "excerpt": "Pytorch implementation of Global Filter Networks for Image Classification---arXiv 2021.07.01  \nPytorch implementation of Rotate to Attend: Convolutional Triplet Attention Module---WACV 2021  \nPytorch implementation of Coordinate Attention for Efficient Mobile Network Design ---CVPR 2021 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8158728613685822
      ],
      "excerpt": "\"Squeeze-and-Excitation Networks\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8066618902493231
      ],
      "excerpt": "\"EPSANet: An Efficient Pyramid Split Attention Block on Convolutional Neural Network\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8600468505041553
      ],
      "excerpt": "\"SA-NET: SHUFFLE ATTENTION FOR DEEP CONVOLUTIONAL NEURAL NETWORKS\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9153001108538238
      ],
      "excerpt": "Spatial Group-wise Enhance: Improving Semantic Feature Learning in Convolutional Networks \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9479445827412236
      ],
      "excerpt": "CoAtNet: Marrying Convolution and Attention for All Data Sizes\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9402989271826349,
        0.9693104688217713
      ],
      "excerpt": "Pytorch implementation of \"Deep Residual Learning for Image Recognition---CVPR2016 Best Paper\" \nPytorch implementation of \"Aggregated Residual Transformations for Deep Neural Networks---CVPR2017\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8765713233958897
      ],
      "excerpt": "Pytorch implementation of Patches Are All You Need?---ICLR2022 (Under Review) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.883627913220476
      ],
      "excerpt": "\"Aggregated Residual Transformations for Deep Neural Networks---CVPR2017\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9330904127842033,
        0.9107304115427343
      ],
      "excerpt": "Pytorch implementation of \"ResMLP: Feedforward networks for image classification with data-efficient training---arXiv 2021.05.07\" \nPytorch implementation of \"Pay Attention to MLPs---arXiv 2021.05.17\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8896565628387942,
        0.9693233393948762
      ],
      "excerpt": "\"ResMLP: Feedforward networks for image classification with data-efficient training\" \n\"Pay Attention to MLPs\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9133672273337422,
        0.9690654011254327,
        0.8588141929799006
      ],
      "excerpt": "Pytorch implementation of \"RepVGG: Making VGG-style ConvNets Great Again---CVPR2021\" \nPytorch implementation of \"ACNet: Strengthening the Kernel Skeletons for Powerful CNN via Asymmetric Convolution Blocks---ICCV2019\" \nPytorch implementation of \"Diverse Branch Block: Building a Convolution as an Inception-like Unit---CVPR2021\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8803719052026773
      ],
      "excerpt": "\"ACNet: Strengthening the Kernel Skeletons for Powerful CNN via Asymmetric Convolution Blocks\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8549529507944027,
        0.9717875137101485,
        0.9437212646778202
      ],
      "excerpt": "Pytorch implementation of \"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications---CVPR2017\" \nPytorch implementation of \"Efficientnet: Rethinking model scaling for convolutional neural networks---PMLR2019\" \nPytorch implementation of \"Involution: Inverting the Inherence of Convolution for Visual Recognition---CVPR2021\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9690160168283941
      ],
      "excerpt": "Pytorch implementation of \"CondConv: Conditionally Parameterized Convolutions for Efficient Inference---NeurIPS2019\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8618237785088774
      ],
      "excerpt": "\"Efficientnet: Rethinking model scaling for convolutional neural networks\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "\ud83c\udf40 Pytorch implementation of various Attention Mechanisms, MLP, Re-parameter, Convolution, which is helpful to further understand papers.\u2b50\u2b50\u2b50",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/xmu-xiaoma666/External-Attention-pytorch/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 574,
      "date": "Mon, 27 Dec 2021 03:36:26 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/xmu-xiaoma666/External-Attention-pytorch/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "xmu-xiaoma666/External-Attention-pytorch",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8991199588515429
      ],
      "excerpt": "For \u8fdb\u9636\u8005\uff08Like You\uff09\uff1a \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8825788449249663
      ],
      "excerpt": "For \u5927\u795e\uff08May Be Like You\uff09\uff1a \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8143791736721294
      ],
      "excerpt": "Patches Are All You Need?---ICLR2022 (Under Review) \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8587476985249702
      ],
      "excerpt": "1. External Attention Usage \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8587476985249702,
        0.8587476985249702,
        0.8587476985249702,
        0.8587476985249702,
        0.8587476985249702,
        0.8068881867181333
      ],
      "excerpt": "5. SK Attention Usage \n6. CBAM Attention Usage \n7. BAM Attention Usage \n8. ECA Attention Usage \n9. DANet Attention Usage \n10. Pyramid Split Attention (PSA) Usage \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8984501292763915,
        0.8469142607675822,
        0.8587476985249702
      ],
      "excerpt": "15. A2 Attention Usage \n16. AFT Attention Usage \n17. Outlook Attention Usage \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8461128781032289
      ],
      "excerpt": "20. HaloNet Attention Usage \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8587476985249702
      ],
      "excerpt": "22. CoTAttention Usage \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8368289858699726,
        0.8615266796920952,
        0.8587476985249702
      ],
      "excerpt": "24. S2 Attention Usage \n25. GFNet Attention Usage \n26. Triplet Attention Usage \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8379781605554422,
        0.8437603349255703
      ],
      "excerpt": "29. ParNet Attention Usage \n30. UFO Attention Usage \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8442887485716414
      ],
      "excerpt": "1. ResNet Usage \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8587476985249702,
        0.8587476985249702
      ],
      "excerpt": "3. MobileViT Usage \n4. ConvMixer Usage \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8587476985249702,
        0.8587476985249702,
        0.8587476985249702,
        0.8587476985249702,
        0.8587476985249702
      ],
      "excerpt": "1. RepMLP Usage \n2. MLP-Mixer Usage \n3. ResMLP Usage \n4. gMLP Usage \n5. sMLP Usage \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8587476985249702,
        0.8587476985249702
      ],
      "excerpt": "1. RepVGG Usage \n2. ACNet Usage \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8587476985249702,
        0.8587476985249702,
        0.8587476985249702,
        0.8587476985249702
      ],
      "excerpt": "2. MBConv Usage \n3. Involution Usage \n4. DynamicConv Usage \n5. CondConv Usage \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8934477375241742
      ],
      "excerpt": "from model.rep.ddb import transI_conv_bn \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8934477375241742
      ],
      "excerpt": "from model.rep.ddb import transII_conv_branch \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8934477375241742
      ],
      "excerpt": "from model.rep.ddb import transIII_conv_sequential \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8934477375241742
      ],
      "excerpt": "from model.rep.ddb import transIV_conv_concat \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8934477375241742
      ],
      "excerpt": "from model.rep.ddb import transV_avg \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8934477375241742
      ],
      "excerpt": "from model.rep.ddb import transVI_conv_scale \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/xmu-xiaoma666/External-Attention-pytorch/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2021 xmu-xiaoma666\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "FightingCV Codebase For [***Attention***](#attention-series),[***Backbone***](#backbone-series), [***MLP***](#mlp-series), [***Re-parameter***](#re-parameter-series), [**Convolution**](#convolution-series)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "External-Attention-pytorch",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "xmu-xiaoma666",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/xmu-xiaoma666/External-Attention-pytorch/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2943,
      "date": "Mon, 27 Dec 2021 03:36:26 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "attention",
      "pytorch",
      "paper",
      "cbam",
      "squeeze",
      "excitation-networks",
      "linear-layers",
      "visual-tasks"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\nfrom model.attention.ExternalAttention import ExternalAttention\nimport torch\n\ninput=torch.randn(50,49,512)\nea = ExternalAttention(d_model=512,S=8)\noutput=ea(input)\nprint(output.shape)\n```\n\n***\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\nfrom model.attention.SelfAttention import ScaledDotProductAttention\nimport torch\n\ninput=torch.randn(50,49,512)\nsa = ScaledDotProductAttention(d_model=512, d_k=512, d_v=512, h=8)\noutput=sa(input,input,input)\nprint(output.shape)\n```\n\n***\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\nfrom model.attention.SimplifiedSelfAttention import SimplifiedScaledDotProductAttention\nimport torch\n\ninput=torch.randn(50,49,512)\nssa = SimplifiedScaledDotProductAttention(d_model=512, h=8)\noutput=ssa(input,input,input)\nprint(output.shape)\n\n```\n\n***\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\nfrom model.attention.SEAttention import SEAttention\nimport torch\n\ninput=torch.randn(50,512,7,7)\nse = SEAttention(channel=512,reduction=8)\noutput=se(input)\nprint(output.shape)\n\n```\n\n***\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\nfrom model.attention.SKAttention import SKAttention\nimport torch\n\ninput=torch.randn(50,512,7,7)\nse = SKAttention(channel=512,reduction=8)\noutput=se(input)\nprint(output.shape)\n\n```\n***\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\nfrom model.attention.CBAM import CBAMBlock\nimport torch\n\ninput=torch.randn(50,512,7,7)\nkernel_size=input.shape[2]\ncbam = CBAMBlock(channel=512,reduction=16,kernel_size=kernel_size)\noutput=cbam(input)\nprint(output.shape)\n\n```\n\n***\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\nfrom model.attention.BAM import BAMBlock\nimport torch\n\ninput=torch.randn(50,512,7,7)\nbam = BAMBlock(channel=512,reduction=16,dia_val=2)\noutput=bam(input)\nprint(output.shape)\n\n```\n\n***\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\nfrom model.attention.ECAAttention import ECAAttention\nimport torch\n\ninput=torch.randn(50,512,7,7)\neca = ECAAttention(kernel_size=3)\noutput=eca(input)\nprint(output.shape)\n\n```\n\n***\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\nfrom model.attention.DANet import DAModule\nimport torch\n\ninput=torch.randn(50,512,7,7)\ndanet=DAModule(d_model=512,kernel_size=3,H=7,W=7)\nprint(danet(input).shape)\n\n```\n\n***\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\nfrom model.attention.PSA import PSA\nimport torch\n\ninput=torch.randn(50,512,7,7)\npsa = PSA(channel=512,reduction=8)\noutput=psa(input)\nprint(output.shape)\n\n```\n\n***\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\n\nfrom model.attention.EMSA import EMSA\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(50,64,512)\nemsa = EMSA(d_model=512, d_k=512, d_v=512, h=8,H=8,W=8,ratio=2,apply_transform=True)\noutput=emsa(input,input,input)\nprint(output.shape)\n    \n```\n\n***\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\n\nfrom model.attention.ShuffleAttention import ShuffleAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\ninput=torch.randn(50,512,7,7)\nse = ShuffleAttention(channel=512,G=8)\noutput=se(input)\nprint(output.shape)\n\n    \n```\n\n\n***\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\nfrom model.attention.MUSEAttention import MUSEAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\ninput=torch.randn(50,49,512)\nsa = MUSEAttention(d_model=512, d_k=512, d_v=512, h=8)\noutput=sa(input,input,input)\nprint(output.shape)\n\n```\n\n***\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\nfrom model.attention.SGE import SpatialGroupEnhance\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(50,512,7,7)\nsge = SpatialGroupEnhance(groups=8)\noutput=sge(input)\nprint(output.shape)\n\n```\n\n***\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\nfrom model.attention.A2Atttention import DoubleAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(50,512,7,7)\na2 = DoubleAttention(512,128,128,True)\noutput=a2(input)\nprint(output.shape)\n\n```\n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\nfrom model.attention.AFT import AFT_FULL\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(50,49,512)\naft_full = AFT_FULL(d_model=512, n=49)\noutput=aft_full(input)\nprint(output.shape)\n\n```\n\n\n\n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\nfrom model.attention.OutlookAttention import OutlookAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(50,28,28,512)\noutlook = OutlookAttention(dim=512)\noutput=outlook(input)\nprint(output.shape)\n\n```\n\n\n***\n\n\n\n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\n\nfrom model.attention.ViP import WeightedPermuteMLP\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(64,8,8,512)\nseg_dim=8\nvip=WeightedPermuteMLP(512,seg_dim)\nout=vip(input)\nprint(out.shape)\n\n```\n\n\n***\n\n\n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\n\nfrom model.attention.CoAtNet import CoAtNet\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,3,224,224)\nmbconv=CoAtNet(in_ch=3,image_size=224)\nout=mbconv(input)\nprint(out.shape)\n\n```\n\n\n***\n\n\n\n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\n\nfrom model.attention.HaloAttention import HaloAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,512,8,8)\nhalo = HaloAttention(dim=512,\n    block_size=2,\n    halo_size=1,)\noutput=halo(input)\nprint(output.shape)\n\n```\n\n\n***\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\n\nfrom model.attention.PolarizedSelfAttention import ParallelPolarizedSelfAttention,SequentialPolarizedSelfAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,512,7,7)\npsa = SequentialPolarizedSelfAttention(channel=512)\noutput=psa(input)\nprint(output.shape)\n\n\n```\n\n\n***\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\n\nfrom model.attention.CoTAttention import CoTAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(50,512,7,7)\ncot = CoTAttention(dim=512,kernel_size=3)\noutput=cot(input)\nprint(output.shape)\n\n\n\n```\n\n***\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\n\nfrom model.attention.ResidualAttention import ResidualAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(50,512,7,7)\nresatt = ResidualAttention(channel=512,num_class=1000,la=0.2)\noutput=resatt(input)\nprint(output.shape)\n\n\n\n```\n\n***\n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\nfrom model.attention.S2Attention import S2Attention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(50,512,7,7)\ns2att = S2Attention(channels=512)\noutput=s2att(input)\nprint(output.shape)\n\n```\n\n***\n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\nfrom model.attention.gfnet import GFNet\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nx = torch.randn(1, 3, 224, 224)\ngfnet = GFNet(embed_dim=384, img_size=224, patch_size=16, num_classes=1000)\nout = gfnet(x)\nprint(out.shape)\n\n```\n\n***\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\nfrom model.attention.TripletAttention import TripletAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\ninput=torch.randn(50,512,7,7)\ntriplet = TripletAttention()\noutput=triplet(input)\nprint(output.shape)\n```\n\n\n***\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\nfrom model.attention.CoordAttention import CoordAtt\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninp=torch.rand([2, 96, 56, 56])\ninp_dim, oup_dim = 96, 96\nreduction=32\n\ncoord_attention = CoordAtt(inp_dim, oup_dim, reduction=reduction)\noutput=coord_attention(inp)\nprint(output.shape)\n```\n\n***\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\nfrom model.attention.MobileViTAttention import MobileViTAttention\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    m=MobileViTAttention()\n    input=torch.randn(1,3,49,49)\n    output=m(input)\n    print(output.shape)  #:output:(1,3,49,49)\n    \n```\n\n***\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\nfrom model.attention.ParNetAttention import *\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    input=torch.randn(50,512,7,7)\n    pna = ParNetAttention(channel=512)\n    output=pna(input)\n    print(output.shape) #:50,512,7,7\n    \n```\n\n***\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\nfrom model.attention.UFOAttention import *\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    input=torch.randn(50,49,512)\n    ufo = UFOAttention(d_model=512, d_k=512, d_v=512, h=8)\n    output=ufo(input,input,input)\n    print(output.shape) #:[50, 49, 512]\n    \n```\n\n***\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\n\nfrom model.backbone.resnet import ResNet50,ResNet101,ResNet152\nimport torch\nif __name__ == '__main__':\n    input=torch.randn(50,3,224,224)\n    resnet50=ResNet50(1000)\n    #: resnet101=ResNet101(1000)\n    #: resnet152=ResNet152(1000)\n    out=resnet50(input)\n    print(out.shape)\n\n```\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\n\nfrom model.backbone.resnext import ResNeXt50,ResNeXt101,ResNeXt152\nimport torch\n\nif __name__ == '__main__':\n    input=torch.randn(50,3,224,224)\n    resnext50=ResNeXt50(1000)\n    #: resnext101=ResNeXt101(1000)\n    #: resnext152=ResNeXt152(1000)\n    out=resnext50(input)\n    print(out.shape)\n\n\n```\n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\n\nfrom model.backbone.MobileViT import *\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    input=torch.randn(1,3,224,224)\n\n    #:#:#: mobilevit_xxs\n    mvit_xxs=mobilevit_xxs()\n    out=mvit_xxs(input)\n    print(out.shape)\n\n    #:#:#: mobilevit_xs\n    mvit_xs=mobilevit_xs()\n    out=mvit_xs(input)\n    print(out.shape)\n\n\n    #:#:#: mobilevit_s\n    mvit_s=mobilevit_s()\n    out=mvit_s(input)\n    print(out.shape)\n\n```\n\n\n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\n\nfrom model.backbone.ConvMixer import *\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    x=torch.randn(1,3,224,224)\n    convmixer=ConvMixer(dim=512,depth=12)\n    out=convmixer(x)\n    print(out.shape)  #:[1, 1000]\n\n\n```\n\n\n\n\n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\nfrom model.mlp.repmlp import RepMLP\nimport torch\nfrom torch import nn\n\nN=4 #:batch size\nC=512 #:input dim\nO=1024 #:output dim\nH=14 #:image height\nW=14 #:image width\nh=7 #:patch height\nw=7 #:patch width\nfc1_fc2_reduction=1 #:reduction ratio\nfc3_groups=8 #: groups\nrepconv_kernels=[1,3,5,7] #:kernel list\nrepmlp=RepMLP(C,O,H,W,h,w,fc1_fc2_reduction,fc3_groups,repconv_kernels=repconv_kernels)\nx=torch.randn(N,C,H,W)\nrepmlp.eval()\nfor module in repmlp.modules():\n    if isinstance(module, nn.BatchNorm2d) or isinstance(module, nn.BatchNorm1d):\n        nn.init.uniform_(module.running_mean, 0, 0.1)\n        nn.init.uniform_(module.running_var, 0, 0.1)\n        nn.init.uniform_(module.weight, 0, 0.1)\n        nn.init.uniform_(module.bias, 0, 0.1)\n\n#:training result\nout=repmlp(x)\n#:inference result\nrepmlp.switch_to_deploy()\ndeployout = repmlp(x)\n\nprint(((deployout-out)**2).sum())\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\nfrom model.mlp.mlp_mixer import MlpMixer\nimport torch\nmlp_mixer=MlpMixer(num_classes=1000,num_blocks=10,patch_size=10,tokens_hidden_dim=32,channels_hidden_dim=1024,tokens_mlp_dim=16,channels_mlp_dim=1024)\ninput=torch.randn(50,3,40,40)\noutput=mlp_mixer(input)\nprint(output.shape)\n```\n\n***\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\nfrom model.mlp.resmlp import ResMLP\nimport torch\n\ninput=torch.randn(50,3,14,14)\nresmlp=ResMLP(dim=128,image_size=14,patch_size=7,class_num=1000)\nout=resmlp(input)\nprint(out.shape) #:the last dimention is class_num\n```\n\n***\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\nfrom model.mlp.g_mlp import gMLP\nimport torch\n\nnum_tokens=10000\nbs=50\nlen_sen=49\nnum_layers=6\ninput=torch.randint(num_tokens,(bs,len_sen)) #:bs,len_sen\ngmlp = gMLP(num_tokens=num_tokens,len_sen=len_sen,dim=512,d_ff=1024)\noutput=gmlp(input)\nprint(output.shape)\n```\n\n***\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\nfrom model.mlp.sMLP_block import sMLPBlock\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    input=torch.randn(50,3,224,224)\n    smlp=sMLPBlock(h=224,w=224)\n    out=smlp(input)\n    print(out.shape)\n```\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\n\nfrom model.rep.repvgg import RepBlock\nimport torch\n\n\ninput=torch.randn(50,512,49,49)\nrepblock=RepBlock(512,512)\nrepblock.eval()\nout=repblock(input)\nrepblock._switch_to_deploy()\nout2=repblock(input)\nprint('difference between vgg and repvgg')\nprint(((out2-out)**2).sum())\n```\n\n\n\n***\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\nfrom model.rep.acnet import ACNet\nimport torch\nfrom torch import nn\n\ninput=torch.randn(50,512,49,49)\nacnet=ACNet(512,512)\nacnet.eval()\nout=acnet(input)\nacnet._switch_to_deploy()\nout2=acnet(input)\nprint('difference:')\nprint(((out2-out)**2).sum())\n\n```\n\n\n\n***\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\nfrom model.conv.DepthwiseSeparableConvolution import DepthwiseSeparableConvolution\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,3,224,224)\ndsconv=DepthwiseSeparableConvolution(3,64)\nout=dsconv(input)\nprint(out.shape)\n```\n\n***\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\nfrom model.conv.MBConv import MBConvBlock\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,3,224,224)\nmbconv=MBConvBlock(ksize=3,input_filters=3,output_filters=512,image_size=224)\nout=mbconv(input)\nprint(out.shape)\n\n\n```\n\n***\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\nfrom model.conv.Involution import Involution\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ninput=torch.randn(1,4,64,64)\ninvolution=Involution(kernel_size=3,in_channel=4,stride=2)\nout=involution(input)\nprint(out.shape)\n```\n\n***\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\nfrom model.conv.DynamicConv import *\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    input=torch.randn(2,32,64,64)\n    m=DynamicConv(in_planes=32,out_planes=64,kernel_size=3,stride=1,padding=1,bias=False)\n    out=m(input)\n    print(out.shape) #: 2,32,64,64\n\n```\n\n***\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\nfrom model.conv.CondConv import *\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nif __name__ == '__main__':\n    input=torch.randn(2,32,64,64)\n    m=CondConv(in_planes=32,out_planes=64,kernel_size=3,stride=1,padding=1,bias=False)\n    out=m(input)\n    print(out.shape)\n\n```\n\n***\n",
      "technique": "Header extraction"
    }
  ]
}