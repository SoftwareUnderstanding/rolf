{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2103.14030",
      "https://arxiv.org/abs/2112.11081",
      "https://arxiv.org/abs/2101.03697",
      "https://arxiv.org/abs/2103.13425"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{ding2021repvgg,\ntitle={Repvgg: Making vgg-style convnets great again},\nauthor={Ding, Xiaohan and Zhang, Xiangyu and Ma, Ningning and Han, Jungong and Ding, Guiguang and Sun, Jian},\nbooktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\npages={13733--13742},\nyear={2021}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/DingXiaoH/RepVGG",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "dxh17@mails.tsinghua.edu.cn\n\nGoogle Scholar Profile: https://scholar.google.com/citations?user=CIjw0KoAAAAJ&hl=en\n\nMy open-sourced papers and repos: \n\nThe **Structural Re-parameterization Universe**:\n\n1. RepMLP (preprint, 2021) **MLP-style building block and Architecture**\\\n[RepMLPNet: Hierarchical Vision MLP with Re-parameterized Locality](https://arxiv.org/abs/2112.11081)\\\n[code](https://github.com/DingXiaoH/RepMLP).\n\n2. RepVGG (CVPR 2021) **A super simple and powerful VGG-style ConvNet architecture**. Up to **84.16%** ImageNet top-1 accuracy!\\\n[RepVGG: Making VGG-style ConvNets Great Again](https://arxiv.org/abs/2101.03697)\\\n[code](https://github.com/DingXiaoH/RepVGG).\n\n3. ResRep (ICCV 2021) **State-of-the-art** channel pruning (Res50, 55\\% FLOPs reduction, 76.15\\% acc)\\\n[ResRep: Lossless CNN Pruning via Decoupling Remembering and Forgetting](https://openaccess.thecvf.com/content/ICCV2021/papers/Ding_ResRep_Lossless_CNN_Pruning_via_Decoupling_Remembering_and_Forgetting_ICCV_2021_paper.pdf)\\\n[code](https://github.com/DingXiaoH/ResRep).\n\n4. ACB (ICCV 2019) is a CNN component without any inference-time costs. The first work of our Structural Re-parameterization Universe.\\\n[ACNet: Strengthening the Kernel Skeletons for Powerful CNN via Asymmetric Convolution Blocks](http://openaccess.thecvf.com/content_ICCV_2019/papers/Ding_ACNet_Strengthening_the_Kernel_Skeletons_for_Powerful_CNN_via_Asymmetric_ICCV_2019_paper.pdf).\\\n[code](https://github.com/DingXiaoH/ACNet). \n\n5. DBB (CVPR 2021) is a CNN component with higher performance than ACB and still no inference-time costs. Sometimes I call it ACNet v2 because \"DBB\" is 2 bits larger than \"ACB\" in ASCII (lol).\\\n[Diverse Branch Block: Building a Convolution as an Inception-like Unit](https://arxiv.org/abs/2103.13425)\\\n[code](https://github.com/DingXiaoH/DiverseBranchBlock).\n\n6. COMING SOON\n\n7. COMING SOON\n\n**Model compression and acceleration**:\n\n1. (CVPR 2019) Channel pruning: [Centripetal SGD for Pruning Very Deep Convolutional Networks with Complicated Structure](http://openaccess.thecvf.com/content_CVPR_2019/html/Ding_Centripetal_SGD_for_Pruning_Very_Deep_Convolutional_Networks_With_Complicated_CVPR_2019_paper.html)\\\n[code](https://github.com/DingXiaoH/Centripetal-SGD)\n\n2. (ICML 2019) Channel pruning: [Approximated Oracle Filter Pruning for Destructive CNN Width Optimization](http://proceedings.mlr.press/v97/ding19a.html)\\\n[code](https://github.com/DingXiaoH/AOFP)\n\n3. (NeurIPS 2019) Unstructured pruning: [Global Sparse Momentum SGD for Pruning Very Deep Neural Networks](http://papers.nips.cc/paper/8867-global-sparse-momentum-sgd-for-pruning-very-deep-neural-networks.pdf)\\\n[code](https://github.com/DingXiaoH/GSM-SGD)\n",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-12-31T08:53:43Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-27T15:01:32Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This is a super simple ConvNet architecture that achieves over **80% top-1 accuracy on ImageNet with a stack of 3x3 conv and ReLU**! This repo contains the **pretrained models**, code for building the model, training, and the conversion from training-time model to inference-time, and **an example of using RepVGG for semantic segmentation**.\n\nThe MegEngine version: https://github.com/megvii-model/RepVGG.\n\nTensorRT implemention with C++ API by @upczww https://github.com/upczww/TensorRT-RepVGG. Great work!\n\nAnother PyTorch implementation by @zjykzj https://github.com/ZJCV/ZCls. He also presented detailed benchmarks at https://zcls.readthedocs.io/en/latest/benchmark-repvgg/. Nice work!\n\nIncluded in a famous model zoo (over 7k stars) https://github.com/rwightman/pytorch-image-models.\n\nObjax implementation and models by @benjaminjellis. Great work! https://github.com/benjaminjellis/Objax-RepVGG.\n\nCitation:\n\n    @inproceedings{ding2021repvgg,\n    title={Repvgg: Making vgg-style convnets great again},\n    author={Ding, Xiaohan and Zhang, Xiangyu and Ma, Ningning and Han, Jungong and Ding, Guiguang and Sun, Jian},\n    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n    pages={13733--13742},\n    year={2021}\n    }\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9931491390061495
      ],
      "excerpt": "The model is trained with the codebase of Swin Transformer in 300 epochs. The throughput is tested with the Swin codebase as well. We would like to thank the authors of Swin for their clean and well-structured code.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9210984690827954
      ],
      "excerpt": "Compared to RepVGGs, a training-time RepVGGplus model is deeper and has three auxiliary classifiers, which can be removed for inference. Please check repvggplus.py. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9261246802163806
      ],
      "excerpt": "It has 126M inference-time parameters. The training-time weights file is released at Google Drive and Baidu Cloud. Please check the links below.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8213360600595669,
        0.9560187895509076
      ],
      "excerpt": "#:   Build model and data loader as usual \n        for samples, targets in enumerate(train_data_loader): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8124511788431741
      ],
      "excerpt": "                        loss += 0.1 * criterion(pred, targets)          #:  Assume \"criterion\" is cross-entropy for classification \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.801202683192369,
        0.9267544991588319
      ],
      "excerpt": "June 22, 2021 A pure-VGG model (without SE) seems to outperform some vision transformer models with a better training scheme. Training. \nJune 11, 2021 An example of using a simple toolbox, torch.quantization, to quantize RepVGG. Please check it below. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9566263730902108,
        0.9892064012624906,
        0.9828873235095252,
        0.8861451672592572
      ],
      "excerpt": "June 8, 2021 found out that high-performance quantization required a custom weight decay. Such a weight decay also improves the full-precision accuracy. Will release the quantized models after tuning the hyper-parameters and finishing the QAT. \nApr 25, 2021 A deeper RepVGG model achieves 83.55\\% top-1 accuracy on ImageNet with SE blocks and an input resolution of 320x320 (and a wider version achieves 83.67\\% accuracy without SE). Note that it is trained with 224x224 but tested with 320x320, so that it is still trainable with a global batch size of 256 on a single machine with 8 1080Ti GPUs. If you test it with 224x224, the top-1 accuracy will be 81.82%. It has 1, 8, 14, 24, 1 layers in the 5 stages respectively. The width multipliers are a=2.5 and b=5 (the same as RepVGG-B2). The model name is \"RepVGG-D2se\". The code for building the model (repvgg.py) and testing with 320x320 (the testing example below) has been updated and the weights have been released at Google Drive and Baidu Cloud. Please check the links below. \nApr 4, 2021 A better implementation. For a RepVGG model or a model with RepVGG as one of its components (e.g., the backbone), you can convert the whole model by simply calling switch_to_deploy of every RepVGG block. This is the recommended way. Examples are shown in convert.py and example_pspnet.py. \nfor module in model.modules(): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9846213488802991,
        0.8613720428619895,
        0.99839553682128
      ],
      "excerpt": "Apr 4, 2021 An example of using RepVGG as the backbone of PSPNet for semantic segmentation (example_pspnet.py). It shows how to 1) build a PSPNet with RepVGG backbone, 2) load the ImageNet-pretrained weights, 3) convert the whole model with switch_to_deploy, 4) save and use the converted model for inference. \nJan 13 - Feb 5, 2021 You can get the equivalent kernel and bias in a differentiable way at any time (get_equivalent_kernel_bias in repvgg.py). This may help training-based pruning or quantization. This training script (a super simple PyTorch-official-example-style script) has been tested with RepVGG-A0 and B1. The results are even slightly better than those reported in the paper. \nWe present a simple but powerful architecture of convolutional neural network, which has a VGG-like inference-time body composed of nothing but a stack of 3x3 convolution and ReLU, while the training-time model has a multi-branch topology. Such decoupling of the training-time and inference-time architecture is realized by a structural re-parameterization technique so that the model is named RepVGG. On ImageNet, RepVGG reaches over 80\\% top-1 accuracy, which is the first time for a plain model, to the best of our knowledge. On NVIDIA 1080Ti GPU, RepVGG models run 83% faster than ResNet-50 or 101% faster than ResNet-101 with higher accuracy and show favorable accuracy-speed trade-off compared to the state-of-the-art models like EfficientNet and RegNet. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.938136866688131,
        0.9952105654874612
      ],
      "excerpt": "Note that the argument \"deploy\" builds an inference-time model. \nWe trained for 120 epochs with cosine learning rate decay from 0.1 to 0. We used 8 GPUs, global batch size of 256, weight decay of 1e-4 (no weight decay on fc.bias, bn.bias, rbr_dense.bn.weight and rbr_1x1.bn.weight) (weight decay on rbr_identity.weight makes little difference, and it is better to use it in most of the cases), and the same simple data preprocssing as the PyTorch official example: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9522972742911875
      ],
      "excerpt": "The multi-processing training script in this repo is based on the official PyTorch example for the simplicity and better readability. The only modifications include the model-building part, cosine learning rate scheduler, and the SGD optimizer that uses no weight decay on some parameters. You may find these code segments useful for your training code.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9928284665770525,
        0.8105220662673431,
        0.9690382100814929
      ],
      "excerpt": "The best solution for quantization is to constrain the equivalent kernel (get_equivalent_kernel_bias() in repvgg.py) to be low-bit (e.g., make every param in {-127, -126, .., 126, 127} for int8), instead of constraining the params of every kernel separately for an ordinary model. \nFor the simplicity, we can also use the off-the-shelf quantization toolboxes to quantize RepVGG. We use the simple QAT (quantization-aware training) tool in torch.quantization as an example. \nThe base model is trained with the custom weight decay (--custwd) and converted into inference-time structure. We insert BN after the converted 3x3 conv layers because QAT with torch.quantization requires BN. Specifically, we run the model on ImageNet training set and record the mean/std statistics and use them to initialize the BN layers, and initialize BN.gamma/beta accordingly so that the saved model has the same outputs as the inference-time model.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "RepVGG: Making VGG-style ConvNets Great Again",
      "technique": "GitHub API"
    }
  ],
  "documentation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "https://zcls.readthedocs.io/",
      "technique": "Regular expression"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/DingXiaoH/RepVGG/releases",
    "technique": "GitHub API"
  },
  "faq": [
    {
      "confidence": [
        1
      ],
      "excerpt": "**Q**: Is the inference-time model's output the _same_ as the training-time model?\n\n**A**: Yes. You can verify that by\n```\nimport torch\ntrain_model = create_RepVGG_A0(deploy=False)\ntrain_model.eval()      #: Don't forget to call this before inference.\ndeploy_model = repvgg_model_convert(train_model)\nx = torch.randn(1, 3, 224, 224)\ntrain_y = train_model(x)\ndeploy_y = deploy_model(x)\nprint(((train_y - deploy_y) ** 2).sum())    #: Will be around 1e-10\n```\n\n**Q**: How to use the pretrained RepVGG models for other tasks?\n\n**A**: It is better to finetune the training-time RepVGG models on your datasets. Then you should do the conversion after finetuning and before you deploy the models. For example, say you want to use PSPNet for semantic segmentation, you should build a PSPNet with a training-time RepVGG model as the backbone, load pre-trained weights into the backbone, and finetune the PSPNet on your segmentation dataset. Then you should convert the backbone following the code provided in this repo and keep the other task-specific structures (the PSPNet parts, in this case). The pseudo code will be like\n```\n#:   train_backbone = create_RepVGG_B2(deploy=False)\n#:   train_backbone.load_state_dict(torch.load('RepVGG-B2-train.pth'))\n#:   train_pspnet = build_pspnet(backbone=train_backbone)\n#:   segmentation_train(train_pspnet)\n#:   deploy_pspnet = repvgg_model_convert(train_pspnet)\n#:   segmentation_test(deploy_pspnet)\n```\nThere is an example in **example_pspnet.py**.\n\nFinetuning with a converted RepVGG also makes sense if you insert a BN after each conv (please see step 1 of the quantization part), but the performance may be slightly lower.\n\n**Q**: I tried to finetune your model with multiple GPUs but got an error. Why are the names of params like \"stage1.0.rbr_dense.conv.weight\" in the downloaded weight file but sometimes like \"module.stage1.0.rbr_dense.conv.weight\" (shown by nn.Module.named_parameters()) in my model?\n\n**A**: DistributedDataParallel may prefix \"module.\" to the name of params and cause a mismatch when loading weights by name. The simplest solution is to load the weights (model.load_state_dict(...)) before DistributedDataParallel(model). Otherwise, you may insert \"module.\" before the names like this\n```\ncheckpoint = torch.load(...)    #: This is just a name-value dict\nckpt = {('module.' + k) : v for k, v in checkpoint.items()}\nmodel.load_state_dict(ckpt)\n```\nLikewise, if the param names in the checkpoint file start with \"module.\" but those in your model do not, you may strip the names like line 50 in test.py.\n```\nckpt = {k.replace('module.', ''):v for k,v in checkpoint.items()}   #: strip the names\nmodel.load_state_dict(ckpt)\n```\n**Q**: So a RepVGG model derives the equivalent 3x3 kernels before each forwarding to save computations?\n\n**A**: No! More precisely, we do the conversion only once right after training. Then the training-time model can be discarded, and the resultant model only has 3x3 kernels. We only save and use the resultant model.\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 306,
      "date": "Mon, 27 Dec 2021 22:25:03 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/DingXiaoH/RepVGG/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "DingXiaoH/RepVGG",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8044935986319869
      ],
      "excerpt": "                    if name == 'L2': \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9279130782340925
      ],
      "excerpt": "| Model        | Train image size       | Test size  | ImageNet top-1 | Throughput (examples/second), 320, batchsize=128, 2080Ti) | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9115242540905435,
        0.86837450048623
      ],
      "excerpt": "python convert.py RepVGGplus-L2pse-train.pth RepVGGplus-L2pse-deploy.pth -a RepVGGplus-L2pse \npython test.py [imagenet-folder] deploy RepVGGplus-L2pse-deploy.pth -a RepVGGplus-L2pse -r 320 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8565372567089472
      ],
      "excerpt": "                    #:   'main':     the output of the final layer \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.806026885618321
      ],
      "excerpt": "                    elif 'aux' in name: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9368228960008868,
        0.8216270093103228,
        0.9115242540905435
      ],
      "excerpt": "python convert.py [weights file of the training-time model to load] [path to save] -a [model name] \nFor example, \npython convert.py RepVGG-B2-train.pth RepVGG-B2-deploy.pth -a RepVGG-B2 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8906348828761467
      ],
      "excerpt": "python test.py [imagenet-folder with train and val folders] deploy RepVGG-B2-deploy.pth -a RepVGG-B2 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8764991632675346,
        0.9451720259255556
      ],
      "excerpt": "python train.py -a RepVGG-A0 --dist-url 'tcp://127.0.0.1:23333' --dist-backend 'nccl' --multiprocessing-distributed --world-size 1 --rank 0 --workers 32 [imagenet-folder with train and val folders] --tag hello --custwd --wd 4e-5 \npython test.py [imagenet-folder with train and val folders] train RepVGG-A0_hello_best.pth.tar -a RepVGG-A0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8588791702023231,
        0.8876450317958324,
        0.8966111279133185
      ],
      "excerpt": "python train.py -a RepVGG-A0 --dist-url 'tcp://127.0.0.1:23333' --dist-backend 'nccl' --multiprocessing-distributed --world-size 1 --rank 0 --workers 32 [imagenet-folder] --tag hello --custwd \npython convert.py RepVGG-A0_hello_best.pth.tar RepVGG-A0_base.pth -a RepVGG-A0  \npython insert_bn.py [imagenet-folder] RepVGG-A0_base.pth RepVGG-A0_withBN.pth -a RepVGG-A0 -b 32 -n 40000 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/DingXiaoH/RepVGG/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 DingXiaoH\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "RepVGG: Making VGG-style ConvNets Great Again (CVPR-2021) (PyTorch)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "RepVGG",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "DingXiaoH",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/DingXiaoH/RepVGG/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2180,
      "date": "Mon, 27 Dec 2021 22:25:03 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You may download _all_ of the ImageNet-pretrained models reported in the paper from Google Drive (https://drive.google.com/drive/folders/1Avome4KvNp0Lqh2QwhXO6L5URQjzCjUq?usp=sharing) or Baidu Cloud (https://pan.baidu.com/s/1nCsZlMynnJwbUBKn0ch7dQ, the access code is \"rvgg\"). For the ease of transfer learning on other tasks, they are all training-time models (with identity and 1x1 branches). You may test the accuracy by running\n```\npython test.py [imagenet-folder with train and val folders] train [path to weights file] -a [model name]\n```\nThe default input resolution is 224x224. Here \"train\" indicates the training-time architecture, and the valid model names include\n```\nRepVGG-A0, RepVGG-A1, RepVGG-A2, RepVGG-B0, RepVGG-B1, RepVGG-B1g2, RepVGG-B1g4, RepVGG-B2, RepVGG-B2g2, RepVGG-B2g4, RepVGG-B3, RepVGG-B3g2, RepVGG-B3g4\n```\nFor example,\n```\npython test.py [imagenet-folder with train and val folders] train RepVGG-B2-train.pth -a RepVGG-B2\n```\nTo test the latest model RepVGG-D2se with 320x320 inputs,\n```\npython test.py [imagenet-folder with train and val folders] train RepVGG-D2se-200epochs-train.pth -a RepVGG-D2se -r 320\n```\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```\nfrom repvgg import repvgg_model_convert, create_RepVGG_A0\ntrain_model = create_RepVGG_A0(deploy=False)\ntrain_model.load_state_dict(torch.load('RepVGG-A0-train.pth'))          #: or train from scratch\n#: do whatever you want with train_model\ndeploy_model = repvgg_model_convert(train_model, save_path='RepVGG-A0-deploy.pth')\n#: do whatever you want with deploy_model\n```\nor\n```\ndeploy_model = create_RepVGG_A0(deploy=True)\ndeploy_model.load_state_dict(torch.load('RepVGG-A0-deploy.pth'))\n#: do whatever you want with deploy_model\n```\nIf you use RepVGG as a component of another model, the conversion is as simple as calling **switch_to_deploy** of every RepVGG block. \n\n",
      "technique": "Header extraction"
    }
  ]
}