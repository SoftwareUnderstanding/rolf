{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1901.07031]](https://arxiv.org/pdf/1901.07031.pdf)\n- Densely Connected Convolutional Networks, Huang et al., 2018 [[https://arxiv.org/abs/1608.06993v5]](https://arxiv.org/pdf/1608.06993.pdf)\n- Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization, Selvaraju et al., 2019 [[https://arxiv.org/abs/1610.02391v4]](https://arxiv.org/pdf/1610.02391.pdf)\n- Communication-Efficient Learning of Deep Networks from Decentralized Data, McMahan et al., 2017 [[https://arxiv.org/abs/1602.05629v3]](https://arxiv.org/pdf/1602.05629.pdf)\n- CheXpert implementaion: [Github repository](https://github.com/gaetandi/cheXpert)\n- Grad-CAM implementaion: [Github repository](https://github.com/ooodmt/MLMIP.git)",
      "https://arxiv.org/abs/1608.06993v5]](https://arxiv.org/pdf/1608.06993.pdf)\n- Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization, Selvaraju et al., 2019 [[https://arxiv.org/abs/1610.02391v4]](https://arxiv.org/pdf/1610.02391.pdf)\n- Communication-Efficient Learning of Deep Networks from Decentralized Data, McMahan et al., 2017 [[https://arxiv.org/abs/1602.05629v3]](https://arxiv.org/pdf/1602.05629.pdf)\n- CheXpert implementaion: [Github repository](https://github.com/gaetandi/cheXpert)\n- Grad-CAM implementaion: [Github repository](https://github.com/ooodmt/MLMIP.git)",
      "https://arxiv.org/abs/1610.02391v4]](https://arxiv.org/pdf/1610.02391.pdf)\n- Communication-Efficient Learning of Deep Networks from Decentralized Data, McMahan et al., 2017 [[https://arxiv.org/abs/1602.05629v3]](https://arxiv.org/pdf/1602.05629.pdf)\n- CheXpert implementaion: [Github repository](https://github.com/gaetandi/cheXpert)\n- Grad-CAM implementaion: [Github repository](https://github.com/ooodmt/MLMIP.git)",
      "https://arxiv.org/abs/1602.05629v3]](https://arxiv.org/pdf/1602.05629.pdf)\n- CheXpert implementaion: [Github repository](https://github.com/gaetandi/cheXpert)\n- Grad-CAM implementaion: [Github repository](https://github.com/ooodmt/MLMIP.git)"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison, Irvin et al., 2019 [[arXiv:1901.07031]](https://arxiv.org/pdf/1901.07031.pdf)\n- Densely Connected Convolutional Networks, Huang et al., 2018 [[arXiv:1608.06993v5]](https://arxiv.org/pdf/1608.06993.pdf)\n- Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization, Selvaraju et al., 2019 [[arXiv:1610.02391v4]](https://arxiv.org/pdf/1610.02391.pdf)\n- Communication-Efficient Learning of Deep Networks from Decentralized Data, McMahan et al., 2017 [[arXiv:1602.05629v3]](https://arxiv.org/pdf/1602.05629.pdf)\n- CheXpert implementaion: [Github repository](https://github.com/gaetandi/cheXpert)\n- Grad-CAM implementaion: [Github repository](https://github.com/ooodmt/MLMIP.git)\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "Observation | Experiment AUC | Paper AUC | Difference \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "Cardiomegaly | 0.80 | 0.90 | -0.10 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Stomper10/CheXpert",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-08-13T10:10:01Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-04T14:26:19Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9483404617928037,
        0.9651107349498524,
        0.8405497542102727
      ],
      "excerpt": "CheXpert is a large dataset of chest X-rays and competition for automated chest X-ray interpretation, which features uncertainty labels and radiologist-labeled reference standard evaluation sets. - from the CheXpert webpage description \nThis repository aims to reproduce and improve CheXpert paper's results using the PyTorch library. \nFor the baseline, I tried to use the same model(DenseNet121) and the same parameters as the paper. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8327542518773415
      ],
      "excerpt": "This repository especially referenced here for the coding part. You can easily run the code with the following instructions. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.942888173890186
      ],
      "excerpt": "The following table shows a comparison with the original paper results. I know it's not an accurate comparison since the test set is different. But at least we can roughly gauge the model's performance. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8885129244248329
      ],
      "excerpt": "Mean of 5 obs. | 0.86 | 0.91 | -0.05 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8554336355531361
      ],
      "excerpt": "[x] Apply Grad-CAM method for localization. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Reproducing CheXpert paper using the PyTorch library.",
      "technique": "GitHub API"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "At the bottom of the CheXpert [webpage](https://stanfordmlgroup.github.io/competitions/chexpert/), write a registration form to download the CheXpert dataset.\nYou will receive an email with the download link. Right-click your mouse on the download link(439GB or 11GB) and click 'Copy link address'.\n\nThen, run the following command. Paste your link address inside the double quotation marks(PLEASE RUN WITH QUOTATION MARKS). It will take a while even for the downsampled(11GB) dataset.\n\n```bash\nwget \"link_address_you_copied\"\n```\n\nAfter you downloaded the dataset, run the following command to unzip the `.zip` file.\n\n```bash\nunzip CheXpert-v1.0-small.zip\nunzip CheXpert-v1.0.zip\n```\n\nNow the data is ready. As you see this repository structure, you have to place the `CheXpert-v1.0-small`, and `CheXpert-v1.0` directories with the rest of the files(`.py` and `.ipynb`) at the same location unless you modify the path in the source code.\n\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Stomper10/CheXpert/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3,
      "date": "Wed, 22 Dec 2021 08:26:43 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Stomper10/CheXpert/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Stomper10/CheXpert",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Stomper10/CheXpert/master/CheXpert_DenseNet121_FL.ipynb",
      "https://raw.githubusercontent.com/Stomper10/CheXpert/master/Grad-CAM.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You can set hyperparameters in the `configuration.json` file such as maximum training epoch, type of image, batch size, etc.\nHere are some descriptions of hyperparameters.\n\nName | Example | Description\n:-: | :-: | :-:\nimage_type | \"small\" | The image type of data. <br /> For the original image (439GB), set as `\"\"`.\npre_trained | false | Whether the model is pre-trained.\nnnClassCount | 5 | The number of training observations. Set to 5.\npolicy | \"diff\" | If set to `\"diff\"`, the optimal policy will be applied to each of the 5 competition observations. <br /> If set to `\"ones\"`, the U-Ones policy will be applied. <br /> If else, the U-Zeroes policy will be applied.\nbatch_size | 16 | The number of batch size.\nepochs | 3 | The number of training epochs.\nimgtransResize | 320 | Resizing image in transformation.\ntrain_ratio | 1 | Training data ratio.\nlr | 0.0001 | Learning rate of optimizer.\nbetas | [0.9, 0.999] | Betas of optimizer.\neps | 1e-08 | Eps of optimizer.\nweight_decay | 0 | Weight decay of optimizer.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8884382083831807,
        0.8934222616768606
      ],
      "excerpt": "This repository especially referenced here for the coding part. You can easily run the code with the following instructions. \nThis code is written for the GPU environment. It could be hard to run in CPU environment. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8837680365796365,
        0.9322609392449874
      ],
      "excerpt": "- Python 3.6+ \n- PyTorch 1.7.1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8968929504744036,
        0.9893272198983933
      ],
      "excerpt": "First, use git clone command to download this repository. \ngit clone https://github.com/Stomper10/CheXpert.git \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8354745509245992,
        0.8070481054983674
      ],
      "excerpt": "python3 run_preprocessing.py configuration.json \nYou may get training and validation losses, as well as the test accuracy and ROC curves. You can also check the computational costs. Models(*.pth.tar), test set probabilities(testPROB_frt.txt, testPROB_lat.txt, testPROB_all.txt), and ROC curve(ROC_*.png) files will be saved in the results directory. In addition, the best models for each competition observation is also saved. If you run the code with nohup command, you can also save whole printed outputs. Let me just show you the ROC curves here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.815290996270908
      ],
      "excerpt": "Cardiomegaly | 0.80 | 0.90 | -0.10 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8382556138828143,
        0.8820913995553753,
        0.8682213853501842
      ],
      "excerpt": "Under the ensembles directory, place ONLY experiment output directories you want to aggregate. If you place other directories, it will throw an error. \nWhen running the run_chexpert.py, set --output_path under the ensembles directory. \nrun_ensembles.py have --output_path option just like run_chexpert.py. The ensemble results will be saved in --output_path you set. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8572735166155699,
        0.8572735166155699,
        0.8572735166155699,
        0.8661370683878374
      ],
      "excerpt": "python3 run_chexpert.py configuration.json --output_path=emsembles/experiment_01/ --random_seed=1 \npython3 run_chexpert.py configuration.json --output_path=emsembles/experiment_02/ --random_seed=2 \npython3 run_chexpert.py configuration.json --output_path=emsembles/experiment_03/ --random_seed=3 \npython3 run_ensembles.py configuration.json --output_path=results/ensem_results/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8633215488576798,
        0.8177037314804038
      ],
      "excerpt": "[x] Adjust the number of training data per each class. \n[x] Use whole training dataset(frontal images). \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Stomper10/CheXpert/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "CheXpert: A Large Chest X-Ray Dataset and Competitions",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "CheXpert",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Stomper10",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Stomper10/CheXpert/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Now you can run the model like below. You can use some options to run the model.\n```bash\npython3 run_chexpert.py configuration.json\n```\n\nOptions | Shortcut | Description | Default\n:-: | :-: | :-: | :-:\n--output_path| -o | Path to save results. | results/\n--random_seed | -s | Random seed for reproduction. | 0\n\nI also recommend you to use the `nohup` command if you run this code on server since it takes several hours.\n```bash\nnohup python3 run_chexpert.py configuration.json > progress.txt &\n```\n\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 7,
      "date": "Wed, 22 Dec 2021 08:26:43 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "python",
      "pytorch",
      "jupyter-notebook",
      "chexpert",
      "cnn",
      "cnn-classification",
      "densenet-pytorch"
    ],
    "technique": "GitHub API"
  }
}