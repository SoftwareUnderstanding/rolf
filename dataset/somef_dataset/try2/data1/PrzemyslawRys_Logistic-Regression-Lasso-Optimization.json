{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1407.0202. Let me point out, that here algorithm is used for problem with differentiable log-likelihood, so proximal gradient operator is just an identity.\n\n- **Proximal Gradient** for optimization with Lasso regularization *optimizeLogitLassoProximalGradient*: log-likelihood with penalty of Lasso type is no longer differentiable, but it is convex. In consequence, it is impossible to use standard stochastic gradient, thus proximal gradient should be used instead.\n\n- **Nesterov acceleration** for for optimization with Lasso regularization *optimizeLogitLassoNesterov*: method similar to proximal gradient, but with faster convergence.\n\n## Getting Started\n\nThese instructions will get you a copy of the project up and running on your local machine for development and testing purposes.\n\n### Prerequisites\n\nPlease check the prerequisites below and make sure to install all packages and libraries before running the code.\n\n- *MASS* package for *mvnorm* function, generating observations from multivariate gaussian distribution used in function *generateRandomLogitDataset*\n- *dplyr* package for data wrangling and *%>%* operator,\n\n### Input data structure\n\nThe input data structure is as following: the dataset should be a tibble object with each observation in separate row. The last column should contain binary variable $Y$, which is explained by the model. You could easily generate random dataset of this form using *generateRandomLogitDataset* function from *src* folder.\n\n\n### Example of use\n\n```{r}\nlibrary(MASS"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/PrzemyslawRys/Logistic-Regression-Lasso-Optimization",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-10-10T14:38:26Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-10-20T09:04:03Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9993132955828166,
        0.9921128777190766
      ],
      "excerpt": "The project contains the implementation of different methods for optimization of Logistic Regression Model with and without Lasso regularization. All optimization procedures aim to find model parameters, maximizing likelihood for a given dataset with optionally Lasso penalty. Additionaly, function generating random datasets is provided for testing purposes. The main purpose of this project is to show how different techniques, based on gradient descent could be implemented for Logistic Regression (Logit) Model. Please be aware that R environment allows us to create pretty simple codes, easy to use for educational purposes, but calculations are not very effective. Methods are based on consecutive steps implemented as for loops, what without doubt would be much less time-consuming in, for instance, C++ language. If you are going to reproduce provided functions in C++ please remember about RCPP package for integration of R and C++. \nLogistic Regression (Logit) Model is a basic binary regression model, widely used for scoring, e.g. in area of credit risk. The formula of model is as following: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9979582259972374,
        0.9187270363532373
      ],
      "excerpt": "The variable $p$ is in interval $[0,1]$ and is often interpreted as probability of some event, e.g. default of client. $(\\beta_i){i=1}^n$ is vector of parameters and $(x_i){i=1}^n$ is vector of independent variables. \nThe considered algorithms try to minimize log-likelihood function instead of likelihood, what is equivalent. The gradient of this function is quite easy to calculate. Let start with formula for log-likelihood function: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9359616703014413
      ],
      "excerpt": "So gradient of this function is equal to: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9933003567717257
      ],
      "excerpt": "Where $y_i$ denotes observations of binary depentent variable and $X$ denotes matrix of independent variables, $\\beta$ is vector of model parameters. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Implementation of different methods for optimization of logistic regression model with and without Lasso regularization.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/PrzemyslawRys/Logistic-Regression-Lasso-Optimization/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 30 Dec 2021 02:54:24 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/PrzemyslawRys/Logistic-Regression-Lasso-Optimization/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "PrzemyslawRys/Logistic-Regression-Lasso-Optimization",
    "technique": "GitHub API"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/PrzemyslawRys/Logistic-Regression-Lasso-Optimization/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "R"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Optimization of Logit (Logistic Regression) Model with Lasso Regularization",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Logistic-Regression-Lasso-Optimization",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "PrzemyslawRys",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/PrzemyslawRys/Logistic-Regression-Lasso-Optimization/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please check the prerequisites below and make sure to install all packages and libraries before running the code.\n\n- *MASS* package for *mvnorm* function, generating observations from multivariate gaussian distribution used in function *generateRandomLogitDataset*\n- *dplyr* package for data wrangling and *%>%* operator,\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Thu, 30 Dec 2021 02:54:24 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "These instructions will get you a copy of the project up and running on your local machine for development and testing purposes.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```{r}\nlibrary(MASS)\nlibrary(dplyr)\n\nsource(\"src/fun-getGradientLogLikelihood.R\")\nsource(\"src/fun-generateRandomLogitDataset.R\")\nsource(\"src/fun-optimizeLogitStochasticGradient.R\")\n\n\ndataset <- generateRandomLogitDataset(numberOfVariables = 7,\n                                      numberOfObservations = 20000)\n\nresults <- optimizeLogitStochasticGradient(dataset = dataset,\n                                           gradientStepSize = 0.01,\n                                           batchSize = 500,\n                                           numberOfSteps = 10000,\n                                           historyFrequency = 10)\n```\n\n\n\n",
      "technique": "Header extraction"
    }
  ]
}