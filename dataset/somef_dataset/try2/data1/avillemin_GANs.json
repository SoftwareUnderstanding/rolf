{
  "citation": [
    {
      "confidence": [
        0.874446915586759,
        0.9994195181087455
      ],
      "excerpt": "<p align=\"center\"><img src=\"https://github.com/diegoalejogm/gans/raw/master/.images/dcgan_mnist.gif\"></p> \n[Paper] Generative Adversarial Nets : https://arxiv.org/pdf/1406.2661.pdf \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9977994744046882
      ],
      "excerpt": "[Paper] https://arxiv.org/pdf/1603.07285.pdf \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/avillemin/GANs",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-11-24T13:07:28Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-10-01T14:30:26Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8866488637032891,
        0.9875336691303355
      ],
      "excerpt": "GANs are generative models devised by Goodfellow et al. in 2014. In a GAN setup, two differentiable functions, represented by neural networks, are locked in a game. The two players (the generator and the discriminator) have different roles in this framework, as we will se below. \nIn this repository, I'm trying to generate hand-written digits as below. To do this, I had to learn what is a GAN and how it works. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9346074874477054
      ],
      "excerpt": "[Source] GANs from Scratch 1: A deep introduction. With code in PyTorch and TensorFlow : https://medium.com/ai-society/gans-from-scratch-1-a-deep-introduction-with-code-in-pytorch-and-tensorflow-cb03cdcdba0f \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9688109815292758
      ],
      "excerpt": "Some of the most relevant GAN pros and cons for the are: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9276343333034428
      ],
      "excerpt": "- They are easy to train (since no statistical inference is required), and only back-propogation is needed to obtain gradients \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9234122113202441,
        0.8678384169217103,
        0.9831007160447991,
        0.9915633689335217,
        0.9926253516989948,
        0.9749772712546243,
        0.9784544679344609,
        0.9772537555758666,
        0.9722128521483049,
        0.8997998153206437,
        0.9452273063020694
      ],
      "excerpt": "- No statistical inference can be done with them (except here): GANs belong to the class of direct implicit density models; they model p(x) without explicitly defining the p.d.f. \nGenerative Adversarial Networks are composed of two models: \n- The first model is called a Generator and it aims to generate new data similar to the expected one. The Generator could be asimilated to a human art forger, which creates fake works of art. \n- The second model is named the Discriminator. This model\u2019s goal is to recognize if an input data is \u2018real\u2019\u200a\u2014\u200abelongs to the original dataset\u200a\u2014\u200aor if it is \u2018fake\u2019\u200a\u2014\u200agenerated by a forger. In this scenario, a Discriminator is analogous to the police (or an art expert), which tries to detect artworks as truthful or fraud.   \nThe Generator (forger) needs to learn how to create data in such a way that the Discriminator isn\u2019t able to distinguish it as fake anymore. The competition between these two teams is what improves their knowledge, until the Generator succeeds in creating realistic data. \nA neural network G(z, \u03b8\u2081) is used to model the Generator mentioned above. It\u2019s role is mapping input noise variables z to the desired data space x (say images). Conversely, a second neural network D(x, \u03b8\u2082) models the discriminator and outputs the probability that the data came from the real dataset, in the range (0,1). In both cases, \u03b8\u1d62 represents the weights or parameters that define each neural network.  \nAs a result, the Discriminator is trained to correctly classify the input data as either real or fake. This means it\u2019s weights are updated as to maximize the probability that any real data input x is classified as belonging to the real dataset, while minimizing the probability that any fake image is classified as belonging to the real dataset. In more technical terms, the loss/error function used maximizes the function D(x), and it also minimizes D(G(z)). \nFurthermore, the Generator is trained to fool the Discriminator by generating data as realistic as possible, which means that the Generator\u2019s weight\u2019s are optimized to maximize the probability that any fake image is classified as belonging to the real datase. Formally this means that the loss/error function used for this network maximizes D(G(z)). \nIn practice, the logarithm of the probability (e.g. log D(\u2026)) is used in the loss functions instead of the raw probabilies, since using a log loss heavily penalises classifiers that are confident about an incorrect classification. \nAfter several steps of training, if the Generator and Discriminator have enough capacity (if the networks can approximate the objective functions), they will reach a point at which both cannot improve anymore. At this point, the generator generates realistic synthetic data, and the discriminator is unable to differentiate between the two types of input. \nSince both the generator and discriminator are being modeled with neural, networks, agradient-based optimization algorithm can be used to train the GAN. In our coding example we\u2019ll be using stochastic gradient descent, as it has proven to be succesfull in multiple fields. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9262070771160716
      ],
      "excerpt": "TensorBoard : \"The computations you'll use TensorFlow for - like training a massive deep neural network - can be complex and confusing. To make it easier to understand, debug, and optimize TensorFlow programs, we've included a suite of visualization tools called TensorBoard. You can use TensorBoard to visualize your TensorFlow graph, plot quantitative metrics about the execution of your graph, and show additional data like images that pass through it.\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8468955212438473,
        0.9240984876000686,
        0.9378248910585223
      ],
      "excerpt": "Different types of ReLU, see : https://medium.com/tinymind/a-practical-guide-to-relu-b83ca804f1f7 \nHere we\u2019ll use Adam as the optimization algorithm for both neural networks, with a learning rate of 0.0002. The proposed learning rate was obtained after testing with several values, though it isn\u2019t necessarily the optimal value for this task.   \nThe loss function we\u2019ll be using for this task is named Binary Cross Entopy Loss (BCE Loss), and it will be used for this scenario as it resembles the log-loss for both the Generator and Discriminator defined earlier in the post (see Modeling Mathematically a GAN). Specifically we\u2019ll be taking the average of the loss calculated for each minibatch.    \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9570685320181865
      ],
      "excerpt": "In this formula the values y are named targets, v are the inputs, and w are the weights. Since we don\u2019t need the weight at all, it\u2019ll be set to w\u1d62=1 for all i. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9594558308082346
      ],
      "excerpt": "If we replace v\u1d62 = D(x\u1d62) and y\u1d62=1 \u2200 i (for all i) in the BCE-Loss definition, we obtain the loss related to the real-images. Conversely if we set v\u1d62 = D(G(z\u1d62)) and y\u1d62=0 \u2200 i, we obtain the loss related to the fake-images. In the mathematical model of a GAN I described earlier, the gradient of this had to be ascended, but PyTorch and most other Machine Learning frameworks usually minimize functions instead. Since maximizing a function is equivalent to minimizing it\u2019s negative, and the BCE-Loss term has a minus sign, we don\u2019t need to worry about the sign. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9422479828646618
      ],
      "excerpt": "Maximizing log D(G(z)) is equivalent to minimizing it\u2019s negative and since the BCE-Loss definition has a minus sign, we don\u2019t need to take care of the sign. Similarly to the Discriminator, if we set v\u1d62 = D(G(z\u1d62)) and y\u1d62=1 \u2200 i, we obtain the desired loss to be minimized. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9752621452380099,
        0.9883612414750518
      ],
      "excerpt": "DCGAN is one of the popular and successful network design for GAN. It mainly composes of convolution layers without max pooling or fully connected layers. It uses convolutional stride and transposed convolution for the downsampling and the upsampling. The figure below is the network design for the generator. \nHere is the summary of DCGAN: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8510387212135265
      ],
      "excerpt": "One important point of such convolution operation is that the positional connectivity exists between the input values and the output values. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9485582678904829
      ],
      "excerpt": "More concretely, the 3x3 kernel is used to connect the 9 values in the input matrix to 1 value in the output matrix. A convolution operation forms a many-to-one relationship. Let\u2019s keep this in mind as we need it later on. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8843283655311431
      ],
      "excerpt": "We rearrange the 3x3 kernel into a 4x16 matrix as below: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9786224255391244
      ],
      "excerpt": "Now, suppose we want to go the other direction. We want to associate 1 value in a matrix to 9 values in another matrix. It\u2019s a one-to-many relationship. This is like going backward of convolution operation, and it is the core idea of transposed convolution. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8435462857926844,
        0.9032395286438267
      ],
      "excerpt": "Going backward of Convolution \nWe want to go from 4 (2x2) to 16 (4x4). So, we use a 16x4 matrix. But there is one more thing here. We want to maintain the 1 to 9 relationship. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9451400681672217,
        0.9292096438621397
      ],
      "excerpt": "We have just up-sampled a smaller matrix (2x2) into a larger one (4x4). The transposed convolution maintains the 1 to 9 relationship because of the way it lays out the weights.  \nNB: the actual weight values in the matrix does not have to come from the original convolution matrix. What important is that the weight layout is transposed from that of the convolution matrix. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Generative Adversarial Networks",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/avillemin/GANs/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 5,
      "date": "Wed, 29 Dec 2021 07:32:49 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/avillemin/GANs/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "avillemin/GANs",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/avillemin/GANs/master/GAN.ipynb",
      "https://raw.githubusercontent.com/avillemin/GANs/master/DCGAN.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9038657994739455
      ],
      "excerpt": "The fundamental steps to train a GAN can be described as following: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.826949922467505
      ],
      "excerpt": "[Source] https://towardsdatascience.com/up-sampling-with-transposed-convolution-9ae4f2df52d0 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/avillemin/GANs/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Generative Adversarial Networks",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "GANs",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "avillemin",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/avillemin/GANs/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Wed, 29 Dec 2021 07:32:49 GMT"
    },
    "technique": "GitHub API"
  }
}