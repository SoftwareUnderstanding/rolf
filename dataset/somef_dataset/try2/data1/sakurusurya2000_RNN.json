{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1608.05148"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/sakurusurya2000/RNN",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-09-29T07:22:49Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-04-24T13:07:42Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9547479735268525,
        0.9724443254592803
      ],
      "excerpt": "This project inspired by Google's paper Full Resolution Image Compression with Recurrent Neural Networks (arxiv) and its TensorFlow implementation. \nThe code inside aims to compare (quantitatively and qualitatively) different aspects of compression done by this method and codecs popular today, in different compression levels, for different image resolutions. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8939847250635431,
        0.8011204421600623,
        0.9250575104928522
      ],
      "excerpt": "--quality=0|...|15 - level of compression, default is 8; \n--model=/path/to/model - pre-trained model file, default is google's-compression-model/residual_gru.pb. \nNotice that the image should have width and height multiple of 32 according the paper. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8621948799512976,
        0.8011204421600623
      ],
      "excerpt": "--quality=0|...|15,...,0|...|15 - levels of compression, default is 3,6,9,12; \n--model=/path/to/model - pre-trained model file, default is google's-compression-model/residual_gru.pb. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/sakurusurya2000/RNN/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Wed, 22 Dec 2021 08:23:03 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/sakurusurya2000/RNN/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "sakurusurya2000/RNN",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/sakurusurya2000/RNN/master/install_deps.sh",
      "https://raw.githubusercontent.com/sakurusurya2000/RNN/master/download_model.sh",
      "https://raw.githubusercontent.com/sakurusurya2000/RNN/master/generate_test_samples.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. First, install suitable TensorFlow version. See instruction [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md).\n\n2. Install other project dependencies:\n\n    `./install_deps.sh`\n\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8040302988032438
      ],
      "excerpt": "If you want just to try Google's model, you should run python nn_compression_example.py with following parameters: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8294939570885751
      ],
      "excerpt": "--model=/path/to/model - pre-trained model file, default is google's-compression-model/residual_gru.pb. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8065935614749863
      ],
      "excerpt": "Otherwise, you can successively run python generate_test_samples_regular.py and python generate_test_samples_nn.py. First one will generate some test samples, comressed with the regular codecs (list of codecs you can find in the code) and also reshapes images to be multiple of 32 by both sides. Second script will generate a bunch of test samples, comressed using the method encoding. You can specify the following parameters for the last one: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8294939570885751
      ],
      "excerpt": "--model=/path/to/model - pre-trained model file, default is google's-compression-model/residual_gru.pb. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.912386962014299
      ],
      "excerpt": "Finally, run python benchmark_quality_msssim.py for quality benchmarking based on MS-SSIM metric. It will generate results in CSV format. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/sakurusurya2000/RNN/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Do What The F*ck You Want To Public License",
      "url": "https://api.github.com/licenses/wtfpl"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'            DO WHAT THE FUCK YOU WANT TO PUBLIC LICENSE\\r\\n                    Version 2, December 2004\\r\\n\\r\\n Copyright (C) 2004 Sam Hocevar &#115;&#97;&#109;&#64;&#104;&#111;&#99;&#101;&#118;&#97;&#114;&#46;&#110;&#101;&#116;\\r\\n\\r\\n Everyone is permitted to copy and distribute verbatim or modified\\r\\n copies of this license document, and changing it is allowed as long\\r\\n as the name is changed.\\r\\n\\r\\n            DO WHAT THE FUCK YOU WANT TO PUBLIC LICENSE\\r\\n   TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION\\r\\n\\r\\n  0. You just DO WHAT THE FUCK YOU WANT TO.\\r\\n\\r\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Image Compression Benchmarking",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "RNN",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "sakurusurya2000",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/sakurusurya2000/RNN/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Hardware:\n\n* **GPU is not necessary** but preferable\n* At least 3Gb of RAM\n\nSoftware:\n\n* [Ubuntu](https://www.ubuntu.com/) 14.04+ (tested)\n* [TensorFlow](https://www.tensorflow.org/)\n* [Pillow](https://python-pillow.org/) 3.4+\n* [NumPy](http://www.numpy.org/) 1.11+\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Wed, 22 Dec 2021 08:23:03 GMT"
    },
    "technique": "GitHub API"
  }
}