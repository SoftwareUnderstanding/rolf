{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2010.10392\n\n\"[CharacterBERT: Reconciling ELMo and BERT for Word-LevelOpen-Vocabulary Representations From Characters][paper2]\"\n\n## Table of contents\n\n- [Paper summary](#paper-summary",
      "https://arxiv.org/abs/1810.04805",
      "https://arxiv.org/abs/1802.05365",
      "https://arxiv.org/abs/2103.09645"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\n@article{karimi2021uniparma,\n  title={Uniparma at semeval-2021 task 5: Toxic spans detection using characterbert and bag-of-words model},\n  author={Karimi, Akbar and Rossi, Leonardo and Prati, Andrea},\n  journal={arXiv preprint arXiv:2103.09645},\n  year={2021}\n}\n\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{karimi2021uniparma,\n  title={Uniparma at semeval-2021 task 5: Toxic spans detection using characterbert and bag-of-words model},\n  author={Karimi, Akbar and Rossi, Leonardo and Prati, Andrea},\n  journal={arXiv preprint arXiv:2103.09645},\n  year={2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9418631341622122
      ],
      "excerpt": "How do I reproduce the paper's results? \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "<div style=\"text-align:center\"> \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/IMPLabUniPr/UniParma-at-semeval-2021-task-5",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-28T10:40:06Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-27T06:28:11Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9973827976575503,
        0.9933776518125434,
        0.8795688780272245
      ],
      "excerpt": "This is the repository of the paper \"UniParma at SemEval-2021 Task 5: Toxic Spans Detection Using CharacterBERT and Bag-of-Words Model\". \nThe rest of the description is for the CharacterBERT paper. For the description of our paper, please refer to the provided link above. Also to run CharacterBERT along with our bag of words model, follow the steps in steps.txt. \n\"CharacterBERT: Reconciling ELMo and BERT for Word-LevelOpen-Vocabulary Representations From Characters\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9963748655468393
      ],
      "excerpt": "CharacterBERT is a variant of BERT that produces word-level contextual representations by attending to the characters of each input token. To achieve that, instead of relying on a matrix of pre-defined wordpieces, it uses a CharacterCNN module similar to ELMo to produce representations for arbitrary tokens. Besides this difference, CharacterBERT's architecture is identical BERT's. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9883146237526332
      ],
      "excerpt": "The figure above shows the way context-independent representations are built in BERT and CharacterBERT. Here, we suppose that \"Apple\" is an unknown token and see that BERT splits it into two wordpieces \"Ap\" and \"##ple\" before embedding each unit. On the other hand, CharacterBERT receives the token \"Apple\" as is then attends to its characters to produce a single token embedding. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9919856067742627,
        0.9748014357593692,
        0.99575740124638
      ],
      "excerpt": "In more and more cases, the original BERT is adapted to new domains (e.g. medical domain) by re-training it on specialized corpora. In these cases, the original (general domain) wordpiece vocabulary is kept despite the model being actually used on a different domain, which seemed suboptimal (see Section 2 of the paper). A naive solution would be to train a new BERT from scratch with a specialized wordpiece vocabulary, but training a single BERT is costly let alone training one for each and every domain of interest. \nBERT uses a wordpiece system as a good compromise between the specificity of tokens and generality of characters. However, working with subwords is not very convenient in practice (Should we average the representations to get the original token embedding for word similarity tasks ? Should we only use the first wordpiece of each token in sequence labelling tasks ? ...) \nInspired by ELMo, we use a CharacterCNN module and manage to get a variant of BERT that produces word-level contextual representations and can be re-adapted on any domain without needing to worry about the suitability of any wordpieces. Moreover, attending to the characters of input tokens also allows us to achieve superior robustness to noise (see Section 5.5 of the paper). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8112146660077283
      ],
      "excerpt": "| Keyword                | Model description                                                                                                                                                                                                                                                         | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8247010068927373
      ],
      "excerpt": "<sup>1, 2</sup> <small>We pre-train BERT models as well so that we can fairly compare each CharacterBERT model to it's BERT counterpart. Our BERT models use the same architecture and vocabulary as bert-base-uncased.</small><br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8162159681305379
      ],
      "excerpt": "CharacterBERT's architecture is almost identical to BERT, so you can easilly adapt any code that uses the Transformers library. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model.bert.embeddings.word_embeddings  #: wordpiece embeddings \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9567588029116127
      ],
      "excerpt": ":#:#:#: REPLACING BERT WITH CHARACTER_BERT #:#:#:#: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877,
        0.9688644003642316
      ],
      "excerpt": "model.bert = character_bert_model \nmodel.bert.embeddings.word_embeddings  #: wordpieces are replaced with a CharacterCNN \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9227761612115881
      ],
      "excerpt": "tokenized_text = bert_tokenizer.basic_tokenizer.tokenize(text) #: this is NOT wordpiece tokenization \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8426123507474765
      ],
      "excerpt": "indexer = CharacterIndexer()  #: This converts each token into a list of character indices \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9360209207712284
      ],
      "excerpt": ":#:#:#: USING CHARACTER_BERT FOR INFERENCE #:#:#:#: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Main repository for \"UniParma at SemEval-2021 Task 5: Toxic Spans Detection Using CharacterBERT and Bag-of-Words Model\"",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/IMPLabUniPr/UniParma-at-semeval-2021-task-5/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sat, 25 Dec 2021 13:35:03 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/IMPLabUniPr/UniParma-at-semeval-2021-task-5/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "IMPLabUniPr/UniParma-at-semeval-2021-task-5",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/IMPLabUniPr/UniParma-at-semeval-2021-task-5/main/bow_model/ToxicSpans_SemEval21.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/IMPLabUniPr/UniParma-at-semeval-2021-task-5/main/run_experiments.sh",
      "https://raw.githubusercontent.com/IMPLabUniPr/UniParma-at-semeval-2021-task-5/main/run.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We recommend using a virtual environment that is specific to using CharacterBERT.\n\nIf you do not already have `conda` installed, you can install Miniconda from [this link](https://docs.conda.io/en/latest/miniconda.html#linux-installers) (~450Mb). Then, check that conda is up to date:\n\n```bash\nconda update -n base -c defaults conda\n```\n\nAnd create a fresh conda environment (~220Mb):\n\n```bash\nconda create python=3.8 --name=character-bert\n```\n\nIf not already activated, activate the new conda environment using:\n\n```bash\nconda activate character-bert\n```\n\nThen install the following packages (~3Gb):\n\n```bash\nconda install pytorch torchvision cudatoolkit=10.2 -c pytorch\npip install transformers==3.3.1 scikit-learn==0.23.2\n```\n\n> Note 1: If you will not be running experiments on a GPU, install pyTorch via this command instead `conda install pytorch torchvision cpuonly -c pytorch`\n\n> Note 2: If you just want to be able to load pre-trained CharacterBERT weigths, you do not have to install `scikit-learn` which is only used for computing Precision, Recall, F1 metrics during evaluation.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8498334502192549
      ],
      "excerpt": "You can use the download.py script to download any of the models below: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9551280344871405
      ],
      "excerpt": "For example, to download the medical version of CharacterBERT you can run: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8909838899523493
      ],
      "excerpt": "Or you can download all models by running: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8547402240370136,
        0.9465718491881494
      ],
      "excerpt": "For more complete (but still illustrative) examples you can refer to the run_experiments.sh script which runs a few Classification/SequenceLabelling experiments using BERT/CharacterBERT. \nbash run_experiments.sh \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8967198598353515
      ],
      "excerpt": "    <img src=\"./img/archi-compare.png\" width=\"45%\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9403475490777098
      ],
      "excerpt": "python download.py --model='medical_character_bert' \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9258363020883161
      ],
      "excerpt": "python download.py --model='all' \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "from transformers import BertForSequenceClassification, BertConfig \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8245539886860519
      ],
      "excerpt": "    './pretrained-models/medical_character_bert/') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8191768911750371,
        0.8801854956928516,
        0.9416522774131079
      ],
      "excerpt": ":#:#:#: PREPARING RAW TEXT #:#:#:#: \nfrom transformers import BertTokenizer \nfrom utils.character_cnn import CharacterIndexer \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.857129346874625
      ],
      "excerpt": "output = model(input_tensor)[0] \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/IMPLabUniPr/UniParma-at-semeval-2021-task-5/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Apache License 2.0",
      "url": "https://api.github.com/licenses/apache-2.0"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'Creative Commons Legal Code\\n\\nCC0 1.0 Universal\\n\\n    CREATIVE COMMONS CORPORATION IS NOT A LAW FIRM AND DOES NOT PROVIDE\\n    LEGAL SERVICES. DISTRIBUTION OF THIS DOCUMENT DOES NOT CREATE AN\\n    ATTORNEY-CLIENT RELATIONSHIP. CREATIVE COMMONS PROVIDES THIS\\n    INFORMATION ON AN \"AS-IS\" BASIS. CREATIVE COMMONS MAKES NO WARRANTIES\\n    REGARDING THE USE OF THIS DOCUMENT OR THE INFORMATION OR WORKS\\n    PROVIDED HEREUNDER, AND DISCLAIMS LIABILITY FOR DAMAGES RESULTING FROM\\n    THE USE OF THIS DOCUMENT OR THE INFORMATION OR WORKS PROVIDED\\n    HEREUNDER.\\n\\nStatement of Purpose\\n\\nThe laws of most jurisdictions throughout the world automatically confer\\nexclusive Copyright and Related Rights (defined below) upon the creator\\nand subsequent owner(s) (each and all, an \"owner\") of an original work of\\nauthorship and/or a database (each, a \"Work\").\\n\\nCertain owners wish to permanently relinquish those rights to a Work for\\nthe purpose of contributing to a commons of creative, cultural and\\nscientific works (\"Commons\") that the public can reliably and without fear\\nof later claims of infringement build upon, modify, incorporate in other\\nworks, reuse and redistribute as freely as possible in any form whatsoever\\nand for any purposes, including without limitation commercial purposes.\\nThese owners may contribute to the Commons to promote the ideal of a free\\nculture and the further production of creative, cultural and scientific\\nworks, or to gain reputation or greater distribution for their Work in\\npart through the use and efforts of others.\\n\\nFor these and/or other purposes and motivations, and without any\\nexpectation of additional consideration or compensation, the person\\nassociating CC0 with a Work (the \"Affirmer\"), to the extent that he or she\\nis an owner of Copyright and Related Rights in the Work, voluntarily\\nelects to apply CC0 to the Work and publicly distribute the Work under its\\nterms, with knowledge of his or her Copyright and Related Rights in the\\nWork and the meaning and intended legal effect of CC0 on those rights.\\n\\n1. Copyright and Related Rights. A Work made available under CC0 may be\\nprotected by copyright and related or neighboring rights (\"Copyright and\\nRelated Rights\"). Copyright and Related Rights include, but are not\\nlimited to, the following:\\n\\n  i. the right to reproduce, adapt, distribute, perform, display,\\n     communicate, and translate a Work;\\n ii. moral rights retained by the original author(s) and/or performer(s);\\niii. publicity and privacy rights pertaining to a person\\'s image or\\n     likeness depicted in a Work;\\n iv. rights protecting against unfair competition in regards to a Work,\\n     subject to the limitations in paragraph 4(a), below;\\n  v. rights protecting the extraction, dissemination, use and reuse of data\\n     in a Work;\\n vi. database rights (such as those arising under Directive 96/9/EC of the\\n     European Parliament and of the Council of 11 March 1996 on the legal\\n     protection of databases, and under any national implementation\\n     thereof, including any amended or successor version of such\\n     directive); and\\nvii. other similar, equivalent or corresponding rights throughout the\\n     world based on applicable law or treaty, and any national\\n     implementations thereof.\\n\\n2. Waiver. To the greatest extent permitted by, but not in contravention\\nof, applicable law, Affirmer hereby overtly, fully, permanently,\\nirrevocably and unconditionally waives, abandons, and surrenders all of\\nAffirmer\\'s Copyright and Related Rights and associated claims and causes\\nof action, whether now known or unknown (including existing as well as\\nfuture claims and causes of action), in the Work (i) in all territories\\nworldwide, (ii) for the maximum duration provided by applicable law or\\ntreaty (including future time extensions), (iii) in any current or future\\nmedium and for any number of copies, and (iv) for any purpose whatsoever,\\nincluding without limitation commercial, advertising or promotional\\npurposes (the \"Waiver\"). Affirmer makes the Waiver for the benefit of each\\nmember of the public at large and to the detriment of Affirmer\\'s heirs and\\nsuccessors, fully intending that such Waiver shall not be subject to\\nrevocation, rescission, cancellation, termination, or any other legal or\\nequitable action to disrupt the quiet enjoyment of the Work by the public\\nas contemplated by Affirmer\\'s express Statement of Purpose.\\n\\n3. Public License Fallback. Should any part of the Waiver for any reason\\nbe judged legally invalid or ineffective under applicable law, then the\\nWaiver shall be preserved to the maximum extent permitted taking into\\naccount Affirmer\\'s express Statement of Purpose. In addition, to the\\nextent the Waiver is so judged Affirmer hereby grants to each affected\\nperson a royalty-free, non transferable, non sublicensable, non exclusive,\\nirrevocable and unconditional license to exercise Affirmer\\'s Copyright and\\nRelated Rights in the Work (i) in all territories worldwide, (ii) for the\\nmaximum duration provided by applicable law or treaty (including future\\ntime extensions), (iii) in any current or future medium and for any number\\nof copies, and (iv) for any purpose whatsoever, including without\\nlimitation commercial, advertising or promotional purposes (the\\n\"License\"). The License shall be deemed effective as of the date CC0 was\\napplied by Affirmer to the Work. Should any part of the License for any\\nreason be judged legally invalid or ineffective under applicable law, such\\npartial invalidity or ineffectiveness shall not invalidate the remainder\\nof the License, and in such case Affirmer hereby affirms that he or she\\nwill not (i) exercise any of his or her remaining Copyright and Related\\nRights in the Work or (ii) assert any associated claims and causes of\\naction with respect to the Work, in either case contrary to Affirmer\\'s\\nexpress Statement of Purpose.\\n\\n4. Limitations and Disclaimers.\\n\\n a. No trademark or patent rights held by Affirmer are waived, abandoned,\\n    surrendered, licensed or otherwise affected by this document.\\n b. Affirmer offers the Work as-is and makes no representations or\\n    warranties of any kind concerning the Work, express, implied,\\n    statutory or otherwise, including without limitation warranties of\\n    title, merchantability, fitness for a particular purpose, non\\n    infringement, or the absence of latent or other defects, accuracy, or\\n    the present or absence of errors, whether or not discoverable, all to\\n    the greatest extent permissible under applicable law.\\n c. Affirmer disclaims responsibility for clearing rights of other persons\\n    that may apply to the Work or any use thereof, including without\\n    limitation any person\\'s Copyright and Related Rights in the Work.\\n    Further, Affirmer disclaims responsibility for obtaining any necessary\\n    consents, permissions or other rights required for any use of the\\n    Work.\\n d. Affirmer understands and acknowledges that Creative Commons is not a\\n    party to this document and has no duty or obligation with respect to\\n    this CC0 or use of the Work.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "UniParma at SemEval-2021 Task 5: Toxic Spans Detection Using CharacterBERT and Bag-of-Words Model",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "UniParma-at-semeval-2021-task-5",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "IMPLabUniPr",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/IMPLabUniPr/UniParma-at-semeval-2021-task-5/blob/main/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "In order to use GPUs you will need to make sure the PyTorch version that is in your conda environment matches your machine's configuration. To do that, you may want to run a few tests.\n\nLet's assume you want to use the GPU n\u00b00 on your machine. Then set:\n\n```bash\nexport CUDA_VISIBLE_DEVICES=0\n```\n\nAnd run these commands to check whether pytorch can detect your GPU:\n\n```python\nimport torch\nprint(torch.cuda.is_available())  #: Should return `True`\n```\n\nIf the last command returns `False`, then there is probably a mismatch between the installed PyTorch version and your machine's configuration. To fix that, run `nvidia-smi` in your terminal and check your driver version:\n\n<center><img src=\"img/nvidiasmi.png\" alt=\"drawing\" width=\"550\"/></center>\n\nThen compare this version with the numbers given in the [NVIDIA CUDA Toolkit Release Notes](https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html):\n\n<center><img src=\"img/cudaversions.png\" alt=\"drawing\" width=\"800\"/></center>\n\nIn this example the shown version is `390.116` which corresponds to `CUDA 9.0`. This means that the appropriate command for installing PyTorch is:\n\n```bash\nconda install pytorch torchvision cudatoolkit=9.0 -c pytorch\n```\n\nNow, everything should work fine!\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sat, 25 Dec 2021 13:35:03 GMT"
    },
    "technique": "GitHub API"
  }
}