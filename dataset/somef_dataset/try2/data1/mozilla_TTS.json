{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- https://github.com/keithito/tacotron (Dataset pre-processing)\n- https://github.com/r9y9/tacotron_pytorch (Initial Tacotron architecture)\n- https://github.com/kan-bayashi/ParallelWaveGAN (vocoder library)\n- https://github.com/jaywalnut310/glow-tts (Original Glow-TTS implementation)\n- https://github.com/fatchord/WaveRNN/ (Original WaveRNN implementation)\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1703.10135",
      "https://arxiv.org/abs/1712.05884",
      "https://arxiv.org/abs/2005.11129",
      "https://arxiv.org/abs/2008.03802",
      "https://arxiv.org/abs/1710.08969",
      "https://arxiv.org/abs/1907.09006",
      "https://arxiv.org/abs/1907.09006",
      "https://arxiv.org/abs/1710.10467",
      "https://arxiv.org/abs/1910.06711",
      "https://arxiv.org/abs/2005.05106",
      "https://arxiv.org/abs/1910.11480",
      "https://arxiv.org/abs/1909.11646",
      "https://arxiv.org/abs/2009.00713"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8090016440670298
      ],
      "excerpt": "| \ud83d\udea8 Bug Reports              | GitHub Issue Tracker                  | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8656070203791273
      ],
      "excerpt": "| \ud83c\udf81 Feature Requests & Ideas | GitHub Issue Tracker                  | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9353799258965515
      ],
      "excerpt": "Efficient Multi-GPUs training. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8109194328925066
      ],
      "excerpt": "Guided Attention: paper \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9778940392225741,
        0.8714162992508173
      ],
      "excerpt": "[x] Adapting Neural Vocoder. TTS works with WaveRNN and ParallelWaveGAN (https://github.com/erogol/WaveRNN and https://github.com/erogol/ParallelWaveGAN) \n[x] Multi-speaker embedding. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeOfConduct": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://raw.githubusercontent.com/mozilla/TTS/master/CODE_OF_CONDUCT.md",
    "technique": "File Exploration"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mozilla/TTS",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contribution guidelines\nThis repository is governed by Mozilla's code of conduct and etiquette guidelines. For more details, please read the Mozilla Community Participation Guidelines.\nBefore making a Pull Request, check your changes for basic mistakes and style problems by using a linter. We have cardboardlinter setup in this repository, so for example, if you've made some changes and would like to run the linter on just the differences between your work and master, you can use the follow command:\nbash\npip install pylint cardboardlint\ncardboardlinter --refspec master\nThis will compare the code against master and run the linter on all the changes. To run it automatically as a git pre-commit hook, you can do do the following:\n```bash\ncat <<\\EOF > .git/hooks/pre-commit\n!/bin/bash\nif [ ! -x \"$(command -v cardboardlinter)\" ]; then\n    exit 0\nfi\nFirst, stash index and work dir, keeping only the\nto-be-committed changes in the working directory.\necho \"Stashing working tree changes...\" 1>&2\nold_stash=$(git rev-parse -q --verify refs/stash)\ngit stash save -q --keep-index\nnew_stash=$(git rev-parse -q --verify refs/stash)\nIf there were no changes (e.g., --amend or --allow-empty)\nthen nothing was stashed, and we should skip everything,\nincluding the tests themselves.  (Presumably the tests passed\non the previous commit, so there is no need to re-run them.)\nif [ \"$old_stash\" = \"$new_stash\" ]; then\n    echo \"No changes, skipping lint.\" 1>&2\n    exit 0\nfi\nRun tests\ncardboardlinter --refspec HEAD -n auto\nstatus=$?\nRestore changes\necho \"Restoring working tree changes...\" 1>&2\ngit reset --hard -q && git stash apply --index -q && git stash drop -q\nExit with status from test-run: nonzero prevents commit\nexit $status\nEOF\nchmod +x .git/hooks/pre-commit\n```\nThis will run the linters on just the changes made in your commit.",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-01-23T14:22:06Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-28T09:49:38Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8980692969008903,
        0.8922884354589178,
        0.908925214220865
      ],
      "excerpt": "TTS is a library for advanced Text-to-Speech generation. It's built on the latest research, was designed to achieve the best trade-off among ease-of-training, speed and quality. \nTTS comes with pretrained models, tools for measuring dataset quality and already used in 20+ languages for products and research projects. \n:loudspeaker: English Voice Samples and SoundCloud playlist \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8413312631448417,
        0.9170819819777659
      ],
      "excerpt": ":page_facing_up: Text-to-Speech paper collection \nPlease use our dedicated channels for questions and discussion. Help is much more valuable if it's shared publicly, so that more people can benefit from it. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9273506875485769
      ],
      "excerpt": "| \ud83d\udcbb Docker Image            | Repository by @synesthesiam| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9693233393948762
      ],
      "excerpt": "| \u2728 How to contribute       |TTS/README.md| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.969656451689188
      ],
      "excerpt": "\"Mozilla\" and \"Judy\" are our models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9233262493220061
      ],
      "excerpt": "High performance Deep Learning models for Text2Speech tasks. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9742497168468732,
        0.8405144520661554,
        0.9723677095288299,
        0.8764832697377851
      ],
      "excerpt": "Ability to convert PyTorch models to Tensorflow 2.0 and TFLite for inference. \nReleased models in PyTorch, Tensorflow and TFLite. \nTools to curate Text2Speech datasets underdataset_analysis. \nDemo server for model testing. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8666509257081465
      ],
      "excerpt": "Modular (but not too much) code base enabling easy testing for new ideas. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.822712182650128
      ],
      "excerpt": "You can also help us implement more models. Some TTS related work can be found here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8964291686728832
      ],
      "excerpt": "    |- tts/             (text to speech models) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8477678735851494,
        0.8824411505570533
      ],
      "excerpt": "        |- models/          (model definitions) \n        |- tf/              (Tensorflow 2 utilities and model implementations) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9254739592030977
      ],
      "excerpt": "\"Recent research at Harvard has shown meditating for as little as 8 weeks can actually increase the grey matter in the parts of the brain responsible for emotional regulation and learning.\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.971769527482334
      ],
      "excerpt": "Some of the public datasets that we successfully applied TTS: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9903488324506811,
        0.8079740854107164
      ],
      "excerpt": "This repository is governed by Mozilla's code of conduct and etiquette guidelines. For more details, please read the Mozilla Community Participation Guidelines. \nCreate a new branch. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9511459085763748
      ],
      "excerpt": "Send a PR to dev branch, explain what the change is about. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8570280094806256,
        0.8928218797133634,
        0.9302287581602335
      ],
      "excerpt": "We merge it to the dev branch once things look good.  \nFeel free to ping us at any step you need help using our communication channels. \n[x] Implement the model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": ":robot: :speech_balloon: Deep learning for Text to Speech  (Discussion forum: https://discourse.mozilla.org/c/tts)",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mozilla/TTS/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 892,
      "date": "Tue, 28 Dec 2021 11:17:36 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/mozilla/TTS/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "mozilla/TTS",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/mozilla/TTS/master/notebooks/GE2E-Speaker_Encoder-%20ExtractSpeakerEmbeddings-by-sample.ipynb",
      "https://raw.githubusercontent.com/mozilla/TTS/master/notebooks/DDC_TTS_and_MultiBand_MelGAN_TF_Example.ipynb",
      "https://raw.githubusercontent.com/mozilla/TTS/master/notebooks/PlotUmapLibriTTS.ipynb",
      "https://raw.githubusercontent.com/mozilla/TTS/master/notebooks/Tutorial_Converting_PyTorch_to_TF_to_TFlite.ipynb",
      "https://raw.githubusercontent.com/mozilla/TTS/master/notebooks/TestAttention.ipynb",
      "https://raw.githubusercontent.com/mozilla/TTS/master/notebooks/DDC_TTS_and_ParallelWaveGAN_Example.ipynb",
      "https://raw.githubusercontent.com/mozilla/TTS/master/notebooks/DDC_TTS_and_MultiBand_MelGAN_TFLite_Example.ipynb",
      "https://raw.githubusercontent.com/mozilla/TTS/master/notebooks/AngleProto-Speaker_Encoder-%20ExtractSpeakerEmbeddings-by-sample.ipynb",
      "https://raw.githubusercontent.com/mozilla/TTS/master/notebooks/DDC_TTS_and_MultiBand_MelGAN_Example.ipynb",
      "https://raw.githubusercontent.com/mozilla/TTS/master/notebooks/Demo_Mozilla_TTS_MultiSpeaker_jia_et_al_2018.ipynb",
      "https://raw.githubusercontent.com/mozilla/TTS/master/notebooks/GE2E-CorentinJ-ExtractSpeakerEmbeddings-by-sample.ipynb",
      "https://raw.githubusercontent.com/mozilla/TTS/master/notebooks/Demo_Mozilla_TTS_MultiSpeaker_jia_et_al_2018_With_GST.ipynb",
      "https://raw.githubusercontent.com/mozilla/TTS/master/notebooks/ExtractTTSpectrogram.ipynb",
      "https://raw.githubusercontent.com/mozilla/TTS/master/notebooks/dataset_analysis/CheckDatasetSNR.ipynb",
      "https://raw.githubusercontent.com/mozilla/TTS/master/notebooks/dataset_analysis/PhonemeCoverage.ipynb",
      "https://raw.githubusercontent.com/mozilla/TTS/master/notebooks/dataset_analysis/CheckSpectrograms.ipynb",
      "https://raw.githubusercontent.com/mozilla/TTS/master/notebooks/dataset_analysis/AnalyzeDataset-Copy1.ipynb",
      "https://raw.githubusercontent.com/mozilla/TTS/master/notebooks/dataset_analysis/AnalyzeDataset.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/mozilla/TTS/master/run_tests.sh",
      "https://raw.githubusercontent.com/mozilla/TTS/master/tests/test_glow-tts_train.sh",
      "https://raw.githubusercontent.com/mozilla/TTS/master/tests/test_vocoder_wavegrad_train.sh",
      "https://raw.githubusercontent.com/mozilla/TTS/master/tests/test_server_package.sh",
      "https://raw.githubusercontent.com/mozilla/TTS/master/tests/test_tacotron_train.sh",
      "https://raw.githubusercontent.com/mozilla/TTS/master/tests/test_speedy_speech_train.sh",
      "https://raw.githubusercontent.com/mozilla/TTS/master/tests/test_vocoder_wavernn_train.sh",
      "https://raw.githubusercontent.com/mozilla/TTS/master/tests/test_vocoder_gan_train.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you like to use TTS to try a new idea and like to share your experiments with the community, we urge you to use the following guideline for a better collaboration.\n(If you have an idea for better collaboration, let us know)\n- Create a new branch.\n- Open an issue pointing your branch.\n- Explain your idea and experiment.\n- Share your results regularly. (Tensorboard log files, audio results, visuals etc.)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "TTS supports **python >= 3.6, <3.9**.\n\nIf you are only interested in [synthesizing speech](https://github.com/mozilla/TTS/tree/dev#example-synthesizing-speech-on-terminal-using-the-released-models) with the released TTS models, installing from PyPI is the easiest option.\n\n```bash\npip install TTS\n```\n\nIf you plan to code or train models, clone TTS and install it locally.\n\n```bash\ngit clone https://github.com/mozilla/TTS\npip install -e .\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8711444198174407
      ],
      "excerpt": "| \ud83d\udcbe Installation | TTS/README.md| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8188003401968722
      ],
      "excerpt": "Implement your changes. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.999746712887969
      ],
      "excerpt": "pip install pylint cardboardlint \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8048035211343507
      ],
      "excerpt": "| \ud83d\udc69\ud83c\udffe\u200d\ud83c\udfeb Tutorials and Examples  | TTS/Wiki | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9058405005910268,
        0.8923390729687973
      ],
      "excerpt": "      |- train*.py                  (train your target model.) \n      |- distribute.py              (train your TTS model using Multiple GPUs.) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8465696616018751,
        0.8481215394023953
      ],
      "excerpt": "      |- convert*.py                (convert target torch model to TF.) \n    |- tts/             (text to speech models) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8838148168639296,
        0.9379892500304304
      ],
      "excerpt": "Audio examples: soundcloud \n<img src=\"images/example_model_output.png?raw=true\" alt=\"example_output\" width=\"400\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8891319749909831
      ],
      "excerpt": "You just need to write a simple function to format the dataset. Check datasets/preprocess.py to see some examples. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.844511368524649,
        0.8362510396080425
      ],
      "excerpt": "(if applicable) Implement a test case under tests folder. \n(Optional but Prefered) Run tests.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8278121020599903
      ],
      "excerpt": "[x] Train TTS with r=1 successfully. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/mozilla/TTS/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python",
      "HTML",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Mozilla Public License 2.0",
      "url": "https://api.github.com/licenses/mpl-2.0"
    },
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "TTS: Text-to-Speech for all.",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "TTS",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "mozilla",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mozilla/TTS/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "erogol",
        "body": "This is the first and v0.0.9 release of TTS, an open text-to-speech engine. TTS is still an evolving project and any upcoming release might be significantly different and not backward compatible. \r\n\r\nIn this release, we provide the following models.\r\n\r\n| Language        |Dataset | Model Name| Model Type| Download|\r\n| ------------- |:------:|:-----------------:|-----------------:|----|\r\n|English | LJSpeech | TacotronDCA| tts|[\ud83d\udcbe](https://github.com/mozilla/TTS/releases/download/v0.0.9/tts_models--en--ljspeech--tacotron2-DCA.zip)|\r\n|English         | LJSpeech | Glow-TTS| tts|[\ud83d\udcbe](https://github.com/mozilla/TTS/releases/download/v0.0.9/tts_models--en--ljspeech--glow-tts.zip)|\r\n|Spanish        | M-AILabs| TacotronDDC | tts|[\ud83d\udcbe](https://github.com/mozilla/TTS/releases/download/v0.0.9/tts_models--es--mai--tacotron2-DDC.zip)|\r\n|French          |M_AILabs| TacotronDDC| tts|[\ud83d\udcbe](https://github.com/mozilla/TTS/releases/download/v0.0.9/tts_models--fr--mai--tacotron2-DDC.zip)|\r\n|English         | LJSpeech|  MB-MelGAN| vocoder|[\ud83d\udcbe](https://github.com/mozilla/TTS/releases/download/v0.0.9/vocoder_models--en--ljspeech--mulitband-melgan.zip)|\r\n|Multi-Lang  | LibriTTS| FullBand-MelGAN|  vocoder|[\ud83d\udcbe](https://github.com/mozilla/TTS/releases/download/v0.0.9/vocoder_models--universal--libri-tts--fullband-melgan.zip)|\r\n|Multi-Lang  | LibriTTS| WaveGrad| vocoder| [\ud83d\udcbe](https://github.com/mozilla/TTS/releases/download/v0.0.9/vocoder_models--universal--libri-tts--wavegrad.zip)|\r\n\r\n\r\n## Notes\r\n- Multi-Lang vocoder models are intended for non-English models. \r\n- Vocoder models are independently trained from the tts models with possibly different sampling rates. Therefore, the performance is not optimal.\r\n- All models are trained with phonemes generated by **espeak** back-end (**not espeak-ng**).\r\n\r\nThis release has been tested under Python 3.6, 3.7, and 3.8.  It is strongly suggested to use conda to install the dependencies and set-up the runtime environment. \r\n\r\n",
        "dateCreated": "2021-01-27T10:26:38Z",
        "datePublished": "2021-01-29T00:03:56Z",
        "html_url": "https://github.com/mozilla/TTS/releases/tag/v0.0.9",
        "name": "TTS v0.0.9 (first release)",
        "tag_name": "v0.0.9",
        "tarball_url": "https://api.github.com/repos/mozilla/TTS/tarball/v0.0.9",
        "url": "https://api.github.com/repos/mozilla/TTS/releases/37029753",
        "zipball_url": "https://api.github.com/repos/mozilla/TTS/zipball/v0.0.9"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 5469,
      "date": "Tue, 28 Dec 2021 11:17:36 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "deep-learning",
      "text-to-speech",
      "python",
      "pytorch",
      "tacotron",
      "tts",
      "speaker-encoder",
      "dataset-analysis",
      "tacotron2",
      "tensorflow2",
      "vocoder",
      "melgan",
      "gantts",
      "multiband-melgan",
      "glow-tts",
      "speech"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "After the installation, TTS provides a CLI interface for synthesizing speech using pre-trained models. You can either use your own model or the release models under the TTS project.\n\nListing released TTS models.\n```bash\ntts --list_models\n```\n\nRun a tts and a vocoder model from the released model list. (Simply copy and paste the full model names from the list as arguments for the command below.)\n```bash\ntts --text \"Text for TTS\" \\\n    --model_name \"<type>/<language>/<dataset>/<model_name>\" \\\n    --vocoder_name \"<type>/<language>/<dataset>/<model_name>\" \\\n    --out_path folder/to/save/output/\n```\n\nRun your own TTS model (Using Griffin-Lim Vocoder)\n```bash\ntts --text \"Text for TTS\" \\\n    --model_path path/to/model.pth.tar \\\n    --config_path path/to/config.json \\\n    --out_path output/path/speech.wav\n```\n\nRun your own TTS and Vocoder models\n```bash\ntts --text \"Text for TTS\" \\\n    --model_path path/to/config.json \\\n    --config_path path/to/model.pth.tar \\\n    --out_path output/path/speech.wav \\\n    --vocoder_path path/to/vocoder.pth.tar \\\n    --vocoder_config_path path/to/vocoder_config.json\n```\n\n**Note:** You can use ```./TTS/bin/synthesize.py``` if you prefer running ```tts``` from the TTS project folder.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Here you can find a [CoLab](https://gist.github.com/erogol/97516ad65b44dbddb8cd694953187c5b) notebook for a hands-on example, training LJSpeech. Or you can manually follow the guideline below.\n\nTo start with, split ```metadata.csv``` into train and validation subsets respectively ```metadata_train.csv``` and ```metadata_val.csv```. Note that for text-to-speech, validation performance might be misleading since the loss value does not directly measure the voice quality to the human ear and it also does not measure the attention module performance. Therefore, running the model with new sentences and listening to the results is the best way to go.\n\n```\nshuf metadata.csv > metadata_shuf.csv\nhead -n 12000 metadata_shuf.csv > metadata_train.csv\ntail -n 1100 metadata_shuf.csv > metadata_val.csv\n```\n\nTo train a new model, you need to define your own ```config.json``` to define model details, trainin configuration and more (check the examples). Then call the corressponding train script.\n\nFor instance, in order to train a tacotron or tacotron2 model on LJSpeech dataset, follow these steps.\n\n```bash\npython TTS/bin/train_tacotron.py --config_path TTS/tts/configs/config.json\n```\n\nTo fine-tune a model, use ```--restore_path```.\n\n```bash\npython TTS/bin/train_tacotron.py --config_path TTS/tts/configs/config.json --restore_path /path/to/your/model.pth.tar\n```\n\nTo continue an old training run, use ```--continue_path```.\n\n```bash\npython TTS/bin/train_tacotron.py --continue_path /path/to/your/run_folder/\n```\n\nFor multi-GPU training, call ```distribute.py```. It runs any provided train script in multi-GPU setting.\n\n```bash\nCUDA_VISIBLE_DEVICES=\"0,1,4\" python TTS/bin/distribute.py --script train_tacotron.py --config_path TTS/tts/configs/config.json\n```\n\nEach run creates a new output folder accomodating used ```config.json```, model checkpoints and tensorboard logs.\n\nIn case of any error or intercepted execution, if there is no checkpoint yet under the output folder, the whole folder is going to be removed.\n\nYou can also enjoy Tensorboard,  if you point Tensorboard argument```--logdir``` to the experiment folder.\n\n",
      "technique": "Header extraction"
    }
  ]
}