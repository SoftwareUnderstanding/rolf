{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2005.14165"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8497781681827675,
        0.9257920213413926
      ],
      "excerpt": "{\"src\": \"www.nvidia.com\", \"text\": \"The quick brown fox\", \"type\": \"Eng\", \"id\": \"0\", \"title\": \"First Part\"} \n{\"src\": \"The Internet\", \"text\": \"jumps over the lazy dog\", \"type\": \"Eng\", \"id\": \"42\", \"title\": \"Second Part\"} \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8550101043698384
      ],
      "excerpt": "    --log-interval 10 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "    --eval-iters 10 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8550101043698384
      ],
      "excerpt": "    --log-interval 10 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "    --eval-iters 10 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "    \"initial_scale_power\": 12 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8550101043698384
      ],
      "excerpt": "    --log-interval 10 \\ \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/bigscience-workshop/Megatron-DeepSpeed",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-07-02T17:40:35Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-30T06:41:06Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8727379211821662,
        0.9798893376520016,
        0.8318775917125318,
        0.9840356124664347
      ],
      "excerpt": "This is a detached fork of https://github.com/microsoft/Megatron-DeepSpeed, which in itself is a fork of https://github.com/NVIDIA/Megatron-LM. The former integrates DeepSpeed into the original Megatron-LM code. \nThis fork in turn will include direct changes to the models needed for the BigScience project. This is the repo we use for this project. \nIn addition various code bits and lots of docs are to be found at https://github.com/bigscience-workshop/bigscience. \nPlease note that the rest of this page has been trimmed to only include the info relevant to the BigScience project and also updated to usage with the integrated Deepspeed. You will find the original page with all the tables and training info on Bert and T5 here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8554578620382394,
        0.8255154692892828,
        0.8731156887520776
      ],
      "excerpt": "The name of the text field of the json can be changed by using the --json-key flag in preprocess_data.py The other metadata are optional and are not used in training. \nThe loose json is then processed into a binary format for training. To convert the json into mmap, cached index file, or the lazy loader format use preprocess_data.py. Set the --dataset-impl flag to mmap, cached, or lazy, respectively (default is mmap). \nAn example script to prepare data for GPT training is: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9182251662278886
      ],
      "excerpt": "Sometimes it's hard to work on a very large dataset at once, so one can pre-process it in chunks and then merge those datasets into a single combined indexed dataset. Here is an example: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8180358503579312,
        0.8255565616365096,
        0.81486579441979,
        0.898875088135476
      ],
      "excerpt": "The examples/pretrain_gpt.sh script runs single GPU 345M parameter GPT pretraining. Debugging is the primary use for single GPU training, as the code base and command line arguments are optimized for highly distributed training. Most of the arguments are fairly self-explanatory. By default, the learning rate decays linearly over the training iterations starting at --lr to a minimum set by --min-lr over --lr-decay-iters iterations. The fraction of training iterations used for warmup is set by --lr-warmup-fraction. While this is single GPU training, the batch size specified by --micro-batch-size is a single forward-backward path batch-size and the code will perform gradient accumulation steps until it reaches global-batch-size whcih is the batch size per iteration. \nThe data is partitioned into a 949:50:1 ratio for training/validation/test sets (default is 969:30:1). This partitioning happens on the fly, but is consistent across runs with the same random seed (1234 by default, or specified manually with --seed). We use train-iters as the training iterations requested. Alternatively, one can provide --train-samples which is total number of samples to train on. If this option is present, then instead of providing --lr-decay-iters, one will need to provide --lr-decay-samples. \nThe logging, checkpoint-saving, and evaluation intervals are specified. Checkpointing the activations facilitates the training of larger models and/or batches. Note that the --data-path now includes the additional _text_sentence suffix added in preprocessing, but does not include the file extensions. \nThe tokenization scheme used is BPE (which requires a merge table and a json vocabulary file), the model architecture allows for longer sequences (note that the max position embedding must be greater than or equal to the maximum sequence length), and the --lr-decay-style has been set to cosine decay.  Note that the --data-path now includes the additional _text_document suffix added in preprocessing, but does not include the file extensions. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9753725353468504
      ],
      "excerpt": "For a single GPU the other approach is to emulate distributed with: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9064755090859743
      ],
      "excerpt": "To allow further flexibility we are using Deepspeed PP (pipeline parallelism) and ZeRO-DP along with Megatron normal functionality. That is we replace Megatron's PP with Deepspeed's PP, and we use ZERO-DP for DP. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8953165658079213
      ],
      "excerpt": "on JZ we use a different launching command, see for example the end of  tr1-13B-round1.slurm, but this is also a good fully functional script that you can use. Except it's written for SLURM environment. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9178722649850796,
        0.9949982693522451,
        0.9787969033862918,
        0.952372229471308,
        0.9724110317574541,
        0.9145508039552991
      ],
      "excerpt": "The examples/pretrain_{bert,gpt,t5}_distributed.sh scripts use the PyTorch distributed launcher for distributed training. As such, multi-node training can be achieved by properly setting environment variables and using init_method='env://' in the launcher. See the official PyTorch documentation for further description of these environment variables. By default, multi-node training uses the nccl distributed backend. A simple set of additional arguments and the use of the PyTorch distributed module with the Python flag -m torch.distributed.launch, detailed below, are the only additional requirements to adopt distributed training. \nWe use two types of parallelism: data and model parallelism. We facilitate two distributed data parallel implementations: a simple one of our own that performs gradient all-reduce at the end of back propagation step, and Torch's distributed data parallel wrapper that overlaps gradient reduction with back propagation computation. To switch between these two options use --DDP-impl local or --DDP-impl torch, respectively. As expected, Torch distributed data parallelism is more efficient at larger model sizes. For example, for the 8.3 billion parameters model running on 512 GPUs, the scaling increases from 60% to 76% when Torch's distributed data parallel is used. However, the overlapping method requires more memory and for some configurations (e.g., 2.5 billion parameters using 2-way model parallel and 1.2 billion parameters with no model parallel) can make the overall training slower as a result. We empirically found that using a smaller model in those cases improves the training time. \nSecond, we developed a simple and efficient two-dimensional model-parallel approach. To use tensor model parallelism (splitting execution of a single transformer module over multiple GPUs), add the --tensor-model-parallel-size flag to specify the number of GPUs among which to split the model, along with the arguments passed to the distributed launcher as mentioned above. To use pipeline model parallelism (sharding the transformer modules into stages with an equal number of transformer modules on each stage, and then pipelining execution by breaking the batch into smaller microbatches), use the --pipeline-model-parallel-size flag to specify the number of stages to split the model into (e.g., splitting a model with 24 transformer layers across 4 stages would mean each stage gets 6 transformer layers each). \n<!-- The number of microbatches in a per-pipeline minibatch is controlled by the `--num-microbatches-in-minibatch` argument. With `WORLD_SIZE` GPUs, `TENSOR_MP_SIZE` tensor-model-parallel size, `PIPELINE_MP_SIZE` pipeline-model-parallel-size, `WORLD_SIZE`/(`TENSOR_MP_SIZE` * `PIPELINE_MP_SIZE`) GPUs will be used for data parallelism. The default values for `--tensor-model-parallel-size` and `--pipeline-model-parallel-size` is 1, which will not implement either form of model parallelism. --> \nWe have examples of how to use these two different forms of model parallelism the example scripts ending in distributed_with_mp.sh, note that pipeline parallelism is not currently supported in the T5 model: \nOther than these minor changes, the distributed training is identical to the training on a single GPU. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9239134225427167
      ],
      "excerpt": "see the details on how to do distributed training with the deepspeed launcher a few sections up \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8509615822574821,
        0.964001735176473
      ],
      "excerpt": "We provide several command line arguments, detailed in the scripts listed below, to handle various zero-shot and fine-tuned downstream tasks. However, you can also finetune your model from a pretrained checkpoint on other corpora as desired. To do so, simply add the --finetune flag and adjust the input files and training parameters within the original training script. The iteration count will be reset to zero, and the optimizer and internal state will be reinitialized. If the fine-tuning is interrupted for any reason, be sure to remove the --finetune flag before continuing, otherwise the training will start again from the beginning. \nBecause evaluation requires substantially less memory than training, it may be advantageous to merge a model trained in parallel for use on a single GPU in downstream tasks. The following script accomplishes this. Currently only tensor model parallelism is supported on input and pipeline model parallelsim on the output. This example reads in a model with 2-way tensor model parallelism and writes out a model with 2-way pipeline model parallelism. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8511732668433236
      ],
      "excerpt": "Several downstream tasks are described for both GPT and BERT models below. They can be run in distributed and model parallel modes with the same changes used in the training scripts. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9561309180291553
      ],
      "excerpt": "For even comparison with prior works, we evaluate perplexity on the word-level WikiText-103 test dataset, and appropriately compute perplexity given the change in tokens when using our subword tokenizer. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "    --valid-data $VALID_DATA \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.95981535398063,
        0.9813199792438686
      ],
      "excerpt": "Currently the test suite is not yet plugged into CI and needs to be run manually. For more details please see Testing. \nThis is a community project and we would love to have your help. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Ongoing research training transformer language models at scale, including: BERT & GPT-2",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/bigscience-workshop/Megatron-DeepSpeed/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 28,
      "date": "Thu, 30 Dec 2021 07:55:05 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/bigscience-workshop/Megatron-DeepSpeed/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "bigscience-workshop/Megatron-DeepSpeed",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/bigscience-workshop/Megatron-DeepSpeed/main/run.sh",
      "https://raw.githubusercontent.com/bigscience-workshop/Megatron-DeepSpeed/main/scripts/test_multiple_dataset_sampling/test_sampling.sh",
      "https://raw.githubusercontent.com/bigscience-workshop/Megatron-DeepSpeed/main/megatron/data/test/test_preprocess_data.sh",
      "https://raw.githubusercontent.com/bigscience-workshop/Megatron-DeepSpeed/main/examples/pretrain_gpt_distributed_with_mp.sh",
      "https://raw.githubusercontent.com/bigscience-workshop/Megatron-DeepSpeed/main/examples/create_embeddings.sh",
      "https://raw.githubusercontent.com/bigscience-workshop/Megatron-DeepSpeed/main/examples/pretrain_t5_distributed_with_mp.sh",
      "https://raw.githubusercontent.com/bigscience-workshop/Megatron-DeepSpeed/main/examples/generate_text.sh",
      "https://raw.githubusercontent.com/bigscience-workshop/Megatron-DeepSpeed/main/examples/finetune_mnli_distributed.sh",
      "https://raw.githubusercontent.com/bigscience-workshop/Megatron-DeepSpeed/main/examples/pretrain_gpt_multilingual.sh",
      "https://raw.githubusercontent.com/bigscience-workshop/Megatron-DeepSpeed/main/examples/merge_mp_bert.sh",
      "https://raw.githubusercontent.com/bigscience-workshop/Megatron-DeepSpeed/main/examples/pretrain_gpt_distributed.sh",
      "https://raw.githubusercontent.com/bigscience-workshop/Megatron-DeepSpeed/main/examples/evaluate_zeroshot_gpt.sh",
      "https://raw.githubusercontent.com/bigscience-workshop/Megatron-DeepSpeed/main/examples/pretrain_gpt_single_node.sh",
      "https://raw.githubusercontent.com/bigscience-workshop/Megatron-DeepSpeed/main/examples/pretrain_t5.sh",
      "https://raw.githubusercontent.com/bigscience-workshop/Megatron-DeepSpeed/main/examples/pretrain_t5_distributed.sh",
      "https://raw.githubusercontent.com/bigscience-workshop/Megatron-DeepSpeed/main/examples/pretrain_gpt.sh",
      "https://raw.githubusercontent.com/bigscience-workshop/Megatron-DeepSpeed/main/examples/pretrain_gpt_tiny.sh",
      "https://raw.githubusercontent.com/bigscience-workshop/Megatron-DeepSpeed/main/examples/pretrain_bert_distributed.sh",
      "https://raw.githubusercontent.com/bigscience-workshop/Megatron-DeepSpeed/main/examples/pretrain_gpt3_175B.sh",
      "https://raw.githubusercontent.com/bigscience-workshop/Megatron-DeepSpeed/main/examples/pretrain_bert_distributed_with_mp.sh",
      "https://raw.githubusercontent.com/bigscience-workshop/Megatron-DeepSpeed/main/examples/evaluate_ict_zeroshot_nq.sh",
      "https://raw.githubusercontent.com/bigscience-workshop/Megatron-DeepSpeed/main/examples/pretrain_ict.sh",
      "https://raw.githubusercontent.com/bigscience-workshop/Megatron-DeepSpeed/main/examples/finetune_race_distributed.sh",
      "https://raw.githubusercontent.com/bigscience-workshop/Megatron-DeepSpeed/main/examples/pretrain_bert.sh",
      "https://raw.githubusercontent.com/bigscience-workshop/Megatron-DeepSpeed/main/examples/curriculum_learning/pretrain_gpt_cl.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Install `bigscience-workshop/Megatron-DeepSpeed`\n```\ngit clone https://github.com/bigscience-workshop/Megatron-DeepSpeed\ncd Megatron-DeepSpeed\npip install -r requirements.txt\n```\n\nYou can now use this repo directly by working directly from it. You don't need to install it unless you write your own scripts elsewhere that use the modules in this repo, in which case you may want to do:\n\n```\npip install -e .\n```\n\n2. Install `apex`\n\n```\ngit clone https://github.com/NVIDIA/apex\ncd apex\npip install --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" --no-cache -v --disable-pip-version-check .  2>&1 | tee build.log\n```\n\n(on JZ it's done in a special way, see [here](https://github.com/bigscience-workshop/bigscience/tree/master/jz/envs#apex).)\n\n3. Install `deepspeed` / the `big-science` branch\n\nThen install the `big-science` branch of `deepspeed`:\n\n```\ngit clone https://github.com/microsoft/deepspeed deepspeed-big-science\ncd deepspeed-big-science\ngit checkout big-science\nrm -rf build\nTORCH_CUDA_ARCH_LIST=\"7.0\" DS_BUILD_CPU_ADAM=1 DS_BUILD_AIO=1 DS_BUILD_UTILS=1 pip install -e . --global-option=\"build_ext\" --global-option=\"-j8\" --no-cache -v --disable-pip-version-check\n```\n\nadjust `TORCH_CUDA_ARCH_LIST=\"7.0\"` to the architecture of your NVIDIA GPU (or just remove it altogether if you are not sure how to find one).\n\n(on JZ it's done in a special way, see [here](https://github.com/bigscience-workshop/bigscience/tree/master/jz/envs#deepspeed).)\n\n\n3. CUDA kernels compilation\n\nThe first time you run the training scripts several CUDA kernels will be compiled. Which means you need to have a cuda environment set up in your environment and it should match the version pytorch was built with.\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.857914242723079
      ],
      "excerpt": "    --output-prefix my-gpt2 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.857914242723079
      ],
      "excerpt": "    --output-prefix meg-gpt2_oscar_text_document \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9198057118673182
      ],
      "excerpt": "However, as you will see below you will learn that DeepSpeed requires a distributed enviroment even with a single GPU. Therefore, instead refer to pretrain_gpt_single_node.sh, which will work with this repo. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.87175842795213
      ],
      "excerpt": "Note, we replaced python with deepspeed --num_gpus 1. For multi-gpu training update --num_gpus to the number of GPUs you have. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8353727207163972
      ],
      "excerpt": "bash examples/generate_text.sh \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9728691627597797
      ],
      "excerpt": "The training data requires preprocessing. First, place your training data in a loose json format, with one json containing a text sample per line. For example: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8055548269595095,
        0.874435098235437
      ],
      "excerpt": "The name of the text field of the json can be changed by using the --json-key flag in preprocess_data.py The other metadata are optional and are not used in training. \nThe loose json is then processed into a binary format for training. To convert the json into mmap, cached index file, or the lazy loader format use preprocess_data.py. Set the --dataset-impl flag to mmap, cached, or lazy, respectively (default is mmap). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.828344218455681,
        0.8150321485428955
      ],
      "excerpt": "python tools/preprocess_data.py \\ \n    --input my-corpus.json \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8399726267268325
      ],
      "excerpt": "    --merge-file gpt2-merges.txt \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8407225774071283,
        0.8021114185340723
      ],
      "excerpt": "The output will be two files named, in this case, my-gpt2_text_document.bin and my-gpt2_text_document.idx. The --data-path specified in later GPT training is the full path and new filename, but without the file extension. \nFurther command line arguments are described in the source file preprocess_data.py. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.828344218455681
      ],
      "excerpt": "python tools/merge_preprocessed_data.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8517196249322844
      ],
      "excerpt": "The data is partitioned into a 949:50:1 ratio for training/validation/test sets (default is 969:30:1). This partitioning happens on the fly, but is consistent across runs with the same random seed (1234 by default, or specified manually with --seed). We use train-iters as the training iterations requested. Alternatively, one can provide --train-samples which is total number of samples to train on. If this option is present, then instead of providing --lr-decay-iters, one will need to provide --lr-decay-samples. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8029652574196914
      ],
      "excerpt": "    --train-iters 500000 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8621882743635256,
        0.8621882743635256,
        0.8612630503664754
      ],
      "excerpt": "    --vocab-file $VOCAB_FILE \\ \n    --merge-file $MERGE_FILE \\ \n    --lr-warmup-fraction .01 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.802743024213556
      ],
      "excerpt": "    --eval-interval 100 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991
      ],
      "excerpt": "CMD=\"pretrain_gpt.py $GPT_ARGS $OUTPUT_ARGS $DATA_ARGS\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8335053466139701
      ],
      "excerpt": "MASTER_ADDR=perl -le '$_=$ENV{\"SLURM_JOB_NODELIST\"}; s/,.*//; s/-.*//; s/\\[//; print' \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8969427959835548,
        0.8057218560524222
      ],
      "excerpt": "MASTER_ADDR=localhost MASTER_PORT=9994 RANK=0 LOCAL_RANK=0 python pretrain_gpt.py ... \nFurther command line arguments are described in the source file arguments.py. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8560082273874543
      ],
      "excerpt": "#:    --train-samples 10_000 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8835532006929613
      ],
      "excerpt": "    --train-samples 100 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8621882743635256,
        0.8621882743635256
      ],
      "excerpt": "    --vocab-file $VOCAB_FILE \\ \n    --merge-file $MERGE_FILE \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8029652574196914
      ],
      "excerpt": "#:    --train-iters 500 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.802743024213556
      ],
      "excerpt": "    --eval-interval 100 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991
      ],
      "excerpt": "    $LAUNCHER pretrain_gpt.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8717211346471622
      ],
      "excerpt": "Thanks to @sbmaruf, any HF pretrained tokenizer may be used instead of the Megatron-provided BERT/GPT/T5 tokenizers. You'll need to run preprocessing yourself (tools/preprocess_data.py), using tokenizer-type=PretrainedFromHF and tokenizer-name-or-path=&lt;your_tokenizer&gt;. For example, python tools/preprocess_data.py --input ~/c4_en_train.jsonl --output-prefix c4_en_train --dataset-impl mmap --tokenizer-type PretrainedFromHF --tokenizer-name-or-path t5-small --workers 30 --append-eod \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.828344218455681,
        0.8226763253166589
      ],
      "excerpt": "WORLD_SIZE=$TENSOR_MODEL_PARALLEL_SIZE python tools/merge_mp_partitions.py \\ \n    --model-type BERT \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8621882743635256
      ],
      "excerpt": "    --vocab-file $VOCAB_FILE \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8251269854030854,
        0.8239003742245112
      ],
      "excerpt": "bash examples/generate_text.sh \nWe generate text samples using largely the GPT pretraining script. Few changes need to make, such as we need to provide the path to the pretrained checkpoint, the length of the output samples, whether to generate texts unconditionally (--num-samples to denote how many samples to generate) or conditional (need to pass --sample-input-file &lt;filename&gt; where each line of the file will be used as the conditional texts). There are few optional parameters to play, e.g. top-k, top-p, or greedy (set top-k and top-p to 0) sampling.. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.828344218455681
      ],
      "excerpt": "python tools/generate_samples_gpt.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8554138574456002,
        0.8169917415460671
      ],
      "excerpt": "    --genfile $OUTPUT_FILE \\ \n    --num-samples $NUMBER_OF_SAMPLES \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8621882743635256,
        0.8860772515844052
      ],
      "excerpt": "    --vocab-file $VOCAB_FILE\" \npython tasks/main.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8621882743635256
      ],
      "excerpt": "    --merge-file $MERGE_FILE \\ \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/bigscience-workshop/Megatron-DeepSpeed/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "C++",
      "Cuda",
      "Shell",
      "C",
      "Makefile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other",
      "url": "https://raw.githubusercontent.com/bigscience-workshop/Megatron-DeepSpeed/main/LICENSE"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'The following applies to all files unless otherwise noted:\\n\\n# Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.\\n#\\n# Redistribution and use in source and binary forms, with or without\\n# modification, are permitted provided that the following conditions\\n# are met:\\n#  * Redistributions of source code must retain the above copyright\\n#    notice, this list of conditions and the following disclaimer.\\n#  * Redistributions in binary form must reproduce the above copyright\\n#    notice, this list of conditions and the following disclaimer in the\\n#    documentation and/or other materials provided with the distribution.\\n#  * Neither the name of NVIDIA CORPORATION nor the names of its\\n#    contributors may be used to endorse or promote products derived\\n#    from this software without specific prior written permission.\\n#\\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS\\'\\' AND ANY\\n# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\\n# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\\n# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\\n# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n\\n--\\n\\nThis repository also contains code from Hugging Face Inc., Google Research,\\nFacebook (from their Fairseq project), and Philip Popien. Files from these\\norganizations have notices at the top of each file. Below are licenses\\nused in those files, as indicated.\\n\\n\\n------------- LICENSE FOR huggingface and Google Research code  --------------\\n\\n\\n                                 Apache License\\n                           Version 2.0, January 2004\\n                        http://www.apache.org/licenses/\\n\\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\\n\\n   1. Definitions.\\n\\n      \"License\" shall mean the terms and conditions for use, reproduction,\\n      and distribution as defined by Sections 1 through 9 of this document.\\n\\n      \"Licensor\" shall mean the copyright owner or entity authorized by\\n      the copyright owner that is granting the License.\\n\\n      \"Legal Entity\" shall mean the union of the acting entity and all\\n      other entities that control, are controlled by, or are under common\\n      control with that entity. For the purposes of this definition,\\n      \"control\" means (i) the power, direct or indirect, to cause the\\n      direction or management of such entity, whether by contract or\\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\\n      outstanding shares, or (iii) beneficial ownership of such entity.\\n\\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\\n      exercising permissions granted by this License.\\n\\n      \"Source\" form shall mean the preferred form for making modifications,\\n      including but not limited to software source code, documentation\\n      source, and configuration files.\\n\\n      \"Object\" form shall mean any form resulting from mechanical\\n      transformation or translation of a Source form, including but\\n      not limited to compiled object code, generated documentation,\\n      and conversions to other media types.\\n\\n      \"Work\" shall mean the work of authorship, whether in Source or\\n      Object form, made available under the License, as indicated by a\\n      copyright notice that is included in or attached to the work\\n      (an example is provided in the Appendix below).\\n\\n      \"Derivative Works\" shall mean any work, whether in Source or Object\\n      form, that is based on (or derived from) the Work and for which the\\n      editorial revisions, annotations, elaborations, or other modifications\\n      represent, as a whole, an original work of authorship. For the purposes\\n      of this License, Derivative Works shall not include works that remain\\n      separable from, or merely link (or bind by name) to the interfaces of,\\n      the Work and Derivative Works thereof.\\n\\n      \"Contribution\" shall mean any work of authorship, including\\n      the original version of the Work and any modifications or additions\\n      to that Work or Derivative Works thereof, that is intentionally\\n      submitted to Licensor for inclusion in the Work by the copyright owner\\n      or by an individual or Legal Entity authorized to submit on behalf of\\n      the copyright owner. For the purposes of this definition, \"submitted\"\\n      means any form of electronic, verbal, or written communication sent\\n      to the Licensor or its representatives, including but not limited to\\n      communication on electronic mailing lists, source code control systems,\\n      and issue tracking systems that are managed by, or on behalf of, the\\n      Licensor for the purpose of discussing and improving the Work, but\\n      excluding communication that is conspicuously marked or otherwise\\n      designated in writing by the copyright owner as \"Not a Contribution.\"\\n\\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\\n      on behalf of whom a Contribution has been received by Licensor and\\n      subsequently incorporated within the Work.\\n\\n   2. Grant of Copyright License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      copyright license to reproduce, prepare Derivative Works of,\\n      publicly display, publicly perform, sublicense, and distribute the\\n      Work and such Derivative Works in Source or Object form.\\n\\n   3. Grant of Patent License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      (except as stated in this section) patent license to make, have made,\\n      use, offer to sell, sell, import, and otherwise transfer the Work,\\n      where such license applies only to those patent claims licensable\\n      by such Contributor that are necessarily infringed by their\\n      Contribution(s) alone or by combination of their Contribution(s)\\n      with the Work to which such Contribution(s) was submitted. If You\\n      institute patent litigation against any entity (including a\\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\\n      or a Contribution incorporated within the Work constitutes direct\\n      or contributory patent infringement, then any patent licenses\\n      granted to You under this License for that Work shall terminate\\n      as of the date such litigation is filed.\\n\\n   4. Redistribution. You may reproduce and distribute copies of the\\n      Work or Derivative Works thereof in any medium, with or without\\n      modifications, and in Source or Object form, provided that You\\n      meet the following conditions:\\n\\n      (a) You must give any other recipients of the Work or\\n          Derivative Works a copy of this License; and\\n\\n      (b) You must cause any modified files to carry prominent notices\\n          stating that You changed the files; and\\n\\n      (c) You must retain, in the Source form of any Derivative Works\\n          that You distribute, all copyright, patent, trademark, and\\n          attribution notices from the Source form of the Work,\\n          excluding those notices that do not pertain to any part of\\n          the Derivative Works; and\\n\\n      (d) If the Work includes a \"NOTICE\" text file as part of its\\n          distribution, then any Derivative Works that You distribute must\\n          include a readable copy of the attribution notices contained\\n          within such NOTICE file, excluding those notices that do not\\n          pertain to any part of the Derivative Works, in at least one\\n          of the following places: within a NOTICE text file distributed\\n          as part of the Derivative Works; within the Source form or\\n          documentation, if provided along with the Derivative Works; or,\\n          within a display generated by the Derivative Works, if and\\n          wherever such third-party notices normally appear. The contents\\n          of the NOTICE file are for informational purposes only and\\n          do not modify the License. You may add Your own attribution\\n          notices within Derivative Works that You distribute, alongside\\n          or as an addendum to the NOTICE text from the Work, provided\\n          that such additional attribution notices cannot be construed\\n          as modifying the License.\\n\\n      You may add Your own copyright statement to Your modifications and\\n      may provide additional or different license terms and conditions\\n      for use, reproduction, or distribution of Your modifications, or\\n      for any such Derivative Works as a whole, provided Your use,\\n      reproduction, and distribution of the Work otherwise complies with\\n      the conditions stated in this License.\\n\\n   5. Submission of Contributions. Unless You explicitly state otherwise,\\n      any Contribution intentionally submitted for inclusion in the Work\\n      by You to the Licensor shall be under the terms and conditions of\\n      this License, without any additional terms or conditions.\\n      Notwithstanding the above, nothing herein shall supersede or modify\\n      the terms of any separate license agreement you may have executed\\n      with Licensor regarding such Contributions.\\n\\n   6. Trademarks. This License does not grant permission to use the trade\\n      names, trademarks, service marks, or product names of the Licensor,\\n      except as required for reasonable and customary use in describing the\\n      origin of the Work and reproducing the content of the NOTICE file.\\n\\n   7. Disclaimer of Warranty. Unless required by applicable law or\\n      agreed to in writing, Licensor provides the Work (and each\\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\\n      implied, including, without limitation, any warranties or conditions\\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\\n      PARTICULAR PURPOSE. You are solely responsible for determining the\\n      appropriateness of using or redistributing the Work and assume any\\n      risks associated with Your exercise of permissions under this License.\\n\\n   8. Limitation of Liability. In no event and under no legal theory,\\n      whether in tort (including negligence), contract, or otherwise,\\n      unless required by applicable law (such as deliberate and grossly\\n      negligent acts) or agreed to in writing, shall any Contributor be\\n      liable to You for damages, including any direct, indirect, special,\\n      incidental, or consequential damages of any character arising as a\\n      result of this License or out of the use or inability to use the\\n      Work (including but not limited to damages for loss of goodwill,\\n      work stoppage, computer failure or malfunction, or any and all\\n      other commercial damages or losses), even if such Contributor\\n      has been advised of the possibility of such damages.\\n\\n   9. Accepting Warranty or Additional Liability. While redistributing\\n      the Work or Derivative Works thereof, You may choose to offer,\\n      and charge a fee for, acceptance of support, warranty, indemnity,\\n      or other liability obligations and/or rights consistent with this\\n      License. However, in accepting such obligations, You may act only\\n      on Your own behalf and on Your sole responsibility, not on behalf\\n      of any other Contributor, and only if You agree to indemnify,\\n      defend, and hold each Contributor harmless for any liability\\n      incurred by, or claims asserted against, such Contributor by reason\\n      of your accepting any such warranty or additional liability.\\n\\n   END OF TERMS AND CONDITIONS\\n\\n   APPENDIX: How to apply the Apache License to your work.\\n\\n      To apply the Apache License to your work, attach the following\\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\\n      replaced with your own identifying information. (Don\\'t include\\n      the brackets!)  The text should be enclosed in the appropriate\\n      comment syntax for the file format. We also recommend that a\\n      file or class name and description of purpose be included on the\\n      same \"printed page\" as the copyright notice for easier\\n      identification within third-party archives.\\n\\n   Copyright [yyyy] [name of copyright owner]\\n\\n   Licensed under the Apache License, Version 2.0 (the \"License\");\\n   you may not use this file except in compliance with the License.\\n   You may obtain a copy of the License at\\n\\n       http://www.apache.org/licenses/LICENSE-2.0\\n\\n   Unless required by applicable law or agreed to in writing, software\\n   distributed under the License is distributed on an \"AS IS\" BASIS,\\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n   See the License for the specific language governing permissions and\\n   limitations under the License.\\n\\n------------- LICENSE FOR Facebook Fairseq code --------------\\n\\nMIT License\\n\\nCopyright (c) Facebook, Inc. and its affiliates.\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "What is this fork of Megatron-LM and Megatron-DeepSpeed",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Megatron-DeepSpeed",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "bigscience-workshop",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/bigscience-workshop/Megatron-DeepSpeed/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 69,
      "date": "Thu, 30 Dec 2021 07:55:05 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "After installation, there are several possible workflows. The most comprehensive is:\n1. Data preprocessing\n2. Pretraining\n3. Finetuning (Optional for zero-shot tasks)\n4. Downstream task evaluation or text generation\n\nHowever, steps 1 and 2 can be replaced by using one of the pretrained models mentioned above.\n\nWe've provided several scripts for pretraining both BERT and GPT in [`examples`](./examples) directory, as well as scripts for both zero-shot and fine-tuned downstream tasks including MNLI, RACE, WikiText103, and LAMBADA evaluation. There is also a script for GPT interactive text generation.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Here is how you can get ready to train quickly, using a 1GB 79K-record jsonl dataset.\n\n```\nwget https://huggingface.co/bigscience/misc-test-data/resolve/main/stas/oscar-1GB.jsonl.xz\nwget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json\nwget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt\nxz -d oscar-1GB.jsonl.xz\npython tools/preprocess_data.py \\\n    --input oscar-1GB.jsonl \\\n    --output-prefix my-gpt2 \\\n    --vocab gpt2-vocab.json \\\n    --dataset-impl mmap \\\n    --tokenizer-type GPT2BPETokenizer \\\n    --merge-file gpt2-merges.txt \\\n    --append-eod \\\n    --workers 8\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "In `examples/pretrain_gpt3_175B.sh` we have provided an example of how to configure Megatron to run [GPT-3](https://arxiv.org/abs/2005.14165) with 175 billion parameters on 1024 GPUs. The script is designed for [slurm](https://slurm.schedmd.com/documentation.html) with [pyxis](https://github.com/NVIDIA/pyxis) plugin but can be easily adopted to any other scheduler. It uses 8-way and 16-way tensor and pipeline parallelism, respectively. With options `global-batch-size 1536` and `rampup-batch-size 16 16 5859375`, the training will start with global batch size 16 and linearly increase the global batch size to 1536 over 5,859,375 samples with incrmeental steps 16. The training dataset can be either a single set or a multiple datasets combined with a set of weights.\n\nWith full global batch size of 1536 on 1024 A100 GPUs, each iteration takes around 32 seconds resulting in 138 teraFLOPs per GPU which is 44% of the theoretical peak FLOPs.\n\n\n",
      "technique": "Header extraction"
    }
  ]
}