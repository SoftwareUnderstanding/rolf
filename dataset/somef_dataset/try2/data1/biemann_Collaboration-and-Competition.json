{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[image1]: https://github.com/biemann/Collaboration-and-Competition/blob/master/bin/1039.png \"Results\"\n[image2]: https://github.com/biemann/Collaboration-and-Competition/blob/master/bin/tennis.gif \"Trained Agent\"\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/biemann/Collaboration-and-Competition",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-09-17T20:35:46Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-09-17T22:53:06Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "For this project, we will work with the [Tennis](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#tennis) environment.\n\nIn this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n\nThe observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Each agent receives its own, local observation.  Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n\nThe task is episodic, and in order to solve the environment, your agents must get an average score of +0.5 (over 100 consecutive episodes, after taking the maximum over both agents). Specifically,\n\n- After each episode, we add up the rewards that each agent received (without discounting), to get a score for each agent. This yields 2 (potentially different) scores. We then take the maximum of these 2 scores.\n- This yields a single **score** for each episode.\n\nThe environment is considered solved, when the average (over 100 episodes) of those **scores** is at least +0.5.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9864260416552393,
        0.9906688203467202,
        0.8860374456398695,
        0.9574891359160979,
        0.9739897043618297,
        0.9855818617985961,
        0.9729571223437553,
        0.9871828983179622,
        0.9887275958323255,
        0.992279172679253,
        0.9889710512506387
      ],
      "excerpt": "This algorithm is an extension of the classical DDPG algorithm, as used in following paper: https://arxiv.org/pdf/1706.02275.pdf It extends this algorithm to the multi-agent setting, which is the case here as the two agents need to collaborate to achieve the best possible reward. For more details, we refer to the paper. \nOur implementation follows the exercice we had during this nanodegree on multi-agent systems. In contrast to the project, adapting the code to the current environment was more difficult than expected and we refactored a relevant amount of the code to make it work in this setting.  \nWe devided the code into 4 classes: \nThe train.py class, that is the main class in our project that interacts with the Unity environment. The design of this class is quite similar to what we did before in this nanodegree. \nThe maddpg.pyclass turns mostly around the update() function that was heavily refactored in comparision to the course example to make it work in this environment. The idea of the maddpg algorithm is that we have a separate mddpg network for each agent. \nThe ddpg.pyfile contains in fact three classes, that are essential in the algorithm: The DDPGAgentclass, the OUNoiseclass and the ReplayBufferclass. The Ornstein-Uhlenbek noise helps for data exploration and the replay buffer combats a strong colleration between the data.  \nThe model.pyclass contains our neural network architecture, that is being called by the DDPGAgentclass. \nThe network architecture is the same than the one of the previous project: We used for the actor networks 2 layers of 128 neurons each, follwed by the tanh activation function. For the critics, we used 2 layers of 64 neurons. We used the selu activation function. In contrast to the continuous control project, we did not use batch normalisation as it hurt the performence massively. We also tried with 128-128 neurons for the critic network and this architecture also solved the task, but we decided to come back to the smaller architecture as the training was more reliable. \nWe used for both networks a learning of 5e-4. We tried to implement a learning late scheduler without success. Due to the high randomness in learning, we could not predict how the network will behave and so we did not know how to adapt the learning rate. We used a very high tau parameter for the soft updates: 0.02. For the discount factor gamma, we used the relatively high value of 0.99, because we thought that in this environment, it is quite important to maximise the reward in the long run and not shortly (in contrast to say the reacher environment). \nThe network was very sensitive to the Ornstein-Uhlenbeck noise for data exploration. We tried several approaches to make the algorithm learn reliably or quickly. We finally used a relatively small sigma parameter with 0.05. However, we initialise the value of the noise at 2 in our main function. The idea was to slightly reduce noise every step, so that at the end the noise should be relatively small. This gave us the possibility to accelerate the training process in some cases. However, in some cases the agents failed to learn anything at all with the same hyperparameters. It really depends on how well the agent learns at the beginning. If he fails to learn then, the data exploration is becoming smaller with the time, so the agent will be stuck in this state. That is why in the final version, we did not include noise reduction, so that the agent is able to solve the task reliably. However, note that our best results have been achieved with noise reduction. \nWe show here the graph of the agents that were able to solve the task in 1039 episodes (it took 1139 episodes to reach an average score of 0.5 over the last 100 episodes). As described above, the task has been solved using noise reduction. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9838134980567796,
        0.9947408550835685,
        0.9782758120021774
      ],
      "excerpt": "The graph is relatively interesting, as the agents are not able to learn anything at the beginning (sometimes the ball passes once over the net but not more). Past 900 episodes, the agents begin to learn very quickly. Note that the better the agents become, the longer the episodes will be, so that the agents will be able to extract more information on longer episodes than short ones (in contrast, the two previous had a fix length). This may explain the exponential behaviour of this graph. \nNote that this graph is quite different to the graphs of the two previous projects, where the agent learns slowly at the beginning, fast in the middle and slowly again at the end. Also the length of the episodes are far shorter than in the other tasks, so that explains why the number of required episodes is higher. We wanted to solve the task in less than 1000 episodes, but failed to do so. Using the actual parameters, you should expect to solve the task in between 1000 and 2000 episodes. \nAs an illustration of how our trained agents, that achieved a score of 1.8, behave, we show the following gif: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Third project for the Udacity Nanodegree in Deep Reinforcement Learning. It is a multi-agent system, where two independently trained agents need to collaborate for solving a test as well as possible.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/biemann/Collaboration-and-Competition/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Mon, 27 Dec 2021 19:42:20 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/biemann/Collaboration-and-Competition/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "biemann/Collaboration-and-Competition",
    "technique": "GitHub API"
  },
  "invocation": [
    {
      "confidence": [
        0.9503189345333785
      ],
      "excerpt": "python train.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9515752551715031
      ],
      "excerpt": "python test.py \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/biemann/Collaboration-and-Competition/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "ASP",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "(Image References)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Collaboration-and-Competition",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "biemann",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/biemann/Collaboration-and-Competition/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Mon, 27 Dec 2021 19:42:20 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Download the environment from one of the links below.  You need only select the environment that matches your operating system:\n    - Linux: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Linux.zip)\n    - Mac OSX: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis.app.zip)\n    - Windows (32-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Windows_x86.zip)\n    - Windows (64-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Windows_x86_64.zip)\n\n2. Place the file in the DRLND GitHub repository, in the `p3_collab-compet/` folder, and unzip (or decompress) the file. \n\n\n",
      "technique": "Header extraction"
    }
  ]
}