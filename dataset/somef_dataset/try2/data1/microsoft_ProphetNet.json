{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2011.11928",
      "https://arxiv.org/abs/2104.08006",
      "https://arxiv.org/abs/2104.08006",
      "https://arxiv.org/abs/2104.08006",
      "https://arxiv.org/abs/2104.08006"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you extend or use this work, please cite the [paper](https://arxiv.org/pdf/2001.04063) where it was introduced:\n```\n@inproceedings{qi2020prophetnet,\n  title={Prophetnet: Predicting future n-gram for sequence-to-sequence pre-training},\n  author={Qi, Weizhen and Yan, Yu and Gong, Yeyun and Liu, Dayiheng and Duan, Nan and Chen, Jiusheng and Zhang, Ruofei and Zhou, Ming},\n  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings},\n  pages={2401--2410},\n  year={2020}\n}\n@article{qi2021prophetnet,\n  title={ProphetNet-X: Large-Scale Pre-training Models for English, Chinese, Multi-lingual, Dialog, and Code Generation},\n  author={Qi, Weizhen and Gong, Yeyun and Yan, Yu and Xu, Can and Yao, Bolun and Zhou, Bartuer and Cheng, Biao and Jiang, Daxin and Chen, Jiusheng and Zhang, Ruofei and others},\n  journal={arXiv preprint arXiv:2104.08006},\n  year={2021}\n}\n```\n[Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct)\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "This repo is partially referred to Fairseq-v0.9.0 and MASS.\n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{qi2021prophetnet,\n  title={ProphetNet-X: Large-Scale Pre-training Models for English, Chinese, Multi-lingual, Dialog, and Code Generation},\n  author={Qi, Weizhen and Gong, Yeyun and Yan, Yu and Xu, Can and Yao, Bolun and Zhou, Bartuer and Cheng, Biao and Jiang, Daxin and Chen, Jiusheng and Zhang, Ruofei and others},\n  journal={arXiv preprint arXiv:2104.08006},\n  year={2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{qi2020prophetnet,\n  title={Prophetnet: Predicting future n-gram for sequence-to-sequence pre-training},\n  author={Qi, Weizhen and Yan, Yu and Gong, Yeyun and Liu, Dayiheng and Duan, Nan and Chen, Jiusheng and Zhang, Ruofei and Zhou, Ming},\n  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings},\n  pages={2401--2410},\n  year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9449187648369126
      ],
      "excerpt": "- ProphetNet-Multi [link] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9449187648369126
      ],
      "excerpt": "- ProphetNet-Multi-Wiki100 [link] \n",
      "technique": "Supervised classification"
    }
  ],
  "codeOfConduct": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://raw.githubusercontent.com/microsoft/ProphetNet/master/CODE_OF_CONDUCT.md",
    "technique": "File Exploration"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/microsoft/ProphetNet",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contributing\nThis project welcomes contributions and suggestions. Most contributions require you to\nagree to a Contributor License Agreement (CLA) declaring that you have the right to,\nand actually do, grant us the rights to use your contribution. For details, visit\nhttps://cla.microsoft.com.\nWhen you submit a pull request, a CLA-bot will automatically determine whether you need\nto provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the\ninstructions provided by the bot. You will only need to do this once across all repositories using our CLA.\nThis project has adopted the Microsoft Open Source Code of Conduct.\nFor more information see the Code of Conduct FAQ\nor contact opencode@microsoft.com with any additional questions or comments.",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-02-17T20:13:11Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-18T03:00:51Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9720326817124213
      ],
      "excerpt": "This repo provides the code for reproducing the experiments in ProphetNet. In the paper, we propose a new pre-trained language model called ProphetNet for sequence-to-sequence learning with a novel self-supervised objective called future n-gram prediction.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8874196017382427
      ],
      "excerpt": "This repo is still developing, feel free to report bugs and we will fix them ~ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9146036662267655,
        0.8655924571703956,
        0.8655924571703956,
        0.8247478285351854,
        0.978194201784401
      ],
      "excerpt": "Different ProphetNet-X models have the only difference of the vocabulary file. Simply modify one model file and you can evaluate your idea with all the pretrained models and finetuning scripts! \nProphetNet pretrained models for bio-medical text. \nProphetNet pretrained models for protein. \nNew ProphetNet models for long document modeling. \nNew algorithms for Transformer/ProphetNet to reduce inference latency with no hurt to the results. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336436955388453,
        0.9957018832480724
      ],
      "excerpt": "We have released the following checkpoints for pre-trained models as described in the paper of ProphetNet-X(appear soon). \nProphetNet-X is based on ProphetNet, which also serves the ProphetNet-En model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.811603211615371
      ],
      "excerpt": "it's a good choice to download fairseq git repo, checkout v0.9.0, and merge our codes.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Code for EMNLP20 paper: \"ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training\"",
      "technique": "GitHub API"
    }
  ],
  "documentation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "https://fairseq.readthedocs.io/",
      "technique": "Regular expression"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/microsoft/ProphetNet/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 60,
      "date": "Mon, 27 Dec 2021 23:20:03 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/microsoft/ProphetNet/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "microsoft/ProphetNet",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/microsoft/ProphetNet/master/ProphetNet_Multi/inference_ntg.sh",
      "https://raw.githubusercontent.com/microsoft/ProphetNet/master/ProphetNet_Multi/finetune_xprophetnet_qg.sh",
      "https://raw.githubusercontent.com/microsoft/ProphetNet/master/ProphetNet_Multi/finetune_xprophetnet_ntg.sh",
      "https://raw.githubusercontent.com/microsoft/ProphetNet/master/ProphetNet_Multi/inference_qg.sh",
      "https://raw.githubusercontent.com/microsoft/ProphetNet/master/ProphetNet_Multi/xprophetnet_preprocess.sh",
      "https://raw.githubusercontent.com/microsoft/ProphetNet/master/ProphetNet_Code/finetune_code2text.sh",
      "https://raw.githubusercontent.com/microsoft/ProphetNet/master/ProphetNet_Code/inference_code2text_single.sh",
      "https://raw.githubusercontent.com/microsoft/ProphetNet/master/ProphetNet_Zh/inference_chinese_qa.sh",
      "https://raw.githubusercontent.com/microsoft/ProphetNet/master/ProphetNet_Zh/finetune_chinese_summarize_lcsts.sh",
      "https://raw.githubusercontent.com/microsoft/ProphetNet/master/ProphetNet_Zh/inference_summarize_lcsts.sh",
      "https://raw.githubusercontent.com/microsoft/ProphetNet/master/ProphetNet_Zh/inference_chinese_summarization.sh",
      "https://raw.githubusercontent.com/microsoft/ProphetNet/master/ProphetNet_Zh/finetune_chinese_summarize.sh",
      "https://raw.githubusercontent.com/microsoft/ProphetNet/master/ProphetNet_Zh/finetune_chinese_qa.sh",
      "https://raw.githubusercontent.com/microsoft/ProphetNet/master/GLGE_baselines/script/train_prophetnet_base.sh",
      "https://raw.githubusercontent.com/microsoft/ProphetNet/master/GLGE_baselines/script/test_lstm.sh",
      "https://raw.githubusercontent.com/microsoft/ProphetNet/master/GLGE_baselines/script/preprocessed.sh",
      "https://raw.githubusercontent.com/microsoft/ProphetNet/master/GLGE_baselines/script/train_lstm.sh",
      "https://raw.githubusercontent.com/microsoft/ProphetNet/master/GLGE_baselines/script/test_transformer.sh",
      "https://raw.githubusercontent.com/microsoft/ProphetNet/master/GLGE_baselines/script/train_prophetnet.sh",
      "https://raw.githubusercontent.com/microsoft/ProphetNet/master/GLGE_baselines/script/train_transformer.sh",
      "https://raw.githubusercontent.com/microsoft/ProphetNet/master/GLGE_baselines/script/test_prophetnet.sh",
      "https://raw.githubusercontent.com/microsoft/ProphetNet/master/GLGE_baselines/script/preprocessed-all.sh",
      "https://raw.githubusercontent.com/microsoft/ProphetNet/master/GLGE_baselines/script/test_prophetnet_base.sh",
      "https://raw.githubusercontent.com/microsoft/ProphetNet/master/GLGE_baselines/script/run.sh",
      "https://raw.githubusercontent.com/microsoft/ProphetNet/master/ProphetNet_Dialog_Zh/inference_stc_dialog.sh",
      "https://raw.githubusercontent.com/microsoft/ProphetNet/master/ProphetNet_Dialog_Zh/finetune_chinese_stc_dialog.sh",
      "https://raw.githubusercontent.com/microsoft/ProphetNet/master/ProphetNet_Dialog_En/get_data.sh",
      "https://raw.githubusercontent.com/microsoft/ProphetNet/master/ProphetNet_Dialog_En/finetune_scripts/dailydialog/job.sh",
      "https://raw.githubusercontent.com/microsoft/ProphetNet/master/ProphetNet_Dialog_En/finetune_scripts/personachat/job.sh",
      "https://raw.githubusercontent.com/microsoft/ProphetNet/master/ProphetNet_Dialog_En/finetune_scripts/dstc7avsd/job.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.867292731904402
      ],
      "excerpt": "it's a good choice to download fairseq git repo, checkout v0.9.0, and merge our codes.  \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8752494444482888
      ],
      "excerpt": "ProphetNet pretrained models for bio-medical text. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/microsoft/ProphetNet/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'Copyright (c) 2015 Xinlei Chen, Hao Fang, Tsung-Yi Lin, and Ramakrishna Vedantam\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in\\nall copies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\\nTHE SOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "ProphetNet-X",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "ProphetNet",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "microsoft",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/microsoft/ProphetNet/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- pip install torch==1.3.0  \n- pip install fairseq==v0.9.0\n- pip install tensorboardX==1.7  \n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 340,
      "date": "Mon, 27 Dec 2021 23:20:03 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The procedure includes 1) Tokenize, 2) Binarize, 3) Finetune, 4) Inference.  \nProphetNet is implemented on base of Fairseq, which you can refer to [Fairseq Mannual](https://fairseq.readthedocs.io/en/latest/command_line_tools.html).  \n\n**For all the ProphetNet-X models, the only difference is the dictionary, which means different Tokenizers should be used.**\n\nWe take ProphetNet-En for example:\n\nTokenize. Prepare your train.src, train.tgt, and valid, test sets. Input and output of one sample are placed in the .src and .tgt file with one line.    \nUse bert-uncased tokenizer to tokenize your data into word piece. \n```\nfrom transformers import BertTokenizer\n\n\ndef bert_uncased_tokenize(fin, fout):\n    fin = open(fin, 'r', encoding='utf-8')\n    fout = open(fout, 'w', encoding='utf-8')\n    tok = BertTokenizer.from_pretrained('bert-base-uncased')\n    for line in fin:\n        word_pieces = tok.tokenize(line.strip())\n        new_line = \" \".join(word_pieces)\n        fout.write('{}\\n'.format(new_line))\nbert_uncased_tokenize('train.src', 'tokenized_train.src')\nbert_uncased_tokenize('train.tgt', 'tokenized_train.tgt')\nbert_uncased_tokenize('valid.src', 'tokenized_valid.src')\nbert_uncased_tokenize('valid.tgt', 'tokenized_valid.tgt')\nbert_uncased_tokenize('test.src', 'tokenized_test.src')\nbert_uncased_tokenize('test.tgt', 'tokenized_test.tgt')\n```\nBinirize it with fairseq-preprocess\n```\nfairseq-preprocess \\\n--user-dir prophetnet \\\n--task translation_prophetnet \\\n--source-lang src --target-lang tgt \\\n--trainpref tokenized_train --validpref tokenized_valid --testpref tokenized_test \\\n--destdir processed --srcdict vocab.txt --tgtdict vocab.txt \\\n--workers 20\n```\nFine tune with fairseq-train.  \n--disable-ngram-loss\uff1aonly keep the next first token loss.  \n--ngram: number of future tokens to predict. Provided pretrained checkpoint predicts 2 future tokens, and you should set it as 2 to be consistent.    \nIf your device does not support float16, remove --fp16.\n```\nDATA_DIR=processed\nUSER_DIR=./prophetnet\nARCH=ngram_transformer_prophet_large\nCRITERION=ngram_language_loss\nSAVE_DIR=./model\nTENSORBOARD_LOGDIR=./logs\nPRETRAINED_MODEL=pretrained_checkpoints/prophetnet_en.pt\n\nfairseq-train \\\n--fp16 \\\n--user-dir $USER_DIR --task translation_prophetnet --arch $ARCH \\\n--optimizer adam --adam-betas '(0.9, 0.999)' --clip-norm 0.1 \\\n--lr 0.00001 --min-lr 1e-09 \\\n--lr-scheduler inverse_sqrt --warmup-init-lr 1e-07 --warmup-updates 1000 \\\n--dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\n--criterion $CRITERION --label-smoothing 0.1 \\\n--update-freq 1  --max-tokens 1400 --max-sentences 7 \\\n--num-workers 4 \\\n--load-from-pretrained-model $PRETRAINED_MODEL \\\n--ddp-backend=no_c10d --max-epoch 10 \\\n--max-source-positions 512 --max-target-positions 512 \\\n--skip-invalid-size-inputs-valid-test \\\n--save-dir $SAVE_DIR \\\n--keep-last-epochs 10 \\\n--tensorboard-logdir $TENSORBOARD_LOGDIR \\\n$DATA_DIR\n```\nInference with fairseq-generate to generate targets for given processed test files. Or you can [fairseq-interactive](https://fairseq.readthedocs.io/en/latest/command_line_tools.html#fairseq-interactive) to generate answers for your typed-in text (which should also been tokenized).\n```\nBEAM=5\nLENPEN=1.5\nCHECK_POINT=./model/checkpoint5.pt\nTEMP_FILE=fairseq_outputs.txt\nOUTPUT_FILE=sorted_outputs.txt\n\nfairseq-generate processed --path $CHECK_POINT --user-dir prophetnet --task translation_prophetnet --batch-size 80 --gen-subset test --beam $BEAM --num-workers 4 --no-repeat-ngram-size 3 --lenpen $LENPEN 2>&1 > $TEMP_FILE\ngrep ^H $TEMP_FILE | cut -c 3- | sort -n | cut -f3- | sed \"s/ #:#://g\" > $OUTPUT_FILE\n\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}