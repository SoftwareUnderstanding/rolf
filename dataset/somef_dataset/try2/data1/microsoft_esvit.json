{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Our implementation is built partly upon packages: [[Dino](https://github.com/facebookresearch/dino)]  [[Timm](https://github.com/rwightman/pytorch-image-models)]\n\n\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2106.09785",
      "https://arxiv.org/abs/2106.09785",
      "https://arxiv.org/abs/2106.09785"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find this repository useful, please consider giving a star :star:   and citation :beer::\n\n```\n@article{li2021esvit,\n  title={Efficient Self-supervised Vision Transformers for Representation Learning},\n  author={Li, Chunyuan and Yang, Jianwei and Zhang, Pengchuan and Gao, Mei and Xiao, Bin and Dai, Xiyang and Yuan, Lu and Gao, Jianfeng},\n  journal={arXiv preprint arXiv:2106.09785},\n  year={2021}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{li2021esvit,\n  title={Efficient Self-supervised Vision Transformers for Representation Learning},\n  author={Li, Chunyuan and Yang, Jianwei and Zhang, Pengchuan and Gao, Mei and Xiao, Bin and Dai, Xiyang and Yuan, Lu and Gao, Jianfeng},\n  journal={arXiv preprint arXiv:2106.09785},\n  year={2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9667228562716982
      ],
      "excerpt": "[Swin Transformers]  [Vision Longformer]  [Convolutional vision Transformers (CvT)]  [Focal Transformers] \n",
      "technique": "Supervised classification"
    }
  ],
  "codeOfConduct": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://raw.githubusercontent.com/microsoft/esvit/main/CODE_OF_CONDUCT.md",
    "technique": "File Exploration"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/microsoft/esvit",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-07-01T19:19:39Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-23T13:18:24Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9333802867004882
      ],
      "excerpt": "PyTorch implementation for EsViT, built with two techniques:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.97936175683008
      ],
      "excerpt": "Figure: Efficiency vs accuracy comparison under the linear classification protocol on ImageNet. Left: Throughput of all SoTA SSL vision systems, circle sizes indicates model parameter counts; Right: performance over varied parameter counts for models with moderate (throughout/#parameters) ratio. Please refer Section 4.1 for details. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8976413297369146
      ],
      "excerpt": "EsViT (Swin) with network configurations of increased model capacities, pre-trained with both view-level and region-level tasks. ResNet-50 trained with both tasks is shown as a reference. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979918244699695
      ],
      "excerpt": "To train on 1 node with 16 GPUs for Swin-T model size: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9544094977639612
      ],
      "excerpt": "For an invidiual image (with path --image_path $IMG_PATH), we visualize the attention maps and correspondence of the last layer: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8199837720617734
      ],
      "excerpt": "a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8073737513954784,
        0.9783267603835809
      ],
      "excerpt": "This project has adopted the Microsoft Open Source Code of Conduct. \nFor more information see the Code of Conduct FAQ or \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8411378894176985,
        0.9214339495776546
      ],
      "excerpt": "This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft  \ntrademarks or logos is subject to and must follow  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "EsViT: Efficient self-supervised Vision Transformers",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/microsoft/esvit/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 21,
      "date": "Mon, 27 Dec 2021 20:29:47 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/microsoft/esvit/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "microsoft/esvit",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/microsoft/esvit/main/scripts/scripts_local/run_lincls_imagenet.sh",
      "https://raw.githubusercontent.com/microsoft/esvit/main/scripts/scripts_local/run_ssl_imagenet.sh",
      "https://raw.githubusercontent.com/microsoft/esvit/main/scripts/scripts_local/run_analysis.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8151102863474781,
        0.9014040502414133
      ],
      "excerpt": "    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/resnet/resnet50/bl_lr0.0005_gpu16_bs64_multicrop_epoch300_dino_aug/resume_from_ckpt0200/checkpoint.pth\">full ckpt</a></td> \n    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/resnet/resnet50/bl_lr0.0005_gpu16_bs64_multicrop_epoch300_dino_aug/resume_from_ckpt0200/log.txt\">train</a></td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.815828376771889,
        0.9064882312120607,
        0.8186269548719237
      ],
      "excerpt": "    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/swin/swin_tiny/bl_lr0.0005_gpu16_bs32_dense_multicrop_epoch300/checkpoint_best.pth\">full ckpt</a></td> \n    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/swin/swin_tiny/bl_lr0.0005_gpu16_bs32_dense_multicrop_epoch300/log.txt\">train</a></td> \n    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/swin/swin_tiny/bl_lr0.0005_gpu16_bs32_dense_multicrop_epoch300/lincls/epoch0300/log.txt\">linear</a></td>  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.815828376771889,
        0.9064882312120607,
        0.8186269548719237
      ],
      "excerpt": "    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/swin/swin_small/bl_lr0.0005_gpu16_bs32_dense_multicrop_epoch300/checkpoint_best.pth\">full ckpt</a></td> \n    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/swin/swin_small/bl_lr0.0005_gpu16_bs32_dense_multicrop_epoch300/log.txt\">train</a></td> \n    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/swin/swin_small/bl_lr0.0005_gpu16_bs32_dense_multicrop_epoch300/lincls/epoch0300/lr_0.003_n_last_blocks4/log.txt\">linear</a></td>  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.815828376771889,
        0.9064882312120607,
        0.8186269548719237
      ],
      "excerpt": "    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/swin/swin_base/bl_lr0.0005_gpu16_bs32_multicrop_epoch300_dino_aug/continued_from0200_dense/checkpoint_best.pth\">full ckpt</a></td> \n    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/swin/swin_base/bl_lr0.0005_gpu16_bs32_multicrop_epoch300_dino_aug/continued_from0200_dense/log.txt\">train</a></td> \n    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/swin/swin_base/bl_lr0.0005_gpu16_bs32_multicrop_epoch300_dino_aug/continued_from0200_dense/lincls/epoch0260/lr_0.001_n_last_blocks4/log.txt\">linear</a></td>  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.815828376771889,
        0.9064882312120607,
        0.8186269548719237
      ],
      "excerpt": "    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/swin/swin_tiny/bl_lr0.0005_gpu16_bs32_multicrop_epoch300_dino_aug_window14/continued_from0200_dense/checkpoint_best.pth\">full ckpt</a></td> \n    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/swin/swin_tiny/bl_lr0.0005_gpu16_bs32_multicrop_epoch300_dino_aug_window14/continued_from0200_dense/log.txt\">train</a></td> \n    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/swin/swin_tiny/bl_lr0.0005_gpu16_bs32_multicrop_epoch300_dino_aug_window14/continued_from0200_dense/lincls/epoch_last/log.txt\">linear</a></td>  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.815828376771889,
        0.9064882312120607,
        0.8186269548719237
      ],
      "excerpt": "    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/swin/swin_small/bl_lr0.0005_gpu16_bs16_multicrop_epoch300_dino_aug_window14/continued_from0180_dense/checkpoint_best.pth\">full ckpt</a></td> \n    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/swin/swin_small/bl_lr0.0005_gpu16_bs16_multicrop_epoch300_dino_aug_window14/continued_from0180_dense/log.txt\">train</a></td> \n    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/swin/swin_small/bl_lr0.0005_gpu16_bs16_multicrop_epoch300_dino_aug_window14/continued_from0180_dense/lincls/epoch0250/log.txt\">linear</a></td>  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.815828376771889,
        0.9064882312120607,
        0.8186269548719237
      ],
      "excerpt": "    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/swin/swin_base/bl_lr0.0005_nodes4_gpu16_bs8_multicrop_epoch300_dino_aug_window14_lv/continued_from_epoch0200_dense_norm_true/checkpoint_best.pth\">full ckpt</a></td>   \n    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/swin/swin_base/bl_lr0.0005_nodes4_gpu16_bs8_multicrop_epoch300_dino_aug_window14_lv/continued_from_epoch0200_dense_norm_true/log.txt\">train</a></td> \n    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/swin/swin_base/bl_lr0.0005_nodes4_gpu16_bs8_multicrop_epoch300_dino_aug_window14_lv/continued_from_epoch0200_dense_norm_true/lincls/epoch0240/lr_0.001_n_last_blocks4/log.txt\">linear</a></td>  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8151102863474781,
        0.9014040502414133,
        0.8170648955935738
      ],
      "excerpt": "    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/resnet/resnet50/bl_lr0.0005_gpu16_bs64_multicrop_epoch300_dino_aug/checkpoint.pth\">full ckpt</a></td> \n    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/resnet/resnet50/bl_lr0.0005_gpu16_bs64_multicrop_epoch300_dino_aug/log.txt\">train</a></td> \n    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/resnet/resnet50/bl_lr0.0005_gpu16_bs64_multicrop_epoch300_dino_aug/lincls/epoch_last/log.txt\">linear</a></td>  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.815828376771889,
        0.9064882312120607,
        0.8186269548719237
      ],
      "excerpt": "    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/swin/swin_tiny/bl_lr0.0005_gpu16_bs32_multicrop_epoch300_dino_aug_window7/checkpoint.pth\">full ckpt</a></td> \n    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/swin/swin_tiny/bl_lr0.0005_gpu16_bs32_multicrop_epoch300_dino_aug_window7/log.txt\">train</a></td> \n    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/swin/swin_tiny/bl_lr0.0005_gpu16_bs32_multicrop_epoch300_dino_aug_window7/lincls/epoch0300/log.txt\">linear</a></td>  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.815828376771889,
        0.9064882312120607,
        0.8186269548719237
      ],
      "excerpt": "    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/swin/swin_small/bl_lr0.0005_gpu16_bs32_multicrop_epoch300/checkpoint.pth\">full ckpt</a></td> \n    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/swin/swin_small/bl_lr0.0005_gpu16_bs32_multicrop_epoch300/log.txt\">train</a></td> \n    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/swin/swin_small/bl_lr0.0005_gpu16_bs32_multicrop_epoch300/lincls/epoch0300/lr_0.003_n_last_blocks4/log.txt\">linear</a></td>  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.815828376771889,
        0.9064882312120607,
        0.8186269548719237
      ],
      "excerpt": "    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/swin/swin_base/bl_lr0.0005_gpu16_bs32_multicrop_epoch300_dino_aug/checkpoint.pth\">full ckpt</a></td> \n    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/swin/swin_base/bl_lr0.0005_gpu16_bs32_multicrop_epoch300_dino_aug/log.txt\">train</a></td> \n    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/swin/swin_base/bl_lr0.0005_gpu16_bs32_multicrop_epoch300_dino_aug/lincls/epoch0260/lr_0.001_n_last_blocks4/log.txt\">linear</a></td>  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.815828376771889,
        0.9064882312120607,
        0.8186269548719237
      ],
      "excerpt": "    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/swin/swin_tiny/bl_lr0.0005_gpu16_bs64_multicrop_epoch300/checkpoint.pth\">full ckpt</a></td> \n    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/swin/swin_tiny/bl_lr0.0005_gpu16_bs64_multicrop_epoch300/log.txt\">train</a></td> \n    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/swin/swin_tiny/bl_lr0.0005_gpu16_bs64_multicrop_epoch300/lincls/epoch0300/log.txt\">linear</a></td>  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.815828376771889,
        0.9064882312120607,
        0.8186269548719237
      ],
      "excerpt": "    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/swin/swin_tiny/bl_lr0.0005_gpu16_bs64_multicrop_epoch150_dino_aug_window7_webvision1_debug/checkpoint.pth\">full ckpt</a></td> \n    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/swin/swin_tiny/bl_lr0.0005_gpu16_bs64_multicrop_epoch150_dino_aug_window7_webvision1_debug/log.txt\">train</a></td> \n    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/swin/swin_tiny/bl_lr0.0005_gpu16_bs64_multicrop_epoch150_dino_aug_window7_webvision1_debug/lincls/epoch_last/log.txt\">linear</a></td>  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.815828376771889,
        0.9064882312120607,
        0.8186269548719237
      ],
      "excerpt": "    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/swin/swin_tiny/bl_lr0.0005_gpu16_bs64_multicrop_epoch150_dino_aug_window7_openimages_v4_debug/checkpoint.pth\">full ckpt</a></td> \n    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/swin/swin_tiny/bl_lr0.0005_gpu16_bs64_multicrop_epoch150_dino_aug_window7_openimages_v4_debug/log.txt\">train</a></td> \n    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/swin/swin_tiny/bl_lr0.0005_gpu16_bs64_multicrop_epoch150_dino_aug_window7_openimages_v4_debug/lincls/epoch_last/log.txt\">linear</a></td>  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.815828376771889,
        0.9064882312120607,
        0.8186269548719237
      ],
      "excerpt": "    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/swin/swin_tiny/bl_lr0.0005_gpu16_bs64_multicrop_epoch300_dino_aug_window7_imagenet22k_debug/checkpoint.pth\">full ckpt</a></td> \n    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/swin/swin_tiny/bl_lr0.0005_gpu16_bs64_multicrop_epoch300_dino_aug_window7_imagenet22k_debug/log.txt\">train</a></td> \n    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/swin/swin_tiny/bl_lr0.0005_gpu16_bs64_multicrop_epoch300_dino_aug_window7_imagenet22k_debug/lincls/epoch_last/log.txt\">linear</a></td>  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.815828376771889,
        0.9064882312120607,
        0.8186269548719237
      ],
      "excerpt": "    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/vil/vil_2262/bl_lr0.0005_gpu16_bs32_multicrop_epoch300/vil_mode0/checkpoint.pth\">full ckpt</a></td> \n    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/vil/vil_2262/bl_lr0.0005_gpu16_bs32_multicrop_epoch300/vil_mode0/log.txt\">train</a></td> \n    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/vil/vil_2262/bl_lr0.0005_gpu16_bs64_multicrop_epoch300/lincls/epoch0300/4_last_blocks/log.txt\">linear</a></td>  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.815828376771889,
        0.9064882312120607,
        0.8186269548719237
      ],
      "excerpt": "    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/vil/vil_2262/bl_lr0.0005_gpu16_bs64_multicrop_epoch300/continued_from0200_dense/checkpoint.pth\">full ckpt</a></td> \n    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/vil/vil_2262/bl_lr0.0005_gpu16_bs64_multicrop_epoch300/continued_from0200_dense/log.txt\">train</a></td> \n    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/vil/vil_2262/bl_lr0.0005_gpu16_bs64_multicrop_epoch300/continued_from0200_dense/log.txt\">linear</a></td>  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.815828376771889,
        0.9064882312120607,
        0.8186269548719237
      ],
      "excerpt": "    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/cvt/cvt_tiny/bl_lr0.0005_gpu16_bs64_multicrop_epoch300_dino_aug/checkpoint.pth\">full ckpt</a></td> \n    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/cvt/cvt_tiny/bl_lr0.0005_gpu16_bs64_multicrop_epoch300_dino_aug/log.txt\">train</a></td> \n    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/cvt/cvt_tiny/bl_lr0.0005_gpu16_bs64_multicrop_epoch300_dino_aug/lincls/epoch0300/log.txt\">linear</a></td>  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.815828376771889,
        0.9064882312120607,
        0.8186269548719237
      ],
      "excerpt": "    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/cvt/cvt_tiny/bl_lr0.0005_gpu16_bs64_multicrop_epoch300_dino_aug/continued_from0200_dense/checkpoint.pth\">full ckpt</a></td>   \n    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/cvt/cvt_tiny/bl_lr0.0005_gpu16_bs64_multicrop_epoch300_dino_aug/continued_from0200_dense/log.txt\">train</a></td> \n    <td><a href=\"https://chunyleu.blob.core.windows.net/output/ckpts/esvit/cvt/cvt_tiny/bl_lr0.0005_gpu16_bs64_multicrop_epoch300_dino_aug/continued_from0200_dense/lincls/epoch0300/log.txt\">linear</a></td>  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.848221579491972
      ],
      "excerpt": "You can analyze the learned models by running python run_analysis.py. One example to analyze EsViT (Swin-T) is shown. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8003352366805131
      ],
      "excerpt": "a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8377516370773336
      ],
      "excerpt": "  <img width=\"90%\" alt=\"Efficiency vs accuracy comparison under the linear classification protocol on ImageNet with EsViT\" src=\"./plot/esvit_sota.png\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.830052435782184
      ],
      "excerpt": "    <th>pre-train dataset</th> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8029761668390416
      ],
      "excerpt": "    <th>pre-train task</th> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8057896739817119
      ],
      "excerpt": "To train on 1 node with 16 GPUs for Swin-T model size: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9331710292754725
      ],
      "excerpt": "python -m torch.distributed.launch --nproc_per_node=16 main_esvit.py --arch swin_tiny --data_path $DATA_PATH/train --output_dir $OUT_PATH --batch_size_per_gpu 32 --epochs 300 --teacher_temp 0.07 --warmup_epochs 10 --warmup_teacher_temp_epochs 30 --norm_last_layer false --use_dense_prediction True --cfg experiments/imagenet/swin/swin_tiny_patch4_window7_224.yaml  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8934444841632909
      ],
      "excerpt": "The main training script is main_esvit.py and conducts the training loop, taking the following options (among others) as arguments: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9340851731700345
      ],
      "excerpt": "python -m torch.distributed.launch --nproc_per_node=16 main_evsit.py --arch cvt_tiny --data_path $DATA_PATH/train --output_dir $OUT_PATH --batch_size_per_gpu 32 --epochs 300 --teacher_temp 0.07 --warmup_epochs 10 --warmup_teacher_temp_epochs 30 --norm_last_layer false --use_dense_prediction True --aug-opt dino_aug --cfg experiments/imagenet/cvt_v4/s1.yaml \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9096530437705059,
        0.8317148153984437
      ],
      "excerpt": "python -m torch.distributed.launch --nproc_per_node=16 main_evsit.py --arch vil_2262 --data_path $DATA_PATH/train --output_dir $OUT_PATH --batch_size_per_gpu 32 --epochs 300 --teacher_temp 0.07 --warmup_epochs 10 --warmup_teacher_temp_epochs 30 --norm_last_layer false --use_dense_prediction True --aug-opt dino_aug --cfg experiments/imagenet/vil/vil_small/base.yaml MODEL.SPEC.MSVIT.ARCH 'l1,h3,d96,n2,s1,g1,p4,f7,a0_l2,h6,d192,n2,s1,g1,p2,f7,a0_l3,h12,d384,n6,s0,g1,p2,f7,a0_l4,h24,d768,n2,s0,g0,p2,f7,a0' MODEL.SPEC.MSVIT.MODE 1 MODEL.SPEC.MSVIT.VIL_MODE_SWITCH 0.75 \nTo train on 2 nodes with 16 GPUs each (total 32 GPUs) for Swin-Small model size: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9651119379515012,
        0.8088411626528732
      ],
      "excerpt": "python main_evsit_mnodes.py --num_nodes 2 --num_gpus_per_node 16 --data_path $DATA_PATH/train --output_dir $OUT_PATH/continued_from0200_dense --batch_size_per_gpu 16 --arch swin_small --zip_mode True --epochs 300 --teacher_temp 0.07 --warmup_epochs 10 --warmup_teacher_temp_epochs 30 --norm_last_layer false --cfg experiments/imagenet/swin/swin_small_patch4_window14_224.yaml --use_dense_prediction True --pretrained_weights_ckpt $OUT_PATH/checkpoint0200.pth \nTo train a supervised linear classifier on frozen weights on a single node with 4 gpus, run eval_linear.py. To train a k-NN classifier on frozen weights on a single node with 4 gpus, run eval_knn.py. Please specify --arch, --cfg and --pretrained_weights to  choose a pre-trained checkpoint. If you want to evaluate the last checkpoint of EsViT with Swin-T, you can run for example: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8567905481537627,
        0.8378755469673592
      ],
      "excerpt": "python -m torch.distributed.launch --nproc_per_node=4 eval_linear.py --data_path $DATA_PATH --output_dir $OUT_PATH/lincls/epoch0300 --pretrained_weights $CKPT_PATH --checkpoint_key teacher --batch_size_per_gpu 256 --arch swin_tiny --cfg experiments/imagenet/swin/swin_tiny_patch4_window7_224.yaml --n_last_blocks 4 --num_labels 1000 MODEL.NUM_CLASSES 0 \npython -m torch.distributed.launch --nproc_per_node=4 eval_knn.py --data_path $DATA_PATH --dump_features $OUT_PATH/features/epoch0300 --pretrained_weights $CKPT_PATH --checkpoint_key teacher --batch_size_per_gpu 256 --arch swin_tiny --cfg experiments/imagenet/swin/swin_tiny_patch4_window7_224.yaml MODEL.NUM_CLASSES 0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9192979094975035
      ],
      "excerpt": "python run_analysis.py --arch swin_tiny --image_path $IMG_PATH --output_dir $OUT_PATH --pretrained_weights $CKPT_PATH --learning ssl --seed $SEED --cfg experiments/imagenet/swin/swin_tiny_patch4_window7_224.yaml --vis_attention True --vis_correspondence True MODEL.NUM_CLASSES 0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8864721444951194
      ],
      "excerpt": "python run_analysis.py --arch swin_tiny --data_path $DATA_PATH --output_dir $OUT_PATH --pretrained_weights $CKPT_PATH --learning ssl --seed $SEED --cfg experiments/imagenet/swin/swin_tiny_patch4_window7_224.yaml  --measure_correspondence True MODEL.NUM_CLASSES 0 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/microsoft/esvit/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'    MIT License\\n\\n    Copyright (c) Microsoft Corporation.\\n\\n    Permission is hereby granted, free of charge, to any person obtaining a copy\\n    of this software and associated documentation files (the \"Software\"), to deal\\n    in the Software without restriction, including without limitation the rights\\n    to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\n    copies of the Software, and to permit persons to whom the Software is\\n    furnished to do so, subject to the following conditions:\\n\\n    The above copyright notice and this permission notice shall be included in all\\n    copies or substantial portions of the Software.\\n\\n    THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\n    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\n    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\n    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\n    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\n    OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\n    SOFTWARE\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Efficient Self-Supervised Vision Transformers (EsViT)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "esvit",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "microsoft",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/microsoft/esvit/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 266,
      "date": "Mon, 27 Dec 2021 20:29:47 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "self-supervised-learning",
      "vision-transformers"
    ],
    "technique": "GitHub API"
  }
}