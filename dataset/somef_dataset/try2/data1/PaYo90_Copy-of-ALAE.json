{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2004.04467\">2004.04467</a></h4>\n\n## To run the demo\n\nTo run the demo, you will need to have a CUDA capable GPU, PyTorch >= v1.3.1 and cuda/cuDNN drivers installed.\nInstall the required packages:\n\n    pip install -r requirements.txt\n  \nDownload pre-trained models:\n\n    python training_artifacts/download_all.py\n\nRun the demo:\n\n    python interactive_demo.py\n\nYou can specify **yaml** config to use. Configs are located here: https://github.com/podgorskiy/ALAE/tree/master/configs.\nBy default, it uses one for FFHQ dataset.\nYou can change the config using `-c` parameter. To run on `celeb-hq` in 256x256 resolution, run:\n\n    python interactive_demo.py -c celeba-hq256\n\nHowever, for configs other then FFHQ, you need to obtain new principal direction vectors for the attributes.\n\n## Repository organization\n\n#### Running scripts\n\nThe code in the repository is organized in such a way that all scripts must be run from the root of the repository.\nIf you use an IDE (e.g. PyCharm or Visual Studio Code",
      "https://arxiv.org/abs/ <a href=\"https://arxiv.org/abs/2004.04467\">2004.04467</a></h4>\n\n## To run the demo\n\nTo run the demo, you will need to have a CUDA capable GPU, PyTorch >= v1.3.1 and cuda/cuDNN drivers installed.\nInstall the required packages:\n\n    pip install -r requirements.txt\n  \nDownload pre-trained models:\n\n    python training_artifacts/download_all.py\n\nRun the demo:\n\n    python interactive_demo.py\n\nYou can specify **yaml** config to use. Configs are located here: https://github.com/podgorskiy/ALAE/tree/master/configs.\nBy default, it uses one for FFHQ dataset.\nYou can change the config using `-c` parameter. To run on `celeb-hq` in 256x256 resolution, run:\n\n    python interactive_demo.py -c celeba-hq256\n\nHowever, for configs other then FFHQ, you need to obtain new principal direction vectors for the attributes.\n\n## Repository organization\n\n#### Running scripts\n\nThe code in the repository is organized in such a way that all scripts must be run from the root of the repository.\nIf you use an IDE (e.g. PyCharm or Visual Studio Code), just set *Working Directory* to point to the root of the repository.\n\nIf you want to run from the command line, then you also need to set **PYTHONPATH** variable to point to the root of the repository.\n\nFor example, let's say we've cloned repository to *~/ALAE* directory, then do:\n\n    $ cd ~/ALAE\n    $ export PYTHONPATH=$PYTHONPATH:$(pwd)\n\n![pythonpath](https://podgorskiy.com/static/pythonpath.svg)\n\nNow you can run scripts as follows:\n\n    $ python style_mixing/stylemix.py\n\n#### Repository structure\n\n\n| Path | Description\n| :--- | :----------\n| ALAE | Repository root folder\n| &boxvr;&nbsp; configs | Folder with yaml config files.\n| &boxv;&nbsp; &boxvr;&nbsp; bedroom.yaml | Config file for LSUN bedroom dataset at 256x256 resolution.\n| &boxv;&nbsp; &boxvr;&nbsp; celeba.yaml | Config file for CelebA dataset at 128x128 resolution.\n| &boxv;&nbsp; &boxvr;&nbsp; celeba-hq256.yaml | Config file for CelebA-HQ dataset at 256x256 resolution.\n| &boxv;&nbsp; &boxvr;&nbsp; celeba_ablation_nostyle.yaml | Config file for CelebA 128x128 dataset for ablation study (no styles).\n| &boxv;&nbsp; &boxvr;&nbsp; celeba_ablation_separate.yaml | Config file for CelebA 128x128 dataset for ablation study (separate encoder and discriminator).\n| &boxv;&nbsp; &boxvr;&nbsp; celeba_ablation_z_reg.yaml | Config file for CelebA 128x128 dataset for ablation study (regress in Z space, not W).\n| &boxv;&nbsp; &boxvr;&nbsp; ffhq.yaml | Config file for FFHQ dataset at 1024x1024 resolution.\n| &boxv;&nbsp; &boxvr;&nbsp; mnist.yaml | Config file for MNIST dataset using Style architecture.\n| &boxv;&nbsp; &boxur;&nbsp; mnist_fc.yaml | Config file for MNIST dataset using only fully connected layers (Permutation Invariant MNIST).\n| &boxvr;&nbsp; dataset_preparation | Folder with scripts for dataset preparation.\n| &boxv;&nbsp; &boxvr;&nbsp; prepare_celeba_hq_tfrec.py | To prepare TFRecords for CelebA-HQ dataset at 256x256 resolution.\n| &boxv;&nbsp; &boxvr;&nbsp; prepare_celeba_tfrec.py | To prepare TFRecords for CelebA dataset at 128x128 resolution.\n| &boxv;&nbsp; &boxvr;&nbsp; prepare_mnist_tfrec.py | To prepare TFRecords for MNIST dataset.\n| &boxv;&nbsp; &boxvr;&nbsp; split_tfrecords_bedroom.py | To split official TFRecords from StyleGAN paper for LSUN bedroom dataset.\n| &boxv;&nbsp; &boxur;&nbsp; split_tfrecords_ffhq.py | To split official TFRecords from StyleGAN paper for FFHQ dataset.\n| &boxvr;&nbsp; dataset_samples | Folder with sample inputs for different datasets. Used for figures and for test inputs during training.\n| &boxvr;&nbsp; make_figures | Scripts for making various figures.\n| &boxvr;&nbsp; metrics | Scripts for computing metrics.\n| &boxvr;&nbsp; principal_directions | Scripts for computing principal direction vectors for various attributes. **For interactive demo**.\n| &boxvr;&nbsp; style_mixing | Sample inputs and script for producing style-mixing figures.\n| &boxvr;&nbsp; training_artifacts | Default place for saving checkpoints/sample outputs/plots.\n| &boxv;&nbsp; &boxur;&nbsp; download_all.py | Script for downloading all pretrained models.\n| &boxvr;&nbsp; interactive_demo.py | Runnable script for interactive demo.\n| &boxvr;&nbsp; train_alae.py | Runnable script for training.\n| &boxvr;&nbsp; train_alae_separate.py | Runnable script for training for ablation study (separate encoder and discriminator).\n| &boxvr;&nbsp; checkpointer.py | Module for saving/restoring model weights, optimizer state and loss history.\n| &boxvr;&nbsp; custom_adam.py | Customized adam optimizer for learning rate equalization and zero second beta.\n| &boxvr;&nbsp; dataloader.py | Module with dataset classes, loaders, iterators, etc.\n| &boxvr;&nbsp; defaults.py | Definition for config variables with default values.\n| &boxvr;&nbsp; launcher.py | Helper for running multi-GPU, multiprocess training. Sets up config and logging.\n| &boxvr;&nbsp; lod_driver.py | Helper class for managing growing/stabilizing network.\n| &boxvr;&nbsp; lreq.py | Custom `Linear`, `Conv2d` and `ConvTranspose2d` modules for learning rate equalization.\n| &boxvr;&nbsp; model.py | Module with high-level model definition.\n| &boxvr;&nbsp; model_separate.py | Same as above, but for ablation study.\n| &boxvr;&nbsp; net.py | Definition of all network blocks for multiple architectures.\n| &boxvr;&nbsp; registry.py | Registry of network blocks for selecting from config file.\n| &boxvr;&nbsp; scheduler.py | Custom schedulers with warm start and aggregating several optimizers.\n| &boxvr;&nbsp; tracker.py | Module for plotting losses.\n| &boxur;&nbsp; utils.py | Decorator for async call, decorator for caching, registry for network blocks.\n\n\n#### Configs\n\nIn this codebase [**yacs**](https://github.com/rbgirshick/yacs) is used to handle configurations. \n\nMost of the runnable scripts accept `-c` parameter that can specify config files to use.\nFor example, to make reconstruction figures, you can run:\n\n    python make_figures/make_recon_figure_paged.py\n    python make_figures/make_recon_figure_paged.py -c celeba\n    python make_figures/make_recon_figure_paged.py -c celeba-hq256\n    python make_figures/make_recon_figure_paged.py -c bedroom\n    \nThe Default config is `ffhq`.\n\n#### Datasets\n\nTraining is done using TFRecords. TFRecords are read using [DareBlopy](https://github.com/podgorskiy/DareBlopy), which allows using them with Pytorch.\n\nIn config files as well as in all preparation scripts, it is assumed that all datasets are in `/data/datasets/`. You can either change path in config files, either create a symlink to where you store datasets.\n\nThe official way of generating CelebA-HQ can be challenging. Please refer to this page: https://github.com/suvojit-0x55aa/celebA-HQ-dataset-download\nYou can get the pre-generated dataset from: https://drive.google.com/drive/folders/11Vz0fqHS2rXDb5pprgTjpD7S2BAJhi1P\n\n#### Pre-trained models\n\nTo download pre-trained models run:\n\n    python training_artifacts/download_all.py\n\n**Note**: There used to be problems with downloading models from Google Drive due to download limit. \nNow, the script is setup in a such way that if it fails to download data from Google Drive it will try to download it from S3.\n\nIf you experience problems, try deleting all *.pth files, updating *dlutils* package (`pip install dlutils --upgrade`) and then run `download_all.py` again.\nIf that does not solve the problem, please open an issue. Also, you can try downloading models manually from here: https://drive.google.com/drive/folders/1tsI1q1u8QRX5t7_lWCSjpniLGlNY-3VY?usp=sharing\n\n\nIn config files, `OUTPUT_DIR` points to where weights are saved to and read from. For example: `OUTPUT_DIR: training_artifacts/celeba-hq256`\n\nIn `OUTPUT_DIR` it saves a file `last_checkpoint` which contains path to the actual `.pth` pickle with model weight. If you want to test the model with a specific weight file, you can simply modify `last_checkpoint` file.\n\n\n## Generating figures\n\n#### Style-mixing\n\nTo generate style-mixing figures run:\n\n    python style_mixing/stylemix.py -c <config>\n    \nWhere instead of `<config>` put one of: `ffhq`, `celeba`, `celeba-hq256`, `bedroom`\n    \n\n#### Reconstructions\n\nTo generate reconstruction with multiple scale images:\n\n    python make_figures/make_recon_figure_multires.py -c <config>\n    \nTo generate reconstruction from all sample inputs on multiple pages:\n\n    python make_figures/make_recon_figure_paged.py -c <config>\n\nThere are also:\n\n    python make_figures/old/make_recon_figure_celeba.py\n    python make_figures/old/make_recon_figure_bed.py\n\nTo generate reconstruction from test set of FFHQ:\n\n    python make_figures/make_recon_figure_ffhq_real.py\n    \nTo generate interpolation figure:\n\n    python make_figures/make_recon_figure_interpolation.py -c <config>\n    \nTo generate traversals figure:\n\n(For datasets other then FFHQ, you will need to find principal directions first)\n\n    python make_figures/make_traversarls.py -c <config>\n    \n#### Generations\n\nTo make generation figure run:\n\n    make_generation_figure.py -c <config>\n\n## Training\n\nIn addition to installing required packages:\n\n    pip install -r requirements.txt\n\nYou will need to install [DareBlopy](https://github.com/podgorskiy/DareBlopy):\n\n    pip install dareblopy\n\nTo run training:\n\n    python train_alae.py -c <config>\n    \nIt will run multi-GPU training on all available GPUs. It uses `DistributedDataParallel` for parallelism. \nIf only one GPU available, it will run on single GPU, no special care is needed.\n\nThe recommended number of GPUs is 8. Reproducibility on a smaller number of GPUs may have issues. You might need to adjust the batch size in the config file depending on the memory size of the GPUs.\n\n## Running metrics\n\nIn addition to installing required packages and [DareBlopy](https://github.com/podgorskiy/DareBlopy), you need to install TensorFlow and dnnlib from StyleGAN.\n\nTensorflow must be of version `1.10`:\n\n    pip install tensorflow-gpu==1.10\n\nIt requires CUDA version 9.0.\n\nPerhaps, the best way is to use Anaconda to handle this, but I prefer installing CUDA 9.0 from pop-os repositories (works on Ubuntu):\n\n```\nsudo echo \"deb http://apt.pop-os.org/proprietary bionic main\" | sudo tee -a /etc/apt/sources.list.d/pop-proprietary.list\nsudo apt-key adv --keyserver keyserver.ubuntu.com --recv-key 204DD8AEC33A7AFF\nsudo apt update\n\nsudo apt install system76-cuda-9.0\nsudo apt install system76-cudnn-9.0\n```\n\nThen just set `LD_LIBRARY_PATH` variable:\n\n```\nexport LD_LIBRARY_PATH=/usr/lib/cuda-9.0/lib64\n```\n\nDnnlib is a package used in StyleGAN. You can install it with:\n\n    pip install https://github.com/podgorskiy/dnnlib/releases/download/0.0.1/dnnlib-0.0.1-py3-none-any.whl\n \nAll code for running metrics is heavily based on those from StyleGAN repository. It also uses the same pre-trained models:\n\n[https://github.com/NVlabs/stylegan#licenses](https://github.com/NVlabs/stylegan#licenses)\n\n> inception_v3_features.pkl and inception_v3_softmax.pkl are derived from the pre-trained Inception-v3 network by Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. The network was originally shared under Apache 2.0 license on the TensorFlow Models repository.\n> \n> vgg16.pkl and vgg16_zhang_perceptual.pkl are derived from the pre-trained VGG-16 network by Karen Simonyan and Andrew Zisserman. The network was originally shared under Creative Commons BY 4.0 license on the Very Deep Convolutional Networks for Large-Scale Visual Recognition project page.\n> \n> vgg16_zhang_perceptual.pkl is further derived from the pre-trained LPIPS weights by Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The weights were originally shared under BSD 2-Clause \"Simplified\" License on the PerceptualSimilarity repository.\n\nFinally, to run metrics:\n\n    python metrics/fid.py -c <config>       # FID score on generations\n    python metrics/fid_rec.py -c <config>   # FID score on reconstructions\n    python metrics/ppl.py -c <config>       # PPL score on generations\n    python metrics/lpips.py -c <config>     # LPIPS score of reconstructions\n "
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* Stanislav Pidhorskyi, Donald A. Adjeroh, and Gianfranco Doretto. Adversarial Latent Autoencoders. In *Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)*, 2020. [to appear] \n>\n\n    @InProceedings{pidhorskyi2020adversarial,\n     author   = {Pidhorskyi, Stanislav and Adjeroh, Donald A and Doretto, Gianfranco},\n     booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)},\n     title    = {Adversarial Latent Autoencoders},\n     year     = {2020},\n     note     = {[to appear]},\n    }\n<h4 align=\"center\">preprint on arXiv: <a href=\"https://arxiv.org/abs/2004.04467\">2004.04467</a></h4>\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@InProceedings{pidhorskyi2020adversarial,\n author   = {Pidhorskyi, Stanislav and Adjeroh, Donald A and Doretto, Gianfranco},\n booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)},\n title    = {Adversarial Latent Autoencoders},\n year     = {2020},\n note     = {[to appear]},\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/PaYo90/Copy-of-ALAE",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-07-01T19:31:07Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-07-02T11:51:32Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9942701959576825
      ],
      "excerpt": "Abstract: Autoencoder networks are unsupervised approaches aiming at combining generative and representational properties by learning simultaneously an encoder-generator map. Although studied extensively, the issues of whether they have the same generative power of GANs, or learn disentangled representations, have not been fully addressed. We introduce an autoencoder that tackles these issues jointly, which we call Adversarial Latent Autoencoder (ALAE). It is a general architecture that can leverage recent improvements on GAN training procedures. We designed two autoencoders: one based on a MLP encoder, and another based on a StyleGAN generator, which we call StyleALAE. We verify the disentanglement properties of both architectures. We show that StyleALAE can not only generate 1024x1024 face images with comparable quality of StyleGAN, but at the same resolution can also produce face reconstructions and manipulations based on real images. This makes ALAE the first autoencoder able to compare with, and go beyond, the capabilities of a generator-only type of architecture. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8592804432970454
      ],
      "excerpt": "| &boxvr;&nbsp; model.py | Module with high-level model definition. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9172884648772671
      ],
      "excerpt": "| &boxvr;&nbsp; net.py | Definition of all network blocks for multiple architectures. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.899869113587047
      ],
      "excerpt": "In this codebase yacs is used to handle configurations.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8813454725181993
      ],
      "excerpt": "Training is done using TFRecords. TFRecords are read using DareBlopy, which allows using them with Pytorch. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.838633435201321
      ],
      "excerpt": "To generate reconstruction from all sample inputs on multiple pages: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8338388324366361
      ],
      "excerpt": "There are also: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.874961822842629
      ],
      "excerpt": "The recommended number of GPUs is 8. Reproducibility on a smaller number of GPUs may have issues. You might need to adjust the batch size in the config file depending on the memory size of the GPUs. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/PaYo90/Copy-of-ALAE/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Fri, 24 Dec 2021 05:15:17 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/PaYo90/Copy-of-ALAE/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "PaYo90/Copy-of-ALAE",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8168939622155842
      ],
      "excerpt": "For example, to make reconstruction figures, you can run: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8369521649334798
      ],
      "excerpt": "You can get the pre-generated dataset from: https://drive.google.com/drive/folders/11Vz0fqHS2rXDb5pprgTjpD7S2BAJhi1P \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8962913185503505
      ],
      "excerpt": "If you experience problems, try deleting all .pth files, updating dlutils* package (pip install dlutils --upgrade) and then run download_all.py again. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9460543202309338,
        0.9979947896609701,
        0.9898877303533713,
        0.999746712887969
      ],
      "excerpt": "In addition to installing required packages: \npip install -r requirements.txt \nYou will need to install DareBlopy: \npip install dareblopy \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.945736137303317
      ],
      "excerpt": "If only one GPU available, it will run on single GPU, no special care is needed. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8403206284773848
      ],
      "excerpt": "| &boxvr;&nbsp; configs | Folder with yaml config files. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8414348257328301
      ],
      "excerpt": "| &boxv;&nbsp; &boxur;&nbsp; download_all.py | Script for downloading all pretrained models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8182439640973979
      ],
      "excerpt": "| &boxvr;&nbsp; train_alae.py | Runnable script for training. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.802775577662223
      ],
      "excerpt": "| &boxvr;&nbsp; defaults.py | Definition for config variables with default values. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091,
        0.9246227682586091,
        0.9246227682586091,
        0.9246227682586091
      ],
      "excerpt": "python make_figures/make_recon_figure_paged.py \npython make_figures/make_recon_figure_paged.py -c celeba \npython make_figures/make_recon_figure_paged.py -c celeba-hq256 \npython make_figures/make_recon_figure_paged.py -c bedroom \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8681606697325421,
        0.9246227682586091
      ],
      "excerpt": "To download pre-trained models run: \npython training_artifacts/download_all.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8185320969962121
      ],
      "excerpt": "In OUTPUT_DIR it saves a file last_checkpoint which contains path to the actual .pth pickle with model weight. If you want to test the model with a specific weight file, you can simply modify last_checkpoint file. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8411624709648624,
        0.8411624709648624
      ],
      "excerpt": "python make_figures/old/make_recon_figure_celeba.py \npython make_figures/old/make_recon_figure_bed.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python make_figures/make_recon_figure_ffhq_real.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8571102419564544
      ],
      "excerpt": "To run training: \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/PaYo90/Copy-of-ALAE/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "ALAE",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Copy-of-ALAE",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "PaYo90",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/PaYo90/Copy-of-ALAE/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To run the demo, you will need to have a CUDA capable GPU, PyTorch >= v1.3.1 and cuda/cuDNN drivers installed.\nInstall the required packages:\n\n    pip install -r requirements.txt\n  \nDownload pre-trained models:\n\n    python training_artifacts/download_all.py\n\nRun the demo:\n\n    python interactive_demo.py\n\nYou can specify **yaml** config to use. Configs are located here: https://github.com/podgorskiy/ALAE/tree/master/configs.\nBy default, it uses one for FFHQ dataset.\nYou can change the config using `-c` parameter. To run on `celeb-hq` in 256x256 resolution, run:\n\n    python interactive_demo.py -c celeba-hq256\n\nHowever, for configs other then FFHQ, you need to obtain new principal direction vectors for the attributes.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "The code in the repository is organized in such a way that all scripts must be run from the root of the repository.\nIf you use an IDE (e.g. PyCharm or Visual Studio Code), just set *Working Directory* to point to the root of the repository.\n\nIf you want to run from the command line, then you also need to set **PYTHONPATH** variable to point to the root of the repository.\n\nFor example, let's say we've cloned repository to *~/ALAE* directory, then do:\n\n    $ cd ~/ALAE\n    $ export PYTHONPATH=$PYTHONPATH:$(pwd)\n\n![pythonpath](https://podgorskiy.com/static/pythonpath.svg)\n\nNow you can run scripts as follows:\n\n    $ python style_mixing/stylemix.py\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "In addition to installing required packages and [DareBlopy](https://github.com/podgorskiy/DareBlopy), you need to install TensorFlow and dnnlib from StyleGAN.\n\nTensorflow must be of version `1.10`:\n\n    pip install tensorflow-gpu==1.10\n\nIt requires CUDA version 9.0.\n\nPerhaps, the best way is to use Anaconda to handle this, but I prefer installing CUDA 9.0 from pop-os repositories (works on Ubuntu):\n\n```\nsudo echo \"deb http://apt.pop-os.org/proprietary bionic main\" | sudo tee -a /etc/apt/sources.list.d/pop-proprietary.list\nsudo apt-key adv --keyserver keyserver.ubuntu.com --recv-key 204DD8AEC33A7AFF\nsudo apt update\n\nsudo apt install system76-cuda-9.0\nsudo apt install system76-cudnn-9.0\n```\n\nThen just set `LD_LIBRARY_PATH` variable:\n\n```\nexport LD_LIBRARY_PATH=/usr/lib/cuda-9.0/lib64\n```\n\nDnnlib is a package used in StyleGAN. You can install it with:\n\n    pip install https://github.com/podgorskiy/dnnlib/releases/download/0.0.1/dnnlib-0.0.1-py3-none-any.whl\n \nAll code for running metrics is heavily based on those from StyleGAN repository. It also uses the same pre-trained models:\n\n[https://github.com/NVlabs/stylegan#licenses](https://github.com/NVlabs/stylegan#licenses)\n\n> inception_v3_features.pkl and inception_v3_softmax.pkl are derived from the pre-trained Inception-v3 network by Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. The network was originally shared under Apache 2.0 license on the TensorFlow Models repository.\n> \n> vgg16.pkl and vgg16_zhang_perceptual.pkl are derived from the pre-trained VGG-16 network by Karen Simonyan and Andrew Zisserman. The network was originally shared under Creative Commons BY 4.0 license on the Very Deep Convolutional Networks for Large-Scale Visual Recognition project page.\n> \n> vgg16_zhang_perceptual.pkl is further derived from the pre-trained LPIPS weights by Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The weights were originally shared under BSD 2-Clause \"Simplified\" License on the PerceptualSimilarity repository.\n\nFinally, to run metrics:\n\n    python metrics/fid.py -c <config>       ",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Fri, 24 Dec 2021 05:15:17 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To run the demo, you will need to have a CUDA capable GPU, PyTorch >= v1.3.1 and cuda/cuDNN drivers installed.\nInstall the required packages:\n\n    pip install -r requirements.txt\n  \nDownload pre-trained models:\n\n    python training_artifacts/download_all.py\n\nRun the demo:\n\n    python interactive_demo.py\n\nYou can specify **yaml** config to use. Configs are located here: https://github.com/podgorskiy/ALAE/tree/master/configs.\nBy default, it uses one for FFHQ dataset.\nYou can change the config using `-c` parameter. To run on `celeb-hq` in 256x256 resolution, run:\n\n    python interactive_demo.py -c celeba-hq256\n\nHowever, for configs other then FFHQ, you need to obtain new principal direction vectors for the attributes.\n\n",
      "technique": "Header extraction"
    }
  ]
}