{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\n@software{eval-harness,\n  author       = {Gao, Leo and\n                  Tow, Jonathan and\n                  Biderman, Stella and\n                  Black, Sid and\n                  DiPofi, Anthony and\n                  Foster, Charles and\n                  Golding, Laurence and\n                  Hsu, Jeffrey and\n                  McDonell, Kyle and\n                  Muennighoff, Niklas and\n                  Phang, Jason and\n                  Reynolds, Laria and\n                  Tang, Eric and\n                  Thite, Anish and\n                  Wang, Ben and\n                  Wang, Kevin and\n                  Zou, Andy},\n  title        = {A framework for few-shot language model evaluation},\n  month        = sep,\n  year         = 2021,\n  publisher    = {Zenodo},\n  version      = {v0.0.1},\n  doi          = {10.5281/zenodo.5371628},\n  url          = {https://doi.org/10.5281/zenodo.5371628}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@software{eval-harness,\n  author       = {Gao, Leo and\n                  Tow, Jonathan and\n                  Biderman, Stella and\n                  Black, Sid and\n                  DiPofi, Anthony and\n                  Foster, Charles and\n                  Golding, Laurence and\n                  Hsu, Jeffrey and\n                  McDonell, Kyle and\n                  Muennighoff, Niklas and\n                  Phang, Jason and\n                  Reynolds, Laria and\n                  Tang, Eric and\n                  Thite, Anish and\n                  Wang, Ben and\n                  Wang, Kevin and\n                  Zou, Andy},\n  title        = {A framework for few-shot language model evaluation},\n  month        = sep,\n  year         = 2021,\n  publisher    = {Zenodo},\n  version      = {v0.0.1},\n  doi          = {10.5281/zenodo.5371628},\n  url          = {https://doi.org/10.5281/zenodo.5371628}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8356013927728488,
        0.8356013927728488
      ],
      "excerpt": "|wmt16-de-en                                              |     |   |\u2713   |         2999|bleu, chrf, ter                                                               | \n|wmt16-en-de                                              |     |   |\u2713   |         2999|bleu, chrf, ter                                                               | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488,
        0.8356013927728488
      ],
      "excerpt": "|wmt20-de-en                                              |     |   |\u2713   |          785|bleu, chrf, ter                                                               | \n|wmt20-de-fr                                              |     |   |\u2713   |         1619|bleu, chrf, ter                                                               | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "|wmt20-en-de                                              |     |   |\u2713   |         1418|bleu, chrf, ter                                                               | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "|wmt20-fr-de                                              |     |   |\u2713   |         1619|bleu, chrf, ter                                                               | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "    --num_examples 10 \\ \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/EleutherAI/lm-evaluation-harness",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-08-28T00:09:15Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-28T10:26:10Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9415282663979148
      ],
      "excerpt": "This project provides a unified framework to test autoregressive language models (GPT-2, GPT-3, GPTNeo, etc) on a large number of different evaluation tasks. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8885473523803654
      ],
      "excerpt": "Support for GPT-2, GPT-3, GPT-Neo, GPT-NeoX, and GPT-J, with flexible tokenization-agnostic interface \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9615067107009311
      ],
      "excerpt": "To implement a new task in eval harness, see this guide. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8329764299472963
      ],
      "excerpt": "Additional arguments can be provided to the model constructor using the --model_args flag. Most importantly, the gpt2 model can be used to load an arbitrary HuggingFace model as follows: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "    --model gpt2 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8997144715488361,
        0.8075331406717821
      ],
      "excerpt": "There are two major components of the library: \nLMs (language models), e.g. GPT-2, GPT-3 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8614722436677689,
        0.8301886399591762,
        0.9866549378790503,
        0.9534683630116263,
        0.8556629135183091,
        0.9536899247580196
      ],
      "excerpt": "Both LMs (lm_eval.models) and Tasks (lm_eval.tasks) are kept in a registry data structure, for easy CLI instantiation. \nIf you want to extend either models or tasks, simply add a new LM or Task subclass, and decorate with the registry decorator. \nThe GPT-3 Evaluations Project tracks our progress implementing new tasks. Right now, we are focused on getting all the datasets loaded so that we can dedupe against the training data. Implementing the actual evaluations is nice but not necessary at the current moment. \nTo help improve reproducibility, all tasks have a VERSION field. When run from the command line, this is reported in a column in the table, or in the \"version\" field in the evaluator return dict. The purpose of the version is so that if the task definition changes (i.e to fix a bug), then we can know exactly which metrics were computed using the old buggy implementation to avoid unfair comparisons. To enforce this, there are unit tests that make sure the behavior of all tests remains the same as when they were first implemented. Task versions start at 0, and each time a breaking change is made, the version is incremented by one.  \nWhen reporting eval harness results, please also report the version of each task. This can be done either with a separate column in the table, or by reporting the task name with the version appended as such: taskname-v0. \nGiven an LM, we want to evaluate it on a wide range of NLU tasks. We should at least cover the set of tasks in the GPT-3 paper, and any other tasks/benchmarks that are relevant. We will follow the GPT-3 format of a) zero-shot, b) one-shot, c) few-shot evaluation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9440614983834459
      ],
      "excerpt": "* Data downloader (shared with later sections, potentially needs to be directly linked to the latter 2 components) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9518465559111555,
        0.9317494039648401,
        0.9010372279797147,
        0.8669512533182515,
        0.8080808419792455,
        0.8394737202653922
      ],
      "excerpt": "* We should heavily rely on Hugging Face's NLP for this. They are already doing most of the work with handling data scripts/caching. \n* Optionally, we can rely directly on HF-NLP's caching, but that makes it awkward to handle non-HF-NLP datasets. Otherwise, we can just write them out to .jsonl. My feeling is that NLU data storage will be a drop in the bucket compared to LM data. \n* Where we're not using HF-NLP, we can keep the data in the raw format (.jsonl, tsv, etc) and let the other components handle transforming it. \nThe task formatter formats the task input data into an LM-usable format. \n* We should potentially support multiple formats for a given task, e.g. some formats may be better or worse suited for LM evaluation. See also: prompt-engineering \n* The task formatter should also support zero/one/few-shot packing of training examples into an input. This may require weird interactions with the tokenizer for dealing with max-token issues. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9362711325750073
      ],
      "excerpt": "An alternative approach is to collect the output logits and score them against the expected set of outputs. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8131750817606678
      ],
      "excerpt": "* Will thus likely have to be closely tied with the formatter. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A framework for few-shot evaluation of autoregressive language models.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/EleutherAI/lm-evaluation-harness/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 54,
      "date": "Wed, 29 Dec 2021 00:13:58 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/EleutherAI/lm-evaluation-harness/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "EleutherAI/lm-evaluation-harness",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This part is the easiest. I guess we just write out some text files containing the training data? We can let the usual LM preprocessing pipeline handle it from there.\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "With the data downloader in place, we simply need to (1) expose the val/test examples, and (2) remove them from the training set.\n\n* Arguably, (2) should be handled by LM preprocessing in a more general way. There are probably non-NLU-eval cases where we want to remove some specific data from training.\n* Depending on how exactly we do the val/test removal, we may want to format the same example multiple ways to ensure that they don't get leaked into the training set in a slightly tweaked format.\n* Thought experiment: SQuAD is based largely on Wikipedia. What exactly would we want to remove from the LM?\n* [GPT-3]: In GPT-3, they attempted to remove val/test from their LM set, but there was a bug that caused leakage. So they ended up doing the opposite: removing overlaps from the LM set from the val/test. Funky.\n* [GPT-3]: See page 30 and Appendix C for details. They do some funky n-gram based search and removal. We should think about whether we want to follow their protocol exactly\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```bash\npip install lm-eval\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9076023395751326
      ],
      "excerpt": "To inspect what the LM inputs look like, you can run the following command: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8634240051404037
      ],
      "excerpt": "|                    Task Name                            |Train|Val|Test|Val/Test Docs|                                   Metrics                                    | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9333384803827206
      ],
      "excerpt": "python main.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python write_out.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8610795118836423,
        0.8954663190316693
      ],
      "excerpt": "    --output_base_path /path/to/output/folder \nThis will write out one text file for each task. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/EleutherAI/lm-evaluation-harness/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "C++"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 EleutherAI\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Language Model Evaluation Harness",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "lm-evaluation-harness",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "EleutherAI",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/EleutherAI/lm-evaluation-harness/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "leogao2",
        "body": "",
        "dateCreated": "2021-09-01T22:57:52Z",
        "datePublished": "2021-09-02T02:28:08Z",
        "html_url": "https://github.com/EleutherAI/lm-evaluation-harness/releases/tag/v0.0.1",
        "name": "",
        "tag_name": "v0.0.1",
        "tarball_url": "https://api.github.com/repos/EleutherAI/lm-evaluation-harness/tarball/v0.0.1",
        "url": "https://api.github.com/repos/EleutherAI/lm-evaluation-harness/releases/48858622",
        "zipball_url": "https://api.github.com/repos/EleutherAI/lm-evaluation-harness/zipball/v0.0.1"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 169,
      "date": "Wed, 29 Dec 2021 00:13:58 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To evaluate a model, (e.g. GPT-2) on NLU tasks (e.g. LAMBADA, HellaSwag), you can run the following command.\n\n```bash\npython main.py \\\n\t--model gpt2 \\\n\t--device cuda:0 \\\n\t--tasks lambada,hellaswag\n```\n(This uses gpt2-117M by default as per HF defaults, use --model_args to specify other gpt2 sizes)\n\nAdditional arguments can be provided to the model constructor using the `--model_args` flag. Most importantly, the `gpt2` model can be used to load an arbitrary HuggingFace model. For example, to run GPTNeo use the following:\n\n```bash\npython main.py \\\n\t--model gpt2 \\\n\t--model_args pretrained=EleutherAI/gpt-neo-2.7B \\\n\t--device cuda:0 \\\n\t--tasks lambada,hellaswag\n```\n\nIf you have access to the OpenAI API, you can also evaluate GPT-3:\n\n```bash\nexport OPENAI_API_SECRET_KEY=YOUR_KEY_HERE\npython main.py \\\n\t--model gpt3 \\\n\t--model_args engine=davinci \\\n\t--tasks lambada,hellaswag\n```\n\nTo evaluate mesh-transformer-jax models that are not available on HF, please invoke eval harness through [this script](https://github.com/kingoflolz/mesh-transformer-jax/blob/master/eval_harness.py).\n\n",
      "technique": "Header extraction"
    }
  ]
}