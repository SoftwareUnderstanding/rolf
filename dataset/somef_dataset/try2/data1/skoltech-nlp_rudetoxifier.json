{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2105.09052",
      "https://arxiv.org/abs/1810.04805",
      "https://arxiv.org/abs/2105.09052},\n  archivePrefix = {arXiv},\n  eprint    = {2105.09052},\n  timestamp = {Mon, 31 May 2021 16:16:57 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2105-09052.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n\n***\n\n## Contacts\n\nFor any questions please contact Daryna Dementieva via [email](mailto:daryna.dementieva@skoltech.ru"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find this repository helpful, feel free to cite our publication:\n\n```\n@article{DBLP:journals/corr/abs-2105-09052,\n  author    = {Daryna Dementieva and\n               Daniil Moskovskiy and\n               Varvara Logacheva and\n               David Dale and\n               Olga Kozlova and\n               Nikita Semenov and\n               Alexander Panchenko},\n  title     = {Methods for Detoxification of Texts for the Russian Language},\n  journal   = {CoRR},\n  volume    = {abs/2105.09052},\n  year      = {2021},\n  url       = {https://arxiv.org/abs/2105.09052},\n  archivePrefix = {arXiv},\n  eprint    = {2105.09052},\n  timestamp = {Mon, 31 May 2021 16:16:57 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2105-09052.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n\n***\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{DBLP:journals/corr/abs-2105-09052,\n  author    = {Daryna Dementieva and\n               Daniil Moskovskiy and\n               Varvara Logacheva and\n               David Dale and\n               Olga Kozlova and\n               Nikita Semenov and\n               Alexander Panchenko},\n  title     = {Methods for Detoxification of Texts for the Russian Language},\n  journal   = {CoRR},\n  volume    = {abs/2105.09052},\n  year      = {2021},\n  url       = {https://arxiv.org/abs/2105.09052},\n  archivePrefix = {arXiv},\n  eprint    = {2105.09052},\n  timestamp = {Mon, 31 May 2021 16:16:57 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2105-09052.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "|Delete   |0.27   |0.96   |0.85   |0.81   |263.55   |0.10 \u00b1 0.0007   | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9593299683604384
      ],
      "excerpt": "|zero-shot   |0.93   |0.20   |0.00   |0.00   |159.11   |0.10 \u00b1 0.0005   | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "|DeepPavlov fine-tuned   |0.52   |0.86   |0.51   |0.53   |246.68   |0.12 \u00b1 0.0007   | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/skoltech-nlp/rudetoxifier",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "For any questions please contact Daryna Dementieva via [email](mailto:daryna.dementieva@skoltech.ru) or [Telegram](https://t.me/dementyeva_ds).\n",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-05-13T10:07:38Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-16T20:58:02Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9918414297534485
      ],
      "excerpt": "This repository contains models and evaluation methodology for the detoxification task of Russian texts. The original paper \"Methods for Detoxification of Texts for the Russian Language\" was presented at Dialogue-2021 conference. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9166157466044016,
        0.8326814773285598,
        0.8889238761379683,
        0.878279870158917,
        0.9979169814788843,
        0.9774576972151204,
        0.9423068529889035,
        0.9552208260577076,
        0.936792648171958,
        0.9600672186408208
      ],
      "excerpt": "Duplicate: simple duplication of the input; \nDelete: removal of rude and toxic from pre-defined vocab; \nRetrieve: retrieval based on cosine similarity between word embeddings from non-toxic part of RuToxic dataset; \nBased on ruGPT models. This method requires parallel dataset for training. We tested ruGPT-small, ruGPT-medium, and ruGPT-large models in several setups: \n- zero-shot: the model is taken as is (with no fine-tuning). The input is a toxic sentence which we would like to detoxify prepended with the prefix \u201c\u041f\u0435\u0440\u0435\u0444\u0440\u0430\u0437\u0438\u0440\u0443\u0439\u201d (rus. Paraphrase) and followed with the suffix \u201c>>>\u201d to indicate the paraphrasing task \n- few-shot: the model is taken as is. Unlike the previous scenario, we give a prefix consisting of a parallel dataset of toxic and neutral sentences. \n- fine-tuned: the model is fine-tuned for the paraphrasing task on a parallel dataset. \nBased on BERT model. This method does not require parallel dataset for training. One of the tasks on which original BERT was pretrained -- predicting the word that should was replaced with a [MASK] token -- suits delete-retrieve-generate style transfer method. We tested RuBERT and Geotrend pre-trained models in several setups: \n- zero-shot where BERT is taken as is (with no extra fine-tuning); \n- fine-tuned where BERT is fine-tuned on a dataset of toxic and safe sentences to acquire a style- \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9049019583017391,
        0.8593747434457786
      ],
      "excerpt": "The evaluation consists of three types of metrics: \n- style transfer accuracy (STA): accuracy based on toxic/non-toxic classifier (we suppose that the resulted text should be in non-toxic style) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8859084390164932,
        0.821595483358163,
        0.9146211392709562
      ],
      "excerpt": "  - BLEU: accuracy based on n-grams (1-4); \n  - cosine similarity (CS): between vectors of texts\u2019 embeddings. \n- language quality: perplexity (PPL) based on language model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.891516075056469
      ],
      "excerpt": "- data/train: RuToxic dataset, list of Russian rude words, and 200 samples of parallel sentences that were used for ruGPT fine-tuning; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Code and data of \"Methods for Detoxification of Texts for the Russian Language\" paper",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/skoltech-nlp/rudetoxifier/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Tue, 21 Dec 2021 23:10:00 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/skoltech-nlp/rudetoxifier/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "skoltech-nlp/rudetoxifier",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/skoltech-nlp/rudetoxifier/main/notebooks/rudetoxifier_inference.ipynb",
      "https://raw.githubusercontent.com/skoltech-nlp/rudetoxifier/main/models/condBERT/multiword/multiword-base.ipynb"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.8783202937825397
      ],
      "excerpt": "Folder data consists of all used train datasets, test data and naive example of style transfer result: \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/skoltech-nlp/rudetoxifier/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2021 Skolkovo Natural Language Processing Group\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Methods for Detoxification of Texts for the Russian Language (ruDetoxifier)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "rudetoxifier",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "skoltech-nlp",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/skoltech-nlp/rudetoxifier/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 20,
      "date": "Tue, 21 Dec 2021 23:10:00 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "style-transfer",
      "nlp",
      "russian-language"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "In this repository, we release two best models **detoxGPT** and **condBERT** (see [Methodology](https://github.com/skoltech-nlp/rudetoxifier#methodology) for more details). You can try detoxification inference example in this [notebook](https://github.com/skoltech-nlp/rudetoxifier/blob/main/notebooks/rudetoxifier_inference.ipynb) or [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1lSXh8PHGeKTLtuhxYCwHL74qG-V-pkLK?usp=sharing).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Also, you can test our models via [web-demo](https://detoxifier.nlp.zhores.net/) or you can pour out your anger on our [Telegram bot](https://t.me/rudetoxifierbot).\n\n***\n",
      "technique": "Header extraction"
    }
  ]
}