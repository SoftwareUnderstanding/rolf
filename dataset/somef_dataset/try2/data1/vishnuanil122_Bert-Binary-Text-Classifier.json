{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1511.01432",
      "https://arxiv.org/abs/1706.03762"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8450045395698996
      ],
      "excerpt": "Sentence B: he bought a gallon of milk . \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/vishnuverse/Bert-Binary-Text-Classifier",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-01-27T12:36:41Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-01-09T14:33:23Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9260829420234794,
        0.889627016979435,
        0.8667010742680953,
        0.9052415479384156
      ],
      "excerpt": "BERT is a method of pre-training language representations, meaning that we train \na general-purpose \"language understanding\" model on a large text corpus (like \nWikipedia), and then use that model for downstream NLP tasks that we care about \n(like question answering). BERT outperforms previous methods because it is the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9890779187192537,
        0.8842040420238866
      ],
      "excerpt": "is important because an enormous amount of plain text data is publicly available \non the web in many languages. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9723139865992518,
        0.911478160601604
      ],
      "excerpt": "instead generate a representation of each word that is based on the other words \nin the sentence. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.908925214220865
      ],
      "excerpt": "ELMo, and \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.939443406301941,
        0.9370335808561889,
        0.8303468561419067,
        0.9821092939480334
      ],
      "excerpt": "\u2014 but crucially these models are all unidirectional or shallowly \nbidirectional. This means that each word is only contextualized using the words \nto its left (or right). For example, in the sentence I made a bank deposit the \nunidirectional representation of bank is only based on I made a but not \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9627140808341351
      ],
      "excerpt": "\u2014 starting from the very bottom of a deep neural network, so it is deeply \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.961212187435223
      ],
      "excerpt": "BERT uses a simple approach for this: We mask out 15% of the words in the input, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9757941675443742
      ],
      "excerpt": "Input: the man went to the [MASK1] . he bought a [MASK2] of milk. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.880703162501244
      ],
      "excerpt": "In order to learn relationships between sentences, we also train on a simple \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8881000144538649
      ],
      "excerpt": "and B, is B the actual next sentence that comes after A, or just a random \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9389485061907631
      ],
      "excerpt": "Sentence B: he bought a gallon of milk . \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8235602494461995
      ],
      "excerpt": "(Wikipedia + BookCorpus) for a long time (1M \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9435232673675573,
        0.8768208426324942
      ],
      "excerpt": "Pre-training is fairly expensive (four days on 4 to 16 Cloud TPUs), but is a \none-time procedure for each language (current models are English-only, but \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9830242257273324
      ],
      "excerpt": "Fine-tuning is inexpensive. All of the results in the paper can be \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9076606631315258,
        0.9778112548726583,
        0.9922262786684682,
        0.9670263237455218
      ],
      "excerpt": "trained in around 30 minutes on a single Cloud TPU to achieve a Dev F1 score of \n91.0%, which is the single system state-of-the-art. \nThe other important aspect of BERT is that it can be adapted to many types of \nNLP tasks very easily. In the paper, we demonstrate state-of-the-art results on \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8450998151175391
      ],
      "excerpt": "(e.g., NER), and span-level (e.g., SQuAD) tasks with almost no task-specific \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Sms Spam Or Ham Prediction Using Bert",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/vishnuanil122/Bert-Binary-Text-Classifier/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3,
      "date": "Wed, 29 Dec 2021 06:35:28 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/vishnuverse/Bert-Binary-Text-Classifier/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "vishnuverse/Bert-Binary-Text-Classifier",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/vishnuanil122/Bert-Binary-Text-Classifier/master/bert_sms.ipynb",
      "https://raw.githubusercontent.com/vishnuanil122/Bert-Binary-Text-Classifier/master/vgg_augmented_batch_normalised_conv_added.ipynb",
      "https://raw.githubusercontent.com/vishnuanil122/Bert-Binary-Text-Classifier/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb"
    ],
    "technique": "File Exploration"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/vishnuverse/Bert-Binary-Text-Classifier/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# What is BERT?",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Bert-Binary-Text-Classifier",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "vishnuverse",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/vishnuverse/Bert-Binary-Text-Classifier/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Open bert_sms.ipynb in notebook\n2. Change folder names as needed\n3. clone https://github.com/google-research/bert.git\n4. Get Pretrained cased_L-12_H-768_A-12 using !wget https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip\n5. Create bert_output and data folder inside the cloned repository\n6. Inside Data folder save the dataset of in tsv format after splitting them as train.tsv, dev.tsv and val.tsv\n7. Run !python run_classifier.py --task_name=cola --do_train=true --do_eval=true --data_dir=./data --vocab_file=./model/vocab.txt --bert_config_file=./model/bert_config.json --init_checkpoint=./model/bert_model.ckpt --max_seq_length=64 --train_batch_size=2 --learning_rate=2e-5 --num_train_epochs=3.0 --output_dir=./bert_output/ --do_lower_case=False --save_checkpoints_steps 10000\"\n8. Inside output folder you could see eval_results.txt file which contains model evaluation results\n9. Following the training step you predict your data using folllwing command \n  !python run_classifier.py --task_name=cola --do_predict=true --data_dir=./data --vocab_file=./model/vocab.txt --bert_config_file=./model/bert_config.json --init_checkpoint=./bert_output/model.ckpt-4983 --max_seq_length=64 --output_dir=./bert_output/\n10. Above command will create a file called test_results.tsv inside bert_output folder which can be combined with test data using guid.\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Wed, 29 Dec 2021 06:35:28 GMT"
    },
    "technique": "GitHub API"
  }
}