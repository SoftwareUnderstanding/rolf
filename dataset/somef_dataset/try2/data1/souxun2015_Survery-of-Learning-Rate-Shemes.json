{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1608.03983\n\nWhen training a model, it is often recommended to lower the learning rate as the training progresses. This function applies a cosine decay function to a provided initial learning rate.\n\nglobal_step = min(global_step, decay_steps",
      "https://arxiv.org/abs/1608.03983\n\nWhen training a model, it is often recommended to lower the learning rate as the training progresses. This function applies a cosine decay function with restarts to a provided initial learning rate.\n\nThe function returns the decayed learning rate while taking into account possible warm restarts. The learning rate multiplier first decays from 1 to `alpha` for `first_decay_steps` steps. Then, a warm\nrestart is performed. Each new warm restart runs for `t_mul` times more steps and with `m_mul` times smaller initial learning rate.\n```\n- 7. linear_cosine_decay\n```\nSee [Bello et al., ICML2017] Neural Optimizer Search with RL.\nhttps://arxiv.org/abs/1709.07417\n\nFor the idea of warm starts here controlled by `num_periods`,\nsee [Loshchilov & Hutter, ICLR2016] SGDR: Stochastic Gradient Descent with Warm Restarts. https://arxiv.org/abs/1608.03983\nNote that linear cosine decay is more aggressive than cosine decay and larger initial learning rates can typically be used.\n```\n- 8. noisy_linear_cosine_decay\n```\nWhen training a model, it is often recommended to lower the learning rate as the training progresses.  This function applies a noisy linear cosine decay function to a provided initial learning rate.\n\nglobal_step = min(global_step, decay_steps",
      "https://arxiv.org/abs/1709.07417\n\nFor the idea of warm starts here controlled by `num_periods`,\nsee [Loshchilov & Hutter, ICLR2016] SGDR: Stochastic Gradient Descent with Warm Restarts. https://arxiv.org/abs/1608.03983\nNote that linear cosine decay is more aggressive than cosine decay and larger initial learning rates can typically be used.\n```\n- 8. noisy_linear_cosine_decay\n```\nWhen training a model, it is often recommended to lower the learning rate as the training progresses.  This function applies a noisy linear cosine decay function to a provided initial learning rate.\n\nglobal_step = min(global_step, decay_steps",
      "https://arxiv.org/abs/1608.03983\nNote that linear cosine decay is more aggressive than cosine decay and larger initial learning rates can typically be used.\n```\n- 8. noisy_linear_cosine_decay\n```\nWhen training a model, it is often recommended to lower the learning rate as the training progresses.  This function applies a noisy linear cosine decay function to a provided initial learning rate.\n\nglobal_step = min(global_step, decay_steps"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9706028454607681
      ],
      "excerpt": "See [Loshchilov & Hutter, ICLR2016], SGDR: Stochastic Gradient Descent with Warm Restarts. https://arxiv.org/abs/1608.03983 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9706028454607681
      ],
      "excerpt": "See [Loshchilov & Hutter, ICLR2016], SGDR: Stochastic Gradient Descent with Warm Restarts. https://arxiv.org/abs/1608.03983 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9706028454607681
      ],
      "excerpt": "see [Loshchilov & Hutter, ICLR2016] SGDR: Stochastic Gradient Descent with Warm Restarts. https://arxiv.org/abs/1608.03983 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/souxun2015/Survery-of-Learning-Rate-Shemes",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-11-04T19:11:21Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-03-02T20:43:17Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This project mainly introduces the learning rate schemes provided by tensorflow and observes their influences on convolutional neural networks. The problem about how they work is not included as it is difficult to explain. Maybe in the future, I will post it once I get them straight. So, there are 15 learning rate schemes we will talk about:\n- 1. exponential_decay\n- 2. piecewise_constant_decay\n- 3. polynominal_decay\n- 4. inverse_time_decay\n- 5. cosine_decay\n- 6. cosine_decay_restarts\n- 7. linear_cosine_decay\n- 8. noisy_linear_cosine_decay\n- 9. tf.train.GradientDescentOptimizer\n- 10. tf.train.MomentumOptimizer\n- 11. tf.train.AdamOptimizer // tf.train.AdagradOptimizer // tf.train.AdadeletaOptimizer // tf.train.AdagradDAOptimizer\n- 12. tf.train.RMSPropOptimizer\n- 13. tf.train.FtrlOptimizer\nWe conduct experiments on Cifar10 with these shemes, and then make analyses on different combinations among them.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8041100195203695
      ],
      "excerpt": "When training a model, it is often recommended to lower the learning rate as the training progresses.  This function applies an inverse decay function to a provided initial learning rate. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8442112514800088
      ],
      "excerpt": "When training a model, it is often recommended to lower the learning rate as the training progresses. This function applies a cosine decay function with restarts to a provided initial learning rate. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9152729049558601
      ],
      "excerpt": "restart is performed. Each new warm restart runs for t_mul times more steps and with m_mul times smaller initial learning rate. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8324655239109222
      ],
      "excerpt": "See [Bello et al., ICML2017] Neural Optimizer Search with RL. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.935164208671222
      ],
      "excerpt": "For the idea of warm starts here controlled by num_periods, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9166119569650895
      ],
      "excerpt": "Note that linear cosine decay is more aggressive than cosine decay and larger initial learning rates can typically be used. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8171963288271201
      ],
      "excerpt": "When training a model, it is often recommended to lower the learning rate as the training progresses.  This function applies a noisy linear cosine decay function to a provided initial learning rate. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9183427583784268
      ],
      "excerpt": "where eps_t is 0-centered gaussian noise with variance \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9696569678973246
      ],
      "excerpt": "This is  a great post that gives a comprehensive introduction to the optimizer schemes. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9616254554514974
      ],
      "excerpt": "This is original optimizer, the gradient is just based on the current batch. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9720519092663046,
        0.9602052349012334
      ],
      "excerpt": "This optimizer contains a momentum to update the gradients. It means that updating the gradient is relationed to the previous batches. \nIn the its inputs, there is a switch to control how to update the variables(original Momentum or Nesterov Momentum) \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/souxun2015/Survery-of-Learning-Rate-Shemes/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Mon, 27 Dec 2021 23:16:29 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/souxun2015/Survery-of-Learning-Rate-Shemes/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "souxun2015/Survery-of-Learning-Rate-Shemes",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/souxun2015/Survery-of-Learning-Rate-Shemes/master/execute.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8837680365796365
      ],
      "excerpt": ": python \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8936954105699045
      ],
      "excerpt": "- 9. tf.train.GradientDescentOptimizer \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9112887678150431
      ],
      "excerpt": "- 10. tf.train.MomentumOptimizer \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8707564687228986
      ],
      "excerpt": "- 11. tf.train.AdamOptimizer // tf.train.AdagradOptimizer // tf.train.AdadeletaOptimizer // tf.train.AdagradDAOptimizer \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/souxun2015/Survery-of-Learning-Rate-Shemes/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Survery-of-Learning-rate-shemes",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Survery-of-Learning-Rate-Shemes",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "souxun2015",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/souxun2015/Survery-of-Learning-Rate-Shemes/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Mon, 27 Dec 2021 23:16:29 GMT"
    },
    "technique": "GitHub API"
  }
}