{
  "citation": [
    {
      "confidence": [
        0.952096728950289
      ],
      "excerpt": "Implement Dueling DQN (modification to build_models): https://arxiv.org/pdf/1511.06581.pdf \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/abrhaleitela/Wednesday",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-08-15T14:00:23Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-08-15T14:00:54Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.839543877958897,
        0.9019078551230776
      ],
      "excerpt": "Deep reinforcement learning: Deep learning, meet reinforcement learning. \nCore idea is to train a deep Q network on Breakout Atari game. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.836273535046788,
        0.9911111787305787
      ],
      "excerpt": "In supervised learning (recall Monday's imitation learning), loss tells how accurate the network is. \nThis is not quite as straight-forward with Deep Q learning, but it still is a vital debugging tool to see \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9600993141542672
      ],
      "excerpt": "It is still hard to say if agent is improving to right direction or not. Implement more monitoring. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9608327560021472
      ],
      "excerpt": "For every episode, store the sum of all predicted Q-values and number of them (used to calculate average). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.949389842685218,
        0.8521616813824822
      ],
      "excerpt": "Not as trivial as setting all values in a table to specific value, since we work on networks. \nA simple and crude way to do this: Initialize weights (kernel) of the final layer (output layer) to zero and biases to one. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8810187707460357
      ],
      "excerpt": "See documentation for Dense layers for how to change initial values. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8701815487780791
      ],
      "excerpt": "Evaluate and enjoy the model after training. What are the subjective / objective results? \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8063964648279622,
        0.8654748984108295
      ],
      "excerpt": "Try reaching higher average reward by tuning the exploration and other parameters. \nWith some tinkering, you should be able to get to reliable 4.0 average reward in under 100k training steps. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8827234731584235
      ],
      "excerpt": "Visualize Q-values while enjoying using Matplotlib and interactive plots. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9541713156340587
      ],
      "excerpt": "Same as Breakout-v0, but with \"sticky actions\": With some probability, the next action is  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8310506733379902
      ],
      "excerpt": "Try the DQN code on different Atari game (e.g. Pong). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8075565805565561
      ],
      "excerpt": "Implement Double DQN (modification to update_model): http://www0.cs.ucl.ac.uk/staff/d.silver/web/Applications_files/doubledqn.pdf \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/abryeemessi/Wednesday/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sat, 25 Dec 2021 08:45:40 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/abrhaleitela/Wednesday/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "abrhaleitela/Wednesday",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.880530503895669,
        0.8992804999719691
      ],
      "excerpt": "Note: We will use deterministic version of Breakout environment. Literature often reports performance on stochastic version (more realistic task). \nStart by filling in required parts to run the code: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8791637619144624
      ],
      "excerpt": "Try the code on BreakoutNoFrameskip-v4 environment (set with --env argument). \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8233881373807276
      ],
      "excerpt": "Try training the agent with python atari_dqn.py --steps 3000 dummy_model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8352686704513214
      ],
      "excerpt": "Try training again with python atari_dqn.py --steps 3000 dummy_model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8256924409336649,
        0.8624576899006493
      ],
      "excerpt": "  of the episode in the print-out message. \nTrain agent with python atari_dqn.py --steps 2000 dummy_model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8135611064000302
      ],
      "excerpt": "Run agent with python atari_dqn.py --steps 2000 dummy_model again and see if loss seems more reasonable now \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8423340908216131
      ],
      "excerpt": "After each episode, print out the average reward from last 100 episodes \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.824932131786776,
        0.8665178318457835
      ],
      "excerpt": "Print average Q-value after every episode.  \nTry training again with python atari_dqn.py --steps 10000 dummy_model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8042186772420815
      ],
      "excerpt": "Try training agent for a longer time with python atari_dqn.py --steps 50000 proper_model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8806904187126966
      ],
      "excerpt": "You can enjoy your agent with python atari_dqn.py --evaluate --show --limit-fps proper_model \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/abrhaleitela/Wednesday/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Wednesday practicals",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Wednesday",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "abrhaleitela",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/abrhaleitela/Wednesday/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Only one file `atari_dqn.py` with different parameters. For full details command `python atari_dqn.py -h`\n\nExamples:\n\n* Train for 10 000 agent steps (calls to `env.step`): `python atari_dqn.py --steps 10000 path_where_to_store_model`\n* Enjoy trained model: `python atari_dqn.py --show --evaluate --limit-fps path_to_trained_model`\n* Evaluate trained model: `python atari_dqn.py --evaluate path_to_trained_model`\n* Store console output to a file `log.txt`: `python atari_dqn.py --log log.txt path_where_to_store_model`\n    * **Note: Once you start properly training agents, it is a good idea to store these logs for future reference**\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sat, 25 Dec 2021 08:45:40 GMT"
    },
    "technique": "GitHub API"
  }
}