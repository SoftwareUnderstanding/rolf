{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1505.04597",
      "https://arxiv.org/abs/1505.04597\n\n[2]: imgaug library https://imgaug.readthedocs.io/en/latest/\n\n[3]: Liang-Chieh Chen, George Papandreou, Florian Schroff, Hartwig Adam. \"Rethinking Atrous Convolution for Semantic Image Segmentation\". CVPR, 2017. https://arxiv.org/abs/1706.05587",
      "https://arxiv.org/abs/1706.05587"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[1]: Olaf Ronneberger, Philipp Fischer, Thomas Brox. \"U-Net: Convolutional Networks for Biomedical Image Segmentation\". CVPR, 2015. https://arxiv.org/abs/1505.04597\n\n[2]: imgaug library https://imgaug.readthedocs.io/en/latest/\n\n[3]: Liang-Chieh Chen, George Papandreou, Florian Schroff, Hartwig Adam. \"Rethinking Atrous Convolution for Semantic Image Segmentation\". CVPR, 2017. https://arxiv.org/abs/1706.05587\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "Institute: Universitat Politecnica De Catalu\u00f1a \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8444342525991423
      ],
      "excerpt": "| SGD (0.1) | Deeplabv3 | |Weather DA | 66.32| 17 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.881488679963663
      ],
      "excerpt": "|Trained with weather conditions|Pixel accuracy (%)| mIoU (%)| 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15| 16 | 17 | 18 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9745587028363859
      ],
      "excerpt": "| 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9287913210266059
      ],
      "excerpt": "| 0.80 | 0.64 | 0.84 | 0.25 | 0.12 | 0.39 | 0.32 | 0.49 | 0.87 | 0.44 | 0.86 | 0.43 | 0.21 | 0.83 | 0.11 | 0.22 | 0.00 | 0.07 | 0.50 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.943395218253048
      ],
      "excerpt": "| Adam (0.001) |  UNet | Transpose|DA & IF |75.14|35.09 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8444342525991423
      ],
      "excerpt": "| SGD (0.1) | Deeplabv3 | |Weather DA | 66.32| 17 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8444342525991423
      ],
      "excerpt": "| Adam (0.001) | Unet | Transpose|Weather DA |75.72 | 35 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/it6aidl/outdoorsegmentation",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-03-30T21:47:11Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-04-14T09:10:14Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9433596079527314
      ],
      "excerpt": "The repository is structured as follows: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8620386568278726
      ],
      "excerpt": " - toolbox - folder containing two auxiliar tools for working with the dataset: transforming the labels merging some of the classes, creating csv files for loading the images in the dataset and getting stats for class imbalance. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9877557747176124
      ],
      "excerpt": "Based on our team's collective interest in computer vision we decided to pursue a semantic segmentation task, using deep learning to classify objects in road scenarios to further the development of self-driving cars. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9834691270435048,
        0.9226420030905326
      ],
      "excerpt": "[x] Mitigate the class imbalance based on a better understanding of our data. \n[x] Learn how to do a transfer learning from the previous task to another one, for instance, detecting the drivable area. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9332501864039654
      ],
      "excerpt": "[x] Apply data augmentation, generate different kinds of weather such as fog, rain, snowflakes. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "[x] Apply data augmentation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.93984590805057
      ],
      "excerpt": "The Cityscapes dataset includes a diverse set of street scene image captures from 50 different cities around the world designed specifically for training segmentation models. The dataset includes semantic, instance-wise, and dense pixel annotation for a total of 30 classes. The dataset consists of 5,000 images at a resolution of 1024x2048. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9946666442348969,
        0.9434235495515073,
        0.955091680202802
      ],
      "excerpt": "This Dataset class is also prepared to apply different kind of transformations for data augmentation. These transformations can be applied only to the images or to both the image and the target, controlling that the random transformations are applied at the same time. In addition, this class includes the weather transformation used for ensure that the model is prepared to work in any climatological conditions using imgaug.   \nBy default, the loaded images are resized to 256x512 and converted to tensors during the transformation. The loaded targets are also resized to 256x512, with the interpolation parameter set to 0. This resizing has been done for reducing the training resources required. \nA snippet of the transformation code and data loaders is presented below, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9925697987849509,
        0.9483618682578875,
        0.8443468617097704,
        0.9498296210011666,
        0.9298717057330463,
        0.9530762153907857,
        0.9761604027894619,
        0.984994102591794,
        0.9121384148193742,
        0.9943202175086847,
        0.9697997456489682,
        0.9908218358728667,
        0.9849989388126225,
        0.984626189118866,
        0.9899722930159873,
        0.9917034565910018,
        0.8809688914234378
      ],
      "excerpt": "As suggested by the Cityscapes documentation, classes with a label id of 255 were emitted. Also, we used another tool to merge some of the classes with less representation, resulting in a total of 19 distinct classes: Road, Sidewalk, Building, Wall, Fence, Pole, Traffic Light, Traffic Sign, Vegetation, Terrain, Sky, Person, Rider, Car, Truck, Bus, Train, Motorcycle, and Bicycle. In order to know the kind of data we are working with, we created aux functions to calculate the statistics of the train and validation splits. This information was helpful to understand the class imbalance so that we can apply techniques to deal with it that we will explain above. \nThe U-net is a fully convolutional network created specifically for computer vision problems, including segmentation tasks. It became popular because of its efficient use of the GPU and its ability to learn with a limited dataset. What makes the U-net noble from other network architectures is that every pooling layer is mirrored by an up-sampling layer. The following figure shows the U-net contracting path (left side) and an expansive path (right side), both of which are symmetrically structured. \nThis allows the network to reduce spatial information while increasing feature information on its way down, and reduce feature information while increasing station information on the way up, leading to highly efficient image processing. \nSince the U-net downsamples the feature information in the first half of the network, there is a risk of loosing valuable information. To overcome this, we concatenated all the feature maps in the decoding layers with the feature maps from the encoding layers. This assures that any information learned in the ignitions layers will be retained throughout the network. \nIn order to recover the original input resolution at the output of the network, a bi-linear interpolation was performed. For bi-linear interpolation, a weighted average is calculated on the four nearest pixels to achieve a smooth output. The data was interpolated along 2-axis during upsampling, following the following formula, \nwhere f(x,y) is the unknown function, in our case pixel intensity, and the four nearest points being, \nTo improve the quality and efficiency of the upsampling, the bi-linear interpolation was replaced by transposed convolutions. These convolutions make use of learneable parameters to enable the network to \u201clearn\u201d how to best transform each feature map on its own. \nAs a way to test a different approach, and trying to improve the results, we tested a different model: DeepLabv3. Here, we picked the version already existing and pretrained in Torchvision. This architecture review how atrous convolutions are applied to extract dense features for semantic segmentation and how to use a combination of them in parallel under the concept of Atrous Spatial Pyramid Pooling. \nAn optimizer is necessary for minimizing the loss function. In this project both the Adaptive Moment Estimation (Adam) and Stochastic Gradient Descent (SGD) optimizers were tested. SGD calculates the gradient descent for each example, reducing batch redundancies, and improving computational efficiency. Adam is a mixture of the SGD and RMSprop optimizers, and offers an adaptive learning rate, increasing the network's flexibility. \nEvaluating and comparing the experiments is a nuclear part of the scientific work and opens the path to adjust parameters and propose changes. For this project we defined several metrics to compare models and trainings \nThe main metric we used to evaluate the experiments if the accuracy of the model configuration. The model prediction accuracy is calculated dividing the number of encerted pixels by the number of total pixels. However, there is a class that we are ignoring throughout the experiments and does not compute for the accuracy. This class represents objects in the images that are not useful for our purposes (thrash cans and other street objects) \nThe previous metric is a generalization of how our model works overall. This next one gives emphasis on the nature of the data. Intersection over Union (Jaccard score) is an effective, well known metric for pixel classification in object segmentation problems. The IoU score for each class is a number between 0 and 1 calculated by dividing the intersection from the pixels in prediction and GT by the union of these two surfaces. \nThe other metric that illuminated our grievous path through the fathomless darkness of semantic segmentation was the mean Intersection over Union. A mean calculation of every class IoU is used to measure how well is the model classificating all the classes. \nThroughout the process of building a strong model, multiple experiments were conducted in order to track progress. We stuck with the configuration that gave us better results until the moment and build on top of it. \nIn the very beginning, we decided to build an easy-to-code, lightweight model that worked for us to see a nice segmentation result easy to understand and easy to be improved by adding components to the configuration. It also gave some hints about the base results we could achieve. \nIt was intended as well to act as a base-line for the all future experiments. It consisted of a linear version (removing the concatenations) of the UNet. Easy as it was, such a model casts very little precision to the prediction. For this implementation, we decoder phase made use of torch.nn.Upsample to upsample the encoded features. \nAs an optimizer we chose Adam because it needs no additional tuning or adjust hyperparameters. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9871711203725799,
        0.9755332982527724,
        0.9704429133410503
      ],
      "excerpt": "For the second experiments, we improved the network to embrace the concatenations defined in the canonical net. After achieving a not so bad accuracy result of 75%, we moved on to reproduce the full UNet. This of course turned into computational and practical adjustments such as reducing the batch size and we had to wait longer for the experiment to give results. \nThe UNet as it was created in the paper will give us more precision in the predictions since it adds every phase result of the encoder to the decoder to produce the output. Increase of prediction is really evident. \nThe results, apart from the quantitative side, have shown a real qualitative effect in the sharpness of the predictions, as can be seen in the following comparison. In the left hand side we can see the predictions from the model with the concatenations whereas in the right hand side we can see the results of the architecture without these conections. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.872281797076174,
        0.889770489064545
      ],
      "excerpt": "As earlier said, we used torch.nn.UpSample as the component to upsample the encoded features. \nWe did modify the original experiment substituting the transposed convolutions for the convolutions Kernel size 1 in the last step. However, we wanted to test if using a wider kernel (3x3), it can keep better the spatial information for the feature extraction than the orginal 1x1 one. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9929059118030792
      ],
      "excerpt": "There are several other methods to perform the upsampling and we chose the Transposed convolutions. This generated new feature maps double sized in the decoder phases so we end up having the same output size. In practical, the accuracy rised a bit but it was not visible looking at the predictions. The results are shown in the following graph. As these were our better results, we keeped this configuration as our reference. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9720710113685703
      ],
      "excerpt": "For our next experiment and before introducing other techniques, we decided to change the optimizer to SGD to see how it performed. Comparing validation results we would keep Adam optimizer. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9887922709762337,
        0.9568578193934597
      ],
      "excerpt": "After stablishing the UNet version and optimizer that gave us better results, we could start experimenting other techniques to boost the model prediction. Altough this technique it is often used to reduce overfitting, have not suffered such, we wanted to test our net and see how this  affects accuracy in validation and test. The transformations done to the images comprehend random horizontal flips and modifications to brightness, contrast, saturation and hue. \nTraining the network with this technique makes it generalize better on new samples but it gave no better results. As a matter of fact, the test results, where we was expecting a change, suffered a decrease in both metrics. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8849735905824063
      ],
      "excerpt": "The following experiment was oriented to tackle the class imbalance mentioned above. Thus, using the number of pixels calculated for each class, eeights were added to the loss. The method is explained below: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9811478573424856,
        0.9654581806378193,
        0.9257516250862317
      ],
      "excerpt": "the inverted frequency is calculated as the inversion of the normalized number of pixels (normalizing by the total number of pixels). This is used to compensate the imbalance of the classes, as shown in the figure above. \nThe results of this experiment were not as good as expected. We also discarded to keep using this technique in our following experiments. \nThese results are obtained using the validation split \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9151870063699675,
        0.988773750683869,
        0.8192928388648606,
        0.8951112033765977,
        0.8915658754093994
      ],
      "excerpt": "In order for the network to prepare for varying road scenarios, in this experiment, it was trained while running the weather augmentation online to generate rain and snow. \nAfter running the data augmentation experiment and even though not having valuable results, we decided to include some realistic data augmentation. In our case, driving scenario, would be very helpful to add circumstances that drivers find on the daily. Of course, this should help the model to generalize better in exchange of a decreasing validation accuracy. \nThe photos were added a layer of one of these elements (rain, snow, clouds, fog) using python library imgaug. \nAn example of the effect of this transformations using Fog can be shown here \nAnd the results: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.964361196151919,
        0.9593247177537119
      ],
      "excerpt": "Next, based on the few improvements achieved, we decided to try a different approach, using an already existing model with pretrained weights. Also, this will give us the experience doing this kind of approach. Thus, as commented in the architecture section, we used DeepLabv3. \nWith the basic configuration we didn't achieve better results. This is the reason we keep trying things in the next experiments. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9180966882494598,
        0.9418062409975672
      ],
      "excerpt": "We know that in normal conditions Adam will perform better without fine-tuning the hyperparameters, but we wanted to try SGD to know how it will perform with DeepLabv3. \nThe results with SGD were better than using Adam, but not achieving the best results we had with U-net. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9861162586625052
      ],
      "excerpt": "At this point we hadn't tune the hyperparameters so we decided to explore how this could affect the configuration. We thought it could lead us to a steeper loss and accuracy curve, but, even though the curves were similar, it performed slightly better than the previous version of deeplabv3 using our classic learning rate 0.001. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9789156896609728
      ],
      "excerpt": "As the last experiment, we added the same weather data augmentation we performed on our UNet to deeplabv3. The experiment was very disappointing in the validation phase since accuracy dropped to the lowest, but that's the natural answer to data augmentation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9773861245787435
      ],
      "excerpt": "The model has been penalized in the validation dataset but will generalize better for new real world samples. If we compare the results of our best model (DeepLabv3 - SGD - lr=1e-1) that achieved a mIoU of 46% in Test, and this model trained with weather conditions, all against the Test split including weather conditions we can see this effect: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9943202175086847,
        0.9183089735606219,
        0.8691087690926034,
        0.8909439736153802
      ],
      "excerpt": "Evaluating and comparing the experiments is a nuclear part of the scientific work and opens the path to adjust parameters and propose changes. For this project we defined several metrics to compare models and trainings \nBelow it is shown the comparison among all the experiments: \nHere, we can see than the best experiment in terms of accur \nAs we presented in the dataset statistics, we have a noticeable class imbalance, which ends up in an unbalanced IoU. The classes that appear the most in the dataset (pavement, sky) reach a higher IoU than the ones that appear very few times (signals, traffic lights) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9427165905188817,
        0.9972495542564014
      ],
      "excerpt": "The class imbalance penalises the results, since we have several classes with an IoU of almost 0, but the rest of them achieves an acceptable result. \nThe highest mIoU is reached by deeplabv3 with a learning rate of 0.1 but at some sudden points it becomes unstable. Then in order are the UNet (bilinear and transpose), linear and the transpose with data augmentation. Some steps lower are the UNet with data augmentation and inverted frequencies and at last the deeplabv3 with weather data augmentation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9699521840976044,
        0.9970079479932207
      ],
      "excerpt": "The previous metrics were taken in the validation phase of our training. Concluding the experiment we test the model configuration with the test dataset. The results in this phase give us an overall understanding of the performance. \nHere we only compare the results of each of our best configurations for each of our models: UNet and DeepLabv3 with and without weather conditions. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9167608073990754,
        0.9420725021104606,
        0.9618460314252677
      ],
      "excerpt": "In the next figure we can see that after a similar start in the first 15 epochs, and penalized by the low start, SGD has a steeper mIoU curve and might need more epochs to reach Adam's performance in this metric. Also, from the experience of the last experiments, it would have useful to risen the learning rate for SGD optimizer, even though the loss becomes more unstable. \nThe next plot show the accuracy from two experiments. In both the model is UNet with the transposed convolutions for upsampling but in the second experiment we add data augmentation for the training dataset. \nThere really is no noticeable improvement after adding this transformations. We expected it also in the test results since they are unseen samples by the model, but we did not get there an improvement either: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259,
        0.9823353615073337,
        0.9725438665368141,
        0.9819124121209327,
        0.9960566132499558,
        0.9237917913206912,
        0.9773693166289222
      ],
      "excerpt": "| Data augmentation | 77.3  |39 \nOur of our last experiments was changing the learning rate of the optimizer. We did it on several configurations, both UNet and Deeplabv3 and both Adam and SGD, and here we can notice a real change. The \"standard\" learning rate is 0.001 and gives a much smoother curve for loss and also metrics. In every experiment used 0.1 as the learning rate all curves became unpredictable. \nWhat drove us during the whole process of experimentation was the evidence that came out. What we searched over and over through reading and proposals was improvement. But unfortunately we didn't find much, at least in the amount we expected. We did expect to see self-evident results as we have been seeing throughout the course in the labs and in the ML books, but maybe we had a stroke of reality. \nThe strategy of building has been incremental. From a small model performing good to bigger models performing better. Our best configuration has succesively changed as we added well known techniques and even though they haven't cast astonishing results we have trusted the evidence. This is the nature of science and engineering. \nWe have become familiar with the elements that comprise every phase of a DL problem. From choosing the dataset that best fitted our needs to adding cloud layers to images. Coding a model and its environment in such a comprehensive and powerful library as Pytorch has been tough at times but easy overall. It is a sure shot to rely on all the classes and methods that the library offers, and we have only used a tiny part of it. \nTensorboard has become our closest allie. Being able to follow the experiment in real time is essential to contrast executions and take decisions. Scalars, images and best epochs are truly fundamental resources to have the information gathered. \nIt has been our first dive into a self-driven deep learning project and we have found it challenging. Sure thing we will move on to power up this model with the pending tasks and explore other DL problems. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Final project (IT6) for the postgradate course Artificial Intelligence with Deep Learning",
      "technique": "GitHub API"
    }
  ],
  "documentation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "https://imgaug.readthedocs.io/",
      "technique": "Regular expression"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/it6aidl/outdoorsegmentation/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Thu, 23 Dec 2021 15:00:20 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/it6aidl/outdoorsegmentation/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "it6aidl/outdoorsegmentation",
    "technique": "GitHub API"
  },
  "invocation": [
    {
      "confidence": [
        0.804308178997724,
        0.8050677016517059
      ],
      "excerpt": "- experiments -  folder containing an example of the experiments conducted \n - figures - folder containing project images. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.915209195646505,
        0.9442597613294518,
        0.8875589316420536,
        0.9112173996259987,
        0.8926177031890298,
        0.8681241999813707
      ],
      "excerpt": " - Dataset.py - python file containing the dataset class. \n - Train.py - python file containing the train class. \n - Test.py - python file used for doing only the Test step. \n - Unet.py - python file containing the Unet class. \n - UnetModes.py - python file containing extra Unet modes. \n - metrics.py - python file containing metric calculators. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8306485818905339
      ],
      "excerpt": "len( ) - returns dataset size. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8463917323371575,
        0.9270128006368351
      ],
      "excerpt": "  train_dataset = MyDataset(version=ds_version, split='train', joint_transform=joint_transforms, img_transform=img_transforms, url_csv_file=params['dataset_url'], file_suffix=params['file_suffix']) \n  train_loader = utils.data.DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True, num_workers=4) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9124580193826373,
        0.8473859113605982,
        0.9124580193826373
      ],
      "excerpt": "  val_loader = utils.data.DataLoader(val_dataset, batch_size=params['batch_size'], shuffle=False, num_workers=4) \ntest_dataset = MyDataset(version=ds_version, split='test', joint_transform=joint_transforms_vt, img_transform=img_transforms_vt, url_csv_file=params['dataset_url'], file_suffix=params['file_suffix'], add_weather= weather == 'y') \n  test_loader = utils.data.DataLoader(test_dataset, batch_size=params['batch_size'], shuffle=False, num_workers=4) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8633989807152664
      ],
      "excerpt": "-   Test: 250 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": "-   Train: 2952 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8572877222102082
      ],
      "excerpt": "| Adam (0.001) | UNet|Transpose| 83.64|44.01 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8073292555379703
      ],
      "excerpt": "| SGD (0.001) | Unet | Transpose| 80.89|34.26 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8081128479170085
      ],
      "excerpt": "Number of pixels per class (train): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8572877222102082,
        0.8073292555379703
      ],
      "excerpt": "| Adam (0.001) |  UNet| Transpose| |83.64|44.01 \n| SGD (0.001) | UNet| Transpose|| 80.89|34.26 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/it6aidl/outdoorsegmentation/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Repository Structure",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "outdoorsegmentation",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "it6aidl",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/it6aidl/outdoorsegmentation/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 23 Dec 2021 15:00:20 GMT"
    },
    "technique": "GitHub API"
  }
}