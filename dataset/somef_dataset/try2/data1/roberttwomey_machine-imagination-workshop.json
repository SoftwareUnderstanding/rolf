{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1812.04948",
      "https://arxiv.org/abs/2005.14165",
      "https://arxiv.org/abs/1809.11096.\n\n__CLIP__\n\n<!--![image](https://user-images.githubusercontent.com/1598545/118530808-ee890080-b6f9-11eb-8a49-1e1e73097792.png"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Google Deep Mind BigGAN, [Large Scale GAN Training for High Fidelity Natural Image Synthesis](https://deepmind.com/research/publications/large-scale-gan-training-high-fidelity-natural-image-synthesis), 2018\n  - see the BigGAN hands-on notebook above to get a sense for image generation with BigGAN, noise vectors, truncation, and latent interpolation. \n- NVIDIA StyleGAN2, [A Style-Based Generator Architecture for Generative Adversarial Networks](https://arxiv.org/abs/1812.04948), 2019\n  - see for example [https://thispersondoesnotexist.com/](https://thispersondoesnotexist.com/), a photorealistic face generator with StyleGAN2\n- OpenAI GPT-3: [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165), 2020\n  - see Kenric Allado-McDowell's [Pharmako-AI](https://ignota.org/products/pharmako-ai) for an example a book written with GPT-3.\n- OpenAI [CLIP: Connecting Text and Image](https://openai.com/blog/clip/), 2021\n- OpenAI [DALL-E: Creating Images from Text](https://openai.com/blog/dall-e/), 2021\n  - the interactive examples on this page will give you a sense of the kind of technique we will explore during the workshop.\n- Good [list of CLIP-related to text-to-image notebooks on Reddit r/MachineLearning](https://www.reddit.com/r/MachineLearning/comments/ldc6oc/p_list_of_sitesprogramsprojects_that_use_openais/)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9411099656612427
      ],
      "excerpt": "U.Chicago Digital Media Workshop and Poetry and Poetics Workshop | 4-5:30pm CT, May 17, 2021 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8550101043698384,
        0.9507374082549614
      ],
      "excerpt": "| 5:10  | Discussion, Future Directions |  \n| 5:30  | End | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.842790493796475
      ],
      "excerpt": "Generative Adversarial Networks (GANs) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9995729905101087
      ],
      "excerpt": "Ian Goodfellow introduced the architecture in Generative Adversarial Nets, Goodfellow et. al (2014) https://arxiv.org/pdf/1406.2661.pdf \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/roberttwomey/machine-imagination-workshop",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-05-16T15:06:25Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-27T23:18:57Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "With recent advancements in machine learning techniques, researchers have demonstrated remarkable achievements in image synthesis (BigGAN, StyleGAN), textual understanding (GPT-3), and other areas of text and image manipulation. This hands-on workshop introduces state-of-the-art techniques for text-to-image translation, where textual prompts are used to guide the generation of visual imagery. Participants will gain experience with Open AI's CLIP network and Google's BigGAN, using free Google Colab notebooks which they can apply to their own work after the event. We will discuss other relationships between text and image in art and literature; consider the strengths and limitations of these new techniques; and relate these computational processes to human language, perception, and visual expression and imagination. __Please bring a text you would like to experiment with!__\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.896969772198916,
        0.8430983021495915,
        0.9734103341633409,
        0.9305763148422334
      ],
      "excerpt": "| 4:00  | Introductions; Open up Google colab; Introduction to Neural Nets, Generative Adversarial Networks (GANs), Generative Text (Transformers). | \n| 4:10  | Hands on with CoLab notebook: CLIP + BigGAN + CMA-ES; Talk about format of textual \"prompts\"/inputs; Explore visual outputs. | \n| 4:40  | Check in on results. Participants informally share work with group; Q&A about challenges/techniques. Participants continue working. | \n| 5:00  | Hands on with CoLab: Interpolation and latent walks. | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9786141338863343
      ],
      "excerpt": "Neural Networks, or Artificial Neural Networks (ANNs) are networks (graphs) composed of nodes and edges, loosely modelled on the architecture of biological brain. They are generally composed of distinct layers of neurons, where outputs from one feed inputs of another. Broadly, each node resembles a neuron, accepting inputs from a number of other nodes, and defined with its own activiation function, bias, and forward connections. There are many variations on this basic architecture. Above we see a very simple fully connected, feed forward network that takes as an input 28 x 28 pixel grayscale images (784 input signals), and produces a 0-10 digit classifier on the output. Neural networks are used for many generative and predictive tasks across sound, image, text, etc. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.989634585221146
      ],
      "excerpt": "A Generative Adversarial Network (GAN) is a kind of generative model. The basic idea is to set up a game between two players (game theory). The Generator creates samples that resemble the input dataset. The Discriminator evaluates samples to determine if they are real or fake (binary classifier). We can think of the generator as being like a counterfeiter, trying to make fake money, and the discriminator as being like police, trying to allow legitimate money and catch counterfeit money. To succeed in this game, the counterfeiter must learn to make money that is indistinguishable from genuine money, and the generator network must learn to create samples that are drawn from the same distribution as the training data. (adversarial) Both networks are trained simultaneously. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9364651323440066
      ],
      "excerpt": "BigGAN (2018) set a standard for high resolution, high fidelity image synthesis in 2018. It contained four times as many parameters and eight times the batch size of previous models, and synthesized a state of the art 512 x 512 pixel image across 1000 different classes from Imagenet. It was also prohibitively expensive to train! Thankfully Google/Google Brain has released a number of pretrained models for us to explore. Read the paper here https://arxiv.org/abs/1809.11096. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.988525847184422,
        0.9797244081692317,
        0.9537182487767457
      ],
      "excerpt": "CLIP (Contrastive Language-Image Pre-Training) is a neural network trained on a variety of (image, text) pairs. It can be instructed in natural language to predict the most relevant text snippet, given an image, without directly optimizing for the task, similarly to the zero-shot capabilities of GPT-2 and 3. We found CLIP matches the performance of the original ResNet50 on ImageNet \u201czero-shot\u201d without using any of the original 1.28M labeled examples, overcoming several major challenges in computer vision.  \nCLIP pre-trains an image encoder and a text encoder to predict which images were paired with which texts in our dataset. We then use this behavior to turn CLIP into a zero-shot classifier. We convert all of a dataset\u2019s classes into captions such as \u201ca photo of a dog\u201d and predict the class of the caption CLIP estimates best pairs with a given image.  \nCLIP learns from unfiltered, highly varied, and highly noisy data ... text\u2013image pairs that are already publicly available on the internet. See details on the CLIP Model Card \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "text to image notebook with CLIP for workshop on Machine Imagination, Spring 2021",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/roberttwomey/machine-imagination-workshop/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 30 Dec 2021 07:54:31 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/roberttwomey/machine-imagination-workshop/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "roberttwomey/machine-imagination-workshop",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/roberttwomey/machine-imagination-workshop/main/generate_from_stored.ipynb",
      "https://raw.githubusercontent.com/roberttwomey/machine-imagination-workshop/main/BigGAN_handson.ipynb",
      "https://raw.githubusercontent.com/roberttwomey/machine-imagination-workshop/main/text_to_image_BigGAN_CLIP.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8145167573909525
      ],
      "excerpt": "Click on the links below to open the corresponding notebooks in google colab. You can only run one at a time. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/roberttwomey/machine-imagination-workshop/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Machine Imagination: Text to Image Generation with Neural Networks",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "machine-imagination-workshop",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "roberttwomey",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/roberttwomey/machine-imagination-workshop/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3,
      "date": "Thu, 30 Dec 2021 07:54:31 GMT"
    },
    "technique": "GitHub API"
  }
}