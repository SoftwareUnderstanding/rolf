{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2002.02848",
      "https://arxiv.org/abs/1807.03748",
      "https://arxiv.org/abs/1807.03748"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please consider citing this project in your publications if it helps your research.\n\n```\n@misc{rivire2020unsupervised,\n    title={Unsupervised pretraining transfers well across languages},\n    author={Morgane Rivi\u00e8re and Armand Joulin and Pierre-Emmanuel Mazar\u00e9 and Emmanuel Dupoux},\n    year={2020},\n    eprint={2002.02848},\n    archivePrefix={arXiv},\n    primaryClass={eess.AS}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{rivire2020unsupervised,\n    title={Unsupervised pretraining transfers well across languages},\n    author={Morgane Rivi\u00e8re and Armand Joulin and Pierre-Emmanuel Mazar\u00e9 and Emmanuel Dupoux},\n    year={2020},\n    eprint={2002.02848},\n    archivePrefix={arXiv},\n    primaryClass={eess.AS}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeOfConduct": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://raw.githubusercontent.com/facebookresearch/CPC_audio/main/CODE_OF_CONDUCT.md",
    "technique": "File Exploration"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/facebookresearch/CPC_audio",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contributing to CPC_audio\nWe want to make contributing to this project as easy and transparent as\npossible.\nPull Requests\nWe actively welcome your pull requests.\n\nFork the repo and create your branch from master.\nIf you've added code that should be tested, add tests.\nIf you've changed APIs, update the documentation.\nEnsure the test suite passes.\nMake sure your code lints.\nIf you haven't already, complete the Contributor License Agreement (\"CLA\").\n\nContributor License Agreement (\"CLA\")\nIn order to accept your pull request, we need you to submit a CLA. You only need\nto do this once to work on any of Facebook's open source projects.\nComplete your CLA here: https://code.facebook.com/cla\nIssues\nWe use GitHub issues to track public bugs. Please ensure your description is\nclear and has sufficient instructions to be able to reproduce the issue.\nFacebook has a bounty program for the safe\ndisclosure of security bugs. In those cases, please go through the process\noutlined on that page and do not file a public issue.\nCoding Style\n\n2 spaces for indentation rather than tabs\n80 character line length\n...\n\nLicense\nBy contributing to CPC_audio, you agree that your contributions will be licensed\nunder the LICENSE file in the root directory of this source tree.",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-01-07T23:54:36Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-23T11:12:09Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9519500159155015,
        0.8989575832946524
      ],
      "excerpt": "This code implements the Contrast Predictive Coding algorithm on audio data, as described in the paper Unsupervised Pretraining Transfers well Across Languages. This is an unsupervised method to train audio features directly from the raw waveform. \nMoreover, this code also implements all the evaluation metrics used in the paper: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.834352679656635,
        0.946358693349553
      ],
      "excerpt": "- Phone and speaker linear separability \n- Transfer learning on other languages, using the common voices datasets \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8664393375667141
      ],
      "excerpt": "We suggest to train the model either on Librispeech or libri-light. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9918969192066702
      ],
      "excerpt": "The --schedulerRamp option add a learning rate ramp at the beginning of the training: it barely affects the performance of a model with a transformer predictor but is necessary with other models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9530665522921798
      ],
      "excerpt": "After training, the CPC model can output high level features for a variety of tasks. For an input audio file sampled at 16kHz, the provided baseline model will output 256 dimensional output features every 10ms. We provide two linear separability tests one for speaker, one for phonemes, in which a linear classifier is trained on top of the CPC features with aligned labels, and evaluated on a held-out test set. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9852858864611282
      ],
      "excerpt": "Will evaluate the speaker separability of the concatenation of the features from model1 and model2. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8895278742410043
      ],
      "excerpt": "- $PATH_CHECKPOINT is the path pointing to the checkpoint to evaluate \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9481746871319936
      ],
      "excerpt": "- --strict forces each batch of features to contain exactly the same number of frames. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "An implementation of the Contrast Predictive Coding (CPC) method to train audio features in an unsupervised fashion.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/facebookresearch/CPC_audio/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 45,
      "date": "Fri, 24 Dec 2021 16:43:25 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/facebookresearch/CPC_audio/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "facebookresearch/CPC_audio",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The installation is a tiny bit involved due to the torch-audio dependency.\n\n0/ Clone the repo:\n`git clone git@github.com:facebookresearch/CPC_audio.git && cd CPC_audio`\n\n1/ Install libraries which would be required for torch-audio https://github.com/pytorch/audio :\n * MacOS: `brew install sox`\n * Linux: `sudo apt-get install sox libsox-dev libsox-fmt-all`\n\n2/ `conda env create -f environment.yml && conda activate cpc37`\n\n3/ Run setup.py\n`python setup.py develop`\n\nYou can test your installation with:\n`nosetests -d`\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9089913895321117,
        0.9104423264113566
      ],
      "excerpt": "This setup is given for CUDA 9.2 if you use a different version of CUDA then please change the version of cudatoolkit in environment.yml. \nFor more information on the cudatoolkit version to use, please check https://pytorch.org/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.881783257409656
      ],
      "excerpt": "You can run the ABX score on the Zerospeech2017 dataset. To begin, download the dataset here. Then run the ABX evaluation on a given checkpoint with: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9256198906368809
      ],
      "excerpt": "You can now run the experiments described in the paper. To begin, you must train the linear classifier. You will find below the instructions for the Spanish dataset: you can run the experiments on any other dataset in the same fashion. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8265305053200234
      ],
      "excerpt": "The command is quite similar to run the fine-tuning experiments on the 5 hours dataset. For example in French you need to run: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9256198906368809
      ],
      "excerpt": "You can now run the experiments described in the paper. To begin, you must train the linear classifier. You will find below the instructions for the Spanish dataset: you can run the experiments on any other dataset in the same fashion. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8265305053200234
      ],
      "excerpt": "The command is quite similar to run the fine-tuning experiments on the 5 hours dataset. For example in French you need to run: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8471641612109077
      ],
      "excerpt": "python cpc/train.py --pathDB $PATH_AUDIO_FILES --pathCheckpoint $PATH_CHECKPOINT_DIR --pathTrain $TRAINING_SET --pathVal $VAL_SET --file_extension $EXTENSION --rnnMode ffd --schedulerRamp 10 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8401460587837392
      ],
      "excerpt": "Launch cpc/train.py -h to see all the possible options. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9503189345333785
      ],
      "excerpt": "python cpc/train.py --pathCheckpoint $PATH_CHECKPOINT_DIR \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8222245395118741
      ],
      "excerpt": "Train / Val splits as well as phone alignments for librispeech-100h can be found here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8618624312891964
      ],
      "excerpt": "python cpc/eval/linear_separability.py $PATH_DB $TRAINING_SET $VAL_SET $CHECKPOINT_TO_LOAD --pathCheckpoint $PATH_CHECKPOINT \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8618624312891964
      ],
      "excerpt": "python cpc/eval/linear_separability.py $PATH_DB $TRAINING_SET $VAL_SET $CHECKPOINT_TO_LOAD --pathCheckpoint $PATH_CHECKPOINT --pathPhone $PATH_TO_PHONE_LABELS \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8618624312891964
      ],
      "excerpt": "python cpc/eval/linear_separability.py -$PATH_DB $TRAINING_SET $VAL_SET model1.pt model2.pt --pathCheckpoint $PATH_CHECKPOINT \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.835301520757486
      ],
      "excerpt": "You can run the ABX score on the Zerospeech2017 dataset. To begin, download the dataset here. Then run the ABX evaluation on a given checkpoint with: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8423943549381125
      ],
      "excerpt": "python ABX.py from_checkpoint $PATH_CHECKPOINT $PATH_ITEM_FILE $DATASET_PATH --seq_norm --strict --file_extension .wav --out $PATH_OUT \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8667495955866057
      ],
      "excerpt": "for x in fr zh it ru nl sv es tr tt ky; do python cpc/eval/utils/adjust_sample_rate.py ${DIR_CC}/${x}/clips ${DIR_CC}/${x}/validated_phones_reduced.txt ${DIR_CC}/${x}/clips_16k; done \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8265683851808091
      ],
      "excerpt": "To run the training on frozen features with the one hour dataset, just run: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.874867277656233
      ],
      "excerpt": "python cpc/eval/common_voices_eval.py train $PATH_COMMON_VOICES/es/clips_16k $PATH_COMMON_VOICES/es/validated_phones_reduced.txt $CHECKPOINT_TO_TEST --pathTrain $PATH_COMMON_VOICES/es/trainSeqs_1.0_uniform_new_version.txt  --pathVal $PATH_COMMON_VOICES/es/trainSeqs_1.0_uniform_new_version.txt --freeze -o $OUTPUT_DIR \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.874867277656233,
        0.8004328017814024
      ],
      "excerpt": "python cpc/eval/common_voices_eval.py train $PATH_COMMON_VOICES/es/clips_16k $PATH_COMMON_VOICES/es/validated_phones_reduced.txt $CHECKPOINT_TO_TEST --pathTrain $PATH_COMMON_VOICES/es/trainSeqs_5.0_uniform_new_version.txt --pathVal $PATH_COMMON_VOICES/es/trainSeqs_5.0_uniform_new_version.txt --freeze -o $OUTPUT_DIR \nOnce the training is done, you can compute the associated phone error rate (PER) on the test subset. To do so, just run: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8891659580305681
      ],
      "excerpt": "python cpc/eval/common_voices_eval.py per $OUTPUT_DIR --pathVal $PATH_COMMON_VOICES/es/testSeqs_uniform_new_version.txt --pathPhone $PATH_COMMON_VOICES/es/validated_phones_reduced.txt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8667495955866057
      ],
      "excerpt": "for x in fr zh it ru nl sv es tr tt ky; do python cpc/eval/utils/adjust_sample_rate.py ${DIR_CC}/${x}/clips ${DIR_CC}/${x}/validated_phones_reduced.txt ${DIR_CC}/${x}/clips_16k; done \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8265683851808091
      ],
      "excerpt": "To run the training on frozen features with the one hour dataset, just run: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.874867277656233
      ],
      "excerpt": "python cpc/eval/common_voices_eval.py train $PATH_COMMON_VOICES/es/clips_16k $PATH_COMMON_VOICES/es/validated_phones_reduced.txt $CHECKPOINT_TO_TEST --pathTrain $PATH_COMMON_VOICES/es/trainSeqs_1.0_uniform_new_version.txt  --pathVal $PATH_COMMON_VOICES/es/trainSeqs_1.0_uniform_new_version.txt --freeze -o $OUTPUT_DIR \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.874867277656233,
        0.8004328017814024
      ],
      "excerpt": "python cpc/eval/common_voices_eval.py train $PATH_COMMON_VOICES/es/clips_16k $PATH_COMMON_VOICES/es/validated_phones_reduced.txt $CHECKPOINT_TO_TEST --pathTrain $PATH_COMMON_VOICES/es/trainSeqs_5.0_uniform_new_version.txt --pathVal $PATH_COMMON_VOICES/es/trainSeqs_5.0_uniform_new_version.txt --freeze -o $OUTPUT_DIR \nOnce the training is done, you can compute the associated phone error rate (PER) on the test subset. To do so, just run: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8891659580305681
      ],
      "excerpt": "python cpc/eval/common_voices_eval.py per $OUTPUT_DIR --pathVal $PATH_COMMON_VOICES/es/testSeqs_uniform_new_version.txt --pathPhone $PATH_COMMON_VOICES/es/validated_phones_reduced.txt \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/facebookresearch/CPC_audio/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Cython"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) Facebook, Inc. and its affiliates.\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "CPC_audio",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "CPC_audio",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "facebookresearch",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/facebookresearch/CPC_audio/blob/main/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To run a new training session, use:\n\n```bash\npython cpc/train.py --pathDB $PATH_AUDIO_FILES --pathCheckpoint $PATH_CHECKPOINT_DIR --pathTrain $TRAINING_SET --pathVal $VAL_SET --file_extension $EXTENSION\n```\n\nWhere:\n- $PATH_AUDIO_FILES is the directory containing the audio files. The files should be arranged as below:\n```\nPATH_AUDIO_FILES  \n\u2502\n\u2514\u2500\u2500\u2500speaker1\n\u2502   \u2514\u2500\u2500\u2500...\n\u2502         \u2502   seq_11.{$EXTENSION}\n\u2502         \u2502   seq_12.{$EXTENSION}\n\u2502         \u2502   ...\n\u2502   \n\u2514\u2500\u2500\u2500speaker2\n    \u2514\u2500\u2500\u2500...\n          \u2502   seq_21.{$EXTENSION}\n          \u2502   seq_22.{$EXTENSION}\n```\n\nPlease note that each speaker directory can contain an arbitrary number of subdirectories: the speaker label will always be retrieved from the top one. The name of the files isn't relevant. For a concrete example, you can look at the organization of the [Librispeech](http://www.openslr.org/12/) dataset.\n\n- $PATH_CHECKPOINT_DIR in the directory where the checkpoints will be saved\n- $TRAINING_SET is a path to a .txt file containing the list of the training sequences (see [here](https://drive.google.com/drive/folders/1BhJ2umKH3whguxMwifaKtSra0TgAbtfb) for example)\n- $VALIDATION_SET is a path to a .txt file containing the list of the validation sequences\n- $EXTENSION is the extension of each audio file\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "All evaluation scripts can be found in cpc/eval/.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 241,
      "date": "Fri, 24 Dec 2021 16:43:25 GMT"
    },
    "technique": "GitHub API"
  }
}