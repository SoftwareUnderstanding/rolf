{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find this package useful in your research, please consider citing:\n\n    @article{mao2014deep,\n      title={Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)},\n      author={Mao, Junhua and Xu, Wei and Yang, Yi and Wang, Jiang and Huang, Zhiheng and Yuille, Alan},\n      journal={ICLR},\n      year={2015}\n    }\n    \n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{mao2014deep,\n  title={Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)},\n  author={Mao, Junhua and Xu, Wei and Yang, Yi and Wang, Jiang and Huang, Zhiheng and Yuille, Alan},\n  journal={ICLR},\n  year={2015}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mjhucla/TF-mRNN",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2016-06-25T23:45:44Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-26T00:30:58Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This package is a re-implementation of the [m-RNN](http://www.stat.ucla.edu/~junhua.mao/m-RNN.html) image captioning method\nusing [TensorFlow](https://www.tensorflow.org/).\nThe training speed is optimized with buckets of different lengths of the training sentences.\nIt also support the *Beam Search* method to decode image features into \nsentences.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9410614125994565,
        0.9954006824174062,
        0.9925083626898744
      ],
      "excerpt": "  In the training, you can see the loss of your model, but it sometimes very \n  helpful to see the metrics (e.g. BLEU) of the generated sentences for all \n  the checkpoints of the model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Re-implementation of the m-RNN model using TensorFLow",
      "technique": "GitHub API"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Use the following shell to download extracted image features ([Inception-v3](http://arxiv.org/abs/1512.00567) or [VGG](http://arxiv.org/abs/1409.1556)) for MS COCO.\n  ```Shell\n  #: If you want to use inception-v3 image feature, then run:\n  bash ./download_coco_inception_features.sh\n  #: If you want to use VGG image feature, then run:\n  bash ./download_coco_vgg_features.sh\n  ```\n\nAlternatively, you can extract image features yourself, you should download images from [MS COCO](http://mscoco.org/dataset/#download) dataset first.\nPlease make sure that we can find the image on ./datasets/ms_coco/images/ (should have at least train2014 and val2014 folder).\nAfter that, type:\n  ```Shell\n  python ./exp/ms_coco_caption/extract_image_features_all.py\n  ```\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mjhucla/TF-mRNN/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 56,
      "date": "Sat, 25 Dec 2021 18:34:44 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/mjhucla/TF-mRNN/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "mjhucla/TF-mRNN",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/mjhucla/TF-mRNN/master/demo.ipynb",
      "https://raw.githubusercontent.com/mjhucla/TF-mRNN/master/.ipynb_checkpoints/demo-checkpoint.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/mjhucla/TF-mRNN/master/download_coco_vgg_features.sh",
      "https://raw.githubusercontent.com/mjhucla/TF-mRNN/master/setup.sh",
      "https://raw.githubusercontent.com/mjhucla/TF-mRNN/master/download_coco_inception_features.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. install [MS COCO caption toolkit](https://github.com/tylin/coco-caption)\n\n2. Suppose that toolkit is install on $PATH_COCOCap and this package is install at $PATH_mRNN_CR. Create a soft link to COCOCap as follows:\n  ```Shell\n  cd $PATH_mRNN_CR\n  ln -sf $PATH_COCOCap ./external/coco-caption\n  ```\n  \n3. Download necessary data for using a trained m-RNN model.\n  ```Shell\n  bash setup.sh\n  ```\n  \n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8883335143791722
      ],
      "excerpt": "  You can simply open another terminal: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8887588172932509
      ],
      "excerpt": "  python ./exp/ms_coco_caption/create_dictionary.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8887588172932509
      ],
      "excerpt": "  python ./exp/ms_coco_caption/mrnn_trainer_mscoco.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8887588172932509
      ],
      "excerpt": "  python ./exp/ms_coco_caption/mrnn_validator_mscoco.py \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/mjhucla/TF-mRNN/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "TF-mRNN: a TensorFlow library for image captioning.",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "TF-mRNN",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "mjhucla",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mjhucla/TF-mRNN/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- [TensorFlow](https://www.tensorflow.org/) 0.8+\n- python 2.7 (Need ackages of numpy, scipy, nltk. All included in [Anaconda](https://store.continuum.io/cshop/anaconda/))\n- [MS COCO caption toolkit](https://github.com/tylin/coco-caption)\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 107,
      "date": "Sat, 25 Dec 2021 18:34:44 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. install [MS COCO caption toolkit](https://github.com/tylin/coco-caption)\n\n2. Suppose that toolkit is install on $PATH_COCOCap and this package is install at $PATH_mRNN_CR. Create a soft link to COCOCap as follows:\n  ```Shell\n  cd $PATH_mRNN_CR\n  ln -sf $PATH_COCOCap ./external/coco-caption\n  ```\n  \n3. Download necessary data for using a trained m-RNN model.\n  ```Shell\n  bash setup.sh\n  ```\n  \n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "This demo shows how to use a trained model to generate descriptions for an image.\nRun *demo.py* or view *demo.ipynb*\n\nThe configuration of the trained model is: ./model_conf/mrnn_GRU_conf.py.\n\nThe model achieves a CIDEr of 0.890 and a BLEU-4 of 0.282 on the 1000 validation images used in the [m-RNN](http://arxiv.org/abs/1412.6632) paper.\nIt adopts a [transposed weight sharing](http://arxiv.org/abs/1504.06692) strategy that accelerates the training and regularizes the network.\n\n\n",
      "technique": "Header extraction"
    }
  ]
}