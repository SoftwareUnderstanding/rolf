{
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/svikramank/chess-deepRL",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-09-06T19:21:02Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-09-16T12:43:35Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9953519076905217
      ],
      "excerpt": "Using supervised learning on about 10k games, I trained a model (7 residual blocks of 256 filters) to a guesstimate of 1200 elo with 1200 sims/move. One of the strengths of MCTS is it scales quite well with computing power. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8043798772536632,
        0.9854721028968267
      ],
      "excerpt": "Here you can see an example of a game where I (white, ~2000 elo) played against the model in this repo (black): \nUsing the new supervised learning step I created, I've been able to train a model to the point that seems to be learning the openings of chess. Also it seems the model starts to avoid losing naively pieces. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9898514919771697
      ],
      "excerpt": "This model plays in this way after only 5 epoch iterations of the 'opt' worker, the 'eval' worker changed 4 times the best model (4 of 5). At this moment the loss of the 'opt' worker is 5.1 (and still seems to be converging very well). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9251125869632316,
        0.9609238210859374,
        0.8670140839719159
      ],
      "excerpt": "This SL step was also used in the first and original version of AlphaGo and maybe chess is a some complex game that we have to pre-train first the policy model before starting the self-play process (i.e., maybe chess is too much complicated for a self training alone). \nTo use the new SL process is as simple as running in the beginning instead of the worker \"self\" the new worker \"sl\". \nOnce the model converges enough with SL play-data we just stop the worker \"sl\" and start the worker \"self\" so the model will start improving now due to self-play data. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8654194668952989,
        0.9049233340164949,
        0.8037121849200808
      ],
      "excerpt": "To avoid overfitting, I recommend using data sets of at least 3000 games and running at most 3-4 epochs. \nThis AlphaGo Zero implementation consists of three workers: self, opt and eval. \nself is Self-Play to generate training data by self-play using BestModel. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8748840400692672,
        0.8883302851672442
      ],
      "excerpt": "eval is Evaluator to evaluate whether the next-generation model is better than BestModel. If better, replace BestModel. \nNow it's possible to train the model in a distributed way. The only thing needed is to use the new parameter: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8606124781912439,
        0.9374120810814147,
        0.9518602519746595
      ],
      "excerpt": "To set up ChessZero with a GUI, point it to C0uci.bat (or rename to .sh). \nFor example, this is screenshot of the random model using Arena's self-play feature: \ndata/model/model_best_*: BestModel. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Chess reinforcement learning by AlphaGo Zero methods.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/svikramank/chess-deepRL/releases",
    "technique": "GitHub API"
  },
  "executable_example": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "https://mybinder.org/v2/gh/kmader/chess-alpha-zero/master?urlpath=lab",
      "technique": "Regular expression"
    }
  ],
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Wed, 29 Dec 2021 09:07:16 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/svikramank/chess-deepRL/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "svikramank/chess-deepRL",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/svikramank/chess-deepRL/master/notebooks/demo.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```bash\npip install -r requirements.txt\n```\n\nIf you want to use GPU, follow [these instructions](https://www.tensorflow.org/install/) to install with pip3.\n\nMake sure Keras is using Tensorflow and you have Python 3.6.3+. Depending on your environment, you may have to run python3/pip3 instead of python/pip.\n\n\nBasic Usage\n------------\n\nFor training model, execute `Self-Play`, `Trainer` and `Evaluator`.\n\n**Note**: Make sure you are running the scripts from the top-level directory of this repo, i.e. `python src/chess_zero/run.py opt`, not `python run.py opt`.\n\n\nSelf-Play\n--------\n\n```bash\npython src/chess_zero/run.py self\n```\n\nWhen executed, Self-Play will start using BestModel.\nIf the BestModel does not exist, new random model will be created and become BestModel.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9370593088230497
      ],
      "excerpt": "GPU Memory \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9701033046439633
      ],
      "excerpt": "python src/chess_zero/run.py sl \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8224613080859576
      ],
      "excerpt": "opt is Trainer to train model, and generate next-generation models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8002653277518406
      ],
      "excerpt": "--type distributed: use mini config for testing, (see src/chess_zero/configs/distributed.py) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.936025912828802,
        0.9333634975095757,
        0.9423540074017059
      ],
      "excerpt": "python src/chess_zero/run.py self --type distributed (or python src/chess_zero/run.py sl --type distributed) \npython src/chess_zero/run.py opt --type distributed \npython src/chess_zero/run.py eval --type distributed \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8248328724624491,
        0.8019129206775808,
        0.8514808592110918,
        0.8617707930347892
      ],
      "excerpt": "data/model/model_best_*: BestModel. \ndata/model/next_generation/*: next-generation models. \ndata/play_data/play_*.json: generated training data. \nlogs/main.log: log file. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9507533671827105,
        0.8478396656439281
      ],
      "excerpt": "python src/chess_zero/run.py opt \nWhen executed, Training will start. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9598449252742912
      ],
      "excerpt": "python src/chess_zero/run.py eval \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8459327753421999
      ],
      "excerpt": "If error happens, try to change vram_frac in src/configs/mini.py, \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/svikramank/chess-deepRL/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python",
      "Batchfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "chess-deepRL",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "svikramank",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/svikramank/chess-deepRL/blob/master/readme.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Wed, 29 Dec 2021 09:07:16 GMT"
    },
    "technique": "GitHub API"
  }
}