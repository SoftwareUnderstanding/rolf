{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1907.10529\">SpanBERT: Improving Pre-training by Representing and Predicting Spans</a>\n\n4\u3001<a href=\"https://arxiv.org/pdf/1907.11692.pdf\">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a>\n\n5\u3001<a href=\"https://arxiv.org/pdf/1904.00962.pdf\">Large Batch Optimization for Deep Learning: Training BERT in 76 minutes(LAMB"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.946498592459726
      ],
      "excerpt": "    --use_tpu=True  --tpu_name=grpc://10.240.1.66:8470 --tpu_zone=us-central1-a \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8302556419090275
      ],
      "excerpt": "\u4e0b\u8f7d<a href=\"https://drive.google.com/open?id=1HXYMqsXjmA5uIfu_SFqP7r_vZZG-m_H0\">LCQMC</a>\u6570\u636e\u96c6\uff0c\u5305\u542b\u8bad\u7ec3\u3001\u9a8c\u8bc1\u548c\u6d4b\u8bd5\u96c6\uff0c\u8bad\u7ec3\u96c6\u5305\u542b24\u4e07\u53e3\u8bed\u5316\u63cf\u8ff0\u7684\u4e2d\u6587\u53e5\u5b50\u5bf9\uff0c\u6807\u7b7e\u4e3a1\u62160\u30021\u4e3a\u53e5\u5b50\u8bed\u4e49\u76f8\u4f3c\uff0c0\u4e3a\u8bed\u4e49\u4e0d\u76f8\u4f3c\u3002 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9986846822129116
      ],
      "excerpt": "***** 2019-10-15: albert_tiny_zh, 10 times fast than bert base for training and inference, accuracy remains ***** \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8613116749415453
      ],
      "excerpt": "***** 2019-10-04: PyTorch and Keras versions of albert were supported ***** \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9880323298724544
      ],
      "excerpt": "***** 2019-10-02: albert_large_zh,albert_base_zh ***** \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8401878451523918
      ],
      "excerpt": "***** 2019-09-28: codes and test functions *****  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9799883766133161
      ],
      "excerpt": "\u6ce8\uff1aBERT-wwm-ext\u6765\u81ea\u4e8e<a href=\"https://github.com/ymcui/Chinese-BERT-wwm\">\u8fd9\u91cc</a>\uff1bXLNet\u6765\u81ea\u4e8e<a href=\"https://github.com/ymcui/Chinese-PreTrained-XLNet\">\u8fd9\u91cc</a>; RoBERTa-zh-base\uff0c\u630712\u5c42RoBERTa\u4e2d\u6587\u6a21\u578b \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8320601091898192,
        0.9105368110547479
      ],
      "excerpt": "using <a href=\"https://github.com/lonePatient/albert_pytorch\">albert_pytorch \n<a href=\"https://github.com/bojone/bert4keras\">bert4keras</a> \u9002\u914dalbert\uff0c\u80fd\u6210\u529f\u52a0\u8f7dalbert_zh\u7684\u6743\u91cd\uff0c\u53ea\u9700\u8981\u5728load_pretrained_model\u51fd\u6570\u91cc\u52a0\u4e0aalbert=True \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8880929169745804
      ],
      "excerpt": "<a href=\"https://github.com/kpe/bert-for-tf2\">bert-for-tf2</a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "if __name__ == '__main__': \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "...          | 384        | 12 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9601977744463712,
        0.9985435339545422
      ],
      "excerpt": "Cite Us \nBright Liang Xu, albert_zh, (2019), GitHub repository, https://github.com/brightmart/albert_zh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9977083898672244,
        0.9936884742717579,
        0.9877152550794682,
        0.9977994744046882,
        0.9263559178526231
      ],
      "excerpt": "1\u3001<a href=\"https://arxiv.org/pdf/1909.11942.pdf\">ALBERT: A Lite BERT For Self-Supervised Learning Of Language Representations</a> \n2\u3001<a href=\"https://arxiv.org/pdf/1810.04805.pdf\">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a> \n3\u3001<a href=\"https://arxiv.org/abs/1907.10529\">SpanBERT: Improving Pre-training by Representing and Predicting Spans</a> \n4\u3001<a href=\"https://arxiv.org/pdf/1907.11692.pdf\">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a> \n5\u3001<a href=\"https://arxiv.org/pdf/1904.00962.pdf\">Large Batch Optimization for Deep Learning: Training BERT in 76 minutes(LAMB)</a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9105368110547479
      ],
      "excerpt": "8\u3001 <a href=\"https://github.com/lonePatient/albert_pytorch\">albert_pytorch</a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9056419988360834
      ],
      "excerpt": "10\u3001<a href=\"https://github.com/kpe/bert-for-tf2\">load albert with tf2.0</a> \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/brightmart/albert_zh",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-09-26T16:45:30Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-24T02:12:57Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9331701277555743,
        0.9880650925810209,
        0.9780562400442429,
        0.9487902959099389
      ],
      "excerpt": "An Implementation of <a href=\"https://arxiv.org/pdf/1909.11942.pdf\">A Lite Bert For Self-Supervised Learning Language Representations</a> with TensorFlow \nALBert is based on Bert, but with some improvements. It achieves state of the art performance on main benchmarks with 30% parameters less.  \nFor albert_base_zh it only has ten percentage parameters compare of original bert model, and main accuracy is retained.  \nDifferent version of ALBERT pre-trained model for Chinese, including TensorFlow, PyTorch and Keras, is available now. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = AutoModel.from_pretrained(\"MODEL_NAME\") \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9644066435736677
      ],
      "excerpt": "We will use LCQMC dataset for fine-tuning, it is oral language corpus, it is used to train and predict semantic similarity of a pair of sentences. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.954469088664387
      ],
      "excerpt": "    2) for Fine-tuning, you can try to add small percentage of dropout(e.g. 0.1) by changing parameters of  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9812632302046335,
        0.9241102400003087
      ],
      "excerpt": "add method to deploy ablert_tiny to mobile devices with only 0.1 second inference time for sequence length 128, 60M memory ******* \n***** 2019-10-30: add a simple guide about converting the model to Tensorflow Lite for edge deployment ***** \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8921803929016081
      ],
      "excerpt": "add albert_xlarge_zh; albert_base_zh_additional_steps, training with more instances \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8372396012058242
      ],
      "excerpt": "Releasing albert_xlarge on 6th Oct \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8104984712898627,
        0.9432421919932612
      ],
      "excerpt": "Relesed albert_base_zh with only 10% parameters of bert_base, a small model(40M) & training can be very fast.  \nRelased albert_large_zh with only 16% parameters of bert_base(64M) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8617732292294692,
        0.9854022541002323,
        0.9966474696551649
      ],
      "excerpt": "Add codes and test functions for three main changes of albert from bert \nALBERT\u6a21\u578b\u4ecb\u7ecd Introduction of ALBERT \nALBERT\u6a21\u578b\u662fBERT\u7684\u6539\u8fdb\u7248\uff0c\u4e0e\u6700\u8fd1\u5176\u4ed6State of the art\u7684\u6a21\u578b\u4e0d\u540c\u7684\u662f\uff0c\u8fd9\u6b21\u662f\u9884\u8bad\u7ec3\u5c0f\u6a21\u578b\uff0c\u6548\u679c\u66f4\u597d\u3001\u53c2\u6570\u66f4\u5c11\u3002 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9693233393948762
      ],
      "excerpt": " O(V * H) to O(V * E + E * H) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9763295463080993,
        0.9318225634086296
      ],
      "excerpt": "  We maintain that inter-sentence modeling is an important aspect of language understanding, but we propose a loss  \n  based primarily on coherence. That is, for ALBERT, we use a sentence-order prediction (SOP) loss, which avoids topic  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9797716444550015
      ],
      "excerpt": "  consecutive segments but with their order swapped. This forces the model to learn finer-grained distinctions about \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9731389216915819
      ],
      "excerpt": "1\uff09\u53bb\u6389\u4e86dropout  Remove dropout to enlarge capacity of model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9073398484392122,
        0.8841964170601708
      ],
      "excerpt": "    We also note that, even after training for 1M steps, our largest models still do not overfit to their training data.  \n    As a result, we decide to remove dropout to further increase our model capacity. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8216589471083835
      ],
      "excerpt": "\u4e2d\u6587\u4efb\u52a1\u96c6\u4e0a\u6548\u679c\u5bf9\u6bd4\u6d4b\u8bd5 Performance on Chinese datasets \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9519314646172505
      ],
      "excerpt": "\u6a21\u578b\u53c2\u6570\u548c\u914d\u7f6e Configuration of Models \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9089891109071567
      ],
      "excerpt": "We are going to use the new experimental tf->tflite converter that's distributed with the Tensorflow nightly build. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9578678334797005
      ],
      "excerpt": "Benchmark the performance of the TFLite model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9425652548023197,
        0.9296565289127067,
        0.9817872534354104
      ],
      "excerpt": "for details about the performance benchmark tools in TFLite. For example: after \nbuilding the benchmark tool binary for an Android phone, do the following to \nget an idea of how the TFLite model performs on the phone \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9814949460511204,
        0.9764337799585383
      ],
      "excerpt": "of 2019/11/01, the inference latency is ~120ms w/ this converted TFLite model \nusing 4 threads on CPU, and the memory usage is ~60MB for the model during \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9219534150597932
      ],
      "excerpt": "\u5b66\u4e60\u66f2\u7ebf Training Loss of xlarge of albert_zh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9204654826467913
      ],
      "excerpt": "\u6240\u6709\u7684\u53c2\u6570 Parameters of albert_xlarge \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8930030902815865,
        0.9340447966860281
      ],
      "excerpt": "Currently how to use PyTorch version of albert is not clear yet, if you know how to do that, just email us or open an issue. \nYou can also send pull request to report you performance on your task or add methods on how to load models for PyTorch and so on. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8220920378879921
      ],
      "excerpt": "1\u3001<a href=\"https://arxiv.org/pdf/1909.11942.pdf\">ALBERT: A Lite BERT For Self-Supervised Learning Of Language Representations</a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS, \u6d77\u91cf\u4e2d\u6587\u9884\u8bad\u7ec3ALBERT\u6a21\u578b",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/brightmart/albert_zh/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 719,
      "date": "Sun, 26 Dec 2021 22:46:05 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/brightmart/albert_zh/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "brightmart/albert_zh",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/brightmart/albert_zh/master/create_pretrain_data.sh",
      "https://raw.githubusercontent.com/brightmart/albert_zh/master/run_classifier_clue.sh",
      "https://raw.githubusercontent.com/brightmart/albert_zh/master/run_classifier_lcqmc.sh",
      "https://raw.githubusercontent.com/brightmart/albert_zh/master/resources/shell_scripts/create_pretrain_data_batch_webtext.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9893272198983933
      ],
      "excerpt": "   git clone https://github.com/brightmart/albert_zh.git \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9465718491881494
      ],
      "excerpt": "   bash run_classifier_clue.sh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8506852711490298,
        0.9879863063452118,
        0.9906248903846466,
        0.9465718491881494
      ],
      "excerpt": " \u4e00\u952e\u8fd0\u884calbert_tiny_zh(linux,lcqmc\u4efb\u52a1)\uff1a \n 1) git clone https://github.com/brightmart/albert_zh \n 2) cd albert_zh \n 3) bash run_classifier_lcqmc.sh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.818948845421315,
        0.9465718491881494
      ],
      "excerpt": "Run following command \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5373\u53ef\u3002\u9879\u76ee\u81ea\u52a8\u4e86\u4e00\u4e2a\u793a\u4f8b\u7684\u6587\u672c\u6587\u4ef6(data/news_zh_1.txt) \n   bash create_pretrain_data.sh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9748709027320682
      ],
      "excerpt": "GPU(brightmart\u7248, tiny\u6a21\u578b): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9748709027320682
      ],
      "excerpt": "GPU(Google\u7248\u672c, small\u6a21\u578b): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9625196775343342
      ],
      "excerpt": "\u73af\u5883 Environment \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9893272198983933
      ],
      "excerpt": "      git clone https://github.com/brightmart/albert_zh.git \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8843268189300018
      ],
      "excerpt": "a.Convert to PyTorch version and do your tasks through <a href=\"https://github.com/lonePatient/albert_pytorch\">albert_pytorch</a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9669865623748065
      ],
      "excerpt": "pip install tensorflow==1.15 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9762693118089942
      ],
      "excerpt": "pip install tf-nightly \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8261940571844398
      ],
      "excerpt": "using <a href=\"https://github.com/lonePatient/albert_pytorch\">albert_pytorch \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.925427280823981
      ],
      "excerpt": "6\u3001<a href=\"https://github.com/ymcui/LAMB_Optimizer_TF\">LAMB Optimizer,TensorFlow version</a> \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8123861209070334
      ],
      "excerpt": "\u6a21\u578b\u4e0b\u8f7d Download Pre-trained Models of Chinese \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9490462507607219,
        0.8954225961930997
      ],
      "excerpt": "nohup python3 run_pretraining.py --input_file=./data/tf*.tfrecord  \\ \n--output_dir=./my_new_model_path --do_train=True --do_eval=True --bert_config_file=$BERT_BASE_DIR/albert_config_tiny.json \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9405146164793419,
        0.8954225961930997
      ],
      "excerpt": "nohup python3 run_pretraining_google.py --input_file=./data/tf*.tfrecord --eval_batch_size=64 \\ \n--output_dir=./my_new_model_path --do_train=True --do_eval=True --albert_config_file=$BERT_BASE_DIR/albert_config_small_google.json  --export_dir=./my_new_model_path_export \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.926064563030254,
        0.8296594541540707
      ],
      "excerpt": "    nohup python3 run_classifier.py   --task_name=lcqmc_pair   --do_train=true   --do_eval=true   --data_dir=$TEXT_DIR   --vocab_file=./albert_config/vocab.txt  \\ \n    --bert_config_file=./albert_config/albert_config_tiny.json --max_seq_length=128 --train_batch_size=64   --learning_rate=1e-4  --num_train_epochs=5 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.926064563030254,
        0.8296594541540707
      ],
      "excerpt": "    nohup python3 run_classifier_sp_google.py --task_name=lcqmc_pair   --do_train=true   --do_eval=true   --data_dir=$TEXT_DIR   --vocab_file=./albert_config/vocab.txt  \\ \n    --albert_config_file=./$BERT_BASE_DIR/albert_config_small_google.json --max_seq_length=128 --train_batch_size=64   --learning_rate=1e-4   --num_train_epochs=5 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8186347885600107
      ],
      "excerpt": "2\uff09\u4e3a\u52a0\u5feb\u8bad\u7ec3\u901f\u5ea6\uff0c\u4f7f\u7528LAMB\u505a\u4e3a\u4f18\u5316\u5668 Use LAMB as optimizer, to train with big batch size \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8828665034782968
      ],
      "excerpt": "| albert_zh_base | 79.1% | 99.0% | 6h | 1.01| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python test_changes.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8614220618915633,
        0.8074034917940409
      ],
      "excerpt": "  --checkpoint_version=1 --input_meta_graph=./albert_model.ckpt.meta --input_binary=true \nConvert to TFLite format \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8652506211663116
      ],
      "excerpt": "  --output_file=/tmp/albert_tiny_zh.tflite \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "  python convert_albert_tf_checkpoint_to_pytorch.py \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/brightmart/albert_zh/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "albert_zh",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "albert_zh",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "brightmart",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/brightmart/albert_zh/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3503,
      "date": "Sun, 26 Dec 2021 22:46:05 GMT"
    },
    "technique": "GitHub API"
  },
  "support": [
    {
      "confidence": [
        1
      ],
      "excerpt": "    If you are doing pre-train for english or other language,which is not chinese, \n    you should set hyperparameter of non_chinese to True on create_pretraining_data.py; \n    otherwise, by default it is doing chinese pre-train using whole word mask of chinese.\n\n",
      "technique": "Header extraction"
    }
  ],
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "albert",
      "bert",
      "roberta",
      "xlnet",
      "tensorflow",
      "pytorch",
      "pre-trained-model",
      "chinese-corpus",
      "pre-trained"
    ],
    "technique": "GitHub API"
  }
}