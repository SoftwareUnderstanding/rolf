{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We would like to thank JJ World Network Technology Co.,LTD for the generous support and all the contributions from the community contributors.\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1910.04376](https://arxiv.org/abs/1910.04376",
      "https://arxiv.org/abs/1910.04376",
      "https://arxiv.org/abs/2106.06135",
      "https://arxiv.org/abs/2106.06135",
      "https://arxiv.org/abs/1312.5602",
      "https://arxiv.org/abs/1603.01121"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find this repo useful, you may cite:\n\nZha, Daochen, et al. \"RLCard: A Platform for Reinforcement Learning in Card Games.\" IJCAI. 2020.\n```bibtex\n@inproceedings{zha2020rlcard,\n  title={RLCard: A Platform for Reinforcement Learning in Card Games},\n  author={Zha, Daochen and Lai, Kwei-Herng and Huang, Songyi and Cao, Yuanpu and Reddy, Keerthana and Vargas, Juan and Nguyen, Alex and Wei, Ruzhe and Guo, Junyu and Hu, Xia},\n  booktitle={IJCAI},\n  year={2020}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{zha2020rlcard,\n  title={RLCard: A Platform for Reinforcement Learning in Card Games},\n  author={Zha, Daochen and Lai, Kwei-Herng and Huang, Songyi and Cao, Yuanpu and Reddy, Keerthana and Vargas, Juan and Nguyen, Alex and Wei, Ruzhe and Guo, Junyu and Hu, Xia},\n  booktitle={IJCAI},\n  year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8374695145293155
      ],
      "excerpt": "Official Website: https://www.rlcard.org \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9977994744046882
      ],
      "excerpt": "Paper: https://arxiv.org/abs/1910.04376 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9930497565537282,
        0.995649956622313,
        0.9828239758397141,
        0.9989955950558834,
        0.9930497565537282,
        0.9586353538626329,
        0.9989955950558834
      ],
      "excerpt": "| Blackjack (wiki, baike)                                                              | 10^3            | 10^1              | 10^0        | blackjack       | doc, example                     | \n| Leduc Hold\u2019em (paper)                                                                                                                    | 10^2            | 10^2              | 10^0        | leduc-holdem    | doc, example               | \n| Limit Texas Hold'em (wiki, baike)    | 10^14           | 10^3              | 10^0        | limit-holdem    | doc, example         | \n| Dou Dizhu (wiki, baike)                                               | 10^53 ~ 10^83   | 10^23             | 10^4        | doudizhu        | doc, example                      | \n| Mahjong (wiki, baike)                                                | 10^121          | 10^48             | 10^2        | mahjong         | doc, example                         |  \n| No-limit Texas Hold'em (wiki, baike) | 10^162          | 10^3              | 10^4        | no-limit-holdem | doc, example    | \n| UNO (wiki, baike)                                                                      |  10^163         | 10^10             | 10^1        | uno             | doc, example                                 | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/datamllab/rlcard",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contibuting Guide\nContribution to this project is greatly appreciated! If you find any bugs or have any feedback, please create an issue or send a pull request to fix the bug. If you want to contribute codes for new features, please contact daochen.zha@tamu.edu or khlai@tamu.edu. We currently have several plans. Please create an issue or contact us through emails if you have other suggestions.\nRoadmaps\n\nGame Specific Configurations. Now we plan to gradually support game specific configurations. Currently we only support specifying the number of players in Blackjack\nRule-based Agent and Pre-trained Models. Provide more rule-based agents and pre-trained models to benchmark the evaluation. We currently have several models in /models.\nMore Games and Algorithms. Develop more games and algorithms.\nHyperparameter Search Search hyperparameters for each environment and update the best one in the example.\n\nHow to Create a Pull Request\nIf this your first time to contribute to a project, kindly follow the following instructions. You may find Creating a pull request helpful. Mainly, you need to take the following steps to send a pull request:\n\nClick Fork in the upper-right corner of the project main page to create a new branch in your local Github.\nClone the repo from your local repo in your Github.\nMake changes in your computer.\nCommit and push your local changes to your local repo.\nSend a pull request to merge your local branch to the branches in RLCard project.\n\nTesting Your Code\nWe strongly encourage you to write the testing code in parallel with your development. We use unittest in RLCard. An example is Blackjack environment testing.\nMaking Configurable Environments\nWe take Blackjack as an Example to show how we can define game specific configurations in RLCard. The key points are highlighted as follows:\n\nWe add a DEFAULT_GAME_CONFIG in Blackjack Env to define the default values of the game configurations. Each field should start with game_\nModify the game and environment according to the configurations. For example, we need to support multiple players in Blackjack.\nModify Env to add your game to the supported_envs\nWhen making the environment, we pass the newly defined fields in config. For example, we pass config={'game_player_num': 2} for Blackjack.",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-09-05T12:48:01Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-29T03:51:30Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9914808611649049
      ],
      "excerpt": "RLCard is a toolkit for Reinforcement Learning (RL) in card games. It supports multiple card environments with easy-to-use interfaces for implementing various reinforcement learning and searching algorithms. The goal of RLCard is to bridge reinforcement learning and imperfect information games. RLCard is developed by DATA Lab at Texas A&M University and community contributors. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9629962971611608
      ],
      "excerpt": "Related Project: DouZero Project \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9534921941285727,
        0.9606749839908074
      ],
      "excerpt": "*  Slack: Discuss in our #rlcard-project slack channel. \n*  QQ Group: Join our QQ group 665647450. Password: rlcardqqgroup \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9796959607564389,
        0.9685056549950487
      ],
      "excerpt": "*   Please follow DouZero, a strong Dou Dizhu AI and the ICML 2021 paper. An online demo is available here. The algorithm is also integrated in RLCard. See Training DMC on Dou Dizhu. \n*   Our package is used in PettingZoo. Please check it out! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9434703807775607,
        0.9945418128027914,
        0.989396441499103
      ],
      "excerpt": "*   Jupyter Notebook tutorial available! We add some examples in R to call Python interfaces of RLCard with reticulate. See here \n*   Thanks for the contribution of @Clarit7 for supporting different number of players in Blackjack. We call for contributions for gradually making the games more configurable. See here for more details. \n*   Thanks for the contribution of @Clarit7 for the Blackjack and Limit Hold'em human interface. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9778373166613781,
        0.9818746954118089,
        0.9779159741423987,
        0.8987348251224007
      ],
      "excerpt": "*   Human interface of NoLimit Holdem available. The action space of NoLimit Holdem has been abstracted. Thanks for the contribution of @AdrianP-. \n*   New game Gin Rummy and human GUI available. Thanks for the contribution of @billh0420. \n*   PyTorch implementation available. Thanks for the contribution of @mjudell. \nWe provide a complexity estimation for the games on several aspects. InfoSet Number: the number of information sets; InfoSet Size: the average number of states in a single information set; Action Size: the size of the action space. Name: the name that should be passed to rlcard.make to create the game environment. We also provide the link to the documentation and the random example. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9642102220466255,
        0.860059181823877
      ],
      "excerpt": "We provide a model zoo to serve as the baselines. \n| Model                                    | Explanation                                              | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8767230105080804,
        0.8767230105080804,
        0.8767230105080804,
        0.8767230105080804,
        0.8767230105080804
      ],
      "excerpt": "| leduc-holdem-rule-v1                     | Rule-based model for Leduc Hold'em, v1                   | \n| leduc-holdem-rule-v2                     | Rule-based model for Leduc Hold'em, v2                   | \n| uno-rule-v1                              | Rule-based model for UNO, v1                             | \n| limit-holdem-rule-v1                     | Rule-based model for Limit Texas Hold'em, v1             | \n| doudizhu-rule-v1                         | Rule-based model for Dou Dizhu, v1                       | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9109425399404231,
        0.9477521756084192,
        0.8154104550603786,
        0.8154104550603786,
        0.9905442676572473,
        0.8473917605691591
      ],
      "excerpt": "    *   Game specific configurations: These fields start with game_. Currently, we only support game_num_players in Blackjack, . \nOnce the environemnt is made, we can access some information of the game. \n*   env.num_actions: The number of actions. \n*   env.num_players: The number of players. \n*   env.state_shape: The shape of the state space of the observations. \n*   env.action_shape: The shape of the action features (Dou Dizhu's action can encoded as features) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8899440665824293,
        0.9739093782962398
      ],
      "excerpt": "The following interfaces provide a basic usage. It is easy to use but it has assumtions on the agent. The agent must follow agent template.  \n*   env.set_agents(agents): agents is a list of Agent object. The length of the list should be equal to the number of the players in the game. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9033739357198317
      ],
      "excerpt": "For advanced usage, the following interfaces allow flexible operations on the game tree. These interfaces do not make any assumtions on the agent. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8957401968398169
      ],
      "excerpt": "*   env.step_back(): Available only when allow_step_back is True. Take one step backward. This can be used for algorithms that operate on the game tree, such as CFR (chance sampling). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.910944984439711,
        0.9515542837117322,
        0.9958817281525829,
        0.9530653076912694,
        0.8839617034138886
      ],
      "excerpt": "*   env.get_player_id(): Return the Player ID of the current player. \n*   env.get_state(player_id): Return the state that corresponds to player_id. \n*   env.get_payoffs(): In the end of the game, return a list of payoffs for all the players. \n*   env.get_perfect_information(): (Currently only support some of the games) Obtain the perfect information at the current state. \nThe purposes of the main modules are listed as below: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8690135998875518
      ],
      "excerpt": "/docs: Documentation of RLCard. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8494261353785646
      ],
      "excerpt": "/rlcard/agents: Reinforcement learning algorithms and human agents. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Reinforcement Learning / AI Bots in Card (Poker) Games - Blackjack, Leduc, Texas, DouDizhu, Mahjong, UNO.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/datamllab/rlcard/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 376,
      "date": "Wed, 29 Dec 2021 04:37:11 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/datamllab/rlcard/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "datamllab/rlcard",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/datamllab/rlcard/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/datamllab/rlcard/master/examples/scripts/dmc_doudizhu_1_gpu.sh",
      "https://raw.githubusercontent.com/datamllab/rlcard/master/examples/scripts/dmc_doudizhu_4_gpu.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Make sure that you have **Python 3.6+** and **pip** installed. We recommend installing the stable version of `rlcard` with `pip`:\n\n```\npip3 install rlcard\n```\nThe default installation will only include the card environments. To use PyTorch implementation of the training algorithms, run\n```\npip3 install rlcard[torch]\n```\nIf you are in China and the above command is too slow, you can use the mirror provided by Tsinghua University:\n```\npip3 install rlcard -i https://pypi.tuna.tsinghua.edu.cn/simple\n```\nAlternatively, you can clone the latest version with (if you are in China and Github is slow, you can use the mirror in [Gitee](https://gitee.com/daochenzha/rlcard)):\n```\ngit clone https://github.com/datamllab/rlcard.git\n```\nor only clone one branch to make it faster:\n```\ngit clone -b master --single-branch --depth=1 https://github.com/datamllab/rlcard.git\n```\nThen install with\n```\ncd rlcard\npip3 install -e .\npip3 install -e .[torch]\n```\n\nWe also provide [**conda** installation method](https://anaconda.org/toubun/rlcard):\n\n```\nconda install -c toubun rlcard\n```\n\nConda installation only provides the card environments, you need to manually install Pytorch on your demands.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9539130126445136,
        0.9154459384263874
      ],
      "excerpt": "You can use the the following interface to make an environment. You may optionally specify some configurations with a dictionary. \n*   env = rlcard.make(env_id, config={}): Make an environment. env_id is a string of a environment; config is a dictionary that specifies some environment configurations, which are as follows. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8064959820383261
      ],
      "excerpt": "<img width=\"500\" src=\"https://dczha.com/files/rlcard/logo.jpg\" alt=\"Logo\" /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.810094876824159
      ],
      "excerpt": "Dou Dizhu Demo: Demo \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8752284579539807
      ],
      "excerpt": "| Game                                                                                                                                                                                           | InfoSet Number  | InfoSet Size      | Action Size | Name            | Usage                                                                                       | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8403619438156041
      ],
      "excerpt": "| Blackjack (wiki, baike)                                                              | 10^3            | 10^1              | 10^0        | blackjack       | doc, example                     | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8574883249807679,
        0.8403619438156041,
        0.808064770526145,
        0.8259947936838347,
        0.8913996532638498
      ],
      "excerpt": "| Dou Dizhu (wiki, baike)                                               | 10^53 ~ 10^83   | 10^23             | 10^4        | doudizhu        | doc, example                      | \n| Mahjong (wiki, baike)                                                | 10^121          | 10^48             | 10^2        | mahjong         | doc, example                         |  \n| No-limit Texas Hold'em (wiki, baike) | 10^162          | 10^3              | 10^4        | no-limit-holdem | doc, example    | \n| UNO (wiki, baike)                                                                      |  10^163         | 10^10             | 10^1        | uno             | doc, example                                 | \n| Gin Rummy (wiki, baike)                                                         | 10^52           | -                 | -           | gin-rummy       | doc, example                     | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8530475839427083
      ],
      "excerpt": "| Counterfactual Regret Minimization (CFR) | examples/run_cfr.py | [paper] | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8279823069630468
      ],
      "excerpt": "    *   allow_step_back: Default False. True if allowing step_back function to traverse backward in the tree. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8507977012907258
      ],
      "excerpt": "/examples: Examples of using RLCard. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/datamllab/rlcard/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'Copyright (c) 2019 DATA Lab at Texas A&M University\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "RLCard: A Toolkit for Reinforcement Learning in Card Games",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "rlcard",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "datamllab",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/datamllab/rlcard/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "daochenzha",
        "body": "*   Fixed bugs in num_players\r\n*   Improved efficiency of Mahjong\r\n*   Fixed step_back bug in no-limit Holdem\r\n",
        "dateCreated": "2021-08-05T05:03:03Z",
        "datePublished": "2021-08-05T05:23:34Z",
        "html_url": "https://github.com/datamllab/rlcard/releases/tag/1.0.4",
        "name": "RLCard 1.0.4",
        "tag_name": "1.0.4",
        "tarball_url": "https://api.github.com/repos/datamllab/rlcard/tarball/1.0.4",
        "url": "https://api.github.com/repos/datamllab/rlcard/releases/47337387",
        "zipball_url": "https://api.github.com/repos/datamllab/rlcard/zipball/1.0.4"
      },
      {
        "authorType": "User",
        "author_name": "daochenzha",
        "body": "*   Support 8 environments.\r\n*   Support 4 algorithms with Torch training.\r\n*   Support Deep Monte-Carlo (DMC) described in [DouZero](https://github.com/kwai/DouZero).\r\n*   Clean and light-weighted codebase.",
        "dateCreated": "2021-06-16T18:56:19Z",
        "datePublished": "2021-06-17T16:52:24Z",
        "html_url": "https://github.com/datamllab/rlcard/releases/tag/1.0.2",
        "name": "RLCard 1.0.2",
        "tag_name": "1.0.2",
        "tarball_url": "https://api.github.com/repos/datamllab/rlcard/tarball/1.0.2",
        "url": "https://api.github.com/repos/datamllab/rlcard/releases/44812035",
        "zipball_url": "https://api.github.com/repos/datamllab/rlcard/zipball/1.0.2"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1549,
      "date": "Wed, 29 Dec 2021 04:37:11 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "reinforcement-learning",
      "deep-reinforcement-learning",
      "game-ai",
      "poker",
      "card-game",
      "poker-game",
      "openai-gym",
      "gym-environment",
      "card-games",
      "blackjack",
      "texas",
      "uno",
      "mahjong",
      "game-bot",
      "doudizhu",
      "multi-agent",
      "game",
      "ai"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "A **short example** is as below.\n\n```python\nimport rlcard\nfrom rlcard.agents import RandomAgent\n\nenv = rlcard.make('blackjack')\nenv.set_agents([RandomAgent(num_actions=env.num_actions)])\n\nprint(env.num_actions) #: 2\nprint(env.num_players) #: 1\nprint(env.state_shape) #: [[2]]\nprint(env.action_shape) #: [None]\n\ntrajectories, payoffs = env.run()\n```\n\nRLCard can be flexibly connected to various algorithms. See the following examples:\n\n*   [Playing with random agents](docs/toy-examples.md#playing-with-random-agents)\n*   [Deep-Q learning on Blackjack](docs/toy-examples.md#deep-q-learning-on-blackjack)\n*   [Training CFR (chance sampling) on Leduc Hold'em](docs/toy-examples.md#training-cfr-on-leduc-holdem)\n*   [Having fun with pretrained Leduc model](docs/toy-examples.md#having-fun-with-pretrained-leduc-model)\n*   [Training DMC on Dou Dizhu](docs/toy-examples.md#training-dmc-on-dou-dizhu)\n*   [Evaluating Agents](docs/toy-examples.md#evaluating-agents)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Run `examples/human/leduc_holdem_human.py` to play with the pre-trained Leduc Hold'em model. Leduc Hold'em is a simplified version of Texas Hold'em. Rules can be found [here](docs/games.md#leduc-holdem).\n\n```\n>> Leduc Hold'em pre-trained model\n\n>> Start a new game!\n>> Agent 1 chooses raise\n\n=============== Community Card ===============\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2502\n\u2502\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2502\n\u2502\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2502\n\u2502\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2502\n\u2502\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2502\n\u2502\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2502\n\u2502\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n===============   Your Hand    ===============\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502J        \u2502\n\u2502         \u2502\n\u2502         \u2502\n\u2502    \u2665    \u2502\n\u2502         \u2502\n\u2502         \u2502\n\u2502        J\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n===============     Chips      ===============\nYours:   +\nAgent 1: +++\n=========== Actions You Can Choose ===========\n0: call, 1: raise, 2: fold\n\n>> You choose action (integer):\n```\nWe also provide a GUI for easy debugging. Please check [here](https://github.com/datamllab/rlcard-showdown/). Some demos:\n\n![doudizhu-replay](https://github.com/datamllab/rlcard-showdown/blob/master/docs/imgs/doudizhu-replay.png?raw=true)\n![leduc-replay](https://github.com/datamllab/rlcard-showdown/blob/master/docs/imgs/leduc-replay.png?raw=true)\n\n",
      "technique": "Header extraction"
    }
  ]
}