{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1610.10099, https://github.com/paarthneekhara/byteNet-tensorflow\n* Wikipedia Korpus http://www.cs.pomona.edu/~dkauchak/simplification/\n* Deep Voice https://arxiv.org/abs/1710.07654\n* Other papers: http://www.thespermwhale.com/jaseweston/papers/unified_nlp.pdf \\\\ https://arxiv.org/pdf/1404.2188.pdf\n* ABCNN: https://arxiv.org/pdf/1512.05193.pdf\n* Recursive Autoencoder for Paraphrase Detection https://papers.nips.cc/paper/4204-dynamic-pooling-and-unfolding-recursive-autoencoders-for-paraphrase-detection.pdf\n* Convolutional NN (translation",
      "https://arxiv.org/abs/1710.07654\n* Other papers: http://www.thespermwhale.com/jaseweston/papers/unified_nlp.pdf \\\\ https://arxiv.org/pdf/1404.2188.pdf\n* ABCNN: https://arxiv.org/pdf/1512.05193.pdf\n* Recursive Autoencoder for Paraphrase Detection https://papers.nips.cc/paper/4204-dynamic-pooling-and-unfolding-recursive-autoencoders-for-paraphrase-detection.pdf\n* Convolutional NN (translation"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9976848874039342
      ],
      "excerpt": "Bytenet https://arxiv.org/abs/1610.10099, https://github.com/paarthneekhara/byteNet-tensorflow \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9898130747178828,
        0.9556486376343356,
        0.9944484218006108,
        0.9944787686719995,
        0.8111036989382164
      ],
      "excerpt": "Deep Voice https://arxiv.org/abs/1710.07654 \nOther papers: http://www.thespermwhale.com/jaseweston/papers/unified_nlp.pdf \\ https://arxiv.org/pdf/1404.2188.pdf \nABCNN: https://arxiv.org/pdf/1512.05193.pdf \nRecursive Autoencoder for Paraphrase Detection https://papers.nips.cc/paper/4204-dynamic-pooling-and-unfolding-recursive-autoencoders-for-paraphrase-detection.pdf \nConvolutional NN (translation) https://github.com/tobyyouup/conv_seq2seq \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kinimod23/ATS_Project",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-11-22T18:32:27Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-05-25T15:46:21Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8988782607837283
      ],
      "excerpt": "statistics about data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8232240129355647,
        0.8136810850899752,
        0.9720911346265891
      ],
      "excerpt": "think about other ideas for loss functions \n(currently L2-Norm) \nand also Cosine Similarity for End2End \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.991783112129784,
        0.8808024496635244,
        0.9856732441111441
      ],
      "excerpt": "Get deconvolution with pretrained model to work \nThis is really a pain in the ass, i don't know how to do it \nImplemented model with attention and all that \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9013418615244245,
        0.9393874757240446,
        0.8817749274989549,
        0.9765862631238511,
        0.8779983062539366
      ],
      "excerpt": "because we can't feed complex and simple for sentences other than our training set \n  but that is the goal right? \nrestructured code to allow for siamese net as encoder ( concolutional ) and deconvolution as decoder. also to use both together as End2End \ndeconvolution hard to implement because of lack of knowledge regarding tensorflows options for that \nimplemented an image deconvolution with tf.image.resize (not real deconvolution i think) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9002209020334262,
        0.9273444402953047
      ],
      "excerpt": "there were a lot of unknown words hence we switched to FAIRs fasttext trained on english wikipedia \ntried End2End in various forms, for thousends of epochs but didn't get good results \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9666851996494652,
        0.9596483512509224,
        0.935678417512023
      ],
      "excerpt": "tried to implement the loading and restoring of models so that we can train the encoder with cosine similarity as target and later on use this pretrained thing to create inputs for the decoder \ntook me a lot of hours to get this to work. i rebuilt the model various times and now we have a splitted model. one for encoder one for decoder \nCoupling a Bi-CNN Encoder and a CNN decoder \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kinimod23/ATS_Project/releases",
    "technique": "GitHub API"
  },
  "faq": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Taking syntax trees of complex sentences, we want to \"translate\" them into one or moresyntax trees of simpler sentences while keeping the meaning.\nSolved?\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Neural Net consisting of encoder and decoder stage.\nTwo parallel encoder sharing the same weights, one with complex syntax trees as input, the other with the simplified version.\nThe outputs of the encoder are compared with an appropriate metric and a discriminator.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "- determine if the hypothesis of comparability of complex and simplified syntax trees is correct (group effort)\n  - create Syntax trees out of the data (Henny)\n  - evaluate some example sentences manually (group effort)\n- Find a good representation of syntax trees in the literature (group effort)\n- Modify an existing ByteNet implementation to take the new input and visualize it in Tensorboard (Simon)\n- Organise the data such that an encoder can be trained properly\n- Train an encoder, find out if it converges\n- Train a decoder with simplified syntax trees as gold standard\n\n",
      "technique": "Header extraction"
    }
  ],
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Thu, 23 Dec 2021 17:57:55 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/kinimod23/ATS_Project/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "kinimod23/ATS_Project",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/kinimod23/ATS_Project/master/jupyter_notebooks/preprocessing.simplify.ipynb",
      "https://raw.githubusercontent.com/kinimod23/ATS_Project/master/jupyter_notebooks/word2vec.simplify.ipynb",
      "https://raw.githubusercontent.com/kinimod23/ATS_Project/master/jupyter_notebooks/Untitled.ipynb",
      "https://raw.githubusercontent.com/kinimod23/ATS_Project/master/.ipynb_checkpoints/word2vec.simplify-checkpoint.ipynb",
      "https://raw.githubusercontent.com/kinimod23/ATS_Project/master/.ipynb_checkpoints/preprocessing.simplify-checkpoint.ipynb",
      "https://raw.githubusercontent.com/kinimod23/ATS_Project/master/.ipynb_checkpoints/Untitled-checkpoint.ipynb",
      "https://raw.githubusercontent.com/kinimod23/ATS_Project/master/ourABCNN/Untitled.ipynb",
      "https://raw.githubusercontent.com/kinimod23/ATS_Project/master/corpus/preprocessing.simplify.ipynb",
      "https://raw.githubusercontent.com/kinimod23/ATS_Project/master/corpus/word2vec.simplify.ipynb",
      "https://raw.githubusercontent.com/kinimod23/ATS_Project/master/corpus/Untitled.ipynb",
      "https://raw.githubusercontent.com/kinimod23/ATS_Project/master/corpus/.ipynb_checkpoints/text-simplify-checkpoint.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8013209876319369
      ],
      "excerpt": "now using fasttext... \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9024238322518017,
        0.8579790089251601
      ],
      "excerpt": "change test.py           \n( creates output now ) \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/kinimod23/ATS_Project/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python",
      "TeX"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Automatic Text Simplification Project",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "ATS_Project",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "kinimod23",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kinimod23/ATS_Project/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Thu, 23 Dec 2021 17:57:55 GMT"
    },
    "technique": "GitHub API"
  },
  "support": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Approaches in Machine Translation yield state-of-the-art results. Interpreting our task as translation problem, we can use existing ideas and extend them to our needs.\nStajner et al. (2017)\n\n",
      "technique": "Header extraction"
    }
  ],
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "*CNN implementation:*\n\n* _Attention-based CNN for modeling sentence pairs_\n-> [github](https://github.com/galsang/ABCNN)\n\n* Generalization of CNNs by using graph signal processing applied on any graph structure. Definition of convolutional filters on graphs.\n-> [github](https://github.com/mdeff/cnn_graph)\n\n*ByteNet implementation:*\n\n* Tensor2Tensor library of deep learning models with bytenet partly maintained by google -> [github](https://github.com/tensorflow/tensor2tensor)\n\n* bytenet without target network + training framework missing -> [github](https://github.com/NickShahML/bytenet_tensorflow)\n\n* bytenet trained on eng-fr corpus. Relies on TF v1 -> [github](https://github.com/buriburisuri/ByteNet)\n\n* generation trained on shakespeare and translation on ger-en -> [github](https://github.com/paarthneekhara/byteNet-tensorflow)\n\n*Recursive Neural Nets:*\n* Recursive Neural Networks with tree structure in Tensorflow -> [github](https://github.com/erickrf/treernn)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Neural Net consisting of encoder and decoder stage.\nTwo parallel encoder sharing the same weights, one with complex syntax trees as input, the other with the simplified version.\nThe outputs of the encoder are compared with an appropriate metric and a discriminator.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "( Wikipedia simple and complex corpus )\nThe above but improved by Hwang et al. (2015)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "- determine if the hypothesis of comparability of complex and simplified syntax trees is correct (group effort)\n  - create Syntax trees out of the data (Henny)\n  - evaluate some example sentences manually (group effort)\n- Find a good representation of syntax trees in the literature (group effort)\n- Modify an existing ByteNet implementation to take the new input and visualize it in Tensorboard (Simon)\n- Organise the data such that an encoder can be trained properly\n- Train an encoder, find out if it converges\n- Train a decoder with simplified syntax trees as gold standard\n\n",
      "technique": "Header extraction"
    }
  ]
}