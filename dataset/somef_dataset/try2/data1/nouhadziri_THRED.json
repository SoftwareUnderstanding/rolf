{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1811.01063",
      "https://arxiv.org/abs/1409.3215",
      "https://arxiv.org/abs/1605.06069",
      "https://arxiv.org/abs/1606.08340",
      "https://arxiv.org/abs/1811.01063"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please cite the following paper if you used our work in your research:\n```text\n@article{dziri2018augmenting,\n  title={Augmenting Neural Response Generation with Context-Aware Topical Attention},\n  author={Dziri, Nouha and Kamalloo, Ehsan and Mathewson, Kory W and Zaiane, Osmar R},\n  journal={arXiv preprint arXiv:1811.01063},\n  year={2018}\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{dziri2018augmenting,\n  title={Augmenting Neural Response Generation with Context-Aware Topical Attention},\n  author={Dziri, Nouha and Kamalloo, Ehsan and Mathewson, Kory W and Zaiane, Osmar R},\n  journal={arXiv preprint arXiv:1811.01063},\n  year={2018}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/nouhadziri/THRED",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-11-08T05:43:30Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-21T12:38:57Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9483772025253147,
        0.844753191715762
      ],
      "excerpt": "The codebase is evolved from the Tensorflow NMT repository. \nTL;DR Steps to create a dialogue agent using this framework: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9060959296767533
      ],
      "excerpt": " 4. Chat with the trained model using: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8570587923528425,
        0.9145060601969369
      ],
      "excerpt": "Our Reddit dataset, which we call Reddit Conversation Corpus (RCC), is collected from 95 selected subreddits (listed here). \nWe processed Reddit for a 20 month-period ranging from November 2016 until August 2018 (excluding June 2017 and July 2017; we utilized these two months along with the October 2016 data to train an LDA model). Please see here for the details of how the Reddit dataset is built including pre-processing and cleaning the raw Reddit files. The following table summarizes the RCC information: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8400386294114751
      ],
      "excerpt": "In the data files, each line corresponds to a single conversation where utterances are TAB-separated. The topic words appear after the last utterance separated also by a TAB. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8458745188203106,
        0.8603039386461336
      ],
      "excerpt": "In the model config files (i.e., the YAML files in conf), the embedding types can be either of the following: glove840B, fastText, word2vec, and hub_word2vec. For handling the pre-trained embedding vectors, we leverage Pymagnitude and Tensorflow-Hub. \nNote that you can also use random300 (300 refers to the dimension of embedding vectors and can be replaced by any arbitrary value) to learn vectors during training of the response generation models. The settings related to embedding models are provided in word_embeddings.yml. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8751393665486036,
        0.9041275138439769
      ],
      "excerpt": "The implemented models are Seq2Seq, HRED, Topic Aware-Seq2Seq, and THRED. \nNote that while most of the parameters are common among the different models, some models may have additional parameters  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9567806557435676
      ],
      "excerpt": "In the interactive mode, a pre-trained LDA model is required to feed the inferred topic words into the model. We trained an LDA model using Gensim on a Reddit corpus, collected for this purpose. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "The implementation of the paper \"Augmenting Neural Response Generation with Context-Aware Topical Attention\"",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/nouhadziri/THRED/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 22,
      "date": "Fri, 24 Dec 2021 12:45:32 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/nouhadziri/THRED/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "nouhadziri/THRED",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/nouhadziri/THRED/master/bin/reddit.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9946229461119547
      ],
      "excerpt": " 2. Install the dependencies using conda env create -f thred_env.yml (To use pip, see Dependencies) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8527075469330936
      ],
      "excerpt": "| Corpus                | #train| #dev  | #test | Download | Download with topic words| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.866088539038092
      ],
      "excerpt": "To train a model, run the following command: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8822547014689814
      ],
      "excerpt": "With the following command, the model can be tested against the test dataset.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8750334738079452
      ],
      "excerpt": "It can be downloaded from here. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8678446567138026
      ],
      "excerpt": "| Corpus                | #train| #dev  | #test | Download | Download with topic words| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8816430062779613,
        0.8816430062779613,
        0.8816430062779613
      ],
      "excerpt": "| 3 turns per line      | 9.2M  | 508K  | 406K  | download (773MB) | download (2.5GB) |  \n| 4 turns per line      | 4M    | 223K  | 178K  | download  (442MB) | download (1.2GB) \n| 5 turns per line      | 1.8M  | 100K  | 80K   | download (242MB) | download (594MB) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8062623648488539
      ],
      "excerpt": "The training configuration should be defined in a YAML file similar to Tensorflow NMT. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8276719125349875
      ],
      "excerpt": "To train a model, run the following command: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8399918329535985
      ],
      "excerpt": "python main.py --mode train --config &lt;YAML_FILE&gt; \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8291492696463486
      ],
      "excerpt": "python main.py --mode train --model_dir &lt;MODEL_DIR&gt; \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/nouhadziri/THRED/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 Nouha Dziri\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Topical Hierarchical Recurrent Encoder Decoder (THRED)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "THRED",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "nouhadziri",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/nouhadziri/THRED/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": " - Python >= 3.5 (Recommended: 3.6)\n - Tensorflow == 1.12.0\n - Tensorflow-Hub\n - SpaCy >= 2.1.0\n - pymagnitude\n - tqdm\n - redis<sup>1</sup>\n - mistune<sup>1</sup>\n - emot<sup>1</sup>\n - Gensim<sup>1</sup>\n - prompt-toolkit<sup>2</sup>\n\n<sup>1</sup><sub><sup>*packages required only for parsing and cleaning the Reddit data.*</sup></sub>\n<sup>2</sup><sub><sup>*used only for testing dialogue models in command-line interactive mode*</sup></sub>\n \nTo install the dependencies using `pip`, run `pip install -r requirements`.\nAnd for Anaconda, run `conda env create -f thred_env.yml` (recommended).\nOnce done with the dependencies, run `pip install -e .` to install the thred package. \n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 95,
      "date": "Fri, 24 Dec 2021 12:45:32 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "dialogue-generation",
      "sequence-to-sequence",
      "tensorflow",
      "deep-learning"
    ],
    "technique": "GitHub API"
  }
}