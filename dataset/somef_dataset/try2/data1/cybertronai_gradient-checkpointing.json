{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1606.03401v1",
      "https://arxiv.org/abs/cs/0310016"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Academic works describing checkpointed backpropagation:\n[Training Deep Nets with Sublinear Memory Cost, by Chen et al. (2016)](https://arxiv.org/pdf/1604.06174.pdf), [Memory-Efficient Backpropagation Through Time, by Gruslys et al. (2016)](https://arxiv.org/abs/1606.03401v1), [Exact Alpha-Beta Computation in\nLogarithmic Space with Application to MAP Word Graph Construction, by Zweig et al. (2000)](https://www.microsoft.com/en-us/research/publication/exact-alpha-beta-computation-in-logarithmic-space-with-application-to-map-word-graph-construction/), [Debugging Backwards in Time, by Bil Lewis (2003)](https://arxiv.org/abs/cs/0310016), [Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation, by Griewank and Walther (2008)](https://epubs.siam.org/doi/book/10.1137/1.9780898717761)\n\n\n\n- Explanation of using graph_editor to implement checkpointing on TensorFlow graphs:\n<https://github.com/tensorflow/tensorflow/issues/4359#issuecomment-269241038>, [https://github.com/yaroslavvb/stuff/blob/master/simple_rewiring.ipynb](https://github.com/yaroslavvb/stuff/blob/master/simple_rewiring.ipynb)\n\n- Experiment code/details: <https://medium.com/@yaroslavvb/testing-memory-saving-on-v100-8aa716bbdf00>\n\n- TensorFlow memory tracking package:\n<https://github.com/yaroslavvb/chain_constant_memory/blob/master/mem_util_test.py>\n\n- Implementation of \"memory-poor\" backprop strategy in TensorFlow for a simple feed-forward net:\n<https://github.com/yaroslavvb/chain_constant_memory/>\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/cybertronai/gradient-checkpointing",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-09-26T17:25:12Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-23T09:02:48Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9947268362583724,
        0.9967967125793422,
        0.9392524214772161,
        0.8943620490003861
      ],
      "excerpt": "Training very deep neural networks requires a lot of memory. Using the tools in this package, developed jointly by Tim Salimans and Yaroslav Bulatov, you can trade off some of this memory usage with computation to make your model fit into memory more easily. For feed-forward models we were able to fit more than 10x larger models onto our GPU, at only a 20% increase in computation time. \nThe memory intensive part of training deep neural networks is computing the gradient of the loss by backpropagation. By checkpointing nodes in the computation graph defined by your model, and recomputing the parts of the graph in between those nodes during backpropagation, it is possible to calculate this gradient at reduced memory cost. When training deep feed-forward neural networks consisting of n layers, we can reduce the memory consumption to O(sqrt(n)) in this way, at the cost of performing one additional forward pass (see e.g. Training Deep Nets with Sublinear Memory Cost, by Chen et al. (2016)). This repository provides an implementation of this functionality in Tensorflow, using the Tensorflow graph editor to automatically rewrite the computation graph of the backward pass. \nMemory used while training a ResNet model with large batch size, using the regular tf.gradients function and using our memory-optimized gradient implementation \nFor a simple feed-forward neural network with n layers, the computation graph for obtaining gradients looks as follows: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9912470128552384
      ],
      "excerpt": "The activations of the neural network layers correspond to the nodes marked with an f. During the forward pass all these nodes are evaluated in order. The gradient of the loss with respect to the activations and parameters of these layers is indicated by the nodes marked with b. During the backward pass, all these nodes are evaluated in the reversed order. The results obtained for the f nodes are needed to compute the b nodes, and hence all f nodes are kept in memory after the forward pass. Only when backpropagation has progressed far enough to have computed all dependencies, or children, of an f node, can it be erased from memory. This means that the memory required by simple backprop grows linearly with the number of neural net layers n. Below we show the order in which these nodes are computed. The purple shaded circles indicate which of the nodes need to be held in memory at any given time. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9719013935595734
      ],
      "excerpt": "Simple backpropagation as described above is optimal in terms of computation: it only computes each node once. However, if we are willing to recompute nodes we can potentially save a lot of memory. We might for instance simply recompute every node from the forward pass each time we need it. The order of execution, and the memory used, then look as follows: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9987201062091311,
        0.9578497528968268
      ],
      "excerpt": "Using this strategy, the memory required to compute gradients in our graph is constant in the number of neural network layers n, which is optimal in terms of memory. However, note that the number of node evaluations now scales with n^2, whereas it previously scaled as n: Each of the n nodes is recomputed on the order of n times. The computation graph thus becomes much slower to evaluate for deep networks, which makes this method impractical for use in deep learning. \nTo strike a balance between memory and computation we need to come up with a strategy that allows nodes to be recomputed, but not too often. The strategy we use here is to mark a subset of the neural net activations as checkpoint nodes.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9205030055069405,
        0.9790341720260174
      ],
      "excerpt": "Our chosen checkpoint node \nThese checkpoint nodes are kept in memory after the forward pass, while the remaining nodes are recomputed at most once. After being recomputed, the non-checkpoint nodes are kept in memory until they are no longer required. For the case of a simple feed-forward neural net, all neuron activation nodes are graph separators or articulation points of the graph defined by the forward pass. This means that we only need to recompute the nodes between a b node and the last checkpoint preceding it when computing that b node during backprop. When backprop has progressed far enough to reach the checkpoint node, all nodes that were recomputed from it can be erased from memory. The resulting order of computation and memory usage then look as follows \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9958730086137263,
        0.9956007794060313,
        0.9291548770988608
      ],
      "excerpt": "For the simple feed-forward network in our example, the optimal choice is to mark every sqrt(n)-th node as a checkpoint. This way, both the number of checkpoint nodes and the number of nodes inbetween checkpoints are on the order of sqrt(n), which means that the required memory now also scales with the square root of the number of layers in our network. Since every node is recomputed at most once, the additional computation required by this strategy is equivalent to a single forward pass through the network. \nOur package implements checkpointed backprop as shown in Graph 3 above. This is implemented by taking the graph for standard backprop (Graph 1 above) and automatically rewriting it using the Tensorflow graph editor. For graphs that contain articulation points (single node graph dividers) we automatically select checkpoints using the sqrt(n) strategy, giving sqrt(n) memory usage for feed-forward networks. For more general graphs that only contain multi-node graph separators our implementation of checkpointed backprop still works, but we currently require the user to manually select the checkpoints. \nAdditional explanation of computation graphs, memory usage, and gradient computation strategies, can be found in the blog post accompanying our package. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8121132506941932
      ],
      "excerpt": ": monkey patch tf.gradients to point to our custom version, with automatic checkpoint selection \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.948472684732041
      ],
      "excerpt": "Replace gradients_speed with gradients_memory or gradients_collection to use the other methods of checkpoint selection. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Make huge neural nets fit in memory",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/cybertronai/gradient-checkpointing/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 256,
      "date": "Thu, 23 Dec 2021 23:35:02 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/cybertronai/gradient-checkpointing/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "cybertronai/gradient-checkpointing",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/cybertronai/gradient-checkpointing/master/test/tf.sh",
      "https://raw.githubusercontent.com/cybertronai/gradient-checkpointing/master/test/run_all_tests.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\npip install tf-nightly-gpu\npip install toposort networkx pytest\n```\nAlso, when running the tests, make sure that the CUDA Profiling Tools Interface (CUPTI) can be found, e.g. by running `export LD_LIBRARY_PATH=\"${LD_LIBRARY_PATH}:/usr/local/cuda/extras/CUPTI/lib64\"`\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9262771181797623
      ],
      "excerpt": "Following this, all calls totf.gradients` will use the memory saving version instead. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8136385841578703
      ],
      "excerpt": "<img src=\"img/backprop.png\" width=\"1200\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8673503029843032
      ],
      "excerpt": "<img src=\"img/output.gif\" width=\"1200\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8231427514499966
      ],
      "excerpt": "<img src=\"img/output_poor.gif\" width=\"1200\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8712597947926826
      ],
      "excerpt": "<img src=\"img/checkpoint.png\" width=\"1200\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8231427514499966
      ],
      "excerpt": "<img src=\"img/output2.gif\" width=\"1200\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.925671696398174,
        0.9133368656218674,
        0.8045518467021734
      ],
      "excerpt": "import tensorflow as tf \nimport memory_saving_gradients \n: monkey patch tf.gradients to point to our custom version, with automatic checkpoint selection \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9012248701992861,
        0.8458017172850023
      ],
      "excerpt": "import memory_saving_gradients as gc \nfrom tensorflow.python.ops import gradients as tf_gradients \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8616271672684886
      ],
      "excerpt": "Testing memory usage and running time for ResNet on CIFAR10 for different numbers of layers. Batch-size 1280, GTX1080 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/cybertronai/gradient-checkpointing/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'The MIT License\\n\\nCopyright (c) 2018 OpenAI (http://openai.com)\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in\\nall copies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\\nTHE SOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Saving memory using gradient-checkpointing",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "gradient-checkpointing",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "cybertronai",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/cybertronai/gradient-checkpointing/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\npip install tf-nightly-gpu\npip install toposort networkx pytest\n```\nAlso, when running the tests, make sure that the CUDA Profiling Tools Interface (CUPTI) can be found, e.g. by running `export LD_LIBRARY_PATH=\"${LD_LIBRARY_PATH}:/usr/local/cuda/extras/CUPTI/lib64\"`\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2235,
      "date": "Thu, 23 Dec 2021 23:35:02 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This repository provides a drop-in replacement for [tf.gradients](https://www.tensorflow.org/api_docs/python/tf/gradients) in base Tensorflow. Import this function using\n\n```\nfrom memory_saving_gradients import gradients\n```\nand use the `gradients` function like you would normally use `tf.gradients` to compute gradients of losses to parameters. (This assumes you are explicitly calling `tf.gradients`, rather than implicitly inside a `tf.train.Optimizer`).\n\nIn addition to the regular arguments to tf.gradients, our gradients function has one additional argument, *checkpoints*. The *checkpoints* argument tells the gradients function which nodes of the graph you want to checkpoint during the forward pass through your computation graph. The nodes in between the checkpoints are then recomputed during the backward pass. You can supply a list of tensors to checkpoint, `gradients(ys,xs,checkpoints=[tensor1,tensor2])`, or you can use one of several keywords:\n\n- 'collection' (default): This checkpoints all tensors returned by `tf.get_collection('checkpoints')`. You then need to make sure you add tensors to this collection using `tf.add_to_collection('checkpoints', tensor)` when you define your model.\n- 'memory' : This uses a heuristic to automatically select a set of nodes to checkpoint which achieves our desired *O(sqrt(n))* memory usage. The heuristic works by automatically identifying *articulation points* in the graph, i.e. tensors which split the graph into two disconnected parts when removed, and then checkpointing a suitable number of these tensors. This currently works well for many, but not all, models.\n- 'speed' : This option tries to maximize running speed by checkpointing the outputs of all ops that are typically expensive to compute, namely convolutions and matrix multiplies.\n\n",
      "technique": "Header extraction"
    }
  ]
}