{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1801.03924>\n[image2stylegan-paper]: <https://arxiv.org/abs/1904.03189>\n[stylegan1-paper]: <https://arxiv.org/abs/1812.04948>\n[stylegan2-paper]: <https://arxiv.org/abs/1912.04958>\n[stylegan2-fork]: <https://github.com/woctezuma/stylegan2/tree/tiled-projector>\n\n[stylegan2-official-repository]: <https://github.com/NVlabs/stylegan2>\n[stylegan2-ada-official-repository]: <https://github.com/NVlabs/stylegan2-ada>\n[stylegan2-ada-pytorch-repository]: <https://github.com/NVlabs/stylegan2-ada-pytorch>\n[stylegan2-applied-to-steam-banners]: <https://github.com/woctezuma/steam-stylegan2>\n[rolux-repository]: <https://github.com/rolux/stylegan2encoder>\n[learnt-latent-directions]: <https://github.com/a312863063/generators-with-stylegan2>\n[colab-user-interface]: <https://github.com/tg-bomze/StyleGAN2-Face-Modificator>\n[artbreeder-website]: <https://artbreeder.com/>\n\n[style-transfer-code]: <https://github.com/NVlabs/stylegan2/blob/cec605e0834de5404d5c7e5cead7053bdd0e4dde/run_generator.py#L40>\n\n[additional-projection-results]: <https://drive.google.com/drive/folders/1-3SUTqK5RpSHgCjKaDKKGpJdkB7iz2VZ?usp=sharing>\n[google-drive-application-results]: <https://drive.google.com/drive/folders/19bJ9ZTvFRqe2WVryChk7cr-md0_AmzpO?usp=sharing>\n[style-transfer-video-tiled]: <https://drive.google.com/file/d/1-FHJkoPe6r9MWSiBg5wKTaYrv7PUD7I3/view?usp=sharing>\n[style-transfer-video-no-tiled]: <https://drive.google.com/file/d/1-DkouQ1wKlSqvteDWiuca6GeJTp9pqJ-/view?usp=sharing>\n\n[extended-projection-limitations]: <https://github.com/rolux/stylegan2encoder/issues/21>\n[minimal-example-latent-edition]: <https://gist.github.com/woctezuma/139cedb92a94c5ef2675cc9f06851b31>\n\n[wiki-all-the-projections]: <https://github.com/woctezuma/stylegan2-projecting-images/wiki/Projections>\n[wiki-all-the-applications]: <https://github.com/woctezuma/stylegan2-projecting-images/wiki/Applications>\n[wiki-application-morphing]: <https://github.com/woctezuma/stylegan2-projecting-images/wiki/Morphing>\n[wiki-application-style-transfer]: <https://github.com/woctezuma/stylegan2-projecting-images/wiki/Style-Transfer>\n[wiki-application-expression-transfer]: <https://github.com/woctezuma/stylegan2-projecting-images/wiki/Expression-Transfer>\n\n[repo-encoder4editing]: <https://github.com/omertov/encoder4editing>\n[colab-encoder4editing]: <http://colab.research.google.com/github/omertov/encoder4editing/blob/main/notebooks/inference_playground.ipynb>\n[wiki-gif-editing]: <https://github.com/woctezuma/stylegan2-projecting-images/wiki/README>\n[moviepy]: <https://github.com/Zulko/moviepy>\n[gifsicle]: <https://github.com/kohler/gifsicle>\n[toonify-blog-post]: <https://www.justinpinkney.com/toonify-yourself/>\n\n[interfacegan]: <https://github.com/genforce/interfacegan>\n[ganspace]: <https://github.com/harskish/ganspace>\n[ALAE]: <https://github.com/podgorskiy/ALAE>\n[closed-form]: <https://github.com/rosinality/stylegan2-pytorch#closed-form-factorization-httpsarxivorgabs200706600>\n\n[rosasalberto-fork]: <https://github.com/rosasalberto/StyleGAN2-TensorFlow-2.x>\n[rosasalberto-sample-from-latents]: <https://github.com/rosasalberto/StyleGAN2-TensorFlow-2.x/blob/master/example_how_to_use.ipynb>\n[rosasalberto-edit-latents]: <https://github.com/rosasalberto/StyleGAN2-TensorFlow-2.x/blob/master/example_latent_changes.ipynb>\n\n[colab-badge]: <https://colab.research.google.com/assets/colab-badge.svg>",
      "https://arxiv.org/abs/1904.03189>\n[stylegan1-paper]: <https://arxiv.org/abs/1812.04948>\n[stylegan2-paper]: <https://arxiv.org/abs/1912.04958>\n[stylegan2-fork]: <https://github.com/woctezuma/stylegan2/tree/tiled-projector>\n\n[stylegan2-official-repository]: <https://github.com/NVlabs/stylegan2>\n[stylegan2-ada-official-repository]: <https://github.com/NVlabs/stylegan2-ada>\n[stylegan2-ada-pytorch-repository]: <https://github.com/NVlabs/stylegan2-ada-pytorch>\n[stylegan2-applied-to-steam-banners]: <https://github.com/woctezuma/steam-stylegan2>\n[rolux-repository]: <https://github.com/rolux/stylegan2encoder>\n[learnt-latent-directions]: <https://github.com/a312863063/generators-with-stylegan2>\n[colab-user-interface]: <https://github.com/tg-bomze/StyleGAN2-Face-Modificator>\n[artbreeder-website]: <https://artbreeder.com/>\n\n[style-transfer-code]: <https://github.com/NVlabs/stylegan2/blob/cec605e0834de5404d5c7e5cead7053bdd0e4dde/run_generator.py#L40>\n\n[additional-projection-results]: <https://drive.google.com/drive/folders/1-3SUTqK5RpSHgCjKaDKKGpJdkB7iz2VZ?usp=sharing>\n[google-drive-application-results]: <https://drive.google.com/drive/folders/19bJ9ZTvFRqe2WVryChk7cr-md0_AmzpO?usp=sharing>\n[style-transfer-video-tiled]: <https://drive.google.com/file/d/1-FHJkoPe6r9MWSiBg5wKTaYrv7PUD7I3/view?usp=sharing>\n[style-transfer-video-no-tiled]: <https://drive.google.com/file/d/1-DkouQ1wKlSqvteDWiuca6GeJTp9pqJ-/view?usp=sharing>\n\n[extended-projection-limitations]: <https://github.com/rolux/stylegan2encoder/issues/21>\n[minimal-example-latent-edition]: <https://gist.github.com/woctezuma/139cedb92a94c5ef2675cc9f06851b31>\n\n[wiki-all-the-projections]: <https://github.com/woctezuma/stylegan2-projecting-images/wiki/Projections>\n[wiki-all-the-applications]: <https://github.com/woctezuma/stylegan2-projecting-images/wiki/Applications>\n[wiki-application-morphing]: <https://github.com/woctezuma/stylegan2-projecting-images/wiki/Morphing>\n[wiki-application-style-transfer]: <https://github.com/woctezuma/stylegan2-projecting-images/wiki/Style-Transfer>\n[wiki-application-expression-transfer]: <https://github.com/woctezuma/stylegan2-projecting-images/wiki/Expression-Transfer>\n\n[repo-encoder4editing]: <https://github.com/omertov/encoder4editing>\n[colab-encoder4editing]: <http://colab.research.google.com/github/omertov/encoder4editing/blob/main/notebooks/inference_playground.ipynb>\n[wiki-gif-editing]: <https://github.com/woctezuma/stylegan2-projecting-images/wiki/README>\n[moviepy]: <https://github.com/Zulko/moviepy>\n[gifsicle]: <https://github.com/kohler/gifsicle>\n[toonify-blog-post]: <https://www.justinpinkney.com/toonify-yourself/>\n\n[interfacegan]: <https://github.com/genforce/interfacegan>\n[ganspace]: <https://github.com/harskish/ganspace>\n[ALAE]: <https://github.com/podgorskiy/ALAE>\n[closed-form]: <https://github.com/rosinality/stylegan2-pytorch#closed-form-factorization-httpsarxivorgabs200706600>\n\n[rosasalberto-fork]: <https://github.com/rosasalberto/StyleGAN2-TensorFlow-2.x>\n[rosasalberto-sample-from-latents]: <https://github.com/rosasalberto/StyleGAN2-TensorFlow-2.x/blob/master/example_how_to_use.ipynb>\n[rosasalberto-edit-latents]: <https://github.com/rosasalberto/StyleGAN2-TensorFlow-2.x/blob/master/example_latent_changes.ipynb>\n\n[colab-badge]: <https://colab.research.google.com/assets/colab-badge.svg>",
      "https://arxiv.org/abs/1812.04948>\n[stylegan2-paper]: <https://arxiv.org/abs/1912.04958>\n[stylegan2-fork]: <https://github.com/woctezuma/stylegan2/tree/tiled-projector>\n\n[stylegan2-official-repository]: <https://github.com/NVlabs/stylegan2>\n[stylegan2-ada-official-repository]: <https://github.com/NVlabs/stylegan2-ada>\n[stylegan2-ada-pytorch-repository]: <https://github.com/NVlabs/stylegan2-ada-pytorch>\n[stylegan2-applied-to-steam-banners]: <https://github.com/woctezuma/steam-stylegan2>\n[rolux-repository]: <https://github.com/rolux/stylegan2encoder>\n[learnt-latent-directions]: <https://github.com/a312863063/generators-with-stylegan2>\n[colab-user-interface]: <https://github.com/tg-bomze/StyleGAN2-Face-Modificator>\n[artbreeder-website]: <https://artbreeder.com/>\n\n[style-transfer-code]: <https://github.com/NVlabs/stylegan2/blob/cec605e0834de5404d5c7e5cead7053bdd0e4dde/run_generator.py#L40>\n\n[additional-projection-results]: <https://drive.google.com/drive/folders/1-3SUTqK5RpSHgCjKaDKKGpJdkB7iz2VZ?usp=sharing>\n[google-drive-application-results]: <https://drive.google.com/drive/folders/19bJ9ZTvFRqe2WVryChk7cr-md0_AmzpO?usp=sharing>\n[style-transfer-video-tiled]: <https://drive.google.com/file/d/1-FHJkoPe6r9MWSiBg5wKTaYrv7PUD7I3/view?usp=sharing>\n[style-transfer-video-no-tiled]: <https://drive.google.com/file/d/1-DkouQ1wKlSqvteDWiuca6GeJTp9pqJ-/view?usp=sharing>\n\n[extended-projection-limitations]: <https://github.com/rolux/stylegan2encoder/issues/21>\n[minimal-example-latent-edition]: <https://gist.github.com/woctezuma/139cedb92a94c5ef2675cc9f06851b31>\n\n[wiki-all-the-projections]: <https://github.com/woctezuma/stylegan2-projecting-images/wiki/Projections>\n[wiki-all-the-applications]: <https://github.com/woctezuma/stylegan2-projecting-images/wiki/Applications>\n[wiki-application-morphing]: <https://github.com/woctezuma/stylegan2-projecting-images/wiki/Morphing>\n[wiki-application-style-transfer]: <https://github.com/woctezuma/stylegan2-projecting-images/wiki/Style-Transfer>\n[wiki-application-expression-transfer]: <https://github.com/woctezuma/stylegan2-projecting-images/wiki/Expression-Transfer>\n\n[repo-encoder4editing]: <https://github.com/omertov/encoder4editing>\n[colab-encoder4editing]: <http://colab.research.google.com/github/omertov/encoder4editing/blob/main/notebooks/inference_playground.ipynb>\n[wiki-gif-editing]: <https://github.com/woctezuma/stylegan2-projecting-images/wiki/README>\n[moviepy]: <https://github.com/Zulko/moviepy>\n[gifsicle]: <https://github.com/kohler/gifsicle>\n[toonify-blog-post]: <https://www.justinpinkney.com/toonify-yourself/>\n\n[interfacegan]: <https://github.com/genforce/interfacegan>\n[ganspace]: <https://github.com/harskish/ganspace>\n[ALAE]: <https://github.com/podgorskiy/ALAE>\n[closed-form]: <https://github.com/rosinality/stylegan2-pytorch#closed-form-factorization-httpsarxivorgabs200706600>\n\n[rosasalberto-fork]: <https://github.com/rosasalberto/StyleGAN2-TensorFlow-2.x>\n[rosasalberto-sample-from-latents]: <https://github.com/rosasalberto/StyleGAN2-TensorFlow-2.x/blob/master/example_how_to_use.ipynb>\n[rosasalberto-edit-latents]: <https://github.com/rosasalberto/StyleGAN2-TensorFlow-2.x/blob/master/example_latent_changes.ipynb>\n\n[colab-badge]: <https://colab.research.google.com/assets/colab-badge.svg>",
      "https://arxiv.org/abs/1912.04958>\n[stylegan2-fork]: <https://github.com/woctezuma/stylegan2/tree/tiled-projector>\n\n[stylegan2-official-repository]: <https://github.com/NVlabs/stylegan2>\n[stylegan2-ada-official-repository]: <https://github.com/NVlabs/stylegan2-ada>\n[stylegan2-ada-pytorch-repository]: <https://github.com/NVlabs/stylegan2-ada-pytorch>\n[stylegan2-applied-to-steam-banners]: <https://github.com/woctezuma/steam-stylegan2>\n[rolux-repository]: <https://github.com/rolux/stylegan2encoder>\n[learnt-latent-directions]: <https://github.com/a312863063/generators-with-stylegan2>\n[colab-user-interface]: <https://github.com/tg-bomze/StyleGAN2-Face-Modificator>\n[artbreeder-website]: <https://artbreeder.com/>\n\n[style-transfer-code]: <https://github.com/NVlabs/stylegan2/blob/cec605e0834de5404d5c7e5cead7053bdd0e4dde/run_generator.py#L40>\n\n[additional-projection-results]: <https://drive.google.com/drive/folders/1-3SUTqK5RpSHgCjKaDKKGpJdkB7iz2VZ?usp=sharing>\n[google-drive-application-results]: <https://drive.google.com/drive/folders/19bJ9ZTvFRqe2WVryChk7cr-md0_AmzpO?usp=sharing>\n[style-transfer-video-tiled]: <https://drive.google.com/file/d/1-FHJkoPe6r9MWSiBg5wKTaYrv7PUD7I3/view?usp=sharing>\n[style-transfer-video-no-tiled]: <https://drive.google.com/file/d/1-DkouQ1wKlSqvteDWiuca6GeJTp9pqJ-/view?usp=sharing>\n\n[extended-projection-limitations]: <https://github.com/rolux/stylegan2encoder/issues/21>\n[minimal-example-latent-edition]: <https://gist.github.com/woctezuma/139cedb92a94c5ef2675cc9f06851b31>\n\n[wiki-all-the-projections]: <https://github.com/woctezuma/stylegan2-projecting-images/wiki/Projections>\n[wiki-all-the-applications]: <https://github.com/woctezuma/stylegan2-projecting-images/wiki/Applications>\n[wiki-application-morphing]: <https://github.com/woctezuma/stylegan2-projecting-images/wiki/Morphing>\n[wiki-application-style-transfer]: <https://github.com/woctezuma/stylegan2-projecting-images/wiki/Style-Transfer>\n[wiki-application-expression-transfer]: <https://github.com/woctezuma/stylegan2-projecting-images/wiki/Expression-Transfer>\n\n[repo-encoder4editing]: <https://github.com/omertov/encoder4editing>\n[colab-encoder4editing]: <http://colab.research.google.com/github/omertov/encoder4editing/blob/main/notebooks/inference_playground.ipynb>\n[wiki-gif-editing]: <https://github.com/woctezuma/stylegan2-projecting-images/wiki/README>\n[moviepy]: <https://github.com/Zulko/moviepy>\n[gifsicle]: <https://github.com/kohler/gifsicle>\n[toonify-blog-post]: <https://www.justinpinkney.com/toonify-yourself/>\n\n[interfacegan]: <https://github.com/genforce/interfacegan>\n[ganspace]: <https://github.com/harskish/ganspace>\n[ALAE]: <https://github.com/podgorskiy/ALAE>\n[closed-form]: <https://github.com/rosinality/stylegan2-pytorch#closed-form-factorization-httpsarxivorgabs200706600>\n\n[rosasalberto-fork]: <https://github.com/rosasalberto/StyleGAN2-TensorFlow-2.x>\n[rosasalberto-sample-from-latents]: <https://github.com/rosasalberto/StyleGAN2-TensorFlow-2.x/blob/master/example_how_to_use.ipynb>\n[rosasalberto-edit-latents]: <https://github.com/rosasalberto/StyleGAN2-TensorFlow-2.x/blob/master/example_latent_changes.ipynb>\n\n[colab-badge]: <https://colab.research.google.com/assets/colab-badge.svg>"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "-   StyleGAN2:\n    -   [StyleGAN2][stylegan2-official-repository] / [StyleGAN2-ADA][stylegan2-ada-official-repository] / [StyleGAN2-ADA-PyTorch][stylegan2-ada-pytorch-repository]\n    -   [Steam-StyleGAN2][stylegan2-applied-to-steam-banners]\n    -   My [fork][stylegan2-fork] of StyleGAN2 to project a batch of images, using any projection (original or extended)\n-   Programming resources:\n    -   [rolux/stylegan2encoder][rolux-repository]: align faces based on detected landmarks (FFHQ pre-processing)\n    -   Learnt [latent directions][learnt-latent-directions] tailored for StyleGAN2 (required for expression transfer)\n    -   Minimal [example code][minimal-example-latent-edition] for morphing and expression transfer\n-   Experimenting materials:\n    -   The website [ArtBreeder][artbreeder-website] by Joel Simon\n    -   Colab [user interface][colab-user-interface] for extended projection and expression transfer\n    -   A fast projector called [`encoder4editing`][repo-encoder4editing] and released in 2021 [![Open In Colab][colab-badge]][colab-encoder4editing]\n-   Reading materials:\n    -   A blog post about editing projected images to add a [cartoon][toonify-blog-post] effect\n    -   On the Wiki: [GIF editing][wiki-gif-editing] with [MoviePy][moviepy] and [Gifsicle][gifsicle]\n-   Papers:\n    - [Karras, Tero, et al. *A Style-Based Generator Architecture for Generative Adversarial Networks*. CVPR 2019.][stylegan1-paper]\n    - [Abdal, Rameen, et al. *Image2StyleGAN: How to Embed Images Into the StyleGAN Latent Space?*. ICCV 2019.][image2stylegan-paper]\n    - [Karras, Tero, et al. *Analyzing and Improving the Image Quality of StyleGAN*. CVPR 2020.][stylegan2-paper]\n\n<!-- Definitions -->\n\n[french-president]: <https://cdn.static01.nicematin.com/media/npo/1440w/2017/06/emmanuel-macron.jpg>\n[french-president-archive]: <https://raw.githubusercontent.com/wiki/woctezuma/stylegan2-projecting-images/img/emmanuel-macron.jpg>\n[french-government]: <https://fr.wikipedia.org/wiki/Gouvernement_%C3%89douard_Philippe_(2)#Galerie_du_gouvernement_actuel>\n[french-government-archive]: <https://raw.githubusercontent.com/wiki/woctezuma/stylegan2-projecting-images/img/french-government-links.txt>\n[famous-paintings-archive]: <https://raw.githubusercontent.com/wiki/woctezuma/stylegan2-projecting-images/img/famous-paintings-links.txt>\n[FFHQ dataset]: <https://github.com/NVlabs/ffhq-dataset>\n[dlib]: <http://dlib.net/face_landmark_detection.py.html>\n[FFHQ pre-processing code]: <https://github.com/NVlabs/ffhq-dataset/blob/master/download_ffhq.py>\n\n[stylegan2_projecting_images]: <https://colab.research.google.com/github/woctezuma/stylegan2-projecting-images/blob/master/stylegan2_projecting_images.ipynb>\n[stylegan2_projecting_images_with_fork]: <https://colab.research.google.com/github/woctezuma/stylegan2-projecting-images/blob/master/stylegan2_projecting_images_with_my_fork.ipynb>\n[stylegan2_editing_latent_vectors]: <https://colab.research.google.com/github/woctezuma/stylegan2-projecting-images/blob/master/stylegan2_editing_latent_vectors.ipynb>\n\n[lpips-paper]: <https://arxiv.org/abs/1801.03924>\n[image2stylegan-paper]: <https://arxiv.org/abs/1904.03189>\n[stylegan1-paper]: <https://arxiv.org/abs/1812.04948>\n[stylegan2-paper]: <https://arxiv.org/abs/1912.04958>\n[stylegan2-fork]: <https://github.com/woctezuma/stylegan2/tree/tiled-projector>\n\n[stylegan2-official-repository]: <https://github.com/NVlabs/stylegan2>\n[stylegan2-ada-official-repository]: <https://github.com/NVlabs/stylegan2-ada>\n[stylegan2-ada-pytorch-repository]: <https://github.com/NVlabs/stylegan2-ada-pytorch>\n[stylegan2-applied-to-steam-banners]: <https://github.com/woctezuma/steam-stylegan2>\n[rolux-repository]: <https://github.com/rolux/stylegan2encoder>\n[learnt-latent-directions]: <https://github.com/a312863063/generators-with-stylegan2>\n[colab-user-interface]: <https://github.com/tg-bomze/StyleGAN2-Face-Modificator>\n[artbreeder-website]: <https://artbreeder.com/>\n\n[style-transfer-code]: <https://github.com/NVlabs/stylegan2/blob/cec605e0834de5404d5c7e5cead7053bdd0e4dde/run_generator.py#L40>\n\n[additional-projection-results]: <https://drive.google.com/drive/folders/1-3SUTqK5RpSHgCjKaDKKGpJdkB7iz2VZ?usp=sharing>\n[google-drive-application-results]: <https://drive.google.com/drive/folders/19bJ9ZTvFRqe2WVryChk7cr-md0_AmzpO?usp=sharing>\n[style-transfer-video-tiled]: <https://drive.google.com/file/d/1-FHJkoPe6r9MWSiBg5wKTaYrv7PUD7I3/view?usp=sharing>\n[style-transfer-video-no-tiled]: <https://drive.google.com/file/d/1-DkouQ1wKlSqvteDWiuca6GeJTp9pqJ-/view?usp=sharing>\n\n[extended-projection-limitations]: <https://github.com/rolux/stylegan2encoder/issues/21>\n[minimal-example-latent-edition]: <https://gist.github.com/woctezuma/139cedb92a94c5ef2675cc9f06851b31>\n\n[wiki-all-the-projections]: <https://github.com/woctezuma/stylegan2-projecting-images/wiki/Projections>\n[wiki-all-the-applications]: <https://github.com/woctezuma/stylegan2-projecting-images/wiki/Applications>\n[wiki-application-morphing]: <https://github.com/woctezuma/stylegan2-projecting-images/wiki/Morphing>\n[wiki-application-style-transfer]: <https://github.com/woctezuma/stylegan2-projecting-images/wiki/Style-Transfer>\n[wiki-application-expression-transfer]: <https://github.com/woctezuma/stylegan2-projecting-images/wiki/Expression-Transfer>\n\n[repo-encoder4editing]: <https://github.com/omertov/encoder4editing>\n[colab-encoder4editing]: <http://colab.research.google.com/github/omertov/encoder4editing/blob/main/notebooks/inference_playground.ipynb>\n[wiki-gif-editing]: <https://github.com/woctezuma/stylegan2-projecting-images/wiki/README>\n[moviepy]: <https://github.com/Zulko/moviepy>\n[gifsicle]: <https://github.com/kohler/gifsicle>\n[toonify-blog-post]: <https://www.justinpinkney.com/toonify-yourself/>\n\n[interfacegan]: <https://github.com/genforce/interfacegan>\n[ganspace]: <https://github.com/harskish/ganspace>\n[ALAE]: <https://github.com/podgorskiy/ALAE>\n[closed-form]: <https://github.com/rosinality/stylegan2-pytorch#closed-form-factorization-httpsarxivorgabs200706600>\n\n[rosasalberto-fork]: <https://github.com/rosasalberto/StyleGAN2-TensorFlow-2.x>\n[rosasalberto-sample-from-latents]: <https://github.com/rosasalberto/StyleGAN2-TensorFlow-2.x/blob/master/example_how_to_use.ipynb>\n[rosasalberto-edit-latents]: <https://github.com/rosasalberto/StyleGAN2-TensorFlow-2.x/blob/master/example_latent_changes.ipynb>\n\n[colab-badge]: <https://colab.research.google.com/assets/colab-badge.svg>\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "    - La Joconde,  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9507374082549614,
        0.8356013927728488,
        0.9507374082549614
      ],
      "excerpt": "    - La Naissance de V\u00e9nus, \n    - Ginevra de' Benci, \n    - La Jeune Fille \u00e0 la perle. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9593299683604384
      ],
      "excerpt": "| | \u2514 style_mixing_42-07-10-29-41_42-07-22-39.jpg \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9593299683604384
      ],
      "excerpt": "|   \u2514 style_mixing_42-07-10-29-41_42-07-22-39.jpg \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/woctezuma/stylegan2-projecting-images",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-02-14T19:09:00Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-23T03:12:49Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9982468427137312,
        0.9300676591819853
      ],
      "excerpt": "The goal of this Google Colab notebook is to project images to latent space with StyleGAN2. \nData consists of: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.814049460732869
      ],
      "excerpt": "-   5 pictures of famous paintings, found on Wikipedia ([list][famous-paintings-archive]): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8489639650179258
      ],
      "excerpt": "-   either center-cropping (to 1024x1024 resolution) as sole pre-processing, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9650403813990771
      ],
      "excerpt": "Finally, the pre-processed image can be projected to the latent space of the StyleGAN2 model trained with configuration f on the Flickr-Faces-HQ (FFHQ) dataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9103353337023916
      ],
      "excerpt": "The result below is obtained with center-cropping as sole pre-processing, hence some issues with the projection. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9652339459959842,
        0.9817401366627431,
        0.9143455261944404
      ],
      "excerpt": "The background, the hair, the ears, and the suit are relatively well reproduced, but the face is wrong, especially the neck (in the original image) is confused with the chin (in the projected images). \nIt is possible that the face is too small relatively to the rest of the image, compared to the FFHQ training dataset, hence the poor results of the projection. \nThe result below is obtained with the same pre-processing as for the FFHQ dataset, which allows to avoid the projection issues mentioned above. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9879171117803347,
        0.9077778884581477,
        0.8072619825380768
      ],
      "excerpt": "For the rest of the repository, the same-preprocessing as for the FFHQ dataset is used. \nAdditional projection results are shown [on the Wiki][wiki-all-the-projections]. \nTo make it easier to download them, they are also shared on [Google Drive][additional-projection-results]. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9567588029116127
      ],
      "excerpt": "\u251c generated_images_tiled/     #: projections with `W(1,*)` \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8346254031596093,
        0.9638258357876468
      ],
      "excerpt": "Images below allow us to compare results obtained with the original projection W(1,*) and the extended projection W(18,*). \nA projected image obtained with W(18,*) is expected to be closer to the target image, [at the expense of semantics][extended-projection-limitations]. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8111055627902403
      ],
      "excerpt": "From top to bottom: aligned target image, projection with W(1,*), projection with W(18,*). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8111055627902403
      ],
      "excerpt": "From top to bottom: aligned target image, projection with W(1,*), projection with W(18,*). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8111055627902403
      ],
      "excerpt": "From top to bottom: aligned target image, projection with W(1,*), projection with W(18,*). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9684745220975228
      ],
      "excerpt": "In the following, we assume that real images have been projected, so that we have access to their latent codes, of shape (1, 512) or (18, 512) depending on the projection method. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9115326975291035,
        0.847558616597404
      ],
      "excerpt": "-   shown [on the Wiki][wiki-all-the-applications], \n-   shared on [Google Drive][google-drive-application-results]. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9693233393948762
      ],
      "excerpt": "| | \u2514 morphing_07_01.jpg        #: face n\u00b07 to face n\u00b01 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8147897010537557,
        0.8147897010537557
      ],
      "excerpt": "| \u251c no_tiled_small.mp4          #: with 2 reference faces \n| \u251c no_ tiled.mp4               #: with 4 reference faces \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8578751094316168,
        0.8892006345553087
      ],
      "excerpt": "Results are shown [on the Wiki][wiki-application-morphing]. \nStyle transfer consists in a crossover of latent vectors at the layer level (cf. [this piece of code][style-transfer-code]). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9023308863756632,
        0.9581515111782112,
        0.8547038313912221
      ],
      "excerpt": "The latent vector of the reference face is used for the first 7 layers. \nThe latent vector of the face whose style has to be copied is used for the remaining 11 layers. \nResults are shown [on the Wiki][wiki-application-style-transfer]. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9071975387252511
      ],
      "excerpt": "Expression transfer consists in the addition of: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8775720520358095
      ],
      "excerpt": "Results are shown [on the Wiki][wiki-application-expression-transfer]. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Projecting images to latent space with StyleGAN2.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/woctezuma/stylegan2-projecting-images/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 24,
      "date": "Wed, 29 Dec 2021 09:06:47 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/woctezuma/stylegan2-projecting-images/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "woctezuma/stylegan2-projecting-images",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/woctezuma/stylegan2-projecting-images/master/stylegan2_projecting_images_with_my_fork.ipynb",
      "https://raw.githubusercontent.com/woctezuma/stylegan2-projecting-images/master/stylegan2_projecting_images.ipynb",
      "https://raw.githubusercontent.com/woctezuma/stylegan2-projecting-images/master/stylegan2_editing_latent_vectors.ipynb"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.8117876608288335
      ],
      "excerpt": "<img alt=\"Face landmarks\" src=\"https://raw.githubusercontent.com/wiki/woctezuma/stylegan2-projecting-images/face_landmarks/landmarks_macron.jpg\" width=\"250\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8550723541638865
      ],
      "excerpt": "| | \u2514 expression_01_age.jpg     #: face n\u00b01 ; age \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639986685036579
      ],
      "excerpt": "|   \u2514 expression_01_age.jpg \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8103280003700739
      ],
      "excerpt": "| | \u2514 morphing_07_01.jpg        #: face n\u00b07 to face n\u00b01 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639986685036579
      ],
      "excerpt": "|   \u2514 morphing_07_01.jpg \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.873127686041587
      ],
      "excerpt": "| | \u2514 style_mixing_42-07-10-29-41_42-07-22-39.jpg \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.873127686041587
      ],
      "excerpt": "|   \u2514 style_mixing_42-07-10-29-41_42-07-22-39.jpg \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/woctezuma/stylegan2-projecting-images/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 Wok\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "StyleGAN2: projecting images",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "stylegan2-projecting-images",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "woctezuma",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/woctezuma/stylegan2-projecting-images/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 207,
      "date": "Wed, 29 Dec 2021 09:06:47 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "google-colab",
      "google-colaboratory",
      "google-colab-notebook",
      "colab",
      "colaboratory",
      "colab-notebook",
      "gan",
      "generative-adversarial-network",
      "stylegan",
      "style-gan",
      "stylegan-model",
      "stylegan2"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To discover how to project a real image using the original StyleGAN2 implementation, run:\n-   [`stylegan2_projecting_images.ipynb`][stylegan2_projecting_images]\n[![Open In Colab][colab-badge]][stylegan2_projecting_images]\n\nTo process the projection of **a batch** of images, using either `W(1,*)` (original) or `W(18,*)` (extended), run:\n-   [`stylegan2_projecting_images_with_my_fork.ipynb`][stylegan2_projecting_images_with_fork]\n[![Open In Colab][colab-badge]][stylegan2_projecting_images_with_fork]\n\nTo edit latent vectors of projected images, run:\n-   [`stylegan2_editing_latent_vectors.ipynb`][stylegan2_editing_latent_vectors]\n[![Open In Colab][colab-badge]][stylegan2_editing_latent_vectors]\n\nFor more information about `W(1,*)` and `W(18,*)`, please refer to the [the original paper][stylegan2-paper] (section 5 on page 7):\n\n> Inverting the synthesis network $g$ is an interesting problem that has many applications.\n> Manipulating a given image in the latent feature space requires finding a matching latent code $w$ for it first.\n\nThe following is about `W(18,*)`:\n> Previous research suggests that instead of finding a common latent code $w$, the results improve if a separate $w$ is chosen for each layer of the generator.\n> The same approach was used in an early encoder implementation.\n\nThe following is about `W(1,*)`, which is the approach used in the original implementation:\n> While extending the latent space in this fashion finds a closer match to a given image, it also enables projecting arbitrary images that should have no latent representation.\n> Instead, we concentrate on finding latent codes in the original, unextended latent space, as these correspond to images that the generator could have produced.\n\n",
      "technique": "Header extraction"
    }
  ]
}