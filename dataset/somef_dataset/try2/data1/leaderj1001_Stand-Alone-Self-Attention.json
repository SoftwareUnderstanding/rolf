{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1906.05909",
      "https://arxiv.org/abs/1904.09925"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "  - [ResNet Pytorch CIFAR](https://github.com/kuangliu/pytorch-cifar)\n  - [ResNet Pytorch](https://github.com/pytorch/vision/blob/8350645b680b5dc0ef347de82deea5ae3f8ca3dc/torchvision/models/resnet.py)\n  - Thank you :)\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/leaderj1001/Stand-Alone-Self-Attention",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-07-03T13:18:13Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-20T20:39:37Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8282328203432069,
        0.8282328203432069,
        0.8282328203432069,
        0.8282328203432069,
        0.8282328203432069,
        0.8282328203432069
      ],
      "excerpt": "Prajit Ramachandran (Google Research, Brain Team) \nNiki Parmar (Google Research, Brain Team) \nAshish Vaswani (Google Research, Brain Team) \nIrwan Bello (Google Research, Brain Team) \nAnselm Levskaya (Google Research, Brain Team) \nJonathon Shlens (Google Research, Brain Team) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.964206439284686
      ],
      "excerpt": "The row and column offsets are associated with an embedding  and  respectively each with dimension . The row and column offset embeddings are concatenated to form . This spatial-relative attention is now defined as below equation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8945317191428339
      ],
      "excerpt": "I refer to the following paper when implementing this part. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9056537592890719,
        0.9105605891379306,
        0.9874404749804127
      ],
      "excerpt": "Replacing Spatial Convolutions<br> \nA 2 \u00d7 2 average pooling with stride 2 operation follows the attention layer whenever spatial downsampling is required. \nThis work applies the transform on the ResNet family of architectures. The proposed transform swaps the 3 \u00d7 3 spatial convolution with a self-attention layer as defined in Equation 3. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9776692655414991,
        0.9317902622978311
      ],
      "excerpt": "The initial layers of a CNN, sometimes referred to as the stem, play a critical role in learning local features such as edges, which later layers use to identify global objects. \nThe stem performs self-attention within each 4 \u00d7 4 spatial block of the original image, followed by batch normalization and a 4 \u00d7 4 max pool operation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Implementing Stand-Alone Self-Attention in Vision Models using Pytorch",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/leaderj1001/Stand-Alone-Self-Attention/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 74,
      "date": "Wed, 29 Dec 2021 16:55:12 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/leaderj1001/Stand-Alone-Self-Attention/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "leaderj1001/Stand-Alone-Self-Attention",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "  - Spatial extent: 7\n  - Attention heads: 8\n  - Layers:\n    - ResNet 26: [1, 2, 4, 1]\n    - ResNet 38: [2, 3, 5, 2]\n    - ResNet 50: [3, 4, 6, 3]\n    \n| Datasets | Model | Accuracy | Parameters (My Model, Paper Model)\n| :---: | :---: | :---: | :---: |\nCIFAR-10 | ResNet 26 | 90.94% | 8.30M, -\nCIFAR-10 | Naive ResNet 26 | 94.29% | 8.74M\nCIFAR-10 | ResNet 26 + stem | 90.22% | 8.30M, -\nCIFAR-10 | ResNet 38 (WORK IN PROCESS) | 89.46% | 12.1M, -\nCIFAR-10 | Naive ResNet 38 | 94.93% | 15.0M\nCIFAR-10 | ResNet 50 (WORK IN PROCESS) | | 16.0M, -\nIMAGENET | ResNet 26 (WORK IN PROCESS) | | 10.3M, 10.3M\nIMAGENET | ResNet 38 (WORK IN PROCESS) | | 14.1M, 14.1M\nIMAGENET | ResNet 50 (WORK IN PROCESS) | | 18.0M, 18.0M\n\n",
      "technique": "Header extraction"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/leaderj1001/Stand-Alone-Self-Attention/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 Myeongjun Kim\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Implementing Stand-Alone Self-Attention in Vision Models using Pytorch (13 Jun 2019)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Stand-Alone-Self-Attention",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "leaderj1001",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/leaderj1001/Stand-Alone-Self-Attention/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "  - torch==1.0.1\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 382,
      "date": "Wed, 29 Dec 2021 16:55:12 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "stand-alone-self-attention",
      "self-attention",
      "pytorch"
    ],
    "technique": "GitHub API"
  }
}