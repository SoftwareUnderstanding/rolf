{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1706.03762"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/whr94621/NJUNMT-pytorch",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you have any question, please contact [whr94621@foxmail.com](mailto:whr94621@foxmail.com)",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-03-31T09:19:51Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-10T13:25:28Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8506460229423587,
        0.8669121488949849,
        0.9752554759221925,
        0.9809582345177142,
        0.9441357393146788
      ],
      "excerpt": "NJUNMT-pytorch is an open-source toolkit for neural machine translation. \nThis toolkit is highly research-oriented, which contains some common baseline \nDL4MT-tutorial: A rnn-base nmt model widely used as baseline. To our knowledge, this is the \nonly pytorch implementation which is exactly the same as original model.(nmtpytorch is another pytorch implementation but with minor structure difference.) \nAttention is all you need: A strong nmt model introduced by Google, which only relies on attenion \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9968029537584643
      ],
      "excerpt": "Table of Contents \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9467387342424483
      ],
      "excerpt": "We highly recommend not to set the limitation of the number of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8241919683866752
      ],
      "excerpt": "transformer_nist_zh2en_bpe.yaml: to run a Transformer model on NIST Chinese to English using BPE. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9316963712479776
      ],
      "excerpt": "To further learn how to configure a NMT training task, see this wiki page. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8697742491631683,
        0.9229761229875236
      ],
      "excerpt": "MyModel.best.final: Final best model, i.e., the model achieved best performance on validation set. Only model parameters are kept in it. \nWhen training is over, our code will automatically save the best model. Usually you could just use the final best model, which is named as xxxx.best.final, to translate. This model achieves the best performance on the validation set. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8868218691346859
      ],
      "excerpt": "Also our code support ensemble decoding. See more options by running python -m src.bin.ensemble_translate --help \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/whr94621/NJUNMT-pytorch/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 31,
      "date": "Sun, 26 Dec 2021 15:59:13 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/whr94621/NJUNMT-pytorch/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "whr94621/NJUNMT-pytorch",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/whr94621/NJUNMT-pytorch/master/scripts/train.sh",
      "https://raw.githubusercontent.com/whr94621/NJUNMT-pytorch/master/scripts/translate.sh",
      "https://raw.githubusercontent.com/whr94621/NJUNMT-pytorch/master/.travis/install.sh",
      "https://raw.githubusercontent.com/whr94621/NJUNMT-pytorch/master/unittests/run_all_test.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9096104964140866
      ],
      "excerpt": "1. Build Vocabulary \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8606280910157142
      ],
      "excerpt": "0. Quick Start \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8174540907975313
      ],
      "excerpt": "3. Training \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8184605694110304
      ],
      "excerpt": "python ./scripts/build_dictionary.py --help \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8008765007753428,
        0.8008765007753428
      ],
      "excerpt": "dl4mt_nist_zh2en.yaml: to run a DL4MT model on NIST Chinese to Enligsh \ntransformer_nist_zh2en.yaml: to run a Transformer model on NIST Chinese to English \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8008765007753428
      ],
      "excerpt": "transformer_wmt14_en2de.yaml: to run a Transformer model on WMT14 English to German \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9379118997810907
      ],
      "excerpt": "python -m src.bin.train \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9003478393399441
      ],
      "excerpt": "See detail options by running python -m src.bin.train --help. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8637661436624472
      ],
      "excerpt": "MyModel.ckpt: A text file recording names of all the kept checkpoints \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8181661677799777
      ],
      "excerpt": "python -m src.bin.translate \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8244869010020718
      ],
      "excerpt": "See detail options by running python -m src.bin.translate --help. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/whr94621/NJUNMT-pytorch/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Perl",
      "Emacs Lisp",
      "Roff",
      "Smalltalk",
      "Ruby",
      "NewLisp",
      "Shell",
      "JavaScript",
      "Slash",
      "SystemVerilog"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2018 OpenNMT\\nCopyright (c) 2018 WEI HAORAN\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "NJUNMT-pytorch",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "NJUNMT-pytorch",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "whr94621",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/whr94621/NJUNMT-pytorch/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "whr94621",
        "body": "### New Features - Travis CI Enable\r\nWe add components for travis ci. Now all tests can only run on CPU.\r\n\r\n### New Features - Standalone Logging Module\r\nWe combine functions about logging into a standalone module. Now we can redirect logging info to files when using ```tqdm``` at the same time.\r\n\r\n### New Features - Different way to batch\r\nThis feature enables you to use different way to batch your data. We provide two method, \"samples\" and \"tokens\". \"samples\" means how many bi-text pairs (samples) in one batch, while \"tokens\" means how many tokens in one batch (if there are several sentences in one sample, this means most tokens among them). You can use these two kinds of method by setting \"batching_key\" as \"samples\" or \"tokens\".\r\n\r\n### New Features - Delayed Update\r\nThis feature enables you to emuluate multi GPUs on a single GPU. By setting ```update_cycle``` as a value larger than 1, the model will compute forward and accumulate gradients for this many steps before parameters update, which behaves like one update step with actual batch size as ```update_cycle * batch_size```. For example, if we want to use 25000 tokens in a batch on a single 1080 GPU(8GB Mem), we can set ```batch_size``` as 1250 and ```update_cycle``` as 20. This will prevent OOM problem.\r\n\r\n### Improvments\r\n\r\n- Put source sentences with similar length into a batch during inference. This significantly improve the speed of beam search(beam size is 5) from 497.01 tokens/sec to 1179.05 tokens/sec on a single 1080ti GPU.\r\n- Use SacreBLEU to compute BLEU scores instead of ```multi-bleu.perl``` and ```multi-bleu-detok.perl```.\r\n- Add ```AdamW``` and ```Adafactor```\r\n- Count the number of pads when using \"tokens\" as batching key.\r\n- Add ensemble decoding with different checkpoints.\r\n- Add length penalty when decoding.\r\n- Save k-best models.\r\n- Add ```dim_per_head``` option for transformer. Now ```dim_per_head``` * ```n_head``` can not equal to ```d_model```.\r\n- Add exponential moving average (EMA).\r\n- Support pytorch 0.4.1\r\n\r\n### Bugs fix\r\n- Mask padding in generator.\r\n- RAM will not continue increasing during training.\r\n- close file handles when shuffling is over\r\n- fix the typo of ```Criterion```\r\n\r\n### API Changes\r\n\r\n- Refactor RNN for data parallelism support.\r\n",
        "dateCreated": "2018-10-09T10:11:21Z",
        "datePublished": "2018-10-09T12:44:43Z",
        "html_url": "https://github.com/whr94621/NJUNMT-pytorch/releases/tag/20181009",
        "name": "Stable version with all commits until Oct 9, 2018",
        "tag_name": "20181009",
        "tarball_url": "https://api.github.com/repos/whr94621/NJUNMT-pytorch/tarball/20181009",
        "url": "https://api.github.com/repos/whr94621/NJUNMT-pytorch/releases/13327716",
        "zipball_url": "https://api.github.com/repos/whr94621/NJUNMT-pytorch/zipball/20181009"
      },
      {
        "authorType": "User",
        "author_name": "whr94621",
        "body": "",
        "dateCreated": "2018-09-14T02:09:54Z",
        "datePublished": "2018-09-14T02:11:39Z",
        "html_url": "https://github.com/whr94621/NJUNMT-pytorch/releases/tag/2018.09-rc0",
        "name": "release candidate of update in Sep 2018",
        "tag_name": "2018.09-rc0",
        "tarball_url": "https://api.github.com/repos/whr94621/NJUNMT-pytorch/tarball/2018.09-rc0",
        "url": "https://api.github.com/repos/whr94621/NJUNMT-pytorch/releases/12894009",
        "zipball_url": "https://api.github.com/repos/whr94621/NJUNMT-pytorch/zipball/2018.09-rc0"
      },
      {
        "authorType": "User",
        "author_name": "whr94621",
        "body": "This is the final version using Pytorch 0.3.1.\r\nWe will only provide minimum maintenance and bug fix.\r\n",
        "dateCreated": "2018-05-07T16:04:01Z",
        "datePublished": "2018-05-16T07:53:07Z",
        "html_url": "https://github.com/whr94621/NJUNMT-pytorch/releases/tag/v0.3",
        "name": "NJUNMT-pytorch using Pytorch-0.3.1",
        "tag_name": "v0.3",
        "tarball_url": "https://api.github.com/repos/whr94621/NJUNMT-pytorch/tarball/v0.3",
        "url": "https://api.github.com/repos/whr94621/NJUNMT-pytorch/releases/11021474",
        "zipball_url": "https://api.github.com/repos/whr94621/NJUNMT-pytorch/zipball/v0.3"
      }
    ],
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- python 3.5+\n- pytorch 0.4.0+\n- tqdm\n- tensorboardX\n- sacrebleu\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 90,
      "date": "Sun, 26 Dec 2021 15:59:13 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "pytorch",
      "attention-is-all-you-need",
      "neural-machine-translation",
      "nmt"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We provide push-button scripts to setup training and inference of\ntransformer model on NIST Chinese-English Corpus (only on NJUNLP's\nserver). Just execute under root directory of this repo\n``` bash\nbash ./scripts/train.sh\n```\nfor training and\n``` bash\n#: 3 means decoding on NIST 2003. This value\n#: can also be 4,5,6, which represents NIST 2004, 2005, 2006 respectively. \nbash ./scripts/translate.sh 3 \n```\n\n",
      "technique": "Header extraction"
    }
  ]
}