{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1609.03499 (2016).](https://arxiv.org/pdf/1609.03499.pdf)\n\n## Installation instructions\n\nThe code has only been tested and verified with Python 3.6. Assuming you have an installation of [pipenv](https://docs.pipenv.org/) for Python 3, you may clone the project, navigate to the root folder and run:\n\n```bash\nmake install\n```\n\nThis will most likely take care of the dependencies, unless you're using Windows.\n\n## Reproducibility: Running the examples\n\nIn the `examples` folder you will find a small sample of data, downloaded from the [LJ Speech Dataset](https://keithito.com/LJ-Speech-Dataset/). The dataset originally contains about 24 hours of speech, but I selected just a few files to create a small proof of concept, since I ran the training on my laptop and training such a complex architecture on a huge dataset was not viable for me. I used 50 files for training and 6 for validation.\n\n### Training\n\nTo train the network with the small amount of data provided in the package, navigate to the `examples` directory and run:\n\n```bash\npipenv run python train_small.py\n```\n\nFeel free to also tweak the parameters and add more data, if your computational resources allow it (e.g. use AWS spot instances with GPUs). For example, I see posts around the internet that use 1000-2000 epochs. I used 20, because an order of magnitude higher would take days to train. The filter size should also probably be larger (e.g. 64), and the residual blocks should be more (but keep in mind the paper recommends dilation rate `mod9`).\n\nIn the figure below, you may see a plot of the training loss, using the default parameters currently in `wavenet.examples.train_small`. It's obvious that the model is far from saturation.\n\n![Training Loss](wavenet/examples/training_loss.png)\n\n\n### Generating sound\n\nUsing the little network that I trained, the generated wavefile sounds like plain noise. However, if you'd like to generate your own wavefile, tweak the parameters accordingly (e.g. point to your own model) and run:\n\n```bash\npipenv run python generate_small.py\n```"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9987413024038266
      ],
      "excerpt": "This repository contains a basic implementation of the WaveNet as described in the paper published by DeepMind: Oord, Aaron van den, et al. \"Wavenet: A generative model for raw audio.\" arXiv preprint arXiv:1609.03499 (2016). \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/peustr/wavenet",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-07-04T07:33:27Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-28T08:10:50Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8892909951032695
      ],
      "excerpt": "This repository contains a basic implementation of the WaveNet as described in the paper published by DeepMind: Oord, Aaron van den, et al. \"Wavenet: A generative model for raw audio.\" arXiv preprint arXiv:1609.03499 (2016). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9577595946553553,
        0.9102442690090302,
        0.8023187087103166
      ],
      "excerpt": "Feel free to also tweak the parameters and add more data, if your computational resources allow it (e.g. use AWS spot instances with GPUs). For example, I see posts around the internet that use 1000-2000 epochs. I used 20, because an order of magnitude higher would take days to train. The filter size should also probably be larger (e.g. 64), and the residual blocks should be more (but keep in mind the paper recommends dilation rate mod9). \nIn the figure below, you may see a plot of the training loss, using the default parameters currently in wavenet.examples.train_small. It's obvious that the model is far from saturation. \nUsing the little network that I trained, the generated wavefile sounds like plain noise. However, if you'd like to generate your own wavefile, tweak the parameters accordingly (e.g. point to your own model) and run: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Basic implementation of the WaveNet as described in the paper published by DeepMind",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/peustr/wavenet/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 7,
      "date": "Mon, 27 Dec 2021 10:03:29 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/peustr/wavenet/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "peustr/wavenet",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The code has only been tested and verified with Python 3.6. Assuming you have an installation of [pipenv](https://docs.pipenv.org/) for Python 3, you may clone the project, navigate to the root folder and run:\n\n```bash\nmake install\n```\n\nThis will most likely take care of the dependencies, unless you're using Windows.\n\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.841362224686359
      ],
      "excerpt": "pipenv run python train_small.py \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/peustr/wavenet/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Makefile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "WaveNet Keras implementation",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "wavenet",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "peustr",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/peustr/wavenet/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "In the `examples` folder you will find a small sample of data, downloaded from the [LJ Speech Dataset](https://keithito.com/LJ-Speech-Dataset/). The dataset originally contains about 24 hours of speech, but I selected just a few files to create a small proof of concept, since I ran the training on my laptop and training such a complex architecture on a huge dataset was not viable for me. I used 50 files for training and 6 for validation.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 20,
      "date": "Mon, 27 Dec 2021 10:03:29 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "In the `examples` folder you will find a small sample of data, downloaded from the [LJ Speech Dataset](https://keithito.com/LJ-Speech-Dataset/). The dataset originally contains about 24 hours of speech, but I selected just a few files to create a small proof of concept, since I ran the training on my laptop and training such a complex architecture on a huge dataset was not viable for me. I used 50 files for training and 6 for validation.\n\n",
      "technique": "Header extraction"
    }
  ]
}