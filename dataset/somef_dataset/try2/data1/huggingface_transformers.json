{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1909.11942",
      "https://arxiv.org/abs/2010.12321",
      "https://arxiv.org/abs/2109.09701",
      "https://arxiv.org/abs/2106.08254",
      "https://arxiv.org/abs/1810.04805",
      "https://arxiv.org/abs/1907.12461",
      "https://arxiv.org/abs/2007.14062",
      "https://arxiv.org/abs/2007.14062",
      "https://arxiv.org/abs/2004.13637",
      "https://arxiv.org/abs/2004.13637",
      "https://arxiv.org/abs/2010.10499",
      "https://arxiv.org/abs/2105.13626",
      "https://arxiv.org/abs/1911.03894",
      "https://arxiv.org/abs/2103.06874",
      "https://arxiv.org/abs/2103.00020",
      "https://arxiv.org/abs/2008.02496",
      "https://arxiv.org/abs/2012.00413",
      "https://arxiv.org/abs/1909.05858",
      "https://arxiv.org/abs/2006.03654",
      "https://arxiv.org/abs/2006.03654",
      "https://arxiv.org/abs/2012.12877",
      "https://arxiv.org/abs/2005.12872",
      "https://arxiv.org/abs/1911.00536",
      "https://arxiv.org/abs/1910.01108",
      "https://arxiv.org/abs/2004.04906",
      "https://arxiv.org/abs/1907.12461",
      "https://arxiv.org/abs/2003.10555",
      "https://arxiv.org/abs/1912.05372",
      "https://arxiv.org/abs/2105.03824",
      "https://arxiv.org/abs/2006.03236",
      "https://arxiv.org/abs/2106.07447",
      "https://arxiv.org/abs/2101.01321",
      "https://arxiv.org/abs/1912.13318",
      "https://arxiv.org/abs/2012.14740",
      "https://arxiv.org/abs/2104.08836",
      "https://arxiv.org/abs/2004.05150",
      "https://arxiv.org/abs/2004.05150",
      "https://arxiv.org/abs/2010.01057",
      "https://arxiv.org/abs/2110.08151",
      "https://arxiv.org/abs/1908.07490",
      "https://arxiv.org/abs/2010.11125",
      "https://arxiv.org/abs/2001.08210",
      "https://arxiv.org/abs/2008.00401",
      "https://arxiv.org/abs/1909.08053",
      "https://arxiv.org/abs/1909.08053",
      "https://arxiv.org/abs/2004.09297",
      "https://arxiv.org/abs/2010.11934",
      "https://arxiv.org/abs/1912.08777",
      "https://arxiv.org/abs/2107.14795",
      "https://arxiv.org/abs/2001.04063",
      "https://arxiv.org/abs/2004.09602",
      "https://arxiv.org/abs/2001.04451",
      "https://arxiv.org/abs/1907.11692",
      "https://arxiv.org/abs/2105.15203",
      "https://arxiv.org/abs/2109.06870",
      "https://arxiv.org/abs/2109.06870",
      "https://arxiv.org/abs/2010.05171",
      "https://arxiv.org/abs/2104.06678",
      "https://arxiv.org/abs/2101.00438",
      "https://arxiv.org/abs/2006.11316",
      "https://arxiv.org/abs/1910.10683",
      "https://arxiv.org/abs/2004.02349",
      "https://arxiv.org/abs/1901.02860",
      "https://arxiv.org/abs/2109.10282",
      "https://arxiv.org/abs/2101.07597",
      "https://arxiv.org/abs/2110.05752",
      "https://arxiv.org/abs/2010.11929",
      "https://arxiv.org/abs/2110.13900",
      "https://arxiv.org/abs/2006.11477",
      "https://arxiv.org/abs/2109.11680",
      "https://arxiv.org/abs/1901.07291",
      "https://arxiv.org/abs/2001.04063",
      "https://arxiv.org/abs/1911.02116",
      "https://arxiv.org/abs/1906.08237",
      "https://arxiv.org/abs/2006.13979",
      "https://arxiv.org/abs/2111.09296"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We now have a [paper](https://www.aclweb.org/anthology/2020.emnlp-demos.6/) you can cite for the \ud83e\udd17 Transformers library:\n```bibtex\n@inproceedings{wolf-etal-2020-transformers,\n    title = \"Transformers: State-of-the-Art Natural Language Processing\",\n    author = \"Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R\u00e9mi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush\",\n    booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations\",\n    month = oct,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.emnlp-demos.6\",\n    pages = \"38--45\"\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "cff-version: \"1.2.0\"\ndate-released: 2020-10\nmessage: \"If you use this software, please cite it using these metadata.\"\ntitle: \"Transformers: State-of-the-Art Natural Language Processing\"\nurl: \"https://github.com/huggingface/transformers\"\nauthors: \n  - family-names: Wolf\n    given-names: Thomas\n  - family-names: Debut\n    given-names: Lysandre\n  - family-names: Sanh\n    given-names: Victor\n  - family-names: Chaumond\n    given-names: Julien\n  - family-names: Delangue\n    given-names: Clement\n  - family-names: Moi\n    given-names: Anthony\n  - family-names: Cistac\n    given-names: Perric\n  - family-names: Ma\n    given-names: Clara\n  - family-names: Jernite\n    given-names: Yacine\n  - family-names: Plu\n    given-names: Julien\n  - family-names: Xu\n    given-names: Canwen\n  - family-names: \"Le Scao\"\n    given-names: Teven\n  - family-names: Gugger\n    given-names: Sylvain\n  - family-names: Drame\n    given-names: Mariama\n  - family-names: Lhoest\n    given-names: Quentin\n  - family-names: Rush\n    given-names: \"Alexander M.\"\npreferred-citation:\n  type: conference-paper\n  authors:\n  - family-names: Wolf\n    given-names: Thomas\n  - family-names: Debut\n    given-names: Lysandre\n  - family-names: Sanh\n    given-names: Victor\n  - family-names: Chaumond\n    given-names: Julien\n  - family-names: Delangue\n    given-names: Clement\n  - family-names: Moi\n    given-names: Anthony\n  - family-names: Cistac\n    given-names: Perric\n  - family-names: Ma\n    given-names: Clara\n  - family-names: Jernite\n    given-names: Yacine\n  - family-names: Plu\n    given-names: Julien\n  - family-names: Xu\n    given-names: Canwen\n  - family-names: \"Le Scao\"\n    given-names: Teven\n  - family-names: Gugger\n    given-names: Sylvain\n  - family-names: Drame\n    given-names: Mariama\n  - family-names: Lhoest\n    given-names: Quentin\n  - family-names: Rush\n    given-names: \"Alexander M.\"\n  booktitle: \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations\"\n  month: 10\n  start: 38\n  end: 45\n  title: \"Transformers: State-of-the-Art Natural Language Processing\"\n  year: 2020\n  publisher: \"Association for Computational Linguistics\"\n  url: \"https://www.aclweb.org/anthology/2020.emnlp-demos.6\"\n  address: \"Online\"",
      "technique": "File Exploration"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{wolf-etal-2020-transformers,\n    title = \"Transformers: State-of-the-Art Natural Language Processing\",\n    author = \"Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R\u00e9mi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush\",\n    booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations\",\n    month = oct,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.emnlp-demos.6\",\n    pages = \"38--45\"\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8049556878500507
      ],
      "excerpt": "ALBERT (from Google Research and the Toyota Technological Institute at Chicago) released with the paper ALBERT: A Lite BERT for Self-supervised Learning of Language Representations, by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.960844153059911,
        0.960844153059911
      ],
      "excerpt": "BigBird-RoBERTa (from Google Research) released with the paper Big Bird: Transformers for Longer Sequences by Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed. \nBigBird-Pegasus (from Google Research) released with the paper Big Bird: Transformers for Longer Sequences by Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999999997570228
      ],
      "excerpt": "CPM (from Tsinghua University) released with the paper CPM: A Large-scale Generative Chinese Pre-trained Language Model by Zhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin, Yusheng Su, Haozhe Ji, Jian Guan, Fanchao Qi, Xiaozhi Wang, Yanan Zheng, Guoyang Zeng, Huanqi Cao, Shengqi Chen, Daixuan Li, Zhenbo Sun, Zhiyuan Liu, Minlie Huang, Wentao Han, Jie Tang, Juanzi Li, Xiaoyan Zhu, Maosong Sun. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9224120781423357,
        0.9224120781423357
      ],
      "excerpt": "DeBERTa (from Microsoft) released with the paper DeBERTa: Decoding-enhanced BERT with Disentangled Attention by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen. \nDeBERTa-v2 (from Microsoft) released with the paper DeBERTa: Decoding-enhanced BERT with Disentangled Attention by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9998549814448879
      ],
      "excerpt": "DialoGPT (from Microsoft Research) released with the paper DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation by Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9947441578983044
      ],
      "excerpt": "Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8158728424649643
      ],
      "excerpt": "GPT Neo (from EleutherAI) released in the repository EleutherAI/gpt-neo by Sid Black, Stella Biderman, Leo Gao, Phil Wang and Connor Leahy. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9961672157318373,
        0.9999448650437389,
        0.9893818392506678
      ],
      "excerpt": "LayoutLM (from Microsoft Research Asia) released with the paper LayoutLM: Pre-training of Text and Layout for Document Image Understanding by Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou. \nLayoutLMv2 (from Microsoft Research Asia) released with the paper LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding by Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, Min Zhang, Lidong Zhou. \nLayoutXLM (from Microsoft Research Asia) released with the paper LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding by Yiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Furu Wei. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9914552515116442
      ],
      "excerpt": "MPNet (from Microsoft Research) released with the paper MPNet: Masked and Permuted Pre-training for Language Understanding by Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9994608875320523
      ],
      "excerpt": "ProphetNet (from Microsoft Research) released with the paper ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang and Ming Zhou. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9698456178435246
      ],
      "excerpt": "RoFormer (from ZhuiyiTechnology), released together with the paper a RoFormer: Enhanced Transformer with Rotary Position Embedding by Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9054674780638166,
        0.9054674780638166
      ],
      "excerpt": "SEW (from ASAPP) released with the paper Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition by Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q. Weinberger, Yoav Artzi. \nSEW-D (from ASAPP) released with the paper Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition by Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q. Weinberger, Yoav Artzi. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9481636676461799,
        0.9991182586083446
      ],
      "excerpt": "T5 (from Google AI) released with the paper Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer by Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu. \nT5v1.1 (from Google AI) released in the repository google-research/text-to-text-transfer-transformer by Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9957552296411708,
        0.9996946423201425
      ],
      "excerpt": "TrOCR (from Microsoft), released together with the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, Furu Wei. \nUniSpeech (from Microsoft Research) released with the paper UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei, Michael Zeng, Xuedong Huang. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999999996650928
      ],
      "excerpt": "AWARE PRE-TRAINING by Sanyuan Chen, Yu Wu, Chengyi Wang, Zhengyang Chen, Zhuo Chen, Shujie Liu, Jian Wu, Yao Qian, Furu Wei, Jinyu Li, Xiangzhan Yu. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9926629811017625,
        0.9999999919830032
      ],
      "excerpt": "VisualBERT (from UCLA NLP) released with the paper VisualBERT: A Simple and Performant Baseline for Vision and Language by Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang. \nWavLM (from Microsoft Research) released with the paper WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing by Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Furu Wei. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9994608875320523
      ],
      "excerpt": "XLM-ProphetNet (from Microsoft Research) released with the paper ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang and Ming Zhou. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeOfConduct": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://raw.githubusercontent.com/huggingface/transformers/master/CODE_OF_CONDUCT.md",
    "technique": "File Exploration"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/huggingface/transformers",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "../../CONTRIBUTING.md",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-10-29T13:56:00Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-20T10:30:02Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9612533282159047
      ],
      "excerpt": "To immediately use a model on a given input (text, image, audio, ...), we provide the pipeline API. Pipelines group together a pretrained model with the preprocessing that was used during that model's training. Here is how to quickly use a pipeline to classify positive versus negative texts: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9786684922669034
      ],
      "excerpt": "classifier('We are very happy to introduce pipeline to the transformers repository.') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9826495951660394
      ],
      "excerpt": "The second line of code downloads and caches the pretrained model used by the pipeline, while the third evaluates it on the given text. Here the answer is \"positive\" with a confidence of 99.97%. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9430870119737885
      ],
      "excerpt": "...     'question': 'What is the name of the repository ?', \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9705463568106852,
        0.9789439873332314
      ],
      "excerpt": "In addition to the answer, the pretrained model used here returned its confidence score, along with the start position and end position of the answer in the tokenized sentence. You can learn more about the tasks supported by the pipeline API in this tutorial. \nTo download and use any of the pretrained models on your given task, all it takes is three lines of code. Here is the PyTorch version: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = AutoModel.from_pretrained(\"bert-base-uncased\") \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.891475770063986
      ],
      "excerpt": "And here is the equivalent code for TensorFlow:python \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = TFAutoModel.from_pretrained(\"bert-base-uncased\") \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9445524874417685,
        0.9771351941680707,
        0.9320994217202709
      ],
      "excerpt": "The tokenizer is responsible for all the preprocessing the pretrained model expects, and can be called directly on a single string (as in the above examples) or a list. It will output a dictionary that you can use in downstream code or simply directly pass to your model using the ** argument unpacking operator. \nThe model itself is a regular Pytorch nn.Module or a TensorFlow tf.keras.Model (depending on your backend) which you can use normally. This tutorial explains how to integrate such a model into a classic PyTorch or TensorFlow training loop, or how to use our Trainer API to quickly fine-tune on a new dataset. \nThis repository is tested on Python 3.6+, Flax 0.3.2+, PyTorch 1.3.1+ and TensorFlow 2.3+. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9205382708895892
      ],
      "excerpt": "All the model checkpoints provided by \ud83e\udd17 Transformers are seamlessly integrated from the huggingface.co model hub where they are uploaded directly by users and organizations. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9628720376714208,
        0.9736652794802054,
        0.8764726064202877,
        0.8790231868260262,
        0.8150015223675746
      ],
      "excerpt": "\ud83e\udd17 Transformers currently provides the following architectures (see here for a high-level summary of each them): \nALBERT (from Google Research and the Toyota Technological Institute at Chicago) released with the paper ALBERT: A Lite BERT for Self-supervised Learning of Language Representations, by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut. \nBART (from Facebook) released with the paper BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension by Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov and Luke Zettlemoyer. \nBARThez (from \u00c9cole polytechnique) released with the paper BARThez: a Skilled Pretrained French Sequence-to-Sequence Model by Moussa Kamal Eddine, Antoine J.-P. Tixier, Michalis Vazirgiannis. \nBARTpho (from VinAI Research) released with the paper BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese by Nguyen Luong Tran, Duong Minh Le and Dat Quoc Nguyen. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9788134372186559,
        0.8991001657299316
      ],
      "excerpt": "BERT (from Google) released with the paper BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. \nBERTweet (from VinAI Research) released with the paper BERTweet: A pre-trained language model for English Tweets by Dat Quoc Nguyen, Thanh Vu and Anh Tuan Nguyen. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9296033481198636,
        0.9296033481198636,
        0.8869669729109408,
        0.8828908931033665,
        0.9660127435188732,
        0.8884194706926981,
        0.8389097829231387
      ],
      "excerpt": "Blenderbot (from Facebook) released with the paper Recipes for building an open-domain chatbot by Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston. \nBlenderbotSmall (from Facebook) released with the paper Recipes for building an open-domain chatbot by Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston. \nBORT (from Alexa) released with the paper Optimal Subarchitecture Extraction For BERT by Adrian de Wynter and Daniel J. Perry. \nByT5 (from Google Research) released with the paper ByT5: Towards a token-free future with pre-trained byte-to-byte models by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel. \nCamemBERT (from Inria/Facebook/Sorbonne) released with the paper CamemBERT: a Tasty French Language Model by Louis Martin, Benjamin Muller, Pedro Javier Ortiz Su\u00e1rez*, Yoann Dupont, Laurent Romary, \u00c9ric Villemonte de la Clergerie, Djam\u00e9 Seddah and Beno\u00eet Sagot. \nCANINE (from Google Research) released with the paper CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation by Jonathan H. Clark, Dan Garrette, Iulia Turc, John Wieting. \nCLIP (from OpenAI) released with the paper Learning Transferable Visual Models From Natural Language Supervision by Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9118863848161362,
        0.8124770529775768,
        0.8124770529775768,
        0.8328263732438693,
        0.937863236660039
      ],
      "excerpt": "CTRL (from Salesforce) released with the paper CTRL: A Conditional Transformer Language Model for Controllable Generation by Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney, Caiming Xiong and Richard Socher. \nDeBERTa (from Microsoft) released with the paper DeBERTa: Decoding-enhanced BERT with Disentangled Attention by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen. \nDeBERTa-v2 (from Microsoft) released with the paper DeBERTa: Decoding-enhanced BERT with Disentangled Attention by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen. \nDeiT (from Facebook) released with the paper Training data-efficient image transformers & distillation through attention by Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Herv\u00e9 J\u00e9gou. \nDETR (from Facebook) released with the paper End-to-End Object Detection with Transformers by Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9656123185123114,
        0.9463945794320573
      ],
      "excerpt": "DistilBERT (from HuggingFace), released together with the paper DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter by Victor Sanh, Lysandre Debut and Thomas Wolf. The same method has been applied to compress GPT2 into DistilGPT2, RoBERTa into DistilRoBERTa, Multilingual BERT into DistilmBERT and a German version of DistilBERT. \nDPR (from Facebook) released with the paper Dense Passage Retrieval \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8049613910530294,
        0.9060543876814151,
        0.9166771747727696,
        0.9693243584750562,
        0.8356071331431575,
        0.8530797045319042
      ],
      "excerpt": "ELECTRA (from Google Research/Stanford University) released with the paper ELECTRA: Pre-training text encoders as discriminators rather than generators by Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning. \nFlauBERT (from CNRS) released with the paper FlauBERT: Unsupervised Language Model Pre-training for French by Hang Le, Lo\u00efc Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre Allauzen, Beno\u00eet Crabb\u00e9, Laurent Besacier, Didier Schwab. \nFNet (from Google Research) released with the paper FNet: Mixing Tokens with Fourier Transforms by James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon. \nFunnel Transformer (from CMU/Google Brain) released with the paper Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le. \nGPT (from OpenAI) released with the paper Improving Language Understanding by Generative Pre-Training by Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever. \nGPT-2 (from OpenAI) released with the paper Language Models are Unsupervised Multitask Learners by Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei and Ilya Sutskever. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8933011636586505
      ],
      "excerpt": "Hubert (from Facebook) released with the paper HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8517737614749551,
        0.9585109201012286,
        0.8588771203825143,
        0.8156419144498069,
        0.8719564929187055,
        0.8365556173176075,
        0.9237057369666539,
        0.9176892250319914,
        0.9176892250319914
      ],
      "excerpt": "LUKE (from Studio Ousia) released with the paper LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention by Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, Yuji Matsumoto. \nmLUKE (from Studio Ousia) released with the paper mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models by Ryokan Ri, Ikuya Yamada, and Yoshimasa Tsuruoka. \nLXMERT (from UNC Chapel Hill) released with the paper LXMERT: Learning Cross-Modality Encoder Representations from Transformers for Open-Domain Question Answering by Hao Tan and Mohit Bansal. \nM2M100 (from Facebook) released with the paper Beyond English-Centric Multilingual Machine Translation by Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, Armand Joulin. \nMarianMT Machine translation models trained using OPUS data by J\u00f6rg Tiedemann. The Marian Framework is being developed by the Microsoft Translator Team. \nMBart (from Facebook) released with the paper Multilingual Denoising Pre-training for Neural Machine Translation by Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, Luke Zettlemoyer. \nMBart-50 (from Facebook) released with the paper Multilingual Translation with Extensible Multilingual Pretraining and Finetuning by Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, Angela Fan. \nMegatron-BERT (from NVIDIA) released with the paper Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper and Bryan Catanzaro. \nMegatron-GPT2 (from NVIDIA) released with the paper Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper and Bryan Catanzaro. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8479881222019631,
        0.8909422767557146
      ],
      "excerpt": "MT5 (from Google AI) released with the paper mT5: A massively multilingual pre-trained text-to-text transformer by Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel. \nPegasus (from Google) released with the paper PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8600812667465064
      ],
      "excerpt": "PhoBERT (from VinAI Research) released with the paper PhoBERT: Pre-trained language models for Vietnamese by Dat Quoc Nguyen and Anh Tuan Nguyen. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8545838051476162
      ],
      "excerpt": "Reformer (from Google Research) released with the paper Reformer: The Efficient Transformer by Nikita Kitaev, \u0141ukasz Kaiser, Anselm Levskaya. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8244021621274669,
        0.893865947708711
      ],
      "excerpt": "RoBERTa (from Facebook), released together with the paper a Robustly Optimized BERT Pretraining Approach by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov. \nRoFormer (from ZhuiyiTechnology), released together with the paper a RoFormer: Enhanced Transformer with Rotary Position Embedding by Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8451299786491898,
        0.8362846878691914
      ],
      "excerpt": "SpeechToTextTransformer (from Facebook), released together with the paper fairseq S2T: Fast Speech-to-Text Modeling with fairseq by Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Dmytro Okhonko, Juan Pino. \nSpeechToTextTransformer2 (from Facebook), released together with the paper Large-Scale Self- and Semi-Supervised Learning for Speech Translation by Changhan Wang, Anne Wu, Juan Pino, Alexei Baevski, Michael Auli, Alexis Conneau. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.983640433156996,
        0.9538996010811599
      ],
      "excerpt": "T5 (from Google AI) released with the paper Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer by Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu. \nT5v1.1 (from Google AI) released in the repository google-research/text-to-text-transfer-transformer by Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8935326417771288
      ],
      "excerpt": "Transformer-XL (from Google/CMU) released with the paper Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context by Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8440558451933003
      ],
      "excerpt": "UniSpeechSat (from Microsoft Research) released with the paper UNISPEECH-SAT: UNIVERSAL SPEECH REPRESENTATION LEARNING WITH SPEAKER \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8974982353407919
      ],
      "excerpt": "Vision Transformer (ViT) (from Google AI) released with the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9168391122766739
      ],
      "excerpt": "Wav2Vec2 (from Facebook AI) released with the paper wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9594426707598807
      ],
      "excerpt": "XLM (from Facebook) released together with the paper Cross-lingual Language Model Pretraining by Guillaume Lample and Alexis Conneau. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9099751131449999,
        0.9410141275101865,
        0.832414646010384
      ],
      "excerpt": "XLM-RoBERTa (from Facebook AI), released together with the paper Unsupervised Cross-lingual Representation Learning at Scale by Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov. \nXLNet (from Google/CMU) released with the paper \u200bXLNet: Generalized Autoregressive Pretraining for Language Understanding by Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le. \nXLSR-Wav2Vec2 (from Facebook AI) released with the paper Unsupervised Cross-Lingual Representation Learning For Speech Recognition by Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, Michael Auli. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9776193860886575,
        0.8920355310829552,
        0.881972809871478
      ],
      "excerpt": "Want to contribute a new model? We have added a detailed guide and templates to guide you in the process of adding a new model. You can find them in the templates folder of the repository. Be sure to check the contributing guidelines and contact the maintainers or open an issue to collect feedbacks before starting your PR. \nTo check if each model has an implementation in Flax, PyTorch or TensorFlow, or has an associated tokenizer backed by the \ud83e\udd17 Tokenizers library, refer to this table. \nThese implementations have been tested on several datasets (see the example scripts) and should match the performance of the original implementations. You can find more details on performance in the Examples section of the documentation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9641810759755608,
        0.821989989834198
      ],
      "excerpt": "| Preprocessing tutorial | Using the Tokenizer class to prepare data for the models | \n| Training and fine-tuning | Using the models provided by \ud83e\udd17 Transformers in a PyTorch/TensorFlow training loop and the Trainer API | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "\ud83e\udd17 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.",
      "technique": "GitHub API"
    }
  ],
  "documentation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "https://jax.readthedocs.io/",
      "technique": "Regular expression"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/huggingface/transformers/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 13184,
      "date": "Mon, 20 Dec 2021 10:37:42 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/huggingface/transformers/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "huggingface/transformers",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/huggingface/transformers/master/docker/transformers-pytorch-cpu/Dockerfile",
      "https://raw.githubusercontent.com/huggingface/transformers/master/docker/transformers-pytorch-tpu/Dockerfile",
      "https://raw.githubusercontent.com/huggingface/transformers/master/docker/transformers-tensorflow-cpu/Dockerfile",
      "https://raw.githubusercontent.com/huggingface/transformers/master/docker/transformers-tensorflow-gpu/Dockerfile",
      "https://raw.githubusercontent.com/huggingface/transformers/master/docker/transformers-cpu/Dockerfile",
      "https://raw.githubusercontent.com/huggingface/transformers/master/docker/transformers-pytorch-gpu/Dockerfile",
      "https://raw.githubusercontent.com/huggingface/transformers/master/docker/transformers-gpu/Dockerfile",
      "https://raw.githubusercontent.com/huggingface/transformers/master/examples/research_projects/quantization-qdqbert/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/huggingface/transformers/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/huggingface/transformers/master/examples/research_projects/movement-pruning/Saving_PruneBERT.ipynb",
      "https://raw.githubusercontent.com/huggingface/transformers/master/examples/research_projects/lxmert/demo.ipynb",
      "https://raw.githubusercontent.com/huggingface/transformers/master/examples/research_projects/visual_bert/demo.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/huggingface/transformers/master/scripts/fsmt/convert-allenai-wmt19.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/scripts/fsmt/eval-allenai-wmt16.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/scripts/fsmt/eval-facebook-wmt19.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/scripts/fsmt/s3-move.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/scripts/fsmt/convert-allenai-wmt16.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/scripts/fsmt/tests-to-run.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/scripts/fsmt/eval-allenai-wmt19.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/scripts/fsmt/convert-facebook-wmt19.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/scripts/tatoeba/upload_models.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/.github/conda/build.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/docker/transformers-pytorch-tpu/docker-entrypoint.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/examples/pytorch/multiple-choice/run_no_trainer.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/examples/pytorch/token-classification/run_no_trainer.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/examples/pytorch/token-classification/run.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/examples/legacy/pytorch-lightning/run_pos.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/examples/legacy/pytorch-lightning/run_glue.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/examples/legacy/pytorch-lightning/run_ner.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/examples/legacy/seq2seq/finetune_tpu.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/examples/legacy/seq2seq/train_distil_marian_enro_tpu.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/examples/legacy/seq2seq/train_mbart_cc25_enro.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/examples/legacy/seq2seq/train_distilbart_cnn.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/examples/legacy/seq2seq/finetune.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/examples/legacy/seq2seq/train_distil_marian_enro.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/examples/legacy/token-classification/run_pos.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/examples/legacy/token-classification/run_chunk.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/examples/legacy/token-classification/run.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/examples/research_projects/performer/full_script.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/examples/research_projects/performer/sanity_script.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/examples/research_projects/deebert/eval_deebert.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/examples/research_projects/deebert/train_deebert.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/examples/research_projects/deebert/entropy_eval.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/examples/research_projects/seq2seq-distillation/distil_marian_no_teacher.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/examples/research_projects/seq2seq-distillation/finetune_pegasus_xsum.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/examples/research_projects/seq2seq-distillation/dynamic_bs_example.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/examples/research_projects/seq2seq-distillation/finetune_bart_tiny.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/examples/research_projects/seq2seq-distillation/train_mbart_cc25_enro.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/examples/research_projects/seq2seq-distillation/train_distilbart_cnn.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/examples/research_projects/seq2seq-distillation/distil_marian_enro_teacher.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/examples/research_projects/seq2seq-distillation/finetune.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/examples/research_projects/seq2seq-distillation/finetune_t5.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/examples/research_projects/seq2seq-distillation/train_distilbart_xsum.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/examples/research_projects/rag-end2end-retriever/finetune_rag_ray_end2end.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/examples/research_projects/rag-end2end-retriever/test_run/test_finetune.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/examples/research_projects/rag-end2end-retriever/test_run/test_rag_new_features.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/examples/research_projects/rag/finetune_rag.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/examples/research_projects/rag/finetune_rag_ray.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/examples/research_projects/wav2vec2/finetune_base_timit_asr.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/examples/research_projects/wav2vec2/finetune_wav2vec2_xlsr_turkish.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/examples/research_projects/wav2vec2/finetune_large_lv60_timit_asr.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/examples/research_projects/wav2vec2/finetune_base_100.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/examples/research_projects/wav2vec2/finetune_large_xlsr_53_arabic_speech_corpus.sh",
      "https://raw.githubusercontent.com/huggingface/transformers/master/examples/research_projects/wav2vec2/finetune_large_lv60_100.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8837680365796365
      ],
      "excerpt": "``` python \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9690925363488673,
        0.9370653482261196,
        0.9926181230482156,
        0.9591960852504129,
        0.9446458146199199,
        0.999746712887969,
        0.9194275961246684,
        0.961539783229912,
        0.9489738818989587
      ],
      "excerpt": "You should install \ud83e\udd17 Transformers in a virtual environment. If you're unfamiliar with Python virtual environments, check out the user guide. \nFirst, create a virtual environment with the version of Python you're going to use and activate it. \nThen, you will need to install at least one of Flax, PyTorch or TensorFlow. \nPlease refer to TensorFlow installation page, PyTorch installation page and/or Flax installation page regarding the specific install command for your platform. \nWhen one of those backends has been installed, \ud83e\udd17 Transformers can be installed using pip as follows: \npip install transformers \nIf you'd like to play with the examples or need the bleeding edge of the code and can't wait for a new release, you must install the library from source. \nSince Transformers version v4.0.0, we now have a conda channel: huggingface. \n\ud83e\udd17 Transformers can be installed using conda as follows: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9995234057802402,
        0.9874067599925529
      ],
      "excerpt": "conda install -c huggingface transformers \nFollow the installation pages of Flax, PyTorch or TensorFlow to see how to install them with conda. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "from transformers import AutoTokenizer, AutoModel \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "from transformers import AutoTokenizer, TFAutoModel \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/huggingface/transformers/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell",
      "Dockerfile",
      "Makefile",
      "Jsonnet"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Apache License 2.0",
      "url": "https://api.github.com/licenses/apache-2.0"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'Copyright 2018- The Hugging Face team. All rights reserved.\\n\\n                                 Apache License\\n                           Version 2.0, January 2004\\n                        http://www.apache.org/licenses/\\n\\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\\n\\n   1. Definitions.\\n\\n      \"License\" shall mean the terms and conditions for use, reproduction,\\n      and distribution as defined by Sections 1 through 9 of this document.\\n\\n      \"Licensor\" shall mean the copyright owner or entity authorized by\\n      the copyright owner that is granting the License.\\n\\n      \"Legal Entity\" shall mean the union of the acting entity and all\\n      other entities that control, are controlled by, or are under common\\n      control with that entity. For the purposes of this definition,\\n      \"control\" means (i) the power, direct or indirect, to cause the\\n      direction or management of such entity, whether by contract or\\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\\n      outstanding shares, or (iii) beneficial ownership of such entity.\\n\\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\\n      exercising permissions granted by this License.\\n\\n      \"Source\" form shall mean the preferred form for making modifications,\\n      including but not limited to software source code, documentation\\n      source, and configuration files.\\n\\n      \"Object\" form shall mean any form resulting from mechanical\\n      transformation or translation of a Source form, including but\\n      not limited to compiled object code, generated documentation,\\n      and conversions to other media types.\\n\\n      \"Work\" shall mean the work of authorship, whether in Source or\\n      Object form, made available under the License, as indicated by a\\n      copyright notice that is included in or attached to the work\\n      (an example is provided in the Appendix below).\\n\\n      \"Derivative Works\" shall mean any work, whether in Source or Object\\n      form, that is based on (or derived from) the Work and for which the\\n      editorial revisions, annotations, elaborations, or other modifications\\n      represent, as a whole, an original work of authorship. For the purposes\\n      of this License, Derivative Works shall not include works that remain\\n      separable from, or merely link (or bind by name) to the interfaces of,\\n      the Work and Derivative Works thereof.\\n\\n      \"Contribution\" shall mean any work of authorship, including\\n      the original version of the Work and any modifications or additions\\n      to that Work or Derivative Works thereof, that is intentionally\\n      submitted to Licensor for inclusion in the Work by the copyright owner\\n      or by an individual or Legal Entity authorized to submit on behalf of\\n      the copyright owner. For the purposes of this definition, \"submitted\"\\n      means any form of electronic, verbal, or written communication sent\\n      to the Licensor or its representatives, including but not limited to\\n      communication on electronic mailing lists, source code control systems,\\n      and issue tracking systems that are managed by, or on behalf of, the\\n      Licensor for the purpose of discussing and improving the Work, but\\n      excluding communication that is conspicuously marked or otherwise\\n      designated in writing by the copyright owner as \"Not a Contribution.\"\\n\\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\\n      on behalf of whom a Contribution has been received by Licensor and\\n      subsequently incorporated within the Work.\\n\\n   2. Grant of Copyright License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      copyright license to reproduce, prepare Derivative Works of,\\n      publicly display, publicly perform, sublicense, and distribute the\\n      Work and such Derivative Works in Source or Object form.\\n\\n   3. Grant of Patent License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      (except as stated in this section) patent license to make, have made,\\n      use, offer to sell, sell, import, and otherwise transfer the Work,\\n      where such license applies only to those patent claims licensable\\n      by such Contributor that are necessarily infringed by their\\n      Contribution(s) alone or by combination of their Contribution(s)\\n      with the Work to which such Contribution(s) was submitted. If You\\n      institute patent litigation against any entity (including a\\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\\n      or a Contribution incorporated within the Work constitutes direct\\n      or contributory patent infringement, then any patent licenses\\n      granted to You under this License for that Work shall terminate\\n      as of the date such litigation is filed.\\n\\n   4. Redistribution. You may reproduce and distribute copies of the\\n      Work or Derivative Works thereof in any medium, with or without\\n      modifications, and in Source or Object form, provided that You\\n      meet the following conditions:\\n\\n      (a) You must give any other recipients of the Work or\\n          Derivative Works a copy of this License; and\\n\\n      (b) You must cause any modified files to carry prominent notices\\n          stating that You changed the files; and\\n\\n      (c) You must retain, in the Source form of any Derivative Works\\n          that You distribute, all copyright, patent, trademark, and\\n          attribution notices from the Source form of the Work,\\n          excluding those notices that do not pertain to any part of\\n          the Derivative Works; and\\n\\n      (d) If the Work includes a \"NOTICE\" text file as part of its\\n          distribution, then any Derivative Works that You distribute must\\n          include a readable copy of the attribution notices contained\\n          within such NOTICE file, excluding those notices that do not\\n          pertain to any part of the Derivative Works, in at least one\\n          of the following places: within a NOTICE text file distributed\\n          as part of the Derivative Works; within the Source form or\\n          documentation, if provided along with the Derivative Works; or,\\n          within a display generated by the Derivative Works, if and\\n          wherever such third-party notices normally appear. The contents\\n          of the NOTICE file are for informational purposes only and\\n          do not modify the License. You may add Your own attribution\\n          notices within Derivative Works that You distribute, alongside\\n          or as an addendum to the NOTICE text from the Work, provided\\n          that such additional attribution notices cannot be construed\\n          as modifying the License.\\n\\n      You may add Your own copyright statement to Your modifications and\\n      may provide additional or different license terms and conditions\\n      for use, reproduction, or distribution of Your modifications, or\\n      for any such Derivative Works as a whole, provided Your use,\\n      reproduction, and distribution of the Work otherwise complies with\\n      the conditions stated in this License.\\n\\n   5. Submission of Contributions. Unless You explicitly state otherwise,\\n      any Contribution intentionally submitted for inclusion in the Work\\n      by You to the Licensor shall be under the terms and conditions of\\n      this License, without any additional terms or conditions.\\n      Notwithstanding the above, nothing herein shall supersede or modify\\n      the terms of any separate license agreement you may have executed\\n      with Licensor regarding such Contributions.\\n\\n   6. Trademarks. This License does not grant permission to use the trade\\n      names, trademarks, service marks, or product names of the Licensor,\\n      except as required for reasonable and customary use in describing the\\n      origin of the Work and reproducing the content of the NOTICE file.\\n\\n   7. Disclaimer of Warranty. Unless required by applicable law or\\n      agreed to in writing, Licensor provides the Work (and each\\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\\n      implied, including, without limitation, any warranties or conditions\\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\\n      PARTICULAR PURPOSE. You are solely responsible for determining the\\n      appropriateness of using or redistributing the Work and assume any\\n      risks associated with Your exercise of permissions under this License.\\n\\n   8. Limitation of Liability. In no event and under no legal theory,\\n      whether in tort (including negligence), contract, or otherwise,\\n      unless required by applicable law (such as deliberate and grossly\\n      negligent acts) or agreed to in writing, shall any Contributor be\\n      liable to You for damages, including any direct, indirect, special,\\n      incidental, or consequential damages of any character arising as a\\n      result of this License or out of the use or inability to use the\\n      Work (including but not limited to damages for loss of goodwill,\\n      work stoppage, computer failure or malfunction, or any and all\\n      other commercial damages or losses), even if such Contributor\\n      has been advised of the possibility of such damages.\\n\\n   9. Accepting Warranty or Additional Liability. While redistributing\\n      the Work or Derivative Works thereof, You may choose to offer,\\n      and charge a fee for, acceptance of support, warranty, indemnity,\\n      or other liability obligations and/or rights consistent with this\\n      License. However, in accepting such obligations, You may act only\\n      on Your own behalf and on Your sole responsibility, not on behalf\\n      of any other Contributor, and only if You agree to indemnify,\\n      defend, and hold each Contributor harmless for any liability\\n      incurred by, or claims asserted against, such Contributor by reason\\n      of your accepting any such warranty or additional liability.\\n\\n   END OF TERMS AND CONDITIONS\\n\\n   APPENDIX: How to apply the Apache License to your work.\\n\\n      To apply the Apache License to your work, attach the following\\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\\n      replaced with your own identifying information. (Don\\'t include\\n      the brackets!)  The text should be enclosed in the appropriate\\n      comment syntax for the file format. We also recommend that a\\n      file or class name and description of purpose be included on the\\n      same \"printed page\" as the copyright notice for easier\\n      identification within third-party archives.\\n\\n   Copyright [yyyy] [name of copyright owner]\\n\\n   Licensed under the Apache License, Version 2.0 (the \"License\");\\n   you may not use this file except in compliance with the License.\\n   You may obtain a copy of the License at\\n\\n       http://www.apache.org/licenses/LICENSE-2.0\\n\\n   Unless required by applicable law or agreed to in writing, software\\n   distributed under the License is distributed on an \"AS IS\" BASIS,\\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n   See the License for the specific language governing permissions and\\n   limitations under the License.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Online demos",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "transformers",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "huggingface",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/huggingface/transformers/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "sgugger",
        "body": "# v4.14.1 Patch release\r\n\r\nFixes a circular import when TensorFlow and Onnx are both installed (#14787)",
        "dateCreated": "2021-12-15T18:54:52Z",
        "datePublished": "2021-12-15T19:02:04Z",
        "html_url": "https://github.com/huggingface/transformers/releases/tag/v4.14.1",
        "name": "v4.14.1: Patch release",
        "tag_name": "v4.14.1",
        "tarball_url": "https://api.github.com/repos/huggingface/transformers/tarball/v4.14.1",
        "url": "https://api.github.com/repos/huggingface/transformers/releases/55393827",
        "zipball_url": "https://api.github.com/repos/huggingface/transformers/zipball/v4.14.1"
      },
      {
        "authorType": "User",
        "author_name": "LysandreJik",
        "body": "## Perceiver\r\n\r\nThe Perceiver model was released in the previous version:\r\n\r\n> ### Perceiver\r\n> \r\n> Eight new models are released as part of the Perceiver implementation: `PerceiverModel`, `PerceiverForMaskedLM`, `PerceiverForSequenceClassification`, `PerceiverForImageClassificationLearned`, `PerceiverForImageClassificationFourier`, `PerceiverForImageClassificationConvProcessing`, `PerceiverForOpticalFlow`, `PerceiverForMultimodalAutoencoding`, in PyTorch.\r\n> \r\n> The Perceiver IO model was proposed in [Perceiver IO: A General Architecture for Structured Inputs & Outputs](https://arxiv.org/abs/2107.14795) by Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch,\r\n> Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier H\u00e9naff, Matthew M.\r\n> Botvinick, Andrew Zisserman, Oriol Vinyals, Jo\u00e3o Carreira.\r\n> \r\n> * Add Perceiver IO by @NielsRogge in https://github.com/huggingface/transformers/pull/14487\r\n> \r\n> Compatible checkpoints can be found on the hub: https://huggingface.co/models?other=perceiver\r\n> \r\n\r\nVersion v4.14.0 adds support for Perceiver in multiple pipelines, including the fill mask and sequence classification pipelines.\r\n\r\n## Keras model cards\r\n\r\nThe Keras push to hub callback now generates model cards when pushing to the model hub. Additionally to the callback, model cards will be generated by default by the model.push_to_hub() method.\r\n\r\n* TF model cards by @Rocketknight1 in https://github.com/huggingface/transformers/pull/14720\r\n\r\n## What's Changed\r\n* Fix : wrong link in the documentation (ConvBERT vs DistilBERT) by @Tikquuss in https://github.com/huggingface/transformers/pull/14705\r\n* Put back open in colab markers by @sgugger in https://github.com/huggingface/transformers/pull/14684\r\n* Fix doc examples: KeyError by @ydshieh in https://github.com/huggingface/transformers/pull/14699\r\n* Fix doc examples: 'CausalLMOutput...' object has no attribute 'last_hidden_state' by @ydshieh in https://github.com/huggingface/transformers/pull/14678\r\n* Adding `Perceiver` to `AutoTokenizer`. by @Narsil in https://github.com/huggingface/transformers/pull/14711\r\n* Fix doc examples: unexpected keyword argument by @ydshieh in https://github.com/huggingface/transformers/pull/14689\r\n* Automatically build doc notebooks by @sgugger in https://github.com/huggingface/transformers/pull/14718\r\n* Fix special character in MDX by @sgugger in https://github.com/huggingface/transformers/pull/14721\r\n* Fixing tests for perceiver (texts) by @Narsil in https://github.com/huggingface/transformers/pull/14719\r\n* [doc] document MoE model approach and current solutions by @stas00 in https://github.com/huggingface/transformers/pull/14725\r\n* [Flax examples] remove dependancy on pytorch training args by @patil-suraj in https://github.com/huggingface/transformers/pull/14636\r\n* Update bug-report.md by @patrickvonplaten in https://github.com/huggingface/transformers/pull/14715\r\n* [Adafactor] Fix adafactor by @patrickvonplaten in https://github.com/huggingface/transformers/pull/14713\r\n* Code parrot minor fixes/niceties by @ncoop57 in https://github.com/huggingface/transformers/pull/14666\r\n* Fix doc examples: modify config before super().__init__ by @ydshieh in https://github.com/huggingface/transformers/pull/14697\r\n* Improve documentation of some models by @NielsRogge in https://github.com/huggingface/transformers/pull/14695\r\n* Skip Perceiver tests by @LysandreJik in https://github.com/huggingface/transformers/pull/14745\r\n* Add ability to get a list of supported pipeline tasks by @codesue in https://github.com/huggingface/transformers/pull/14732\r\n* Fix the perceiver docs by @LysandreJik in https://github.com/huggingface/transformers/pull/14748\r\n* [CI/pt-nightly] switch to cuda-11.3 by @stas00 in https://github.com/huggingface/transformers/pull/14726\r\n* Swap TF and PT code inside two blocks by @LucienShui in https://github.com/huggingface/transformers/pull/14742\r\n* Fix doc examples: cannot import name by @ydshieh in https://github.com/huggingface/transformers/pull/14698\r\n* Fix: change tooslow to slow by @ydshieh in https://github.com/huggingface/transformers/pull/14734\r\n* Small fixes for the doc by @sgugger in https://github.com/huggingface/transformers/pull/14751\r\n* Update transformers metadata by @sgugger in https://github.com/huggingface/transformers/pull/14724\r\n* Mention no images added to repository by @LysandreJik in https://github.com/huggingface/transformers/pull/14738\r\n* Avoid using tf.tile in embeddings for TF models by @ydshieh in https://github.com/huggingface/transformers/pull/14735\r\n* Change how to load config of XLNetLMHeadModel by @josutk in https://github.com/huggingface/transformers/pull/14746\r\n* Improve perceiver by @NielsRogge in https://github.com/huggingface/transformers/pull/14750\r\n* Convert Trainer doc page to MarkDown by @sgugger in https://github.com/huggingface/transformers/pull/14753\r\n* Update Table of Contents by @sgugger in https://github.com/huggingface/transformers/pull/14755\r\n* Fixing tests for Perceiver by @Narsil in https://github.com/huggingface/transformers/pull/14739\r\n* Make data shuffling in `run_clm_flax.py` respect global seed by @bminixhofer in https://github.com/huggingface/transformers/pull/13410\r\n* Adding support for multiple mask tokens. by @Narsil in https://github.com/huggingface/transformers/pull/14716\r\n* Fix broken links to distillation on index page of documentation by @amitness in https://github.com/huggingface/transformers/pull/14722\r\n* [doc] performance: groups of operations by compute-intensity by @stas00 in https://github.com/huggingface/transformers/pull/14757\r\n* Fix the doc_build_test job by @sgugger in https://github.com/huggingface/transformers/pull/14774\r\n* Fix preprocess_function in run_summarization_flax.py by @ydshieh in https://github.com/huggingface/transformers/pull/14769\r\n* Simplify T5 docs by @xhlulu in https://github.com/huggingface/transformers/pull/14776\r\n\r\n* Update Perceiver code examples by @NielsRogge in https://github.com/huggingface/transformers/pull/14783\r\n\r\n## New Contributors\r\n* @Tikquuss made their first contribution in https://github.com/huggingface/transformers/pull/14705\r\n* @codesue made their first contribution in https://github.com/huggingface/transformers/pull/14732\r\n* @LucienShui made their first contribution in https://github.com/huggingface/transformers/pull/14742\r\n* @josutk made their first contribution in https://github.com/huggingface/transformers/pull/14746\r\n* @amitness made their first contribution in https://github.com/huggingface/transformers/pull/14722\r\n\r\n**Full Changelog**: https://github.com/huggingface/transformers/compare/v4.13.0...v4.14.0",
        "dateCreated": "2021-12-15T17:20:51Z",
        "datePublished": "2021-12-15T17:27:37Z",
        "html_url": "https://github.com/huggingface/transformers/releases/tag/v4.14.0",
        "name": "v4.14.0: Perceiver, Keras model cards",
        "tag_name": "v4.14.0",
        "tarball_url": "https://api.github.com/repos/huggingface/transformers/tarball/v4.14.0",
        "url": "https://api.github.com/repos/huggingface/transformers/releases/55381232",
        "zipball_url": "https://api.github.com/repos/huggingface/transformers/zipball/v4.14.0"
      },
      {
        "authorType": "User",
        "author_name": "LysandreJik",
        "body": "## New Model additions\r\n\r\n### Perceiver\r\n\r\nEight new models are released as part of the Perceiver implementation: `PerceiverModel`, `PerceiverForMaskedLM`, `PerceiverForSequenceClassification`, `PerceiverForImageClassificationLearned`, `PerceiverForImageClassificationFourier`, `PerceiverForImageClassificationConvProcessing`, `PerceiverForOpticalFlow`, `PerceiverForMultimodalAutoencoding`, in PyTorch.\r\n\r\nThe Perceiver IO model was proposed in [Perceiver IO: A General Architecture for Structured Inputs & Outputs](https://arxiv.org/abs/2107.14795) by Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch,\r\nCatalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier H\u00e9naff, Matthew M.\r\nBotvinick, Andrew Zisserman, Oriol Vinyals, Jo\u00e3o Carreira.\r\n\r\n* Add Perceiver IO by @NielsRogge in https://github.com/huggingface/transformers/pull/14487\r\n\r\nCompatible checkpoints can be found on the hub: https://huggingface.co/models?other=perceiver\r\n\r\n### mLUKE\r\n\r\nThe mLUKE tokenizer is added. The tokenizer can be used for the multilingual variant of LUKE.\r\n\r\nThe mLUKE model was proposed in [mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models](https://arxiv.org/abs/2110.08151) by Ryokan Ri, Ikuya Yamada, and Yoshimasa Tsuruoka. It's a multilingual extension\r\nof the [LUKE model](https://arxiv.org/abs/2010.01057) trained on the basis of XLM-RoBERTa.\r\n\r\n* Add mLUKE by @Ryou0634 in https://github.com/huggingface/transformers/pull/14640\r\n\r\nCompatible checkpoints can be found on the hub: https://huggingface.co/models?other=luke\r\n\r\n### ImageGPT\r\n\r\nThree new models are released as part of the ImageGPT integration: `ImageGPTModel`, `ImageGPTForCausalImageModeling`, `ImageGPTForImageClassification`, in PyTorch. \r\n\r\nThe ImageGPT model was proposed in [Generative Pretraining from Pixels](https://openai.com/blog/image-gpt/) by Mark\r\nChen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, Ilya Sutskever. ImageGPT (iGPT) is a GPT-2-like\r\nmodel trained to predict the next pixel value, allowing for both unconditional and conditional image generation.\r\n\r\n* Add ImageGPT by @NielsRogge in https://github.com/huggingface/transformers/pull/14240\r\n\r\nCompatible checkpoints can be found on the hub: https://huggingface.co/models?other=imagegpt\r\n\r\n### QDQBert\r\n\r\nEight new models are released as part of the QDQBert implementation: `QDQBertModel`, `QDQBertLMHeadModel`, `QDQBertForMaskedLM`, `QDQBertForSequenceClassification`, `QDQBertForNextSentencePrediction`, `QDQBertForMultipleChoice`, `QDQBertForTokenClassification`, `QDQBertForQuestionAnswering`, in PyTorch.\r\n\r\nThe QDQBERT model can be referenced in [Integer Quantization for Deep Learning Inference: Principles and Empirical\r\nEvaluation](https://arxiv.org/abs/2004.09602) by Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev and Paulius\r\nMicikevicius.\r\n\r\n* Add QDQBert model and quantization examples of SQUAD task by @shangz-ai in https://github.com/huggingface/transformers/pull/14066\r\n\r\n### Semantic Segmentation models\r\n\r\n*The semantic Segmentation models' API is unstable and bound to change between this version and the next*.\r\n\r\nThe first semantic segmentation models are added. In semantic segmentation, the goal is to predict a class label for every pixel of an image. The models that are added are SegFormer (by NVIDIA) and BEiT (by Microsoft Research). BEiT was already available in the library, but this release includes the model with a semantic segmentation head.\r\n\r\nThe SegFormer model was proposed in [SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers](https://arxiv.org/abs/2105.15203) by Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, Ping Luo. The model consists of a hierarchical Transformer encoder and a lightweight all-MLP decode head to achieve great results on image segmentation benchmarks such as ADE20K and Cityscapes.\r\n\r\nThe BEiT model was proposed in [BEiT: BERT Pre-Training of Image Transformers](https://arxiv.org/abs/2106.08254) by Hangbo Bao, Li Dong, Furu Wei. Rather than pre-training the model to predict the class of an image (as done in the original ViT paper), BEiT models are pre-trained to predict visual tokens from the codebook of OpenAI\u2019s DALL-E model given masked patches.\r\n\r\n* Add SegFormer by @NielsRogge in https://github.com/huggingface/transformers/pull/14019\r\n* Add BeitForSemanticSegmentation by @NielsRogge in https://github.com/huggingface/transformers/pull/14096\r\n\r\n### Vision-text dual encoder\r\n\r\nAdds VisionTextDualEncoder model in PyTorch and Flax to be able to load any pre-trained vision (ViT, DeiT, BeiT, CLIP's vision model) and text (BERT, ROBERTA) model in the library for vision-text tasks like CLIP.\r\n\r\nThis model pairs a vision and text encoder and adds projection layers to project the embeddings to another embeddings space with similar dimensions. which can then be used to align the two modalities.\r\n\r\n* VisionTextDualEncoder by @patil-suraj in https://github.com/huggingface/transformers/pull/13511\r\n\r\n### CodeParrot\r\n\r\nCodeParrot, a model trained to generate code, has been open-sourced in the research projects by @lvwerra.\r\n\r\n* Add CodeParrot \ud83e\udd9c codebase by @lvwerra in https://github.com/huggingface/transformers/pull/14536\r\n\r\n## Language model support for ASR\r\n\r\n* Add language model support for CTC models by @patrickvonplaten in https://github.com/huggingface/transformers/pull/14339\r\nLanguage model boosted decoding is added for all CTC models via https://github.com/kensho-technologies/pyctcdecode and https://github.com/kpu/kenlm. \r\n\r\nSee https://huggingface.co/patrickvonplaten/wav2vec2-xlsr-53-es-kenlm for more information.\r\n\r\n## Flax-specific additions\r\n\r\nAdds Flax version of the vision encoder-decoder model, and adds a Flax version of GPT-J.\r\n\r\n* Add FlaxVisionEncoderDecoderModel by @ydshieh in https://github.com/huggingface/transformers/pull/13359\r\n* FlaxGPTJ by @patil-suraj in https://github.com/huggingface/transformers/pull/14396\r\n\r\n## TensorFlow-specific additions\r\n\r\nVision transformers are here! Convnets are so 2012, now that ML is [converging on self-attention as a universal model](https://twitter.com/karpathy/status/1468370605229547522).\r\n* Add TFViTModel by @ydshieh in https://github.com/huggingface/transformers/pull/13778\r\n\r\nWant to handle real-world tables, where text and data are positioned in a 2D grid? [TAPAS](https://huggingface.co/docs/transformers/model_doc/tapas) is now here for both TensorFlow and PyTorch.\r\n* Tapas tf by @kamalkraj in https://github.com/huggingface/transformers/pull/13393\r\n\r\nAutomatic checkpointing and cloud saves to the HuggingFace Hub during training are now live, allowing you to resume training when it's interrupted, even if your initial instance is terminated. This is an area of very active development - watch this space for future developments, including automatic model card creation and more.\r\n* Add model checkpointing to push_to_hub and PushToHubCallback by @Rocketknight1 in https://github.com/huggingface/transformers/pull/14492\r\n\r\n\r\n## Auto-processors\r\n\r\nA new class to automatically select processors is added: `AutoProcessor`. It can be used for all models that require a processor, in both computer vision and audio.\r\n\r\n* Auto processor by @sgugger in https://github.com/huggingface/transformers/pull/14465\r\n\r\n## New documentation frontend\r\n\r\nA new documentation frontend is out for the `transformers` library! The goal with this documentation is to be better aligned with the rest of our website, and contains tools to improve readability. The documentation can now be written in markdown rather than RST.\r\n\r\n* Doc new front by @sgugger in https://github.com/huggingface/transformers/pull/14590\r\n\r\n### LayoutLM Improvements\r\n\r\nThe LayoutLMv2 feature extractor now supports non-English languages, and LayoutXLM gets its own processor.\r\n\r\n* LayoutLMv2FeatureExtractor now supports non-English languages when applying Tesseract OCR. by @Xargonus in https://github.com/huggingface/transformers/pull/14514\r\n* Add LayoutXLMProcessor (and LayoutXLMTokenizer, LayoutXLMTokenizerFast) by @NielsRogge in https://github.com/huggingface/transformers/pull/14115\r\n\r\n\r\n## Trainer Improvements\r\n\r\nYou can now take advantage of the Ampere hardware with the Trainer:\r\n\r\n- `--bf16` - do training or eval in mixed precision of bfloat16\r\n- `--bf16_full_eval` - do eval in full bfloat16\r\n- `--tf32` control having TF32 mode on/off\r\n\r\n## Improvements and bugfixes\r\n\r\n* Replace assertions with RuntimeError exceptions by @ddrm86 in https://github.com/huggingface/transformers/pull/14186\r\n* Adding `batch_size` support for (almost) all pipelines by @Narsil in https://github.com/huggingface/transformers/pull/13724\r\n* Remove n_ctx from configs by @thomasw21 in https://github.com/huggingface/transformers/pull/14165\r\n* Add `BlenderbotTokenizerFast` by @stancld in https://github.com/huggingface/transformers/pull/13720\r\n* Adding `handle_long_generation` paramters for `text-generation` pipeline. by @Narsil in https://github.com/huggingface/transformers/pull/14118\r\n* Fix pipeline tests env and fetch by @sgugger in https://github.com/huggingface/transformers/pull/14209\r\n* Generalize problem_type to all sequence classification models by @sgugger in https://github.com/huggingface/transformers/pull/14180\r\n* Fixing image segmentation with inference mode. by @Narsil in https://github.com/huggingface/transformers/pull/14204\r\n* Add a condition for checking labels by @hrxorxm in https://github.com/huggingface/transformers/pull/14211\r\n* Torch 1.10 by @LysandreJik in https://github.com/huggingface/transformers/pull/14169\r\n* Add more missing models to models/__init__.py by @ydshieh in https://github.com/huggingface/transformers/pull/14177\r\n* Clarify QA examples by @NielsRogge in https://github.com/huggingface/transformers/pull/14172\r\n* Fixing `image-segmentation` tests. by @Narsil in https://github.com/huggingface/transformers/pull/14223\r\n* Tensor location is already handled by @Narsil in https://github.com/huggingface/transformers/pull/14224\r\n* Raising exceptions instead of using assertions for few models by @pdcoded in https://github.com/huggingface/transformers/pull/14219\r\n* Fix the write problem in trainer.py comment by @wmathor in https://github.com/huggingface/transformers/pull/14202\r\n* [GPTJ] enable common tests and few fixes by @patil-suraj in https://github.com/huggingface/transformers/pull/14190\r\n* improving efficiency of mlflow metric logging by @wamartin-aml in https://github.com/huggingface/transformers/pull/14232\r\n* Fix generation docstring by @qqaatw in https://github.com/huggingface/transformers/pull/14216\r\n* Fix test_configuration_tie in FlaxEncoderDecoderModelTest by @ydshieh in https://github.com/huggingface/transformers/pull/14076\r\n* [Tests] Fix DistilHubert path by @anton-l in https://github.com/huggingface/transformers/pull/14245\r\n* Add PushToHubCallback in main init by @sgugger in https://github.com/huggingface/transformers/pull/14246\r\n* Fixes Beit training for PyTorch 1.10+ by @sgugger in https://github.com/huggingface/transformers/pull/14249\r\n* Added Beit model ouput class by @lumliolum in https://github.com/huggingface/transformers/pull/14133\r\n* Update Transformers to huggingface_hub >= 0.1.0 by @sgugger in https://github.com/huggingface/transformers/pull/14251\r\n* Add cross attentions to TFGPT2Model by @ydshieh in https://github.com/huggingface/transformers/pull/14038\r\n* [Wav2Vec2] Adapt conversion script by @patrickvonplaten in https://github.com/huggingface/transformers/pull/14258\r\n* Put `load_image` function in `image_utils.py` & fix image rotation issue by @mishig25 in https://github.com/huggingface/transformers/pull/14062\r\n* minimal fixes to run DataCollatorForWholeWordMask with return_tensors=\"np\" and return_tensors=\"tf\" by @dwyatte in https://github.com/huggingface/transformers/pull/13891\r\n* Adding support for `truncation` parameter on `feature-extraction` pipeline. by @Narsil in https://github.com/huggingface/transformers/pull/14193\r\n* Fix of issue #13327: Wrong weight initialization for TF t5 model by @dshirron in https://github.com/huggingface/transformers/pull/14241\r\n* Fixing typo in error message. by @Narsil in https://github.com/huggingface/transformers/pull/14226\r\n* Pin Keras cause they messed their release by @sgugger in https://github.com/huggingface/transformers/pull/14262\r\n* Quality explain by @sgugger in https://github.com/huggingface/transformers/pull/14264\r\n* Add more instructions to the release guide by @sgugger in https://github.com/huggingface/transformers/pull/14263\r\n* Fixing slow pipeline tests by @Narsil in https://github.com/huggingface/transformers/pull/14260\r\n* Fixing mishandling of `ignore_labels`. by @Narsil in https://github.com/huggingface/transformers/pull/14274\r\n* improve rewrite state_dict missing _metadata by @changwangss in https://github.com/huggingface/transformers/pull/14276\r\n* Removing Keras version pinning by @Rocketknight1 in https://github.com/huggingface/transformers/pull/14280\r\n* Pin TF until tests are fixed by @sgugger in https://github.com/huggingface/transformers/pull/14283\r\n* [Hubert Docs] Make sure example uses a fine-tuned model by @patrickvonplaten in https://github.com/huggingface/transformers/pull/14291\r\n* Add new LFS prune API by @sgugger in https://github.com/huggingface/transformers/pull/14294\r\n* Remove `DPRPretrainedModel` from docs by @xhlulu in https://github.com/huggingface/transformers/pull/14300\r\n* Handle long answer needs to be updated. by @Narsil in https://github.com/huggingface/transformers/pull/14279\r\n* [tests] Fix SegFormer and BEiT tests by @NielsRogge in https://github.com/huggingface/transformers/pull/14289\r\n* Fix typo on PPLM example README by @Beomi in https://github.com/huggingface/transformers/pull/14287\r\n* [Marian Conversion] Fix eos_token_id conversion in conversion script by @patrickvonplaten in https://github.com/huggingface/transformers/pull/14320\r\n* [Tests] Update audio classification tests to support torch 1.10 by @anton-l in https://github.com/huggingface/transformers/pull/14318\r\n* [TFWav2Vec2Model] Fix input shapes in TFWav2Vec2WeightNormConv1D by @anton-l in https://github.com/huggingface/transformers/pull/14319\r\n* Fixing tests on master. by @Narsil in https://github.com/huggingface/transformers/pull/14317\r\n* Fixing mutable default argument in `pipeline`. by @Narsil in https://github.com/huggingface/transformers/pull/14316\r\n* Changed relative imports to absolute to allow convert_graph_to_onnx.py to run as a script. by @nbertagnolli in https://github.com/huggingface/transformers/pull/14325\r\n* Expand dynamic supported objects to configs and tokenizers by @sgugger in https://github.com/huggingface/transformers/pull/14296\r\n* [deepspeed] Enable multiple test runs on single box, defer to DS_TEST_PORT if set by @jeffra in https://github.com/huggingface/transformers/pull/14331\r\n* Small change to Wav2Vec2 model to support Tensor-Parallelism with DeepSpeed by @RezaYazdaniAminabadi in https://github.com/huggingface/transformers/pull/14298\r\n* Correct order of overflowing tokens for LayoutLmV2 tokenizer by @Apoorvgarg-creator in https://github.com/huggingface/transformers/pull/13495\r\n* Update Seq2Seq QA example script to use SQuAD metric. by @karthikrangasai in https://github.com/huggingface/transformers/pull/14335\r\n* remove an irrelevant test from test_modeling_tf_layoutlm by @ydshieh in https://github.com/huggingface/transformers/pull/14341\r\n* bump flax version by @patil-suraj in https://github.com/huggingface/transformers/pull/14343\r\n* Rewrite guides for fine-tuning with Datasets by @stevhliu in https://github.com/huggingface/transformers/pull/13923\r\n* [Bert2Bert] allow bert2bert + relative embeddings by @patrickvonplaten in https://github.com/huggingface/transformers/pull/14324\r\n* Support for TF >= 2.7 by @sgugger in https://github.com/huggingface/transformers/pull/14345\r\n* `BatchFeature`: Convert `List[np.ndarray]` to `np.ndarray` before converting to pytorch tensors by @eladsegal in https://github.com/huggingface/transformers/pull/14306\r\n* Adding some quality of life for `pipeline` function. by @Narsil in https://github.com/huggingface/transformers/pull/14322\r\n* Fix fast tokenization problems by @qqaatw in https://github.com/huggingface/transformers/pull/13930\r\n* Add notebook INC quantization for text classification tasks by @echarlaix in https://github.com/huggingface/transformers/pull/14293\r\n* enhance rewrite state_dict missing _metadata by @changwangss in https://github.com/huggingface/transformers/pull/14348\r\n* Fix list index out of range when padding nested empty lists by @qqaatw in https://github.com/huggingface/transformers/pull/13876\r\n* [testing] solve the port conflict by @stas00 in https://github.com/huggingface/transformers/pull/14362\r\n* Fix Flax params dtype by @patil-suraj in https://github.com/huggingface/transformers/pull/13098\r\n* [flax generate] allow passing params to encode by @patil-suraj in https://github.com/huggingface/transformers/pull/14370\r\n* Experimenting with adding proper get_config() and from_config() methods by @Rocketknight1 in https://github.com/huggingface/transformers/pull/14361\r\n* Fixing requirements for TF LM models and use correct model mappings by @Rocketknight1 in https://github.com/huggingface/transformers/pull/14372\r\n* fix loading flax bf16 weights in pt by @patil-suraj in https://github.com/huggingface/transformers/pull/14369\r\n* [wav2vec2] fix --gradient_checkpointing by @stas00 in https://github.com/huggingface/transformers/pull/13964\r\n* Adding support for raw python `generator` in addition to `Dataset` for pipelines by @Narsil in https://github.com/huggingface/transformers/pull/14352\r\n* minor doc fix by @patil-suraj in https://github.com/huggingface/transformers/pull/14377\r\n* [Wav2Vec2 Example] Improve fine-tuning script by @patrickvonplaten in https://github.com/huggingface/transformers/pull/14373\r\n* Use `AlbertConverter` for FNet instead of using FNet's own converter by @qqaatw in https://github.com/huggingface/transformers/pull/14365\r\n* Add support for WMT21 tokenizer in M2M100Tokenizer by @patil-suraj in https://github.com/huggingface/transformers/pull/14376\r\n* [M2M100Tokenizer] fix _build_translation_inputs by @patil-suraj in https://github.com/huggingface/transformers/pull/14382\r\n* Raise exceptions instead of using asserts  in modeling_openai #12789 by @nbertagnolli in https://github.com/huggingface/transformers/pull/14386\r\n* [doc] performance and parallelism updates by @stas00 in https://github.com/huggingface/transformers/pull/14391\r\n* Quick fix to TF summarization example by @Rocketknight1 in https://github.com/huggingface/transformers/pull/14401\r\n* [Speech2Text2] Enable tokenizers by @patrickvonplaten in https://github.com/huggingface/transformers/pull/14390\r\n* Fix TFViT by @NielsRogge in https://github.com/huggingface/transformers/pull/14399\r\n* Fix weight loading issue by @ydshieh in https://github.com/huggingface/transformers/pull/14016\r\n* Replace BertLayerNorm with LayerNorm by @eldarkurtic in https://github.com/huggingface/transformers/pull/14385\r\n* [Wav2Vec2] Make sure that gradient checkpointing is only run if needed by @patrickvonplaten in https://github.com/huggingface/transformers/pull/14407\r\n* Allow per-version configurations by @LysandreJik in https://github.com/huggingface/transformers/pull/14344\r\n* Fix gradient_checkpointing backward compatibility by @sgugger in https://github.com/huggingface/transformers/pull/14408\r\n* Add forward method to dummy models by @sgugger in https://github.com/huggingface/transformers/pull/14419\r\n* Avoid looping when data exhausted by @valentindey in https://github.com/huggingface/transformers/pull/14413\r\n* Debug doc by @sgugger in https://github.com/huggingface/transformers/pull/14424\r\n* [Wav2Vec2] Add New Wav2Vec2 Translation by @patrickvonplaten in https://github.com/huggingface/transformers/pull/14392\r\n* Improve semantic segmentation models by @NielsRogge in https://github.com/huggingface/transformers/pull/14355\r\n* [Gradient checkpoining] Update Wav2Vec scripts by @falcaopetri in https://github.com/huggingface/transformers/pull/14036\r\n* [Bart] Fix docs by @patrickvonplaten in https://github.com/huggingface/transformers/pull/14434\r\n* [WIP] Ensure TF model configs can be converted to proper JSON by @Zahlii in https://github.com/huggingface/transformers/pull/14415\r\n* Recover Deleted XNLI Instructions by @Helw150 in https://github.com/huggingface/transformers/pull/14437\r\n* Fix EncoderDecoderModel code example by @NielsRogge in https://github.com/huggingface/transformers/pull/14441\r\n* Add a post init method to all models by @sgugger in https://github.com/huggingface/transformers/pull/14431\r\n* Fix finite IterableDataset test on multiple GPUs by @sgugger in https://github.com/huggingface/transformers/pull/14445\r\n* [Bert, et al] fix early device assignment by @stas00 in https://github.com/huggingface/transformers/pull/14447\r\n* Add GitPython to quality tools by @LysandreJik in https://github.com/huggingface/transformers/pull/14459\r\n* [ImageGPT] Small fixes by @NielsRogge in https://github.com/huggingface/transformers/pull/14460\r\n* [Generation] Allow `inputs_embeds` as an input by @patrickvonplaten in https://github.com/huggingface/transformers/pull/14443\r\n* Adding support for `hidden_states` and `attentions` in unbatching support. by @Narsil in https://github.com/huggingface/transformers/pull/14420\r\n* add Tuple as possible type hint for EvalPredictions label_ids by @ameasure in https://github.com/huggingface/transformers/pull/14473\r\n* Fix dummy objects for quantization by @sgugger in https://github.com/huggingface/transformers/pull/14478\r\n* Moving pipeline tests from `Narsil` to `hf-internal-testing`. by @Narsil in https://github.com/huggingface/transformers/pull/14463\r\n* Improve `add-new-pipeline` docs a bit by @stancld in https://github.com/huggingface/transformers/pull/14485\r\n* [test] add test for --config_overrides by @stas00 in https://github.com/huggingface/transformers/pull/14466\r\n* Support for Training with BF16 by @JamesDeAntonis in https://github.com/huggingface/transformers/pull/13207\r\n* fixes some key names for in LayoutLMv2 / LayoutXLM tokenizers by @valentindey in https://github.com/huggingface/transformers/pull/14493\r\n* Switch from using sum for flattening lists of lists in group_texts by @nbroad1881 in https://github.com/huggingface/transformers/pull/14472\r\n* [deepspeed] zero inference by @stas00 in https://github.com/huggingface/transformers/pull/14253\r\n* add cache_dir for tokenizer verification loading by @vmaryasin in https://github.com/huggingface/transformers/pull/14508\r\n* Fix feature extraction utils import by @LysandreJik in https://github.com/huggingface/transformers/pull/14515\r\n* [Tests] Improve vision tests by @NielsRogge in https://github.com/huggingface/transformers/pull/14458\r\n* [CI] clear `~/.cache/torch_extensions` between builds by @stas00 in https://github.com/huggingface/transformers/pull/14520\r\n* Fix a slow test. by @Narsil in https://github.com/huggingface/transformers/pull/14527\r\n* added save_directories for _psave_pretrained_pt and _tf, changed model to tf_model and pt_model, enable the notebook to run cleanly from top to bottom without error by @cfregly in https://github.com/huggingface/transformers/pull/14529\r\n* Quicktour updates by @LysandreJik in https://github.com/huggingface/transformers/pull/14533\r\n* Fixes by @LysandreJik in https://github.com/huggingface/transformers/pull/14534\r\n* [flax] unfreeze initial cache in gpt models by @patil-suraj in https://github.com/huggingface/transformers/pull/14535\r\n* Tokenizers docs: Specify which class contains `__call__` method by @xhlulu in https://github.com/huggingface/transformers/pull/14379\r\n* Rename ImageGPT by @NielsRogge in https://github.com/huggingface/transformers/pull/14526\r\n* [Generate] Fix generate with inputs_embeds on GPU by @patrickvonplaten in https://github.com/huggingface/transformers/pull/14564\r\n* [Flax] token-classification model steps enumerate start from 1 by @kamalkraj in https://github.com/huggingface/transformers/pull/14547\r\n* Fix sentinel token IDs in data collator for Flax T5 pretraining script by @rahuln in https://github.com/huggingface/transformers/pull/14477\r\n* Fix backend regex by @sgugger in https://github.com/huggingface/transformers/pull/14566\r\n* [Flax] Add FlaxBlenderbot by @stancld in https://github.com/huggingface/transformers/pull/13633\r\n* Add documentation for multi-label classification by @gsnidero in https://github.com/huggingface/transformers/pull/14168\r\n* use functional interface for softmax in attention by @t-vi in https://github.com/huggingface/transformers/pull/14198\r\n* Fix mask token handling by @qqaatw in https://github.com/huggingface/transformers/pull/14364\r\n* [doc] bf16/tf32 guide by @stas00 in https://github.com/huggingface/transformers/pull/14579\r\n* Rename toctree.yml -> _toctree.yml by @mishig25 in https://github.com/huggingface/transformers/pull/14594\r\n* Update doc img links by @mishig25 in https://github.com/huggingface/transformers/pull/14593\r\n* Adds a git pull instruction to the documentation builder by @LysandreJik in https://github.com/huggingface/transformers/pull/14597\r\n* [Flax] Add FlaxBlenderbotSmall by @stancld in https://github.com/huggingface/transformers/pull/14576\r\n* Python 3.6 -> Python 3.7 for TF runs by @LysandreJik in https://github.com/huggingface/transformers/pull/14598\r\n* change tf.math.divide with int(/) in distilbert model by @yis11178 in https://github.com/huggingface/transformers/pull/14600\r\n* fix #14524 (IndexError when mask prob is too low) by @nikvaessen in https://github.com/huggingface/transformers/pull/14525\r\n* Improve tokenizer tests by @qqaatw in https://github.com/huggingface/transformers/pull/13594\r\n* [CI] move env print to util, add pt, nccl versions by @stas00 in https://github.com/huggingface/transformers/pull/14607\r\n* 2022 is the year of multi-modality by @LysandreJik in https://github.com/huggingface/transformers/pull/14610\r\n* Fix doc builder by @LysandreJik in https://github.com/huggingface/transformers/pull/14616\r\n* [trainer] add tf32-mode control by @stas00 in https://github.com/huggingface/transformers/pull/14606\r\n* Make DefaultDataCollator importable from root by @Rocketknight1 in https://github.com/huggingface/transformers/pull/14588\r\n* fix a typo by @yuchenlin in https://github.com/huggingface/transformers/pull/14626\r\n* updated pytorch token-classification readme by @kamalkraj in https://github.com/huggingface/transformers/pull/14624\r\n* Add Flax example tests by @patil-suraj in https://github.com/huggingface/transformers/pull/14599\r\n* fix typo by @patil-suraj in https://github.com/huggingface/transformers/pull/14635\r\n* add flax example tests in CI workflow by @patil-suraj in https://github.com/huggingface/transformers/pull/14637\r\n* [urls to hub] Replace outdated model tags with their now-canonical pipeline types by @julien-c in https://github.com/huggingface/transformers/pull/14617\r\n* Update the example of exporting Bart + BeamSearch to ONNX module to resolve comments. by @fatcat-z in https://github.com/huggingface/transformers/pull/14310\r\n* Add GPTJForQuestionAnswering by @tucan9389 in https://github.com/huggingface/transformers/pull/14503\r\n* doc: mismatch between pooler/d_output by @guhur in https://github.com/huggingface/transformers/pull/14641\r\n* fix flax example tests by @patil-suraj in https://github.com/huggingface/transformers/pull/14643\r\n* Auto processor fix by @LysandreJik in https://github.com/huggingface/transformers/pull/14623\r\n* Fix syntax for class references by @sgugger in https://github.com/huggingface/transformers/pull/14644\r\n* Add a job to test the documentation build by @sgugger in https://github.com/huggingface/transformers/pull/14645\r\n* fix flax examples tests by @patil-suraj in https://github.com/huggingface/transformers/pull/14646\r\n* Use cross_attention_hidden_size in Encoder-Decoder models by @ydshieh in https://github.com/huggingface/transformers/pull/14378\r\n* [deepspeed] fix --load_best_model_at_end by @stas00 in https://github.com/huggingface/transformers/pull/14652\r\n* quick fix SummarizationPipeline error messages by @NouamaneTazi in https://github.com/huggingface/transformers/pull/14618\r\n* Fix a Bug, trainer_seq2seq.py, in the else branch at Line 172, generation_inputs should be a dict by @TranSirius in https://github.com/huggingface/transformers/pull/14546\r\n* [trainer] conditional ctx managers into one wrapper by @stas00 in https://github.com/huggingface/transformers/pull/14663\r\n* Fixing Dataset for TQA + token-classification. by @Narsil in https://github.com/huggingface/transformers/pull/14658\r\n* fix deprecated tf method by @ZOHETH in https://github.com/huggingface/transformers/pull/14671\r\n* Fix doc builder by @LysandreJik in https://github.com/huggingface/transformers/pull/14676\r\n* [AutoProcessor] Add Wav2Vec2WithLM & small fix #14675 (@patrickvonplaten)\r\n* Added support for other features for already supported models #14358 (@michaelbenayoun)\r\n* Revert \"Added support for other features for already supported models\" #14679 (@lewtun)\r\n* Convert tutorials #14665 (@sgugger)\r\n* fix: verify jsonlines file in run_translation (#14660) #14661 (@GaurangTandon)\r\n* Improvements to Comet Integration #14680 (@DN6)\r\n* Fixes in init #14681 (@sgugger)\r\n* Revert open-in-colab and add perceiver #14683 (@sgugger)\r\n* Fix wrong checkpoint paths in doc examples #14685 (@ydshieh)\r\n* [bf16 support] tweaks #14580 (@stas00)\r\n* [trainer] support UserDict inputs (torch-nightly) #14688 (@stas00)\r\n* Move pyctcdecode #14686 (@sgugger)\r\n* Make MLuke tokenizer tests slow #14690 (@sgugger)\r\n* Fix doc examples: name '...' is not defined #14687 (@ydshieh)\r\n* Add a job to test doc building (for realsies this time) #14662 (@sgugger)\r\n* Fix Perceiver tests #14703 (@NielsRogge)\r\n* add str hub token to repository when provided else fallback to default #14682 (@philschmid)\r\n* Fix typo in toctree #14704 (@mishig25)\r\n\r\n## New Contributors\r\n* @hrxorxm made their first contribution in https://github.com/huggingface/transformers/pull/14211\r\n* @pdcoded made their first contribution in https://github.com/huggingface/transformers/pull/14219\r\n* @wmathor made their first contribution in https://github.com/huggingface/transformers/pull/14202\r\n* @wamartin-aml made their first contribution in https://github.com/huggingface/transformers/pull/14232\r\n* @lumliolum made their first contribution in https://github.com/huggingface/transformers/pull/14133\r\n* @dwyatte made their first contribution in https://github.com/huggingface/transformers/pull/13891\r\n* @dshirron made their first contribution in https://github.com/huggingface/transformers/pull/14241\r\n* @changwangss made their first contribution in https://github.com/huggingface/transformers/pull/14276\r\n* @xhlulu made their first contribution in https://github.com/huggingface/transformers/pull/14300\r\n* @Beomi made their first contribution in https://github.com/huggingface/transformers/pull/14287\r\n* @nbertagnolli made their first contribution in https://github.com/huggingface/transformers/pull/14325\r\n* @jeffra made their first contribution in https://github.com/huggingface/transformers/pull/14331\r\n* @RezaYazdaniAminabadi made their first contribution in https://github.com/huggingface/transformers/pull/14298\r\n* @echarlaix made their first contribution in https://github.com/huggingface/transformers/pull/14293\r\n* @valentindey made their first contribution in https://github.com/huggingface/transformers/pull/14413\r\n* @Zahlii made their first contribution in https://github.com/huggingface/transformers/pull/14415\r\n* @Helw150 made their first contribution in https://github.com/huggingface/transformers/pull/14437\r\n* @shangz-ai made their first contribution in https://github.com/huggingface/transformers/pull/14066\r\n* @vmaryasin made their first contribution in https://github.com/huggingface/transformers/pull/14508\r\n* @cfregly made their first contribution in https://github.com/huggingface/transformers/pull/14529\r\n* @Xargonus made their first contribution in https://github.com/huggingface/transformers/pull/14514\r\n* @rahuln made their first contribution in https://github.com/huggingface/transformers/pull/14477\r\n* @gsnidero made their first contribution in https://github.com/huggingface/transformers/pull/14168\r\n* @t-vi made their first contribution in https://github.com/huggingface/transformers/pull/14198\r\n* @JamesDeAntonis made their first contribution in https://github.com/huggingface/transformers/pull/13207\r\n* @yis11178 made their first contribution in https://github.com/huggingface/transformers/pull/14600\r\n* @nikvaessen made their first contribution in https://github.com/huggingface/transformers/pull/14525\r\n* @yuchenlin made their first contribution in https://github.com/huggingface/transformers/pull/14626\r\n* @Ryou0634 made their first contribution in https://github.com/huggingface/transformers/pull/14640\r\n* @NouamaneTazi made their first contribution in https://github.com/huggingface/transformers/pull/14618\r\n* @TranSirius made their first contribution in https://github.com/huggingface/transformers/pull/14546\r\n* @ZOHETH made their first contribution in https://github.com/huggingface/transformers/pull/14671\r\n\r\n**Full Changelog**: https://github.com/huggingface/transformers/compare/v4.12.0...v4.13.0",
        "dateCreated": "2021-12-09T15:57:38Z",
        "datePublished": "2021-12-09T16:07:24Z",
        "html_url": "https://github.com/huggingface/transformers/releases/tag/v4.13.0",
        "name": "v4.13.0: Perceiver, ImageGPT, mLUKE, Vision-Text dual encoders, QDQBert, new documentation frontend",
        "tag_name": "v4.13.0",
        "tarball_url": "https://api.github.com/repos/huggingface/transformers/tarball/v4.13.0",
        "url": "https://api.github.com/repos/huggingface/transformers/releases/54887450",
        "zipball_url": "https://api.github.com/repos/huggingface/transformers/zipball/v4.13.0"
      },
      {
        "authorType": "User",
        "author_name": "LysandreJik",
        "body": "Reverts a commit that introduced other issues:\r\n\r\n- Revert \"Experimenting with adding proper get_config() and from_config() methods (#14361)\"",
        "dateCreated": "2021-11-17T16:36:19Z",
        "datePublished": "2021-11-17T16:38:32Z",
        "html_url": "https://github.com/huggingface/transformers/releases/tag/v4.12.5",
        "name": "v4.12.5: Patch release",
        "tag_name": "v4.12.5",
        "tarball_url": "https://api.github.com/repos/huggingface/transformers/tarball/v4.12.5",
        "url": "https://api.github.com/repos/huggingface/transformers/releases/53572989",
        "zipball_url": "https://api.github.com/repos/huggingface/transformers/zipball/v4.12.5"
      },
      {
        "authorType": "User",
        "author_name": "LysandreJik",
        "body": "- Fix gradient_checkpointing backward compatibility (#14408) \r\n- [Wav2Vec2] Make sure that gradient checkpointing is only run if needed (#14407) \r\n- Experimenting with adding proper get_config() and from_config() methods (#14361) \r\n- enhance rewrite state_dict missing _metadata (#14348)\r\n- Support for TF >= 2.7 (#14345)\r\n- improve rewrite state_dict missing _metadata (#14276)\r\n- Fix of issue #13327: Wrong weight initialization for TF t5 model (#14241)",
        "dateCreated": "2021-11-16T22:25:59Z",
        "datePublished": "2021-11-16T22:31:27Z",
        "html_url": "https://github.com/huggingface/transformers/releases/tag/v4.12.4",
        "name": "v4.12.4: Patch release",
        "tag_name": "v4.12.4",
        "tarball_url": "https://api.github.com/repos/huggingface/transformers/tarball/v4.12.4",
        "url": "https://api.github.com/repos/huggingface/transformers/releases/53509941",
        "zipball_url": "https://api.github.com/repos/huggingface/transformers/zipball/v4.12.4"
      },
      {
        "authorType": "User",
        "author_name": "sgugger",
        "body": "# v4.12.3: Patch release\r\n\r\n- Add PushToHubCallback in main init (#14246)\r\n- Supports huggingface_hub >= 0.1.0",
        "dateCreated": "2021-11-03T12:57:53Z",
        "datePublished": "2021-11-03T13:05:08Z",
        "html_url": "https://github.com/huggingface/transformers/releases/tag/v4.12.3",
        "name": "v4.12.3: Patch release",
        "tag_name": "v4.12.3",
        "tarball_url": "https://api.github.com/repos/huggingface/transformers/tarball/v4.12.3",
        "url": "https://api.github.com/repos/huggingface/transformers/releases/52594111",
        "zipball_url": "https://api.github.com/repos/huggingface/transformers/zipball/v4.12.3"
      },
      {
        "authorType": "User",
        "author_name": "LysandreJik",
        "body": "Fixes an issue with the image segmentation pipeline and PyTorch's inference mode.",
        "dateCreated": "2021-10-29T18:48:15Z",
        "datePublished": "2021-10-29T18:52:07Z",
        "html_url": "https://github.com/huggingface/transformers/releases/tag/v4.12.2",
        "name": "v4.12.2: Patch release",
        "tag_name": "v4.12.2",
        "tarball_url": "https://api.github.com/repos/huggingface/transformers/tarball/v4.12.2",
        "url": "https://api.github.com/repos/huggingface/transformers/releases/52337895",
        "zipball_url": "https://api.github.com/repos/huggingface/transformers/zipball/v4.12.2"
      },
      {
        "authorType": "User",
        "author_name": "LysandreJik",
        "body": "Enables torch 1.10.0",
        "dateCreated": "2021-10-29T17:46:03Z",
        "datePublished": "2021-10-29T18:43:39Z",
        "html_url": "https://github.com/huggingface/transformers/releases/tag/v4.12.1",
        "name": "v4.12.1: Patch release",
        "tag_name": "v4.12.1",
        "tarball_url": "https://api.github.com/repos/huggingface/transformers/tarball/v4.12.1",
        "url": "https://api.github.com/repos/huggingface/transformers/releases/52333971",
        "zipball_url": "https://api.github.com/repos/huggingface/transformers/zipball/v4.12.1"
      },
      {
        "authorType": "User",
        "author_name": "LysandreJik",
        "body": "## TrOCR and VisionEncoderDecoderModel\r\n\r\nOne new model is released as part of the TrOCR implementation: `TrOCRForCausalLM`, in PyTorch. It comes along a new `VisionEncoderDecoderModel` class, which allows to mix-and-match any vision Transformer encoder with any text Transformer as decoder, similar to the existing `SpeechEncoderDecoderModel` class.\r\n\r\nThe TrOCR model was proposed in [TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models](https://arxiv.org/abs/2109.10282), by Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, Furu Wei.\r\n\r\nThe TrOCR model consists of an image transformer encoder and an autoregressive text transformer to perform optical character recognition in an end-to-end manner.\r\n\r\n* Add TrOCR + VisionEncoderDecoderModel by @NielsRogge in https://github.com/huggingface/transformers/pull/13874\r\n\r\nCompatible checkpoints can be found on the Hub: https://huggingface.co/models?other=trocr\r\n\r\n## SEW & SEW-D\r\n\r\nSEW and SEW-D (Squeezed and Efficient Wav2Vec) were proposed in [Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition](https://arxiv.org/abs/2109.06870) by Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q. Weinberger, Yoav Artzi.\r\n\r\nSEW and SEW-D models use a Wav2Vec-style feature encoder and introduce temporal downsampling to reduce the length of the transformer encoder. SEW-D additionally replaces the transformer encoder with a DeBERTa one. Both models achieve significant inference speedups without sacrificing the speech recognition quality. \r\n\r\n* Add the SEW and SEW-D speech models by @anton-l in https://github.com/huggingface/transformers/pull/13962\r\n* Add SEW CTC models by @anton-l in https://github.com/huggingface/transformers/pull/14158\r\n\r\nCompatible checkpoints are available on the Hub: https://huggingface.co/models?other=sew and https://huggingface.co/models?other=sew-d\r\n\r\n## DistilHuBERT\r\n\r\nDistilHuBERT was proposed in [DistilHuBERT: Speech Representation Learning by Layer-wise Distillation of Hidden-unit BERT](https://arxiv.org/abs/2110.01900), by Heng-Jui Chang, Shu-wen Yang, Hung-yi Lee.\r\n\r\nDistilHuBERT is a distilled version of the HuBERT model. Using only two transformer layers, the model scores competitively on the SUPERB benchmark tasks.\r\n\r\nCompatible checkpoint is available on the Hub: https://huggingface.co/ntu-spml/distilhubert\r\n\r\n## TensorFlow improvements\r\n\r\nSeveral bug fixes and UX improvements for TensorFlow\r\n\r\n### Keras callback \r\n\r\nIntroduction of a Keras callback to push to the hub each epoch, or after a given number of steps:\r\n\r\n* Keras callback to push to hub each epoch, or after N steps by @Rocketknight1 in https://github.com/huggingface/transformers/pull/13773\r\n\r\n### Updates on the encoder-decoder framework\r\n\r\nThe encoder-decoder framework is now available in TensorFlow, allowing mixing and matching different encoders and decoders together into a single encoder-decoder architecture!\r\n\r\n* Add TFEncoderDecoderModel + Add cross-attention to some TF models by @ydshieh in https://github.com/huggingface/transformers/pull/13222\r\n\r\nBesides this, the `EncoderDecoderModel` classes have been updated to work similar to models like BART and T5. From now on, users don't need to pass `decoder_input_ids` themselves anymore to the model. Instead, they will be created automatically based on the `labels` (namely by shifting them one position to the right, replacing -100 by the `pad_token_id` and prepending the `decoder_start_token_id`). Note that this may result in training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0 that set the `decoder_input_ids` = `labels`.\r\n\r\n* Fix EncoderDecoderModel classes to be more like BART and T5 by @NielsRogge  in https://github.com/huggingface/transformers/pull/14139\r\n\r\n## Speech improvements\r\n\r\n* Add DistilHuBERT  by @anton-l in https://github.com/huggingface/transformers/pull/14174\r\n* [Speech Examples] Add pytorch speech pretraining by @patrickvonplaten in https://github.com/huggingface/transformers/pull/13877\r\n* [Speech Examples] Add new audio feature by @patrickvonplaten in https://github.com/huggingface/transformers/pull/14027\r\n* Add ASR colabs by @patrickvonplaten in https://github.com/huggingface/transformers/pull/14067\r\n* [ASR] Make speech recognition example more general to load any tokenizer by @patrickvonplaten in https://github.com/huggingface/transformers/pull/14079\r\n* [Examples] Add an official audio classification example by @anton-l in https://github.com/huggingface/transformers/pull/13722\r\n* [Examples] Use Audio feature in speech classification by @anton-l in https://github.com/huggingface/transformers/pull/14052\r\n\r\n## Auto-model API\r\n\r\nTo make it easier to extend the Transformers library, every Auto class a new `register` method, that allows you to register your own custom models, configurations or tokenizers. See more in the [documentation](https://huggingface.co/transformers/model_doc/auto.html#extending-the-auto-classes)\r\n\r\n* Add an API to register objects to Auto classes by @sgugger in https://github.com/huggingface/transformers/pull/13989\r\n\r\n## Bug fixes and improvements\r\n\r\n* Fix filtering in test fetcher utils by @sgugger in https://github.com/huggingface/transformers/pull/13766\r\n* Fix warning for gradient_checkpointing by @sgugger in https://github.com/huggingface/transformers/pull/13767\r\n* Implement len in IterableDatasetShard by @sgugger in https://github.com/huggingface/transformers/pull/13780\r\n* [Wav2Vec2] Better error message by @patrickvonplaten in https://github.com/huggingface/transformers/pull/13777\r\n* Fix LayoutLM ONNX test error by @nishprabhu in https://github.com/huggingface/transformers/pull/13710\r\n* Enable readme link synchronization by @qqaatw in https://github.com/huggingface/transformers/pull/13785\r\n* Fix length of IterableDatasetShard and add test by @sgugger in https://github.com/huggingface/transformers/pull/13792\r\n* [docs/gpt-j] addd instructions for how minimize CPU RAM usage by @patil-suraj in https://github.com/huggingface/transformers/pull/13795\r\n* [examples `run_glue.py`] missing requirements `scipy`, `sklearn` by @stas00 in https://github.com/huggingface/transformers/pull/13768\r\n* [examples/flax] use Repository API for push_to_hub by @patil-suraj in https://github.com/huggingface/transformers/pull/13672\r\n* Fix gather for TPU by @sgugger in https://github.com/huggingface/transformers/pull/13813\r\n* [testing] auto-replay captured streams by @stas00 in https://github.com/huggingface/transformers/pull/13803\r\n* Add MultiBERTs conversion script by @gchhablani in https://github.com/huggingface/transformers/pull/13077\r\n* [Examples] Improve mapping in accelerate examples by @patrickvonplaten in https://github.com/huggingface/transformers/pull/13810\r\n* [DPR] Correct init by @patrickvonplaten in https://github.com/huggingface/transformers/pull/13796\r\n* skip gptj slow generate tests by @patil-suraj in https://github.com/huggingface/transformers/pull/13809\r\n* Fix warning situation: UserWarning: max_length is ignored when padding=True\" by @shirayu in https://github.com/huggingface/transformers/pull/13829\r\n* Updating CITATION.cff to fix GitHub citation prompt BibTeX output. by @arfon in https://github.com/huggingface/transformers/pull/13833\r\n* Add TF notebooks by @Rocketknight1 in https://github.com/huggingface/transformers/pull/13793\r\n* Bart: check if decoder_inputs_embeds is set by @silviu-oprea in https://github.com/huggingface/transformers/pull/13800\r\n* include megatron_gpt2 in installed modules by @stas00 in https://github.com/huggingface/transformers/pull/13834\r\n* Delete MultiBERTs conversion script by @gchhablani in https://github.com/huggingface/transformers/pull/13852\r\n* Remove a duplicated bullet point in the GPT-J doc by @yaserabdelaziz in https://github.com/huggingface/transformers/pull/13851\r\n* Add Mistral GPT-2 Stability Tweaks by @siddk in https://github.com/huggingface/transformers/pull/13573\r\n* Fix broken link to distill models in docs by @Randl in https://github.com/huggingface/transformers/pull/13848\r\n* :sparkles: update image classification example by @nateraw in https://github.com/huggingface/transformers/pull/13824\r\n* Update no_* argument (HfArgumentParser) by @BramVanroy in https://github.com/huggingface/transformers/pull/13865\r\n* Update Tatoeba conversion by @Traubert in https://github.com/huggingface/transformers/pull/13757\r\n* Fixing 1-length special tokens cut. by @Narsil in https://github.com/huggingface/transformers/pull/13862\r\n* Fix flax summarization example: save checkpoint after each epoch and push checkpoint to the hub by @ydshieh in https://github.com/huggingface/transformers/pull/13872\r\n* Fixing empty prompts for text-generation when BOS exists. by @Narsil in https://github.com/huggingface/transformers/pull/13859\r\n* Improve error message when loading models from Hub by @aphedges in https://github.com/huggingface/transformers/pull/13836\r\n* Initial support for symbolic tracing with torch.fx allowing dynamic axes by @michaelbenayoun in https://github.com/huggingface/transformers/pull/13579\r\n* Allow dataset to be an optional argument for (Distributed)LengthGroupedSampler by @ZhaofengWu in https://github.com/huggingface/transformers/pull/13820\r\n* Fixing question-answering with long contexts  by @Narsil in https://github.com/huggingface/transformers/pull/13873\r\n* fix(integrations): consider test metrics by @borisdayma in https://github.com/huggingface/transformers/pull/13888\r\n* fix: replace asserts by value error by @m5l14i11 in https://github.com/huggingface/transformers/pull/13894\r\n* Update parallelism.md by @hyunwoongko in https://github.com/huggingface/transformers/pull/13892\r\n* Autodocument the list of ONNX-supported models by @sgugger in https://github.com/huggingface/transformers/pull/13884\r\n* Fixing GPU for token-classification in a better way. by @Narsil in https://github.com/huggingface/transformers/pull/13856\r\n* Update FSNER code in examples->research_projects->fsner by @sayef in https://github.com/huggingface/transformers/pull/13864\r\n* Replace assert statements with exceptions by @ddrm86 in https://github.com/huggingface/transformers/pull/13871\r\n* Fixing Backward compatiblity for zero-shot by @Narsil in https://github.com/huggingface/transformers/pull/13855\r\n* Update run_qa.py - CorrectTypo by @akulagrawal in https://github.com/huggingface/transformers/pull/13857\r\n* T5ForConditionalGeneration: enabling using past_key_values and labels in training by @yssjtu in https://github.com/huggingface/transformers/pull/13805\r\n* Fix trainer logging_nan_inf_filter in torch_xla mode by @ymwangg in https://github.com/huggingface/transformers/pull/13896\r\n* Fix hp search for non sigopt backends by @sgugger in https://github.com/huggingface/transformers/pull/13897\r\n* [Trainer] Fix nan-loss condition by @anton-l in https://github.com/huggingface/transformers/pull/13911\r\n* Raise exceptions instead of asserts in utils/download_glue_data by @hirotasoshu in https://github.com/huggingface/transformers/pull/13907\r\n* Add an example of exporting BartModel + BeamSearch to ONNX module. by @fatcat-z in https://github.com/huggingface/transformers/pull/13765\r\n* #12789 Replace assert statements with exceptions by @djroxx2000 in https://github.com/huggingface/transformers/pull/13909\r\n* Add missing whitespace to multiline strings by @aphedges in https://github.com/huggingface/transformers/pull/13916\r\n* [Wav2Vec2] Fix mask_feature_prob by @patrickvonplaten in https://github.com/huggingface/transformers/pull/13921\r\n* Fixes a minor doc issue (missing character) by @mishig25 in https://github.com/huggingface/transformers/pull/13922\r\n* Fix LED by @Rocketknight1 in https://github.com/huggingface/transformers/pull/13882\r\n* Add BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese by @datquocnguyen in https://github.com/huggingface/transformers/pull/13788\r\n* [trainer] memory metrics: add memory at the start report by @stas00 in https://github.com/huggingface/transformers/pull/13915\r\n* Image Segmentation pipeline by @mishig25 in https://github.com/huggingface/transformers/pull/13828\r\n* Adding support for tokens being suffixes or part of each other. by @Narsil in https://github.com/huggingface/transformers/pull/13918\r\n* Adds `PreTrainedModel.framework` attribute by @StellaAthena in https://github.com/huggingface/transformers/pull/13817\r\n* Fixed typo: herBERT -> HerBERT by @adamjankaczmarek in https://github.com/huggingface/transformers/pull/13936\r\n* [Generation] Fix max_new_tokens by @patrickvonplaten in https://github.com/huggingface/transformers/pull/13919\r\n* Fix typo in README.md by @fullyz in https://github.com/huggingface/transformers/pull/13883\r\n* Update bug-report.md by @LysandreJik in https://github.com/huggingface/transformers/pull/13934\r\n* fix issue #13904 -attribute does not exist-  by @oraby8 in https://github.com/huggingface/transformers/pull/13942\r\n* Raise ValueError instead of asserts in src/transformers/benchmark/benchmark.py by @AkechiShiro in https://github.com/huggingface/transformers/pull/13951\r\n* Honor existing attention mask in tokenzier.pad by @sgugger in https://github.com/huggingface/transformers/pull/13926\r\n* [Gradient checkpoining] Correct disabling `find_unused_parameters` in Trainer when gradient checkpointing is enabled by @patrickvonplaten in https://github.com/huggingface/transformers/pull/13961\r\n* Change DataCollatorForSeq2Seq to pad labels to a multiple of `pad_to_multiple_of` by @affjljoo3581 in https://github.com/huggingface/transformers/pull/13949\r\n* Replace assert with unittest assertions by @LuisFerTR in https://github.com/huggingface/transformers/pull/13957\r\n* Raise exceptions instead of asserts in  src/transformers/data/processors/xnli.py by @midhun1998 in https://github.com/huggingface/transformers/pull/13945\r\n* Make username optional in hub_model_id by @sgugger in https://github.com/huggingface/transformers/pull/13940\r\n* Raise exceptions instead of asserts in src/transformers/data/processors/utils.py by @killazz67 in https://github.com/huggingface/transformers/pull/13938\r\n* Replace assert by ValueError of src/transformers/models/electra/modeling_{electra,tf_electra}.py and all other models that had copies by @AkechiShiro in https://github.com/huggingface/transformers/pull/13955\r\n* Fix missing tpu variable in benchmark_args_tf.py by @hardianlawi in https://github.com/huggingface/transformers/pull/13968\r\n* Specify im-seg mask greyscole mode by @mishig25 in https://github.com/huggingface/transformers/pull/13974\r\n* [Wav2Vec2] Make sure tensors are always bool for mask_indices by @patrickvonplaten in https://github.com/huggingface/transformers/pull/13977\r\n* Fixing the lecture values by making sure defaults are not changed by @Narsil in https://github.com/huggingface/transformers/pull/13976\r\n* [parallel doc] dealing with layers larger than one gpu by @stas00 in https://github.com/huggingface/transformers/pull/13980\r\n* Remove wrong model_args supplied by @qqaatw in https://github.com/huggingface/transformers/pull/13937\r\n* Allow single byte decoding by @patrickvonplaten in https://github.com/huggingface/transformers/pull/13988\r\n* Replace assertion with ValueError exception by @ddrm86 in https://github.com/huggingface/transformers/pull/14006\r\n* Add strong test for configuration attributes by @sgugger in https://github.com/huggingface/transformers/pull/14000\r\n* Fix FNet tokenizer tests by @LysandreJik in https://github.com/huggingface/transformers/pull/13995\r\n* [Testing] Move speech datasets to `hf-internal` testing ... by @patrickvonplaten in https://github.com/huggingface/transformers/pull/14008\r\n* Raise exceptions instead of asserts in src/transformers/models/bart/modeling_flax_[bart, marian, mbart, pegasus].py by @killazz67 in https://github.com/huggingface/transformers/pull/13939\r\n* Scatter dummies + skip pipeline tests by @LysandreJik in https://github.com/huggingface/transformers/pull/13996\r\n* Fixed horizon_length for PPLM by @jacksukk in https://github.com/huggingface/transformers/pull/13886\r\n* Fix: replace assert statements with exceptions in file src/transformers/models/lxmert/modeling_lxmert.py by @murilo-goncalves in https://github.com/huggingface/transformers/pull/14029\r\n* [Docs] More general docstrings by @patrickvonplaten in https://github.com/huggingface/transformers/pull/14028\r\n* [CLIP] minor fixes by @patil-suraj in https://github.com/huggingface/transformers/pull/14026\r\n* Don't duplicate the elements in dir by @sgugger in https://github.com/huggingface/transformers/pull/14023\r\n* Replace assertions with ValueError exceptions by @ddrm86 in https://github.com/huggingface/transformers/pull/14018\r\n* Fixes typo in `modeling_speech_to_text` by @mishig25 in https://github.com/huggingface/transformers/pull/14044\r\n* [Speech] Move all examples to new audio feature by @patrickvonplaten in https://github.com/huggingface/transformers/pull/14045\r\n* Update SEW integration test tolerance by @anton-l in https://github.com/huggingface/transformers/pull/14048\r\n* [Flax] Clip fix test by @patrickvonplaten in https://github.com/huggingface/transformers/pull/14046\r\n* Fix save when laod_best_model_at_end=True by @sgugger in https://github.com/huggingface/transformers/pull/14054\r\n* [Speech] Refactor Examples by @patrickvonplaten in https://github.com/huggingface/transformers/pull/14040\r\n* fix typo by @yyy-Apple in https://github.com/huggingface/transformers/pull/14049\r\n* Fix typo by @ihoromi4 in https://github.com/huggingface/transformers/pull/14056\r\n* [FX] Fix passing None as concrete args when tracing by @thomasw21 in https://github.com/huggingface/transformers/pull/14022\r\n* TF Model train and eval step metrics for seq2seq models. by @pedro-r-marques in https://github.com/huggingface/transformers/pull/14009\r\n* update to_py_obj to support np.number by @PrettyMeng in https://github.com/huggingface/transformers/pull/14064\r\n* Trainer._load_rng_state() path fix (#14069) by @tlby in https://github.com/huggingface/transformers/pull/14071\r\n* replace assert with exception in src/transformers/utils/model_pararallel_utils.py by @skpig in https://github.com/huggingface/transformers/pull/14072\r\n* Add missing autocast() in Trainer.prediction_step() by @juice500ml in https://github.com/huggingface/transformers/pull/14075\r\n* Fix assert in src/transformers/data/datasets/language_modeling.py by @skpig in https://github.com/huggingface/transformers/pull/14077\r\n* Fix label attribution in token classification examples by @sgugger in https://github.com/huggingface/transformers/pull/14055\r\n* Context managers by @lvwerra in https://github.com/huggingface/transformers/pull/13900\r\n* Fix broken link in the translation section of task summaries by @h4iku in https://github.com/huggingface/transformers/pull/14087\r\n* [ASR] Small fix model card creation by @patrickvonplaten in https://github.com/huggingface/transformers/pull/14093\r\n* Change asserts in src/transformers/models/xlnet/ to raise ValueError by @WestonKing-Leatham in https://github.com/huggingface/transformers/pull/14088\r\n* Replace assertions with ValueError exceptions by @ddrm86 in https://github.com/huggingface/transformers/pull/14061\r\n* [Typo] Replace \"Masked\" with \"Causal\" in TF CLM script by @cakiki in https://github.com/huggingface/transformers/pull/14014\r\n* [Examples] Add audio classification notebooks by @anton-l in https://github.com/huggingface/transformers/pull/14099\r\n* Fix ignore_mismatched_sizes by @qqaatw in https://github.com/huggingface/transformers/pull/14085\r\n* Fix typo in comment by @stalkermustang in https://github.com/huggingface/transformers/pull/14102\r\n* Replace assertion with ValueError exception by @ddrm86 in https://github.com/huggingface/transformers/pull/14098\r\n* fix typo in license docstring by @21jun in https://github.com/huggingface/transformers/pull/14094\r\n* Fix a typo in preprocessing docs by @h4iku in https://github.com/huggingface/transformers/pull/14108\r\n* Replace assertions with ValueError exceptions by @iDeepverma in https://github.com/huggingface/transformers/pull/14091\r\n* [tests] fix hubert test sort by @patrickvonplaten in https://github.com/huggingface/transformers/pull/14116\r\n* Replace assert statements with exceptions (#13871) by @ddrm86 in https://github.com/huggingface/transformers/pull/13901\r\n* Translate README.md to Korean by @yeounyi in https://github.com/huggingface/transformers/pull/14015\r\n* Replace assertions with valueError Exeptions by @jyshdewangan in https://github.com/huggingface/transformers/pull/14117\r\n* Fix assertion in models by @skpig in https://github.com/huggingface/transformers/pull/14090\r\n* [wav2vec2] Add missing --validation_split_percentage data arg by @falcaopetri in https://github.com/huggingface/transformers/pull/14119\r\n* Rename variables with unclear naming by @qqaatw in https://github.com/huggingface/transformers/pull/14122\r\n* Update TP parallel GEMM image by @hyunwoongko in https://github.com/huggingface/transformers/pull/14112\r\n* Fix some typos in the docs by @h4iku in https://github.com/huggingface/transformers/pull/14126\r\n* Supporting Seq2Seq model for question answering task by @karthikrangasai in https://github.com/huggingface/transformers/pull/13432\r\n* Fix rendering of examples version links by @h4iku in https://github.com/huggingface/transformers/pull/14134\r\n* Fix some writing issues in the docs by @h4iku in https://github.com/huggingface/transformers/pull/14136\r\n* BartEnocder add set_input_embeddings by @Liangtaiwan in https://github.com/huggingface/transformers/pull/13960\r\n* Remove unneeded `to_tensor()` in TF inline example by @Rocketknight1 in https://github.com/huggingface/transformers/pull/14140\r\n* Enable DefaultDataCollator class by @Rocketknight1 in https://github.com/huggingface/transformers/pull/14141\r\n* Fix lazy init to stop hiding errors in import by @sgugger in https://github.com/huggingface/transformers/pull/14124\r\n* Add TF<>PT and Flax<>PT everywhere by @patrickvonplaten in https://github.com/huggingface/transformers/pull/14047\r\n* Add Camembert to models exportable with ONNX by @ChainYo in https://github.com/huggingface/transformers/pull/14059\r\n* [Speech Recognition CTC] Add auth token to fine-tune private models by @patrickvonplaten in https://github.com/huggingface/transformers/pull/14154\r\n* Add vision_encoder_decoder to models/__init__.py by @ydshieh in https://github.com/huggingface/transformers/pull/14151\r\n* [Speech Recognition] - Distributed training: Make sure vocab file removal and creation don't interfer  by @patrickvonplaten in https://github.com/huggingface/transformers/pull/14161\r\n* Include Keras tensor in the allowed types by @sergiovalmac in https://github.com/huggingface/transformers/pull/14155\r\n* [megatron_gpt2] dynamic gelu, add tokenizer, save config by @stas00 in https://github.com/huggingface/transformers/pull/13928\r\n* Add Unispeech & Unispeech-SAT by @patrickvonplaten in https://github.com/huggingface/transformers/pull/13963\r\n* [ONNX] Add symbolic function for XSoftmax op for exporting to ONNX. by @fatcat-z in https://github.com/huggingface/transformers/pull/14013\r\n* Typo on ner accelerate example code by @monologg in https://github.com/huggingface/transformers/pull/14150\r\n* fix typos in error messages in speech recognition example and modelcard.py by @mgoldey in https://github.com/huggingface/transformers/pull/14166\r\n* Replace assertions with ValueError exception by @huberemanuel in https://github.com/huggingface/transformers/pull/14142\r\n* switch to inference_mode from no_gard by @kamalkraj in https://github.com/huggingface/transformers/pull/13667\r\n* Fix gelu test for torch 1.10 by @LysandreJik in https://github.com/huggingface/transformers/pull/14167\r\n* [Gradient checkpointing] Enable for Deberta + DebertaV2 + SEW-D by @patrickvonplaten in https://github.com/huggingface/transformers/pull/14175\r\n* [Pipelines] Fix ASR model types check by @anton-l in https://github.com/huggingface/transformers/pull/14178\r\n* Replace assert of data/data_collator.py by ValueError by @AkechiShiro in https://github.com/huggingface/transformers/pull/14131\r\n* [TPU tests] Enable first TPU examples pytorch by @patrickvonplaten in https://github.com/huggingface/transformers/pull/14121\r\n* [modeling_utils] respect original dtype in _get_resized_lm_head by @stas00 in https://github.com/huggingface/transformers/pull/14181\r\n\r\n## New Contributors\r\n* @arfon made their first contribution in https://github.com/huggingface/transformers/pull/13833\r\n* @silviu-oprea made their first contribution in https://github.com/huggingface/transformers/pull/13800\r\n* @yaserabdelaziz made their first contribution in https://github.com/huggingface/transformers/pull/13851\r\n* @Randl made their first contribution in https://github.com/huggingface/transformers/pull/13848\r\n* @Traubert made their first contribution in https://github.com/huggingface/transformers/pull/13757\r\n* @ZhaofengWu made their first contribution in https://github.com/huggingface/transformers/pull/13820\r\n* @m5l14i11 made their first contribution in https://github.com/huggingface/transformers/pull/13894\r\n* @hyunwoongko made their first contribution in https://github.com/huggingface/transformers/pull/13892\r\n* @ddrm86 made their first contribution in https://github.com/huggingface/transformers/pull/13871\r\n* @akulagrawal made their first contribution in https://github.com/huggingface/transformers/pull/13857\r\n* @yssjtu made their first contribution in https://github.com/huggingface/transformers/pull/13805\r\n* @ymwangg made their first contribution in https://github.com/huggingface/transformers/pull/13896\r\n* @hirotasoshu made their first contribution in https://github.com/huggingface/transformers/pull/13907\r\n* @fatcat-z made their first contribution in https://github.com/huggingface/transformers/pull/13765\r\n* @djroxx2000 made their first contribution in https://github.com/huggingface/transformers/pull/13909\r\n* @adamjankaczmarek made their first contribution in https://github.com/huggingface/transformers/pull/13936\r\n* @oraby8 made their first contribution in https://github.com/huggingface/transformers/pull/13942\r\n* @AkechiShiro made their first contribution in https://github.com/huggingface/transformers/pull/13951\r\n* @affjljoo3581 made their first contribution in https://github.com/huggingface/transformers/pull/13949\r\n* @LuisFerTR made their first contribution in https://github.com/huggingface/transformers/pull/13957\r\n* @midhun1998 made their first contribution in https://github.com/huggingface/transformers/pull/13945\r\n* @killazz67 made their first contribution in https://github.com/huggingface/transformers/pull/13938\r\n* @hardianlawi made their first contribution in https://github.com/huggingface/transformers/pull/13968\r\n* @jacksukk made their first contribution in https://github.com/huggingface/transformers/pull/13886\r\n* @murilo-goncalves made their first contribution in https://github.com/huggingface/transformers/pull/14029\r\n* @yyy-Apple made their first contribution in https://github.com/huggingface/transformers/pull/14049\r\n* @ihoromi4 made their first contribution in https://github.com/huggingface/transformers/pull/14056\r\n* @thomasw21 made their first contribution in https://github.com/huggingface/transformers/pull/14022\r\n* @pedro-r-marques made their first contribution in https://github.com/huggingface/transformers/pull/14009\r\n* @PrettyMeng made their first contribution in https://github.com/huggingface/transformers/pull/14064\r\n* @tlby made their first contribution in https://github.com/huggingface/transformers/pull/14071\r\n* @skpig made their first contribution in https://github.com/huggingface/transformers/pull/14072\r\n* @juice500ml made their first contribution in https://github.com/huggingface/transformers/pull/14075\r\n* @h4iku made their first contribution in https://github.com/huggingface/transformers/pull/14087\r\n* @WestonKing-Leatham made their first contribution in https://github.com/huggingface/transformers/pull/14088\r\n* @cakiki made their first contribution in https://github.com/huggingface/transformers/pull/14014\r\n* @stalkermustang made their first contribution in https://github.com/huggingface/transformers/pull/14102\r\n* @iDeepverma made their first contribution in https://github.com/huggingface/transformers/pull/14091\r\n* @yeounyi made their first contribution in https://github.com/huggingface/transformers/pull/14015\r\n* @jyshdewangan made their first contribution in https://github.com/huggingface/transformers/pull/14117\r\n* @karthikrangasai made their first contribution in https://github.com/huggingface/transformers/pull/13432\r\n* @ChainYo made their first contribution in https://github.com/huggingface/transformers/pull/14059\r\n* @sergiovalmac made their first contribution in https://github.com/huggingface/transformers/pull/14155\r\n* @huberemanuel made their first contribution in https://github.com/huggingface/transformers/pull/14142\r\n\r\n**Full Changelog**: https://github.com/huggingface/transformers/compare/v4.11.0...v4.12.0",
        "dateCreated": "2021-10-28T16:10:14Z",
        "datePublished": "2021-10-28T16:57:28Z",
        "html_url": "https://github.com/huggingface/transformers/releases/tag/v4.12.0",
        "name": "v4.12.0: TrOCR, SEW & SEW-D, Unispeech & Unispeech-SAT, BARTPho",
        "tag_name": "v4.12.0",
        "tarball_url": "https://api.github.com/repos/huggingface/transformers/tarball/v4.12.0",
        "url": "https://api.github.com/repos/huggingface/transformers/releases/52225884",
        "zipball_url": "https://api.github.com/repos/huggingface/transformers/zipball/v4.12.0"
      },
      {
        "authorType": "User",
        "author_name": "LysandreJik",
        "body": "# v4.11.3: Patch release\r\n\r\nThis patch release fixes a few issues encountered since the release of v4.11.2:\r\n\r\n- [DPR] Correct init (#13796)\r\n- Fix warning situation: UserWarning: max_length is ignored when padding=True\" (#13829)\r\n- Bart: check if decoder_inputs_embeds is set (#13800)\r\n- include megatron_gpt2 in installed modules (#13834)\r\n- Fixing 1-length special tokens cut. (#13862)\r\n- Fixing empty prompts for text-generation when BOS exists. (#13859)\r\n- Fixing question-answering with long contexts  (#13873)\r\n- Fixing GPU for token-classification in a better way. (#13856)\r\n- Fixing Backward compatiblity for zero-shot (#13855)\r\n- Fix hp search for non sigopt backends (#13897)\r\n- Fix trainer logging_nan_inf_filter in torch_xla mode #13896 (@ymwangg)\r\n- [Trainer] Fix nan-loss condition #13911 (@anton-l)",
        "dateCreated": "2021-10-06T16:48:36Z",
        "datePublished": "2021-10-06T17:00:51Z",
        "html_url": "https://github.com/huggingface/transformers/releases/tag/v4.11.3",
        "name": "v4.11.3: Patch release",
        "tag_name": "v4.11.3",
        "tarball_url": "https://api.github.com/repos/huggingface/transformers/tarball/v4.11.3",
        "url": "https://api.github.com/repos/huggingface/transformers/releases/50907343",
        "zipball_url": "https://api.github.com/repos/huggingface/transformers/zipball/v4.11.3"
      },
      {
        "authorType": "User",
        "author_name": "sgugger",
        "body": "# v4.11.2: Patch release\r\n\r\nFix the `Trainer` API on TPU:\r\n- Fix gather for TPU #13813",
        "dateCreated": "2021-09-30T15:55:05Z",
        "datePublished": "2021-09-30T15:55:59Z",
        "html_url": "https://github.com/huggingface/transformers/releases/tag/v4.11.2",
        "name": "v4.11.2: Patch release",
        "tag_name": "v4.11.2",
        "tarball_url": "https://api.github.com/repos/huggingface/transformers/tarball/v4.11.2",
        "url": "https://api.github.com/repos/huggingface/transformers/releases/50564262",
        "zipball_url": "https://api.github.com/repos/huggingface/transformers/zipball/v4.11.2"
      },
      {
        "authorType": "User",
        "author_name": "sgugger",
        "body": "# v4.11.1: Patch release\r\n\r\nPatch release with a few bug fixes:\r\n- [Wav2Vec2] Better error message (#13777)\r\n- Fix LayoutLM ONNX test error (#13710)\r\n- Fix warning for gradient_checkpointing (#13767)\r\n- Implement len in IterableDatasetShard (#13780)\r\n- Fix length of IterableDatasetShard and add test (#13792)",
        "dateCreated": "2021-09-29T16:05:08Z",
        "datePublished": "2021-09-29T16:06:53Z",
        "html_url": "https://github.com/huggingface/transformers/releases/tag/v4.11.1",
        "name": "v4.11.1: Patch release",
        "tag_name": "v4.11.1",
        "tarball_url": "https://api.github.com/repos/huggingface/transformers/tarball/v4.11.1",
        "url": "https://api.github.com/repos/huggingface/transformers/releases/50487209",
        "zipball_url": "https://api.github.com/repos/huggingface/transformers/zipball/v4.11.1"
      },
      {
        "authorType": "User",
        "author_name": "LysandreJik",
        "body": "# v4.11.0: GPT-J, Speech2Text2, FNet, Pipeline GPU utilization, dynamic model code loading\r\n \r\n## GPT-J\r\n\r\nThree new models are released as part of the GPT-J implementation: `GPTJModel`, `GPTJForCausalLM`, `GPTJForSequenceClassification`, in PyTorch.\r\n\r\nThe GPT-J model was released in the [kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax) repository by Ben Wang and Aran Komatsuzaki. It is a GPT-2-like causal language model trained on the Pile dataset.\r\n\r\nIt was contributed by @StellaAthena, @kurumuz, @EricHallahan, and @leogao2.\r\n\r\n- GPT-J-6B #13022 (@StellaAthena)\r\n\r\nCompatible checkpoints can be found on the Hub: https://huggingface.co/models?filter=gptj\r\n\r\n## SpeechEncoderDecoder & Speech2Text2\r\n\r\nOne new model is released as part of the Speech2Text2 implementation: `Speech2Text2ForCausalLM`, in PyTorch.\r\n\r\nThe Speech2Text2 model is used together with Wav2Vec2 for Speech Translation models proposed in [Large-Scale Self- and Semi-Supervised Learning for Speech Translation](https://arxiv.org/abs/2104.06678) by Changhan Wang, Anne Wu, Juan Pino, Alexei Baevski, Michael Auli, Alexis Conneau.\r\n\r\nSpeech2Text2 is a decoder-only transformer model that can be used with any speech encoder-only, such as Wav2Vec2 or HuBERT for Speech-to-Text tasks. Please refer to the [SpeechEncoderDecoder](https://huggingface.co/transformers/master/model_doc/speechencoderdecoder.html) class on how to combine Speech2Text2 with any speech encoder-only model.\r\n\r\n- Add SpeechEncoderDecoder & Speech2Text2 #13186 (@patrickvonplaten)\r\n\r\nCompatible checkpoints can be found on the Hub: https://huggingface.co/models?other=speech2text2\r\n\r\n## FNet\r\n\r\nEight new models are released as part of the FNet implementation: `FNetModel`, `FNetForPreTraining`, `FNetForMaskedLM`, `FNetForNextSentencePrediction`, `FNetForSequenceClassification`, `FNetForMultipleChoice`, `FNetForTokenClassification`, `FNetForQuestionAnswering`,  in PyTorch.\r\n\r\nThe FNet model was proposed in [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) by James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon. The model replaces the self-attention layer in a BERT model with a fourier transform which returns only the real parts of the transform. The model is significantly faster than the BERT model because it has fewer parameters and is more memory efficient. The model achieves about 92-97% accuracy of BERT counterparts on GLUE benchmark, and trains much faster than the BERT model.\r\n\r\n- Add FNet #13045 (@gchhablani)\r\n\r\nCompatible checkpoints can be found on the Hub: https://huggingface.co/models?other=fnet\r\n\r\n## TensorFlow improvements\r\n\r\n Several bug fixes and UX improvements for Tensorflow:\r\n\r\n- Users should notice much fewer unnecessary warnings and less 'console spam' in general while using Transformers with TensorFlow.\r\n- TensorFlow models should be less picky about the specific integer dtypes (int32/int64) that are passed as input\r\n\r\nChanges to compile() and train_step()\r\n\r\n- You can now compile our TensorFlow models without passing a loss argument! If you do, the model will compute loss internally during the forward pass and then use this value to fit() on. This makes it much more convenient to get the right loss, particularly since many models have unique losses for certain tasks that are easy to overlook and annoying to reimplement. Remember to pass your labels as the \"labels\" key of your input dict when doing this, so that they're accessible to the model during the forward pass. There is no change to the behavior if you pass a loss argument, so all old code should remain unaffected by this change.\r\n\r\nAssociated PRs:\r\n\r\n- Modified TF train_step #13678 (@Rocketknight1)\r\n- Fix Tensorflow T5 with int64 input #13479 (@Rocketknight1)\r\n- MarianMT int dtype fix #13496 (@Rocketknight1)\r\n- Removed console spam from misfiring warnings #13625 (@Rocketknight1)\r\n\r\n## Pipelines\r\n\r\n### Pipeline refactor\r\n\r\nThe pipelines underwent a large refactor that should make contributing pipelines much simpler, and much less error-prone. As part of this refactor, PyTorch-based pipelines are now optimized for GPU performance based on PyTorch's `Dataset`s and `DataLoader`s. \r\n\r\nSee below for an example leveraging the `superb` dataset.\r\n```py\r\npipe = pipeline(\"automatic-speech-recognition\", model=\"facebook/wav2vec2-base-960h\", device=0)\r\ndataset = datasets.load_dataset(\"superb\", name=\"asr\", split=\"test\")\r\n\r\n# KeyDataset (only `pt`) will simply return the item in the dict returned by the dataset item\r\n# as we're not interested in the `target` part of the dataset.\r\nfor out in tqdm.tqdm(pipe(KeyDataset(dataset, \"file\"))):\r\n    print(out)\r\n    # {\"text\": \"NUMBER TEN FRESH NELLY IS WAITING ON YOU GOOD NIGHT HUSBAND\"}\r\n    # {\"text\": ....}\r\n    # ....\r\n```\r\n\r\n- [Large PR] Entire rework of pipelines. #13308 (@Narsil)\r\n\r\n### Audio classification pipeline\r\n\r\nAdditionally, an additional pipeline is available, for audio classification.\r\n\r\n- Add the `AudioClassificationPipeline` #13342 (@anton-l)\r\n- Enabling automatic loading of tokenizer with `pipeline` for `audio-classification`. #13376 (@Narsil)\r\n\r\n## Setters for common properties\r\n\r\nVersion v4.11.0 introduces setters for common configuration properties. Different configurations have different properties as coming from different implementations.\r\n\r\nOne such example is the `BertConfig` having the `hidden_size` attribute, while the `GPT2Config` has the `n_embed` attribute, which are essentially the same.\r\n\r\nThe newly introduced setters allow setting such properties through a standardized naming scheme, even on configuration objects that do not have them by default.\r\n\r\nSee the following code sample for an example:\r\n\r\n```\r\nfrom transformers import GPT2Config\r\nconfig = GPT2Config()\r\n\r\nconfig.hidden_size = 4  # Failed previously\r\nconfig = GPT2Config(hidden_size =4)  # Failed previously\r\n\r\nconfig.n_embed  # returns 4\r\nconfig.hidden_size  # returns 4\r\n```\r\n\r\n- Update model configs - Allow setters for common properties #13026 (@nreimers)\r\n\r\n## Dynamic model code loading\r\n\r\nAn experimental feature adding support for model files hosted on the hub is added as part of this release. A walkthrough is available in the [PR description](https://github.com/huggingface/transformers/pull/13467). \r\n\r\n:warning: This means that code files will be fetched from the hub to be executed locally. An additional argument, `trust_remote_code` is required when instantiating the model from the hub. We heavily encourage you to also specify a `revision` if using code from another user's or organization's repository.\r\n\r\n- Dynamically load model code from the Hub #13467 (@sgugger)\r\n\r\n## Trainer\r\n\r\nThe `Trainer` has received several new features, the main one being that models are uploaded to the Hub each time you save them locally (you can specify another strategy). This push is asynchronous, so training continues normally without interruption.\r\n\r\nAlso:\r\n- The SigOpt optimization framework is now integrated in the `Trainer` API as an opt-in component.\r\n- The `Trainer` API now supports fine-tuning on distributed CPUs.\r\n\r\nAssociated PRs:\r\n\r\n- Push to hub when saving checkpoints #13503 (@sgugger)\r\n- Add SigOpt HPO to transformers trainer api #13572 (@kding1)\r\n- Add cpu distributed fine-tuning support for transformers Trainer API #13574 (@kding1)\r\n\r\n## Model size CPU memory usage reduction\r\n\r\nThe memory required to load a model in memory using PyTorch's `torch.load` requires twice the amount of memory necessary. An experimental feature allowing model loading while requiring only the model size in terms of memory usage is out in version v4.11.0.\r\n\r\nIt can be used by using the `low_cpu_mem_usage=True` argument with PyTorch pretrained models.\r\n\r\n- 1x model size CPU memory usage for `from_pretrained` #13466 (@stas00)\r\n\r\n\r\n## GPT-Neo: simplified local attention\r\n\r\nThe GPT-Neo local attention was greatly simplified with no loss of performance.\r\n\r\n- [GPT-Neo] Simplify local attention #13491 (@finetuneanon, @patil-suraj)\r\n\r\n\r\n## Breaking changes\r\n\r\n*We strive for no breaking changes between releases - however, some bugs are not discovered for long periods of time, and users may eventually rely on such bugs. We document here such changes that may affect users when updating to a recent version.*\r\n\r\n### Order of overflowing tokens\r\n\r\nThe overflowing tokens returned by the slow tokenizers were returned in the wrong order. This is changed in the PR below.\r\n\r\n- Correct order of overflowing_tokens for slow tokenizer #13179 (@Apoorvgarg-creator)\r\n\r\n### Non-prefixed tokens for token classification pipeline\r\n\r\nUpdates the behavior of `aggregation_strategy` to more closely mimic the deprecated `grouped_entities` pipeline argument.\r\n\r\n- Fixing backward compatiblity for non prefixed tokens (B-, I-). #13493 (@Narsil)\r\n\r\n### Inputs normalization for Wav2Vec2 feature extractor\r\n\r\nThe changes in v4.10 (#12804) introduced a bug in inputs normalization for non-padded tensors that affected Wav2Vec2 fine-tuning.\r\nThis is fixed in the PR below.  \r\n\r\n- [Wav2Vec2] Fix normalization for non-padded tensors #13512 (@patrickvonplaten)\r\n\r\n## General bug fixes and improvements\r\n\r\n- Fixes for the documentation #13361 (@sgugger)\r\n- fix wrong 'cls' masking for bigbird qa model output #13143 (@donggyukimc)\r\n- Improve T5 docs #13240 (@NielsRogge)\r\n- Fix tokenizer saving during training with `Trainer` #12806 (@SaulLu)\r\n- Fix DINO #13369 (@NielsRogge)\r\n- Properly register missing submodules in main init #13372 (@sgugger)\r\n- Add `Hubert` to the `AutoFeatureExtractor` #13366 (@anton-l)\r\n- Add missing feature extractors #13374 (@LysandreJik)\r\n- Fix RemBERT tokenizer initialization #13375 (@LysandreJik)\r\n- [Flax] Fix BigBird #13380 (@patrickvonplaten)\r\n- [GPU Tests] Fix SpeechEncoderDecoder GPU tests #13383 (@patrickvonplaten)\r\n- Fix name and get_class method in AutoFeatureExtractor #13385 (@sgugger)\r\n- [Flax/run_hybrid_clip] Fix duplicating images when captions_per_image exceeds the number of captions, enable truncation #12752 (@edugp)\r\n- Move Flax self-push to test machine #13364 (@patrickvonplaten)\r\n- Torchscript test #13350 (@LysandreJik)\r\n- Torchscript test for DistilBERT #13351 (@LysandreJik)\r\n- Torchscript test for ConvBERT #13352 (@LysandreJik)\r\n- Torchscript test for Flaubert #13353 (@LysandreJik)\r\n- Fix GPT-J _CHECKPOINT_FOR_DOC typo #13368 (@LysandreJik)\r\n- Update clip loss calculation #13217 (@sachinruk)\r\n- Add LayoutXLM tokenizer docs #13373 (@NielsRogge)\r\n- [doc] fix mBART example #13387 (@patil-suraj)\r\n- [docs] Update perplexity.rst to use negative log likelihood #13386 (@madaan)\r\n- [Tests] Fix SpeechEncoderDecoder tests #13395 (@patrickvonplaten)\r\n- [SpeechEncoderDecoder] Fix final test #13396 (@patrickvonplaten)\r\n- \u2728 Add PyTorch image classification example #13134 (@nateraw)\r\n- Fix tests without any real effect in EncoderDecoderMixin #13406 (@ydshieh)\r\n- Fix scheduled tests for `SpeechEncoderDecoderModel` #13422 (@anton-l)\r\n- add torchvision in example test requirements #13438 (@patil-suraj)\r\n- [EncoderDecoder] Fix torch device in tests #13448 (@patrickvonplaten)\r\n- Adding a test for multibytes unicode. #13447 (@Narsil)\r\n- skip image classification example test #13451 (@patil-suraj)\r\n- Add TAPAS MLM-only models #13408 (@NielsRogge)\r\n- Fix scheduled TF Speech tests #13403 (@anton-l)\r\n- Update version of `packaging` package #13454 (@shivdhar)\r\n- Update setup.py #13421 (@anukaal)\r\n- Fix img classification tests #13456 (@nateraw)\r\n- Making it raise real errors on ByT5. #13449 (@Narsil)\r\n- Optimized bad word ids #13433 (@guillaume-be)\r\n- Use powers of 2 in download size calculations #13468 (@anton-l)\r\n- [docs] update dead quickstart link on resuing past for GPT2 #13455 (@shabie)\r\n- fix CLIP conversion script. #13474 (@patil-suraj)\r\n- Deprecate Mirror #13470 (@JetRunner)\r\n- [CLIP] fix logit_scale init #13436 (@patil-suraj)\r\n- Don't modify labels inplace in `LabelSmoother` #13464 (@sgugger)\r\n- Enable automated model list copying for localized READMEs #13465 (@qqaatw)\r\n- Better error raised when cloned without lfs #13401 (@LysandreJik)\r\n- Throw ValueError for mirror downloads #13478 (@JetRunner)\r\n- Fix Tensorflow T5 with int64 input #13479 (@Rocketknight1)\r\n- Object detection pipeline #12886 (@mishig25)\r\n- Typo in \"end_of_word_suffix\" #13477 (@KoichiYasuoka)\r\n- Fixed the MultilabelTrainer document, which would cause a potential bug when executing the code originally documented. #13414 (@Mohan-Zhang-u)\r\n- Fix integration tests for `TFWav2Vec2` and `TFHubert` #13480 (@anton-l)\r\n- Fix typo in deepspeed documentation #13482 (@apohllo)\r\n- flax ner example #13365 (@kamalkraj)\r\n- Fix typo in documentation #13494 (@apohllo)\r\n- MarianMT int dtype fix #13496 (@Rocketknight1)\r\n- [Tentative] Moving slow tokenizer to the Trie world. #13220 (@Narsil)\r\n- Refactor internals for Trainer push_to_hub #13486 (@sgugger)\r\n- examples: minor fixes in flax example readme #13502 (@stefan-it)\r\n- [Wav2Vec2] Fix normalization for non-padded tensors #13512 (@patrickvonplaten)\r\n- TF multiple choice loss fix #13513 (@Rocketknight1)\r\n- [Wav2Vec2] Fix dtype 64 bug #13517 (@patrickvonplaten)\r\n- fix PhophetNet 'use_cache' assignment of no effect #13532 (@holazzer)\r\n- Ignore `past_key_values` during GPT-Neo inference #13521 (@aphedges)\r\n- Fix attention mask size checking for CLIP #13535 (@Renovamen)\r\n- [Speech2Text2] Skip newly added tokenizer test #13536 (@patrickvonplaten)\r\n- [Speech2Text] Give feature extraction higher tolerance #13538 (@patrickvonplaten)\r\n- [tokenizer] use use_auth_token for config #13523 (@stas00)\r\n- Small changes in `perplexity.rst`to make the notebook executable on google collaboratory #13541 (@SaulLu)\r\n- [Feature Extractors] Return attention mask always in int32 #13543 (@patrickvonplaten)\r\n- Nightly torch ci #13550 (@LysandreJik)\r\n- Add long overdue link to the Google TRC project #13501 (@avital)\r\n- Fixing #13381 #13400 (@Narsil)\r\n- fixing BC in `fill-mask` (wasn't tested in theses test suites apparently). #13540 (@Narsil)\r\n- add flax mbart in auto seq2seq lm #13560 (@patil-suraj)\r\n- [Flax] Addition of FlaxPegasus #13420 (@bhadreshpsavani)\r\n- Add checks to build cleaner model cards #13542 (@sgugger)\r\n- separate model card git push from the rest #13514 (@elishowk)\r\n- Fix test_fetcher when setup is updated #13566 (@sgugger)\r\n- [Flax] Fixes typo in Bart based Flax Models #13565 (@bhadreshpsavani)\r\n- Fix GPTNeo onnx export #13524 (@patil-suraj)\r\n- upgrade sentencepiece version #13564 (@elishowk)\r\n- [Pretrained Model] Add resize_position_embeddings #13559 (@patrickvonplaten)\r\n- [ci] nightly: add deepspeed master #13589 (@stas00)\r\n- [Tests] Disable flaky s2t test #13585 (@patrickvonplaten)\r\n- Correct device when resizing position embeddings #13593 (@patrickvonplaten)\r\n- Fix DataCollatorForSeq2Seq when labels are supplied as Numpy array instead of list #13582 (@Rocketknight1)\r\n- Fix a pipeline test with the newly updated weights #13608 (@LysandreJik)\r\n- Fix make fix-copies with type annotations #13586 (@sgugger)\r\n- DataCollatorForTokenClassification numpy fix #13609 (@Rocketknight1)\r\n- Feature Extractor: Wav2Vec2 & Speech2Text - Allow truncation + padding=longest #13600 (@patrickvonplaten)\r\n- [deepspeed] replaced deprecated init arg #13587 (@stas00)\r\n- Properly use test_fetcher for examples #13604 (@sgugger)\r\n- XLMR tokenizer is fully picklable #13577 (@ben-davidson-6)\r\n- Optimize Token Classification models for TPU #13096 (@ibraheem-moosa)\r\n- [Trainer] Add nan/inf logging filter #13619 (@patrickvonplaten)\r\n- Fix special tokens not correctly tokenized #13489 (@qqaatw)\r\n- Removed console spam from misfiring warnings #13625 (@Rocketknight1)\r\n- Use `config_dict_or_path` for deepspeed.zero.Init #13614 (@aphedges)\r\n- Fixes issues with backward pass in LED/Longformer Self-attention #13613 (@aleSuglia)\r\n- fix some docstring in encoder-decoder models #13611 (@ydshieh)\r\n- Updated tiny distilbert models #13631 (@LysandreJik)\r\n- Fix GPT2Config parameters in GPT2ModelTester #13630 (@calpt)\r\n- [run_summarization] fix typo #13647 (@patil-suraj)\r\n- [Fix]Make sure the args tb_writer passed to the TensorBoardCallback works #13636 (@iamlockelightning)\r\n- Fix mT5 documentation #13639 (@ayaka14732)\r\n- Update modeling_tf_deberta.py #13654 (@kamalkraj)\r\n- [megatron_gpt2] checkpoint v3 #13508 (@stas00)\r\n- Change https:/ to https:// to dataset GitHub repo #13644 (@flozi00)\r\n- fix research_projects/mlm_wwm readme.md examples #13646 (@LowinLi)\r\n- Fix typo distilbert doc to code link #13643 (@flozi00)\r\n- Add Speech AutoModels #13655 (@patrickvonplaten)\r\n- beit-flax #13515 (@kamalkraj)\r\n- [FLAX] Question Answering Example #13649 (@kamalkraj)\r\n- Typo \"UNKWOWN\" -> \"UNKNOWN\" #13675 (@kamalkraj)\r\n- [SequenceFeatureExtractor] Rewrite padding logic from pure python to numpy #13650 (@anton-l)\r\n- [SinusoidalPositionalEmbedding] incorrect dtype when resizing in `forward` #13665 (@stas00)\r\n- Add push_to_hub to no_trainer examples #13659 (@sgugger)\r\n- Layoutlm onnx support (Issue #13300) #13562 (@nishprabhu)\r\n- Update modeling_flax_wav2vec2.py #13680 (@kamalkraj)\r\n- [FlaxWav2Vec2] Revive Test #13688 (@patrickvonplaten)\r\n- [AutoTokenizer] Allow creation of tokenizers by tokenizer type #13668 (@patrickvonplaten)\r\n- [Wav2Vec2FeatureExtractor] Fix `extractor.pad()` dtype backwards compatibility #13693 (@anton-l)\r\n- Make gradient_checkpointing a training argument #13657 (@sgugger)\r\n- Assertions to exceptions #13692 (@MocktaiLEngineer)\r\n- Fix non-negligible difference between GPT2 and TFGP2 #13679 (@ydshieh)\r\n- Allow only textual inputs to VisualBert #13687 (@gchhablani)\r\n- Patch training arguments issue #13699 (@LysandreJik)\r\n- Patch training arguments issue #13700 (@LysandreJik)\r\n- [GPT-J] Use the `float16` checkpoints in integration tests #13676 (@anton-l)\r\n- [docs/gpt-j] add a note about tokenizer #13696 (@patil-suraj)\r\n- Fix FNet reference to tpu short seq length #13686 (@gchhablani)\r\n- Add BlenderBot small tokenizer to the init #13367 (@LysandreJik)\r\n- Fix typo in torchscript tests #13701 (@LysandreJik)\r\n- Handle `UnicodeDecodeError` when loading config file #13717 (@qqaatw)\r\n- Add FSNER example in research_projects #13712 (@sayef)\r\n- Replace torch.set_grad_enabled by torch.no_grad #13703 (@LysandreJik)\r\n- [ASR] Add official ASR CTC example to `examples/pytorch/speech-recognition` #13620 (@patrickvonplaten)\r\n- Make assertions only if actually chunking forward #13598 (@joshdevins)\r\n- Use torch.unique_consecutive to check elements are same #13637 (@oToToT)\r\n- Fixing zero-shot backward compatiblity #13725 (@Narsil)\r\n- [Tests] FNetTokenizer #13729 (@patrickvonplaten)\r\n- Warn for unexpected argument combinations #13509 (@shirayu)\r\n- Add model card creation snippet to example scripts #13730 (@gchhablani)\r\n- [Examples] speech recognition - remove gradient checkpointing #13733 (@patrickvonplaten)\r\n- Update test dependence for torch examples #13738 (@sgugger)\r\n- [Tests] Add decorator to FlaxBeit #13743 (@patrickvonplaten)\r\n- Update requirements for speech example #13745 (@sgugger)\r\n- [Trainer] Make sure shown loss in distributed training is correctly averaged over all workers #13681 (@patrickvonplaten)\r\n- [megatron gpt checkpoint conversion] causal mask requires pos_embed dimension #13735 (@stas00)\r\n- [Tests] Cast Hubert model tests to fp16 #13755 (@anton-l)\r\n- Fix type annotations for `distributed_concat()` #13746 (@Renovamen)\r\n- Fix loss computation in Trainer #13760 (@sgugger)\r\n- Silence warning in gradient checkpointing when it's False #13734 (@sgugger)",
        "dateCreated": "2021-09-27T18:16:51Z",
        "datePublished": "2021-09-27T18:20:26Z",
        "html_url": "https://github.com/huggingface/transformers/releases/tag/v4.11.0",
        "name": "v4.11.0: GPT-J, Speech2Text2, FNet, Pipeline GPU utilization, dynamic model code loading",
        "tag_name": "v4.11.0",
        "tarball_url": "https://api.github.com/repos/huggingface/transformers/tarball/v4.11.0",
        "url": "https://api.github.com/repos/huggingface/transformers/releases/50324030",
        "zipball_url": "https://api.github.com/repos/huggingface/transformers/zipball/v4.11.0"
      },
      {
        "authorType": "User",
        "author_name": "LysandreJik",
        "body": "Patches an issue with the serialization of the `TrainingArguments`",
        "dateCreated": "2021-09-22T19:56:41Z",
        "datePublished": "2021-09-22T20:00:36Z",
        "html_url": "https://github.com/huggingface/transformers/releases/tag/v4.10.3",
        "name": "v4.10.3: Patch release",
        "tag_name": "v4.10.3",
        "tarball_url": "https://api.github.com/repos/huggingface/transformers/tarball/v4.10.3",
        "url": "https://api.github.com/repos/huggingface/transformers/releases/50086193",
        "zipball_url": "https://api.github.com/repos/huggingface/transformers/zipball/v4.10.3"
      },
      {
        "authorType": "User",
        "author_name": "patrickvonplaten",
        "body": "- [Wav2Vec2] Fix dtype 64 bug #13517 (@patrickvonplaten)",
        "dateCreated": "2021-09-10T16:21:33Z",
        "datePublished": "2021-09-10T16:37:01Z",
        "html_url": "https://github.com/huggingface/transformers/releases/tag/v4.10.2",
        "name": "v4.10.2: Patch release",
        "tag_name": "v4.10.2",
        "tarball_url": "https://api.github.com/repos/huggingface/transformers/tarball/v4.10.2",
        "url": "https://api.github.com/repos/huggingface/transformers/releases/49360500",
        "zipball_url": "https://api.github.com/repos/huggingface/transformers/zipball/v4.10.2"
      },
      {
        "authorType": "User",
        "author_name": "LysandreJik",
        "body": "- [Wav2Vec2] Fix normalization for non-padded tensors #13512 (@patrickvonplaten)\r\n- Fixing backward compatiblity for non prefixed tokens (B-, I-). #13493 (@Narsil)\r\n- Fixing #13381 #13400 (@Narsil)",
        "dateCreated": "2021-09-10T14:20:42Z",
        "datePublished": "2021-09-10T14:40:45Z",
        "html_url": "https://github.com/huggingface/transformers/releases/tag/v4.10.1",
        "name": "v4.10.1: Patch release",
        "tag_name": "v4.10.1",
        "tarball_url": "https://api.github.com/repos/huggingface/transformers/tarball/v4.10.1",
        "url": "https://api.github.com/repos/huggingface/transformers/releases/49352181",
        "zipball_url": "https://api.github.com/repos/huggingface/transformers/zipball/v4.10.1"
      },
      {
        "authorType": "User",
        "author_name": "LysandreJik",
        "body": "# v4.10.0: LayoutLM-v2, LayoutXLM, BEiT\r\n\r\n## LayoutLM-v2 and LayoutXLM\r\n\r\nFour new models are released as part of the LatourLM-v2 implementation: `LayoutLMv2ForSequenceClassification`, `LayoutLMv2Model`, `LayoutLMv2ForTokenClassification` and `LayoutLMv2ForQuestionAnswering`, in PyTorch.\r\n\r\nThe LayoutLMV2 model was proposed in [LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding](https://arxiv.org/abs/2012.14740) by Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, Min Zhang, Lidong Zhou. LayoutLMV2 improves [LayoutLM](https://huggingface.co/transformers/model_doc/layoutlm.html) to obtain state-of-the-art results across several document image understanding benchmarks:\r\n\r\n- Add LayoutLMv2 + LayoutXLM #12604 (@NielsRogge)\r\n\r\nCompatible checkpoints can be found on the Hub: https://huggingface.co/models?filter=layoutlmv2\r\n\r\n## BEiT\r\n\r\nThree new models are released as part of the BEiT implementation: `BeitModel`, `BeitForMaskedImageModeling`, and `BeitForImageClassification`, in PyTorch.\r\n\r\nThe BEiT model was proposed in [BEiT: BERT Pre-Training of Image Transformers](https://arxiv.org/abs/2106.08254) by Hangbo Bao, Li Dong and Furu Wei. Inspired by BERT, BEiT is the first paper that makes self-supervised pre-training of Vision Transformers (ViTs) outperform supervised pre-training. Rather than pre-training the model to predict the class of an image (as done in the [original ViT paper](https://arxiv.org/abs/2010.11929)), BEiT models are pre-trained to predict visual tokens from the codebook of OpenAI\u2019s [DALL-E model](https://arxiv.org/abs/2102.12092) given masked patches.\r\n\r\n- Add BEiT #12994 (@NielsRogge)\r\n\r\nCompatible checkpoints can be found on the Hub: https://huggingface.co/models?filter=beit\r\n\r\n## Speech improvements\r\n\r\nThe Wav2Vec2 and HuBERT models now have a sequence classification head available.\r\n\r\n- Add Wav2Vec2 & Hubert ForSequenceClassification #13153 (@anton-l)\r\n\r\n## DeBERTa in TensorFlow (@kamalkraj)\r\n\r\nThe DeBERTa and DeBERTa-v2 models have been converted from PyTorch to TensorFlow.\r\n\r\n- Deberta tf #12972 (@kamalkraj)\r\n- Deberta_v2 tf #13120 (@kamalkraj)\r\n\r\n## Flax model additions\r\n\r\nEncoderDecoder, DistilBERT, and ALBERT, now have support in Flax!\r\n\r\n- FlaxEncoderDecoder allowing Bert2Bert and Bert2GPT2 in Flax #13008 (@ydshieh)\r\n- FlaxDistilBERT #13324 (@kamalkraj)\r\n- FlaxAlBERT #13294 (@kamalkraj)\r\n\r\n## TensorFlow examples\r\n\r\nA new example has been added in TensorFlow: multiple choice!\r\nData collators have become framework agnostic and can now work for both TensorFlow and NumPy on top of PyTorch.\r\n\r\n- Add TF multiple choice example #12865 (@Rocketknight1)\r\n- TF/Numpy variants for all DataCollator classes #13105 (@Rocketknight1)\r\n\r\n\r\n## Auto API refactor\r\n\r\nThe Auto APIs have been disentangled from all the other mode modules of the Transformers library, so you can now safely import the Auto classes without importing all the models (and maybe getting errors if your setup is not compatible with one specific model). The actual model classes are only imported when needed.\r\n\r\n- Disentangle auto modules from other modeling files #13023 (@sgugger)\r\n- Fix AutoTokenizer when no fast tokenizer is available #13336 (@sgugger)\r\n\r\n## Slight breaking change\r\n\r\nWhen loading some kinds of corrupted state dictionaries of models, the `PreTrainedModel.from_pretrained` method was sometimes silently ignoring weights. This has now become a real error.\r\n\r\n- Fix from_pretrained with corrupted state_dict #12939 (@sgugger)\r\n\r\n## General improvements and bugfixes\r\n\r\n- Improving pipeline tests #12784 (@Narsil)\r\n- Pin git python to <3.1.19 #12858 (@patrickvonplaten)\r\n- [tests] fix logging_steps requirements #12860 (@stas00)\r\n- [Sequence Feature Extraction] Add truncation #12804 (@patrickvonplaten)\r\n- add `classifier_dropout` to classification heads #12794 (@PhilipMay)\r\n- Fix barrier for SM distributed #12853 (@sgugger)\r\n- Add possibility to ignore imports in test_fecther #12801 (@sgugger)\r\n- Add accelerate to examples requirements #12888 (@sgugger)\r\n- Fix documentation of BigBird tokenizer #12889 (@sgugger)\r\n- Better heuristic for token-classification pipeline. #12611 (@Narsil)\r\n- Fix push_to_hub for TPUs #12895 (@sgugger)\r\n- `Seq2SeqTrainer` set max_length and num_beams only when non None #12899 (@cchen-dialpad)\r\n- [FLAX] Minor fixes in CLM example #12914 (@stefan-it)\r\n- Correct validation_split_percentage argument from int (ex:5) to float (0.05) #12897 (@Elysium1436)\r\n- Fix typo in the example of MobileBertForPreTraining #12919 (@buddhics)\r\n- Add option to set max_len in run_ner #12929 (@sgugger)\r\n- Fix QA examples for roberta tokenizer #12928 (@sgugger)\r\n- Print defaults when using --help for scripts #12930 (@sgugger)\r\n- Fix StoppingCriteria ABC signature #12918 (@willfrey)\r\n- Add missing @classmethod decorators #12927 (@willfrey)\r\n- fix distiller.py #12910 (@chutaklee)\r\n- Update generation_logits_process.py #12901 (@willfrey)\r\n- Update generation_logits_process.py #12900 (@willfrey)\r\n- Update tokenization_auto.py #12896 (@willfrey)\r\n- Fix docstring typo in tokenization_auto.py #12891 (@willfrey)\r\n- [Flax] Correctly Add MT5 #12988 (@patrickvonplaten)\r\n- ONNX v2 raises an Exception when using PyTorch < 1.8.0 #12933 (@mfuntowicz)\r\n- Moving feature-extraction pipeline to new testing scheme #12843 (@Narsil)\r\n- Add CpmTokenizerFast #12938 (@JetRunner)\r\n- fix typo in gradient_checkpointing arg #12855 (@21jun)\r\n- Log Azure ML metrics only for rank 0 #12766 (@harshithapv)\r\n- Add substep end callback method #12951 (@wulu473)\r\n- Add multilingual documentation support #12952 (@JetRunner)\r\n- Fix division by zero in NotebookProgressPar #12953 (@sgugger)\r\n- [FLAX] Minor fixes in LM example #12947 (@stefan-it)\r\n- Prevent `Trainer.evaluate()` crash when using only tensorboardX #12963 (@aphedges)\r\n- Fix typo in example of DPRReader #12954 (@tadejsv)\r\n- Place BigBirdTokenizer in sentencepiece-only objects #12975 (@sgugger)\r\n- fix typo in example/text-classification README #12974 (@fullyz)\r\n- Fix template for inputs docstrings #12976 (@sgugger)\r\n- fix `Trainer.train(resume_from_checkpoint=False)` is causing an exception #12981 (@PhilipMay)\r\n- Cast logits from bf16 to fp32 at the end of TF_T5 #12332 (@szutenberg)\r\n- Update CANINE test #12453 (@NielsRogge)\r\n- pad_to_multiple_of added to DataCollatorForWholeWordMask #12999 (@Aktsvigun)\r\n- [Flax] Align jax flax device name #12987 (@patrickvonplaten)\r\n- [Flax] Correct flax docs #12782 (@patrickvonplaten)\r\n- T5: Create position related tensors directly on device instead of CPU #12846 (@armancohan)\r\n- Skip ProphetNet test #12462 (@LysandreJik)\r\n- Create perplexity.rst #13004 (@sashavor)\r\n- GPT-Neo ONNX export #12911 (@michaelbenayoun)\r\n- Update generate method - Fix floor_divide warning #13013 (@nreimers)\r\n- [Flax] Correct pt to flax conversion if from base to head #13006 (@patrickvonplaten)\r\n- [Flax T5] Speed up t5 training #13012 (@patrickvonplaten)\r\n- FX submodule naming fix #13016 (@michaelbenayoun)\r\n- T5 with past ONNX export #13014 (@michaelbenayoun)\r\n- Fix ONNX test: Put smaller ALBERT model #13028 (@LysandreJik)\r\n- Tpu tie weights #13030 (@sgugger)\r\n- Use min version for huggingface-hub dependency #12961 (@lewtun)\r\n- tfhub.de -> tfhub.dev #12565 (@abhishekkrthakur)\r\n- [Flax] Refactor gpt2 & bert example docs #13024 (@patrickvonplaten)\r\n- Add MBART to models exportable with ONNX #13049 (@LysandreJik)\r\n- Add to ONNX docs #13048 (@LysandreJik)\r\n- Fix small typo in M2M100 doc #13061 (@SaulLu)\r\n- Add try-except for torch_scatter #13040 (@JetRunner)\r\n- docs: add HuggingArtists to community notebooks #13050 (@AlekseyKorshuk)\r\n- Fix ModelOutput instantiation form dictionaries #13067 (@sgugger)\r\n- Roll out the test fetcher on push tests #13055 (@sgugger)\r\n- Fix fallback of test_fetcher #13071 (@sgugger)\r\n- Revert to all tests whil we debug what's wrong #13072 (@sgugger)\r\n- Use original key for label in DataCollatorForTokenClassification #13057 (@ibraheem-moosa)\r\n- [Doctest] Setup, quicktour and task_summary #13078 (@sgugger)\r\n- Add VisualBERT demo notebook #12263 (@gchhablani)\r\n- Install git #13091 (@LysandreJik)\r\n- Fix classifier dropout in AlbertForMultipleChoice #13087 (@ibraheem-moosa)\r\n- Doctests job #13088 (@LysandreJik)\r\n- Fix VisualBert Embeddings #13017 (@gchhablani)\r\n- Proper import for unittest.mock.patch #13085 (@sgugger)\r\n- Reactive test fecthers on scheduled test with proper git install #13097 (@sgugger)\r\n- Change a parameter name in FlaxBartForConditionalGeneration.decode() #13074 (@ydshieh)\r\n- [Flax/JAX] Run jitted tests at every commit #13090 (@patrickvonplaten)\r\n- Rely on huggingface_hub for common tools #13100 (@sgugger)\r\n- [FlaxCLIP] allow passing params to image and text feature methods #13099 (@patil-suraj)\r\n- Ci last fix #13103 (@sgugger)\r\n- Improve type checker performance #13094 (@bschnurr)\r\n- Fix VisualBERT docs #13106 (@gchhablani)\r\n- Fix CircleCI nightly tests #13113 (@sgugger)\r\n- Create py.typed #12893 (@willfrey)\r\n- Fix flax gpt2 hidden states #13109 (@ydshieh)\r\n- Moving fill-mask pipeline to new testing scheme #12943 (@Narsil)\r\n- Fix omitted lazy import for xlm-prophetnet #13052 (@minwhoo)\r\n- Fix classifier dropout in bertForMultipleChoice #13129 (@mandelbrot-walker)\r\n- Fix frameworks table so it's alphabetical #13118 (@osanseviero)\r\n- [Feature Processing Sequence] Remove duplicated code #13051 (@patrickvonplaten)\r\n- Ci continue through smi failure #13140 (@LysandreJik)\r\n- Fix missing `seq_len` in `electra` model when `inputs_embeds` is used. #13128 (@sararb)\r\n- Optimizes ByT5 tokenizer #13119 (@Narsil)\r\n- Add splinter #12955 (@oriram)\r\n- [AutoFeatureExtractor] Fix loading of local folders if config.json exists #13166 (@patrickvonplaten)\r\n- Fix generation docstrings regarding input_ids=None #12823 (@jvamvas)\r\n- Update namespaces inside torch.utils.data to the latest. #13167 (@qqaatw)\r\n- Fix the loss calculation of ProphetNet #13132 (@StevenTang1998)\r\n- Fix LUKE tests #13183 (@NielsRogge)\r\n- Add min and max question length options to TapasTokenizer #12803 (@NielsRogge)\r\n- SageMaker: Fix sagemaker DDP & metric logs #13181 (@philschmid)\r\n- correcting group beam search function output score bug #13211 (@sourabh112)\r\n- Change how \"additional_special_tokens\" argument in the \".from_pretrained\" method of the tokenizer is taken into account #13056 (@SaulLu)\r\n- remove unwanted control-flow code from DeBERTa-V2 #13145 (@kamalkraj)\r\n- Fix load_tf_weights alias. #13159 (@qqaatw)\r\n- Add RemBert to AutoTokenizer #13224 (@LysandreJik)\r\n- Allow local_files_only for fast pretrained tokenizers #13225 (@BramVanroy)\r\n- fix `AutoModel.from_pretrained(..., torch_dtype=...)` #13209 (@stas00)\r\n- Fix broken links in Splinter documentation #13237 (@oriram)\r\n- Custom errors and BatchSizeError #13184 (@AmbiTyga)\r\n- Bump notebook from 6.1.5 to 6.4.1 in /examples/research_projects/lxmert #13226 (@dependabot[bot])\r\n- Update generation_logits_process.py #12671 (@willfrey)\r\n- Remove side effects of disabling gradient computaiton #13257 (@LysandreJik)\r\n- Replace assert statement with if condition and raise ValueError #13263 (@nishprabhu)\r\n- Better notification service #13267 (@LysandreJik)\r\n- Fix failing Hubert test #13261 (@LysandreJik)\r\n- Add CLIP tokenizer to AutoTokenizer #13258 (@LysandreJik)\r\n- Some `model_type`s cannot be in the mapping #13259 (@LysandreJik)\r\n- Add require flax to MT5 Flax test #13260 (@LysandreJik)\r\n- Migrating conversational pipeline tests to new testing format #13114 (@Narsil)\r\n- fix `tokenizer_class_from_name` for models with `-` in the name #13251 (@stas00)\r\n- Add error message concerning revision #13266 (@BramVanroy)\r\n- Move `image-classification` pipeline to new testing #13272 (@Narsil)\r\n- [Hotfix] Fixing the test (warnings was incorrect.) #13278 (@Narsil)\r\n- Moving question_answering tests to the new testing scheme. Had to tweak a little some ModelTesterConfig for pipelines. #13277 (@Narsil)\r\n- Moving `summarization` pipeline to new testing format. #13279 (@Narsil)\r\n- Moving `table-question-answering` pipeline to new testing. #13280 (@Narsil)\r\n- Moving `table-question-answering` pipeline to new testing #13281 (@Narsil)\r\n- Hotfixing master tests. #13282 (@Narsil)\r\n- Moving `text2text-generation` to new pipeline testing mecanism #13283 (@Narsil)\r\n- Add DINO conversion script #13265 (@NielsRogge)\r\n- Moving `text-generation` pipeline to new testing framework. #13285 (@Narsil)\r\n- Moving `token-classification` pipeline to new testing. #13286 (@Narsil)\r\n- examples: add keep_linebreaks option to CLM examples #13150 (@stefan-it)\r\n- Moving `translation` pipeline to new testing scheme. #13297 (@Narsil)\r\n- Fix BeitForMaskedImageModeling #13275 (@NielsRogge)\r\n- Moving `zero-shot-classification` pipeline to new testing. #13299 (@Narsil)\r\n- Fixing mbart50 with `return_tensors` argument too. #13301 (@Narsil)\r\n- [Flax] Correct all return tensors to numpy #13307 (@patrickvonplaten)\r\n\r\n- examples: only use keep_linebreaks when reading TXT files #13320 (@stefan-it)\r\n- Slow tests - run rag token in half precision #13304 (@patrickvonplaten)\r\n- [Slow tests] Disable Wav2Vec2 pretraining test for now #13303 (@patrickvonplaten)\r\n- Announcing the default model used by the pipeline (with a link). #13276 (@Narsil)\r\n- use float 16 in causal mask and masked bias #13194 (@hwijeen)\r\n- \u2728 add citation file #13214 (@flaxel)\r\n- Improve documentation of pooler_output in ModelOutput #13228 (@navjotts)\r\n- fix: typo spelling grammar #13212 (@slowy07)\r\n- Check None before going through iteration #13250 (@qqaatw)\r\n- Use existing functionality for #13251 #13333 (@sgugger)\r\n- neptune.ai logger: add ability to connect to a neptune.ai run #13319 (@fcakyon)\r\n- Update label2id in the model config for run_glue #13334 (@sgugger)\r\n- :bug: fix small model card bugs #13310 (@nateraw)\r\n- Fall back to `observed_batch_size` when the `dataloader` does not know the `batch_size`. #13188 (@mbforbes)\r\n- Fixes #12941 where use_auth_token not been set up early enough #13205 (@bennimmo)\r\n- Correct wrong function signatures on the docs website #13198 (@qqaatw)\r\n- Fix release utils #13337 (@sgugger)\r\n- Add missing module __spec__ #13321 (@laurahanu)\r\n- Use DS callable API to allow hf_scheduler + ds_optimizer #13216 (@tjruwase)\r\n- Tests fetcher tests #13340 (@sgugger)\r\n- [Testing] Add Flax Tests on GPU, Add Speech and Vision to Flax & TF tests #13313 (@patrickvonplaten)\r\n- Fixing a typo in the data_collator documentation #13309 (@Serhiy-Shekhovtsov)\r\n- Add GPT2ForTokenClassification #13290 (@tucan9389)\r\n- Doc mismatch fixed #13345 (@Apoorvgarg-creator)\r\n- Handle nested dict/lists of tensors as inputs in the Trainer #13338 (@sgugger)\r\n- [doc] correct TP implementation resources #13248 (@stas00)\r\n- Fix minor typo in parallelism doc #13289 (@jaketae)\r\n- Set missing seq_length variable when using inputs_embeds with ALBERT & Remove code duplication #13152 (@olenmg)\r\n- TF CLM example fix typo #13002 (@Rocketknight1)\r\n- Add generate kwargs to Seq2SeqTrainingArguments #13339 (@sgugger)",
        "dateCreated": "2021-08-31T13:53:10Z",
        "datePublished": "2021-08-31T14:02:59Z",
        "html_url": "https://github.com/huggingface/transformers/releases/tag/v4.10.0",
        "name": "v4.10.0: LayoutLM-v2, LayoutXLM, BEiT",
        "tag_name": "v4.10.0",
        "tarball_url": "https://api.github.com/repos/huggingface/transformers/tarball/v4.10.0",
        "url": "https://api.github.com/repos/huggingface/transformers/releases/48743276",
        "zipball_url": "https://api.github.com/repos/huggingface/transformers/zipball/v4.10.0"
      },
      {
        "authorType": "User",
        "author_name": "LysandreJik",
        "body": "# v4.9.2: Patch release\r\n\r\n- Tpu tie weights #13030 (@sgugger)\r\n- ONNX fixes & examples: #13048, #13049, #13028, #13014, #12911,  (@mfuntowicz, @michaelbenayoun, @LysandreJik)\r\n- Fix push_to_hub for TPUs #12895 (@sgugger)",
        "dateCreated": "2021-08-09T14:01:47Z",
        "datePublished": "2021-08-09T14:27:29Z",
        "html_url": "https://github.com/huggingface/transformers/releases/tag/v4.9.2",
        "name": "v4.9.2: Patch release",
        "tag_name": "v4.9.2",
        "tarball_url": "https://api.github.com/repos/huggingface/transformers/tarball/v4.9.2",
        "url": "https://api.github.com/repos/huggingface/transformers/releases/47542177",
        "zipball_url": "https://api.github.com/repos/huggingface/transformers/zipball/v4.9.2"
      },
      {
        "authorType": "User",
        "author_name": "sgugger",
        "body": "# v4.9.1: Patch release\r\n\r\nFix barrier for SM distributed #12853 (@sgugger)",
        "dateCreated": "2021-07-26T14:22:17Z",
        "datePublished": "2021-07-26T14:24:25Z",
        "html_url": "https://github.com/huggingface/transformers/releases/tag/v4.9.1",
        "name": "v4.9.1: Patch release",
        "tag_name": "v4.9.1",
        "tarball_url": "https://api.github.com/repos/huggingface/transformers/tarball/v4.9.1",
        "url": "https://api.github.com/repos/huggingface/transformers/releases/46773080",
        "zipball_url": "https://api.github.com/repos/huggingface/transformers/zipball/v4.9.1"
      },
      {
        "authorType": "User",
        "author_name": "LysandreJik",
        "body": " # v4.9.0: TensorFlow examples, CANINE, tokenizer training, ONNX rework\r\n\r\n## ONNX rework\r\n\r\nThis version introduces a new package, `transformers.onnx`, which can be used to export models to ONNX. Contrary to the previous implementation, this approach is meant as an easily extendable package where users may define their own ONNX configurations and export the models they wish to export. \r\n\r\n```bash\r\npython -m transformers.onnx --model=bert-base-cased onnx/bert-base-cased/\r\n```\r\n```\r\nValidating ONNX model...\r\n        -[\u2713] ONNX model outputs' name match reference model ({'pooler_output', 'last_hidden_state'}\r\n        - Validating ONNX Model output \"last_hidden_state\":\r\n                -[\u2713] (2, 8, 768) matchs (2, 8, 768)\r\n                -[\u2713] all values close (atol: 0.0001)\r\n        - Validating ONNX Model output \"pooler_output\":\r\n                -[\u2713] (2, 768) matchs (2, 768)\r\n                -[\u2713] all values close (atol: 0.0001)\r\nAll good, model saved at: onnx/bert-base-cased/model.onnx\r\n```\r\n\r\n- [RFC] Laying down building stone for more flexible ONNX export capabilities #11786 (@mfuntowicz)\r\n\r\n## CANINE model\r\n\r\nFour new models are released as part of the CANINE implementation: `CanineForSequenceClassification`, `CanineForMultipleChoice`, `CanineForTokenClassification` and `CanineForQuestionAnswering`, in PyTorch.\r\n\r\nThe CANINE model was proposed in [CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation](https://arxiv.org/abs/2103.06874) by Jonathan H. Clark, Dan Garrette, Iulia Turc, John Wieting. It\u2019s among the first papers that train a Transformer without using an explicit tokenization step (such as Byte Pair Encoding (BPE), WordPiece, or SentencePiece). Instead, the model is trained directly at a Unicode character level. Training at a character level inevitably comes with a longer sequence length, which CANINE solves with an efficient downsampling strategy, before applying a deep Transformer encoder.\r\n\r\n- Add CANINE #12024 (@NielsRogge)\r\n\r\nCompatible checkpoints can be found on the Hub: https://huggingface.co/models?filter=canine\r\n\r\n## Tokenizer training\r\n\r\nThis version introduces a new method to train a tokenizer from scratch based off of an existing tokenizer configuration.\r\n\r\n```py\r\nfrom datasets import load_dataset\r\nfrom transformers import AutoTokenizer\r\n\r\ndataset = load_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\", split=\"train\")\r\n# We train on batch of texts, 1000 at a time here.\r\nbatch_size = 1000\r\ncorpus = (dataset[i : i + batch_size][\"text\"] for i in range(0, len(dataset), batch_size))\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\r\nnew_tokenizer = tokenizer.train_new_from_iterator(corpus, vocab_size=20000)\r\n```\r\n\r\n- Easily train a new fast tokenizer from a given one - tackle the special tokens format (str or AddedToken) #12420 (@SaulLu)\r\n- Easily train a new fast tokenizer from a given one #12361 (@sgugger)\r\n\r\n## TensorFlow examples\r\n\r\nThe `TFTrainer` is now entering deprecation - and it is replaced by `Keras`. With version v4.9.0 comes the end of a long rework of the TensorFlow examples, for them to be more Keras-idiomatic, clearer, and more robust.\r\n\r\n- NER example for Tensorflow #12469 (@Rocketknight1)\r\n- TF summarization example #12617 (@Rocketknight1)\r\n- Adding TF translation example #12667 (@Rocketknight1)\r\n- Deprecate TFTrainer #12706 (@Rocketknight1)\r\n\r\n## TensorFlow implementations\r\n\r\nHuBERT is now implemented in TensorFlow:\r\n\r\n- Add TFHubertModel #12206 (@will-rice)\r\n\r\n## Breaking changes\r\n\r\nWhen `load_best_model_at_end` was set to `True` in the `TrainingArguments`, having a different `save_strategy` and `eval_strategy` was accepted but the `save_strategy` was overwritten by the `eval_strategy` (the option to keep track of the best model needs to make sure there is an evaluation each time there is a save). This led to a lot of confusion with users not understanding why the script was not doing what it was told, so this situation will now raise an error indicating to set `save_strategy` and `eval_strategy` to the same values, and in the case that value is `\"steps\"`, `save_steps` must be a round multiple of `eval_steps`.\r\n\r\n## General improvements and bugfixes\r\n\r\n- UpdateDescription of TrainingArgs param save_strategy #12328 (@sam-qordoba)\r\n- [Deepspeed] new docs #12077 (@stas00)\r\n- [ray] try fixing import error #12338 (@richardliaw)\r\n- [examples/Flax] move the examples table up #12341 (@patil-suraj)\r\n- Fix torchscript tests #12336 (@LysandreJik)\r\n- Add flax/jax quickstart #12342 (@marcvanzee)\r\n- Fixed a typo in readme #12356 (@MichalPitr)\r\n- Fix exception in prediction loop occurring for certain batch sizes #12350 (@jglaser)\r\n- Add FlaxBigBird QuestionAnswering script #12233 (@vasudevgupta7)\r\n- Replace NotebookProgressReporter by ProgressReporter in Ray Tune run #12357 (@krfricke)\r\n- [examples] remove extra white space from log format #12360 (@stas00)\r\n- fixed multiplechoice tokenization #12362 (@cronoik)\r\n- [trainer] add main_process_first context manager #12351 (@stas00)\r\n- [Examples] Replicates the new --log_level feature to all trainer-based pytorch #12359 (@bhadreshpsavani)\r\n- [Examples] Update Example Template for `--log_level` feature #12365 (@bhadreshpsavani)\r\n- [Examples] Replace `print` statement with `logger.info` in QA example utils #12368 (@bhadreshpsavani)\r\n- Onnx export v2 fixes #12388 (@LysandreJik)\r\n- [Documentation] Warn that DataCollatorForWholeWordMask is limited to BertTokenizer-like tokenizers #12371 (@ionicsolutions)\r\n- Update run_mlm.py #12344 (@TahaAslani)\r\n- Add possibility to maintain full copies of files #12312 (@sgugger)\r\n- [CI] add dependency table sync verification #12364 (@stas00)\r\n- [Examples] Added context manager to datasets map #12367 (@bhadreshpsavani)\r\n- [Flax community event] Add more description to readme #12398 (@patrickvonplaten)\r\n- Remove the need for `einsum` in Albert's attention computation #12394 (@mfuntowicz)\r\n- [Flax] Adapt flax examples to include `push_to_hub` #12391 (@patrickvonplaten)\r\n- Tensorflow LM examples #12358 (@Rocketknight1)\r\n- [Deepspeed] match the trainer log level #12401 (@stas00)\r\n- [Flax] Add T5 pretraining script #12355 (@patrickvonplaten)\r\n- [models] respect dtype of the model when instantiating it #12316 (@stas00)\r\n- Rename detr targets to labels #12280 (@NielsRogge)\r\n- Add out of vocabulary error to ASR models #12288 (@will-rice)\r\n- Fix TFWav2Vec2 SpecAugment #12289 (@will-rice)\r\n- [example/flax] add summarization readme #12393 (@patil-suraj)\r\n- [Flax] Example scripts - correct weight decay #12409 (@patrickvonplaten)\r\n- fix ids_to_tokens naming error in tokenizer of deberta v2 #12412 (@hjptriplebee)\r\n- Minor fixes in original RAG training script #12395 (@shamanez)\r\n- Added talks #12415 (@suzana-ilic)\r\n- [modelcard] fix #12422 (@stas00)\r\n- Add option to save on each training node #12421 (@sgugger)\r\n- Added to talks section #12433 (@suzana-ilic)\r\n- Fix default bool in argparser #12424 (@sgugger)\r\n- Add default bos_token and eos_token for tokenizer of deberta_v2 #12429 (@hjptriplebee)\r\n- fix typo in mt5 configuration docstring #12432 (@fcakyon)\r\n- Add to talks section #12442 (@suzana-ilic)\r\n- [JAX/Flax readme] add philosophy doc #12419 (@patil-suraj)\r\n- [Flax] Add wav2vec2 #12271 (@patrickvonplaten)\r\n- Add test for a WordLevel tokenizer model #12437 (@SaulLu)\r\n- [Flax community event] How to use hub during training #12447 (@patrickvonplaten)\r\n- [Wav2Vec2, Hubert] Fix ctc loss test #12458 (@patrickvonplaten)\r\n- Comment fast GPU TF tests #12452 (@LysandreJik)\r\n- Fix training_args.py barrier for torch_xla #12464 (@jysohn23)\r\n- Added talk details #12465 (@suzana-ilic)\r\n- Add TPU README #12463 (@patrickvonplaten)\r\n- Import check_inits handling of duplicate definitions. #12467 (@Iwontbecreative)\r\n- Validation split added: custom data files @sgugger, @patil-suraj #12407 (@Souvic)\r\n- Fixing bug with param count without embeddings #12461 (@TevenLeScao)\r\n- [roberta] fix lm_head.decoder.weight ignore_key handling #12446 (@stas00)\r\n- Rework notebooks and move them to the Notebooks repo #12471 (@sgugger)\r\n- fixed typo in flax-projects readme #12466 (@mplemay)\r\n- Fix TAPAS test uncovered by #12446 #12480 (@LysandreJik)\r\n- Add guide on how to build demos for the Flax sprint #12468 (@osanseviero)\r\n- Add `Repository` import to the FLAX example script #12501 (@LysandreJik)\r\n- [examples/flax] clip style image-text training example #12491 (@patil-suraj)\r\n- [Flax] Fix wav2vec2 pretrain arguments #12498 (@Wikidepia)\r\n- [Flax] ViT training example #12300 (@patil-suraj)\r\n- Fix order of state and input in Flax Quickstart README #12510 (@navjotts)\r\n- [Flax] Dataset streaming example #12470 (@patrickvonplaten)\r\n- [Flax] Correct flax training scripts #12514 (@patrickvonplaten)\r\n- [Flax] Correct logging steps flax #12515 (@patrickvonplaten)\r\n- [Flax] Fix another bug in logging steps #12516 (@patrickvonplaten)\r\n- [Wav2Vec2] Flax - Adapt wav2vec2 script #12520 (@patrickvonplaten)\r\n- [Flax] Fix hybrid clip #12519 (@patil-suraj)\r\n- [RoFormer] Fix some issues #12397 (@JunnYu)\r\n- FlaxGPTNeo #12493 (@patil-suraj)\r\n- Updated README #12540 (@suzana-ilic)\r\n- Edit readme #12541 (@SaulLu)\r\n- implementing tflxmertmodel integration test #12497 (@sadakmed)\r\n- [Flax] Adapt examples to be able to use eval_steps and save_steps #12543 (@patrickvonplaten)\r\n- [examples/flax] add adafactor optimizer #12544 (@patil-suraj)\r\n- [Flax] Add FlaxMBart #12236 (@stancld)\r\n- Add a warning for broken ProphetNet fine-tuning #12511 (@JetRunner)\r\n- [trainer] add option to ignore keys for the train function too (#11719) #12551 (@shabie)\r\n- MLM training fails with no validation file(same as #12406 for pytorch now) #12517 (@Souvic)\r\n- [Flax] Allow retraining from save checkpoint #12559 (@patrickvonplaten)\r\n- Adding prepare_decoder_input_ids_from_labels methods to all TF ConditionalGeneration models #12560 (@Rocketknight1)\r\n- Remove tf.roll wherever not needed #12512 (@szutenberg)\r\n- Double check for attribute num_examples #12562 (@sgugger)\r\n- [examples/hybrid_clip] fix loading clip vision model #12566 (@patil-suraj)\r\n- Remove logging of GPU count etc from run_t5_mlm_flax.py #12569 (@ibraheem-moosa)\r\n- raise exception when arguments to pipeline are incomplete #12548 (@hwijeen)\r\n- Init pickle #12567 (@sgugger)\r\n- Fix group_lengths for short datasets #12558 (@sgugger)\r\n- Don't stop at num_epochs when using IterableDataset #12561 (@sgugger)\r\n- Fixing the pipeline optimization by reindexing targets (V2) #12330 (@Narsil)\r\n- Fix MT5 init #12591 (@sgugger)\r\n- [model.from_pretrained] raise exception early on failed load #12574 (@stas00)\r\n- [doc] fix broken ref #12597 (@stas00)\r\n- Add Flax sprint project evaluation section #12592 (@osanseviero)\r\n- This will reduce \"Already borrowed error\": #12550 (@Narsil)\r\n- [Flax] Add flax marian #12595 (@patrickvonplaten)\r\n- [Flax] Fix cur step flax examples #12608 (@patrickvonplaten)\r\n- Simplify unk token #12582 (@sgugger)\r\n- Fix arg count for partial functions #12609 (@sgugger)\r\n- Pass `model_kwargs` when loading a model in `pipeline()` #12449 (@aphedges)\r\n- [Flax] Fix mt5 auto #12612 (@patrickvonplaten)\r\n- [Flax Marian] Add marian flax example #12614 (@patrickvonplaten)\r\n- [FLax] Fix marian docs 2 #12615 (@patrickvonplaten)\r\n- [debugging utils] minor doc improvements #12525 (@stas00)\r\n- [doc] DP/PP/TP/etc parallelism #12524 (@stas00)\r\n- [doc] fix anchor #12620 (@stas00)\r\n- [Examples][Flax] added test file in summarization example #12630 (@bhadreshpsavani)\r\n- Point to the right file for hybrid CLIP #12599 (@edugp)\r\n- [flax]fix jax array type check #12638 (@patil-suraj)\r\n- Add tokenizer_file parameter to PreTrainedTokenizerFast docstring #12624 (@lewisbails)\r\n- Skip TestMarian_MT_EN #12649 (@LysandreJik)\r\n- The extended trainer tests should require torch #12650 (@LysandreJik)\r\n- Pickle auto models #12654 (@sgugger)\r\n- Pipeline should be agnostic #12656 (@LysandreJik)\r\n- Fix transfo xl integration test #12652 (@LysandreJik)\r\n- Remove SageMaker documentation #12657 (@philschmid)\r\n- Fixed docs #12646 (@KickItLikeShika)\r\n- fix typo in modeling_t5.py docstring #12640 (@PhilipMay)\r\n- Translate README.md to Simplified Chinese #12596 (@JetRunner)\r\n- Fix typo in README_zh-hans.md #12663 (@JetRunner)\r\n- Updates timeline for project evaluation #12660 (@osanseviero)\r\n- [WIP] Patch BigBird tokenization test #12653 (@LysandreJik)\r\n- **encode_plus() shouldn't run for W2V2CTC #12655 (@LysandreJik)\r\n- Add ByT5 option to example run_t5_mlm_flax.py #12634 (@mapmeld)\r\n- Wrong model is used in example, should be character instead of subword model #12676 (@jsteggink)\r\n- [Blenderbot] Fix docs #12227 (@patrickvonplaten)\r\n- Add option to load a pretrained model with mismatched shapes #12664 (@sgugger)\r\n- Fix minor docstring typos. #12682 (@qqaatw)\r\n- [tokenizer.prepare_seq2seq_batch] change deprecation to be easily actionable #12669 (@stas00)\r\n- [Flax Generation] Correct inconsistencies PyTorch/Flax #12662 (@patrickvonplaten)\r\n- [Deepspeed] adapt multiple models, add zero_to_fp32 tests #12477 (@stas00)\r\n- Add timeout to CI. #12684 (@LysandreJik)\r\n- Fix Tensorflow Bart-like positional encoding #11897 (@JunnYu)\r\n- [Deepspeed] non-native optimizers are mostly ok with zero-offload #12690 (@stas00)\r\n- Fix multiple choice doc examples #12679 (@sgugger)\r\n- Provide mask_time_indices to `_mask_hidden_states` to avoid double masking #12692 (@mfuntowicz)\r\n- Update TF examples README #12703 (@Rocketknight1)\r\n- Fix uninitialized variables when `config.mask_feature_prob > 0` #12705 (@mfuntowicz)\r\n- Only test the files impacted by changes in the diff #12644 (@sgugger)\r\n- flax model parallel training #12590 (@patil-suraj)\r\n- [test] split test into 4 sub-tests to avoid timeout #12710 (@stas00)\r\n- [trainer] release tmp memory in checkpoint load #12718 (@stas00)\r\n- [Flax] Correct shift labels for seq2seq models in Flax #12720 (@patrickvonplaten)\r\n- Fix typo in Speech2TextForConditionalGeneration example #12716 (@will-rice)\r\n- Init adds its own files as impacted #12709 (@sgugger)\r\n- LXMERT integration test typo #12736 (@LysandreJik)\r\n- Fix AutoModel tests #12733 (@LysandreJik)\r\n- Skip test while the model is not available #12739 (@LysandreJik)\r\n- Skip test while the model is not available #12740 (@LysandreJik)\r\n- Translate README.md to Traditional Chinese #12701 (@qqaatw)\r\n- Fix MBart failing test #12737 (@LysandreJik)\r\n- Patch T5 device test #12742 (@LysandreJik)\r\n- Fix DETR integration test #12734 (@LysandreJik)\r\n- Fix led torchscript #12735 (@LysandreJik)\r\n- Remove framework mention #12731 (@LysandreJik)\r\n- [doc] parallelism: Which Strategy To Use When #12712 (@stas00)\r\n- [doc] performance: batch sizes #12725 (@stas00)\r\n- Replace specific tokenizer in log message by AutoTokenizer #12745 (@SaulLu)\r\n- [Wav2Vec2] Correctly pad mask indices for PreTraining #12748 (@patrickvonplaten)\r\n- [doc] testing: how to trigger a self-push workflow #12724 (@stas00)\r\n- add intel-tensorflow-avx512 to the candidates #12751 (@zzhou612)\r\n- [flax/model_parallel] fix typos #12757 (@patil-suraj)\r\n- Turn on eval mode when exporting to ONNX #12758 (@mfuntowicz)\r\n- Preserve `list` type of `additional_special_tokens` in `special_token_map` #12759 (@SaulLu)\r\n- [Wav2Vec2] Padded vectors should not allowed to be sampled #12764 (@patrickvonplaten)\r\n- Add tokenizers class mismatch detection between `cls` and checkpoint #12619 (@europeanplaice)\r\n- Fix push_to_hub docstring and make it appear in doc #12770 (@sgugger)\r\n- [ray] Fix `datasets_modules` ImportError with Ray Tune #12749 (@Yard1)\r\n- Longer timeout for slow tests #12779 (@LysandreJik)\r\n- Enforce eval and save strategies are compatible when --load_best_model_at_end #12786 (@sgugger)\r\n- [CIs] add troubleshooting docs #12791 (@stas00)\r\n- Fix Padded Batch Error 12282 #12487 (@will-rice)\r\n- Flax MLM: Allow validation split when loading dataset from local file #12689 (@fgaim)\r\n- [Longformer] Correct longformer docs #12809 (@patrickvonplaten)\r\n- [CLIP/docs] add and fix examples #12810 (@patil-suraj)\r\n- [trainer] sanity checks for `save_steps=0|None` and `logging_steps=0` #12796 (@stas00)\r\n- Expose get_config() on ModelTesters #12812 (@LysandreJik)\r\n- Refactor slow sentencepiece tokenizers. #11716 (@PhilipMay)\r\n- Refer warmup_ratio when setting warmup_num_steps. #12818 (@tsuchm)\r\n- Add versioning system to fast tokenizer files #12713 (@sgugger)\r\n- Add _CHECKPOINT_FOR_DOC to all models #12811 (@LysandreJik)",
        "dateCreated": "2021-07-22T10:12:14Z",
        "datePublished": "2021-07-22T10:53:51Z",
        "html_url": "https://github.com/huggingface/transformers/releases/tag/v4.9.0",
        "name": "v4.9.0: TensorFlow examples, CANINE, tokenizer training, ONNX rework",
        "tag_name": "v4.9.0",
        "tarball_url": "https://api.github.com/repos/huggingface/transformers/tarball/v4.9.0",
        "url": "https://api.github.com/repos/huggingface/transformers/releases/46540302",
        "zipball_url": "https://api.github.com/repos/huggingface/transformers/zipball/v4.9.0"
      },
      {
        "authorType": "User",
        "author_name": "LysandreJik",
        "body": "# Patch release: v4.8.2\r\n\r\n- Rename detr targets to labels #12280 (@NielsRogge)\r\n- fix ids_to_tokens naming error in tokenizer of deberta v2 #12412 (@hjptriplebee)\r\n- Add option to save on each training node #12421 (@sgugger)",
        "dateCreated": "2021-06-30T12:27:44Z",
        "datePublished": "2021-06-30T12:42:38Z",
        "html_url": "https://github.com/huggingface/transformers/releases/tag/v4.8.2",
        "name": "Patch release: v4.8.2",
        "tag_name": "v4.8.2",
        "tarball_url": "https://api.github.com/repos/huggingface/transformers/tarball/v4.8.2",
        "url": "https://api.github.com/repos/huggingface/transformers/releases/45488244",
        "zipball_url": "https://api.github.com/repos/huggingface/transformers/zipball/v4.8.2"
      },
      {
        "authorType": "User",
        "author_name": "sgugger",
        "body": "# v4.8.1: Patch release\r\n\r\n- Fix default for TensorBoard folder\r\n- Ray Tune install #12338 \r\n- Tests fixes for Torch FX #12336",
        "dateCreated": "2021-06-24T14:12:35Z",
        "datePublished": "2021-06-24T14:15:37Z",
        "html_url": "https://github.com/huggingface/transformers/releases/tag/v4.8.1",
        "name": "v4.8.1: Patch release",
        "tag_name": "v4.8.1",
        "tarball_url": "https://api.github.com/repos/huggingface/transformers/tarball/v4.8.1",
        "url": "https://api.github.com/repos/huggingface/transformers/releases/45172638",
        "zipball_url": "https://api.github.com/repos/huggingface/transformers/zipball/v4.8.1"
      },
      {
        "authorType": "User",
        "author_name": "sgugger",
        "body": "# v4.8.0 Integration with the Hub and Flax/JAX support\r\n\r\n## Integration with the Hub\r\n\r\nOur example scripts and Trainer are now optimized for publishing your model on the [Hugging Face Hub](https://huggingface.co/models), with Tensorboard training metrics, and an automatically authored model card which contains all the relevant metadata, including evaluation results.\r\n\r\n### Trainer Hub integration\r\n\r\nUse --push_to_hub to create a model repo for your training and it will be saved with all relevant metadata at the end of the training.\r\n\r\nOther flags are: \r\n- `push_to_hub_model_id` to control the repo name\r\n- `push_to_hub_organization` to specify an organization\r\n\r\n### Visualizing Training metrics on huggingface.co (based on Tensorboard)\r\n\r\nBy default if you have `tensorboard` installed the training scripts will use it to log, and the logging traces folder is conveniently located inside your model output directory, so you can push them to your model repo by default.\r\n\r\nAny model repo that contains Tensorboard traces will spawn a Tensorboard server:\r\n\r\n![image](https://user-images.githubusercontent.com/35901082/123144141-5c2af980-d429-11eb-8438-16b374d2fe73.png)\r\n\r\nwhich makes it very convenient to see how the training went! This Hub feature is in Beta so let us know if anything looks weird :)\r\n\r\nSee this [model repo](https://huggingface.co/julien-c/reactiongif-roberta/tensorboard)\r\n\r\n### Model card generation\r\n\r\n![image](https://user-images.githubusercontent.com/35901082/123144222-749b1400-d429-11eb-97f6-9834dcc97c6d.png)\r\n\r\nThe model card contains info about the datasets used, the eval results, ...\r\n\r\nMany users were already adding their eval results to their model cards in markdown format, but this is a more structured way of adding them which will make it easier to parse and e.g. represent in leaderboards such as the ones on Papers With Code!\r\n\r\nWe use a format specified in collaboration with [PaperswithCode] (https://github.com/huggingface/huggingface_hub/blame/main/modelcard.md), see also [this repo](https://github.com/paperswithcode/model-index).\r\n\r\n### Model, tokenizer and configurations\r\n\r\nAll models, tokenizers and configurations having a revamp `push_to_hub()` method as well as a `push_to_hub` argument in their `save_pretrained()` method. The workflow of this method is changed a bit to be more like git, with a local clone of the repo in a folder of the working directory, to make it easier to apply patches (use `use_temp_dir=True` to clone in temporary folders for the same behavior as the experimental API).\r\n\r\n- Clean push to hub API #12187 (@sgugger)\r\n\r\n## Flax/JAX  support\r\n\r\nFlax/JAX  is becoming a fully supported backend of the Transformers library with more models having an implementation in it. BART, CLIP and T5 join the already existing models, find the whole list [here](https://huggingface.co/transformers/#supported-frameworks).\r\n\r\n- [Flax] FlaxAutoModelForSeq2SeqLM #12228 (@patil-suraj)\r\n- [FlaxBart] few small fixes #12247 (@patil-suraj)\r\n- [FlaxClip] fix test from/save pretrained test #12284 (@patil-suraj)\r\n- [Flax] [WIP] allow loading head model with base model weights #12255 (@patil-suraj)\r\n- [Flax] Fix flax test save pretrained #12256 (@patrickvonplaten)\r\n- [Flax] Add jax flax to env command #12251 (@patrickvonplaten)\r\n- add FlaxAutoModelForImageClassification in main init #12298 (@patil-suraj)\r\n- Flax T5 #12150 (@vasudevgupta7)\r\n- [Flax T5] Fix weight initialization and fix docs #12327 (@patrickvonplaten)\r\n- Flax summarization script #12230 (@patil-suraj)\r\n- FlaxBartPretrainedModel -> FlaxBartPreTrainedModel #12313 (@sgugger)\r\n\r\n## General improvements and bug fixes\r\n\r\n- AutoTokenizer: infer the class from the tokenizer config if possible #12208 (@sgugger)\r\n- update desc for map in all examples #12226 (@bhavitvyamalik)\r\n- Depreciate pythonic Mish and support PyTorch 1.9 version of Mish #12240 (@digantamisra98)\r\n- [t5 doc] make the example work out of the box #12239 (@stas00)\r\n- Better CI feedback #12279 (@LysandreJik)\r\n- Fix for making student ProphetNet for Seq2Seq Distillation #12130 (@vishal-burman)\r\n- [DeepSpeed] don't ignore --adafactor #12257 (@stas00)\r\n- Tensorflow QA example #12252 (@Rocketknight1)\r\n- [tests] reset report_to to none, avoid deprecation warning #12293 (@stas00)\r\n- [trainer + examples] set log level from CLI #12276 (@stas00)\r\n- [tests] multiple improvements #12294 (@stas00)\r\n- Trainer: adjust wandb installation example #12291 (@stefan-it)\r\n- Fix and improve documentation for LEDForConditionalGeneration #12303 (@ionicsolutions)\r\n- [Flax] Main doc for event orga #12305 (@patrickvonplaten)\r\n- [trainer] 2 bug fixes and a rename #12309 (@stas00)\r\n- [docs] performance #12258 (@stas00)\r\n- Add CodeCarbon Integration #12304 (@JetRunner)\r\n- Optimizing away the `fill-mask` pipeline. #12113 (@Narsil)\r\n- Add output in a dictionary for TF `generate` method #12139 (@stancld)\r\n- Rewrite ProphetNet to adapt converting ONNX friendly #11981 (@jiafatom)\r\n- Add mention of the huggingface_hub methods for offline mode #12320 (@LysandreJik)\r\n- [Flax/JAX] Add how to propose projects markdown #12311 (@patrickvonplaten)\r\n- [TFWav2Vec2] Fix docs #12283 (@chenht2010)\r\n- Add all XxxPreTrainedModel to the main init #12314 (@sgugger)\r\n- Conda build #12323 (@LysandreJik)\r\n- Changed modeling_fx_utils.py to utils/fx.py for clarity #12326 (@michaelbenayoun)\r\n",
        "dateCreated": "2021-06-23T17:25:15Z",
        "datePublished": "2021-06-23T17:28:58Z",
        "html_url": "https://github.com/huggingface/transformers/releases/tag/v4.8.0",
        "name": "v4.8.0 Integration with the Hub and Flax/JAX support",
        "tag_name": "v4.8.0",
        "tarball_url": "https://api.github.com/repos/huggingface/transformers/tarball/v4.8.0",
        "url": "https://api.github.com/repos/huggingface/transformers/releases/45115185",
        "zipball_url": "https://api.github.com/repos/huggingface/transformers/zipball/v4.8.0"
      },
      {
        "authorType": "User",
        "author_name": "LysandreJik",
        "body": "# v4.7.0: DETR, RoFormer, ByT5, Hubert, support for torch 1.9.0\r\n\r\n## DETR (@NielsRogge)\r\n\r\nThree new models are released as part of the DETR implementation: `DetrModel`, `DetrForObjectDetection` and `DetrForSegmentation`, in PyTorch.\r\n\r\nDETR consists of a convolutional backbone followed by an encoder-decoder Transformer which can be trained end-to-end for object detection. It greatly simplifies a lot of the complexity of models like Faster-R-CNN and Mask-R-CNN, which use things like region proposals, non-maximum suppression procedure, and anchor generation. Moreover, DETR can also be naturally extended to perform panoptic segmentation, by simply adding a mask head on top of the decoder outputs.\r\n\r\nDETR can support any [timm](https://github.com/rwightman/pytorch-image-models) backbone.\r\n\r\nThe DETR model was proposed in [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872) by Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko.\r\n\r\n- Add DETR #11653 (@NielsRogge)\r\n- Improve DETR #12147 (@NielsRogge)\r\n\r\nCompatible checkpoints can be found on the Hub: https://huggingface.co/models?filter=detr\r\n\r\n## ByT5 (@patrickvonplaten)\r\n\r\nA new tokenizer is released as part of the ByT5 implementation: `ByT5Tokenizer`. It can be used with the T5 family of models.\r\n\r\nThe ByT5 model was presented in [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel.\r\n\r\n- ByT5 model #11971 (@patrickvonplaten)\r\n\r\nCompatible checkpoints can be found on the Hub: https://huggingface.co/models?search=byt5\r\n\r\n##  RoFormer (@JunnYu)\r\n\r\n14 new models are released as part of the RoFormer implementation: `RoFormerModel`, `RoFormerForCausalLM`, `RoFormerForMaskedLM`, `RoFormerForSequenceClassification`, `RoFormerForTokenClassification`, `RoFormerForQuestionAnswering` and `RoFormerForMultipleChoice`, `TFRoFormerModel`, `TFRoFormerForCausalLM`, `TFRoFormerForMaskedLM`, `TFRoFormerForSequenceClassification`, `TFRoFormerForTokenClassification`, `TFRoFormerForQuestionAnswering` and `TFRoFormerForMultipleChoice`, in PyTorch and TensorFlow.\r\n\r\nRoFormer is a BERT-like autoencoding model with rotary position embeddings. Rotary position embeddings have shown improved performance on classification tasks with long texts. The RoFormer model was proposed in [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/pdf/2104.09864v1.pdf) by Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu.\r\n\r\n- Add new model RoFormer (use rotary position embedding ) #11684 (@JunnYu)\r\n\r\nCompatible checkpoints can be found on the Hub: https://huggingface.co/models?filter=roformer\r\n\r\n## HuBERT (@patrickvonplaten)\r\n\r\nHuBERT is a speech model that accepts a float array corresponding to the raw waveform of the speech signal.\r\n\r\nHuBERT was proposed in [HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units](https://arxiv.org/abs/2106.07447) by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed.\r\n\r\nTwo new models are released as part of the HuBERT implementation: `HubertModel` and `HubertForCTC`, in PyTorch.\r\n\r\nCompatible checkpoints can be found on the Hub: https://huggingface.co/models?filter=hubert\r\n\r\n- Hubert #11889 (@patrickvonplaten)\r\n\r\n## Hugging Face Course - Part 1\r\n\r\nOn Monday, June 14th, 2021, we released the first part of the Hugging Face Course. The course is focused on the Hugging Face ecosystem, including `transformers`. Most of the material in the course is now linked from the `transformers` documentation which now includes videos to explain singular concepts.\r\n\r\n- Add video links to the documentation #12162 (@sgugger)\r\n- Add link to the course #12229 (@sgugger)\r\n\r\n## TensorFlow additions\r\n\r\nThe Wav2Vec2 model can now be used in TensorFlow:\r\n\r\n- Adding TFWav2Vec2Model #11617 (@will-rice)\r\n\r\n## PyTorch 1.9 support\r\n\r\n- Add support for torch 1.9.0 #12224 (@LysandreJik )\r\n- fix pt-1.9.0 add_ deprecation #12217 (@stas00)\r\n\r\n## Notebooks\r\n\r\n- @NielsRogge has contributed five tutorials on the usage of BERT in his repository: [Transformers-Tutorials](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/DETR)\r\n- [Community Notebooks] Add Emotion Speech Noteboook #11900 (@patrickvonplaten)\r\n\r\n## General improvements and bugfixes\r\n\r\n- Vit deit fixes #11309 (@NielsRogge)\r\n- Enable option for subword regularization in more tokenizers. #11417 (@PhilipMay)\r\n- Fix gpt-2 warnings #11709 (@LysandreJik)\r\n- [Flax] Fix BERT initialization & token_type_ids default #11695 (@patrickvonplaten)\r\n- BigBird on TPU #11651 (@vasudevgupta7)\r\n- [T5] Add 3D attention mask to T5 model (2) (#9643) #11197 (@lexhuismans)\r\n- Fix loading the best model on the last stage of training #11718 (@vbyno)\r\n- Fix T5 beam search when using parallelize #11717 (@OyvindTafjord)\r\n- [Flax] Correct example script #11726 (@patrickvonplaten)\r\n- Add Cloud details to README #11706 (@marcvanzee)\r\n- Experimental symbolic tracing feature with torch.fx for BERT, ELECTRA and T5 #11475 (@michaelbenayoun)\r\n- Improvements to Flax finetuning script #11727 (@marcvanzee)\r\n- Remove tapas model card #11739 (@julien-c)\r\n- Add visual + link to Premium Support webpage #11740 (@julien-c)\r\n- Issue with symbolic tracing for T5 #11742 (@michaelbenayoun)\r\n- [BigBird Pegasus] Make tests faster #11744 (@patrickvonplaten)\r\n- Use new evaluation loop in TrainerQA #11746 (@sgugger)\r\n- Flax BERT fix token type init #11750 (@patrickvonplaten)\r\n- [TokenClassification] Label realignment for subword aggregation #11680 (@Narsil)\r\n- Fix checkpoint deletion #11748 (@sgugger)\r\n- Fix incorrect newline in #11650 #11757 (@oToToT)\r\n- Add more subsections to main doc #11758 (@patrickvonplaten)\r\n- Fixed: Better names for nlp variables in pipelines' tests and docs. #11752 (@01-vyom)\r\n- add `dataset_name` to data_args and added accuracy metric #11760 (@philschmid)\r\n- Add Flax Examples and Cloud TPU README #11753 (@avital)\r\n- Fix a bug in summarization example which did not load model from config properly #11762 (@tomy0000000)\r\n- FlaxGPT2 #11556 (@patil-suraj)\r\n- Fix usage of head masks by PT encoder-decoder models' `generate()` function #11621 (@stancld)\r\n- [T5 failing CI] Fix generate test #11770 (@patrickvonplaten)\r\n- [Flax MLM] Refactor run mlm with optax #11745 (@patrickvonplaten)\r\n- Add DOI badge to README #11771 (@albertvillanova)\r\n- Deprecate commands from the transformers-cli that are in the hf-cli #11779 (@LysandreJik)\r\n- Fix release utilpattern in conf.py #11784 (@sgugger)\r\n- Fix regression in regression #11785 (@sgugger)\r\n- A cleaner and more scalable implementation of symbolic tracing #11763 (@michaelbenayoun)\r\n- Fix failing test on Windows Platform #11589 (@Lynx1820)\r\n- [Flax] Align GLUE training script with mlm training script #11778 (@patrickvonplaten)\r\n- Patch recursive import #11812 (@LysandreJik)\r\n- fix roformer config doc #11813 (@JunnYu)\r\n- [Flax] Small fixes in `run_flax_glue.py` #11820 (@patrickvonplaten)\r\n- [Deepspeed] support `zero.Init` in `from_config` #11805 (@stas00)\r\n- Add flax text class colab #11824 (@patrickvonplaten)\r\n- Faster list concat for trainer_pt_utils.get_length_grouped_indices() #11825 (@ctheodoris)\r\n- Replace double occurrences as the last step #11367 (@LysandreJik)\r\n- [Flax] Fix PyTorch import error #11839 (@patrickvonplaten)\r\n- Fix reference to XLNet #11846 (@sgugger)\r\n- Switch mem metrics flag #11851 (@sgugger)\r\n- Fix flos single node #11844 (@TevenLeScao)\r\n- Fix two typos in docs #11852 (@nickls)\r\n- [Trainer] Report both steps and num samples per second #11818 (@sgugger)\r\n- Add some tests to the slow suite #11860 (@LysandreJik)\r\n- Enable memory metrics in tests that need it #11859 (@LysandreJik)\r\n- fixed a small typo in the CONTRIBUTING doc #11856 (@stsuchi)\r\n- typo #11858 (@WrRan)\r\n- Add option to log only once in multinode training #11819 (@sgugger)\r\n- [Wav2Vec2] SpecAugment Fast #11764 (@patrickvonplaten)\r\n- [lm examples] fix overflow in perplexity calc #11855 (@stas00)\r\n- [Examples] create model with custom config on the fly #11798 (@stas00)\r\n- [Wav2Vec2ForCTC] example typo fixed #11878 (@madprogramer)\r\n- [AutomaticSpeechRecognitionPipeline] Ensure input tensors are on device #11874 (@francescorubbo)\r\n- Fix usage of head masks by TF encoder-decoder models' `generate()` function #11775 (@stancld)\r\n- Correcting comments in T5Stack to reflect correct tuple order #11330 (@talkhaldi)\r\n- [Flax] Allow dataclasses to be jitted #11886 (@patrickvonplaten)\r\n- changing find_batch_size to work with tokenizer outputs #11890 (@joerenner)\r\n- Link official Cloud TPU JAX docs #11892 (@avital)\r\n- Flax Generate #11777 (@patrickvonplaten)\r\n- Update deepspeed config to reflect hyperparameter search parameters #11896 (@Mindful)\r\n- Adding new argument `max_new_tokens` for generate. #11476 (@Narsil)\r\n- Added Sequence Classification class in GPTNeo #11906 (@bhadreshpsavani)\r\n- [Flax] Return Attention from BERT, ELECTRA, RoBERTa and GPT2 #11918 (@jayendra13)\r\n- Test optuna and ray #11924 (@LysandreJik)\r\n- Use `self.assertEqual` instead of `assert` in deberta v2 test. #11935 (@PhilipMay)\r\n- Remove redundant `nn.log_softmax` in `run_flax_glue.py` #11920 (@n2cholas)\r\n- Add MT5ForConditionalGeneration as supported arch. to summarization README #11961 (@PhilipMay)\r\n- Add FlaxCLIP #11883 (@patil-suraj)\r\n- RAG-2nd2end-revamp #11893 (@shamanez)\r\n- modify qa-trainer #11872 (@zhangfanTJU)\r\n- get_ordinal(local=True) replaced with get_local_ordinal() in training_args.py #11922 (@BassaniRiccardo)\r\n- reinitialize wandb config for each hyperparameter search run #11945 (@Mindful)\r\n- Add regression tests for slow sentencepiece tokenizers. #11737 (@PhilipMay)\r\n- Authorize args when instantiating an AutoModel #11956 (@LysandreJik)\r\n- Neptune.ai integration #11937 (@vbyno)\r\n- [deepspeed] docs #11940 (@stas00)\r\n- typo correction #11973 (@JminJ)\r\n - Typo in usage example, changed to device instead of torch_device #11979 (@albertovilla)\r\n- [DeepSpeed] decouple `DeepSpeedConfigHF` from `Trainer` #11966 (@stas00)\r\n- [Trainer] add train loss and flops metrics reports #11980 (@stas00)\r\n- Bump urllib3 from 1.25.8 to 1.26.5 in /examples/research_projects/lxmert #11983 (@dependabot[bot])\r\n- [RAG] Fix rag from pretrained question encoder generator behavior #11962 (@patrickvonplaten)\r\n- Fix examples in VisualBERT docs #11990 (@gchhablani)\r\n- [docs] fix xref to `PreTrainedModel.generate` #11049 (@stas00)\r\n- Update return introduction of `forward` method #11976 (@kouyk)\r\n- [deepspeed] Move code and doc into standalone files #11984 (@stas00)\r\n- [deepspeed] add nvme test skip rule #11997 (@stas00)\r\n- Fix weight decay masking in `run_flax_glue.py` #11964 (@n2cholas)\r\n- [Flax] Refactor MLM #12013 (@patrickvonplaten)\r\n- [Deepspeed] Assert on mismatches between ds and hf args #12021 (@stas00)\r\n- [TrainerArguments] format and sort __repr__, add __str__ #12018 (@stas00)\r\n- Fixed Typo in modeling_bart.py #12035 (@ceevaaa)\r\n- Fix deberta 2 Tokenizer Integration Test #12017 (@PhilipMay)\r\n- fix past_key_values docs #12049 (@patil-suraj)\r\n- [JAX] Bump jax lib #12053 (@patrickvonplaten)\r\n- Fixes bug that appears when using QA bert and distilation. #12026 (@madlag)\r\n- Extend pipelines for automodel tupels #12025 (@Narsil)\r\n- Add optional grouped parsers description to HfArgumentParser #12042 (@peteriz)\r\n- adds metric prefix. #12057 (@riklopfer)\r\n- [CI] skip failing test #12059 (@stas00)\r\n- Fix LUKE integration tests #12066 (@NielsRogge)\r\n- Fix tapas issue #12063 (@NielsRogge)\r\n- updated the original RAG implementation to be compatible with latest Pytorch-Lightning #11806 (@shamanez)\r\n- Replace legacy tensor.Tensor with torch.tensor/torch.empty #12027 (@mariosasko)\r\n- Add torch to requirements.txt in language-modeling #12040 (@cdleong)\r\n- Properly indent block_size #12070 (@sgugger)\r\n- [Deepspeed] various fixes #12058 (@stas00)\r\n- [Deepspeed Wav2vec2] integration #11638 (@stas00)\r\n- Update run_ner.py with id2label config #12001 (@KoichiYasuoka)\r\n- [wav2vec2 / Deepspeed] sync LayerDrop for Wav2Vec2Encoder + tests #12076 (@stas00)\r\n- [test] support more than 2 gpus #12074 (@stas00)\r\n- Wav2Vec2 Pretraining #11306 (@anton-l)\r\n- [examples/flax] pass decay_mask fn to optimizer #12087 (@patil-suraj)\r\n- [versions] rm require_version_examples #12088 (@stas00)\r\n- [Wav2Vec2ForPretraining] Correct checkpoints wav2vec2 & fix tests #12089 (@patrickvonplaten)\r\n- Add text_column_name and label_column_name to run_ner and run_ner_no_trainer args #12083 (@kumapo)\r\n- CLIPFeatureExtractor should resize images with kept aspect ratio #11994 (@TobiasNorlund)\r\n- New TF GLUE example #12028 (@Rocketknight1)\r\n- Appending label2id and id2label to models for inference #12102 (@Rocketknight1)\r\n- Fix a condition in test_generate_with_head_masking #11911 (@stancld)\r\n- [Flax] Adding Visual-Transformer #11951 (@jayendra13)\r\n- add relevant description to tqdm in examples #11927 (@bhavitvyamalik)\r\n- Fix head masking generate tests #12110 (@patrickvonplaten)\r\n- Flax CLM script #12023 (@patil-suraj)\r\n- Add from_pretrained to dummy timm objects #12097 (@LysandreJik)\r\n- Fix t5 error message #12136 (@cccntu)\r\n- Fix megatron_gpt2 attention block's causal mask #12007 (@novatig)\r\n- Add mlm pretraining xla torch readme #12011 (@patrickvonplaten)\r\n- add readme for flax clm #12111 (@patil-suraj)\r\n- [Flax] Add FlaxBart models #11537 (@stancld)\r\n- Feature to use the PreTrainedTokenizerFast class as a stand-alone tokenizer #11810 (@SaulLu)\r\n- [Flax] Add links to google colabs #12146 (@patrickvonplaten)\r\n- Don't log anything before logging is setup in examples #12121 (@sgugger)\r\n- Use text_column_name variable instead of \"text\" #12132 (@nbroad1881)\r\n- [lm examples] Replicate --config_overrides addition to other LM examples #12135 (@kumar-abhishek)\r\n- [Flax] fix error message #12148 (@patil-suraj)\r\n- [optim] implement AdafactorSchedule #12123 (@stas00)\r\n- [style] consistent nn. and nn.functional #12124 (@stas00)\r\n- [Flax] Fix flax pt equivalence tests #12154 (@patrickvonplaten)\r\n- [style] consistent nn. and nn.functional: part2: templates #12153 (@stas00)\r\n- Flax Big Bird #11967 (@vasudevgupta7)\r\n- [style] consistent nn. and nn.functional: part 3 `tests` #12155 (@stas00)\r\n- [style] consistent nn. and nn.functional: part 4 `examples` #12156 (@stas00)\r\n- consistent nn. and nn.functional: part 5 docs #12161 (@stas00)\r\n- [Flax generate] Add params to generate #12171 (@patrickvonplaten)\r\n- Use a released version of optax rather than installing from Git. #12173 (@avital)\r\n- Have dummy processors have a `from_pretrained` method #12145 (@LysandreJik)\r\n- Add course banner #12157 (@sgugger)\r\n- Enable add_prefix_space on run_ner if necessary #12116 (@kumapo)\r\n- Update AutoModel classes in summarization example #12178 (@ionicsolutions)\r\n- Ray Tune Integration Updates #12134 (@amogkam)\r\n- [testing] ensure concurrent pytest workers use a unique port for torch.dist #12166 (@stas00)\r\n- Model card defaults #12122 (@sgugger)\r\n- Temporarily deactivate torch-scatter while we wait for new release #12181 (@LysandreJik)\r\n- Temporarily deactivate torchhub test #12184 (@sgugger)\r\n- [Flax] Add Beam Search #12131 (@patrickvonplaten)\r\n- updated DLC images and sample notebooks #12191 (@philschmid)\r\n- Enabling AutoTokenizer for HubertConfig. #12198 (@Narsil)\r\n- Use yaml to create metadata #12185 (@sgugger)\r\n- [Docs] fixed broken link #12205 (@bhadreshpsavani)\r\n- Pipeline update & tests #12207 (@LysandreJik)",
        "dateCreated": "2021-06-17T16:06:41Z",
        "datePublished": "2021-06-17T16:20:52Z",
        "html_url": "https://github.com/huggingface/transformers/releases/tag/v4.7.0",
        "name": "v4.7.0: DETR, RoFormer, ByT5, HuBERT, support for torch 1.9.0",
        "tag_name": "v4.7.0",
        "tarball_url": "https://api.github.com/repos/huggingface/transformers/tarball/v4.7.0",
        "url": "https://api.github.com/repos/huggingface/transformers/releases/44784822",
        "zipball_url": "https://api.github.com/repos/huggingface/transformers/zipball/v4.7.0"
      },
      {
        "authorType": "User",
        "author_name": "sgugger",
        "body": "Fix regression in models for sequence classification used for regression tasks #11785 \r\nFix checkpoint deletion when load_bert_model_at_end = True #11748\r\nFix evaluation in question answering examples #11746\r\nFix release utils #11784",
        "dateCreated": "2021-05-20T14:46:51Z",
        "datePublished": "2021-05-20T14:57:12Z",
        "html_url": "https://github.com/huggingface/transformers/releases/tag/v4.6.1",
        "name": "v4.6.1: Patch release",
        "tag_name": "v4.6.1",
        "tarball_url": "https://api.github.com/repos/huggingface/transformers/tarball/v4.6.1",
        "url": "https://api.github.com/repos/huggingface/transformers/releases/43293763",
        "zipball_url": "https://api.github.com/repos/huggingface/transformers/zipball/v4.6.1"
      },
      {
        "authorType": "User",
        "author_name": "LysandreJik",
        "body": "# v4.6.0: ViT, DeiT, CLIP, LUKE, BigBirdPegasus, MegatronBERT\r\n\r\nTransformers aren't just for text - they can handle a huge range of input types, and there's been a flurry of papers and new models in the last few months applying them to vision tasks that had traditionally been dominated by convolutional networks. With this release, we're delighted to announce that several state-of-the-art pretrained vision and multimodal text+vision transformer models are now accessible in the huggingface/transformers repo. Give them a try!\r\n\r\n## ViT (@NielsRogge)\r\n\r\nTwo new models are released as part of the ViT implementation: `ViTModel` and `ViTForImageClassification`, in PyTorch.\r\n\r\nViT is an image transformer-based model obtaining state-of-the-art results on image classification tasks. It was the first paper that successfully trained a Transformer encoder on ImageNet, attaining very good results compared to familiar convolutional architectures.\r\n\r\nThe Vision Transformer (ViT) model was proposed in An [Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby. \r\n\r\nCompatible checkpoints can be found on the Hub: https://huggingface.co/models?filter=vit\r\n\r\n## DeiT (@NielsRogge)\r\n\r\nThree new models are released as part of the DeiT implementation: `DeiTModel`, `DeiTForImageClassification` and `DeiTForImageClassificationWithTeacher`, in PyTorch.\r\n\r\nDeiT is an image transformer model similar to the ViT model. DeiT (data-efficient image transformers) models are more efficiently trained transformers for image classification, requiring far less data and far less computing resources compared to the original ViT models.\r\n\r\nThe DeiT model was proposed in [Training data-efficient image transformers & distillation through attention](https://arxiv.org/abs/2012.12877) by Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Herv\u00e9 J\u00e9gou. \r\n\r\nCompatible checkpoints can be found on the Hub: https://huggingface.co/models?filter=deit\r\n\r\n- Add DeiT (PyTorch) #11056 (@NielsRogge)\r\n\r\n## CLIP (@patil-suraj)\r\n\r\nThree new models are released as part of the CLIP implementation: `CLIPModel`, `CLIPVisionModel` and `CLIPTextModel`, in PyTorch.\r\n\r\nCLIP (Contrastive Language-Image Pre-Training) is a neural network trained on a variety of (image, text) pairs. It can be instructed in natural language to predict the most relevant text snippet, given an image, without directly optimizing for the task, similarly to the zero-shot capabilities of GPT-2 and 3.\r\n\r\nThe CLIP model was proposed in [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) by Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever. \r\n\r\nCompatible checkpoints can be found on the Hub: https://huggingface.co/models?filter=clip\r\n\r\n- CLIP #11445 (@patil-suraj)\r\n\r\n## BigBirdPegasus (@vasudevgupta7)\r\n\r\nBigBird is a sparse-attention-based transformer that extends Transformer based models, such as BERT to much longer sequences. In addition to sparse attention, BigBird also applies global attention as well as random attention to the input sequence. Theoretically, it has been shown that applying sparse, global, and random attention approximates full attention while being computationally much more efficient for longer sequences. As a consequence of the capability to handle longer context, BigBird has shown improved performance on various long document NLP tasks, such as question answering and summarization, compared to BERT or RoBERTa.\r\n\r\nThe BigBird model was proposed in [Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062) by Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others. \r\n\r\n- Add BigBirdPegasus #10991 (@vasudevgupta7)\r\n\r\nCompatible checkpoints can be found on the Hub: https://huggingface.co/models?filter=bigbird_pegasus\r\n\r\n## LUKE (@NielsRogge, @ikuyamada)\r\n\r\nLUKE is based on RoBERTa and adds entity embeddings as well as an entity-aware self-attention mechanism, which helps improve performance on various downstream tasks involving reasoning about entities such as named entity recognition, extractive and cloze-style question answering, entity typing, and relation classification.\r\n\r\nThe LUKE model was proposed in LUKE: [Deep Contextualized Entity Representations with Entity-aware Self-attention](https://arxiv.org/abs/2010.01057) by Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, and Yuji Matsumoto.\r\n\r\n- Add LUKE #11223 (@NielsRogge, @ikuyamada)\r\n\r\nCompatible checkpoints can be found on the Hub: https://huggingface.co/models?filter=luke\r\n\r\n## Megatron (@jdemouth)\r\n\r\nThe MegatronBERT model is added to the library, giving access to the 345m variants. \r\n\r\nIt is implemented comes with nine different models: `MegatronBertModel`, `MegatronBertForMaskedLM`, `MegatronBertForCausalLM`, `MegatronBertForNextSentencePrediction`, `MegatronBertForPreTraining`, `MegatronBertForSequenceClassification`, `MegatronBertForMultipleChoice`, `MegatronBertForTokenClassification`, `MegatronBertForQuestionAnswering`, in PyTorch.\r\n\r\nThe MegatronBERT model was proposed in [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053) by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper and Bryan Catanzaro.\r\n\r\n- Add nvidia megatron models #10911 (@jdemouth)\r\n\r\n## Hub integration in Transformers\r\n\r\nThe Hugging Face Hub integrates better within `transformers`, through two new added features:\r\n- Models, configurations and tokenizers now have a `push_to_hub` method to automatically push their state to the hub.\r\n- The `Trainer` can now automatically push its underlying model, configuration and tokenizer in a similar fashion. Additionally, it is able to create a draft of model card on the fly with the training hyperparameters and evaluation results.\r\n\r\n- Auto modelcard #11599 (@sgugger)\r\n- Trainer push to hub #11328 (@sgugger)\r\n\r\n## DeepSpeed ZeRO Stage 3 & ZeRO-Infinity\r\n\r\nThe `Trainer` now integrates two additional stages of ZeRO: ZeRO stage 3 for parameter partitioning, and ZeRO Infinity which extends CPU Offload with NVMe Offload.\r\n\r\n- [DeepSpeed] ZeRO Stage 3 #10753 (@stas00) [release notes](https://github.com/huggingface/transformers/issues/11044)\r\n- [Deepspeed] ZeRO-Infinity integration plus config revamp #11418 (@stas00) [release notes](https://github.com/huggingface/transformers/issues/11464)\r\n- PLease read both release notes for configuration file changes\r\n\r\n\r\n## Flax\r\n\r\nFlax support is getting more robust, with model code stabilizing and new models being added to the library.\r\n\r\n- [FlaxRoberta] Add FlaxRobertaModels & adapt run_mlm_flax.py #11470 (@patrickvonplaten)\r\n- [Flax] Add Electra models #11426 (@CoderPat)\r\n- Adds Flax BERT finetuning example on GLUE #11564 (@marcvanzee)\r\n\r\n## TensorFlow\r\n\r\nWe welcome @Rocketknight1 as a TensorFlow contributor. This version includes a brand new TensorFlow example based on Keras, which will be followed by examples covering most tasks.\r\nAdditionally, more TensorFlow setups are covered by adding support for AMD-based GPUs and M1 Macs.\r\n\r\n- Merge new TF example script #11360 (@Rocketknight1)\r\n- Update TF text classification example #11496 (@Rocketknight1)\r\n- run_text_classification.py fix #11660 (@Rocketknight1)\r\n- Accept tensorflow-rocm package when checking TF availability #11595 (@mvsjober)\r\n- Add MacOS TF version #11674 (@jplu)\r\n\r\n## Pipelines\r\n\r\nTwo new pipelines are added:\r\n\r\n- Adding `AutomaticSpeechRecognitionPipeline`. #11337 (@Narsil)\r\n- Add the ImageClassificationPipeline #11598 (@LysandreJik)\r\n\r\n## Notebooks\r\n\r\n- [Community notebooks] Add Wav2Vec notebook for creating captions for YT Clips #11142 (@Muennighoff)\r\n- add bigbird-pegasus evaluation notebook #11654 (@vasudevgupta7)\r\n- Vit notebooks + vit/deit fixes #11309 (@NielsRogge)\r\n\r\n## General improvements and bugfixes\r\n\r\n- [doc] gpt-neo #11098 (@stas00)\r\n- Auto feature extractor #11097 (@sgugger)\r\n- accelerate question answering examples with no trainer #11091 (@theainerd)\r\n- dead link fixed #11103 (@cronoik)\r\n- GPTNeo: handle padded wte (#11078) #11079 (@leogao2)\r\n- fix: The 'warn' method is deprecated #11105 (@stas00)\r\n- [examples] fix white space #11099 (@stas00)\r\n- Dummies multi backend #11100 (@sgugger)\r\n- Some styling of the training table in Notebooks #11118 (@sgugger)\r\n- Adds a note to resize the token embedding matrix when adding special \u2026 #11120 (@LysandreJik)\r\n- [BigBird] fix bigbird slow tests #11109 (@vasudevgupta7)\r\n- [versions] handle version requirement ranges #11110 (@stas00)\r\n- Adds use_auth_token with pipelines #11123 (@philschmid)\r\n- Fix and refactor check_repo #11127 (@sgugger)\r\n- Fix typing error in Trainer class (prediction_step) #11138 (@jannisborn)\r\n- Typo fix of the name of BertLMHeadModel in BERT doc #11133 (@forest1988)\r\n- [run_clm] clarify why we get the tokenizer warning on long input #11145 (@stas00)\r\n- [trainer] solve \"scheduler before optimizer step\" warning #11144 (@stas00)\r\n- Add fairscale and deepspeed back to the CI #11147 (@LysandreJik)\r\n- Updates SageMaker docs for updating DLCs #11140 (@philschmid)\r\n- Don't duplicate logs in TensorBoard and handle --use_env #11141 (@sgugger)\r\n- Run mlm pad to multiple for fp16 #11128 (@ak314)\r\n- [tests] relocate core integration tests #11146 (@stas00)\r\n- [setup] extras[docs] must include 'all' #11148 (@stas00)\r\n- Add support for multiple models for one config in auto classes #11150 (@sgugger)\r\n- [setup] make fairscale and deepspeed setup extras #11151 (@stas00)\r\n- typo #11152 (@stas00)\r\n- Fix LogitsProcessor documentation #11130 (@k-tahiro)\r\n- Correct typographical error in README.md #11161 (@Seyviour)\r\n- Make `get_special_tokens_mask` consider all tokens #11163 (@sgugger)\r\n- Add a special tokenizer for CPM model #11068 (@JetRunner)\r\n- [examples/translation] support mBART-50 and M2M100 fine-tuning #11170 (@patil-suraj)\r\n- [examples run_clm] fix _LazyModule hasher error #11168 (@stas00)\r\n- added json dump and extraction of train run time #11167 (@philschmid)\r\n- Minor typos fixed #11182 (@cronoik)\r\n- model_path should be ignored as the checkpoint path #11157 (@tsuchm)\r\n- Added documentation for data collator. #10941 (@fghuman)\r\n- Fix typo #11188 (@tma15)\r\n- Replaced `which` with `who` #11183 (@cronoik)\r\n- Import torch.utils.checkpoint in ProphetNet #11214 (@LysandreJik)\r\n- Sagemaker test docs update for framework upgrade #11206 (@philschmid)\r\n- Use MSELoss with single class label in (M)BartForSequenceClassification #11178 (@calpt)\r\n- wav2vec2 converter: create the proper vocab.json while converting fairseq wav2vec2 finetuned model #11041 (@cceyda)\r\n- Add Matt as the TensorFlow reference #11212 (@LysandreJik)\r\n- Fix GPT-2 warnings #11213 (@LysandreJik)\r\n- fix docs for decoder_input_ids #11221 (@patil-suraj)\r\n- Add documentation for BertJapanese #11219 (@forest1988)\r\n- Replace error by warning when loading an architecture in another #11207 (@sgugger)\r\n- Refactor GPT2 #11225 (@patil-suraj)\r\n- Doc check: a bit of clean up #11224 (@sgugger)\r\n- added cache_dir=model_args.cache_dir to all example with cache_dir arg #11220 (@philschmid)\r\n- Avoid using no_sync on SageMaker DP #11229 (@sgugger)\r\n- Indent code block in the documentation #11233 (@sgugger)\r\n- Run CI on deepspeed and fairscale #11172 (@LysandreJik)\r\n- [Deepspeed] zero3 tests band aid #11235 (@stas00)\r\n- Wav2Vec2 CommonVoice training - Save the processor before training starts #10910 (@Nithin-Holla)\r\n- Make \"embeddings\" plural in warning message within tokenization_utils_base #11228 (@jstremme)\r\n- Stale bot updated #10562 (@LysandreJik)\r\n- Close open files to suppress ResourceWarning #11240 (@parakalan)\r\n- Fix dimention misspellings. #11238 (@odellus)\r\n- Add prefix to examples in model_doc rst #11226 (@forest1988)\r\n- [troubleshooting] add 2 points of reference to the offline mode #11236 (@stas00)\r\n- Fix #10128 #11248 (@sgugger)\r\n- [deepspeed] test on one node 2 gpus max #11237 (@stas00)\r\n- Trainer iterable dataset #11254 (@sgugger)\r\n- Adding pipeline task aliases. #11247 (@Narsil)\r\n- Support for set_epoch in IterableDataset #11258 (@sgugger)\r\n- Tokenizer fast save #11234 (@sgugger)\r\n- update dependency_versions_table #11273 (@stas00)\r\n- Workflow fixes #11270 (@LysandreJik)\r\n- Enabling multilingual models for translation pipelines. #10536 (@Narsil)\r\n- Trainer support for IterableDataset for evaluation and predict #11286 (@sgugger)\r\n- move device statements outside if statements #11292 (@e-yi)\r\n- modify double considering special tokens in `language_modeling.py` #11275 (@taepd)\r\n- [Trainer] fix the placement on device with fp16_full_eval #11322 (@stas00)\r\n- [Trainer] Add a progress bar for batches skipped #11324 (@sgugger)\r\n- Load checkpoint without re-creating the model #11318 (@sgugger)\r\n- Added translation example script #11196 (@rajvi-k)\r\n- [Generate] Remove outdated code #11331 (@patrickvonplaten)\r\n- [GPTNeo] create local attention mask ones #11335 (@patil-suraj)\r\n- Update to use datasets remove_cloumns method #11343 (@sgugger)\r\n- Add an error message for Reformer w/ .backward() #11117 (@forest1988)\r\n- Removed `max_length` from being mandatory within `generate`. #11314 (@Narsil)\r\n- Honor contributors to models #11329 (@sgugger)\r\n- [deepspeed] fix resume from checkpoint #11352 (@stas00)\r\n- Examples reorg #11350 (@sgugger)\r\n- Extract metric_key_prefix during NotebookProgressCallback.on_evaluate #11347 (@lewtun)\r\n- [testing doc] bring doc up to date #11359 (@stas00)\r\n- Remove boiler plate code #11340 (@patrickvonplaten)\r\n- Move old TF text classification script to legacy #11361 (@Rocketknight1)\r\n- [contributing doc] explain/link to good first issue #11346 (@stas00)\r\n- Fix token_type_ids error for big_bird model. #11355 (@wlhgtc)\r\n- [Wav2Vec2] Fix special tokens for Wav2Vec2 tokenizer #11349 (@patrickvonplaten)\r\n- [Flax] Correct typo #11374 (@patrickvonplaten)\r\n- [run_translation.py] fix typo #11372 (@johnson7788)\r\n- Add space #11373 (@tma15)\r\n- Correctly cast num_train_epochs to int #11379 (@Rocketknight1)\r\n- Fix typo #11369 (@penut85420)\r\n- Fix Trainer with remove_unused_columns=False #11382 (@sgugger)\r\n- [Flax] Big FlaxBert Refactor #11364 (@patrickvonplaten)\r\n- [Flax] Typo #11393 (@patrickvonplaten)\r\n- [Flax] Correct Flax <=> PyTorch conversion #11394 (@patrickvonplaten)\r\n- Fix small typo in text #11396 (@maksym-del)\r\n- Fix typos in README for text-classification #11391 (@yoshitomo-matsubara)\r\n- [Blenderbot] Integration Test should be slow #11395 (@patrickvonplaten)\r\n- Fixed trainer total_flos relaoding in distributed mode #11383 (@TevenLeScao)\r\n- [Wav2Vec2] Correct conversion script #11400 (@patrickvonplaten)\r\n- added support for exporting of T5 models to onnx with past_key_values. #10651 (@Ki6an)\r\n- Fixing bug in generation #11297 (@nicola-decao)\r\n- Fix cross-attention head mask for Torch encoder-decoder models #10605 (@stancld)\r\n- Default to accuracy metric in run_glue_no_trainer #11405 (@sgugger)\r\n- Enable option for subword regularization in `XLMRobertaTokenizer` #11149 (@PhilipMay)\r\n- wrong parentclass in documentation #11410 (@cronoik)\r\n- EncoderDecoderConfigs should not create new objects #11300 (@cronoik)\r\n- Updating checkpoint for GPT2ForSequenceClassification #11334 #11434 (@abiolaTresor)\r\n- [BigBird] enable BigBirdForQuestionAnswering to return pooler output #11439 (@vasudevgupta7)\r\n- Upgrade Black to version 21.4b0 #11442 (@patrickvonplaten)\r\n- TF BART models - Add `cross_attentions` to model output and fix cross-attention head masking #10699 (@stancld)\r\n- Add basic support for FP16 in SageMaker model parallelism #11407 (@sgugger)\r\n- Fix link to the TPU launcher script in the pytorch examples #11427 (@amineabdaoui)\r\n- Typo fixes #11432 (@LSinev)\r\n- Pass along seed to DistributedSampler #11406 (@sgugger)\r\n- Clarify description of the is_split_into_words argument #11449 (@kstathou)\r\n- [docs] fix invalid class name #11438 (@stas00)\r\n- [Makefile] make sure to test against the local checkout #11437 (@stas00)\r\n- Give each hub test a different repo name #11453 (@sgugger)\r\n- [Examples] Fixes inconsistency around eval vs val and predict vs test #11380 (@bhadreshpsavani)\r\n- Variable Correction for Consistency in Distillation Example #11444 (@jaimeenahn)\r\n- Remove max length beam scorer #11378 (@GeetDsa)\r\n- update QuickTour docs to reflect model output object #11462 (@hamelsmu)\r\n- Finish Making Quick Tour respect the model object #11467 (@hamelsmu)\r\n- fix docs for decoder_input_ids #11466 (@patil-suraj)\r\n- Update min versions in README and add Flax #11472 (@sgugger)\r\n- Update `PreTrainedTokenizerBase` to check/handle batch length for `text_pair` parameter #11486 (@hamelsmu)\r\n- [Docs] remove paragraph on CI from installation instructions #11493 (@hamelsmu)\r\n- [Flax] Add docstrings & model outputs #11498 (@patrickvonplaten)\r\n- Reformat to make code clearer in tokenizer call #11497 (@sgugger)\r\n- solved coefficient issue for the TF version of gelu_fast #11514 (@michaelbenayoun)\r\n- Split checkpoint from model_name_or_path in examples #11492 (@sgugger)\r\n- Pin HuggingFace Hub dependency #11502 (@LysandreJik)\r\n- correct incorrect dimension comment in Longformer model #11494 (@fredo838)\r\n- Fix `sp_model_kwargs` param missing at unpickle in `XLMRobertaTokenizer` #11430 (@PhilipMay)\r\n- [Master] Make style #11520 (@patrickvonplaten)\r\n- Update README.md #11489 (@mrm8488)\r\n- T5 Gradient Checkpointing #11353 (@ceshine)\r\n- Implement Fast Tokenization for Deberta #11387 (@ShubhamSanghvi)\r\n- Accepts BatchEncoding in LengthGroupedSampler #11431 (@tma15)\r\n- Fix do_eval default value in training_args.py #11511 (@bonniehyeon)\r\n- [examples, translation/summerization] resize token embeds #11524 (@patil-suraj)\r\n- Run model templates on master #11527 (@LysandreJik)\r\n- [Examples] Added support for test-file in QA examples with no trainer #11510 (@bhadreshpsavani)\r\n- Add Stas and Suraj as authors #11526 (@sgugger)\r\n- Improve task summary docs #11513 (@hamelsmu)\r\n- [debug utils] activation/weights underflow/overflow detector #11274 (@stas00)\r\n- [DeepSpeed] fp32 support #11499 (@stas00)\r\n- Fix examples in M2M100 docstrings #11540 (@lewtun)\r\n- [Flax BERT/Roberta] few small fixes #11558 (@patil-suraj)\r\n- [Wav2Vec2] Fix convert #11562 (@patrickvonplaten)\r\n- Remove `datasets` submodule. #11563 (@LysandreJik)\r\n- fix the mlm longformer example by changing [MASK] to <mask> #11559 (@fredo838)\r\n- [Wav2vec2] Fixed tokenization mistakes while adding single-char tokens to tokenizer #11538 (@Muktan)\r\n- Fix metric computation in `run_glue_no_trainer` #11569 (@sgugger)\r\n- Fixes a useless warning in `generate`. #11566 (@Narsil)\r\n- Fix checkpointing in SageMaker MP #11481 (@sgugger)\r\n- Update training tutorial #11533 (@sgugger)\r\n- [Deepspeed] fix resize_token_embeddings #11572 (@stas00)\r\n- Add multi-class, multi-label and regression to transformers #11012 (@abhi1thakur)\r\n- add importlib_metadata as dependency as it is required for py<3.8 #11490 (@cdeepali)\r\n- Enable added tokens #11325 (@LysandreJik)\r\n- Make quality scripts work when one backend is missing. #11573 (@sgugger)\r\n- Removes SageMakerTrainer code but keeps class as wrapper #11587 (@philschmid)\r\n- Reproducible checkpoint #11582 (@sgugger)\r\n- [trainer] document resume randomness #11588 (@stas00)\r\n- [template runner CI] copies need to be fixed too #11585 (@stas00)\r\n- add importlib_metadata and huggingface_hub as dependency in the conda recipe #11591 (@cdeepali)\r\n- Pytorch - Lazy initialization of models #11471 (@patrickvonplaten)\r\n- fix head_mask for albert encoder part(`AlbertTransformer`) #11596 (@baeseongsu)\r\n- Fix Python version #11607 (@LysandreJik)\r\n- fix typo in command #11605 (@vipulraheja)\r\n- Fix typo in docstring #11611 (@eldarkurtic)\r\n- Re-styling in seq2seq attention #11613 (@sgugger)\r\n- [Lazy init] Fix edge cases #11615 (@patrickvonplaten)\r\n- [cuda ext tests] fixing tests #11619 (@stas00)\r\n- Fix RNG saves in distributed mode. #11620 (@sgugger)\r\n- Fix comment in run_clm_no_trainer.py #11624 (@cccntu)\r\n- make fix copy #11627 (@patrickvonplaten)\r\n- Reduce to 1 worker and set timeout for GPU TF tests #11633 (@LysandreJik)\r\n- [self-push CI] sync with self-scheduled #11637 (@stas00)\r\n- [examples] fix sys.path in conftest.py #11636 (@stas00)\r\n- [Examples] Check key exists in datasets first #11503 (@oToToT)\r\n- [Examples] Fix invalid links after reorg #11650 (@oToToT)\r\n- Update code example #11631 (@NielsRogge)\r\n- Add missing git dependency for RAG example #11634 (@lhoestq)\r\n- updated user permissions based on umask #11119 (@bhavitvyamalik)\r\n- Big Bird Fast Tokenizer implementation #11075 (@tanmaylaud)\r\n- Save scaler state dict when checkpointing #11663 (@sgugger)\r\n- [BigBird Pegasus] Add config to auto tokenizer #11667 (@patrickvonplaten)\r\n- Fixes NoneType exception when topk is larger than one coupled with a small context in the Question-Answering pipeline #11628 (@psorianom)\r\n- Add --text_column to run_summarization_no_trainer #11673 (@cccntu)\r\n- Fix docstring of description about input_ids #11672 (@nxznm)\r\n- Grammar and style edits for the frontpage README #11679 (@Rocketknight1)\r\n- Fix TF Roberta for mixed precision training #11675 (@jplu)\r\n- Test checkpointing #11682 (@sgugger)\r\n- Fix clip docs #11694 (@patil-suraj)\r\n- [Flax] Updates README and fixes bug #11701 (@marcvanzee)\r\n- remove defaults to None if optional #11703 (@PhilipMay)",
        "dateCreated": "2021-05-12T15:03:39Z",
        "datePublished": "2021-05-12T15:07:35Z",
        "html_url": "https://github.com/huggingface/transformers/releases/tag/v4.6.0",
        "name": "v4.6.0: ViT, DeiT, CLIP, LUKE, BigBirdPegasus, MegatronBERT",
        "tag_name": "v4.6.0",
        "tarball_url": "https://api.github.com/repos/huggingface/transformers/tarball/v4.6.0",
        "url": "https://api.github.com/repos/huggingface/transformers/releases/42794395",
        "zipball_url": "https://api.github.com/repos/huggingface/transformers/zipball/v4.6.0"
      },
      {
        "authorType": "User",
        "author_name": "sgugger",
        "body": "- Fix `pipeline` when used with private models (#11123)\r\n- Fix loading an architecture in an other (#11207) ",
        "dateCreated": "2021-04-13T15:18:35Z",
        "datePublished": "2021-04-13T15:25:42Z",
        "html_url": "https://github.com/huggingface/transformers/releases/tag/v4.5.1",
        "name": "v4.5.1: Patch release",
        "tag_name": "v4.5.1",
        "tarball_url": "https://api.github.com/repos/huggingface/transformers/tarball/v4.5.1",
        "url": "https://api.github.com/repos/huggingface/transformers/releases/41362558",
        "zipball_url": "https://api.github.com/repos/huggingface/transformers/zipball/v4.5.1"
      },
      {
        "authorType": "User",
        "author_name": "LysandreJik",
        "body": "# v4.5.0: BigBird, GPT Neo, Examples, Flax support\r\n\r\n## BigBird (@vasudevgupta7)\r\n\r\nSeven new models are released as part of the BigBird implementation: `BigBirdModel`, `BigBirdForPreTraining`, `BigBirdForMaskedLM`, `BigBirdForCausalLM`, `BigBirdForSequenceClassification`, `BigBirdForMultipleChoice`, `BigBirdForQuestionAnswering` in PyTorch.\r\n\r\nBigBird is a sparse-attention based transformer which extends Transformer based models, such as BERT to much longer sequences. In addition to sparse attention, BigBird also applies global attention as well as random attention to the input sequence.\r\n\r\nThe BigBird model was proposed in [Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062) by Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed.\r\n\r\nIt is released with an accompanying blog post: [Understanding BigBird's Block Sparse Attention](https://huggingface.co/blog/big-bird)\r\n\r\nCompatible checkpoints can be found on the Hub: https://huggingface.co/models?filter=big_bird\r\n\r\n- BigBird #10183 (@vasudevgupta7)\r\n- [BigBird] Fix big bird gpu test #10967 (@patrickvonplaten)\r\n- [Notebook] add BigBird trivia qa notebook #10995 (@patrickvonplaten)\r\n- [Docs] Add blog to BigBird docs #10997 (@patrickvonplaten)\r\n\r\n## GPT Neo (@patil-suraj)\r\n\r\nTwo new models are released as part of the GPT Neo implementation: `GPTNeoModel`, `GPTNeoForCausalLM` in PyTorch.\r\n\r\nGPT\u2060-\u2060Neo is the code name for a family of transformer-based language models loosely styled around the GPT architecture. EleutherAI's primary goal is to replicate a GPT\u2060-\u20603 DaVinci-sized model and open-source it to the public. \r\n\r\nThe implementation within Transformers is a GPT2-like causal language model trained on the Pile dataset.\r\n\r\nCompatible checkpoints can be found on the Hub: https://huggingface.co/models?filter=gpt_neo\r\n\r\n- GPT Neo #10848 (@patil-suraj)\r\n- GPT Neo few fixes #10968 (@patil-suraj)\r\n- GPT Neo configuration needs to be set to use GPT2 tokenizer #10992 (@LysandreJik)\r\n- [GPT Neo] fix example in config #10993 (@patil-suraj)\r\n- GPT Neo cleanup #10985 (@patil-suraj )\r\n\r\n## Examples\r\n\r\nFeatures have been added to some examples, and additional examples have been added. \r\n\r\n### Raw training loop examples\r\n\r\nBased on the [`accelerate`](https://github.com/huggingface/accelerate) library, examples completely exposing the training loop are now part of the library. For easy customization if you want to try a new research idea!\r\n\r\n- Expand a bit the presentation of examples #10799 (@sgugger)\r\n- Add `examples/multiple-choice/run_swag_no_trainer.py` #10934 (@stancld)\r\n- Update the example template for a no Trainer option #10865 (@sgugger)\r\n- Add `examples/run_ner_no_trainer.py` #10902 (@stancld)\r\n- Add `examples/language_modeling/run_mlm_no_trainer.py` #11001 (@hemildesai)\r\n- Add `examples/language_modeling/run_clm_no_trainer.py` #11026 (@hemildesai)\r\n\r\n### Standardize examples with Trainer\r\n\r\nThanks to the amazing contributions of @bhadreshpsavani, all examples with Trainer are now standardized and all support the predict stage and will return/save metrics in the same fashion.\r\n\r\n- [Example] Updating Question Answering examples for Predict Stage #10792 (@bhadreshpsavani)\r\n- [Examples] Added predict stage and Updated Example Template #10868 (@bhadreshpsavani)\r\n- [Example] Fixed finename for Saving null_odds in the evaluation stage in QA Examples #10939 (@bhadreshpsavani)\r\n- [trainer] Fixes Typo in Predict Method of Trainer #10861 (@bhadreshpsavani)\r\n\r\n## Trainer & SageMaker Model Parallelism\r\n\r\nThe `Trainer` now supports SageMaker model parallelism out of the box, the old `SageMakerTrainer` is deprecated as a consequence and will be removed in version 5.\r\n\r\n- Merge trainers #10975 (@sgugger)\r\n- added new notebook and merge of trainer #11015 (@philschmid)\r\n\r\n## FLAX\r\n\r\nFLAX support has been widened to support all model heads of the BERT architecture, alongside a general conversion script for checkpoints in PyTorch to be used in FLAX.\r\n\r\nAuto models now have a FLAX implementation.\r\n\r\n- [Flax] Add general conversion script #10809 (@patrickvonplaten)\r\n- [Flax] Add other BERT classes #10977 (@patrickvonplaten)\r\n- Refactor AutoModel classes and add Flax Auto classes #11027 (@sgugger)\r\n\r\n## General improvements and bugfixes\r\n\r\n- Patches the full import failure and adds a test #10750 (@LysandreJik)\r\n- Patches full import failure when sentencepiece is not installed #10752 (@LysandreJik)\r\n- [Deepspeed] Allow HF optimizer and scheduler to be passed to deepspeed #10464 (@cli99)\r\n- Fix ProphetNet Flaky Test #10771 (@patrickvonplaten)\r\n- [doc] [testing] extend the pytest -k section with more examples #10761 (@stas00)\r\n- Wav2Vec2 - fix flaky test #10773 (@patrickvonplaten)\r\n- [DeepSpeed] simplify init #10762 (@stas00)\r\n- [DeepSpeed] improve checkpoint loading code plus tests #10760 (@stas00)\r\n- [trainer] make failure to find a resume checkpoint fatal + tests #10777 (@stas00)\r\n- [Issue template] need to update/extend who to tag #10728 (@stas00)\r\n- [examples] document resuming #10776 (@stas00)\r\n- Check copies blackify #10775 (@sgugger)\r\n- Smmp batch not divisible by microbatches fix #10778 (@mansimane)\r\n- Add support for detecting intel-tensorflow version #10781 (@mfuntowicz)\r\n- wav2vec2: support datasets other than LibriSpeech #10581 (@elgeish)\r\n- add run_common_voice script #10767 (@patil-suraj)\r\n- Fix bug in input check for LengthGroupSampler #10783 (@thominj)\r\n- [file_utils] do not gobble certain kinds of requests.ConnectionError #10235 (@julien-c)\r\n- from_pretrained: check that the pretrained model is for the right model architecture #10586 (@vimarshc)\r\n- [examples/seq2seq/README.md] fix t5 examples #10734 (@stas00)\r\n- Fix distributed evaluation #10795 (@sgugger)\r\n- Add XLSR-Wav2Vec2 Fine-Tuning README.md #10786 (@patrickvonplaten)\r\n- addressing vulnerability report in research project deps #10802 (@stas00)\r\n- fix backend tokenizer args override: key mismatch #10686 (@theo-m)\r\n- [XLSR-Wav2Vec2 Info doc] Add a couple of lines #10806 (@patrickvonplaten)\r\n- Add transformers id to hub requests #10811 (@philschmid)\r\n- wav2vec doc tweaks #10808 (@julien-c)\r\n- Sort init import #10801 (@sgugger)\r\n- [wav2vec sprint doc] add doc for Local machine #10828 (@patil-suraj)\r\n- Add new community notebook - wav2vec2 with GPT #10794 (@voidful)\r\n- [Wav2Vec2] Small improvements for wav2vec2 info script #10829 (@patrickvonplaten)\r\n- [Wav2Vec2] Small tab fix #10846 (@patrickvonplaten)\r\n- Fix: typo in FINE_TUNE_XLSR_WAV2VEC2.md #10849 (@qqhann)\r\n- Bump jinja2 from 2.11.2 to 2.11.3 in /examples/research_projects/lxmert #10818 (@dependabot[bot])\r\n- [vulnerability] in example deps fix #10817 (@stas00)\r\n- Correct AutoConfig call docstrings #10822 (@Sebelino)\r\n- [makefile] autogenerate target #10814 (@stas00)\r\n- Fix on_step_begin and on_step_end Callback Sequencing #10839 (@siddk)\r\n- feat(wandb): logging and configuration improvements #10826 (@borisdayma)\r\n- Modify the Trainer class to handle simultaneous execution of Ray Tune and Weights & Biases #10823 (@ruanchaves)\r\n- Use DataCollatorForSeq2Seq in run_summarization in all cases #10856 (@elsanns)\r\n- [Generate] Add save mode logits processor to remove nans and infs if necessary #10769 (@patrickvonplaten)\r\n- Make convert_to_onnx runable as script again #10857 (@sgugger)\r\n- [trainer] fix nan in full-fp16 label_smoothing eval #10815 (@stas00)\r\n- Fix p_mask cls token masking in question-answering pipeline #10863 (@mmaslankowska-neurosys)\r\n- Amazon SageMaker Documentation #10867 (@philschmid)\r\n- [file_utils] import refactor #10859 (@stas00)\r\n- Fixed confusing order of args in generate() docstring #10862 (@RafaelWO)\r\n- Sm trainer smp init fix #10870 (@philschmid)\r\n- Fix test_trainer_distributed #10875 (@sgugger)\r\n- Add new notebook links in the docs #10876 (@sgugger)\r\n- error type of tokenizer in __init__ definition #10879 (@ZhengZixiang)\r\n- [Community notebooks] Add notebook for fine-tuning Bart with Trainer in two langs #10883 (@elsanns)\r\n- Fix overflowing bad word ids #10889 (@LysandreJik)\r\n- Remove version warning in pretrained BART models #10890 (@sgugger)\r\n- Update Training Arguments Documentation: ignore_skip_data -> ignore_data_skip #10891 (@siddk)\r\n- run_glue_no_trainer: datasets -> raw_datasets #10898 (@jethrokuan)\r\n- updates sagemaker documentation #10899 (@philschmid)\r\n- Fix comment in modeling_t5.py #10886 (@lexhuismans)\r\n- Rename NLP library to Datasets library #10920 (@tomy0000000)\r\n- [vulnerability] fix dependency #10914 (@stas00)\r\n- Add ImageFeatureExtractionMixin #10905 (@sgugger)\r\n- Return global attentions (see #7514) #10906 (@gui11aume)\r\n- Updated colab links in readme of examples #10932 (@WybeKoper)\r\n- Fix initializing BertJapaneseTokenizer with AutoTokenizers #10936 (@singletongue)\r\n- Instantiate model only once in pipeline #10888 (@sgugger)\r\n- Use pre-computed lengths, if available, when grouping by length #10953 (@pcuenca)\r\n- [trainer metrics] fix cpu mem metrics; reformat runtime metric #10937 (@stas00)\r\n- [vulnerability] dep fix #10954 (@stas00)\r\n- Fixes in the templates #10951 (@sgugger)\r\n- Sagemaker test #10925 (@philschmid)\r\n- Fix summarization notebook link #10959 (@philschmid)\r\n- improved sagemaker documentation for git_config and examples #10966 (@philschmid)\r\n- Fixed a bug where the `pipeline.framework` would actually contain a fully qualified model. #10970 (@Narsil)\r\n- added py7zr #10971 (@philschmid)\r\n- fix md file to avoid evaluation crash #10962 (@ydshieh)\r\n- Fixed some typos and removed legacy url #10989 (@WybeKoper)\r\n- Sagemaker test fix #10987 (@philschmid)\r\n- Fix the checkpoint for I-BERT #10994 (@LysandreJik)\r\n- Add more metadata to the user agent #10972 (@sgugger)\r\n- Enforce string-formatting with f-strings #10980 (@sgugger)\r\n- In the group by length documentation length is misspelled as legnth #11000 (@JohnnyC08)\r\n- Fix Adafactor documentation (recommend correct settings) #10526 (@jsrozner)\r\n- Improve the speed of adding tokens from added_tokens.json #10780 (@cchen-dialpad)\r\n- Add Vision Transformer and ViTFeatureExtractor #10950 (@NielsRogge)\r\n- DebertaTokenizer Rework closes #10258 #10703 (@cronoik)\r\n- [doc] no more bucket #10793 (@julien-c)\r\n- Layout lm tf 2 #10636 (@atahmasb)\r\n- fixed typo: logging instead of logger #11025 (@versis)\r\n- Add a script to check inits are consistent #11024 (@sgugger)\r\n- fix incorrect case for s|Pretrained|PreTrained| #11048 (@stas00)\r\n- [doc] fix code-block rendering #11053 (@erensahin)\r\n- Pin docutils #11062 (@LysandreJik)\r\n- Remove unnecessary space #11060 (@LysandreJik)\r\n- Some models have no tokenizers #11064 (@LysandreJik)\r\n- Documentation about loading a fast tokenizer within Transformers #11029 (@LysandreJik)\r\n- Add example for registering callbacks with trainers #10928 (@amalad)\r\n- Replace pkg_resources with importlib_metadata #11061 (@konstin)\r\n- Add center_crop to ImageFeatureExtractionMixin #11066 (@sgugger)\r\n- Document common config attributes #11070 (@sgugger)\r\n- Fix distributed gather for tuples of tensors of varying sizes #11071 (@sgugger)\r\n- Make a base init in FeatureExtractionMixin #11074 (@sgugger)\r\n- Add Readme for language modeling scripts with custom training loop and accelerate #11073 (@hemildesai)\r\n- HF emoji unicode doesn't work in console #11081 (@stas00)\r\n- added social thumbnail for docs #11083 (@philschmid)\r\n- added new merged Trainer test #11090 (@philschmid)",
        "dateCreated": "2021-04-06T16:38:03Z",
        "datePublished": "2021-04-06T16:50:00Z",
        "html_url": "https://github.com/huggingface/transformers/releases/tag/v4.5.0",
        "name": "v4.5.0: BigBird, GPT Neo, Examples, Flax support",
        "tag_name": "v4.5.0",
        "tarball_url": "https://api.github.com/repos/huggingface/transformers/tarball/v4.5.0",
        "url": "https://api.github.com/repos/huggingface/transformers/releases/40995352",
        "zipball_url": "https://api.github.com/repos/huggingface/transformers/zipball/v4.5.0"
      },
      {
        "authorType": "User",
        "author_name": "sgugger",
        "body": "- Add support for detecting intel-tensorflow version\r\n- Fix distributed evaluation on SageMaker with distributed evaluation",
        "dateCreated": "2021-03-18T19:09:21Z",
        "datePublished": "2021-03-18T19:17:38Z",
        "html_url": "https://github.com/huggingface/transformers/releases/tag/v4.4.2",
        "name": "Patch release V4.4.2",
        "tag_name": "v4.4.2",
        "tarball_url": "https://api.github.com/repos/huggingface/transformers/tarball/v4.4.2",
        "url": "https://api.github.com/repos/huggingface/transformers/releases/40026421",
        "zipball_url": "https://api.github.com/repos/huggingface/transformers/zipball/v4.4.2"
      },
      {
        "authorType": "User",
        "author_name": "LysandreJik",
        "body": "# v4.4.0: S2T, M2M100, I-BERT, mBART-50, DeBERTa-v2, XLSR-Wav2Vec2\r\n\r\n## SpeechToText\r\n\r\nTwo new models are released as part of the S2T implementation: `Speech2TextModel` and `Speech2TextForConditionalGeneration`, in PyTorch.\r\n\r\nSpeech2Text is a speech model that accepts a float tensor of log-mel filter-bank features extracted from the speech signal. It\u2019s a transformer-based seq2seq model, so the transcripts/translations are generated autoregressively.\r\n\r\nThe Speech2Text model was proposed in [fairseq S2T: Fast Speech-to-Text Modeling with fairseq](https://arxiv.org/abs/2010.05171) by Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Dmytro Okhonko, Juan Pino. \r\n\r\nCompatible checkpoints can be found on the Hub: https://huggingface.co/models?filter=speech_to_text\r\n\r\n- Speech2TextTransformer #10175 (@patil-suraj)\r\n\r\n## M2M100\r\n\r\nTwo new models are released as part of the M2M100 implementation: `M2M100Model` and `M2M100ForConditionalGeneration`, in PyTorch.\r\n\r\nM2M100 is a multilingual encoder-decoder (seq-to-seq) model primarily intended for translation tasks.\r\n\r\nThe M2M100 model was proposed in [Beyond English-Centric Multilingual Machine Translation](https://arxiv.org/abs/2010.11125) by Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, Armand Joulin.\r\n\r\nCompatible checkpoints can be found on the Hub: https://huggingface.co/models?filter=m2m_100\r\n\r\n- Add m2m100 #10236 (@patil-suraj)\r\n\r\n## I-BERT\r\n\r\nSix new models are released as part of the I-BERT implementation: `IBertModel`, `IBertForMaskedLM`, `IBertForSequenceClassification`, `IBertForMultipleChoice`, `IBertForTokenClassification` and `IBertForQuestionAnswering`, in PyTorch.\r\n\r\nI-BERT is a quantized version of RoBERTa running inference up to four times faster. \r\n\r\nThe I-BERT framework in PyTorch allows to identify the best parameters for quantization. Once the model is exported in a framework that supports int8 execution (such as TensorRT), a speedup of up to 4x is visible, with no loss in performance thanks to the parameter search.\r\n\r\nThe I-BERT model was proposed in [I-BERT: Integer-only BERT Quantization](https://arxiv.org/abs/2101.01321) by Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W. Mahoney and Kurt Keutzer.\r\n\r\nCompatible checkpoints can be found on the Hub: https://huggingface.co/models?filter=ibert\r\n\r\n- I-BERT model support #10153 (@kssteven418)\r\n- [IBert] Correct link to paper #10445 (@patrickvonplaten)\r\n- Add I-BERT to README #10462 (@LysandreJik)\r\n\r\nCompatible checkpoints can be found on the Hub: https://huggingface.co/models?filter=speech_to_text\r\n\r\n## mBART-50\r\n\r\nMBart-50 is created using the original mbart-large-cc25 checkpoint by extending its embedding layers with randomly initialized vectors for an extra set of 25 language tokens and then pretrained on 50 languages. \r\n\r\nThe MBart model was presented in [Multilingual Translation with Extensible Multilingual Pretraining and Finetuning](https://arxiv.org/abs/2008.00401) by Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, Angela Fan.\r\n\r\nCompatible checkpoints can be found on the Hub: https://huggingface.co/models?filter=mbart-50\r\n\r\n- Add mBART-50 #10154 (@patil-suraj)\r\n\r\n## DeBERTa-v2\r\n\r\nFixe new models are released as part of the DeBERTa-v2 implementation: `DebertaV2Model`, `DebertaV2ForMaskedLM`, `DebertaV2ForSequenceClassification`, `DeberaV2ForTokenClassification` and `DebertaV2ForQuestionAnswering`, in PyTorch.\r\n\r\nThe DeBERTa model was proposed in [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen. It is based on Google\u2019s BERT model released in 2018 and Facebook\u2019s RoBERTa model released in 2019.\r\n\r\nIt builds on RoBERTa with disentangled attention and enhanced mask decoder training with half of the data used in RoBERTa.\r\n\r\nCompatible checkpoints can be found on the Hub: https://huggingface.co/models?filter=deberta-v2\r\n\r\n- Integrate DeBERTa v2(the 1.5B model surpassed human performance on Su\u2026 #10018 (@BigBird01)\r\n- DeBERTa-v2 fixes #10328 (@LysandreJik)\r\n\r\n## Wav2Vec2\r\n\r\n### XLSR-Wav2Vec2\r\n\r\nThe XLSR-Wav2Vec2 model was proposed in [Unsupervised Cross-Lingual Representation Learning For Speech Recognition](https://arxiv.org/abs/2006.13979) by Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, Michael Auli.\r\n\r\nThe checkpoint corresponding to that model is added to the model hub: [facebook/\r\nwav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53)\r\n\r\n- [XLSR-Wav2Vec2] Add multi-lingual Wav2Vec2 models #10648 (@patrickvonplaten)\r\n\r\n### Training script\r\n\r\nA fine-tuning script showcasing how the Wav2Vec2 model can be trained has been added.\r\n\r\n- Add Fine-Tuning for Wav2Vec2 #10145 (@patrickvonplaten)\r\n\r\n### Further improvements\r\n\r\nThe Wav2Vec2 architecture becomes more stable as several changes are done to its architecture. This introduces feature extractors and feature processors as the pre-processing aspect of multi-modal speech models.\r\n\r\n- Deprecate Wav2Vec2ForMaskedLM and add Wav2Vec2ForCTC #10089 (@patrickvonplaten)\r\n- Fix example in Wav2Vec2 documentation #10096 (@abhishekkrthakur)\r\n- [Wav2Vec2] Remove unused config #10457 (@patrickvonplaten)\r\n- [Wav2Vec2FeatureExtractor] smal fixes #10455 (@patil-suraj)\r\n- [Wav2Vec2] Improve Tokenizer & Model for batched inference #10117 (@patrickvonplaten)\r\n- [PretrainedFeatureExtractor] + Wav2Vec2FeatureExtractor, Wav2Vec2Processor, Wav2Vec2Tokenizer #10324 (@patrickvonplaten)\r\n- [Wav2Vec2 Example Script] Typo #10547 (@patrickvonplaten)\r\n- [Wav2Vec2] Make wav2vec2 test deterministic #10714 (@patrickvonplaten)\r\n- [Wav2Vec2] Fix documentation inaccuracy #10694 (@MikeG112)\r\n\r\n## AMP & XLA Support for TensorFlow models\r\n\r\nMost of the TensorFlow models are now compatible with automatic mixed precision and have XLA support.\r\n\r\n- Add AMP for TF Albert #10141 (@jplu)\r\n- Unlock XLA test for TF ConvBert #10207 (@jplu)\r\n- Making TF BART-like models XLA and AMP compliant #10191 (@jplu)\r\n- Making TF XLM-like models XLA and AMP compliant #10211 (@jplu)\r\n- Make TF CTRL compliant with XLA and AMP #10209 (@jplu)\r\n- Making TF GPT2 compliant with XLA and AMP #10230 (@jplu)\r\n- Making TF Funnel compliant with AMP #10216 (@jplu)\r\n- Making TF Lxmert model compliant with AMP #10257 (@jplu)\r\n- Making TF MobileBert model compliant with AMP #10259 (@jplu)\r\n- Making TF MPNet model compliant with XLA #10260 (@jplu)\r\n- Making TF T5 model compliant with AMP and XLA #10262 (@jplu)\r\n- Making TF TransfoXL model compliant with AMP #10264 (@jplu)\r\n- Making TF OpenAI GPT model compliant with AMP and XLA #10261 (@jplu)\r\n- Rework the AMP for TF XLNet #10274 (@jplu)\r\n- Making TF Longformer-like models compliant with AMP #10233 (@jplu)\r\n\r\n## SageMaker Trainer for model parallelism\r\n\r\nWe are rolling out experimental support for model parallelism on SageMaker with a new `SageMakerTrainer` that can be used in place of the regular `Trainer`. This is a temporary class that will be removed in a future version, the end goal is to have `Trainer` support this feature out of the box.\r\n\r\n- Add SageMakerTrainer for model paralellism #10122 (@sgugger)\r\n- Extend trainer logging for sm #10633 (@philschmid)\r\n- Sagemaker Model Parallel tensoboard writing fix #10403 (@mansimane)\r\n- Multiple fixes in SageMakerTrainer #10687 (@sgugger)\r\n- Add DistributedSamplerWithLoop #10746 (@sgugger)\r\n\r\n## General improvements and bugfixes\r\n\r\n- [trainer] deepspeed bug fixes and tests #10039 (@stas00)\r\n- Removing run_pl_glue.py from text classification docs, include run_xnli.py & run_tf_text_classification.py #10066 (@cbjuan)\r\n- remove token_type_ids from TokenizerBertGeneration output #10070 (@sadakmed)\r\n- [deepspeed tests] transition to new tests dir #10080 (@stas00)\r\n- Added integration tests for Pytorch implementation of the ELECTRA model #10073 (@spatil6)\r\n- Fix naming in TF MobileBERT #10095 (@jplu)\r\n- [examples/s2s] add test set predictions #10085 (@patil-suraj)\r\n- Logging propagation #10092 (@LysandreJik)\r\n- Fix some edge cases in report_to and add deprecation warnings #10100 (@sgugger)\r\n- Add head_mask and decoder_head_mask to TF LED #9988 (@stancld)\r\n- Replace strided slice with tf.expand_dims #10078 (@jplu)\r\n- Fix Faiss Import #10103 (@patrickvonplaten)\r\n- [RAG] fix generate #10094 (@patil-suraj)\r\n- Fix TFConvBertModelIntegrationTest::test_inference_masked_lm Test #10104 (@abhishekkrthakur)\r\n- doc: update W&B related doc #10086 (@borisdayma)\r\n- Remove speed metrics from default compute objective [WIP] #10107 (@shiva-z)\r\n- Fix tokenizers training in notebooks #10110 (@n1t0)\r\n- [DeepSpeed docs] new information #9610 (@stas00)\r\n- [CI] build docs faster #10115 (@stas00)\r\n- [scheduled github CI] add deepspeed fairscale deps #10116 (@stas00)\r\n- Line endings should be LF across repo and not CRLF #10119 (@LysandreJik)\r\n- Fix TF LED/Longformer attentions computation #10007 (@jplu)\r\n- remove adjust_logits_during_generation method #10087 (@patil-suraj)\r\n- [DeepSpeed] restore memory for evaluation #10114 (@stas00)\r\n- Update run_xnli.py to use Datasets library #9829 (@Qbiwan)\r\n- Add new community notebook - Blenderbot #10126 (@lordtt13)\r\n- [DeepSpeed in notebooks] Jupyter + Colab #10130 (@stas00)\r\n- [examples/run_s2s] remove task_specific_params and update rouge computation #10133 (@patil-suraj)\r\n- Fix typo in GPT2DoubleHeadsModel docs #10148 (@M-Salti)\r\n- [hf_api] delete deprecated methods and tests #10159 (@julien-c)\r\n- Revert propagation #10171 (@LysandreJik)\r\n- Conversion from slow to fast for BPE spm vocabs contained an error. #10120 (@Narsil)\r\n- Fix typo in comments #10157 (@mrm8488)\r\n- Fix typo in comment #10156 (@mrm8488)\r\n- [Doc] Fix version control in internal pages #10124 (@sgugger)\r\n- [t5 tokenizer] add info logs #9897 (@stas00)\r\n- Fix v2 model loading issue #10129 (@BigBird01)\r\n- Fix datasets set_format #10178 (@sgugger)\r\n- Fixing NER pipeline for list inputs. #10184 (@Narsil)\r\n- Add new model to labels that should not stale #10187 (@LysandreJik)\r\n- Check TF ops for ONNX compliance #10025 (@jplu)\r\n- [RAG] fix tokenizer #10167 (@patil-suraj)\r\n- Fix TF template #10189 (@jplu)\r\n- fix run_seq2seq.py; porting trainer tests to it #10162 (@stas00)\r\n- Specify dataset dtype #10195 (@LysandreJik)\r\n- [CI] make the examples sub-group of tests run always #10196 (@stas00)\r\n- [WIP][examples/seq2seq] move old s2s scripts to legacy #10136 (@patil-suraj)\r\n- set tgt_lang of MBart Tokenizer for summarization #10205 (@HeroadZ)\r\n- Store FLOS as floats to avoid overflow. #10213 (@sgugger)\r\n- Fix add_token_positions in custom datasets tutorial #10217 (@joeddav)\r\n- [trainer] fix ignored columns logger #10219 (@stas00)\r\n- Factor out methods #10215 (@LysandreJik)\r\n- Fix head masking for TFT5 models #9877 (@stancld)\r\n- [CI] 2 fixes #10248 (@stas00)\r\n- [trainer] refactor place_model_on_device logic, add deepspeed #10243 (@stas00)\r\n- [Trainer] doc update #10241 (@stas00)\r\n- Reduce the time spent for the TF slow tests #10152 (@jplu)\r\n- Introduce warmup_ratio training argument #10229 (@tanmay17061)\r\n- [Trainer] memory tracker metrics #10225 (@stas00)\r\n- Script for distilling zero-shot classifier to more efficient student #10244 (@joeddav)\r\n- [test] fix func signature #10271 (@stas00)\r\n- [trainer] implement support for full fp16 in evaluation/predict #10268 (@stas00)\r\n- [ISSUES.md] propose using google colab to reproduce problems #10270 (@stas00)\r\n- Introduce logging_strategy training argument #10267 (@tanmay17061)\r\n- [CI] Kill any run-away pytest processes #10281 (@stas00)\r\n- Patch zero shot distillation script cuda issue #10284 (@joeddav)\r\n- Move the TF NER example #10276 (@jplu)\r\n- Fix example links in the task summary #10291 (@sgugger)\r\n- fixes #10303 #10304 (@cronoik)\r\n- [ci] don't fail when there are no zombies #10308 (@stas00)\r\n- fix typo in conversion script #10316 (@tagucci)\r\n- Add note to resize token embeddings matrix when adding new tokens to voc #10331 (@LysandreJik)\r\n- Deprecate prepare_seq2seq_batch #10287 (@sgugger)\r\n- [examples/seq2seq] defensive programming + expand/correct README #10295 (@stas00)\r\n- [Trainer] implement gradient_accumulation_steps support in DeepSpeed integration #10310 (@stas00)\r\n- Loading from last checkpoint functionality in Trainer.train #10334 (@tanmay17061)\r\n- [trainer] add Trainer methods for metrics logging and saving #10266 (@stas00)\r\n- Fix evaluation with label smoothing in Trainer #10338 (@sgugger)\r\n- Fix broken examples/seq2seq/README.md markdown #10344 (@Wikidepia)\r\n- [bert-base-german-cased] use model repo, not external bucket #10353 (@julien-c)\r\n- [Trainer/Deepspeed] handle get_last_lr() before first step() #10362 (@stas00)\r\n- ConvBERT fix torch <> tf weights conversion #10314 (@abhishekkrthakur)\r\n\r\n- fix deprecated reference `tokenizer.max_len` in glue.py #10220 (@poedator)\r\n- [trainer] move secondary methods into a separate file #10363 (@stas00)\r\n- Run GA on every push even on forks #10383 (@LysandreJik)\r\n- GA: only run model templates once #10388 (@LysandreJik)\r\n- Bugfix: Removal of padding_idx in BartLearnedPositionalEmbedding #10200 (@mingruimingrui)\r\n- Remove unused variable in example for Q&A #10392 (@abhishekkrthakur)\r\n- Ignore unexpected weights from PT conversion #10397 (@LysandreJik)\r\n- Add support for ZeRO-2/3 and ZeRO-offload in fairscale #10354 (@sgugger)\r\n- Fix None in add_token_positions - issue #10210 #10374 (@andreabac3)\r\n- Make Barthez tokenizer tests a bit faster #10399 (@sgugger)\r\n- Fix run_glue evaluation when model has a label correspondence #10401 (@sgugger)\r\n- [ci, flax] non-existing models are unlikely to pass tests #10409 (@julien-c)\r\n\r\n- [LED] Correct Docs #10419 (@patrickvonplaten)\r\n- Add Ray Tune hyperparameter search integration test #10414 (@krfricke)\r\n- Ray Tune Integration Bug Fixes #10406 (@amogkam)\r\n- [examples] better model example #10427 (@stas00)\r\n- Fix conda-build #10431 (@LysandreJik)\r\n- [run_seq2seq.py] restore functionality: saving to test_generations.txt #10428 (@stas00)\r\n- updated logging and saving metrics #10436 (@bhadreshpsavani)\r\n- Introduce save_strategy training argument #10286 (@tanmay17061)\r\n- Adds terms to Glossary #10443 (@darigovresearch)\r\n- Fixes compatibility bug when using grouped beam search and constrained decoding together #10475 (@mnschmit)\r\n- Generate can return cross-attention weights too #10493 (@Mehrad0711)\r\n- Fix typos #10489 (@WybeKoper)\r\n- [T5] Fix speed degradation bug t5 #10496 (@patrickvonplaten)\r\n- feat(docs): navigate with left/right arrow keys #10481 (@ydcjeff)\r\n- Refactor checkpoint name in BERT and MobileBERT #10424 (@sgugger)\r\n- remap MODEL_FOR_QUESTION_ANSWERING_MAPPING classes to names auto-generated file #10487 (@stas00)\r\n- Fix the bug in constructing the all_hidden_states of DeBERTa v2 #10466 (@felixgwu)\r\n- Smp grad accum #10488 (@sgugger)\r\n- Remove unsupported methods from ModelOutput doc #10505 (@sgugger)\r\n- Not always consider a local model a checkpoint in run_glue #10517 (@sgugger)\r\n- Removes overwrites for output_dir #10521 (@philschmid)\r\n- Rework TPU checkpointing in Trainer #10504 (@sgugger)\r\n- [ProphetNet] Bart-like Refactor #10501 (@patrickvonplaten)\r\n- Fix example of custom Trainer to reflect signature of compute_loss #10537 (@lewtun)\r\n- Fixing conversation test for torch 1.8 #10545 (@Narsil)\r\n- Fix torch 1.8.0 segmentation fault #10546 (@LysandreJik)\r\n- Fixed dead link in Trainer documentation #10554 (@jwa018)\r\n- Typo correction. #10531 (@cliang1453)\r\n- Fix embeddings for PyTorch 1.8 #10549 (@sgugger)\r\n- Stale Bot #10509 (@LysandreJik)\r\n- Refactoring checkpoint names for multiple models #10527 (@danielpatrickhug)\r\n- offline mode for firewalled envs #10407 (@stas00)\r\n- fix tf doc bug #10570 (@Sniper970119)\r\n- [run_seq2seq] fix nltk lookup #10585 (@stas00)\r\n- Fix typo in docstring for pipeline #10591 (@silvershine157)\r\n- wrong model used for BART Summarization example #10582 (@orena1)\r\n- [M2M100] fix positional embeddings #10590 (@patil-suraj)\r\n- Enable torch 1.8.0 on GPU CI #10593 (@LysandreJik)\r\n- tokenization_marian.py: use current_spm for decoding #10357 (@Mehrad0711)\r\n- [trainer] fix double wrapping + test #10583 (@stas00)\r\n- Fix version control with anchors #10595 (@sgugger)\r\n- offline mode for firewalled envs (part 2) #10569 (@stas00)\r\n- [examples tests] various fixes #10584 (@stas00)\r\n- Added max_sample_ arguments #10551 (@bhadreshpsavani)\r\n- [examples tests on multigpu] resolving require_torch_non_multi_gpu_but_fix_me #10561 (@stas00)\r\n- Check layer types for Optimizer construction #10598 (@sgugger)\r\n- Speedup tf tests #10601 (@LysandreJik)\r\n- [docs] How to solve \"Title level inconsistent\" sphinx error #10600 (@stas00)\r\n- [FeatureExtractorSavingUtils] Refactor PretrainedFeatureExtractor #10594 (@patrickvonplaten)\r\n- fix flaky m2m100 test #10604 (@patil-suraj)\r\n- [examples template] added max_sample args and metrics changes #10602 (@bhadreshpsavani)\r\n- Fairscale FSDP fix model save #10596 (@sgugger)\r\n- Fix tests of TrainerCallback #10615 (@sgugger)\r\n- Fixes an issue in `text-classification` where MNLI eval/test datasets are not being preprocessed. #10621 (@allenwang28)\r\n- [M2M100] remove final_logits_bias #10606 (@patil-suraj)\r\n- Add new GLUE example with no Trainer. #10555 (@sgugger)\r\n- Copy tokenizer files in each of their repo #10624 (@sgugger)\r\n- Document Trainer limitation on custom models #10635 (@sgugger)\r\n- Fix Longformer tokenizer filename #10653 (@LysandreJik)\r\n- Update README.md #10647 (@Arvid-pku)\r\n- Ensure metric results are JSON-serializable #10632 (@sgugger)\r\n- S2S + M2M100 should be available in tokenization_auto #10657 (@LysandreJik)\r\n- Remove special treatment for custom vocab files #10637 (@sgugger)\r\n- [S2T] fix example in docs #10667 (@patil-suraj)\r\n- W2v2 test require torch #10665 (@LysandreJik)\r\n- Fix Marian/TFMarian tokenization tests #10661 (@LysandreJik)\r\n- Fixes Pegasus tokenization tests #10671 (@LysandreJik)\r\n- Onnx fix test #10663 (@mfuntowicz)\r\n- Fix integration slow tests #10670 (@sgugger)\r\n- Specify minimum version for sacrebleu #10662 (@LysandreJik)\r\n- Add DeBERTa to MODEL_FOR_PRETRAINING_MAPPING #10668 (@jeswan)\r\n- Fix broken link #10656 (@WybeKoper)\r\n- fix typing error for HfArgumentParser for Optional[bool] #10672 (@bfineran)\r\n- MT5 integration test: adjust loss difference #10669 (@LysandreJik)\r\n- Adding new parameter to `generate`: `max_time`. #9846 (@Narsil)\r\n- TensorFlow tests: having from_pt set to True requires torch to be installed. #10664 (@LysandreJik)\r\n- Add auto_wrap option in fairscale integration #10673 (@sgugger)\r\n- fix: #10628 expanduser path in TrainingArguments #10660 (@PaulLerner)\r\n- Pass encoder outputs into GenerationMixin #10599 (@ymfa)\r\n- [wip] [deepspeed] AdamW is now supported by default #9624 (@stas00)\r\n- [Tests] RAG #10679 (@patrickvonplaten)\r\n- enable loading Mbart50Tokenizer with AutoTokenizer #10690 (@patil-suraj)\r\n- Wrong link to super class #10709 (@cronoik)\r\n- Distributed barrier before loading model #10685 (@sgugger)\r\n- GPT2DoubleHeadsModel made parallelizable #10658 (@ishalyminov)\r\n- split seq2seq script into summarization & translation #10611 (@theo-m)\r\n- Adding required flags to non-default arguments in hf_argparser #10688 (@Craigacp)\r\n- Fix backward compatibility with EvaluationStrategy #10718 (@sgugger)\r\n- Tests run on Docker #10681 (@LysandreJik)\r\n- Rename zero-shot pipeline multi_class argument #10727 (@joeddav)\r\n- Add minimum version check in examples #10724 (@sgugger)\r\n- independent training / eval with local files #10710 (@riklopfer)\r\n- Flax testing should not run the full torch test suite #10725 (@patrickvonplaten)",
        "dateCreated": "2021-03-16T15:33:48Z",
        "datePublished": "2021-03-16T15:39:49Z",
        "html_url": "https://github.com/huggingface/transformers/releases/tag/v4.4.0",
        "name": "v4.4.0: S2T, M2M100, I-BERT, mBART-50, DeBERTa-v2, XLSR-Wav2Vec2",
        "tag_name": "v4.4.0",
        "tarball_url": "https://api.github.com/repos/huggingface/transformers/tarball/v4.4.0",
        "url": "https://api.github.com/repos/huggingface/transformers/releases/39875203",
        "zipball_url": "https://api.github.com/repos/huggingface/transformers/zipball/v4.4.0"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 55799,
      "date": "Mon, 20 Dec 2021 10:37:42 GMT"
    },
    "technique": "GitHub API"
  },
  "support": [
    {
      "confidence": [
        1
      ],
      "excerpt": "<a target=\"_blank\" href=\"https://huggingface.co/support\">\n    <img alt=\"HuggingFace Expert Acceleration Program\" src=\"https://huggingface.co/front/thumbnails/support.png\" style=\"max-width: 600px; border: 1px solid #eee; border-radius: 4px; box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.05);\">\n</a><br>\n\n",
      "technique": "Header extraction"
    }
  ],
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "nlp",
      "natural-language-processing",
      "natural-language-understanding",
      "pytorch",
      "language-model",
      "natural-language-generation",
      "tensorflow",
      "bert",
      "language-models",
      "pytorch-transformers",
      "nlp-library",
      "transformer",
      "model-hub",
      "pretrained-models",
      "jax",
      "flax",
      "seq2seq",
      "speech-recognition",
      "hacktoberfest",
      "python"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You can test most of our models directly on their pages from the [model hub](https://huggingface.co/models). We also offer [private model hosting, versioning, & an inference API](https://huggingface.co/pricing) for public and private models.\n\nHere are a few examples:\n\n In Natural Language Processing:\n- [Masked word completion with BERT](https://huggingface.co/bert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+France)\n- [Name Entity Recognition with Electra](https://huggingface.co/dbmdz/electra-large-discriminator-finetuned-conll03-english?text=My+name+is+Sarah+and+I+live+in+London+city)\n- [Text generation with GPT-2](https://huggingface.co/gpt2?text=A+long+time+ago%2C+)\n- [Natural Language Inference with RoBERTa](https://huggingface.co/roberta-large-mnli?text=The+dog+was+lost.+Nobody+lost+any+animal)\n- [Summarization with BART](https://huggingface.co/facebook/bart-large-cnn?text=The+tower+is+324+metres+%281%2C063+ft%29+tall%2C+about+the+same+height+as+an+81-storey+building%2C+and+the+tallest+structure+in+Paris.+Its+base+is+square%2C+measuring+125+metres+%28410+ft%29+on+each+side.+During+its+construction%2C+the+Eiffel+Tower+surpassed+the+Washington+Monument+to+become+the+tallest+man-made+structure+in+the+world%2C+a+title+it+held+for+41+years+until+the+Chrysler+Building+in+New+York+City+was+finished+in+1930.+It+was+the+first+structure+to+reach+a+height+of+300+metres.+Due+to+the+addition+of+a+broadcasting+aerial+at+the+top+of+the+tower+in+1957%2C+it+is+now+taller+than+the+Chrysler+Building+by+5.2+metres+%2817+ft%29.+Excluding+transmitters%2C+the+Eiffel+Tower+is+the+second+tallest+free-standing+structure+in+France+after+the+Millau+Viaduct)\n- [Question answering with DistilBERT](https://huggingface.co/distilbert-base-uncased-distilled-squad?text=Which+name+is+also+used+to+describe+the+Amazon+rainforest+in+English%3F&context=The+Amazon+rainforest+%28Portuguese%3A+Floresta+Amaz%C3%B4nica+or+Amaz%C3%B4nia%3B+Spanish%3A+Selva+Amaz%C3%B3nica%2C+Amazon%C3%ADa+or+usually+Amazonia%3B+French%3A+For%C3%AAt+amazonienne%3B+Dutch%3A+Amazoneregenwoud%29%2C+also+known+in+English+as+Amazonia+or+the+Amazon+Jungle%2C+is+a+moist+broadleaf+forest+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C000%2C000+square+kilometres+%282%2C700%2C000+sq+mi%29%2C+of+which+5%2C500%2C000+square+kilometres+%282%2C100%2C000+sq+mi%29+are+covered+by+the+rainforest.+This+region+includes+territory+belonging+to+nine+nations.+The+majority+of+the+forest+is+contained+within+Brazil%2C+with+60%25+of+the+rainforest%2C+followed+by+Peru+with+13%25%2C+Colombia+with+10%25%2C+and+with+minor+amounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States+or+departments+in+four+nations+contain+%22Amazonas%22+in+their+names.+The+Amazon+represents+over+half+of+the+planet%27s+remaining+rainforests%2C+and+comprises+the+largest+and+most+biodiverse+tract+of+tropical+rainforest+in+the+world%2C+with+an+estimated+390+billion+individual+trees+divided+into+16%2C000+species)\n- [Translation with T5](https://huggingface.co/t5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)\n\nIn Computer Vision:\n- [Image classification with ViT](https://huggingface.co/google/vit-base-patch16-224)\n- [Object Detection with DETR](https://huggingface.co/facebook/detr-resnet-50)\n- [Image Segmentation with DETR](https://huggingface.co/facebook/detr-resnet-50-panoptic)\n\nIn Audio:\n- [Automatic Speech Recognition with Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base-960h)\n- [Keyword Spotting with Wav2Vec2](https://huggingface.co/superb/wav2vec2-base-superb-ks)\n\n**[Write With Transformer](https://transformer.huggingface.co)**, built by the Hugging Face team, is the official demo of this repo\u2019s text generation capabilities.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Easy-to-use state-of-the-art models:\n    - High performance on natural language understanding & generation, computer vision, and audio tasks.\n    - Low barrier to entry for educators and practitioners.\n    - Few user-facing abstractions with just three classes to learn.\n    - A unified API for using all our pretrained models.\n\n1. Lower compute costs, smaller carbon footprint:\n    - Researchers can share trained models instead of always retraining.\n    - Practitioners can reduce compute time and production costs.\n    - Dozens of architectures with over 20,000 pretrained models, some in more than 100 languages.\n\n1. Choose the right framework for every part of a model's lifetime:\n    - Train state-of-the-art models in 3 lines of code.\n    - Move a single model between TF2.0/PyTorch/JAX frameworks at will.\n    - Seamlessly pick the right framework for training, evaluation and production.\n\n1. Easily customize a model or an example to your needs:\n    - We provide examples for each architecture to reproduce the results published by its original authors.\n    - Model internals are exposed as consistently as possible.\n    - Model files can be used independently of the library for quick experiments.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "- This library is not a modular toolbox of building blocks for neural nets. The code in the model files is not refactored with additional abstractions on purpose, so that researchers can quickly iterate on each of the models without diving into additional abstractions/files.\n- The training API is not intended to work on any model but is optimized to work with the models provided by the library. For generic machine learning loops, you should use another library.\n- While we strive to present as many use cases as possible, the scripts in our [examples folder](https://github.com/huggingface/transformers/tree/master/examples) are just that: examples. It is expected that they won't work out-of-the box on your specific problem and that you will be required to change a few lines of code to adapt them to your needs.\n\n",
      "technique": "Header extraction"
    }
  ]
}