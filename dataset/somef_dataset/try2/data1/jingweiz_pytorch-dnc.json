{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1410.5401",
      "https://arxiv.org/abs/1706.09520",
      "https://arxiv.org/abs/1706.09520"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find this library useful and would like to cite it, the following would be appropriate:\n```\n@misc{pytorch-dnc,\n  author = {Zhang, Jingwei},\n  title = {jingweiz/pytorch-dnc},\n  url = {https://github.com/jingweiz/pytorch-dnc},\n  year = {2017}\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{pytorch-dnc,\n  author = {Zhang, Jingwei},\n  title = {jingweiz/pytorch-dnc},\n  url = {https://github.com/jingweiz/pytorch-dnc},\n  year = {2017}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{zhang2017neural,\n  title={Neural SLAM},\n  author={Zhang, Jingwei and Tai, Lei and Boedecker, Joschka and Burgard, Wolfram and Liu, Ming},\n  journal={arXiv preprint arXiv:1706.09520},\n  year={2017}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jingweiz/pytorch-dnc",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-05-10T21:01:44Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-20T08:38:24Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8187517415072366
      ],
      "excerpt": "  NTM on the copy task (top 2 rows, 1st row converges to sequentially write to lower locations, 2nd row converges to sequentially write to upper locations) and \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9265049940845391
      ],
      "excerpt": "Sample loggings while training DNC on the repeat-copy task (we use WARNING as the logging level currently to get rid of the INFO printouts from visdom): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8661615832411004
      ],
      "excerpt": "This repo currently contains the following algorithms: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8697175385572169
      ],
      "excerpt": "NOTE: we follow the exact code structure as pytorch-rl so as to make the code easily transplantable. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8797846341041263,
        0.9821638434864027
      ],
      "excerpt": " where we list all the integrated Env, Circuit, Agent into Dict's. \n All of the core classes are implemented in ./core/. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.858208922890907,
        0.9124615693475211
      ],
      "excerpt": "* *_vb: torch.autograd.Variable's or a list of such objects \n* *_ts: torch.Tensor's or a list of such objects \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.901567859112835
      ],
      "excerpt": "The difference between NTM & DNC is stated as follows in the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.980126273743079
      ],
      "excerpt": "the predecessor to the DNC described in this work. It used a similar \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9285097370502183,
        0.8671331694548959,
        0.9233726569362015
      ],
      "excerpt": "matrix, but differed in the access mechanism used to interface with the memory. \nIn the NTM, content-based addressing was combined with location-based addressing \nto allow the network to iterate through memory locations in order of their \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8907325561612678
      ],
      "excerpt": "network to store and retrieve temporal sequences in contiguous blocks of memory. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9224235653012727,
        0.817914226789908,
        0.8829915141138818
      ],
      "excerpt": "that blocks of allocated memory do not overlap and interfere\u2014a basic problem of \ncomputer memory management. Interference is not an issue for the dynamic memory \nallocation used by DNCs, which provides single free locations at a time, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8572708634451307,
        0.9386956587602404,
        0.8642780640749854,
        0.9140007415260286,
        0.8416044148738577,
        0.9573496228584109
      ],
      "excerpt": "the NTM has no way of freeing locations that have already been written to and, \nhence, no way of reusing memory when processing long sequences. This problem is \naddressed in DNCs by the free gates used for de-allocation. Third, sequential \ninformation is preserved only as long as the NTM continues to iterate through \nconsecutive locations; as soon as the write head jumps to a different part of \nthe memory (using content-based addressing) the order of writes before and after \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9148813818727085
      ],
      "excerpt": "The part where NTM & DNC differs is the Accessor, where in the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9104003692028195
      ],
      "excerpt": "we use this to make the code more consistent) and DNC uses the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9416701307530325,
        0.9017831249923752
      ],
      "excerpt": "use this to make the code more consistent). The _content_focus() is the \nsame for both classes, but the _location_focus() for DNC is much \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.922016756914817
      ],
      "excerpt": "_access() to interact with the external memory. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9180634444797925
      ],
      "excerpt": "designed for reinforcement learning settings as in \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8570871199948232
      ],
      "excerpt": "providing datasets for supervised learning, so the reward, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Neural Turing Machine (NTM) & Differentiable Neural Computer (DNC) with pytorch & visdom",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jingweiz/pytorch-dnc/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 51,
      "date": "Fri, 24 Dec 2021 12:35:49 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jingweiz/pytorch-dnc/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "jingweiz/pytorch-dnc",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8678070928093194
      ],
      "excerpt": "[WARNING ] (MainProcess) bash$: python -m visdom.server \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8511910080348464
      ],
      "excerpt": "NOTE: we follow the exact code structure as pytorch-rl so as to make the code easily transplantable. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8792843423306563
      ],
      "excerpt": "pytorch-rl; here we use it for \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9133993238769265
      ],
      "excerpt": "Sample on-line plotting while training(avg loss)/testing(write/read weights & memory) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8857046413490627,
        0.8857046413490627,
        0.8857046413490627
      ],
      "excerpt": "<img src=\"/assets/ntm_copy_train_revised_16_0.png\" width=\"205\"/> <img src=\"/assets/ntm_copy_test_revised_16_0.gif\" width=\"600\"/> \n<img src=\"/assets/ntm_copy_train_revised_16_1.png\" width=\"205\"/> <img src=\"/assets/ntm_copy_test_revised_16_1.gif\" width=\"600\"/> \n<img src=\"/assets/dnc_repeat_copy_train_revised_tanh.png\" width=\"205\"/> <img src=\"/assets/dnc_repeat_copy_test_revised.gif\" width=\"600\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8748594493968603
      ],
      "excerpt": "[WARNING ] (MainProcess) No Pretrained Model. Will Train From Scratch. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8174540907975313
      ],
      "excerpt": "[WARNING ] (MainProcess) Training Stats:   avg_loss:         0.014866309287 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9586232994076559
      ],
      "excerpt": "* ./utils/factory.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8696232577613517
      ],
      "excerpt": " then the ./main.py will do it all (NOTE: this ./main.py file never needs to be modified). \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jingweiz/pytorch-dnc/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'The MIT License (MIT)\\n\\nCopyright (c) 2017 Jingwei Zhang\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "**Neural Turing Machine** (NTM) &",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "pytorch-dnc",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "jingweiz",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jingweiz/pytorch-dnc/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Python 2.7\n- [PyTorch >=v0.2.0](http://pytorch.org/)\n- [Visdom](https://github.com/facebookresearch/visdom)\n*******\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You only need to modify some parameters in ```./utils/options.py``` to train a new configuration.\n\n* Configure your training in ```./utils/options.py```:\n> * ```line 12```: add an entry into ```CONFIGS``` to define your training (```agent_type```, ```env_type```, ```game```, ```circuit_type```)\n> * ```line 28```: choose the entry you just added\n> * ```line 24-25```: fill in your machine/cluster ID (```MACHINE```) and timestamp (```TIMESTAMP```) to define your training signature (```MACHINE_TIMESTAMP```),\n the corresponding model file and the log file of this training will be saved under this signature (```./models/MACHINE_TIMESTAMP.pth``` & ```./logs/MACHINE_TIMESTAMP.log``` respectively).\n Also the visdom visualization will be displayed under this signature (first activate the visdom server by type in bash: ```python -m visdom.server &```, then open this address in your browser: ```http://localhost:8097/env/MACHINE_TIMESTAMP```)\n> * ```line 28```: to train a model, set ```mode=1``` (training visualization will be under ```http://localhost:8097/env/MACHINE_TIMESTAMP```); to test the model of this current training, all you need to do is to set ```mode=2``` (testing visualization will be under ```http://localhost:8097/env/MACHINE_TIMESTAMP_test```).\n\n* Run:\n> ```python main.py```\n*******\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 266,
      "date": "Fri, 24 Dec 2021 12:35:49 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "ntm",
      "dnc",
      "pytorch",
      "visdom",
      "external-memory",
      "deep-learning"
    ],
    "technique": "GitHub API"
  }
}