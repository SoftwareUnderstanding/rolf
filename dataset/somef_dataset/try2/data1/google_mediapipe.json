{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1906.08172"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9840846195802034
      ],
      "excerpt": "Face Detection                                                                                                                 | Face Mesh                                                                                                       | Iris                                                                                                      | Hands                                                                                                      | Pose                                                                                                      | Holistic \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9935724698798681
      ],
      "excerpt": "Hair Segmentation                                                                                                                       | Object Detection                                                                                                                     | Box Tracking                                                                                                                | Instant Motion Tracking                                                                                                                               | Objectron                                                                                                             | KNIFT \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8714162992508173
      ],
      "excerpt": "Face Mesh                             | \u2705                                                               | \u2705                                                       | \u2705                                                       | \u2705                                                             | \u2705                                                             | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8840467439138965
      ],
      "excerpt": "Pose                                       | \u2705                                                               | \u2705                                                       | \u2705                                                       | \u2705                                                             | \u2705                                                             | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9146894306581498,
        0.8356013927728488,
        0.8356013927728488
      ],
      "excerpt": "Object Detection               | \u2705                                                               | \u2705                                                       | \u2705                                                       |                                                               |                                                               | \u2705 \nBox Tracking                       | \u2705                                                               | \u2705                                                       | \u2705                                                       |                                                               |                                                               | \nInstant Motion Tracking | \u2705                                                               |                                                         |                                                         |                                                               |                                                               | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9527542357995875
      ],
      "excerpt": "MediaPipe Holistic - Simultaneous Face, Hand and Pose Prediction, on Device \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9705056183299773
      ],
      "excerpt": "Object Detection and Tracking using MediaPipe \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8334316807263773
      ],
      "excerpt": "MediaPipe Madrid Meetup, 16 Dec 2019 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9985400328818164,
        0.9268240064709804,
        0.8011036476841559,
        0.8356013927728488,
        0.9222383658450612,
        0.9978442862800369,
        0.9911777030586864
      ],
      "excerpt": "ML Conference, Berlin, 11 Dec 2019 \nMediaPipe Berlin Meetup, Google Berlin, 11 Dec 2019 \nThe 3rd Workshop on YouTube-8M Large Scale Video Understanding Workshop, \n    Seoul, Korea ICCV \n    2019 \nAI DevWorld 2019, 10 Oct 2019, San Jose, CA \nGoogle Industry Workshop at ICIP 2019, 24 Sept 2019, Taipei, Taiwan \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9990778029584345
      ],
      "excerpt": "Open sourced at CVPR 2019, 17~20 June, Long Beach, CA \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/google/mediapipe",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contributing guidelines\nWhat type of pull request do we accept into MediaPipe repository?\n\nBug fixes\nDocumentation fixes\n\nFor new feature additions (e.g., new graphs and calculators), we are currently not planning to accept new feature pull requests into the MediaPipe repository. Instead, we like to get contributors to create their own repositories of the new feature and list it at Awesome MediaPipe. This will allow contributors to more quickly get their code out to the community.\nBefore sending your pull requests, make sure you followed this list.\n\nRead contributing guidelines.\nRead Code of Conduct.\nEnsure you have signed the Contributor License Agreement (CLA).\n\nHow to become a contributor and submit your own code\nContributor License Agreements\nWe'd love to accept your patches! Before we can take them, we have to jump a couple of legal hurdles.\nPlease fill out either the individual or corporate Contributor License Agreement (CLA).\n\nIf you are an individual writing original source code and you're sure you own the intellectual property, then you'll need to sign an individual CLA.\nIf you work for a company that wants to allow you to contribute your work, then you'll need to sign a corporate CLA.\n\nFollow either of the two links above to access the appropriate CLA and instructions for how to sign and return it. Once we receive it, we'll be able to accept your pull requests.\nNOTE: Only original source code from you and other people that have signed the CLA can be accepted into the main repository.\nContributing code\nIf you have bug fixes and documentation fixes to MediaPipe, send us your pull requests! For those\njust getting started, GitHub has a howto.\nMediaPipe team members will be assigned to review your pull requests. Once the bug/documentation fixes are verified, a MediaPipe team member will acknowledge your contribution in the pull request comments, manually merge the fixes into our internal codebase upstream, and apply the to be closed label to the pull request. These fixes will later be pushed to GitHub in the next release, and a MediaPipe team member will then close the pull request.",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-06-13T19:16:41Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-29T13:45:19Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8525504834653382
      ],
      "excerpt": "ML solutions for live and streaming media. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9470375987859874
      ],
      "excerpt": "End-to-End acceleration: Built-in fast ML inference and processing accelerated even on common hardware | Build once, deploy anywhere: Unified solution works across Android, iOS, desktop/cloud, web and IoT \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8267044703128984
      ],
      "excerpt": "Ready-to-use solutions: Cutting-edge ML solutions demonstrating full power of the framework            | Free and open source: Framework and solutions both under Apache 2.0, fully extensible and customizable \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8259476137228465
      ],
      "excerpt": "<!-- Whenever this table is updated, paste a copy to solutions/solutions.md. --> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955996838438399,
        0.925919823793206,
        0.8127436506657173,
        0.8189522504524913,
        0.9079495796977959
      ],
      "excerpt": "See also \nMediaPipe Models and Model Cards \nfor ML models released in MediaPipe. \nBringing artworks to life with AR \n    in Google Developers Blog \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9079495796977959,
        0.9004866766321523
      ],
      "excerpt": "    in Google Developers Blog \nSignAll SDK: Sign language interface using MediaPipe is now available for \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9079495796977959
      ],
      "excerpt": "    in Google Developers Blog \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8849006430678149,
        0.9425013285897409,
        0.8849006430678149,
        0.8583447580759391,
        0.9079495796977959
      ],
      "excerpt": "    in Google AI Blog \nBackground Features in Google Meet, Powered by Web ML \n    in Google AI Blog \nMediaPipe 3D Face Transform \n    in Google Developers Blog \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9079495796977959
      ],
      "excerpt": "    in Google Developers Blog \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8849006430678149
      ],
      "excerpt": "    in Google AI Blog \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8849006430678149
      ],
      "excerpt": "    in Google AI Blog \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9079495796977959
      ],
      "excerpt": "    in Google Developers Blog \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9079495796977959,
        0.8998421450554995,
        0.8849006430678149
      ],
      "excerpt": "    in Google Developers Blog \nReal-Time 3D Object Detection on Mobile Devices with MediaPipe \n    in Google AI Blog \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8849006430678149,
        0.9208415585052967,
        0.9079495796977959
      ],
      "excerpt": "    in Google AI Blog \nMediaPipe on the Web \n    in Google Developers Blog \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9079495796977959,
        0.9288778025138584,
        0.8849006430678149
      ],
      "excerpt": "    in Google Developers Blog \nOn-Device, Real-Time Hand Tracking with MediaPipe \n    in Google AI Blog \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8999531193718416
      ],
      "excerpt": "Awesome MediaPipe - A curated list of awesome \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8786581515769303
      ],
      "excerpt": "Slack community for MediaPipe users \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8582388343924158,
        0.9143357640443611
      ],
      "excerpt": "MediaPipe is currently in alpha at v0.7. We may be still making breaking API \nchanges and expect to get to stable APIs by v1.0. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8137094165563061
      ],
      "excerpt": "We use GitHub issues for tracking requests and bugs. Please post questions to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Cross-platform, customizable ML solutions for live and streaming media.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/google/mediapipe/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3175,
      "date": "Wed, 29 Dec 2021 14:33:34 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/google/mediapipe/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "google/mediapipe",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/google/mediapipe/master/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/google/mediapipe/tree/master/docs",
      "https://github.com/google/mediapipe/tree/master/mediapipe/docs"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/google/mediapipe/master/build_ios_examples.sh",
      "https://raw.githubusercontent.com/google/mediapipe/master/setup_opencv.sh",
      "https://raw.githubusercontent.com/google/mediapipe/master/build_desktop_examples.sh",
      "https://raw.githubusercontent.com/google/mediapipe/master/setup_android_sdk_and_ndk.sh",
      "https://raw.githubusercontent.com/google/mediapipe/master/build_android_examples.sh",
      "https://raw.githubusercontent.com/google/mediapipe/master/mediapipe/graphs/object_detection_3d/obj_parser/obj_cleanup.sh"
    ],
    "technique": "File Exploration"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/google/mediapipe/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "C++",
      "Starlark",
      "Java",
      "Python",
      "Objective-C++",
      "Objective-C",
      "C",
      "Shell",
      "JavaScript",
      "HTML",
      "Dockerfile",
      "Makefile",
      "Batchfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Apache License 2.0",
      "url": "https://api.github.com/licenses/apache-2.0"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'                                 Apache License\\n                           Version 2.0, January 2004\\n                        http://www.apache.org/licenses/\\n\\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\\n\\n   1. Definitions.\\n\\n      \"License\" shall mean the terms and conditions for use, reproduction,\\n      and distribution as defined by Sections 1 through 9 of this document.\\n\\n      \"Licensor\" shall mean the copyright owner or entity authorized by\\n      the copyright owner that is granting the License.\\n\\n      \"Legal Entity\" shall mean the union of the acting entity and all\\n      other entities that control, are controlled by, or are under common\\n      control with that entity. For the purposes of this definition,\\n      \"control\" means (i) the power, direct or indirect, to cause the\\n      direction or management of such entity, whether by contract or\\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\\n      outstanding shares, or (iii) beneficial ownership of such entity.\\n\\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\\n      exercising permissions granted by this License.\\n\\n      \"Source\" form shall mean the preferred form for making modifications,\\n      including but not limited to software source code, documentation\\n      source, and configuration files.\\n\\n      \"Object\" form shall mean any form resulting from mechanical\\n      transformation or translation of a Source form, including but\\n      not limited to compiled object code, generated documentation,\\n      and conversions to other media types.\\n\\n      \"Work\" shall mean the work of authorship, whether in Source or\\n      Object form, made available under the License, as indicated by a\\n      copyright notice that is included in or attached to the work\\n      (an example is provided in the Appendix below).\\n\\n      \"Derivative Works\" shall mean any work, whether in Source or Object\\n      form, that is based on (or derived from) the Work and for which the\\n      editorial revisions, annotations, elaborations, or other modifications\\n      represent, as a whole, an original work of authorship. For the purposes\\n      of this License, Derivative Works shall not include works that remain\\n      separable from, or merely link (or bind by name) to the interfaces of,\\n      the Work and Derivative Works thereof.\\n\\n      \"Contribution\" shall mean any work of authorship, including\\n      the original version of the Work and any modifications or additions\\n      to that Work or Derivative Works thereof, that is intentionally\\n      submitted to Licensor for inclusion in the Work by the copyright owner\\n      or by an individual or Legal Entity authorized to submit on behalf of\\n      the copyright owner. For the purposes of this definition, \"submitted\"\\n      means any form of electronic, verbal, or written communication sent\\n      to the Licensor or its representatives, including but not limited to\\n      communication on electronic mailing lists, source code control systems,\\n      and issue tracking systems that are managed by, or on behalf of, the\\n      Licensor for the purpose of discussing and improving the Work, but\\n      excluding communication that is conspicuously marked or otherwise\\n      designated in writing by the copyright owner as \"Not a Contribution.\"\\n\\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\\n      on behalf of whom a Contribution has been received by Licensor and\\n      subsequently incorporated within the Work.\\n\\n   2. Grant of Copyright License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      copyright license to reproduce, prepare Derivative Works of,\\n      publicly display, publicly perform, sublicense, and distribute the\\n      Work and such Derivative Works in Source or Object form.\\n\\n   3. Grant of Patent License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      (except as stated in this section) patent license to make, have made,\\n      use, offer to sell, sell, import, and otherwise transfer the Work,\\n      where such license applies only to those patent claims licensable\\n      by such Contributor that are necessarily infringed by their\\n      Contribution(s) alone or by combination of their Contribution(s)\\n      with the Work to which such Contribution(s) was submitted. If You\\n      institute patent litigation against any entity (including a\\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\\n      or a Contribution incorporated within the Work constitutes direct\\n      or contributory patent infringement, then any patent licenses\\n      granted to You under this License for that Work shall terminate\\n      as of the date such litigation is filed.\\n\\n   4. Redistribution. You may reproduce and distribute copies of the\\n      Work or Derivative Works thereof in any medium, with or without\\n      modifications, and in Source or Object form, provided that You\\n      meet the following conditions:\\n\\n      (a) You must give any other recipients of the Work or\\n          Derivative Works a copy of this License; and\\n\\n      (b) You must cause any modified files to carry prominent notices\\n          stating that You changed the files; and\\n\\n      (c) You must retain, in the Source form of any Derivative Works\\n          that You distribute, all copyright, patent, trademark, and\\n          attribution notices from the Source form of the Work,\\n          excluding those notices that do not pertain to any part of\\n          the Derivative Works; and\\n\\n      (d) If the Work includes a \"NOTICE\" text file as part of its\\n          distribution, then any Derivative Works that You distribute must\\n          include a readable copy of the attribution notices contained\\n          within such NOTICE file, excluding those notices that do not\\n          pertain to any part of the Derivative Works, in at least one\\n          of the following places: within a NOTICE text file distributed\\n          as part of the Derivative Works; within the Source form or\\n          documentation, if provided along with the Derivative Works; or,\\n          within a display generated by the Derivative Works, if and\\n          wherever such third-party notices normally appear. The contents\\n          of the NOTICE file are for informational purposes only and\\n          do not modify the License. You may add Your own attribution\\n          notices within Derivative Works that You distribute, alongside\\n          or as an addendum to the NOTICE text from the Work, provided\\n          that such additional attribution notices cannot be construed\\n          as modifying the License.\\n\\n      You may add Your own copyright statement to Your modifications and\\n      may provide additional or different license terms and conditions\\n      for use, reproduction, or distribution of Your modifications, or\\n      for any such Derivative Works as a whole, provided Your use,\\n      reproduction, and distribution of the Work otherwise complies with\\n      the conditions stated in this License.\\n\\n   5. Submission of Contributions. Unless You explicitly state otherwise,\\n      any Contribution intentionally submitted for inclusion in the Work\\n      by You to the Licensor shall be under the terms and conditions of\\n      this License, without any additional terms or conditions.\\n      Notwithstanding the above, nothing herein shall supersede or modify\\n      the terms of any separate license agreement you may have executed\\n      with Licensor regarding such Contributions.\\n\\n   6. Trademarks. This License does not grant permission to use the trade\\n      names, trademarks, service marks, or product names of the Licensor,\\n      except as required for reasonable and customary use in describing the\\n      origin of the Work and reproducing the content of the NOTICE file.\\n\\n   7. Disclaimer of Warranty. Unless required by applicable law or\\n      agreed to in writing, Licensor provides the Work (and each\\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\\n      implied, including, without limitation, any warranties or conditions\\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\\n      PARTICULAR PURPOSE. You are solely responsible for determining the\\n      appropriateness of using or redistributing the Work and assume any\\n      risks associated with Your exercise of permissions under this License.\\n\\n   8. Limitation of Liability. In no event and under no legal theory,\\n      whether in tort (including negligence), contract, or otherwise,\\n      unless required by applicable law (such as deliberate and grossly\\n      negligent acts) or agreed to in writing, shall any Contributor be\\n      liable to You for damages, including any direct, indirect, special,\\n      incidental, or consequential damages of any character arising as a\\n      result of this License or out of the use or inability to use the\\n      Work (including but not limited to damages for loss of goodwill,\\n      work stoppage, computer failure or malfunction, or any and all\\n      other commercial damages or losses), even if such Contributor\\n      has been advised of the possibility of such damages.\\n\\n   9. Accepting Warranty or Additional Liability. While redistributing\\n      the Work or Derivative Works thereof, You may choose to offer,\\n      and charge a fee for, acceptance of support, warranty, indemnity,\\n      or other liability obligations and/or rights consistent with this\\n      License. However, in accepting such obligations, You may act only\\n      on Your own behalf and on Your sole responsibility, not on behalf\\n      of any other Contributor, and only if You agree to indemnify,\\n      defend, and hold each Contributor harmless for any liability\\n      incurred by, or claims asserted against, such Contributor by reason\\n      of your accepting any such warranty or additional liability.\\n\\n   END OF TERMS AND CONDITIONS\\n\\n   APPENDIX: How to apply the Apache License to your work.\\n\\n      To apply the Apache License to your work, attach the following\\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\\n      replaced with your own identifying information. (Don\\'t include\\n      the brackets!)  The text should be enclosed in the appropriate\\n      comment syntax for the file format. We also recommend that a\\n      file or class name and description of purpose be included on the\\n      same \"printed page\" as the copyright notice for easier\\n      identification within third-party archives.\\n\\n   Copyright [yyyy] [name of copyright owner]\\n\\n   Licensed under the Apache License, Version 2.0 (the \"License\");\\n   you may not use this file except in compliance with the License.\\n   You may obtain a copy of the License at\\n\\n       http://www.apache.org/licenses/LICENSE-2.0\\n\\n   Unless required by applicable law or agreed to in writing, software\\n   distributed under the License is distributed on an \"AS IS\" BASIS,\\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n   See the License for the specific language governing permissions and\\n   limitations under the License.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Live ML anywhere",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "mediapipe",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "google",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/google/mediapipe/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "jiuqiant",
        "body": "[MediaPipe Android Solutions](https://google.github.io/mediapipe/getting_started/android_solutions.html)\r\n-  MediaPipe Hands, Face Detection, and Face Mesh Android Solutions are now available in [Google's Maven Repository](https://maven.google.com/web/index.html?#com.google.mediapipe).\r\n- The Android Studio example project is available in [mediapipe/examples/android/solutions](https://github.com/google/mediapipe/tree/master/mediapipe/examples/android/solutions).\r\n\r\n[MediaPipe Hands](https://google.github.io/mediapipe/solutions/hands.html)\r\n- MediaPipe Hands models are updated.\r\n- MediaPipe Hands now supports outputting [world landmarks](https://google.github.io/mediapipe/solutions/hands.html#multi_hand_world_landmarks) in world coordinates.\r\n\r\nMediaPipe Dependencies\r\n\r\n- MediaPipe [Python wheels](https://pypi.org/project/mediapipe/0.8.9.1/) are now supporting Python 3.10.\r\n- The MediaPipe dependency library protobuf, tensorflow, cere solver, pybind, and apple support are updated.\r\n- The recommended Bazel version is updated to 4.2.1.\r\n- The recommended Android SDK and NDK versions are updated.",
        "dateCreated": "2021-12-13T23:56:02Z",
        "datePublished": "2021-12-14T00:28:13Z",
        "html_url": "https://github.com/google/mediapipe/releases/tag/v0.8.9",
        "name": "MediaPipe Android Solutions and Hand World Landmarks",
        "tag_name": "v0.8.9",
        "tarball_url": "https://api.github.com/repos/google/mediapipe/tarball/v0.8.9",
        "url": "https://api.github.com/repos/google/mediapipe/releases/55232008",
        "zipball_url": "https://api.github.com/repos/google/mediapipe/zipball/v0.8.9"
      },
      {
        "authorType": "User",
        "author_name": "chuoling",
        "body": "* [MediaPipe Face Mesh](https://solutions.mediapipe.dev/face_mesh)\r\n   * Added an [`refine_landmarks`](https://google.github.io/mediapipe/solutions/face_mesh#refine_landmarks) option in the Solution APIs to further improve landmarks around eyes and lips, and output additional landmarks around the irises.\r\n   * Released the [Attention Mesh ML model](https://google.github.io/mediapipe/solutions/face_mesh.html#attention-mesh-model) that enables `refine_landmarks`.\r\n* [MediaPipe Holistic](https://solutions.mediapipe.dev/holistic)\r\n   * Added the [`enable_segmentation`](https://google.github.io/mediapipe/solutions/holistic#enable_segmentation) and [`smooth_segmentation`](https://google.github.io/mediapipe/solutions/holistic#smooth_segmentation) option in the Solution APIs, previously only available in [MediaPipe Pose](https://solutions.mediapipe.dev/pose). \r\n",
        "dateCreated": "2021-10-06T21:27:49Z",
        "datePublished": "2021-10-07T01:48:52Z",
        "html_url": "https://github.com/google/mediapipe/releases/tag/v0.8.8",
        "name": "Additional options in MediaPipe Face Mesh and Holistic",
        "tag_name": "v0.8.8",
        "tarball_url": "https://api.github.com/repos/google/mediapipe/tarball/v0.8.8",
        "url": "https://api.github.com/repos/google/mediapipe/releases/50919548",
        "zipball_url": "https://api.github.com/repos/google/mediapipe/zipball/v0.8.8"
      },
      {
        "authorType": "User",
        "author_name": "jiuqiant",
        "body": "Merged  https://github.com/google/mediapipe/pull/1969, https://github.com/google/mediapipe/pull/2039, https://github.com/google/mediapipe/pull/2101, https://github.com/google/mediapipe/pull/2136, https://github.com/google/mediapipe/pull/2154, https://github.com/google/mediapipe/pull/2201, https://github.com/google/mediapipe/pull/2291, https://github.com/google/mediapipe/pull/2292, https://github.com/google/mediapipe/pull/2347, https://github.com/google/mediapipe/pull/2350, https://github.com/google/mediapipe/pull/2450, https://github.com/google/mediapipe/pull/2462, and https://github.com/google/mediapipe/pull/2464.\r\n\r\nA huge thank you to all contributors:\r\n@Abduttayyeb\r\n@brettkoonce\r\n@chr0nikler\u00a0\r\n@daniel13520cs\r\n@gabrielsanchez\r\n@GantMan\r\n@GzuPark\u00a0\r\n@homuler\r\n@magamig\r\n@PeterPocsi\r\n@TomHsiao1260\u00a0\r\n@yuripourre\u00a0\r\n\r\n\r\n\r\n\r\n",
        "dateCreated": "2021-09-02T01:15:31Z",
        "datePublished": "2021-09-02T16:43:16Z",
        "html_url": "https://github.com/google/mediapipe/releases/tag/v0.8.7.1",
        "name": "Merge PRs. Thank you to all contributors!",
        "tag_name": "v0.8.7.1",
        "tarball_url": "https://api.github.com/repos/google/mediapipe/tarball/v0.8.7.1",
        "url": "https://api.github.com/repos/google/mediapipe/releases/48907987",
        "zipball_url": "https://api.github.com/repos/google/mediapipe/zipball/v0.8.7.1"
      },
      {
        "authorType": "User",
        "author_name": "jiuqiant",
        "body": "- MediaPipe Pose now outputs \"segmentation_mask\" when [`enable_segmentation`](https://google.github.io/mediapipe/solutions/pose.html#enable_segmentation) is set.\r\n- MediaPipe Python [FaceMesh](https://google.github.io/mediapipe/solutions/face_mesh.html#python-solution-api), [Hands](https://google.github.io/mediapipe/solutions/hands.html#python-solution-api), [Pose](https://google.github.io/mediapipe/solutions/pose.html#python-solution-api), and [Holistic](https://google.github.io/mediapipe/solutions/holistic.html#python-solution-api) have new drawing styles. Please refer to the documentation and the Colab examples for more details.\r\n- MediaPipe now offers Android [FaceMesh](https://google.github.io/mediapipe/solutions/face_mesh.html#android-solution-api) and [Hands](https://google.github.io/mediapipe/solutions/hands.html#android-solution-api) solution APIs (currently in Alpha). The maven artifacts are available in [Google's Maven Repository](https://maven.google.com/web/index.html?#com.google.mediapipe). To try the example apps, please import [the solution example Android Studio project](https://github.com/google/mediapipe/tree/master/mediapipe/examples/android/solutions) on either Linux, macOS, and Windows.",
        "dateCreated": "2021-08-19T00:45:46Z",
        "datePublished": "2021-08-19T02:11:42Z",
        "html_url": "https://github.com/google/mediapipe/releases/tag/v0.8.7",
        "name": "MediaPipe Pose Segmentation and Android Solution APIs",
        "tag_name": "v0.8.7",
        "tarball_url": "https://api.github.com/repos/google/mediapipe/tarball/v0.8.7",
        "url": "https://api.github.com/repos/google/mediapipe/releases/48083163",
        "zipball_url": "https://api.github.com/repos/google/mediapipe/zipball/v0.8.7"
      },
      {
        "authorType": "User",
        "author_name": "chuoling",
        "body": "* MediaPipe Pose (and Holistic) now also outputs estimated real-world 3D coordinates of pose landmarks (in meters with the origin at center of hips)\r\n* MediaPipe Face Detection now supports a \"model_selection\" option to switch between short-range and full-range models\r\n",
        "dateCreated": "2021-06-28T17:17:10Z",
        "datePublished": "2021-07-02T18:40:50Z",
        "html_url": "https://github.com/google/mediapipe/releases/tag/v0.8.6",
        "name": "3D world landmarks in MediaPipe Pose and full-range model in MediaPipe Face Detection",
        "tag_name": "v0.8.6",
        "tarball_url": "https://api.github.com/repos/google/mediapipe/tarball/v0.8.6",
        "url": "https://api.github.com/repos/google/mediapipe/releases/45189053",
        "zipball_url": "https://api.github.com/repos/google/mediapipe/zipball/v0.8.6"
      },
      {
        "authorType": "User",
        "author_name": "chuoling",
        "body": "* Released [MediaPipe Selfie Segmentation](https://solutions.mediapipe.dev/selfie_segmentation)\r\n    * JavaScript Solution API w/ a CodePen example\r\n    * Python Solution API w/ a Colab example\r\n    * Android, iOS and desktop example apps",
        "dateCreated": "2021-06-03T21:32:02Z",
        "datePublished": "2021-06-04T00:23:03Z",
        "html_url": "https://github.com/google/mediapipe/releases/tag/v0.8.5",
        "name": "MediaPipe Selfie Segmentation",
        "tag_name": "v0.8.5",
        "tarball_url": "https://api.github.com/repos/google/mediapipe/tarball/v0.8.5",
        "url": "https://api.github.com/repos/google/mediapipe/releases/44071999",
        "zipball_url": "https://api.github.com/repos/google/mediapipe/zipball/v0.8.5"
      },
      {
        "authorType": "User",
        "author_name": "chuoling",
        "body": "* Better pose landmark accuracy with the updated pose detection and pose landmark model\r\n* Added a `lite` and a `heavy` version of the pose landmark model, in addition to the existing `full` version (also updated and improved)\r\n* Added a `model_complexity` config option in [MediaPipe Pose](https://solutions.mediapipe.dev/pose#model_complexity) and [MediaPipe Holistic](https://solutions.mediapipe.dev/holistic#model_complexity) Solution APIs to select across the 3 model versions\r\n* Removed the `upper_body_only` option in MediaPipe Pose and MediaPipe Holistic, as the standard model now already handles upper-body-only use cases well\r\n",
        "dateCreated": "2021-05-10T20:42:02Z",
        "datePublished": "2021-05-11T02:56:25Z",
        "html_url": "https://github.com/google/mediapipe/releases/tag/v0.8.4",
        "name": "Model update in MediaPipe Pose and MediaPipe Holistic",
        "tag_name": "v0.8.4",
        "tarball_url": "https://api.github.com/repos/google/mediapipe/tarball/v0.8.4",
        "url": "https://api.github.com/repos/google/mediapipe/releases/42446936",
        "zipball_url": "https://api.github.com/repos/google/mediapipe/zipball/v0.8.4"
      },
      {
        "authorType": "User",
        "author_name": "chuoling",
        "body": "* Move up minimum bazel version to [3.7.2](https://github.com/google/mediapipe/blob/7c331ad58b2cca0dca468e342768900041d65adc/WORKSPACE#L17), and encourage the use of [Bazelisk](https://docs.bazel.build/versions/master/install-bazelisk.html) in [Installation](https://google.github.io/mediapipe/getting_started/install.html).\r\n* Move up [TensorFlow dependency](https://github.com/google/mediapipe/blob/7c331ad58b2cca0dca468e342768900041d65adc/WORKSPACE#L369).\r\n* MediaPipe Objectron [desktop/C++ example](https://solutions.mediapipe.dev/objectron#desktop).\r\n",
        "dateCreated": "2021-03-26T02:09:18Z",
        "datePublished": "2021-03-29T17:09:02Z",
        "html_url": "https://github.com/google/mediapipe/releases/tag/0.8.3.2",
        "name": "Move minimum bazel version to 3.7.2",
        "tag_name": "0.8.3.2",
        "tarball_url": "https://api.github.com/repos/google/mediapipe/tarball/0.8.3.2",
        "url": "https://api.github.com/repos/google/mediapipe/releases/40516440",
        "zipball_url": "https://api.github.com/repos/google/mediapipe/zipball/0.8.3.2"
      },
      {
        "authorType": "User",
        "author_name": "chuoling",
        "body": "* [MediaPipe Pose](https://solutions.mediapipe.dev/pose)\r\n    * New pose landmark model with Z coordinates ([BlazePose GHUM 3D](https://solutions.mediapipe.dev/pose#pose-landmark-model-blazepose-ghum-3d)), generated from synthetic data\r\n    * Added tutorials and Colabs for [Pose Classification](https://solutions.mediapipe.dev/pose_classification)\r\n* [MediaPipe Objectron](https://solutions.mediapipe.dev/objectron)\r\n    * Added Python Solution API\r\n* [MediaPipe Face Detection](https://solutions.mediapipe.dev/face_detection)\r\n    * Added Python Solution API and JavaScript Solution API\r\n* [MediaPipe Face Mesh](https://solutions.mediapipe.dev/face_mesh)\r\n    * [Face Geometry Module](https://solutions.mediapipe.dev/face_mesh#face-geometry-module) ([code](https://github.com/google/mediapipe/tree/master/mediapipe/modules/face_geometry)) now supports [face-detection input](https://github.com/google/mediapipe/blob/master/mediapipe/modules/face_geometry/face_geometry_from_detection.pbtxt) (to generate a face geometry from a detection), in addition to the existing support for [face-landmark input](https://github.com/google/mediapipe/blob/master/mediapipe/modules/face_geometry/face_geometry_from_landmarks.pbtxt)\r\n* [Image](https://github.com/google/mediapipe/blob/master/mediapipe/framework/formats/image.h): a new multi-backend image data container: \r\n    * Covering both CPU and GPU storage\r\n    * Currently used in [ImageToTensorCalculator](https://github.com/google/mediapipe/blob/master/mediapipe/calculators/tensor/image_to_tensor_calculator.cc) and [Face Detection Module](https://github.com/google/mediapipe/tree/master/mediapipe/modules/face_detection)\r\n    * Companion utility [ToImageCalculator](https://github.com/google/mediapipe/blob/master/mediapipe/calculators/util/to_image_calculator.cc) and [FromImageCalculator](https://github.com/google/mediapipe/blob/master/mediapipe/calculators/util/from_image_calculator.cc)",
        "dateCreated": "2021-02-27T21:21:55Z",
        "datePublished": "2021-02-27T22:06:10Z",
        "html_url": "https://github.com/google/mediapipe/releases/tag/0.8.3.1",
        "name": "MediaPipe Pose with 3D landmarks, and more Python & JavaScript Solution APIs",
        "tag_name": "0.8.3.1",
        "tarball_url": "https://api.github.com/repos/google/mediapipe/tarball/0.8.3.1",
        "url": "https://api.github.com/repos/google/mediapipe/releases/39002469",
        "zipball_url": "https://api.github.com/repos/google/mediapipe/zipball/0.8.3.1"
      },
      {
        "authorType": "User",
        "author_name": "chuoling",
        "body": "*  Released [experimental new framework APIs](https://github.com/google/mediapipe/tree/master/mediapipe/framework/api2/README.md)\r\n*  Updated [MediaPipe Objectron](https://solutions.mediapipe.dev/objectron) to support simultaneous detection of multiple objects\r\n*  Bug fixes\r\n    *  Fixed a crash in the [MediaPipe Iris](https://solutions.mediapipe.dev/iris) iOS example app\r\n    *  Fixed memory leak in Android example apps (in [ExternalTextureConverter](https://github.com/google/mediapipe/blob/master/mediapipe/java/com/google/mediapipe/components/ExternalTextureConverter.java))\r\n    *  Fixed a file read mode issue in [pose_landmark_model_loader.pbtxt](https://github.com/google/mediapipe/blob/master/mediapipe/modules/pose_landmark/pose_landmark_model_loader.pbtxt#L51).\r\n",
        "dateCreated": "2020-12-16T05:05:25Z",
        "datePublished": "2020-12-16T05:20:41Z",
        "html_url": "https://github.com/google/mediapipe/releases/tag/0.8.2",
        "name": "Experimental new framework APIs",
        "tag_name": "0.8.2",
        "tarball_url": "https://api.github.com/repos/google/mediapipe/tarball/0.8.2",
        "url": "https://api.github.com/repos/google/mediapipe/releases/35330264",
        "zipball_url": "https://api.github.com/repos/google/mediapipe/zipball/0.8.2"
      },
      {
        "authorType": "User",
        "author_name": "chuoling",
        "body": "* [MediaPipe Holistic](https://solutions.mediapipe.dev/holistic) for real-time simultaneous tracking of human pose, face and hand landmarks\r\n     * [Google AI blog post](https://mediapipe.page.link/holistic_ai_blog_post)\r\n* [MediaPipe in Python](https://docs.mediapipe.dev/getting_started/python)\r\n     * Ready-to-use Python Solution APIs for MediaPipe Face Mesh, Hands, Pose and Holistic\r\n     * Try it in [MediaPipe on Google Colab](https://docs.mediapipe.dev/getting_started/python#mediapipe-on-google-colab)\r\n* [MediaPipe in JavaScript](https://docs.mediapipe.dev/getting_started/javascript)\r\n     * Ready-to-use JavaScript Solution APIs for MediaPipe Face Mesh, Hands, Pose and Holistic\r\n     * Try it in [MediaPipe on CodePen](https://code.mediapipe.dev/codepen)\r\n* Switch [build](https://github.com/google/mediapipe/blob/master/.bazelrc) from C++14 to C++17\r\n",
        "dateCreated": "2020-12-10T04:28:50Z",
        "datePublished": "2020-12-10T18:22:37Z",
        "html_url": "https://github.com/google/mediapipe/releases/tag/0.8.1",
        "name": "MediaPipe Holistic, and MediaPipe in Python and JavaScript",
        "tag_name": "0.8.1",
        "tarball_url": "https://api.github.com/repos/google/mediapipe/tarball/0.8.1",
        "url": "https://api.github.com/repos/google/mediapipe/releases/35073840",
        "zipball_url": "https://api.github.com/repos/google/mediapipe/zipball/0.8.1"
      },
      {
        "authorType": "User",
        "author_name": "chuoling",
        "body": "* [MediaPipe in Python](https://google.github.io/mediapipe/#mediapipe-in-python)\r\n     * Released Python Solution API for [MediaPipe Face Mesh](https://solutions.mediapipe.dev/face_mesh#python) and [MediaPipe Hands](https://solutions.mediapipe.dev/hands#python)\r\n     * Updated Python Solution API for [MediaPipe Pose](https://solutions.mediapipe.dev/pose#python)\r\n     * Also released usage examples as [Google Colabs](https://google.github.io/mediapipe/#mediapipe-in-python)\r\n* [MediaPipe Objectron](https://solutions.mediapipe.dev/objectron)\r\n     * Released a faster two-stage pipeline\r\n     * Added support for more object classes: shoe, chair, cup and camera\r\n     * Released [training dataset](https://github.com/google-research-datasets/Objectron/), (to be) announced in a [Google AI Blog post](https://mediapipe.page.link/objectron_dataset_ai_blog)\r\n* New [Tensor](https://github.com/google/mediapipe/blob/master/mediapipe/framework/formats/tensor.h) class\r\n     * Released as a multi-dimensional tensor data container, supporting multiple backends like CPU, Metal buffer, GL buffer and GL texture 2D\r\n     * Added new Tensor-based pre-processing, inference and post-processing calculators in [mediapipe/calculators/tensor](https://github.com/google/mediapipe/tree/master/mediapipe/calculators/tensor), branched from existing calculators in [mediapipe/calculators/tflite](https://github.com/google/mediapipe/tree/master/mediapipe/calculators/tflite)\r\n     * Most of the (sub-)graph in [mediapipe/modules](https://github.com/google/mediapipe/tree/master/mediapipe/modules) have been updated to use Tensor and the associated calculators. The plan is to fully migrate all in the repo (and deprecate [mediapipe/calculators/tflite](https://github.com/google/mediapipe/tree/master/mediapipe/calculators/tflite) by end of 2020.\r\n* [MediaPipe Hands](https://solutions.mediapipe.dev/hands)\r\n     * Refactored [graphs](https://github.com/google/mediapipe/tree/master/mediapipe/graphs/hand_tracking) to depend on the new [palm_detection](https://github.com/google/mediapipe/tree/master/mediapipe/modules/palm_detection) and [hand_landmark module](https://github.com/google/mediapipe/tree/master/mediapipe/modules/hand_landmark).\r\n     * Improved model speed for both palm detection and hand landmark.\r\n     * Extended the main hand tracking [example apps](http://solutions.mediapipe.dev/hands#example-apps) to support multiple hands, to replace the separate multi-hand tracking examples.\r\n* [MediaPipe Face Detection](https://solutions.mediapipe.dev/face_detection)\r\n     * Refactored [graphs](https://github.com/google/mediapipe/tree/master/mediapipe/graphs/face_detection) to depend on the [face_detection module](https://github.com/google/mediapipe/tree/master/mediapipe/modules/face_detection).\r\n",
        "dateCreated": "2020-11-05T00:09:58Z",
        "datePublished": "2020-11-05T00:23:24Z",
        "html_url": "https://github.com/google/mediapipe/releases/tag/0.8.0",
        "name": "Python Solution APIs",
        "tag_name": "0.8.0",
        "tarball_url": "https://api.github.com/repos/google/mediapipe/tarball/0.8.0",
        "url": "https://api.github.com/repos/google/mediapipe/releases/33473224",
        "zipball_url": "https://api.github.com/repos/google/mediapipe/zipball/0.8.0"
      },
      {
        "authorType": "User",
        "author_name": "chuoling",
        "body": "* Added 3D Face Transform to [MediaPipe Face Mesh](https://solutions.mediapipe.dev/face_mesh)\r\n     * Enables [Face Effect Rendering](https://solutions.mediapipe.dev/face_mesh#effect-renderer)\r\n     * Demonstrated with an [Face Effect Example App](https://solutions.mediapipe.dev/face_mesh#face-effect-example)\r\n     * More info in [Google Developers Blog post](https://mediapipe.page.link/face-geometry-blog)\r\n* Added [MediaPipe Models and Model Cards](https://solutions.mediapipe.dev/models) doc",
        "dateCreated": "2020-09-17T15:52:51Z",
        "datePublished": "2020-09-25T16:54:03Z",
        "html_url": "https://github.com/google/mediapipe/releases/tag/v0.7.11",
        "name": "3D Face Transform in MediaPipe Face Mesh",
        "tag_name": "v0.7.11",
        "tarball_url": "https://api.github.com/repos/google/mediapipe/tarball/v0.7.11",
        "url": "https://api.github.com/repos/google/mediapipe/releases/31479041",
        "zipball_url": "https://api.github.com/repos/google/mediapipe/zipball/v0.7.11"
      },
      {
        "authorType": "User",
        "author_name": "chuoling",
        "body": "* [MediaPipe Instant Motion Tracking](https://solutions.mediapipe.dev/instant_motion_tracking) that performs AR tracking without initialization or calibration.\r\n     * [Google Developers Blog post](https://mediapipe.page.link/instant-motion-tracking-blog)\r\n* [MediaPipe BlazePose in Python](https://solutions.mediapipe.dev/pose#python) now uses `pip install mediapipe` instead of building from source. Also published a [Colab](https://mediapipe.page.link/mp-py-colab) example.\r\n* [MediaPipe Iris](https://solutions.mediapipe.dev/iris) updated to output a set of 478 3D landmarks, including 468 face landmarks from [MediaPipe Face Mesh](https://solutions.mediapipe.dev/face_mesh), with those around the eyes further refined, and 10 additional iris landmarks appended at the end.\r\n",
        "dateCreated": "2020-08-30T23:52:55Z",
        "datePublished": "2020-08-31T17:00:35Z",
        "html_url": "https://github.com/google/mediapipe/releases/tag/v0.7.10",
        "name": "MediaPipe Instant Motion Tracking",
        "tag_name": "v0.7.10",
        "tarball_url": "https://api.github.com/repos/google/mediapipe/tarball/v0.7.10",
        "url": "https://api.github.com/repos/google/mediapipe/releases/30406123",
        "zipball_url": "https://api.github.com/repos/google/mediapipe/zipball/v0.7.10"
      },
      {
        "authorType": "User",
        "author_name": "chuoling",
        "body": "* [MediaPipe Pose](https://solutions.mediapipe.dev/pose) for upper-body pose tracking\r\n     * [Google AI blog post for full and upper body Blazepose](https://mediapipe.page.link/blazepose-blog)\r\n* Preview of [Python support](https://google.github.io/mediapipe/getting_started/building_examples.html#python) that runs MediaPipe Pose in Python interpreter\r\n\r\n",
        "dateCreated": "2020-08-13T08:32:11Z",
        "datePublished": "2020-08-13T08:36:43Z",
        "html_url": "https://github.com/google/mediapipe/releases/tag/v0.7.9",
        "name": "MediaPipe Pose",
        "tag_name": "v0.7.9",
        "tarball_url": "https://api.github.com/repos/google/mediapipe/tarball/v0.7.9",
        "url": "https://api.github.com/repos/google/mediapipe/releases/29637258",
        "zipball_url": "https://api.github.com/repos/google/mediapipe/zipball/v0.7.9"
      },
      {
        "authorType": "User",
        "author_name": "mgyong",
        "body": "* [MediaPipe Iris](https://solutions.mediapipe.dev/iris) for iris tracking and single-image depth-from-iris\r\n* Fixed mirrored text rendering in iOS example apps\r\n* [Google AI blog post on MediaPipe Iris](https://mediapipe.page.link/iris-blog)\r\n* [Iris tracking web demo](https://viz.mediapipe.dev/demo/iris_tracking)\r\n* [Depth estimation from Iris web demo](https://viz.mediapipe.dev/demo/iris_depth)\r\n",
        "dateCreated": "2020-08-06T02:14:25Z",
        "datePublished": "2020-08-06T04:54:19Z",
        "html_url": "https://github.com/google/mediapipe/releases/tag/v0.7.8",
        "name": "MediaPipe Iris - Iris tracking and depth estimation",
        "tag_name": "v0.7.8",
        "tarball_url": "https://api.github.com/repos/google/mediapipe/tarball/v0.7.8",
        "url": "https://api.github.com/repos/google/mediapipe/releases/29393008",
        "zipball_url": "https://api.github.com/repos/google/mediapipe/zipball/v0.7.8"
      },
      {
        "authorType": "User",
        "author_name": "chuoling",
        "body": "* Added support for automatic provisioning for [building iOS examples](https://google.github.io/mediapipe/getting_started/building_examples.html#ios)\r\n* Fixed sample trace file (https://github.com/google/mediapipe/issues/849)\r\n",
        "dateCreated": "2020-07-30T01:18:36Z",
        "datePublished": "2020-07-30T03:57:30Z",
        "html_url": "https://github.com/google/mediapipe/releases/tag/v0.7.7",
        "name": "Automatic provisioning for iOS examples",
        "tag_name": "v0.7.7",
        "tarball_url": "https://api.github.com/repos/google/mediapipe/tarball/v0.7.7",
        "url": "https://api.github.com/repos/google/mediapipe/releases/29099510",
        "zipball_url": "https://api.github.com/repos/google/mediapipe/zipball/v0.7.7"
      },
      {
        "authorType": "User",
        "author_name": "mgyong",
        "body": "*  [TfLiteTensorsToLandmarksCalculator](https://github.com/google/mediapipe/blob/master/mediapipe/calculators/tflite/tflite_tensors_to_landmarks_calculator.cc)\r\n    * It now always [normalizes Z coordinates by image width (as for X)](https://github.com/google/mediapipe/blob/e9fbe868e55fa23aaabc31f9f847c22287062850/mediapipe/calculators/tflite/tflite_tensors_to_landmarks_calculator.cc#L223) when producing NORM_LANDMARKS output, assuming a weak perspective projection camera model.\r\n    * The [normalize_z](https://github.com/google/mediapipe/blob/e9fbe868e55fa23aaabc31f9f847c22287062850/mediapipe/calculators/tflite/tflite_tensors_to_landmarks_calculator.proto#L53) option can be used to further normalize Z coordinates by an additional factor. For instance, [normalize_z: 0.4](https://github.com/google/mediapipe/blob/e9fbe868e55fa23aaabc31f9f847c22287062850/mediapipe/graphs/hand_tracking/subgraphs/hand_landmark_gpu.pbtxt#L149) is now used in hand tracking to better account for the Z coordinate distribution in the training data.\r\n* Bug fixes\r\n    * Fixes issues on Ubuntu 20.04, resolving #820.\r\n* Documentation update\r\n    * Added [instructions](https://google.github.io/mediapipe/getting_started/install.html#installing-on-debian-and-ubuntu) for building for Nvidia Jetson and Raspberry Pi devices with ARM Ubuntu. ",
        "dateCreated": "2020-07-09T01:24:46Z",
        "datePublished": "2020-07-09T01:39:16Z",
        "html_url": "https://github.com/google/mediapipe/releases/tag/v0.7.6",
        "name": "Changes in Z coordinate normalization in 3D landmarks",
        "tag_name": "v0.7.6",
        "tarball_url": "https://api.github.com/repos/google/mediapipe/tarball/v0.7.6",
        "url": "https://api.github.com/repos/google/mediapipe/releases/28382349",
        "zipball_url": "https://api.github.com/repos/google/mediapipe/zipball/v0.7.6"
      },
      {
        "authorType": "User",
        "author_name": "mgyong",
        "body": "*   MediaPipe Hand \r\n           *  [Updated model, documentation and pipeline](https://solutions.mediapipe.dev/hands)\r\n           *  Improved hand tracking quality in various scenarios\r\n           *  Added handedness prediction, e.g. left or right hand.\r\n           *  [See tweet for visualization](https://twitter.com/GoogleAI/status/1265319835283537921)\r\n*   MediaPipe Face Detection\r\n           *  [Updated model for front-facing camera](https://solutions.mediapipe.dev/face_detection#resources)\r\n           *  [Added model for back-facing camera](https://solutions.mediapipe.dev/face_detection#resources), which better handles smaller faces\r\n*   MediaPipe AutoFlip \r\n           *  [Updated to use face detection model for back-facing camera](https://solutions.mediapipe.dev/autoflip)\r\n*   [Support for running MediaPipe on Windows - experimental](https://google.github.io/mediapipe/getting_started/install.html#installing-on-windows)\r\n           * MediaPipe TFLite CPU desktop live and offline demos only. No TFLite GPU and TensorFlow support yet. \r\n           * Compiled by MSVC through Bazel. No MinGW and Cygwin support yet.\r\n           * MediaPipe Android apps are NOT buildable on native Windows yet.\r\n*   [Tracing and profiling MediaPipe graphs - experimental](https://google.github.io/mediapipe/getting_started/install.html#installing-on-windows)\r\n           *  [Support for collecting trace logs on Android, iOS, Linux](https://google.github.io/mediapipe/tools/tracing_and_profiling.html#collecting-the-logs)\r\n           *  To analyze the trace logs, [MediaPipe Visualizer](https://viz.mediapipe.dev)\r\n*   Community related\r\n           *  [Awesome MediaPipe: curation of code related to MediaPipe](https://mediapipe.org)\r\n           *  [Slack community for MediaPipe users](https://mediapipe.page.link/joinslack)\r\n",
        "dateCreated": "2020-05-21T18:47:31Z",
        "datePublished": "2020-05-21T19:02:13Z",
        "html_url": "https://github.com/google/mediapipe/releases/tag/v0.7.5",
        "name": "MediaPipe Hand AutoFlip updated, Windows support, Tracing/profiling",
        "tag_name": "v0.7.5",
        "tarball_url": "https://api.github.com/repos/google/mediapipe/tarball/v0.7.5",
        "url": "https://api.github.com/repos/google/mediapipe/releases/26769867",
        "zipball_url": "https://api.github.com/repos/google/mediapipe/zipball/v0.7.5"
      },
      {
        "authorType": "User",
        "author_name": "mgyong",
        "body": "*   [MediaPipe KNIFT](http://solutions.mediapipe.dev/knift)\r\n      *   See also [Google Developers Blog post](https://developers.googleblog.com/2020/04/mediapipe-knift-template-based-feature-matching.html)",
        "dateCreated": "2020-04-22T02:43:01Z",
        "datePublished": "2020-04-22T16:37:56Z",
        "html_url": "https://github.com/google/mediapipe/releases/tag/v0.7.4",
        "name": "MediaPipe KNIFT (Template-based Feature Matching)",
        "tag_name": "v0.7.4",
        "tarball_url": "https://api.github.com/repos/google/mediapipe/tarball/v0.7.4",
        "url": "https://api.github.com/repos/google/mediapipe/releases/25773730",
        "zipball_url": "https://api.github.com/repos/google/mediapipe/zipball/v0.7.4"
      },
      {
        "authorType": "User",
        "author_name": "mgyong",
        "body": "*   [MediaPipe Face Mesh](https://solutions.mediapipe.dev/face_mesh) (3D face landmark estimation with 468 landmarks)\r\n*   [MediaPipe Modules](https://github.com/google/mediapipe/tree/master/mediapipe/modules)\r\n\t*   [Face Detection](https://github.com/google/mediapipe/tree/master/mediapipe/modules/face_detection)\r\n\t*   [Face Landmark example](https://github.com/google/mediapipe/tree/master/mediapipe/modules/face_landmark)",
        "dateCreated": "2020-04-07T04:28:19Z",
        "datePublished": "2020-04-14T00:18:35Z",
        "html_url": "https://github.com/google/mediapipe/releases/tag/v0.7.3.1",
        "name": "MediaPipe Face Mesh",
        "tag_name": "v0.7.3.1",
        "tarball_url": "https://api.github.com/repos/google/mediapipe/tarball/v0.7.3.1",
        "url": "https://api.github.com/repos/google/mediapipe/releases/25467441",
        "zipball_url": "https://api.github.com/repos/google/mediapipe/zipball/v0.7.3.1"
      },
      {
        "authorType": "User",
        "author_name": "jiuqiant",
        "body": "*  Support Bazel 2 and 3. MediaPipe will no longer support Bazel 0.x and 1.x.\r\n*  MediaPipe's TensorFlow dependency is now pointing to https://github.com/tensorflow/tensorflow/commit/805e47cea96c7e8c6fccf494d40a2392dc99fdd8, which is submitted on April 1, 2020. \r\n*  MediaPipe's Protobuf dependency is now pointing to [protobuf-3.11.4](https://github.com/protocolbuffers/protobuf/releases/tag/v3.11.4).\r\n*  Solve Xlib's Status & absl::Status compilation issue on linux.\r\n*  Fix setup_opencv.sh.\r\n*  Fix media sequence documentation by following the suggestions in https://github.com/google/mediapipe/issues/566.\r\n",
        "dateCreated": "2020-04-07T04:28:19Z",
        "datePublished": "2020-04-07T17:27:06Z",
        "html_url": "https://github.com/google/mediapipe/releases/tag/v0.7.2",
        "name": "Bazel 2&3 Support and Dependency Bump",
        "tag_name": "v0.7.2",
        "tarball_url": "https://api.github.com/repos/google/mediapipe/tarball/v0.7.2",
        "url": "https://api.github.com/repos/google/mediapipe/releases/25289153",
        "zipball_url": "https://api.github.com/repos/google/mediapipe/zipball/v0.7.2"
      },
      {
        "authorType": "User",
        "author_name": "mgyong",
        "body": "*   CPU acceleration with [XNNPack](https://github.com/google/XNNPACK)\r\n*   Fix iOS memory leak issue: https://github.com/google/mediapipe/issues/387\r\n*   Support detection proto in MediaPipe AAR\r\n*   Add hair segmentation desktop live demo\r\n*   [Google Open Source code search for MediaPipe Github repository](https://cs.opensource.google/mediapipe/mediapipe)\r\n",
        "dateCreated": "2020-03-20T22:28:51Z",
        "datePublished": "2020-03-24T20:56:34Z",
        "html_url": "https://github.com/google/mediapipe/releases/tag/v0.7.1",
        "name": "CPU acceleration with XNNPack, Google Open Source code search",
        "tag_name": "v0.7.1",
        "tarball_url": "https://api.github.com/repos/google/mediapipe/tarball/v0.7.1",
        "url": "https://api.github.com/repos/google/mediapipe/releases/24827071",
        "zipball_url": "https://api.github.com/repos/google/mediapipe/zipball/v0.7.1"
      },
      {
        "authorType": "User",
        "author_name": "mgyong",
        "body": "*   [MediaPipe Objectron](https://solutions.mediapipe.dev/objectron) (3D object detection and tracking using only RGB images)\r\n      *  [Objectron calculators](https://github.com/google/mediapipe/tree/master/mediapipe/graphs/object_detection_3d/calculators)\r\n      *  See also [Google AI Blog post](https://ai.googleblog.com/2020/03/real-time-3d-object-detection-on-mobile.html)\r\n*   Bug fixes \r\n      *  Update Coral dependences to match v0.6.9 release\r\n",
        "dateCreated": "2020-03-11T01:14:25Z",
        "datePublished": "2020-03-11T02:37:14Z",
        "html_url": "https://github.com/google/mediapipe/releases/tag/v0.7.0",
        "name": "MediaPipe Objectron (3D Object Detection)",
        "tag_name": "v0.7.0",
        "tarball_url": "https://api.github.com/repos/google/mediapipe/tarball/v0.7.0",
        "url": "https://api.github.com/repos/google/mediapipe/releases/24410314",
        "zipball_url": "https://api.github.com/repos/google/mediapipe/zipball/v0.7.0"
      },
      {
        "authorType": "User",
        "author_name": "jiuqiant",
        "body": "## v0.6.9 - MediaPipe Dependency Bump\r\n\r\n*   MediaPipe's Abseil dependency is now pointing to [abseil's LTS_2020_02_25 release](https://github.com/abseil/abseil-cpp/releases/tag/20200225).\r\n*   MediaPipe is now switched to [absl::Status](https://github.com/abseil/abseil-cpp/tree/master/absl/status).\r\n*   MediaPipe's TensorFlow version is now pointing to [commit 77e9ffb9b2bfb1a4f7056e62d84039626923e328](https://github.com/tensorflow/tensorflow/commit/77e9ffb9b2bfb1a4f7056e62d84039626923e328), which is submitted on Feb 12, 2020.\r\n*   The range of Bazel version that MediaPipe supports is now narrowed. We support Bazel from v1.0.0 to v1.2.1.",
        "dateCreated": "2020-03-11T01:14:25Z",
        "datePublished": "2020-03-02T18:50:00Z",
        "html_url": "https://github.com/google/mediapipe/releases/tag/v0.6.9",
        "name": "MediaPipe v0.6.9 - MediaPipe Dependency Bump",
        "tag_name": "v0.6.9",
        "tarball_url": "https://api.github.com/repos/google/mediapipe/tarball/v0.6.9",
        "url": "https://api.github.com/repos/google/mediapipe/releases/24142600",
        "zipball_url": "https://api.github.com/repos/google/mediapipe/zipball/v0.6.9"
      },
      {
        "authorType": "User",
        "author_name": "mgyong",
        "body": "## v0.6.8.1 - MediaPipe web demos, YouTube Channel, new calculators\r\n\r\n*   improved android camera support (camera properties including).  \r\n*   subgraph node and stream naming improvement\r\n*   bug fix for data race in SimulationClockExecutor \r\n*   concurrency macros update for ABSL (GUARDED_BY --> ABSL_GUARDED_BY)\r\n*   image_cropping_calculator improvement.\r\n*   new utility calculator, ConstantSidePacketCalculato\r\n*   Web demos \r\n     *  [Edge Detection](https://viz.mediapipe.dev/runner/demos/edge_detection/edge_detection.html)\r\n     *  [Face Detection](https://viz.mediapipe.dev/runner/demos/face_detection/face_detection.html)\r\n     *  [Hands Tracking](https://viz.mediapipe.dev/runner/demos/hand_tracking/hand_tracking.html)\r\n     *  [Hair Segmentation](https://viz.mediapipe.dev/runner/demos/hair_segmentation/hair_segmentation.html)\r\n*   Documentation\r\n     *  Corel example improvement\r\n     *  [YouTube channel](https://www.youtube.com/channel/UCObqmpuSMx-usADtL_qdMAw)\r\n     *  [MediaPipe on the Web - Google Dev Blog Post](https://developers.googleblog.com/2020/01/mediapipe-on-web.html)\r\n\r\n",
        "dateCreated": "2020-02-10T22:13:25Z",
        "datePublished": "2020-02-11T00:49:18Z",
        "html_url": "https://github.com/google/mediapipe/releases/tag/v0.6.8.1",
        "name": "MediaPipe web demos, YouTube Channel, new calculators",
        "tag_name": "v0.6.8.1",
        "tarball_url": "https://api.github.com/repos/google/mediapipe/tarball/v0.6.8.1",
        "url": "https://api.github.com/repos/google/mediapipe/releases/23552940",
        "zipball_url": "https://api.github.com/repos/google/mediapipe/zipball/v0.6.8.1"
      },
      {
        "authorType": "User",
        "author_name": "jiuqiant",
        "body": "### MediaPipe v0.6.8 - AutoFlip: Automatic video cropping desktop pipeline\r\n* [MediaPipe AutoFlip desktop example](https://github.com/google/mediapipe/tree/master/mediapipe/examples/desktop/autoflip)\r\n* Bazel maximum version check (version 1.2.1)\r\n* [Added way to extract detection protos from object detection android mobile GPU example](https://github.com/google/mediapipe/commit/ae6be10afe59a6a99d8a68007784706ac98720dd#diff-e2d9229283509a972282c1d03f41ec3d)\r\n* Bug fixes \r\n    * previous_loopback_calculator\r\n    * split_vector_calculator\r\n    * scale_image_calculator\r\n* Documentation\r\n* [MediaPipe AutoFlip documentation](https://github.com/google/mediapipe/blob/master/mediapipe/docs/autoflip.md)\r\n* Fixes for [framework concept](https://github.com/google/mediapipe/commit/ae6be10afe59a6a99d8a68007784706ac98720dd#diff-31805dad11a7299bfc5fa68e64a3611a), [coral face](https://github.com/google/mediapipe/commit/ae6be10afe59a6a99d8a68007784706ac98720dd#diff-44a9f2c7d5adb5e036037bc747a4365a) and [object detection](https://github.com/google/mediapipe/commit/ae6be10afe59a6a99d8a68007784706ac98720dd#diff-dab963cc487732c4c0640be442119e2c) docs\r\n",
        "dateCreated": "2020-01-10T21:13:24Z",
        "datePublished": "2020-01-10T23:55:39Z",
        "html_url": "https://github.com/google/mediapipe/releases/tag/v0.6.8",
        "name": "MediaPipe v0.6.8 - AutoFlip: Automatic video cropping desktop pipeline",
        "tag_name": "v0.6.8",
        "tarball_url": "https://api.github.com/repos/google/mediapipe/tarball/v0.6.8",
        "url": "https://api.github.com/repos/google/mediapipe/releases/22757838",
        "zipball_url": "https://api.github.com/repos/google/mediapipe/zipball/v0.6.8"
      },
      {
        "authorType": "User",
        "author_name": "mgyong",
        "body": "## MediaPipe v0.6.7.1 - Tracking library and Object Detection & Tracking example\r\n* [MediaPipe Tracking library](https://github.com/google/mediapipe/tree/master/mediapipe/util/tracking)\r\n* MediaPipe Object Detection (GPU) & Tracking example [android](https://github.com/google/mediapipe/blob/master/mediapipe/examples/android/src/java/com/google/mediapipe/apps/multihandtrackinggpu/MainActivity.java) \r\n* Bug fixes \r\n    *  (Multi)handtracking demo crashes on Mali and PowerVR GPUs [android]\r\n    *  iOS fixes - better documentation (https://github.com/google/mediapipe/issues/25, https://github.com/google/mediapipe/issues/135), iOS crash (https://github.com/google/mediapipe/issues/145)\r\n* .Documentation\r\n    *  [MediaPipe Object Detection (GPU) & Tracking](https://github.com/google/mediapipe/blob/master/mediapipe/docs/object_tracking_mobile_gpu.md)\r\n    *  [Google Developer MediaPipe Object Tracking blog post](https://mediapipe.page.link/objecttrackingblog)\r\n\r\n\r\n",
        "dateCreated": "2019-12-09T21:11:22Z",
        "datePublished": "2019-12-09T21:52:23Z",
        "html_url": "https://github.com/google/mediapipe/releases/tag/v0.6.7.1",
        "name": "MediaPipe v0.6.7.1 - Tracking library and Object Detection & Tracking example",
        "tag_name": "v0.6.7.1",
        "tarball_url": "https://api.github.com/repos/google/mediapipe/tarball/v0.6.7.1",
        "url": "https://api.github.com/repos/google/mediapipe/releases/22090601",
        "zipball_url": "https://api.github.com/repos/google/mediapipe/zipball/v0.6.7.1"
      },
      {
        "authorType": "User",
        "author_name": "mgyong",
        "body": "## MediaPipe v0.6.6 - Obj C and Java API for vector<proto> & handtrackinggpu AAR\r\n* Multihands tracking example to get hand landmarks\r\n     * [android](https://github.com/google/mediapipe/blob/master/mediapipe/examples/android/src/java/com/google/mediapipe/apps/multihandtrackinggpu/MainActivity.java) \r\n     *   [ios](https://github.com/google/mediapipe/blob/master/mediapipe/examples/ios/multihandtrackinggpu/ViewController.mm)\r\n* Hands tracking example to get hand landmarks\r\n     * [android](https://github.com/google/mediapipe/blob/master/mediapipe/examples/android/src/java/com/google/mediapipe/apps/handtrackinggpu/MainActivity.java)                 \r\n     *   [ios](https://github.com/google/mediapipe/blob/master/mediapipe/examples/ios/handtrackinggpu/ViewController.mm)\r\n* [Multi-hand Tracking AAR Example](https://github.com/jiuqiant/mediapipe_multi_hands_tracking_aar_example)\r\n* Add basic audio support to opencv video decoder and encoder calculator\r\n* Add NNAPI inference support to TfLiteInferenceCalculator on Android.\r\n* Bug fixes \r\n    *  Switch to Abseil LTS version [issue 264](https://github.com/google/mediapipe/issues/264)\r\n    *  Downgrade to use Bazel 1.1.0 on macOS\r\n    *  Fix a docker setup issue [issue 276](https://github.com/google/mediapipe/issues/276) \r\n    *  Access Hand landmarks in Android & iOS issues [219](https://github.com/google/mediapipe/issues/219), [230](https://github.com/google/mediapipe/issues/230), [79](https://github.com/google/mediapipe/issues/79), [64](https://github.com/google/mediapipe/issues/64), [237](https://github.com/google/mediapipe/issues/237)",
        "dateCreated": "2019-12-03T01:54:10Z",
        "datePublished": "2019-12-03T06:17:08Z",
        "html_url": "https://github.com/google/mediapipe/releases/tag/v0.6.6",
        "name": "MediaPipe v0.6.6 - Obj C and Java API for vector<proto> & handtrackinggpu AAR",
        "tag_name": "v0.6.6",
        "tarball_url": "https://api.github.com/repos/google/mediapipe/tarball/v0.6.6",
        "url": "https://api.github.com/repos/google/mediapipe/releases/21918122",
        "zipball_url": "https://api.github.com/repos/google/mediapipe/zipball/v0.6.6"
      },
      {
        "authorType": "User",
        "author_name": "jiuqiant",
        "body": "* MediaPipe on Coral dev board EdgeTPU\r\n    *  Dockerfile for cross compiling\r\n    *  [Example including quantized models & graphs for Object detection & Face detection](https://github.com/google/mediapipe/blob/master/mediapipe/docs/examples.md#google-coral-machine-learning-acceleration-with-google-edgetpu)\r\n* Bug fixes\r\n    *  iOS Tulsi setup bug\r\n    *  TFLite OpenGL ES delegate fix integration that fixes [issue 219](https://github.com/google/mediapipe/issues/219), [issue 232](https://github.com/google/mediapipe/issues/232), [issue 104](https://github.com/google/mediapipe/issues/104), [issue 87](https://github.com/google/mediapipe/issues/87), and [issue 41](https://github.com/google/mediapipe/issues/41)\r\n* Documentation\r\n    *  [Setup MediaPipe on Coral dev board Edge TPU](https://github.com/google/mediapipe/blob/master/mediapipe/examples/coral/README.md)\r\n    *  [Coral Object detection example](https://github.com/google/mediapipe/blob/master/mediapipe/docs/object_detection_coral_devboard.md)\r\n    *  [Coral Face detection example](https://github.com/google/mediapipe/blob/master/mediapipe/docs/face_detection_coral_devboard.md)\r\n",
        "dateCreated": "2019-11-22T01:34:09Z",
        "datePublished": "2019-11-22T18:19:28Z",
        "html_url": "https://github.com/google/mediapipe/releases/tag/v0.6.5",
        "name": "MediaPipe v0.6.5 - MediaPipe Coral setup and examples",
        "tag_name": "v0.6.5",
        "tarball_url": "https://api.github.com/repos/google/mediapipe/tarball/v0.6.5",
        "url": "https://api.github.com/repos/google/mediapipe/releases/21694392",
        "zipball_url": "https://api.github.com/repos/google/mediapipe/zipball/v0.6.5"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 15559,
      "date": "Wed, 29 Dec 2021 14:33:34 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "mediapipe",
      "c-plus-plus",
      "computer-vision",
      "deep-learning",
      "android",
      "video-processing",
      "audio-processing",
      "mobile-development",
      "machine-learning",
      "inference",
      "graph-framework",
      "graph-based",
      "calculator",
      "framework",
      "pipeline-framework",
      "stream-processing",
      "perception"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To start using MediaPipe\n[solutions](https://google.github.io/mediapipe/solutions/solutions) with only a few\nlines code, see example code and demos in\n[MediaPipe in Python](https://google.github.io/mediapipe/getting_started/python) and\n[MediaPipe in JavaScript](https://google.github.io/mediapipe/getting_started/javascript).\n\nTo use MediaPipe in C++, Android and iOS, which allow further customization of\nthe [solutions](https://google.github.io/mediapipe/solutions/solutions) as well as\nbuilding your own, learn how to\n[install](https://google.github.io/mediapipe/getting_started/install) MediaPipe and\nstart building example applications in\n[C++](https://google.github.io/mediapipe/getting_started/cpp),\n[Android](https://google.github.io/mediapipe/getting_started/android) and\n[iOS](https://google.github.io/mediapipe/getting_started/ios).\n\nThe source code is hosted in the\n[MediaPipe Github repository](https://github.com/google/mediapipe), and you can\nrun code search using\n[Google Open Source Code Search](https://cs.opensource.google/mediapipe/mediapipe).\n\n",
      "technique": "Header extraction"
    }
  ]
}