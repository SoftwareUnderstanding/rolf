{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1602.01783",
      "https://arxiv.org/abs/1611.01224",
      "https://arxiv.org/abs/1602.01783",
      "https://arxiv.org/abs/1707.06887",
      "https://arxiv.org/abs/1509.06461",
      "https://arxiv.org/abs/1512.04860",
      "https://arxiv.org/abs/1509.02971",
      "https://arxiv.org/abs/1510.09142",
      "https://arxiv.org/abs/1806.06923",
      "https://arxiv.org/abs/1702.08892",
      "https://arxiv.org/abs/1707.06347",
      "https://arxiv.org/abs/1710.02298",
      "https://arxiv.org/abs/1812.05905",
      "https://arxiv.org/abs/1502.05477",
      "https://arxiv.org/abs/1506.02438",
      "https://arxiv.org/abs/1802.09477",
      "https://arxiv.org/abs/1706.10295",
      "https://arxiv.org/abs/1511.05952",
      "https://arxiv.org/abs/1511.06581",
      "https://arxiv.org/abs/1603.00748",
      "https://arxiv.org/abs/1507.06527"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To cite ChainerRL in publications, please cite our [JMLR paper](https://www.jmlr.org/papers/v22/20-376.html):\n\n```\n@article{JMLR:v22:20-376,\n  author  = {Yasuhiro Fujita and Prabhat Nagarajan and Toshiki Kataoka and Takahiro Ishikawa},\n  title   = {ChainerRL: A Deep Reinforcement Learning Library},\n  journal = {Journal of Machine Learning Research},\n  year    = {2021},\n  volume  = {22},\n  number  = {77},\n  pages   = {1-14},\n  url     = {http://jmlr.org/papers/v22/20-376.html}\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{JMLR:v22:20-376,\n  author  = {Yasuhiro Fujita and Prabhat Nagarajan and Toshiki Kataoka and Takahiro Ishikawa},\n  title   = {ChainerRL: A Deep Reinforcement Learning Library},\n  journal = {Journal of Machine Learning Research},\n  year    = {2021},\n  volume  = {22},\n  number  = {77},\n  pages   = {1-14},\n  url     = {http://jmlr.org/papers/v22/20-376.html}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/chainer/chainerrl",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contributing to ChainerRL\nAny kind of contribution to ChainerRL would be highly appreciated!\nContribution examples:\n- Thumbing up to good issues or pull requests :+1:\n- Opening issues about questions, bugs, installation problems, feature requests, algorithm requests etc.\n- Sending pull requests\nIf you could kindly send a PR to ChainerRL, please make sure all the tests successfully pass.\nWhen contributing to ChainerRL, we highly recommend installing the additional set of developer requirements, which may be necessary for testing:\npip install -r requirements-dev.txt\nTesting\nTo test chainerrl modules, install and run pytest (after installing pytest). Pass -m \"not gpu\" to skip tests that require gpu. E.g.\n$ pytest -m \"not gpu\"\nTo test examples, run test_examples.sh [gpu device id]. -1 would run examples with only cpu.\nCoding style\nWe use PEP8. To check your code, use autopep8 and flake8 packages.\n$ pip install autopep8 flake8\n$ autopep8 --diff path/to/your/code.py\n$ flake8 path/to/your/code.py",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-01-30T04:58:15Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-23T11:46:08Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9915833907546049
      ],
      "excerpt": "ChainerRL (this repository) is a deep reinforcement learning library that implements various state-of-the-art deep reinforcement algorithms in Python using Chainer, a flexible deep learning framework. PFRL is the PyTorch analog of ChainerRL. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9968029537584643
      ],
      "excerpt": "- A2C (Synchronous variant of A3C) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9567588029116127
      ],
      "excerpt": "- ACER (Actor-Critic with Experience Replay) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8434893976312938
      ],
      "excerpt": "- TRPO (Trust Region Policy Optimization) with GAE (Generalized Advantage Estimation) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9775861334117679,
        0.8516801605769577
      ],
      "excerpt": "ChainerRL has a set of accompanying visualization tools in order to aid developers' ability to understand and debug their RL agents. With this visualization tool, the behavior of ChainerRL agents can be easily inspected from a browser UI. \nEnvironments that support the subset of OpenAI Gym's interface (reset and step methods) can be used. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "ChainerRL is a deep reinforcement learning library built on top of Chainer.",
      "technique": "GitHub API"
    }
  ],
  "documentation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "http://chainerrl.readthedocs.io/",
      "technique": "Regular expression"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/chainer/chainerrl/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 218,
      "date": "Tue, 28 Dec 2021 02:08:58 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/chainer/chainerrl/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "chainer/chainerrl",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/chainer/chainerrl/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/chainer/chainerrl/master/examples/quickstart/quickstart.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/chainer/chainerrl/master/test_examples.sh",
      "https://raw.githubusercontent.com/chainer/chainerrl/master/.pfnci/script.sh",
      "https://raw.githubusercontent.com/chainer/chainerrl/master/.pfnci/run.sh",
      "https://raw.githubusercontent.com/chainer/chainerrl/master/examples_tests/atari/test_double_iqn.sh",
      "https://raw.githubusercontent.com/chainer/chainerrl/master/examples_tests/atari/test_a3c.sh",
      "https://raw.githubusercontent.com/chainer/chainerrl/master/examples_tests/atari/test_dqn_batch.sh",
      "https://raw.githubusercontent.com/chainer/chainerrl/master/examples_tests/atari/test_drqn.sh",
      "https://raw.githubusercontent.com/chainer/chainerrl/master/examples_tests/atari/test_collect_demos.sh",
      "https://raw.githubusercontent.com/chainer/chainerrl/master/examples_tests/atari/test_categorical_dqn.sh",
      "https://raw.githubusercontent.com/chainer/chainerrl/master/examples_tests/atari/test_nsq.sh",
      "https://raw.githubusercontent.com/chainer/chainerrl/master/examples_tests/atari/test_ppo.sh",
      "https://raw.githubusercontent.com/chainer/chainerrl/master/examples_tests/atari/test_acer.sh",
      "https://raw.githubusercontent.com/chainer/chainerrl/master/examples_tests/atari/test_dqn.sh",
      "https://raw.githubusercontent.com/chainer/chainerrl/master/examples_tests/atari/test_a2c.sh",
      "https://raw.githubusercontent.com/chainer/chainerrl/master/examples_tests/atari/reproduction/test_iqn.sh",
      "https://raw.githubusercontent.com/chainer/chainerrl/master/examples_tests/atari/reproduction/test_rainbow.sh",
      "https://raw.githubusercontent.com/chainer/chainerrl/master/examples_tests/atari/reproduction/test_dqn.sh",
      "https://raw.githubusercontent.com/chainer/chainerrl/master/examples_tests/grasping/test_dqn.sh",
      "https://raw.githubusercontent.com/chainer/chainerrl/master/examples_tests/gym/test_iqn.sh",
      "https://raw.githubusercontent.com/chainer/chainerrl/master/examples_tests/gym/test_a3c.sh",
      "https://raw.githubusercontent.com/chainer/chainerrl/master/examples_tests/gym/test_reinforce.sh",
      "https://raw.githubusercontent.com/chainer/chainerrl/master/examples_tests/gym/test_categorical_dqn.sh",
      "https://raw.githubusercontent.com/chainer/chainerrl/master/examples_tests/gym/test_acer.sh",
      "https://raw.githubusercontent.com/chainer/chainerrl/master/examples_tests/gym/test_pcl.sh",
      "https://raw.githubusercontent.com/chainer/chainerrl/master/examples_tests/gym/test_dqn.sh",
      "https://raw.githubusercontent.com/chainer/chainerrl/master/examples_tests/gym/test_a2c.sh",
      "https://raw.githubusercontent.com/chainer/chainerrl/master/examples_tests/mujoco/test_ddpg_batch.sh",
      "https://raw.githubusercontent.com/chainer/chainerrl/master/examples_tests/mujoco/test_trpo.sh",
      "https://raw.githubusercontent.com/chainer/chainerrl/master/examples_tests/mujoco/test_ddpg.sh",
      "https://raw.githubusercontent.com/chainer/chainerrl/master/examples_tests/mujoco/test_ppo.sh",
      "https://raw.githubusercontent.com/chainer/chainerrl/master/examples_tests/mujoco/test_ppo_batch.sh",
      "https://raw.githubusercontent.com/chainer/chainerrl/master/examples_tests/mujoco/reproduction/test_trpo.sh",
      "https://raw.githubusercontent.com/chainer/chainerrl/master/examples_tests/mujoco/reproduction/test_ddpg.sh",
      "https://raw.githubusercontent.com/chainer/chainerrl/master/examples_tests/mujoco/reproduction/test_td3.sh",
      "https://raw.githubusercontent.com/chainer/chainerrl/master/examples_tests/mujoco/reproduction/test_ppo.sh",
      "https://raw.githubusercontent.com/chainer/chainerrl/master/examples_tests/mujoco/reproduction/test_soft_actor_critic.sh",
      "https://raw.githubusercontent.com/chainer/chainerrl/master/examples_tests/atlas/test_soft_actor_critic.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "ChainerRL is tested with 3.6. For other requirements, see [requirements.txt](requirements.txt).\n\nChainerRL can be installed via PyPI:\n```\npip install chainerrl\n```\n\nIt can also be installed from the source code:\n```\npython setup.py install\n```\n\nRefer to [Installation](http://chainerrl.readthedocs.io/en/latest/install.html) for more information on installation. \n\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8327402992948221
      ],
      "excerpt": "  - examples: [atari] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8838148168639296
      ],
      "excerpt": "  - examples: [mujoco reproduction] [mujoco] [mujoco (batched)] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8327402992948221
      ],
      "excerpt": "  - examples: [mujoco reproduction] [atari] [mujoco] [mujoco (batched)] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8327402992948221
      ],
      "excerpt": "  - examples: [atari reproduction] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8838148168639296
      ],
      "excerpt": "  - examples: [mujoco reproduction] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8838148168639296
      ],
      "excerpt": "  - examples: [mujoco] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8838148168639296
      ],
      "excerpt": "  - examples: [mujoco reproduction] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8838148168639296
      ],
      "excerpt": "  - examples: [Rainbow] [DQN/DoubleDQN/PAL] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8838148168639296
      ],
      "excerpt": "  - examples: [Rainbow] [DQN/DoubleDQN/PAL] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8838148168639296
      ],
      "excerpt": "  - examples: [Rainbow] [DQN/DoubleDQN/PAL] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8838148168639296
      ],
      "excerpt": "  - examples: [DQN] \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/chainer/chainerrl/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2017 Preferred Networks, Inc.\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "ChainerRL and PFRL",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "chainerrl",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "chainer",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/chainer/chainerrl/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "muupan",
        "body": "# Announcement\r\n\r\nThis release will probably be the final major update under the name of ChainerRL. The development team is planning to switch its backend from Chainer to PyTorch and continue its development as OSS.\r\n\r\n# Important enhancements\r\n\r\n- Soft Actor-Critic (https://arxiv.org/abs/1812.05905) with benchmark results is added.\r\n  - Agent class: `chainerrl.agents.SoftActorCritic`\r\n  - Example and benchmark results (MuJoCo): https://github.com/chainer/chainerrl/tree/v0.8.0/examples/mujoco/reproduction/soft_actor_critic\r\n  - Example (Roboschool Atlas): https://github.com/chainer/chainerrl/tree/v0.8.0/examples/atlas\r\n- Trained models of benchmark results are now downloadable. See READMEs of examples.\r\n  - For Atari envs: DQN, IQN, Rainbow, A3C\r\n  - For MuJoCo envs: DDPG, PPO, TRPO, TD3, Soft Actor-Critic\r\n- DQN-based agents now support recurrent models in a new, more efficient interface.\r\n  - Example: https://github.com/chainer/chainerrl/tree/v0.8.0/examples/atari/train_drqn_ale.py\r\n- TRPO now supports recurrent models and batch training.\r\n- A variant of IQN with double Q-learning is added.\r\n  - Agent class: `chainerrl.agents.DoubleIQN`.\r\n  - Example: https://github.com/chainer/chainerrl/tree/v0.8.0/examples/atari/train_double_iqn.py\r\n- IQN now supports prioritized experience replay.\r\n\r\n\r\n# Important bugfixes\r\n\r\n- The bug that the update of `CategoricalDoubleDQN` is same as that of `CategoricalDQN` is fixed.\r\n- The bug that batch training with N-step or episodic replay buffers does not work is fixed.\r\n- The bug that weight normalization is `PrioritizedReplayBuffer` with `normalize_by_max == 'batch'` is wrong is fixed.\r\n\r\n# Important destructive changes\r\n\r\n- Support of Python 2 is dropped. ChainerRL is now only tested with Python 3.5.1+.\r\n- The interface of DQN-based agents to use recurrent models has changed. See the DRQN example: https://github.com/chainer/chainerrl/tree/v0.8.0/examples/atari/train_drqn_ale.py\r\n\r\n# All updates\r\n\r\n## Enhancements\r\n\r\n- Recurrent DQN families with a new interface (#436)\r\n- Recurrent and batched TRPO (#446)\r\n- Add Soft Actor-Critic agent (#457)\r\n- Code to collect demonstrations from an agent. (#468)\r\n- Monitor with ContinuingTimeLimit support (#491)\r\n- Fix B007: Loop control variable not used within the loop body (#502)\r\n- Double IQN (#503)\r\n- Fix B006: Do not use mutable data structures for argument defaults. (#504)\r\n- Splits Replay Buffers into separate files in a replay_buffers module (#506)\r\n- Use chainer.grad in ACER (#511)\r\n- Prioritized Double IQN (#518)\r\n- Add policy loss to TD3's logged statistics (#524)\r\n- Adds checkpoint frequencies for serial and batch Agents. (#525)\r\n- Add a deterministic mode to IQN for stable tests (#529)\r\n- Use Link.cleargrads instead of Link.zerograds in REINFORCE (#536)\r\n- Use cupyx.scatter_add instead of cupy.scatter_add (#537)\r\n- Avoid cupy.zeros_like with numpy.ndrray (#538)\r\n- Use get_device_from_id since get_device is deprecated (#539)\r\n- Releases trained models for all reproduced agents (#565)\r\n\r\n## Documentation\r\n\r\n- Typo fix in Replay Buffer Docs (#507)\r\n- Fixes typo in docstring for AsyncEvaluator (#508)\r\n- Improve the algorithm list on README (#509)\r\n- Add Explorers to Documentation (#514)\r\n- Fixes syntax errors in ReplayBuffer docs. (#515)\r\n- Adds policies to the documentation (#516)\r\n- Adds demonstration collection to experiments docs (#517)\r\n- Adds List of Batch Agents to the README (#543)\r\n- Add documentation for Q-functions and some missing details in docstrings (#556)\r\n- Add comment on environment version difference (#582)\r\n- Adds ChainerRL Bibtex to the README (#584)\r\n- Minor Typo Fix (#585)\r\n\r\n## Examples\r\n\r\n- Rename examples directories (#487)\r\n- Adds training times for reproduced Mujoco results (#497)\r\n- Adds additional information to Grasping Example README (#501)\r\n- Fixes a comment in PPO example (#521)\r\n- Rainbow Scores (#546)\r\n- Update train_a3c.py (#547, thanks @xinyuewang1!)\r\n- Update train_a3c.py (#548, thanks @xinyuewang1!)\r\n- Improves formatting of IQN training times (#549)\r\n- Corrects Scores in Examples (#552)\r\n- Removes GPU option from README (#564)\r\n- Releases trained models for all reproduced agents (#565)\r\n- Add an example script for RoboschoolAtlasForwardWalk-v1 (#577)\r\n- Corrects Rainbow Results (#580)\r\n- Adds proper A3C scores (#581)\r\n\r\n## Testing\r\n\r\n- Add CI configs (#478)\r\n- Specify ubuntu 16.04 for Travis CI and modify a dependency accordingly (#520)\r\n- Remove a tailing space of DoubleIQN (#526)\r\n- Add a deterministic mode to IQN for stable tests (#529)\r\n- Fix import error when chainer==7.0.0b3 (#531)\r\n- Make test_monitor.py work on flexCI (#533)\r\n- Improve parameter distributions used in TestGaussianDistribution (#540)\r\n- Increase flexCI's time limit to 20min (#550)\r\n- decrease amount of decimal digits required to 4 (#554)\r\n- Use attrs<19.2.0 with pytest (#569)\r\n- Run slow tests with flexCI (#575)\r\n- Typo fix in CI comment. (#576)\r\n- Adds time to DDPG Tests (#587)\r\n- Fix CI errors due to pyglet, zipp, mock, and gym (#592)\r\n\r\n## Bugfixes\r\n\r\n- Fix a bug in `batch_recurrent_experiences` regarding next_action (#528)\r\n- Fix ValueError in SARSA with GPU (#534)\r\n- fix function call (#541)\r\n- Pass env_id to replay_buffer methods to fix batch training (#558)\r\n- Fixes Categorical Double DQN Error. (#567)\r\n- Fix weight normalization inside prioritized experience replay (#570)\r\n",
        "dateCreated": "2020-02-14T05:28:52Z",
        "datePublished": "2020-02-14T05:32:03Z",
        "html_url": "https://github.com/chainer/chainerrl/releases/tag/v0.8.0",
        "name": "v0.8.0",
        "tag_name": "v0.8.0",
        "tarball_url": "https://api.github.com/repos/chainer/chainerrl/tarball/v0.8.0",
        "url": "https://api.github.com/repos/chainer/chainerrl/releases/23435337",
        "zipball_url": "https://api.github.com/repos/chainer/chainerrl/zipball/v0.8.0"
      },
      {
        "authorType": "User",
        "author_name": "muupan",
        "body": "# Important enhancements\r\n\r\n- Rainbow (https://arxiv.org/abs/1710.02298) with benchmark results is added. (thanks @seann999!)\r\n  - Agent class: `chainerrl.agents.CategoricalDoubleDQN`\r\n  - Example and benchmark results: https://github.com/chainer/chainerrl/tree/v0.7.0/examples/atari/rainbow\r\n- TD3 (https://arxiv.org/abs/1802.09477) with benchmark results is added.\r\n  - Agent class: `chainerrl.agents.TD3`\r\n  - Example and benchmark results: https://github.com/chainer/chainerrl/tree/v0.7.0/examples/mujoco/td3\r\n- PPO now supports recurrent models.\r\n  - Example: https://github.com/chainer/chainerrl/tree/v0.7.0/examples/ale/train_ppo_ale.py (with `--recurrent` option)\r\n  - Results: https://github.com/chainer/chainerrl/pull/431\r\n- DDPG now supports batch training\r\n  - Example: https://github.com/chainer/chainerrl/tree/v0.7.0/examples/gym/train_ddpg_batch_gym.py\r\n\r\n# Important bugfixes\r\n\r\n- The bug that some examples use the same random seed across envs for `env.seed` is fixed.\r\n- The bug that batch training with n-step return and/or recurrent models is not successful is fixed.\r\n- The bug that `examples/ale/train_dqn_ale.py` uses `LinearDecayEpsilonGreedy` even when NoisyNet is used is fixed.\r\n- The bug that `examples/ale/train_dqn_ale.py` does not use the value specified by `--noisy-net-sigma` is fixed.\r\n- The bug that `chainerrl.links.to_factorized_noisy` does not work correctly with `chainerrl.links.Sequence` is fixed.\r\n\r\n# Important destructive changes\r\n\r\n- `chainerrl.experiments.train_agent_async` now requires `eval_n_steps` (number of timesteps for each evaluation phase) and `eval_n_episodes` (number of episodes for each evaluation phase) to be explicitly specified, with one of them being None.\r\n- `examples/ale/dqn_phi.py` is removed.\r\n- `chainerrl.initializers.LeCunNormal` is removed. Use `chainer.initializers.LeCunNormal` instead.\r\n\r\n# All updates\r\n\r\n## Enhancement\r\n- Rainbow (#374)\r\n- Make copy_param support scalar parameters (#410)\r\n- Enables batch DDPG agents to be trained. (#416)\r\n- Enables asynchronous time-based evaluations of agents. (#420)\r\n- Removes obsolete dqn_phi file (#424)\r\n- Add Branched and use it to simplify train_ppo_batch_gym.py (#427)\r\n- Remove LeCunNormal since Chainer has it from v3 (#428)\r\n- Precompute log probability in PPO (#430)\r\n- Recurrent PPO with a stateless recurrent model interface (#431)\r\n- Replace Variable.data with Variable.array (again) (#434)\r\n- Make IQN work with tuple observations (#435)\r\n- Add VectorStackFrame to reduce memory usage in train_dqn_batch_ale.py (#443)\r\n- DDPG example that reproduces the TD3 paper (#452)\r\n- TD3 agent (#453)\r\n- update requirements.txt and setup.py for gym (#461)\r\n- Support `gym>=0.12.2` by stopping to use underscore methods in gym wrappers (#462)\r\n- Add warning about numpy 1.16.0 (#476)\r\n\r\n## Documentation\r\n- Link to abstract pages on ArXiv (#409)\r\n- fixes typo (#412)\r\n- Fixes file path in grasping example README (#422)\r\n- Add links to references (#425)\r\n- Fixes minor grammar mistake in A3C ALE example (#432)\r\n- Add explanation of `examples/atari` (#437)\r\n- Link to chainer/chainer, not pfnet/chainer (#439)\r\n- Link to chainer/chainer(rl), not pfnet/chainer(rl) (#440)\r\n- fix & add docstring for FCStateQFunctionWithDiscreteAction (#441)\r\n- Fixes a typo in train_agent_batch Documentation. (#444)\r\n- Adds Rainbow to main README (#447)\r\n- Fixes Docstring in IQN (#451)\r\n- Improves Rainbow README (#458)\r\n- very small fix: add missing doc for eval_performance. (#459)\r\n- Adds IQN Results to readme (#469)\r\n- Adds IQN to the documentation. (#470)\r\n- Adds reference to mujoco folder in the examples README (#474)\r\n- Fixes incorrect comment. (#490)\r\n\r\n## Examples\r\n- Rainbow (#374)\r\n- Create an IQN example aimed at reproducing the original paper and its evaluation protocol. (#408)\r\n- Benchmarks DQN example (#414)\r\n- Enables batch DDPG agents to be trained. (#416)\r\n- Fixes scores for Demon Attack (#418)\r\n- Set observation_space of kuka env correctly (#421)\r\n- Fixes error in setting explorer in DQN ALE example. (#423)\r\n- Add Branched and use it to simplify train_ppo_batch_gym.py (#427)\r\n- A3C Example for reproducing paper results. (#433)\r\n- PPO example that reproduces the \"Deep Reinforcement Learning that Matters\" paper (#448)\r\n- DDPG example that reproduces the TD3 paper (#452)\r\n- TD3 agent (#453)\r\n- Apply `noisy_net_sigma` parameter (#465)\r\n\r\n## Testing\r\n- Use Python 3.6 in Travis CI (#411)\r\n- Increase tolerance of TestGaussianDistribution.test_entropy since sometimes it failed (#438)\r\n- make FrameStack follow original spaces (#445)\r\n- Split test_examples.sh (#472)\r\n- Fix Travis error (#492)\r\n- Use Python 3.6 for ipynb (#493)\r\n\r\n## Bugfixes\r\n- bugfix (#360, thanks @corochann!)\r\n- Fixes error in setting explorer in DQN ALE example. (#423)\r\n- Make sure the agent sees when episodes end (#429)\r\n- Pass env_id to replay buffer methods to correctly support batch training (#442)\r\n- Add VectorStackFrame to reduce memory usage in train_dqn_batch_ale.py (#443)\r\n- Fix a bug of unintentionally using same process indices (#455)\r\n- Make cv2 dependency optional (#456)\r\n- fix ScaledFloatFrame.observation_space (#460)\r\n- Apply `noisy_net_sigma` parameter (#465)\r\n- Match EpisodicReplayBuffer.sample with ReplayBuffer.sample (#485)\r\n- Make `to_factorized_noisy` work with sequential links (#489)\r\n",
        "dateCreated": "2019-06-28T08:54:18Z",
        "datePublished": "2019-06-28T09:47:14Z",
        "html_url": "https://github.com/chainer/chainerrl/releases/tag/v0.7.0",
        "name": "v0.7.0",
        "tag_name": "v0.7.0",
        "tarball_url": "https://api.github.com/repos/chainer/chainerrl/tarball/v0.7.0",
        "url": "https://api.github.com/repos/chainer/chainerrl/releases/18285234",
        "zipball_url": "https://api.github.com/repos/chainer/chainerrl/zipball/v0.7.0"
      },
      {
        "authorType": "User",
        "author_name": "muupan",
        "body": "# Important enhancements\r\n\r\n- Implicit Quantile Network (IQN) https://arxiv.org/abs/1806.06923 agent is added: `chainerrl.agents.IQN`.\r\n- Training DQN and its variants with N-step returns is supported.\r\n- Resetting env with `done=False` via `info` dict is supported. When `env.step` returns a `info` dict with `info['needs_reset']=True`, env is reset. This feature is useful for implementing a continuing env.\r\n- Evaluation with a fixed number of timesteps is supported (except async training). This evaluation protocol is popular in Atari benchmarks.\r\n  - `examples/atari/dqn` now implements the same evaluation protocol as the Nature DQN paper.\r\n- An example script of training a DoubleDQN agent for a PyBullet-based robotic grasping env is added: `examples/grasping`.\r\n\r\n# Important bugfixes\r\n- The bug that PPO's `obs_normalizer` was not saved is fixed.\r\n- The bug that NonbiasWeightDecay didn't work with newer versions of Chainer is fixed.\r\n- The bug that `argv` argument was ignored by `chainerrl.experiments.prepare_output_dir` is fixed.\r\n\r\n# Important destructive changes\r\n\r\n- `train_agent_with_evaluation` and `train_agent_batch_with_evaluation` now require `eval_n_steps` (number of timesteps for each evaluation phase) and `eval_n_episodes` (number of episodes for each evaluation phase) to be explicitly specified, with one of them being `None`.\r\n- `train_agent_with_evaluation`'s `max_episode_len` argument is renamed to `train_max_episode_len`.\r\n- `ReplayBuffer.sample` now returns a list of lists of N experiences to support N-step returns.\r\n\r\n# All updates\r\n\r\n## Enhancement\r\n- Implicit quantile networks (IQN) (#288)\r\n- Adds N-step learning for DQN-based agents. (#317)\r\n- Replaywarning (#321)\r\n- Close envs in async training (#343)\r\n- Allow envs to send a 'needs_reset' signal (#356)\r\n- Changes variable names in train_agent_with_evaluation (#358)\r\n- Use chainer.dataset.concat_examples in batch_states (#366)\r\n- Implements Time-based evaluations (#367)\r\n\r\n## Documentation\r\n- Add long description for pypi (#357, thanks @ljvmiranda921!)\r\n- A small change to the installation documentation (#369)\r\n- Adds a link to the ChainerRL visualizer from the main repository (#370)\r\n- adds implicit quantile networks to readme (#393)\r\n- Fix DQN.update's docstring (#394)\r\n\r\n## Examples\r\n- Grasping example (#371)\r\n- Adds Deepmind Scores to README in DQN Example (#383)\r\n\r\n## Testing\r\n- Fix `TestTrainAgentAsync` (#363)\r\n- Use AbnormalExitCodeWarning for nonzero exitcode warnings (#378)\r\n- Avoid random test failures due to asynchronousness (#380)\r\n- Drop hacking (#381)\r\n- Avoid gym 0.11.0 in Travis (#396)\r\n- Stabilize and speed up A3C tests (#401)\r\n- Reduce ACER's test cases and maximum timesteps (#404)\r\n- Add tests of IQN examples (#405)\r\n\r\n## Bugfixes\r\n- Avoid UnicodeDecodeError in setup.py (#365)\r\n- Save and load obs_normalizer of PPO (#377)\r\n- Make NonbiasWeightDecay work again (#390)\r\n- bug fix (#391, thanks @tappy27!)\r\n- Fix episodic training of DDPG (#399)\r\n- Fix PGT's training (#400)\r\n- Fix ResidualDQN's training (#402)",
        "dateCreated": "2019-02-28T08:49:28Z",
        "datePublished": "2019-02-28T08:50:51Z",
        "html_url": "https://github.com/chainer/chainerrl/releases/tag/v0.6.0",
        "name": "v0.6.0",
        "tag_name": "v0.6.0",
        "tarball_url": "https://api.github.com/repos/chainer/chainerrl/tarball/v0.6.0",
        "url": "https://api.github.com/repos/chainer/chainerrl/releases/15825658",
        "zipball_url": "https://api.github.com/repos/chainer/chainerrl/zipball/v0.6.0"
      },
      {
        "authorType": "User",
        "author_name": "muupan",
        "body": "# Important enhancements\r\n\r\n- Batch synchronized training using multiple environment instances and a single GPU is supported for some agents:\r\n  - A2C (added as `chainerrl.agents.A2C`)\r\n  - PPO\r\n  - DQN and other agents that inherits DQN except SARSA\r\n- `examples/ale/train_dqn_ale.py` now follows \"Tuned DoubleDQN\" setting by default, and supports prioritized experience replay as an option\r\n- `examples/atari/train_dqn.py` is added as a basic example of applying DQN to Atari.\r\n\r\n# Important bugfixes\r\n\r\n- A bug in `chainerrl.agents.CategoricalDQN` that deteriorates performance is fixed\r\n- A bug in `atari_wrappers.LazyFrame` that unnecessarily increases memory usage is fixed\r\n\r\n# Important destructive changes\r\n\r\n- `chainerrl.replay_buffer.PrioritizedReplayBuffer` and `chainerrl.replay_buffer.PrioritizedEpisodicReplayBuffer` are updated:\r\n  - become FIFO (First In, First Out), reducing memory usage in Atari games\r\n  - compute priorities more closely following the paper\r\n- `eval_explorer` argument of `chainerrl.experiments.train_agent_*` is dropped (use `chainerrl.wrappers.RandomizeAction` for evaluation-time epsilon-greedy)\r\n- Interface of `chainerrl.agents.PPO` has changed a lot\r\n- Support of Chainer v2 is dropped\r\n- Support of gym<0.9.7 is dropped \r\n- Support of loading chainerrl<=0.2.0's replay buffer is dropped\r\n\r\n# All updates\r\n\r\n## Enhancement\r\n- A2C (#149, thanks @iory!)\r\n- Add wrappers to cast observations (#160)\r\n- Fix on flake8 3.5.0 (#214)\r\n- Use ()-shaped array for scalar loss (#219)\r\n- FIFO prioritized replay buffer (#277)\r\n- Update Policy class to inherit ABCMeta (#280, thanks @uidilr!)\r\n- Batch PPO Implementation (#295, thanks @ljvmiranda921!)\r\n- Mimic the details of prioritized experience replay (#301)\r\n- Add ScaleReward wrapper (#304)\r\n- Remove GaussianPolicy and obsolete policies (#305)\r\n- Make random access queue sampling code cleaner (#309)\r\n- Support gym==0.10.8 (#324)\r\n- Batch A2C/PPO/DQN (#326)\r\n- Use RandomizeAction wrapper instead of Explorer in evaluation (#328)\r\n- remove duplicate lines (typo) (#329, thanks @monado3!)\r\n- Merge consecutive with statements (#333)\r\n- Use Variable.array instead of Variable.data (#336)\r\n- Remove code for Chainer v2 (#337)\r\n- Implement __getitem__ for ActionValue (#339)\r\n- Count updates of DQN (#341)\r\n- Move Atari Wrappers (#349)\r\n- Render wrapper (#350)\r\n\r\n## Documentation\r\n- fixes minor typos (#306)\r\n- fixes typo (#307)\r\n- Typos (#308)\r\n- fixes readme typo (#310)\r\n- Adds partial list of paper implementations with links to the main README (#311)\r\n- Adds another paper to list (#312)\r\n- adds some instructions regarding testing for potential contributors (#315)\r\n- Remove duplication of DQN in docs (#334)\r\n- nit on grammar of a comment: (#354)\r\n\r\n## Examples\r\n- Tuned DoubleDQN with prioritized experience replay (#302)\r\n- adds some descriptions to parseargs arguments (#319)\r\n- Make clip_eps positive (#340)\r\n- updates env in ddpg example (#345)\r\n- Examples (#348)\r\n\r\n## Testing\r\n- Fix Travis CI errors (#318)\r\n- Parse Chainer version with packaging.version (#322)\r\n- removes tests for old replay buffer (#347)\r\n\r\n## Bugfixes\r\n- Fix the error caused by inexact delta_z (#314)\r\n- Stop caching the result of numpy.concatenate in LazyFrames (#332)",
        "dateCreated": "2018-11-14T09:14:57Z",
        "datePublished": "2018-11-15T08:05:49Z",
        "html_url": "https://github.com/chainer/chainerrl/releases/tag/v0.5.0",
        "name": "v0.5.0",
        "tag_name": "v0.5.0",
        "tarball_url": "https://api.github.com/repos/chainer/chainerrl/tarball/v0.5.0",
        "url": "https://api.github.com/repos/chainer/chainerrl/releases/14015272",
        "zipball_url": "https://api.github.com/repos/chainer/chainerrl/zipball/v0.5.0"
      },
      {
        "authorType": "User",
        "author_name": "muupan",
        "body": "# Important enhancements\r\n\r\n- TRPO (trust region policy optimization) is added: `chainerrl.agents.TRPO`.\r\n- C51 (categorical DQN) is added: `chainerrl.agents.CategoricalDQN`.\r\n- NoisyNet is added: `chainerrl.links.FactorizedNoisyLinear` and `chainerrl.links.to_factorized_noisy`.\r\n- Python 3.7 is supported\r\n- Examples were improved in terms of logging and random seed setting\r\n\r\n# Important destructive changes\r\n\r\n- The `async` module is renamed `async_` for Python 3.7 support.\r\n\r\n# All updates\r\n\r\n## Enhancements\r\n\r\n- TRPO agent (#204)\r\n- Use numpy random (#206)\r\n- Add gpus argument for chainerrl.misc.set_random_seed (#207)\r\n- More check on nesting AttributeSavingMixin (#208)\r\n- show error message (#210, thanks @corochann!)\r\n- Add an option to set whether the agent is saved every time the score is improved (#213)\r\n- Make tests check exit status of subprocesses (#215)\r\n- make ReplayBuffer.load() compatible with v0.2.0. (#216, thanks @mr4msm!)\r\n- Add requirements-dev.txt (#222)\r\n- Align act and act_and_train's signature to the Agent interface (#230, thanks @lyx-x!)\r\n- Support dtype arg of spaces.Box (#231)\r\n- Set outdir to results and add help strings (#248)\r\n- Categorical DQN (C51) (#249)\r\n- Remove DiscreteActionValue.sample_epsilon_greedy_actions (#259)\r\n- Remove DQN.compute_q_values (#260)\r\n- Enable to change batch_states in PPO (#261, thanks @kuni-kuni!)\r\n- Remove unnecessary declaration and substitution of 'done' in the train_agent function (#271, thanks @uidilr!)\r\n\r\n## Documentation\r\n\r\n- Update the contribution guide to use pytest (#220)\r\n- Add docstring to ALE and fix seed range (#234)\r\n- Fix docstrings of DDPG (#241)\r\n- Update the algorithm section of README (#246)\r\n- Add CategoricalDQN to README (#252)\r\n- Remove unnecessary comments from examples/gym/train_categorical_dqn_gym.py (#255)\r\n- Update README.md of examples/ale (#275)\r\n\r\n## Examples\r\n\r\n- Fix OMP_NUM_THREADS setting (#235)\r\n- Improve random seed setting in ALE examples (#239)\r\n- Improve random seed setting for all examples (#243)\r\n- Use gym and atari wrappers instead of chainerrl.envs.ale (#253)\r\n- Remove unused args from examples/ale/train_categorical_dqn_ale.py and examples/ale/train_dqn_ale.py (#256)\r\n- Remove unused --profile argument (#258)\r\n- Hyperlink DOI against preferred resolver (#266, thanks @katrinleinweber!)\r\n\r\n## Testing\r\n\r\n- Fix import chainer.testing.condition (#200)\r\n- Use pytest (#209)\r\n- Fix PCL tests (#211)\r\n- Test loading v0.2.0 replay buffers (#217)\r\n- Use assertRaises instead of expectedFailure (#218)\r\n- Improve travis script (#242)\r\n- Run autopep8 in travis ci (#247)\r\n- Switch autopep8 and hacking (#257)\r\n- Use hacking 1.0 (#262)\r\n- Fix a too long line of PPO (#264)\r\n- Update to hacking 1.1.0 (#274)\r\n- Add tests of DQN's loss functions (#279)\r\n\r\n## Bugfixes\r\n\r\n- gym 0.9.6 is not working with python2 (#226)\r\n- Tiny fix: argument passing in SoftmaxDistribution (#228, thanks @lyx-x!)\r\n- Add docstring to ALE and fix seed range (#234)\r\n- except both Exception and KeyboardInterrupt (#250, thanks @uenoku!)\r\n- Switch autopep8 and hacking (#257)\r\n- Modify `async` to `async_` to support Python 3.7 (#286, thanks @mmilk1231!)\r\n- Noisy network fixes (#287, thanks @seann999!)\r\n",
        "dateCreated": "2018-07-23T13:16:52Z",
        "datePublished": "2018-07-23T13:19:31Z",
        "html_url": "https://github.com/chainer/chainerrl/releases/tag/v0.4.0",
        "name": "v0.4.0",
        "tag_name": "v0.4.0",
        "tarball_url": "https://api.github.com/repos/chainer/chainerrl/tarball/v0.4.0",
        "url": "https://api.github.com/repos/chainer/chainerrl/releases/12055311",
        "zipball_url": "https://api.github.com/repos/chainer/chainerrl/zipball/v0.4.0"
      },
      {
        "authorType": "User",
        "author_name": "muupan",
        "body": "# Important enhancements\r\n- Both Chainer v2 and v3 are now supported\r\n- PPO (Proximal Policy Optimization) has been added: `chainerrl.agents.PPO`\r\n- Replay buffers has been made faster\r\n\r\n# Important destructive changes\r\n- Episodic replay buffers' `__len__` now counts the number of transitions, not episodes\r\n- ALE's grayscale conversion formula has been corrected\r\n- FCGaussianPolicyWithFixedCovariance now has a nonlinearity before the last layer\r\n\r\n# All updates\r\n\r\n## Enhancements\r\n- Add RMSpropAsync and NonbiasWeightDecay to `optimizers/__init__.py` (#113)\r\n- Use init_scope (#116)\r\n- Remove ALE dependency (#121)\r\n- Support environments without git command (#124)\r\n- Add PPO agent (#126)\r\n- add .gitignore (#127, thanks @knorth55!)\r\n- Use faster queue for replay buffers (#131)\r\n- Use F.matmul instead of F.batch_matmul (#141)\r\n- Add a utility function to draw a computational graph (#166)\r\n- Improve MLPBN (#171)\r\n- Improve StateActionQFunctions (#172)\r\n- Improve deterministic policies (#173)\r\n- Fix InvertGradients (#185)\r\n- Remove unused functions in DQN (#188)\r\n- Warn about negative exit code of child processes (#194)\r\n\r\n## Documentation\r\n- Add animation gifs (#107)\r\n- Synchronize docs version with package version (#111)\r\n- Add logo (#136)\r\n- [policies/gaussian_policy] Improve docstring (#140, thanks @iory!)\r\n- Improve docstrings (#142)\r\n- Fix a typo (#146)\r\n- Fix a broken link to travis ci (#153)\r\n- Add PPO to README as an implemented algorithm (#168)\r\n- Improve the docstring of AdditiveGaussian (#170)\r\n- Add docsting on eval_max_episode_len (#177)\r\n- Add docstring to DuelingDQN (#187)\r\n- Suppress Sphinx' warning in the docstring of PCL (#198)\r\n\r\n## Example\r\n- fix typo (#122)\r\n- Use Chain.init_scope in the quick start (#148)\r\n- Draw computational graphs in `train_dqn_ale.py` (#192)\r\n- Draw computational graphs in `train_dqn_gym.py` (#195)\r\n- Draw computational graphs in `train_a3c_ale.py` (#197)\r\n\r\n## Testing\r\n- Add CHAINER_VERSION config to CI (#143)\r\n- Specify --outdir on 2nd test (#154)\r\n- Return dict for info of env.step (#162)\r\n- Fix import error in tests (#180)\r\n- Mark TestBiasCorrection as slow (#181)\r\n- Add tests for SingleActionValue (#191)\r\n\r\n## Bugfixes\r\n- Fix save/load in EpisodicReplayBuffer (#130)\r\n- Fix REINFORCE's missing initialization of t (#133)\r\n- Fix episodic buffer `__len__` (#155)\r\n- Remove duplicated import of explorers (#163)\r\n- Fix missing nonlinearity before the last layer (#165)\r\n- Use bytestrings to write git outputs (#178)\r\n- Patches to envs.ALE (#182)\r\n- Fix QuadraticActionValue and add tests (#190)\r\n",
        "dateCreated": "2017-12-08T09:19:17Z",
        "datePublished": "2017-12-08T09:40:32Z",
        "html_url": "https://github.com/chainer/chainerrl/releases/tag/v0.3.0",
        "name": "v0.3.0",
        "tag_name": "v0.3.0",
        "tarball_url": "https://api.github.com/repos/chainer/chainerrl/tarball/v0.3.0",
        "url": "https://api.github.com/repos/chainer/chainerrl/releases/8826246",
        "zipball_url": "https://api.github.com/repos/chainer/chainerrl/zipball/v0.3.0"
      },
      {
        "authorType": "User",
        "author_name": "muupan",
        "body": "Enhancements:\r\n\r\n- Agents\r\n  - REINFORCE #81\r\n- Training helper functions\r\n  - Hook functions #85\r\n  - Add more columns to scores.txt: episodes, max and min #78\r\n  - Improve naming of the output directories #72 #77\r\n  - Use logger instead of print #60\r\n  - Make train_agent_async's eval_interval optional #93\r\n- Misc\r\n  - Use Gumbel-Max trick for categorical sampling in GPU #88 #104\r\n  - Remove test arguments from links (use chainer.config instead) #100\r\n\r\nFixes:\r\n\r\n- Fix argument names #86\r\n- Fix option names #71\r\n- Fix the issue that average_loss is not updated #95\r\n\r\nDependency changes:\r\n\r\n- Switch to Chainer v2 #100\r\n\r\nChanges that can affect performance:\r\n\r\n- train_agent_async won't decay learning rate by default any more. Use hook functions instead.",
        "dateCreated": "2017-06-08T06:49:03Z",
        "datePublished": "2017-06-08T06:54:02Z",
        "html_url": "https://github.com/chainer/chainerrl/releases/tag/v0.2.0",
        "name": "v0.2.0",
        "tag_name": "v0.2.0",
        "tarball_url": "https://api.github.com/repos/chainer/chainerrl/tarball/v0.2.0",
        "url": "https://api.github.com/repos/chainer/chainerrl/releases/6643972",
        "zipball_url": "https://api.github.com/repos/chainer/chainerrl/zipball/v0.2.0"
      },
      {
        "authorType": "User",
        "author_name": "muupan",
        "body": "Enhancements:\r\n- SARSA #39 \r\n- Boltzmann explorer #40 \r\n- ACER for continuous actions #29 \r\n- PCL #45 #57 \r\n- Prioritized Replay #44 #57 \r\n\r\nFixes:\r\n- Fix spelling: s/updator/updater/ #48 ",
        "dateCreated": "2017-03-23T07:40:34Z",
        "datePublished": "2017-03-27T02:54:39Z",
        "html_url": "https://github.com/chainer/chainerrl/releases/tag/v0.1.0",
        "name": "v0.1.0",
        "tag_name": "v0.1.0",
        "tarball_url": "https://api.github.com/repos/chainer/chainerrl/tarball/v0.1.0",
        "url": "https://api.github.com/repos/chainer/chainerrl/releases/5874150",
        "zipball_url": "https://api.github.com/repos/chainer/chainerrl/zipball/v0.1.0"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1017,
      "date": "Tue, 28 Dec 2021 02:08:58 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "chainer",
      "reinforcement-learning",
      "deep-learning",
      "machine-learning",
      "python",
      "dqn",
      "actor-critic"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You can try [ChainerRL Quickstart Guide](examples/quickstart/quickstart.ipynb) first, or check the [examples](examples) ready for Atari 2600 and Open AI Gym.\n\nFor more information, you can refer to [ChainerRL's documentation](http://chainerrl.readthedocs.io/en/latest/index.html).\n\n",
      "technique": "Header extraction"
    }
  ]
}