{
  "citation": [
    {
      "confidence": [
        0.9919784769950174
      ],
      "excerpt": "Aalto University         Aalto University       New York University \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "CIFAR-10 CLASSIFICATION \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999509179074231,
        0.9108175915712649
      ],
      "excerpt": "Deep residual learning for Image Recognition, Shortcut mapping in Neural networks https://arxiv.org/pdf/1512.03385.pdf - MICROSOFT RESEARCH \nhttps://blog.waya.ai/deep-residual-learning-9610bb62c355,basic , Back propagation calculus: welch labs: https://www.youtube.com/watch?v=bxe2T-V8XRs&list=PLiaHhY2iBX9hdHaRr6b7XevZtgZRa1PoU  \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Kunaldargan/Paper-summerization--DEEP-LEARNING-MADE-EASIER-BY-LINEAR-TRANSFORMATIONS-IN-PERCEPTRONS--",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-10-19T01:47:37Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-09-04T05:35:08Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8795623000027036
      ],
      "excerpt": "\u2018DEEP LEARNING MADE EASIER BY LINEAR TRANSFORMATIONS IN \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9853723380769818,
        0.9975311720691523,
        0.9891477994896539
      ],
      "excerpt": "Introduction and Aim of the Paper: \nThis paper talks about the Transformations for implementing Deep Neural Networks using short-connections architecture of two to five hidden layers deep to model and which is implemented to solve the problem of Image recognition in MNIST Handwritten digits and CFIAR-10 classification (using unsupervised autoencoder).Writer have first analyzed theoretically ,that transformations by noting , \u2018they make Fisher information matrix close to a diagonal matrix and hence standard gradient closer to the natural gradient(Quasi-Newton\u2019s Method or BFGS)[5]\u2019 ,which is much faster algorithm for optimization of the model but due to harder implementation in higher dimensions it cannot be used. \nFurther, Proposed paper experimentally proves given the input as normalized (scaled) and centered (close to zero) values to address the problem of vanishing/exploding gradients, output is transformed according to the transformations which results in basic gradient descent can be applied as efficiently as other state-of-the-art algorithms in terms speed and even provides better generalization and be used on both supervised and unsupervised way of learning. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8795623000027036
      ],
      "excerpt": "\u2018DEEP LEARNING MADE EASIER BY LINEAR TRANSFORMATIONS IN \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9853723380769818,
        0.9975311720691523,
        0.9891477994896539
      ],
      "excerpt": "Introduction and Aim of the Paper: \nThis paper talks about the Transformations for implementing Deep Neural Networks using short-connections architecture of two to five hidden layers deep to model and which is implemented to solve the problem of Image recognition in MNIST Handwritten digits and CFIAR-10 classification (using unsupervised autoencoder).Writer have first analyzed theoretically ,that transformations by noting , \u2018they make Fisher information matrix close to a diagonal matrix and hence standard gradient closer to the natural gradient(Quasi-Newton\u2019s Method or BFGS)[5]\u2019 ,which is much faster algorithm for optimization of the model but due to harder implementation in higher dimensions it cannot be used. \nFurther, Proposed paper experimentally proves given the input as normalized (scaled) and centered (close to zero) values to address the problem of vanishing/exploding gradients, output is transformed according to the transformations which results in basic gradient descent can be applied as efficiently as other state-of-the-art algorithms in terms speed and even provides better generalization and be used on both supervised and unsupervised way of learning. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8396820820684948,
        0.9608115899148452,
        0.8900887029275205,
        0.9882797440925489,
        0.8148557679215329
      ],
      "excerpt": "https://qph.ec.quoracdn.net/main-qimg-b1fcbef975924b2ec4ad3a851e9f3934                                 In the context of artificial neural networks, the relu is an activation function fx \nDeep neural networks (up to 50 layers or 150 layers) suffers from the problem of accuracy saturation [2], i.e. after reaching a certain max accuracy (plateau or U curve) model doesn\u2019t gets better or accuracy starts decreasing, and this not due to overfitting, here authors have proved experimentally that if a shallow neural network achieves a certain accuracy then any deeper NN can be at least as well as the shallower NN. Extra layers instead of providing better estimates starts contributing to bigger error and learning becomes inefficient, it follows that \n{Zero Mapping} F(x) = H(x)-x  \uf0e8 F(x) is the error value, H(x) is the predicted value of x \nHere the role of short cut connections comes into play, short cut connections are those which skips one or more layers (fig2.) and contribute (x) directly to f(x) hence changing zero mapping (trying to get to identity mapping, [2] Microsoft paper proved an intuitive hypothesis that it is easier to optimize new identity mapping than to previous zero mapping (estimation of a small value error). \nZero Mapping} F(x) +x= H(x) \uf0e8 short connection, easier to optimize and doesn\u2019t added any extra parameter. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9833038767952531,
        0.9886675603416709,
        0.9906774320495803,
        0.9917237261190806,
        0.9586904729232857
      ],
      "excerpt": "Before going further, it is essential to get an idea of what fisher Information matrix and what is log-likelihood which forms the basis of entire theory in statistics called point estimation, (referred from statistics course Nptel) \nLikelihood: In statistics MLE (Maximum likelihood Estimation) is a method of estimation of underlying parameter given the observations of a statistical distribution and likelihood is a function l \nIt is not simply the summation of natural log of probability density function f(x/\u019f), or observed values which are set into pdf, but the trick is to maximize L wrt. \u019f and estimate it using MAP (maximum a posteriori). \nFisher Information: \u201cFisher information\u00a0(sometimes simply called\u00a0information [1]) is a way of measuring the amount of information that an observable random variable\u00a0X\u00a0carries about an unknown parameter\u00a0\u03b8\u00a0of a distribution that models\u00a0X.\u201d{Wiki} Formally it is the variance of the score or the expected value of the observation. \u201cThe Fisher-information matrix is used to calculate the\u00a0covariance matrices\u00a0associated with\u00a0maximum-likelihood\u00a0estimates\u201d {Wiki} \nAll of this gets importance when we look to find out Fisher\u2019s Information Matrix, which is defined as : Let us consider a family of density functions F that maps a parameter \u03b8 \u2208 RP to a probability density function P(z), P : R N \u2192 [0, \u221e), where z\u2208 R N . Any choice of \u03b8 \u2208 R P defines a particular density function P\u03b8(z) = F\u03b8(z) and by considering all possible \u03b8 values, we explore the set F,For further derivation refer {Wiki: https://en.wikipedia.org/wiki/Fisher_information} \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9770360013715295
      ],
      "excerpt": "\u201cIn this paper, we transform the nonlinearities in the hidden neurons. We explain the usefulness of these transformations by studying the Fisher information matrix\u201d \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9851742955088998
      ],
      "excerpt": "For implementation of the proposed transformations writer has made use of yt = Af (Bxt) + Cxt + \u0190t , Which represents a direct connection with bias over which activation has been applied  \u201d A, B, and C are the weight matrices, and \u0190t  is the noise which is assumed to be zero mean and Gaussian\u201d, weights contains bias ; \u03b1i and \u03b2i parameters for each activation function f: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9388190733100235
      ],
      "excerpt": "Here , writer have converted f\u2019(xi)=0 and  f\u2019\u2019(xi)=0 so that overall function  , which gives freedom to introduce shortcut mapping at this point , Deep \u2013residual NN is easier to optimize . \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9419784761157455
      ],
      "excerpt": "As second order optimization techniques perform better but cannot be used for high dimensional and large models due to heavy computations, writer has put these transformations in front  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8370987432249763
      ],
      "excerpt": "Transformations are result in matrix of higherdemisions ie, matrix of FIM (containing eigen values )is mapped like a covariance matrix ie, \u201cWhen the units are not completely uncorrelated put 0 in Information matrix ,eventually this technique should move FIM(Fisher\u2019s Information matrix to a diagonal matrix) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9069565684533518
      ],
      "excerpt": "Positive Side Effect: By the result of Normalization of problem of saturation and plateau solver \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9507020342734561
      ],
      "excerpt": "Learning rate is determined as estimation error of MLE estimation of \u019f, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8323305403973421
      ],
      "excerpt": "Online learning: 2 step process of training (weights randomization) and validation separately(learning rate adjustment) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8531379745421729
      ],
      "excerpt": "As MNIST data consists of huge set of 60000 images as samples and 10000 test samples, Dimensionality reduction has been performed by subtracting mean activation values from the data, it reduces 2828=784 input parameters to 200 ,thereafter random rotations has been applied in order to make model generic and insensitive from in-class changes, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8633505266721847,
        0.8959003835517511
      ],
      "excerpt": "Implementation:\u201d The experiments were run with an 8-core desktop computer and Matlab implementation. The learning time was measured with Matlab function cputime, which counts time spent by each core\u201d \nTransformation have performed satisfactorily and have decreased the norm of FIM and hence small learning rate and small gradient. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9308659781866949
      ],
      "excerpt": "Model: PCA converted 32323=3072 to 500 ; 500-500-500-10 structure of NN ; n. Learning time was restricted to 10000 seconds, the base learning rate was set by hand to \u03b3 = 0.3, weight decay parameter was \u03bb = 0.001 and the regularization noise was used with standard deviation 0.4 for each input \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8863326019372328
      ],
      "excerpt": "Model: The network topology is 784\u2013500\u2013250\u201330\u2013250\u2013500\u2013784 with tanh, output scaling -1 to 1 to match tanh, learning rate was \u03b3 = 0.05, weight decay parameter was \u03bb = 0.001 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9760089626961604,
        0.9095451085512626,
        0.883770026078875,
        0.8664381144534617,
        0.9447216854542361
      ],
      "excerpt": "Benefits of Transformations and future scope of discussions: \nA basic stochastic gradient optimization became faster than state-of-the-art learning algorithms \nPushes Standard GD to Natural GD by making non diagonal terms in FIM close to 0 \nMethod generalizes well and can applied or further discussed for future applications \nAnother future direction is to introduce a third transformation. While currently we aim at making the Fisher information matrix diagonal, we could also make it closer to the unit matrix. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9950266576251988,
        0.8276690292418247
      ],
      "excerpt": "\u201cSummarizer\u2019s final comment: This paper is really one of high class, readers are requested to try to understand the concepts in brief and think more about the concepts, how they affect overall fulfilment of the objective and brief introduction about every concept is included in this review, for formal proofs and deeper understanding it\u2019s highly recommended to go through Resources all. Thankyou\u201c \nSimilar Paper with basis for explanation about On- Line Learning Back Propagation Algorithm: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8474059315998298
      ],
      "excerpt": "Fisher Information matrix , Log-likelihood, MATLAB implementation of Deep Neural Networks, Normalization, Regularization : http://cs231n.github.io/neural-networks-2/ \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Kunaldargan/Paper-summerization--DEEP-LEARNING-MADE-EASIER-BY-LINEAR-TRANSFORMATIONS-IN-PERCEPTRONS--/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 29 Dec 2021 20:24:26 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Kunaldargan/Paper-summerization--DEEP-LEARNING-MADE-EASIER-BY-LINEAR-TRANSFORMATIONS-IN-PERCEPTRONS--/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Kunaldargan/Paper-summerization--DEEP-LEARNING-MADE-EASIER-BY-LINEAR-TRANSFORMATIONS-IN-PERCEPTRONS--",
    "technique": "GitHub API"
  },
  "invocation": [
    {
      "confidence": [
        0.8053283908359294
      ],
      "excerpt": "As MNIST data consists of huge set of 60000 images as samples and 10000 test samples, Dimensionality reduction has been performed by subtracting mean activation values from the data, it reduces 2828=784 input parameters to 200 ,thereafter random rotations has been applied in order to make model generic and insensitive from in-class changes, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8214473645765031
      ],
      "excerpt": "Data: 3232 pictures, 50000 samples and 10000 test samples \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Kunaldargan/Paper-summerization--DEEP-LEARNING-MADE-EASIER-BY-LINEAR-TRANSFORMATIONS-IN-PERCEPTRONS--/issues{/number}",
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Paper-summerization--DEEP-LEARNING-MADE-EASIER-BY-LINEAR-TRANSFORMATIONS-IN-PERCEPTRONS-",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Paper-summerization--DEEP-LEARNING-MADE-EASIER-BY-LINEAR-TRANSFORMATIONS-IN-PERCEPTRONS--",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Kunaldargan",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Kunaldargan/Paper-summerization--DEEP-LEARNING-MADE-EASIER-BY-LINEAR-TRANSFORMATIONS-IN-PERCEPTRONS--/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 29 Dec 2021 20:24:26 GMT"
    },
    "technique": "GitHub API"
  }
}