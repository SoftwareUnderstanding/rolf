{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2007.11571",
      "https://arxiv.org/abs/2007.11571",
      "https://arxiv.org/abs/2007.11571"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please cite as \n```bibtex\n@article{liu2020neural,\n  title={Neural Sparse Voxel Fields},\n  author={Liu, Lingjie and Gu, Jiatao and Lin, Kyaw Zaw and Chua, Tat-Seng and Theobalt, Christian},\n  journal={NeurIPS},\n  year={2020}\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{liu2020neural,\n  title={Neural Sparse Voxel Fields},\n  author={Liu, Lingjie and Gu, Jiatao and Lin, Kyaw Zaw and Chua, Tat-Seng and Theobalt, Christian},\n  journal={NeurIPS},\n  year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9981801470506324
      ],
      "excerpt": "Neural Sparse Voxel Fields (Liu et al., 2020, <span style=\"color:red\">NeurIPS 2020 Spotlight</span>). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8095926758317811
      ],
      "excerpt": "* NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis (Mildenhall et al., 2020). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8652339495538537
      ],
      "excerpt": "Please also cite the original papers if you use any of them in your work. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeOfConduct": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://raw.githubusercontent.com/facebookresearch/NSVF/main/CODE_OF_CONDUCT.md",
    "technique": "File Exploration"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/facebookresearch/NSVF",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contributing to Neural Sparse Voxel Fields (NSVF)\nWe want to make contributing to this project as easy and transparent as\npossible.\nPull Requests\nWe actively welcome your pull requests.\n\nFork the repo and create your branch from master.\nIf you've added code that should be tested, add tests.\nIf you've changed APIs, update the documentation.\nEnsure the test suite passes.\nMake sure your code lints.\nIf you haven't already, complete the Contributor License Agreement (\"CLA\").\n\nContributor License Agreement (\"CLA\")\nIn order to accept your pull request, we need you to submit a CLA. You only need\nto do this once to work on any of Facebook's open source projects.\nComplete your CLA here: https://code.facebook.com/cla\nIssues\nWe use GitHub issues to track public bugs. Please ensure your description is\nclear and has sufficient instructions to be able to reproduce the issue.\nLicense\nBy contributing to Neural Sparse Voxel Fields,\nyou agree that your contributions will be licensed under the LICENSE file in\nthe root directory of this",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-08-22T01:45:06Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-24T08:42:45Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9326039772032875,
        0.8894841219025454
      ],
      "excerpt": "Photo-realistic free-viewpoint rendering of real-world scenes using classical computer graphics techniques is a challenging problem because it requires the difficult step of capturing detailed appearance and geometry models. \nNeural rendering is an emerging field that employs deep neural networks to implicitly learn scene representations encapsulating both geometry and appearance from 2D observations with or without a coarse geometry. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9739873462833367
      ],
      "excerpt": "Here is the official repo for the paper: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.976671000876387
      ],
      "excerpt": "We also provide our unofficial implementation for: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8994218768797435
      ],
      "excerpt": "Given the dataset of a single scene ({DATASET}), we use the following command for training an NSVF model to synthesize novel views at 800x800 pixels, with a batch size of 4 images per GPU and 2048 rays per image. By default, the code will automatically detect all available GPUs. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9281076864608502,
        0.9667136121091544,
        0.9073933210038294,
        0.8185628993189101,
        0.806776172308393,
        0.9506979274479467
      ],
      "excerpt": "By setting --no-sampling-at-reader, the model only samples pixels in the projected image region of sparse voxels for training. \nBy default, we set the ray-marching step size to be the ratio 1/8 (0.125) of the voxel size which is typically described in the bbox.txt file. \nIt is optional to turn on --use-octree. It will build a sparse voxel octree to speed-up the ray-voxel intersection especially when the number of voxels is larger than 10000. \nBy setting --pruning-every-steps as 2500, the model performs self-pruning at every 2500 steps. \nBy setting --half-voxel-size-at and --reduce-step-size-at as 5000,25000,75000,  the voxel size and step size are halved at 5k, 25k and 75k, respectively. \nNote that, although above parameter settings are used for most of the experiments in the paper, it is possible to tune these parameters to achieve better quality. Besides the above parameters, other parameters can also use default settings. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.944650794345405,
        0.8614422137833428
      ],
      "excerpt": "There are more examples of training scripts to reproduce the results of our paper under examples. \nOnce the model is trained, the following command is used to evaluate rendering quality on the test views given the {MODEL_PATH}. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8975001392854125
      ],
      "excerpt": "Note that we override the raymarching_tolerance to 0.01 to enable early termination for rendering speed-up. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "    --model-overrides '{\"chunk_size\":512,\"raymarching_tolerance\":0.01}' \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9445687843444261
      ],
      "excerpt": "Our code also supports rendering for given camera poses. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "    --model-overrides '{\"chunk_size\":512,\"raymarching_tolerance\":0.01}' \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8815383340920007
      ],
      "excerpt": "The code also supports rendering with camera poses defined in a .txt file. Please refer to this example. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Open source code for the paper of Neural Sparse Voxel Fields.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/facebookresearch/NSVF/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 57,
      "date": "Sun, 26 Dec 2021 16:06:21 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/facebookresearch/NSVF/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "facebookresearch/NSVF",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/facebookresearch/NSVF/tree/main/docs"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/facebookresearch/NSVF/main/examples/render/render_wineholder.sh",
      "https://raw.githubusercontent.com/facebookresearch/NSVF/main/examples/render/render_jade.sh",
      "https://raw.githubusercontent.com/facebookresearch/NSVF/main/examples/valid/valid_wineholder.sh",
      "https://raw.githubusercontent.com/facebookresearch/NSVF/main/examples/train/train_wineholder_with_slurm.sh",
      "https://raw.githubusercontent.com/facebookresearch/NSVF/main/examples/train/train_jade.sh",
      "https://raw.githubusercontent.com/facebookresearch/NSVF/main/examples/train/train_family.sh",
      "https://raw.githubusercontent.com/facebookresearch/NSVF/main/examples/train/train_wineholder.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To prepare a new dataset of a single scene for training and testing, please follow the data structure:\n\n```bash\n<dataset_name>\n|-- bbox.txt         #: bounding-box file\n|-- intrinsics.txt   #: 4x4 camera intrinsics\n|-- rgb\n    |-- 0.png        #: target image for each view\n    |-- 1.png\n    ...\n|-- pose\n    |-- 0.txt        #: camera pose for each view (4x4 matrices)\n    |-- 1.txt\n    ...\n[optional]\n|-- test_traj.txt    #: camera pose for free-view rendering demonstration (4N x 4)\n```\n\nwhere the ``bbox.txt`` file contains a line describing the initial bounding box and voxel size:\n\n```bash\nx_min y_min z_min x_max y_max z_max initial_voxel_size\n```\n\nNote that the file names of target images and those of the corresponding camera pose files are not required to be exactly the same. However, the orders of these two kinds of files (sorted by string) must match.  The datasets are split with view indices.\nFor example, \"``train (0..100)``, ``valid (100..200)`` and ``test (200..400)``\" mean the first 100 views for training, 100-199th views for validation, and 200-399th views for testing.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "This code is implemented in PyTorch using [fairseq framework](https://github.com/pytorch/fairseq).\n\nThe code has been tested on the following system:\n\n* Python 3.7\n* PyTorch 1.4.0\n* [Nvidia apex library](https://github.com/NVIDIA/apex) (optional)\n* Nvidia GPU (Tesla V100 32GB) CUDA 10.1\n\nOnly learning and rendering on GPUs are supported.\n\nTo install, first clone this repo and install all dependencies:\n\n```bash\npip install -r requirements.txt\n```\n\nThen,  run\n\n```bash\npip install --editable ./\n```\n\nOr if you want to install the code locally, run:\n\n```bash\npython setup.py build_ext --inplace\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8381553229756074
      ],
      "excerpt": "    --save-interval-updates 500 --max-update 150000 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8097686585672471
      ],
      "excerpt": "tensorboard --logdir=${SAVE}/tensorboard --port=10000 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8661176197453521
      ],
      "excerpt": "    --name ${NAME} \\ \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8955162047114402,
        0.8955162047114402,
        0.9227429352306429,
        0.9227429352306429
      ],
      "excerpt": "Synthetic-NSVF | download (.zip) | 0_* (training) 1_* (validation) 2_* (testing) \nSynthetic-NeRF | download (.zip) | 0_* (training) 1_* (validation) 2_* (testing) \nBlendedMVS  | download (.zip) | 0_* (training) 1_* (testing) \nTanks&Temples | download (.zip) | 0_* (training) 1_* (testing) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9495054113571121
      ],
      "excerpt": "python -u train.py ${DATASET} \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8277198070363508
      ],
      "excerpt": "    --initial-boundingbox ${DATASET}/bbox.txt \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8336686503214243
      ],
      "excerpt": "    --lr 0.001 --lr-scheduler \"polynomial_decay\" --total-num-update 150000 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8463818456036317
      ],
      "excerpt": "    | tee -a $SAVE/train.log \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.921108411272225
      ],
      "excerpt": "python validate.py ${DATASET} \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.936748333178686
      ],
      "excerpt": "    --model-overrides '{\"chunk_size\":512,\"raymarching_tolerance\":0.01,\"tensorboard_logdir\":\"\",\"eval_lpips\":True}' \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8482219329733792
      ],
      "excerpt": "<img src='docs/figs/results.gif'/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8611039262608726
      ],
      "excerpt": "python render.py ${DATASET} \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8963608510563088
      ],
      "excerpt": "    --model-overrides '{\"chunk_size\":512,\"raymarching_tolerance\":0.01}' \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.816286369960627,
        0.8298630027365435
      ],
      "excerpt": "    --render-output ${SAVE}/output \\ \n    --render-output-types \"color\" \"depth\" \"voxel\" \"normal\" --render-combine-output \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8611039262608726
      ],
      "excerpt": "python render.py ${DATASET} \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8963608510563088
      ],
      "excerpt": "    --model-overrides '{\"chunk_size\":512,\"raymarching_tolerance\":0.01}' \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.816286369960627,
        0.8298630027365435
      ],
      "excerpt": "    --render-output ${SAVE}/output \\ \n    --render-output-types \"color\" \"depth\" \"voxel\" \"normal\" --render-combine-output \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8964877701688859
      ],
      "excerpt": "python extract.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.833565565511204,
        0.8421074476017179
      ],
      "excerpt": "    --output ${SAVE} \\ \n    --name ${NAME} \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8555989122933799
      ],
      "excerpt": "    --mc-num-samples-per-halfvoxel 5 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/facebookresearch/NSVF/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Cuda",
      "C++",
      "C"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) Facebook, Inc. and its affiliates.\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Neural Sparse Voxel Fields (NSVF)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "NSVF",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "facebookresearch",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/facebookresearch/NSVF/blob/main/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This code is implemented in PyTorch using [fairseq framework](https://github.com/pytorch/fairseq).\n\nThe code has been tested on the following system:\n\n* Python 3.7\n* PyTorch 1.4.0\n* [Nvidia apex library](https://github.com/NVIDIA/apex) (optional)\n* Nvidia GPU (Tesla V100 32GB) CUDA 10.1\n\nOnly learning and rendering on GPUs are supported.\n\nTo install, first clone this repo and install all dependencies:\n\n```bash\npip install -r requirements.txt\n```\n\nThen,  run\n\n```bash\npip install --editable ./\n```\n\nOr if you want to install the code locally, run:\n\n```bash\npython setup.py build_ext --inplace\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 452,
      "date": "Sun, 26 Dec 2021 16:06:21 GMT"
    },
    "technique": "GitHub API"
  }
}