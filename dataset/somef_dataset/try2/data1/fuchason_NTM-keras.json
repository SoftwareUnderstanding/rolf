{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1410.5401, as a backend neutral recurrent keras layer.\n\nA very default experiment, the copy task, is provided, too.\n\nIn the end there is a TODO-List. Help would be appreciated!\n\nNOTE:\n* There is a nicely formatted paper describing the rough idea of the NTM, implementation difficulties and which discusses the\n  copy experiment. It is available here in the repository as The_NTM_-_Introduction_And_Implementation.pdf. \n* You may want to change the LOGDIR_BASE in testing_utils.py to something that works for you or just set a symbolic\n  link.\n\n\n### User guide\nFor a quick start on the copy task, type \n\n    python main.py -v ntm\n\nwhile in a python enviroment which has tensorflow, keras and numpy.\nHaving tensorflow-gpu is recommend, as everything is about 20x faster.\nIn my case this experiment takes about 100 minutes on a NVIDIA GTX 1050 Ti.\nThe -v is optional and offers much more detailed information about the achieved accuracy, and also after every training\nepoch.\nLogging data is written LOGDIR_BASE, which is ./logs/ by default. View them with tensorboard:\n\n    tensorboard --logdir ./logs\n\nIf you've luck and not had a terrible run (that can happen, unfortunately"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8251022327183287
      ],
      "excerpt": "  please write me a message. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/fuchason/NTM-keras",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-07-01T03:21:23Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-09-03T09:01:17Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This code tries to implement the Neural Turing Machine, as found in \nhttps://arxiv.org/abs/1410.5401, as a backend neutral recurrent keras layer.\n\nA very default experiment, the copy task, is provided, too.\n\nIn the end there is a TODO-List. Help would be appreciated!\n\nNOTE:\n* There is a nicely formatted paper describing the rough idea of the NTM, implementation difficulties and which discusses the\n  copy experiment. It is available here in the repository as The_NTM_-_Introduction_And_Implementation.pdf. \n* You may want to change the LOGDIR_BASE in testing_utils.py to something that works for you or just set a symbolic\n  link.\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8314978916413865,
        0.9802995052316326
      ],
      "excerpt": "API CHANGE: Controller models now must have linear activation. The activation of the NTM-Layer is selected \n  by the new parameter \"activation\" (default: \"linear\"). For all the stuff that interacts with the memory we now \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9387889942517645
      ],
      "excerpt": "Unfortunately we lost backend neutrality: As tf.slice is used extensivly, we have to either try getting K.slice or \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.942957968612403,
        0.826337821930532,
        0.9571465731668988,
        0.954941800435652
      ],
      "excerpt": "*  n_width: This is the width of the memory matrix. Increasing this increases computational complexity in O(n^2). The \n   controller shape is not dependant on this, making weight transfer possible. \nm_depth: This is the depth of the memory matrix. Increasing this increases the number of trainable weights in O(m^2). It also changes controller shape.  \ncontroller_model: This parameter allows you to place a keras model of appropriate shape as the controller. The \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9581327041622392,
        0.8480359338244036
      ],
      "excerpt": "Note that we used linear as the last activation layer! This is of critical importance. \nThe activation of the NTM-layer can be set the parameter activation (default: linear). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8437102295707588
      ],
      "excerpt": "Also note that every statefull controller must carry around his own state, as was done here with  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8836479180524478,
        0.8665269993619706,
        0.8237583069194698
      ],
      "excerpt": "[ ] Support for get and set config to better enable model saving \n[x] A bit of code cleaning: especially the controller output splitting is ugly as hell. \n[x] Support for arbitrary activation functions would be nice, currently restricted to sigmoid. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9294281402940795
      ],
      "excerpt": "[ ] Maybe add the other experiments of the original paper? \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/fuchason/NTM-keras/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sat, 25 Dec 2021 20:47:06 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/fuchason/NTM-keras/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "fuchason/NTM-keras",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "For a quick start on the copy task, type \n\n    python main.py -v ntm\n\nwhile in a python enviroment which has tensorflow, keras and numpy.\nHaving tensorflow-gpu is recommend, as everything is about 20x faster.\nIn my case this experiment takes about 100 minutes on a NVIDIA GTX 1050 Ti.\nThe -v is optional and offers much more detailed information about the achieved accuracy, and also after every training\nepoch.\nLogging data is written LOGDIR_BASE, which is ./logs/ by default. View them with tensorboard:\n\n    tensorboard --logdir ./logs\n\nIf you've luck and not had a terrible run (that can happen, unfortunately), you now have a machine capable of copying a\ngiven sequence! I wonder if we could have achieved that any other way ...\n\nThese results are especially interesting compared to an LSTM model: Run\n\n    python main.py lstm\n\nThis builds 3 layers of LSTM with and goes through the same testing procedure\nas above, which for me resulted in a training time of approximately 1h (same GPU) and \n(roughly) 100%, 100%, 94%, 50%, 50% accuracy at the respective test lengths.\nThis shows that the NTM has advantages over LSTM in some cases. Especially considering the LSTM model has about 807.200\ntrainable parameters while the NTM had a mere 3100! \n\nHave fun playing around, maybe with other controllers? dense, double_dense and lstm are build in.\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.841615829449826
      ],
      "excerpt": "  (by default both are 1). \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "                    stateful=True, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "from ntm import controller_input_output_shape \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8120709119833421
      ],
      "excerpt": "[x] Arbitrary number of read and write heads \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/fuchason/NTM-keras/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "BSD 3-Clause \"New\" or \"Revised\" License",
      "url": "https://api.github.com/licenses/bsd-3-clause"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'BSD 3-Clause License\\n\\nCopyright (c) 2017, Florian Unger\\nAll rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met:\\n\\n Redistributions of source code must retain the above copyright notice, this\\n  list of conditions and the following disclaimer.\\n\\n Redistributions in binary form must reproduce the above copyright notice,\\n  this list of conditions and the following disclaimer in the documentation\\n  and/or other materials provided with the distribution.\\n\\n* Neither the name of the copyright holder nor the names of its\\n  contributors may be used to endorse or promote products derived from\\n  this software without specific prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "## Changelog 0.2:",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "NTM-keras",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "fuchason",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/fuchason/NTM-keras/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sat, 25 Dec 2021 20:47:06 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "More or less minimal code example:\n\n    from keras.models import Sequential\n    from keras.optimizers import Adam\n    from ntm import NeuralTuringMachine as NTM\n\n    model = Sequential()\n    model.name = \"NTM_-_\" + controller_model.name\n\n    ntm = NTM(output_dim, n_slots=50, m_depth=20, shift_range=3,\n              controller_model=None,\n              return_sequences=True,\n              input_shape=(None, input_dim), \n              batch_size = 100)\n    model.add(ntm)\n\n    sgd = Adam(lr=learning_rate, clipnorm=clipnorm)\n    model.compile(loss='binary_crossentropy', optimizer=sgd,\n                   metrics = ['binary_accuracy'], sample_weight_mode=\"temporal\")\n\nWhat if we instead want a more complex controller? Design it, e.g. double LSTM:\n\n    controller = Sequential()\n    controller.name=ntm_controller_architecture\n    controller.add(LSTM(units=150,\n                        stateful=True,\n                        implementation=2,   ",
      "technique": "Header extraction"
    }
  ]
}