{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2010.01412"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Jannoshh/simple-sam",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-01-29T19:50:38Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-19T07:17:38Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9712159442048467
      ],
      "excerpt": "This is an unofficial repository for Sharpness-Aware Minimization for Efficiently Improving Generalization. <br> <br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8947157213531767
      ],
      "excerpt": "Optimizing only the training loss value, as is commonly done, can easily lead to suboptimal model quality. Motivated by the connection between \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8511831434998349,
        0.9204329492754743
      ],
      "excerpt": "parameters that lie in neighborhoods having uniformly low loss, an optimization problem on which gradient descent can be performed efficiently. \nThe implementation uses Tensorflow 2 and is heavily inspired by davda54's PyTorch implementation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8007149204983646
      ],
      "excerpt": "| A sharp minimum to which a ResNet trained with SGD converged | A wide minimum to which the same ResNet trained with SAM converged. | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9156070444464878
      ],
      "excerpt": "Performs the first optimization step that finds the weights with the highest loss in the local rho-neighborhood. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9023204600127737
      ],
      "excerpt": "Performs the second optimization step that updates the original weights with the gradient from the (locally) highest point in the loss landscape. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Sharpness-Aware Minimization for Efficiently Improving Generalization",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Jannoshh/simple-sam/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 7,
      "date": "Fri, 24 Dec 2021 16:42:29 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Jannoshh/simple-sam/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Jannoshh/simple-sam",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Jannoshh/simple-sam/main/examples/mnist_example_keras_fit.ipynb",
      "https://raw.githubusercontent.com/Jannoshh/simple-sam/main/examples/cifar10_example.ipynb",
      "https://raw.githubusercontent.com/Jannoshh/simple-sam/main/examples/mnist_sam.ipynb",
      "https://raw.githubusercontent.com/Jannoshh/simple-sam/main/examples/mnist_no_sam.ipynb"
    ],
    "technique": "File Exploration"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Jannoshh/simple-sam/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2021 Jannes Elstner\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "simple-SAM",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "simple-sam",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Jannoshh",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Jannoshh/simple-sam/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 26,
      "date": "Fri, 24 Dec 2021 16:42:29 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "sam",
      "optimizer",
      "tensorflow",
      "sharpness-aware",
      "training",
      "sharpness",
      "topology"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Using SAM is easy in custom training loops:\n\n```python\n...\n\nfrom sam import SAM\n\nmodel = YourModel()\nbase_optimizer = tf.keras.optimizers.SGD()  #: define an optimizer for the \"sharpness-aware\" update\noptimizer = SAM(base_optimizer)\n\n...\n\n@tf.function\ndef train_step_SAM(images, labels):\n    with tf.GradientTape() as tape:\n        predictions = model(images, training=True)\n        loss = loss_object(labels, predictions)\n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.first_step(gradients, model.trainable_variables)\n\n    with tf.GradientTape() as tape:\n        predictions = model(images, training=True)\n        loss = loss_object(labels, predictions)\n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.second_step(gradients, model.trainable_variables)\n\n...\n\nfor x, y in dataset:\n    train_step_SAM(x, y)\n  \n...\n```\n\nIf you want to use the Keras API:\n\n```python\n...\n\nfrom sam import sam_train_step\n\n#: override the train_step function of the keras model\nclass YourModel(tf.keras.Model):\n    def train_step(self, data):\n        return sam_train_step(self, data)\n\ninputs = Input(...)\noutputs = ...\nmodel = YourModel(inputs, outputs)\n\nmodel.compile(...)\nmodel.fit(x_train, y_train, epochs=3)\n\n...\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}