{
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lonePatient/BERT-chinese-text-classification-pytorch",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-02-01T14:18:45Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-12T05:59:08Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.983810348415787,
        0.9652771208610523
      ],
      "excerpt": "This repo contains a PyTorch implementation of a pretrained BERT model  for chinese text classification. \nAt the root of the project, you will see: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "|  \u2514\u2500\u2500 model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8596982069667689
      ],
      "excerpt": "|  \u2514\u2500\u2500 output #:save the ouput of model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.974376041305329,
        0.9528725823047942
      ],
      "excerpt": "When converting the tensorflow checkpoint into the pytorch, it's expected to choice the \"bert_model.ckpt\", instead of \"bert_model.ckpt.index\", as the input file. Otherwise, you will see that the model can learn nothing and give almost same random outputs for any inputs. This means, in fact, you have not loaded the true ckpt for your model \nWhen using multiple GPUs, the non-tensor calculations, such as accuracy and f1_score, are not supported by DataParallel instance \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "This repo contains a PyTorch implementation of a pretrained BERT model  for text classification.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lonePatient/BERT-chinese-text-classification-pytorch/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 18,
      "date": "Fri, 24 Dec 2021 11:22:07 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/lonePatient/BERT-chinese-text-classification-pytorch/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "lonePatient/BERT-chinese-text-classification-pytorch",
    "technique": "GitHub API"
  },
  "invocation": [
    {
      "confidence": [
        0.9336801098518991,
        0.9336801098518991
      ],
      "excerpt": "|  |  \u2514\u2500\u2500 lrscheduler.py\u3000\u3000 \n|  |  \u2514\u2500\u2500 trainingmonitor.py\u3000 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991
      ],
      "excerpt": "|  |  \u2514\u2500\u2500 bert_processor.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8277380791614312
      ],
      "excerpt": "|  \u2514\u2500\u2500 output #:save the ouput of model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8653523522410159,
        0.828520898273279
      ],
      "excerpt": "|  \u2514\u2500\u2500 train #:used for training a model \n|  |  \u2514\u2500\u2500 trainer.py  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8197328191806175,
        0.9336801098518991
      ],
      "excerpt": "|  \u2514\u2500\u2500 utils #: a set of utility functions \n\u251c\u2500\u2500 run_bert.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.853492186136904
      ],
      "excerpt": "| avg / total |   0.98    |  0.98  |   0.98   |  15000  | \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/lonePatient/BERT-chinese-text-classification-pytorch/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "BERT Chinese text classification by PyTorch",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "BERT-chinese-text-classification-pytorch",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "lonePatient",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lonePatient/BERT-chinese-text-classification-pytorch/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\n- csv\r\n- tqdm\r\n- numpy\r\n- pickle\r\n- scikit-learn\r\n- PyTorch 1.0\r\n- matplotlib\r\n- pytorch_transformers=1.1.0\r\n\r\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 79,
      "date": "Fri, 24 Dec 2021 11:22:07 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "bert",
      "pytorch",
      "nlp",
      "chinese-text-classification",
      "chinese",
      "text-classification"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\nyou need download pretrained chinese bert model\r\n\r\n1. Download the Bert pretrained model from [s3](https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-pytorch_model.bin) \r\n2. Download the Bert config file from [s3](https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-config.json) \r\n3. Download the Bert vocab file from [s3](https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt) \r\n4. modify `bert-base-chinese-pytorch_model.bin` to `pytorch_model.bin` , `bert-base-chinese-config.json` to `config.json` ,`bert-base-chinese-vocab.txt` to `vocab.txt`\r\n5. place `model` ,`config` and `vocab` file into  the `/pybert/pretrain/bert/base-uncased` directory.\r\n2. `pip install pytorch-transformers` from [github](https://github.com/huggingface/pytorch-transformers).\r\n4. Prepare [BaiduNet](https://pan.baidu.com/s/1Gn0rHHhrod6ed8LDTJ-rtA){password:ruxu}, you can modify the `io.bert_processor.py` to adapt your data.\r\n5. Modify configuration information in `pybert/config/base.py`(the path of data,...).\r\n6. Run `python run_bert.py --do_data` to preprocess data.\r\n7. Run `python run_bert.py --do_train --save_best` to fine tuning bert model.\r\n8. Run `run_bert.py --do_test --do_lower_case` to predict new data.\r\n\r\n",
      "technique": "Header extraction"
    }
  ]
}