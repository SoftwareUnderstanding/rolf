{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "I would like to thank\n[Daniel C. Alexander](http://www0.cs.ucl.ac.uk/staff/d.alexander/) at University College London, UK, \n[Antonio Criminisi](https://scholar.google.co.uk/citations?user=YHmzvmMAAAAJ&hl=en/) at Amazon Research, \nand [Aditya Nori](https://www.microsoft.com/en-us/research/people/adityan/) at Microsoft Research Cambridge\nfor their valuable contributions to this paper. \n\n\n\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1512.03385",
      "https://arxiv.org/abs/1711.11503"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you use this code for your research, please cite our ICML paper:\n```\n@inproceedings{AdaptiveNeuralTrees19,\n  title={Adaptive Neural Trees},\n  author={Tanno, Ryutaro and Arulkumaran, Kai and Alexander, Daniel and Criminisi, Antonio and Nori, Aditya},\n  booktitle={Proceedings of the 36th International Conference on Machine Learning (ICML)},\n  year={2019},\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{AdaptiveNeuralTrees19,\n  title={Adaptive Neural Trees},\n  author={Tanno, Ryutaro and Arulkumaran, Kai and Alexander, Daniel and Criminisi, Antonio and Nori, Aditya},\n  booktitle={Proceedings of the 36th International Conference on Machine Learning (ICML)},\n  year={2019},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.958160488819474
      ],
      "excerpt": "Paper (ICML'19) | Video from London ML meetup \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rtanno21609/AdaptiveNeuralTrees",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-05-08T09:18:32Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-16T03:02:19Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.976604428667353,
        0.8345353560027715
      ],
      "excerpt": "This repository contains our PyTorch implementation of Adaptive Neural Trees (ANTs). \nThe code was written by Ryutaro Tanno and  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Adaptive Neural Trees ",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rtanno21609/AdaptiveNeuralTrees/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 20,
      "date": "Wed, 22 Dec 2021 03:09:07 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/rtanno21609/AdaptiveNeuralTrees/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "rtanno21609/AdaptiveNeuralTrees",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/rtanno21609/AdaptiveNeuralTrees/master/notebooks/example_cifar10.ipynb",
      "https://raw.githubusercontent.com/rtanno21609/AdaptiveNeuralTrees/master/notebooks/example_mnist.ipynb",
      "https://raw.githubusercontent.com/rtanno21609/AdaptiveNeuralTrees/master/notebooks/.ipynb_checkpoints/example_cifar10-checkpoint.ipynb",
      "https://raw.githubusercontent.com/rtanno21609/AdaptiveNeuralTrees/master/notebooks/.ipynb_checkpoints/example_mnist-checkpoint.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/rtanno21609/AdaptiveNeuralTrees/master/install.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Clone this repo:\n```bash\ngit clone https://github.com/rtanno21609/AdaptiveNeuralTrees.git\ncd AdaptiveNeuralTrees\n```\n- (Optional) create a new Conda environment and activate it:\n```bash\nconda create -n ANT python=2.7\nsource activate ANT\n```\n- Run the following to install required packages.\n``` \nbash ./install.sh\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/rtanno21609/AdaptiveNeuralTrees/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2017 rtanno21609\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Adaptive Neural Trees",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "AdaptiveNeuralTrees",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "rtanno21609",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rtanno21609/AdaptiveNeuralTrees/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Linux or macOS\n- Python 2.7\n- Anaconda >= 4.5 \n- CPU or NVIDIA GPU + CUDA 8.0 \n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 123,
      "date": "Wed, 22 Dec 2021 03:09:07 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "An example command for training/testing an ANT is given below.\n\n```bash\npython tree.py --experiment test_ant_cifar10  #:name of experiment \\\n               --subexperiment myant  #:name of subexperiment \\\n               --dataset cifar10   #:dataset \\\n                #: Model details:    \\\n               --router_ver 3        #:type of router module \\\n               --router_ngf 128      #:no. of kernels in routers \\\n               --router_k 3          #:spatial size of kernels in routers \\\n               --transformer_ver 5   #:type of transformer module \\\n               --transformer_ngf 128 #:no. of kernels in transformers \\\n               --transformer_k 3     #:spatial size of kernels in transformers \\\n               --solver_ver 6        #:type of solver module \\\n               --batch_norm          #:apply batch-norm \\\n               --maxdepth 10         #:maximum depth of the tree-structure \\\n                #: Training details: \\\n               --batch-size 512    #:batch size \\\n               --augmentation_on   #:apply data augmentation \\\n               --scheduler step_lr #:learning rate scheduling \\\n               --criteria avg_valid_loss #: splitting criteria\n               --epochs_patience 5 #:no. of patience per node for growth phase \\\n               --epochs_node 100   #:max no. of epochs per node for growth phase \\\n               --epochs_finetune 200 #:no. of epochs for fine-tuning phase \\\n               #: Others: \\\n               --seed 0            #:randomisation seed\n               --num_workers 0     #:no. of CPU subprocesses used for data loading \\\n               --visualise_split  #: save the tree structure every epoch \\\n```\nThe model configurations and optimisation trajectory (e.g value of\ntrain/validation loss at each time point) are saved in `records.jason` in the \ndirectory `./experiments/dataset/experiment/subexperiment/checkpoints`. Similarly,\ntree structure and best trained model are saved as `tree_structures.json`\nand `model.pth`, respectively under the same directory. If the visualisation option \n`--visualise_split` is used, the tree architecture of the ANT is saved in the PNG\nformat in the directory `./experiments/dataset/experiment/subexperiment/cfigures`.\n\nBy default, the average classification accuracy is also computed\non train/valid/test sets for every epoch and saved in `records.jason` file, so\nrunning `tree.py` would suffice for both training and testing an ANT of particular \nconfigurations. \n\n**Jupyter Notebooks**\n\nWe have also included two Jupter notebooks `./notebooks/example_mnist.ipynb`\nand `./notebooks/example_cifar10.ipynb`, which illustrate how this repository \ncan be used to train ANTs on MNIST and CIFAR-10 image recognition datasets. \n\n\n**Primitive modules**\n\nDefining an ANT amounts to specifying the forms of primitive modules: routers,\ntransformers and solvers. The table below provides the list of currently implemented\nprimitive modules. You can try any combination of three\nto construct an ANT. \n\n| Type | Router | Transformer  | Solver |\n| ------------- |:-------------:  | :-----------:|:-----:|\n| 1     | 1 x Conv + GAP + Sigmoid | Identity function | Linear classifier  |\n| 2     | 1 x Conv + GAP + 1 x FC   | 1 x Conv | MLP with 2 hidden layers  |\n| 3     | 2 x Conv + GAP + 1 x FC   | 1 x Conv + 1 x MaxPool | MLP with 1 hidden layer |\n| 4     | MLP with 1 hidden layer   | Bottleneck residual block ([He et al., 2015](https://arxiv.org/abs/1512.03385)) | GAP + 2 FC layers + Softmax |\n| 5     | GAP + 2 x FC layers ([Veit et al., 2017](https://arxiv.org/abs/1711.11503)) | 2 x Conv + 1 x MaxPool | MLP with 1 hidden layer in AlexNet ([layers-80sec.cfg](https://github.com/BVLC/caffe/blob/master/examples/mnist/lenet.prototxt))    |\n| 6     | 1 x Conv +  GAP + 2 x FC | Whole VGG13 architecture (without the linear layer) | GAP + 1 FC layers + Softmax  |\n\nFor the detailed definitions of respective modules, please see `utils.py` and \n`models.py`. \n\n",
      "technique": "Header extraction"
    }
  ]
}