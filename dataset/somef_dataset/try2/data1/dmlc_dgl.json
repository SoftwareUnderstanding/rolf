{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2102.09548",
      "https://arxiv.org/abs/1912.00552",
      "https://arxiv.org/abs/1904.05530",
      "https://arxiv.org/abs/1911.07532",
      "https://arxiv.org/abs/1907.03822",
      "https://arxiv.org/abs/2009.12710",
      "https://arxiv.org/abs/2004.05718",
      "https://arxiv.org/abs/2009.08299",
      "https://arxiv.org/abs/2011.01619",
      "https://arxiv.org/abs/2011.00402",
      "https://arxiv.org/abs/2009.06946",
      "https://arxiv.org/abs/2011.13748",
      "https://arxiv.org/abs/2009.14068",
      "https://arxiv.org/abs/2010.07668",
      "https://arxiv.org/abs/2010.06253",
      "https://arxiv.org/abs/2009.13752",
      "https://arxiv.org/abs/2009.05552",
      "https://arxiv.org/abs/2008.11416",
      "https://arxiv.org/abs/2012.15024",
      "https://arxiv.org/abs/2008.03226",
      "https://arxiv.org/abs/2008.05994",
      "https://arxiv.org/abs/2006.07846",
      "https://arxiv.org/abs/2101.07773",
      "https://arxiv.org/abs/2102.10338",
      "https://arxiv.org/abs/2102.06800",
      "https://arxiv.org/abs/2102.08863",
      "https://arxiv.org/abs/2104.01488",
      "https://arxiv.org/abs/2103.02565",
      "https://arxiv.org/abs/2105.06035",
      "https://arxiv.org/abs/2103.11794",
      "https://arxiv.org/abs/2104.03057",
      "https://arxiv.org/abs/2105.00795",
      "https://arxiv.org/abs/2105.02048",
      "https://arxiv.org/abs/2106.04362",
      "https://arxiv.org/abs/2106.08556",
      "https://arxiv.org/abs/2104.12950",
      "https://arxiv.org/abs/2105.09720",
      "https://arxiv.org/abs/2103.03330",
      "https://arxiv.org/abs/2011.11048v5",
      "https://arxiv.org/abs/1909.01315"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you use DGL in a scientific publication, we would appreciate citations to the following paper:\n```\n@article{wang2019dgl,\n    title={Deep Graph Library: A Graph-Centric, Highly-Performant Package for Graph Neural Networks},\n    author={Minjie Wang and Da Zheng and Zihao Ye and Quan Gan and Mufei Li and Xiang Song and Jinjing Zhou and Chao Ma and Lingfan Yu and Yu Gai and Tianjun Xiao and Tong He and George Karypis and Jinyang Li and Zheng Zhang},\n    year={2019},\n    journal={arXiv preprint arXiv:1909.01315}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{wang2019dgl,\n    title={Deep Graph Library: A Graph-Centric, Highly-Performant Package for Graph Neural Networks},\n    author={Minjie Wang and Da Zheng and Zihao Ye and Quan Gan and Mufei Li and Xiang Song and Jinjing Zhou and Chao Ma and Lingfan Yu and Yu Gai and Tianjun Xiao and Tong He and George Karypis and Jinyang Li and Zheng Zhang},\n    year={2019},\n    journal={arXiv preprint arXiv:1909.01315}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9105368110547479,
        0.8548390257852161
      ],
      "excerpt": "Benchmarking GNN: https://github.com/graphdeeplearning/benchmarking-gnns \nOGB: a collection of realistic, large-scale, and diverse benchmark datasets for machine learning on graphs. https://ogb.stanford.edu/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9105368110547479
      ],
      "excerpt": "GNN-RecSys: https://github.com/je-dbl/GNN-RecSys \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8587814738756394
      ],
      "excerpt": "GNNLens2: Visualization tool for Graph Neural Networks. https://github.com/dmlc/GNNLens2 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9997342669066169,
        0.9202779422164958,
        0.9951715937282142,
        0.9989284155045624,
        0.9992536709631865
      ],
      "excerpt": "Open Graph Benchmarks: Datasets for Machine Learning on Graphs, NeurIPS'20, Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, Jure Leskovec \nDropEdge: Towards Deep Graph Convolutional Networks on Node Classification, ICLR'20, Yu Rong, Wenbing Huang, Tingyang Xu, Junzhou Huan \nDiscourse-Aware Neural Extractive Text Summarization, ACL'20, Jiacheng Xu, Zhe Gan, Yu Cheng, Jingjing Liu \nGCC: Graph Contrastive Coding for Graph Neural Network Pre-Training, KDD'20, Jiezhong Qiu, Qibin Chen, Yuxiao Dong, Jing Zhang, Hongxia Yang, Ming Ding, Kuansan Wang, Jie Tang \nDGL-KE: Training Knowledge Graph Embeddings at Scale, SIGIR'20, Da Zheng, Xiang Song, Chao Ma, Zeyuan Tan, Zihao Ye, Jin Dong, Hao Xiong, Zheng Zhang, George Karypis \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8836965660196757,
        0.9999999967621704
      ],
      "excerpt": "Finding Patient Zero: Learning Contagion Source with Graph Neural Networks, Chintan Shah, Nima Dehmamy, Nicola Perra, Matteo Chinazzi, Albert-L\u00e1szl\u00f3 Barab\u00e1si, Alessandro Vespignani, Rose Yu \nFeatGraph: A Flexible and Efficient Backend for Graph Neural Network Systems, SC'20, Yuwei Hu, Zihao Ye, Minjie Wang, Jiali Yu, Da Zheng, Mu Li, Zheng Zhang, Zhiru Zhang, Yida Wang \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9998994193484723
      ],
      "excerpt": "11. [**BP-Transformer: Modelling Long-Range Context via Binary Partitioning.**](https://arxiv.org/pdf/1911.04070.pdf), *Zihao Ye, Qipeng Guo, Quan Gan, Xipeng Qiu, Zheng Zhang* \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999985326116666,
        0.9998397786746344,
        0.9959945642429223,
        0.999998972295702,
        0.9965313462678748,
        0.9999961340714214,
        0.9999871489469674,
        0.9983677297481248,
        0.9938429690333589,
        0.9842276095586568,
        0.999921477499691,
        0.999986434520775,
        0.9324514288664576,
        0.9345147114274623,
        0.9888992644661146,
        0.9969358559246954,
        0.9983602706024438,
        0.9999999989469757,
        0.9715895316145551,
        0.9999999984954968,
        0.9727866907064037,
        0.8462621535789817,
        0.9891823759859844,
        0.9999335628380387,
        0.999999657389255,
        0.997339620397001
      ],
      "excerpt": "1. [**JAKET: Joint Pre-training of Knowledge Graph and Language Understanding**](https://arxiv.org/pdf/2010.00796.pdf), *Donghan Yu, Chenguang Zhu, Yiming Yang, Michael Zeng* \n1. [**Architectural Implications of Graph Neural Networks**](https://arxiv.org/pdf/2009.00804.pdf), *Zhihui Zhang, Jingwen Leng, Lingxiao Ma, Youshan Miao, Chao Li, Minyi Guo* \n1. [**Combining Reinforcement Learning and Constraint Programming for Combinatorial Optimization**](https://arxiv.org/pdf/2006.01610.pdf), *Quentin Cappart, Thierry Moisan, Louis-Martin Rousseau1, Isabeau Pr\u00e9mont-Schwarz, and Andre Cire* \n1. [**Therapeutics Data Commons: Machine Learning Datasets and Tasks for Therapeutics**](https://arxiv.org/abs/2102.09548) ([code repo](https://github.com/mims-harvard/TDC)), *Kexin Huang, Tianfan Fu, Wenhao Gao, Yue Zhao, Yusuf Roohani, Jure Leskovec, Connor W. Coley, Cao Xiao, Jimeng Sun, Marinka Zitnik* \n1. [**Sparse Graph Attention Networks**](https://arxiv.org/abs/1912.00552), *Yang Ye, Shihao Ji* \n1. [**On Self-Distilling Graph Neural Network**](https://arxiv.org/pdf/2011.02255.pdf), *Yuzhao Chen, Yatao Bian, Xi Xiao, Yu Rong, Tingyang Xu, Junzhou Huang* \n1. [**Learning Robust Node Representations on Graphs**](https://arxiv.org/pdf/2008.11416.pdf), *Xu Chen, Ya Zhang, Ivor Tsang, and Yuangang Pan* \n1. [**Recurrent Event Network: Autoregressive Structure Inference over Temporal Knowledge Graphs**](https://arxiv.org/abs/1904.05530), *Woojeong Jin, Meng Qu, Xisen Jin, Xiang Ren* \n1. [**Graph Neural Ordinary Differential Equations**](https://arxiv.org/abs/1911.07532), *Michael Poli, Stefano Massaroli, Junyoung Park, Atsushi Yamashita, Hajime Asama, Jinkyoo Park* \n1. [**FusedMM: A Unified SDDMM-SpMM Kernel for Graph Embedding and Graph Neural Networks**](https://arxiv.org/pdf/2011.06391.pdf), *Md. Khaledur Rahman, Majedul Haque Sujon, , Ariful Azad* \n1. [**An Efficient Neighborhood-based Interaction Model for Recommendation on Heterogeneous Graph**](https://arxiv.org/pdf/2007.00216.pdf), KDD'20 *Jiarui Jin, Jiarui Qin, Yuchen Fang, Kounianhua Du, Weinan Zhang, Yong Yu, Zheng Zhang, Alexander J. Smola* \n1. [**Learning Interaction Models of Structured Neighborhood on Heterogeneous Information Network**](https://arxiv.org/pdf/2011.12683.pdf), *Jiarui Jin, Kounianhua Du, Weinan Zhang, Jiarui Qin, Yuchen Fang, Yong Yu, Zheng Zhang, Alexander J. Smola* \n1. [**Graphein - a Python Library for Geometric Deep Learning and Network Analysis on Protein Structures**](https://www.biorxiv.org/content/10.1101/2020.07.15.204701v1), *Arian R. Jamasb, Pietro Li\u00f3, Tom L. Blundell* \n1. [**Graph Policy Gradients for Large Scale Robot Control**](https://arxiv.org/abs/1907.03822), *Arbaaz Khan, Ekaterina Tolstaya, Alejandro Ribeiro, Vijay Kumar* \n1. [**Heterogeneous Molecular Graph Neural Networks for Predicting Molecule Properties**](https://arxiv.org/abs/2009.12710), *Zeren Shui, George Karypis* \n1. [**Could Graph Neural Networks Learn Better Molecular Representation for Drug Discovery? A Comparison Study of Descriptor-based and Graph-based Models**](https://assets.researchsquare.com/files/rs-81439/v1_stamped.pdf), *Dejun Jiang, Zhenxing Wu, Chang-Yu Hsieh, Guangyong Chen, Ben Liao, Zhe Wang, Chao Shen, Dongsheng Cao, Jian Wu, Tingjun Hou* \n1. [**Principal Neighbourhood Aggregation for Graph Nets**](https://arxiv.org/abs/2004.05718), *Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Li\u00f2, Petar Veli\u010dkovi\u0107* \n1. [**Collective Multi-type Entity Alignment Between Knowledge Graphs**](https://dl.acm.org/doi/abs/10.1145/3366423.3380289), *Qi Zhu, Hao Wei, Bunyamin Sisman, Da Zheng, Christos Faloutsos, Xin Luna Dong, Jiawei Han* \n1. [**Graph Representation Forecasting of Patient's Medical Conditions: towards A Digital Twin**](https://arxiv.org/abs/2009.08299), *Pietro Barbiero, Ramon Vi\u00f1as Torn\u00e9, Pietro Li\u00f3* \n1. [**Relational Graph Learning on Visual and Kinematics Embeddings for Accurate Gesture Recognition in Robotic Surgery**](https://arxiv.org/abs/2011.01619), *Yong-Hao Long, Jie-Ying Wu, Bo Lu, Yue-Ming Jin, Mathias Unberath, Yun-Hui Liu, Pheng-Ann Heng and Qi Dou* \n1. [**Dark Reciprocal-Rank: Boosting Graph-Convolutional Self-Localization Network via Teacher-to-student Knowledge Transfer**](https://arxiv.org/abs/2011.00402), *Takeda Koji, Tanaka Kanji* \n1. [**Graph InfoClust: Leveraging Cluster-Level Node Information For Unsupervised Graph Representation Learning**](https://arxiv.org/abs/2009.06946), *Costas Mavromatis, George Karypis* \n1. [**GraphSeam: Supervised Graph Learning Framework for Semantic UV Mapping**](https://arxiv.org/abs/2011.13748), *Fatemeh Teimury, Bruno Roy, Juan Sebastian Casallas, David macdonald, Mark Coates* \n1. [**Comprehensive Study on Molecular Supervised Learning with Graph Neural Networks**](https://pubs.acs.org/doi/10.1021/acs.jcim.0c00416), *Doyeong Hwang, Soojung Yang, Yongchan Kwon, Kyung Hoon Lee, Grace Lee, Hanseok Jo, Seyeol Yoon, and Seongok Ryu* \n1. [**A graph auto-encoder model for miRNA-disease associations prediction**](https://academic.oup.com/bib/advance-article-abstract/doi/10.1093/bib/bbaa240/5929824?redirectedFrom=fulltext), *Zhengwei Li, Jiashu Li, Ru Nie, Zhu-Hong You, Wenzheng Bao* \n1. [**Graph convolutional regression of cardiac depolarization from sparse endocardial maps**](https://arxiv.org/abs/2009.14068), STACOM 2020 workshop, *Felix Meister, Tiziano Passerini, Chlo\u00e9 Audigier, \u00c8ric Lluch, Viorel Mihalef, Hiroshi Ashikaga, Andreas Maier, Henry Halperin, Tommaso Mansi* \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9993667710026511,
        0.9998894953119913,
        0.9994669255401568,
        0.999886844020166,
        0.8927477029470567,
        0.9988779717517391,
        0.9968663744817429,
        0.9999999976001845,
        0.9641543344130075,
        0.9228060864426734
      ],
      "excerpt": "1. [**Inducing Alignment Structure with Gated Graph Attention Networks for Sentence Matching**](https://arxiv.org/abs/2010.07668), *Peng Cui, Le Hu, Yuanchao Liu* \n1. [**Enhancing Extractive Text Summarization with Topic-Aware Graph Neural Networks**](https://arxiv.org/abs/2010.06253), COLING'20, *Peng Cui, Le Hu, Yuanchao Liu* \n1. [**Double Graph Based Reasoning for Document-level Relation Extraction**](https://arxiv.org/abs/2009.13752), EMNLP'20, *Shuang Zeng, Runxin Xu, Baobao Chang, Lei Li* \n1. [**Systematic Generalization on gSCAN with Language Conditioned Embedding**](https://arxiv.org/abs/2009.05552), AACL-IJCNLP'20, *Tong Gao, Qi Huang, Raymond J. Mooney* \n1. [**Automatic selection of clustering algorithms using supervised graph embedding**](https://arxiv.org/pdf/2011.08225.pdf), *Noy Cohen-Shapira, Lior Rokach* \n1. [**Improving Learning to Branch via Reinforcement Learning**](https://openreview.net/forum?id=z4D7-PTxTb), *Haoran Sun, Wenbo Chen, Hui Li, Le Song* \n1. [**A Practical Guide to Graph Neural Networks**](https://arxiv.org/pdf/2010.05234.pdf), *Isaac Ronald Ward, Jack Joyner, Casey Lickfold, Stash Rowe, Yulan Guo, Mohammed Bennamoun*, [code](https://github.com/isolabs/gnn-tutorial) \n1. [**APAN: Asynchronous Propagation Attention Network for Real-time Temporal Graph Embedding**](https://arxiv.org/pdf/2011.11545.pdf), SIGMOD'21, *Xuhong Wang, Ding Lyu, Mengjian Li, Yang Xia, Qi Yang, Xinwen Wang, Xinguang Wang, Ping Cui, Yupu Yang, Bowen Sun, Zhenyu Guo, Junkui Li* \n1. [**Uncertainty-Matching Graph Neural Networks to Defend Against Poisoning Attacks**](https://arxiv.org/pdf/2009.14455.pdf), *Uday Shankar Shanthamallu, Jayaraman J. Thiagarajan, Andreas Spanias* \n1. [**Computing Graph Neural Networks: A Survey from Algorithms to Accelerators**](https://arxiv.org/pdf/2010.00130.pdf), *Sergi Abadal, Akshay Jain, Robert Guirado, Jorge L\u00f3pez-Alonso, Eduard Alarc\u00f3n* \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9939920415125988,
        0.8992670114299561,
        0.9974038995351353,
        0.998741122969861,
        0.9999786630194265,
        0.9995200714518394,
        0.991026739598457,
        0.9988755970880412,
        0.9999473000858546,
        0.9999999960717219,
        0.9998000629575255,
        0.9843294290125753,
        0.9996968581939406,
        0.997336256929996,
        0.8894037435088588,
        0.9981057174737893,
        0.977799577827207
      ],
      "excerpt": "1. [**Relation-aware Graph Attention Networks with Relational Position Encodings for Emotion Recognition in Conversations**](https://www.aclweb.org/anthology/2020.emnlp-main.597.pdf), *Taichi Ishiwatari, Yuki Yasuda, Taro Miyazaki, Jun Goto* \n1. [**PGM-Explainer: Probabilistic Graphical Model Explanations for Graph Neural Networks**](https://proceedings.neurips.cc/paper/2020/file/8fb134f258b1f7865a6ab2d935a897c9-Paper.pdf), *Minh N. Vu, My T. Thai* \n1. [**A Generalization of Transformer Networks to Graphs**](https://arxiv.org/pdf/2012.09699.pdf), *Vijay Prakash Dwivedi, Xavier Bresson* \n1. [**Discourse-Aware Neural Extractive Text Summarization**](https://www.aclweb.org/anthology/2020.acl-main.451.pdf), ACL'20, *Jiacheng Xu, Zhe Gan, Yu Cheng, Jingjing Liu* \n1. [**Learning Robust Node Representations on Graphs**](https://arxiv.org/abs/2008.11416), *Xu Chen, Ya Zhang, Ivor Tsang, Yuangang Pan* \n1. [**Adaptive Graph Diffusion Networks with Hop-wise Attention**](https://arxiv.org/abs/2012.15024), *Chuxiong Sun, Guoshi Wu* \n1. [**The Photoswitch Dataset: A Molecular Machine Learning Benchmark for the Advancement of Synthetic Chemistry**](https://arxiv.org/abs/2008.03226), *Aditya R. Thawani, Ryan-Rhys Griffiths, Arian Jamasb, Anthony Bourached, Penelope Jones, William McCorkindale, Alexander A. Aldrick, Alpha A. Lee* \n1. [**A community-powered search of machine learning strategy space to find NMR property prediction models**](https://arxiv.org/abs/2008.05994), *Lars A. Bratholm, Will Gerrard, Brandon Anderson, Shaojie Bai, Sunghwan Choi, Lam Dang, Pavel Hanchar, Addison Howard, Guillaume Huard, Sanghoon Kim, Zico Kolter, Risi Kondor, Mordechai Kornbluth, Youhan Lee, Youngsoo Lee, Jonathan P. Mailoa, Thanh Tu Nguyen, Milos Popovic, Goran Rakocevic, Walter Reade, Wonho Song, Luka Stojanovic, Erik H. Thiede, Nebojsa Tijanic, Andres Torrubia, Devin Willmott, Craig P. Butts, David R. Glowacki, Kaggle participants* \n1. [**Adaptive Layout Decomposition with Graph Embedding Neural Networks**](http://www.cse.cuhk.edu.hk/~byu/papers/C98-DAC2020-MPL-Selector.pdf), *Wei Li, Jialu Xia, Yuzhe Ma, Jialu Li, Yibo Lin, Bei Yu*, DAC'20 \n1. [**Transfer Learning with Graph Neural Networks for Optoelectronic Properties of Conjugated Oligomers**](https://aip.scitation.org/doi/10.1063/5.0037863), J. Chem. Phys. 154, *Chee-Kong Lee, Chengqiang Lu, Yue Yu, Qiming Sun, Chang-Yu Hsieh, Shengyu Zhang, Qi Liu, and  Liang Shi* \n1. [**Jet tagging in the Lund plane with graph networks**](https://link.springer.com/article/10.1007/JHEP03(2021)052), Journal of High Energy Physics 2021, *Fr\u00e9d\u00e9ric A. Dreyer and Huilin Qu* \n1. [**Global Attention Improves Graph Networks Generalization**](https://arxiv.org/abs/2006.07846), *Omri Puny, Heli Ben-Hamu, and Yaron Lipman* \n1. [**Learning over Families of Sets -- Hypergraph Representation Learning for Higher Order Tasks**](https://arxiv.org/abs/2101.07773), SDM 2021, *Balasubramaniam Srinivasan, Da Zheng, and George Karypis* \n1. [**SSFG: Stochastically Scaling Features and Gradients for Regularizing Graph Convolution Networks**](https://arxiv.org/abs/2102.10338), *Haimin Zhang, Min Xu* \n1. [**Application and evaluation of knowledge graph embeddings in biomedical data**](https://peerj.com/articles/cs-341/), PeerJ Computer Science 7:e341, *Mona Alshahrani\u200b, Maha A. Thafar, Magbubah Essack* \n1. [**MoTSE: an interpretable task similarity estimator for small molecular property prediction tasks**](https://www.biorxiv.org/content/10.1101/2021.01.13.426608v2), bioRxiv 2021.01.13.426608, *Han Li, Xinyi Zhao, Shuya Li, Fangping Wan, Dan Zhao, Jianyang Zeng* \n1. [**Reinforcement Learning For Data Poisoning on Graph Neural Networks**](https://arxiv.org/abs/2102.06800), *Jacob Dineen, A S M Ahsan-Ul Haque, Matthew Bielskas* \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999999993791278,
        0.9999386139744889
      ],
      "excerpt": "1. [**Joint stroke classification and text line grouping in online handwritten documents with edge pooling attention networks**](https://www.sciencedirect.com/science/article/abs/pii/S0031320321000467), Pattern Recognition, *Jun-Yu Ye, Yan-Ming Zhang, Qing Yang, Cheng-Lin Liu* \n1. [**Toward Accurate Predictions of Atomic Properties via Quantum Mechanics Descriptors Augmented Graph Convolutional Neural Network: Application of This Novel Approach in NMR Chemical Shifts Predictions**](https://pubs.acs.org/doi/full/10.1021/acs.jpclett.0c02654), The Journal of Physical Chemistry Letters, *Peng Gao, Jie Zhang, Yuzhu Sun, and Jianguo Yu* \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9959940864906657,
        0.9403508498243055,
        0.9533961576247013,
        0.9999999952639911,
        0.9998287985447475,
        0.9999986995935828,
        0.9999607540355129,
        0.8881287336968915,
        0.9979700129894682,
        0.9968213789275826,
        0.9836618219703566,
        0.995875201154818,
        0.999702154891163,
        0.8632114710380886,
        0.9998733364975461,
        0.9999388168315072,
        0.9989406700621934,
        0.9983246870030056,
        0.9984763368112034,
        0.9697723085370433,
        0.9999579228460109,
        0.9994845203199795,
        0.999676658238791,
        0.998540216294799
      ],
      "excerpt": "1. [**Medical Entity Disambiguation Using Graph Neural Networks**](https://arxiv.org/abs/2104.01488), *Alina Vretinaris, Chuan Lei, Vasilis Efthymiou, Xiao Qin, Fatma \u00d6zcan* \n1. [**Chemistry-informed Macromolecule Graph Representation for Similarity Computation and Supervised Learning**](https://arxiv.org/abs/2103.02565), *Somesh Mohapatra, Joyce An, Rafael G\u00f3mez-Bombarelli* \n1. [**Characterizing and Forecasting User Engagement with In-app Action Graph: A Case Study of Snapchat**](https://arxiv.org/pdf/1906.00355.pdf), *Yozen Liu, Xiaolin Shi, Lucas Pierce, Xiang Ren* \n1. [**GIPA: General Information Propagation Algorithm for Graph Learning**](https://arxiv.org/abs/2105.06035), *Qinkai Zheng, Houyi Li, Peng Zhang, Zhixiong Yang, Guowei Zhang, Xintan Zeng, Yongchao Liu* \n1. [**Graph Ensemble Learning over Multiple Dependency Trees for Aspect-level Sentiment Classification**](https://arxiv.org/abs/2103.11794), NAACL'21, *Xiaochen Hou, Peng Qi, Guangtao Wang, Rex Ying, Jing Huang, Xiaodong He, Bowen Zhou* \n1. [**Enhancing Scientific Papers Summarization with Citation Graph**](https://arxiv.org/abs/2104.03057), AAAI'21, *Chenxin An, Ming Zhong, Yiran Chen, Danqing Wang, Xipeng Qiu, Xuanjing Huang* \n1. [**Improving Graph Representation Learning by Contrastive Regularization**](https://arxiv.org/pdf/2101.11525.pdf), *Kaili Ma, Haochen Yang, Han Yang, Tatiana Jin, Pengfei Chen, Yongqiang Chen, Barakeel Fanseu Kamhoua, James Cheng* \n1. [**Extract the Knowledge of Graph Neural Networks and Go Beyond it: An Effective Knowledge Distillation Framework**](https://arxiv.org/pdf/2103.02885.pdf), WWW'21, *Cheng Yang, Jiawei Liu, Chuan Shi* \n1. [**VIKING: Adversarial Attack on Network Embeddings via Supervised Network Poisoning**](https://arxiv.org/pdf/2102.07164.pdf), PAKDD'21, *Viresh Gupta, Tanmoy Chakraborty* \n1. [**Knowledge Graph Embedding using Graph Convolutional Networks with Relation-Aware Attention**](https://arxiv.org/pdf/2102.07200.pdf), *Nasrullah Sheikh, Xiao Qin, Berthold Reinwald, Christoph Miksovic, Thomas Gschwind, Paolo Scotton* \n1. [**SLAPS: Self-Supervision Improves Structure Learning for Graph Neural Networks**](https://arxiv.org/pdf/2102.05034.pdf), *Bahare Fatemi, Layla El Asri, Seyed Mehran Kazemi* \n1. [**Finding Needles in Heterogeneous Haystacks**](https://homepage.divms.uiowa.edu/~badhikari/assets/doc/papers/CONGCNIAAI2021.pdf), AAAI'21, *Bijaya Adhikari, Liangyue Li, Nikhil Rao, Karthik Subbian* \n1. [**RetCL: A Selection-based Approach for Retrosynthesis via Contrastive Learning**](https://arxiv.org/abs/2105.00795), IJCAI 2021, *Hankook Lee, Sungsoo Ahn, Seung-Woo Seo, You Young Song, Eunho Yang, Sung-Ju Hwang, Jinwoo Shin* \n1. [**Accurate Prediction of Free Solvation Energy of Organic Molecules via Graph Attention Network and Message Passing Neural Network from Pairwise Atomistic Interactions**](https://arxiv.org/abs/2105.02048), *Ramin Ansari, Amirata Ghorbani* \n1. [**DIPS-Plus: The Enhanced Database of Interacting Protein Structures for Interface Prediction**](https://arxiv.org/abs/2106.04362), *Alex Morehead, Chen Chen, Ada Sedova, Jianlin Cheng* \n1. [**Coreference-Aware Dialogue Summarization**](https://arxiv.org/abs/2106.08556), SIGDIAL'21, *Zhengyuan Liu, Ke Shi, Nancy F. Chen* \n1. [**Document Structure aware Relational Graph Convolutional Networks for Ontology Population**](https://arxiv.org/abs/2104.12950), arXiv, *Abhay M Shalghar, Ayush Kumar, Balaji Ganesan, Aswin Kannan, Shobha G* \n1. [**Covid-19 Detection from Chest X-ray and Patient Metadata using Graph Convolutional Neural Networks**](https://arxiv.org/abs/2105.09720), *Thosini Bamunu Mudiyanselage, Nipuna Senanayake, Chunyan Ji, Yi Pan, Yanqing Zhang* \n1. [**Rossmann-toolbox: a deep learning-based protocol for the prediction and design of cofactor specificity in Rossmann fold proteins**](https://academic.oup.com/bib/advance-article/doi/10.1093/bib/bbab371/6375059), Briefings in Bioinformatics, *Kamil Kaminski, Jan Ludwiczak, Maciej Jasinski, Adriana Bukala, Rafal Madaj, Krzysztof Szczepaniak, Stanislaw Dunin-Horkawicz* \n1. [**LGESQL: Line Graph Enhanced Text-to-SQL Model with Mixed Local and Non-Local Relations**](https://arxiv.org/pdf/2106.01093.pdf), ACL'21, *Ruisheng Cao, Lu Chen, Zhi Chen, Yanbin Zhao, Su Zhu, Kai Yu* \n1. [**Enhancing Graph Neural Networks via auxiliary training for semi-supervised node classification**](https://www.sciencedirect.com/science/article/pii/S0950705121001477), Knowledge-Based System'21, *Yao Wu, Yu Song, Hong Huang, Fanghua Ye, Xing Xie, Hai Jin* \n1. [**Modeling Graph Node Correlations with Neighbor Mixture Models**](https://arxiv.org/pdf/2103.15966.pdf), *Linfeng Liu, Michael C. Hughes, Li-Ping Liu* \n1. [**COMBINING PHYSICS AND MACHINE LEARNING FOR NETWORK FLOW ESTIMATION**](https://openreview.net/pdf/9dc2744a465941220de07cf308acf822ec8aaa64.pdf), ICLR'21, *Arlei Silva, Furkan Kocayusufoglu, Saber Jafarpour, Francesco Bullo, Ananthram Swami, Ambuj Singh* \n1. [**A Classification Method for Academic Resources Based on a Graph Attention Network**](https://www.mdpi.com/1999-5903/13/3/64/htm), Future Internet'21, *Jie Yu, Yaliu Li, Chenle Pan and Junwei Wang* \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999996455393247,
        0.999996605532735,
        0.9970440741706947
      ],
      "excerpt": "1. [**Graph Attention Multi-Layer Perception**](https://github.com/PKU-DAIR/GAMLP/blob/main/GAMLP.pdf), *Wentao Zhang, Ziqi Yin, Zeang Sheng, Wen Ouyang, Xiaosen Li, Yangyu Tao, Zhi Yang, Bin Cui* \n1. [**GNNLens: A Visual Analytics Approach for Prediction Error Diagnosis of Graph Neural Networks**](https://arxiv.org/abs/2011.11048v5), *Zhihua Jin, Yong Wang, Qianwen Wang, Yao Ming, Tengfei Ma, Huamin Qu* \n1. [**How Attentive are Graph Attention Networks?**](https://arxiv.org/pdf/2105.14491.pdf), *Shaked Brody, Uri Alon, Eran Yahav*, [code](https://github.com/tech-srl/how_attentive_are_gats) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dmlc/dgl",
    "technique": "GitHub API"
  },
  "contributors": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contributing to DGL\nContribution is always welcomed. A good starting place is the roadmap issue, where\nyou can find our current milestones. All contributions must go through pull requests\nand be reviewed by the committers. See our contribution\nguide for more details.\nOnce your contribution is accepted and merged, congratulations, you are now a\ncontributor to the DGL project.  We will put your name in the list below.\nContributors\n\nMinjie Wang from AWS\nDa Zheng from AWS\nQuan Gan from AWS\nMufei Li from AWS\nJinjing Zhou from AWS\nXiang Song from AWS\nTianjun Xiao from AWS\nTong He from AWS\nJian Zhang from AWS\nQipeng Guo from AWS\nXiangkun Hu from AWS\nYing Rui from AWS\nIsrat Nisa from AWS\nZheng Zhang from AWS\nZihao Ye from University of Washington\nChao Ma\nQidong\nLingfan Yu from New York University\nYu Gai from University of California, Berkeyley\nQi Huang from New York University\nDominique LaSalle from Nvidia\nPawel Piotrowcz from Intel\nMichal Szarmach from Intel\nIzabela Mazur from Intel\nSanchit Misra from Intel\nSheng Zha from AWS\nYifei Ma from  AWS\nYizhi Liu from AWS\nKay Liu from UIC\nTianqi Zhang from SJTU\nHengrui Zhang\nSeung Won Min from UIUC\n@hbsun2113: GraphSAGE in PyTorch\nTianyi Zhang: SGC in PyTorch\nJun Chen: GIN in PyTorch\nAymen Waheb: APPNP in PyTorch\nChengqiang Lu: MGCN, SchNet and MPNN in PyTorch\nGongze Cao: Cluster GCN\nYicheng Wu: RotatE in PyTorch\nHao Xiong: DeepWalk in PyTorch\nZhi Lin: Integrate FeatGraph into DGL\nAndrew Tsesis: Framework-Agnostic Graph Ops\nBrett Koonce\n@giuseppefutia\n@mori97\n@xnuohz\nHao Jin from Amazon\nXin Yao from Nvidia\nAbdurrahman Yasar from Nvidia\nShaked Brody from Technion",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-04-20T14:49:09Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-22T12:21:01Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9887960606482273,
        0.9987451080510448,
        0.9703927858734442,
        0.9185026632587524,
        0.836812447122004,
        0.8596094355856246
      ],
      "excerpt": "DGL provides a powerful graph object that can reside on either CPU or GPU. It bundles structural data as well as features for a better control. We provide a variety of functions for computing with graph objects including efficient and customizable message passing primitives for Graph Neural Networks. \nThe field of graph deep learning is still rapidly evolving and many research ideas emerge by standing on the shoulders of giants. To ease the process, DGL collects a rich set of example implementations of popular GNN models of a wide range of topics. Researchers can search for related models to innovate new ideas from or use them as baselines for experiments. Moreover, DGL provides many state-of-the-art GNN layers and modules for users to build new model architectures. DGL is one of the preferred platforms for many standard graph deep learning benchmarks including OGB and GNNBenchmarks. \nIt is convenient to train models using DGL on large-scale graphs across multiple GPUs or multiple machines. DGL extensively optimizes the whole stack to reduce the overhead in communication, memory consumption and synchronization. As a result, DGL can easily scale to billion-sized graphs. See the system performance note for the comparison with the other tools. \nNow DistDGL ParMETIS implementation also provides support for hetero graph by adding back the dropped edges back into the partitioned graph thus handling parMetis hetero graph usecase. \nDGL-LifeSci: a DGL-based package for various applications in life science with graph neural networks. https://github.com/awslabs/dgl-lifesci \nDGL-KE: a high performance, easy-to-use, and scalable package for learning large-scale knowledge graph embeddings. https://github.com/awslabs/dgl-ke \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8880738203581411,
        0.9541305977652395
      ],
      "excerpt": "OGB: a collection of realistic, large-scale, and diverse benchmark datasets for machine learning on graphs. https://ogb.stanford.edu/ \nGraph4NLP: an easy-to-use library for R&D at the intersection of Deep Learning on Graphs and Natural Language Processing. https://github.com/graph4ai/graph4nlp \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9696022454815831
      ],
      "excerpt": "Amazon Neptune ML: a new capability of Neptune that uses Graph Neural Networks (GNNs), a machine learning technique purpose-built for graphs, to make easy, fast, and more accurate predictions using graph data. https://aws.amazon.com/cn/neptune/machine-learning/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8102510326960723
      ],
      "excerpt": "DropEdge: Towards Deep Graph Convolutional Networks on Node Classification, ICLR'20, Yu Rong, Wenbing Huang, Tingyang Xu, Junzhou Huan \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8812192128516243
      ],
      "excerpt": "12. [**OptiMol: Optimization of Binding Affinities in Chemical Space for Drug Discovery**](https://www.biorxiv.org/content/biorxiv/early/2020/06/16/2020.05.23.112201.full.pdf), *Jacques Boitreaud,Vincent Mallet, Carlos Oliver, J\u00e9r\u00f4me Waldisp\u00fchl* \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8458025873071822
      ],
      "excerpt": "1. [**Could Graph Neural Networks Learn Better Molecular Representation for Drug Discovery? A Comparison Study of Descriptor-based and Graph-based Models**](https://assets.researchsquare.com/files/rs-81439/v1_stamped.pdf), *Dejun Jiang, Zhenxing Wu, Chang-Yu Hsieh, Guangyong Chen, Ben Liao, Zhe Wang, Chao Shen, Dongsheng Cao, Jian Wu, Tingjun Hou* \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9119346130203942
      ],
      "excerpt": "1. [**Combining Self-Organizing and Graph Neural Networks for Modeling Deformable Objects in Robotic Manipulation**](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7806087/), Frotiers in Robotics and AI, *Valencia, Angel J., and Pierre Payeur* \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Python package built to ease deep learning on graph, on top of existing DL frameworks.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dmlc/dgl/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1950,
      "date": "Thu, 23 Dec 2021 00:52:02 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/dmlc/dgl/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "dmlc/dgl",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/dmlc/dgl/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/dmlc/dgl/master/conda/dgl/run_test.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/conda/dgl/build.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/benchmarks/run.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/benchmarks/scripts/build_dgl_asv.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/benchmarks/scripts/install_dgl_asv.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/benchmarks/scripts/publish.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/tests/scripts/task_unit_test.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/tests/scripts/task_lint.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/tests/scripts/task_example_test.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/tests/scripts/build_dgl.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/tests/scripts/task_pytorch_tutorial_test.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/tests/scripts/task_mxnet_tutorial_test.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/tests/scripts/task_cpp_unit_test.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/docker/install/ubuntu_install_conda.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/docker/install/ubuntu_install_torch.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/docker/install/ubuntu_install_mxnet_gpu.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/docker/install/ubuntu_install_java.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/docker/install/ubuntu_install_mxnet_cpu.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/docker/install/ubuntu_install_python_package.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/docker/install/ubuntu_install_torch_1.2.0.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/docker/install/ubuntu_install_antlr.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/docker/install/ubuntu_install_python.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/docker/install/ubuntu_install_core.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/docker/install/ubuntu_install_build.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/examples/mxnet/scenegraph/validate_reldn.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/examples/mxnet/scenegraph/train_reldn.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/examples/mxnet/scenegraph/train_faster_rcnn.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/examples/pytorch/graphwriter/test.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/examples/pytorch/graphwriter/prepare_data.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/examples/pytorch/graphwriter/run.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/examples/pytorch/hilander/scripts/train_inat.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/examples/pytorch/hilander/scripts/train_deepglint.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/examples/pytorch/hilander/scripts/test_deepglint_imdb_sampled_as_deepglint.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/examples/pytorch/hilander/scripts/test_deepglint_hannah.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/examples/pytorch/hilander/scripts/test_deepglint_imdb.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/examples/pytorch/hilander/scripts/test_inat.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/examples/pytorch/hilander/scripts/train_inat_resampled_1_in_6_per_class.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/examples/pytorch/hilander/scripts/test_inat_train_on_resampled_1_in_6_per_class.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/examples/pytorch/compGCN/get_wn18rr.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/examples/pytorch/compGCN/get_fb15k-237.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/examples/pytorch/cluster_gcn/run_ppi.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/examples/pytorch/cluster_gcn/run_reddit.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/examples/pytorch/rgcn/experimental/preprocessing_dist_training/pre_process_dist_training.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/examples/pytorch/gxn/scripts/run_gxn_early_stop.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/examples/pytorch/gxn/scripts/run_gxn.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/examples/pytorch/GATNE-T/scripts/run_example.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/examples/pytorch/GATNE-T/scripts/run_example_sparse.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/examples/pytorch/GATNE-T/scripts/run_example_sparse_multi_gpus.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/examples/pytorch/NGCF/Data/load_gowalla.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/examples/pytorch/NGCF/Data/load_amazon-book.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/tensoradapter/pytorch/build.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/apps/kg/distributed/launch.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/apps/kg/distributed/fb15k_transe_l2.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/apps/kg/distributed/freebase_complex.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/apps/kg/distributed/freebase_distmult.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/apps/kg/distributed/freebase_transe_l2.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/apps/kg/distributed/partition.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/apps/kg/config/best_config.sh",
      "https://raw.githubusercontent.com/dmlc/dgl/master/docs/clean.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8918974083095406
      ],
      "excerpt": "Benchmarking GNN: https://github.com/graphdeeplearning/benchmarking-gnns \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8918974083095406
      ],
      "excerpt": "GNN-RecSys: https://github.com/je-dbl/GNN-RecSys \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8928022221395381
      ],
      "excerpt": "1. [**Learning from Non-Binary Constituency Trees via Tensor Decomposition**](https://github.com/danielecastellana22/tensor-tree-nn), COLING'20, *Daniele Castellana, Davide Bacciu* \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8646918260842117
      ],
      "excerpt": "1. [**Generalising Recursive Neural Models by Tensor Decomposition**](https://github.com/danielecastellana22/tensor-tree-nn), IJCNN'20, *Daniele Castellana, Davide Bacciu* \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/dmlc/dgl/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "C++",
      "Cuda",
      "CMake",
      "Shell",
      "C",
      "Cython",
      "Batchfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Apache License 2.0",
      "url": "https://api.github.com/licenses/apache-2.0"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'                                 Apache License\\n                           Version 2.0, January 2004\\n                        http://www.apache.org/licenses/\\n\\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\\n\\n   1. Definitions.\\n\\n      \"License\" shall mean the terms and conditions for use, reproduction,\\n      and distribution as defined by Sections 1 through 9 of this document.\\n\\n      \"Licensor\" shall mean the copyright owner or entity authorized by\\n      the copyright owner that is granting the License.\\n\\n      \"Legal Entity\" shall mean the union of the acting entity and all\\n      other entities that control, are controlled by, or are under common\\n      control with that entity. For the purposes of this definition,\\n      \"control\" means (i) the power, direct or indirect, to cause the\\n      direction or management of such entity, whether by contract or\\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\\n      outstanding shares, or (iii) beneficial ownership of such entity.\\n\\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\\n      exercising permissions granted by this License.\\n\\n      \"Source\" form shall mean the preferred form for making modifications,\\n      including but not limited to software source code, documentation\\n      source, and configuration files.\\n\\n      \"Object\" form shall mean any form resulting from mechanical\\n      transformation or translation of a Source form, including but\\n      not limited to compiled object code, generated documentation,\\n      and conversions to other media types.\\n\\n      \"Work\" shall mean the work of authorship, whether in Source or\\n      Object form, made available under the License, as indicated by a\\n      copyright notice that is included in or attached to the work\\n      (an example is provided in the Appendix below).\\n\\n      \"Derivative Works\" shall mean any work, whether in Source or Object\\n      form, that is based on (or derived from) the Work and for which the\\n      editorial revisions, annotations, elaborations, or other modifications\\n      represent, as a whole, an original work of authorship. For the purposes\\n      of this License, Derivative Works shall not include works that remain\\n      separable from, or merely link (or bind by name) to the interfaces of,\\n      the Work and Derivative Works thereof.\\n\\n      \"Contribution\" shall mean any work of authorship, including\\n      the original version of the Work and any modifications or additions\\n      to that Work or Derivative Works thereof, that is intentionally\\n      submitted to Licensor for inclusion in the Work by the copyright owner\\n      or by an individual or Legal Entity authorized to submit on behalf of\\n      the copyright owner. For the purposes of this definition, \"submitted\"\\n      means any form of electronic, verbal, or written communication sent\\n      to the Licensor or its representatives, including but not limited to\\n      communication on electronic mailing lists, source code control systems,\\n      and issue tracking systems that are managed by, or on behalf of, the\\n      Licensor for the purpose of discussing and improving the Work, but\\n      excluding communication that is conspicuously marked or otherwise\\n      designated in writing by the copyright owner as \"Not a Contribution.\"\\n\\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\\n      on behalf of whom a Contribution has been received by Licensor and\\n      subsequently incorporated within the Work.\\n\\n   2. Grant of Copyright License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      copyright license to reproduce, prepare Derivative Works of,\\n      publicly display, publicly perform, sublicense, and distribute the\\n      Work and such Derivative Works in Source or Object form.\\n\\n   3. Grant of Patent License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      (except as stated in this section) patent license to make, have made,\\n      use, offer to sell, sell, import, and otherwise transfer the Work,\\n      where such license applies only to those patent claims licensable\\n      by such Contributor that are necessarily infringed by their\\n      Contribution(s) alone or by combination of their Contribution(s)\\n      with the Work to which such Contribution(s) was submitted. If You\\n      institute patent litigation against any entity (including a\\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\\n      or a Contribution incorporated within the Work constitutes direct\\n      or contributory patent infringement, then any patent licenses\\n      granted to You under this License for that Work shall terminate\\n      as of the date such litigation is filed.\\n\\n   4. Redistribution. You may reproduce and distribute copies of the\\n      Work or Derivative Works thereof in any medium, with or without\\n      modifications, and in Source or Object form, provided that You\\n      meet the following conditions:\\n\\n      (a) You must give any other recipients of the Work or\\n          Derivative Works a copy of this License; and\\n\\n      (b) You must cause any modified files to carry prominent notices\\n          stating that You changed the files; and\\n\\n      (c) You must retain, in the Source form of any Derivative Works\\n          that You distribute, all copyright, patent, trademark, and\\n          attribution notices from the Source form of the Work,\\n          excluding those notices that do not pertain to any part of\\n          the Derivative Works; and\\n\\n      (d) If the Work includes a \"NOTICE\" text file as part of its\\n          distribution, then any Derivative Works that You distribute must\\n          include a readable copy of the attribution notices contained\\n          within such NOTICE file, excluding those notices that do not\\n          pertain to any part of the Derivative Works, in at least one\\n          of the following places: within a NOTICE text file distributed\\n          as part of the Derivative Works; within the Source form or\\n          documentation, if provided along with the Derivative Works; or,\\n          within a display generated by the Derivative Works, if and\\n          wherever such third-party notices normally appear. The contents\\n          of the NOTICE file are for informational purposes only and\\n          do not modify the License. You may add Your own attribution\\n          notices within Derivative Works that You distribute, alongside\\n          or as an addendum to the NOTICE text from the Work, provided\\n          that such additional attribution notices cannot be construed\\n          as modifying the License.\\n\\n      You may add Your own copyright statement to Your modifications and\\n      may provide additional or different license terms and conditions\\n      for use, reproduction, or distribution of Your modifications, or\\n      for any such Derivative Works as a whole, provided Your use,\\n      reproduction, and distribution of the Work otherwise complies with\\n      the conditions stated in this License.\\n\\n   5. Submission of Contributions. Unless You explicitly state otherwise,\\n      any Contribution intentionally submitted for inclusion in the Work\\n      by You to the Licensor shall be under the terms and conditions of\\n      this License, without any additional terms or conditions.\\n      Notwithstanding the above, nothing herein shall supersede or modify\\n      the terms of any separate license agreement you may have executed\\n      with Licensor regarding such Contributions.\\n\\n   6. Trademarks. This License does not grant permission to use the trade\\n      names, trademarks, service marks, or product names of the Licensor,\\n      except as required for reasonable and customary use in describing the\\n      origin of the Work and reproducing the content of the NOTICE file.\\n\\n   7. Disclaimer of Warranty. Unless required by applicable law or\\n      agreed to in writing, Licensor provides the Work (and each\\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\\n      implied, including, without limitation, any warranties or conditions\\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\\n      PARTICULAR PURPOSE. You are solely responsible for determining the\\n      appropriateness of using or redistributing the Work and assume any\\n      risks associated with Your exercise of permissions under this License.\\n\\n   8. Limitation of Liability. In no event and under no legal theory,\\n      whether in tort (including negligence), contract, or otherwise,\\n      unless required by applicable law (such as deliberate and grossly\\n      negligent acts) or agreed to in writing, shall any Contributor be\\n      liable to You for damages, including any direct, indirect, special,\\n      incidental, or consequential damages of any character arising as a\\n      result of this License or out of the use or inability to use the\\n      Work (including but not limited to damages for loss of goodwill,\\n      work stoppage, computer failure or malfunction, or any and all\\n      other commercial damages or losses), even if such Contributor\\n      has been advised of the possibility of such damages.\\n\\n   9. Accepting Warranty or Additional Liability. While redistributing\\n      the Work or Derivative Works thereof, You may choose to offer,\\n      and charge a fee for, acceptance of support, warranty, indemnity,\\n      or other liability obligations and/or rights consistent with this\\n      License. However, in accepting such obligations, You may act only\\n      on Your own behalf and on Your sole responsibility, not on behalf\\n      of any other Contributor, and only if You agree to indemnify,\\n      defend, and hold each Contributor harmless for any liability\\n      incurred by, or claims asserted against, such Contributor by reason\\n      of your accepting any such warranty or additional liability.\\n\\n   END OF TERMS AND CONDITIONS\\n\\n   APPENDIX: How to apply the Apache License to your work.\\n\\n      To apply the Apache License to your work, attach the following\\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\\n      replaced with your own identifying information. (Don\\'t include\\n      the brackets!)  The text should be enclosed in the appropriate\\n      comment syntax for the file format. We also recommend that a\\n      file or class name and description of purpose be included on the\\n      same \"printed page\" as the copyright notice for easier\\n      identification within third-party archives.\\n\\n   Copyright [yyyy] [name of copyright owner]\\n\\n   Licensed under the Apache License, Version 2.0 (the \"License\");\\n   you may not use this file except in compliance with the License.\\n   You may obtain a copy of the License at\\n\\n       http://www.apache.org/licenses/LICENSE-2.0\\n\\n   Unless required by applicable law or agreed to in writing, software\\n   distributed under the License is distributed on an \"AS IS\" BASIS,\\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n   See the License for the specific language governing permissions and\\n   limitations under the License.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Highlighted Features",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "dgl",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "dmlc",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dmlc/dgl/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "BarclayII",
        "body": "# 0.7.2 Release Notes\r\n\r\nThis is a patch release targeting CUDA 11.3 and PyTorch 1.10. It contains (1) distributed training on heterogeneous graphs, and (2) bug fixes and code reorganization commits. The performance impact should be minimal.\r\n\r\nTo install with CUDA 11.3 support, run either\r\n```\r\npip install dgl-cu113 -f https://data.dgl.ai/wheels/repo.html\r\n```\r\nor\r\n```\r\nconda install -c dglteam dgl-cuda11.3\r\n```\r\n\r\n## Distributed Training on Heterogeneous Graphs\r\n\r\nWe have made the interface of distributed sampling on heterogeneous graph consistent with single-machine code.  Please refer to https://github.com/dmlc/dgl/blob/0.7.x/examples/pytorch/rgcn/experimental/entity_classify_dist.py for the new code.\r\n\r\n## Other fixes\r\n\r\n* [Bugfix] Fix bugs of farthest_point_sampler (#3327, @sangyx)\r\n* [Bugfix] Fix sparse embeddings for PyTorch < 1.7 #3291 (#3333)\r\n* Fixes bug in hg.update_all causing crash #3312 (#3345, @sanchit-misra)\r\n* [Bugfix] And PYTHONPATH in server launch. (#3352)\r\n* [CPU][Sampling][Performance] Improve sampling on the CPU. (#3274, @nv-dlasalle)\r\n* [Performance, CPU] Rewriting OpenMP pragmas into parallel_for (#3171, @tpatejko)\r\n* [Build] Fix OpenMP header inclusion for Mac builds (#3325)\r\n* [Performance] improve coo2csr space complexity when row is not sorted (#3326)\r\n* [BugFix] initialize data if null when converting from row sorted coo to csr (#3360)\r\n* fix broadcast tensor dim in `dgl.broadcast_nodes` (#3351, @jwyyy)\r\n* [BugFix] fix typo in fakenews dataset variable name (#3363, @kayzliu)\r\n* [Doc] Added md5sum info for OGB-LSC dataset (#3332, @msharmavikram)\r\n* [Feature] Graceful handling of exceptions thrown within OpenMP blocks (#3353)\r\n* Fix torch import in example (#3372, @jwyyy)\r\n* [Distributed] Allow user to pass-in extra env parameters when launching a distributed training task. (#3375)\r\n* [BugFix] extract gz into target dir (#3389)\r\n* [Model] Refine GraphSAINT (#3328 @ljh1064126026 )\r\n* [Bug] check dtype before convert to gk (#3414)\r\n* [BugFix] add count_nonzero() into SA_Client (#3417)\r\n* [Bug] Do not skip graphconv even no edge exists (#3416)\r\n* Fix edge ID exclusion when both g and g_sampling are specified in EdgeDataLoader(#3322)\r\n* [Bugfix] three bugs related to using DGL as a subdirectory(third_party) of another project. (#3379, @yuanzexi )\r\n* [PyTorch][Bugfix] Use uint8 instead of bool in pytorch to be compatible with nightly version (#3406, #3454, @nv-dlasalle)\r\n* [Fix] Use ==/!= to compare constant literals (str, bytes, int, float, tuple) (#3415, @cclauss)\r\n* [Bugfix][Pytorch] Fix model save and load bug of stgcn_wave (#3303, @HaoWei-TomTom )\r\n* [BugFix] Avoid Memory Leak Issue in PyTorch Backend (#3386, @chwan-rice )\r\n* [Fix] Split nccl sparse push into two groups (#3404, @nv-dlasalle )\r\n* [Doc] remove duplicate papers (#3393, @chwan-rice )\r\n* Fix GINConv backward #3437 (#3440)\r\n* [bugfix] Fix compilation with CUDA 11.5's CUB (#3468, @nv-dlasalle )\r\n* [Example][Performance] Enable faster validation for pytorch graphsage example (#3361, @nv-dlasalle )\r\n* [Doc] Evaluation Tutorial for Link Prediction (#3463)",
        "dateCreated": "2021-11-05T05:13:11Z",
        "datePublished": "2021-11-08T04:09:08Z",
        "html_url": "https://github.com/dmlc/dgl/releases/tag/0.7.2",
        "name": "0.7.2",
        "tag_name": "0.7.2",
        "tarball_url": "https://api.github.com/repos/dmlc/dgl/tarball/0.7.2",
        "url": "https://api.github.com/repos/dmlc/dgl/releases/52888349",
        "zipball_url": "https://api.github.com/repos/dmlc/dgl/zipball/0.7.2"
      },
      {
        "authorType": "User",
        "author_name": "BarclayII",
        "body": "# 0.7.1 Release Notes\r\n\r\n0.7.1 is a minor release with multiple fixes and a few new models/features/optimizations included as follows.\r\n\r\n**Note: We noticed that 0.7.1 for Linux is unavailable on our anaconda repository.  We are currently working on this issue.  For now, please use pip installation instead.**\r\n\r\n## New models\r\n\r\n* GCN-based spam review detection (#3145, @kayzliu)\r\n* CARE-GNN (#3187, @kayzliu)\r\n* GeniePath (#3199, @kayzliu)\r\n* EEG-GCNN (#3186, @JOHNW02)\r\n* EvolveGCN (#3190, @maqy1995)\r\n\r\n## New Features\r\n\r\n* Allows providing username in `tools/launch.py` (#3202, @erickim555)\r\n* Refactor and allows customized Python binary names in `tools/launch.py` (#3205, @erickim555)\r\n* Add support for distributed preprocessing for heterogeneous graphs (#3137, @ankit-garg)\r\n* Correctly pass all DGL client server environment variables for user-defined multi-command (#3245, @erickim555)\r\n* You can configure the DGL configuration directory with environment variable `DGLDEFAULTDIR` (#3277, @konstantino)\r\n\r\n## Optimizations\r\n\r\n* Improve usage of pinned memory in sparse optimizer (#3207, @nv-dlasalle)\r\n* Optimized counting of nonzero entries of DistTensor (#3203, @freeliuzc)\r\n* Remove activation cache if not required (#3258)\r\n* Edge excluding in EdgeDataLoader on GPU (#3226, @nv-dlasalle)\r\n\r\n## Fixes\r\n\r\n* Update numbers for HiLANDER model (#3175)\r\n* New training and test scripts for HiLANDER (#3180)\r\n* Fix potential starving in socket receiver (#3176, @JingchengYu94)\r\n* Fix typo in Tensorflow backend (#3182, @lululxvi)\r\n* Add WeightBasis documentation (#3189)\r\n* Default ntypes/etypes consistency between `dgl.DGLGraph` and `dgl.graph` (#3198)\r\n* Set sharing strategy for SEAL example (#3167, @KounianhuaDu)\r\n* Remove `DGL_LOADALL` in doc builds (#3150, @lululxvi)\r\n* Fix distributed training hang with multiple samplers (#3169)\r\n* Fix `random_walk` documentation inconsistency (#3188)\r\n* Fix `curand_init()` calls in rowwise sampling leading to not-so-random results (#3196, @nv-dlasalle)\r\n* Fix `force_reload` parameter of `FraudDataset` (#3210, @Orion-wyc)\r\n* Fix check for `num_workers` for using `ScalarDataBatcher` (#3219, @nv-dlasalle)\r\n* Tensoradapter linking issues (#3225, #3246, @nv-dlasalle)\r\n* Diffpool loss did not consider the loss of first diffpooling layer (#3233, @yinpeiqi)\r\n* Fix CUDA 11.1 SPMM crashing with duplicate edges (#3265)\r\n* Fix `DotGatConv` attention bug when computing `edge_softmax` (#3272, @Flawless1202)\r\n* `RelGraphConv` reshape argument is incorrect (#3256, @minchenGrab)\r\n* Documentation typos and fixes (#3214, #3221, #3244, #3231, #3261, #3264, #3275, #3285, @amorehead, @blokhinnv, @kalinin-sanja)",
        "dateCreated": "2021-08-27T01:32:20Z",
        "datePublished": "2021-08-29T05:35:20Z",
        "html_url": "https://github.com/dmlc/dgl/releases/tag/0.7.1",
        "name": "0.7.1",
        "tag_name": "0.7.1",
        "tarball_url": "https://api.github.com/repos/dmlc/dgl/tarball/0.7.1",
        "url": "https://api.github.com/repos/dmlc/dgl/releases/48625593",
        "zipball_url": "https://api.github.com/repos/dmlc/dgl/zipball/0.7.1"
      },
      {
        "authorType": "User",
        "author_name": "BarclayII",
        "body": "This is a new major release with various system optimizations, new features and enhancements, new models and bugfixes.\r\n\r\n## Important: Change on PyPI Installation\r\n\r\n**DGL pip wheels are no longer shipped on PyPI.**  Use the following command to install DGL with pip:\r\n\r\n* `pip install dgl -f https://data.dgl.ai/wheels/repo.html` for CPU.\r\n* `pip install dgl-cuXX -f https://data.dgl.ai/wheels/repo.html` for CUDA.\r\n* `pip install --pre dgl -f https://data.dgl.ai/wheels-test/repo.html` for CPU nightly builds.\r\n* `pip install --pre dgl-cuXX -f https://data.dgl.ai/wheels-test/repo.html` for CUDA nightly builds.\r\n\r\nThis does not impact conda installation.\r\n\r\n## GPU-based Neighbor Sampling\r\n\r\nDGL now supports uniform neighbor sampling and MFG conversion on GPU, contributed by @nv-dlasalle from NVIDIA. Experiment for GraphSAGE on the ogbn-product graph gets a *>10x* speedup (reduced from 113s to 11s per epoch) on a g3.16x instance. The following docs have been updated accordingly:\r\n\r\n* A new user guide chapter [Using GPU for Neighborhood Sampling](https://docs.dgl.ai/guide/minibatch-gpu-sampling.html) about when and how to use this new feature.\r\n* The API doc of [NodeDataLoader](https://docs.dgl.ai/api/python/dgl.dataloading.html#dgl.dataloading.pytorch.NodeDataLoader).\r\n\r\n## New Tutorials for Multi-GPU and Distributed Training\r\n\r\nThe release brings two new tutorials about multi-GPU training for node classification and graph classification, respectively. There is also a new tutorial about distributed training across multiple machines. All of them are available at https://docs.dgl.ai/.\r\n\r\n![image](https://user-images.githubusercontent.com/2978100/126580019-87014776-4177-4965-9815-a736b226f129.png)\r\n\r\n## Improved CPU Message Passing Kernel\r\n\r\nThe update includes a new CPU implementation of the core GSpMM kernel for GNN message passing, thanks to @sanchit-misra from Intel. The new kernel performs tiling on the sparse CSR matrix and leverages Intel\u2019s LibXSMM for kernel generation, which gives an up to *4.4x speedup* over the old kernel. Please read their paper  https://arxiv.org/abs/2104.06700 for details.\r\n\r\n## More efficient NodeEmbedding for multi-GPU training and distributed training\r\n\r\nDGL now utilizes NCCL to synchronize the gradients of sparse node embeddings (`dgl.nn.NodeEmbedding`) during training (credits to @nv-dlasalle from NVIDIA). The NCCL feature is available in both `dgl.optim.SparseAdam` and `dgl.optim.SparseAdagrad`. Experiments show a *20% speedup* (reduced from 47.2s to 39.5s per epoch) on a g4dn.12xlarge (4 T4 GPU) instance for training RGCN on ogbn-mag graph. The optimization is automatically turned on when NCCL backend support is detected.\r\n\r\nThe sparse optimizers for `dgl.distributed.DistEmbedding` now use a synchronized gradient update strategy. We add a new optimizer `dgl.distributed.optim.SparseAdam`. The `dgl.distributed.SparseAdagrad` has been moved to `dgl.distributed.optim.SparseAdagrad`.\r\n\r\n## Sparse-sparse Matrix Multiplication and Addition Support\r\n\r\nWe add two new APIs `dgl.adj_product_graph` and `dgl.adj_sum_graph` that perform sparse-sparse matrix multiplications and additions as graph operations respectively. They can run with both CPU and GPU with autograd support. An example usage of these functions is [Graph Transformer Networks](https://github.com/BUPT-GAMMA/OpenHGNN/blob/main/openhgnn/models/GTN_sparse.py).\r\n\r\n## PyTorch Lightning Compatibility\r\n\r\nDGL is now compatible with PyTorch Lightning for single-GPU training or training with DistributedDataParallel.  See this example of training GraphSAGE with PyTorch Lightning.\r\n\r\n* Node classification: https://github.com/dmlc/dgl/blob/master/examples/pytorch/graphsage/train_lightning.py\r\n* Unsupervised learning: https://github.com/dmlc/dgl/blob/master/examples/pytorch/graphsage/train_lightning_unsupervised.py\r\n\r\nWe thank @justusschock for making DGL DataLoaders compatible with PyTorch Lightning (#2886).\r\n\r\n## New Models\r\n![0 7-high](https://user-images.githubusercontent.com/2978100/126580451-8c180047-84b7-4d36-9b05-ecf294997bf2.gif)\r\n\r\nA batch of *19 new model examples* are added to DGL in 0.7 bringing the total number to be 90+. Users can now use the search bar on https://www.dgl.ai/ to quickly locate the examples with tagged keywords. Below is the list of new models added.\r\n\r\n* Interaction Networks for Learning about Objects, Relations, and Physics (https://arxiv.org/abs/1612.00222.pdf) (#2794, @Ericcsr)\r\n* Multi-GPU RGAT for OGB-LSC Node Classification (#2835, @maqy1995)\r\n* Network Embedding with Completely-imbalanced Labels (https://ieeexplore.ieee.org/document/8979355) (#2813, @Fizyhsp)\r\n* Temporal Graph Networks improved (#2860, @Ericcsr)\r\n* Diffusion Convolutional Recurrent Neural Network (https://arxiv.org/abs/1707.01926) (#2858, @Ericcsr)\r\n* Gated Attention Networks for Learning on Large and Spatiotemporal Graphs (https://arxiv.org/abs/1803.07294) (#2858, @Ericcsr)\r\n* DeeperGCN (https://arxiv.org/abs/2006.07739) (#2831, @xnuohz)\r\n* Deep Graph Contrastive Representation Learning (https://arxiv.org/abs/2006.04131) (#2828, #3009, @hengruizhang98)\r\n* Graph Neural Networks Inspired by Classical Iterative Algorithms (https://arxiv.org/abs/2103.06064) (#2770, @FFTTYY)\r\n* GraphSAINT (#2792) (@lt610)\r\n* Label Propagation (#2852, @xnuohz)\r\n* Combining Label Propagation and Simple Models Out-performs Graph Neural Networks (https://arxiv.org/abs/2010.13993) (#2852, @xnuohz)\r\n* GCNII (#2874, @kyawlin)\r\n* Latent Dirichlet Allocation on GPU (#2883, @yifeim)\r\n* A Heterogeneous Information Network based Cross Domain Insurance Recommendation System for Cold Start Users (#2864, @KounianhuaDu)\r\n* Five heterogeneous graph models: HetGNN/GTN/HAN/NSHE/MAGNN (#2993, @Theheavens)\r\n* New OGB-arxiv and OGB-proteins results (#3018, @Espylapiza)\r\n* Heterogeneous Graph Attention Networks with minibatch sampling (#3005, @maqy1995)\r\n* Learning Hierarchical Graph Neural Networks for Image Clustering (https://arxiv.org/abs/2107.01319) (#3087, #3105)\r\n\r\n## New Datasets\r\n\r\n* Two fake news datasets, Gossipcop and Politifact.  (#2876, #2939, @kayzliu)\r\n* Two fraud datasets extracted from Yelp and Amazon.  See https://arxiv.org/pdf/2008.08692.pdf and https://ponderly.github.io/pub/PCGNN_WWW2021.pdf for details.  (#2876, #2908, @kayzliu)\r\n\r\n## New Functionalities\r\n\r\n* KD-Tree, Brute-force family, and NN-descent implementation of KNN (#2767, #2892, #2941) (@lygztq)\r\n* BLAS-based KNN implementation on GPU (#2868, @milesial)\r\n* A new API `dgl.sample_neighbors_biased` for biased neighbor sampling where each node has a tag, and each tag has its own (unnormalized) probability (#1665, #2987, @soodoshll).  We also provide two helper functions `sort_csr_by_tag` and `sort_csc_by_tag` to sort the internal storage of a graph based on tags to allow such kind of neighbor sampling (#1664, @soodoshll).\r\n* Distributed sparse Adam node embedding optimizer (#2733)\r\n* Heterogeneous graph\u2019s `multi_update_all` now supports user-defined cross-type reducers (#2891, @Secbone)\r\n* Add `in_degrees` and `out_degrees` supports to `dgl.DistGraph` (#2918)\r\n* A new API [`dgl.sampling.node2vec_random_walk`](https://docs.dgl.ai/generated/dgl.sampling.node2vec_random_walk.html#dgl.sampling.node2vec_random_walk) for Node2vec random walks (#2992, @Smilexuhc)\r\n* `dgl.node_subgraph`, `dgl.edge_subgraph`, `dgl.in_subgraph` and `dgl.out_subgraph` all have a `relabel_nodes` argument to allow graph compaction (i.e. removing the nodes with no edges). (#2929)\r\n* Allow direct slicing of a batched graph without constructing a new data structure.  (#2349, #2851, #2965)\r\n* Allow setting the distributed node embeddings with `NodeEmbedding.all_set_embedding()` (#3047)\r\n* Graphs can be directly created from CSR or CSC representations on either CPU or GPU (#3045). See the API doc of [`dgl.graph`](https://docs.dgl.ai/generated/dgl.graph.html#dgl.graph) for more details.\r\n* A new `dgl.reorder` API to permute a graph according to RCMK, METIS or custom strategy (#3063)\r\n* `dgl.nn.GraphConv` now has a left normalization which divides the outgoing messages by out-degrees, equivalent to random-walk normalization (#3114)\r\n* Add a new `exclude='self'` to EdgeDataLoader to exclude the edges sampled in the current minibatch alone during neighbor sampling when reverse edges are not available (#3122)\r\n\r\n## Performance Optimizations\r\n\r\n* Check if a COO is sorted to avoid sync during forward/backward and parallelize sorted COO/CSR conversion. (#2645, @nv-dlasalle)\r\n* Faster uniform sampling with replacement (#2953)\r\n* Eliminating ctor & dtor & `IsNullArray` overheads in random walks (#2990, @AjayBrahmakshatriya)\r\n* GatedGCNConv shortcut with one edge type (#2994)\r\n* Hierarchical Partitioning in distributed training with 25% speedup (#3000, @soodoshll)\r\n* Save memory usage in `node_split` and `edge_split` during partitioning (#3132, @JingchengYu94)\r\n\r\n## Other Enhancements\r\n\r\n* Graph partitioning now returns ID mapping from old nodes/edges to new ones (#2857)\r\n* Better error message when `idx_list` out of bound (#2848)\r\n* Kill training jobs on remote machines in distributed training when receiving KeyboardInterrupt (#2881)\r\n* Provide a `dgl.multiprocessing` namespace for multiprocess training with fork and OpenMP (#2905)\r\n* GAT supports multidimensional input features (#2912)\r\n* Users can now specify graph format for distributed training (#2948)\r\n* CI now runs on Kubernetes (#2957)\r\n* `to_heterogeneous(to_homogeneous(hg))` now returns the same `hg`.  (#2958)\r\n* `remove_nodes` and `remove_edges` now preserves batch information. (#3119)\r\n\r\n## Bug Fixes\r\n\r\n* Multiprocessing sampling in distributed training hangs in Python 3.8 (#2315, #2826)\r\n* Use correct NIC for distributed training (#2798, @Tonny-Gu)\r\n* Fix potential TypeError in HGT example (#2830, @zhangtianle)\r\n* Distributed training initialization fails with graphs without node/edge data (#2366, #2838)\r\n* DGL Sparse Optimizer will crash when some DGL NodeEmbedding is not involved in the forward pass (#2856, #2859)\r\n* Fix GATConv shape issues with Residual Connections (#2867, #2921, #2922, #2947, #2962, @xieweiyi, @jxgu1016)\r\n* Moving a graph to GPU will change the default CUDA device (#2895, #2897)\r\n* Remove `__len__` method to stop polluting PyCharm outputs (#2902)\r\n* Inconsistency in the typing of node types and edge types returned by `load_partition` (#2742, @chwan-rice)\r\n* `NodeDataLoader` and `EdgeDataLoader` now supports `DistributedDataParallel` with proper shuffling and batching (#2539, #2911)\r\n* Nonuniform sampling with replacement may dereference null pointer (#2942, #2943, @nv-dlasalle)\r\n* Strange behavior of `bipartite_from_networkx()` (#2808, #2917)\r\n* Make GCMC example compatible with torchtext 0.9+ (#2985, @alexpod1000)\r\n* `dgl.to_homogenous` doesn't work correctly on graphs with 0 nodes of a given type (#2870, #3011)\r\n* TU regression datasets throw errors (#2952, #3010)\r\n* RGCN generates nan in PyTorch 1.8 but not in PyTorch 1.7.x (#2760, #3013, @nv-dlasalle)\r\n* Deal with situation where `num_layers` equals 1 for GraphSAGE (#3066, @Wang-Yu-Qing)\r\n* Lengthen the timeout for distributed node embedding (#2966, #2967 @sojiadeshina)\r\n* Misc fixes in code and documentation (#2844, #2869, #2840, #2879, #2863, #2822, #2907, #2928, #2935, #2960, #2938, #2968, #2961, #2983, #2981, #3017, #3051, #3040, #3064, #3065, #3133, #3139) (@Theheavens, @ab-10, @yunshiuan, @moritzblum, @kayzliu, @universvm, @europeanplaice, etc.)\r\n\r\n## Deprecations\r\n\r\n* `preserve_nodes` argument in `dgl.edge_subgraph` is deprecated and renamed to `relabel_nodes`.\r\n\r\n",
        "dateCreated": "2021-07-21T11:07:51Z",
        "datePublished": "2021-07-22T02:01:12Z",
        "html_url": "https://github.com/dmlc/dgl/releases/tag/v0.7.0",
        "name": "v0.7.0",
        "tag_name": "v0.7.0",
        "tarball_url": "https://api.github.com/repos/dmlc/dgl/tarball/v0.7.0",
        "url": "https://api.github.com/repos/dmlc/dgl/releases/46577404",
        "zipball_url": "https://api.github.com/repos/dmlc/dgl/zipball/v0.7.0"
      },
      {
        "authorType": "User",
        "author_name": "BarclayII",
        "body": "0.6.1 is a minor release after 0.6.0 that includes some bug fixes, performance optimizations and minor feature updates.\r\n\r\n## OGB Large-scale Challenge Baselines\r\n\r\nThis release provides DGL-based baselines for the OGB Large Scale Challenge (https://ogb.stanford.edu/kddcup2021/), specifically the node classification (#2810) and graph classification (#2778) tasks.\r\n\r\nFor node classification in particular, we additionally provide the preprocessed author and institution features, as well as the homogenized graph for download.\r\n\r\n## System Support\r\n\r\n* Tensoradapter now supports PyTorch 1.8.1.\r\n\r\n## Model Updates\r\n\r\n* Boost then Convolve (#2740, credits to @nd7141)\r\n* Distributed GPU training of RGCN (#2709)\r\n* Variational Graph Auto-Encoders (#2587, #2727, credits to @JuliaSun623)\r\n* InfoGraph (#2644, credits to @hengruizhang98)\r\n* DimeNet++ (#2706, credits to @xnuohz)\r\n* GNNExplainer (#2717, credits to @KounianhuaDu)\r\n* Contrastive Multi-View Representation Learning on Graphs (#2739, credits to @hengruizhang98)\r\n* Temporal Graph Networks (#2636, credits to @Ericcsr and thanks to @WangXuhongCN for reviewing)\r\n* Tensorflow EdgeConv module (#2741, credits to @kyawlin)\r\n* CompGCN (#2768, credits to @KounianhuaDu)\r\n* JKNet (#2795, credits to @xnuohz)\r\n\r\n## Feature Updates\r\n\r\n* dgl.nn.CFConv now supports unidirectional bipartite graphs, hence heterogeneous graphs (#2674)\r\n* A QM9 Dataset variant with edge features (#2704 and #2801, credits to @hengruizhang98 and @milesial)\r\n* Display error messages instead of error codes for TCP sockets (#2763)\r\n* Add the ability of specifying the starting point for farthest point sampler (#2755, credits to @lygztq)\r\n* Remove the specification of number of workers and servers in distributed training code and move them to launch script (#2775)\r\n\r\n## Performance Optimizations\r\n\r\n* Optimize the order between message passing and feature transformation in GraphSAGE (#2747)\r\n* Remove duplicate validation in dgl.graph creation (#2789)\r\n* Replacing uniform integer sampling from std::unordered_set to linear search (#2710, credits to @pawelpiotrowicz)\r\n* Automatically setting the number of OMP threads for distributed trainers (#2812)\r\n* Prefer parallelized conversion to CSC from COO instead of transposing CSR (#2793)\r\n\r\n## Bug Fixes\r\n\r\n* Prevents symbol collision of CUB with other libraries and removes thrust dependency (#2758, credits to @nv-dlasalle)\r\n* Temporarily disabling CPU FP16 support due to incomplete code (#2783)\r\n* GraphSAGE on graphs with zero edges produces NaNs (#2786, credits to @drsealks)\r\n* Improvements of DiffPool example (#2730, credits to @lygztq)\r\n* RGCN Link Prediction example sometimes runs beyond given number of epochs (#2757, credits to @turoger)\r\n* Add pseudo code for dgl.nn.HeteroGraphConv to demonstrate how it works (#2729)\r\n* The number of negative edges should be the same as positive edges (#2726, credits to @fang2hou)\r\n* Fix dgl.nn.HeteroGraphConv that cannot be pickled (#2761)\r\n* Add a default value for dgl.dataloading.BlockSampler (#2771, credits to @hengruizhang98)\r\n* Update num_labels to num_classes in datasets (#2769, credits to @andyxukq)\r\n* Remove unused and undefined function in SEAL example (#2791, credits to @ghk829)\r\n* Fix HGT example where relation-specific value tensors are overwritten (#2796)\r\n* Cleanup the process pool correctly when the process exits in distributed training (#2781)\r\n* Fix feature type of ENZYMES in TUDataset (#2800)\r\n* Documentation fixes (#2708, #2721, #2750, #2754, #2744, #2784, #2816, #2817, #2819, credits to @Padarn, @maqy1995, @Michael1015198808, @HuangLED, @xiamr, etc.)\r\n\r\n",
        "dateCreated": "2021-04-07T08:35:50Z",
        "datePublished": "2021-04-08T03:22:48Z",
        "html_url": "https://github.com/dmlc/dgl/releases/tag/0.6.1",
        "name": "v0.6.1",
        "tag_name": "0.6.1",
        "tarball_url": "https://api.github.com/repos/dmlc/dgl/tarball/0.6.1",
        "url": "https://api.github.com/repos/dmlc/dgl/releases/41096075",
        "zipball_url": "https://api.github.com/repos/dmlc/dgl/zipball/0.6.1"
      },
      {
        "authorType": "User",
        "author_name": "BarclayII",
        "body": "This is a binary rebuild of 0.6.0 release that adds support on PyTorch 1.8 + CUDA 11.1.  Please install with either of the following:\r\n\r\n```\r\nconda install dgl-cuda11.1 -c dglteam\r\npip install dgl-cu111\r\n```\r\n\r\nNo feature changes are incorporated.\r\n\r\nCurrently there is an issue in CUB when building with CUDA 11.1 from source where DGL will crash with various CUDA errors or freeze when using with PyTorch 1.8.  You will need to define `CUB_CPP_DIALECT=2003` in the C++ and NVCC flags as a work around.  Consequently, CUDA 11.1 binaries are built with the macro `CUB_CPP_DIALECT=2003` while CUDA 11.0- are built without the macro.",
        "dateCreated": "2021-03-09T05:02:33Z",
        "datePublished": "2021-03-09T18:23:16Z",
        "html_url": "https://github.com/dmlc/dgl/releases/tag/0.6.0post1",
        "name": "v0.6.0post1",
        "tag_name": "0.6.0post1",
        "tarball_url": "https://api.github.com/repos/dmlc/dgl/tarball/0.6.0post1",
        "url": "https://api.github.com/repos/dmlc/dgl/releases/39535513",
        "zipball_url": "https://api.github.com/repos/dmlc/dgl/zipball/0.6.0post1"
      },
      {
        "authorType": "User",
        "author_name": "BarclayII",
        "body": "This new release includes several improvements on DGL\u2019s documentation, distributed training, and fixes and patches to the user experience and system efficiency.\r\n\r\n## Documentation\r\n\r\nThe tutorials have been re-worked in this release to make them more consistent and updated to the latest code base. All tutorials are available for download and can be run locally in Jupyter Notebook.\r\n\r\n* For absolute beginners, start with the brand new [Blitz Introduction to DGL in 120 minutes](https://docs.dgl.ai/en/latest/tutorials/blitz/index.html).\r\n* For those who are interested in mini-batch training of GNNs, read the [Stochastic Training of GNNs tutorials](https://docs.dgl.ai/en/latest/tutorials/large/index.html) which starts from the basic concepts to code examples.\r\n\r\nThanks to the community efforts, DGL\u2019s user guide is now available in Chinese (https://docs.dgl.ai/en/latest/guide_cn/index.html). Credits to @huaiwen @mlsoar @brookhuang16211 [Zhiyu Chen](https://www.zhiyuchen.com) @hhhiddleston @AspirinCode @rewonderful @sleeplessai @kevin-meng @CrawlScript @rr-Yiran [Qingbiao Li](https://qingbiaoli.github.io/)\r\n\r\n## Model Examples\r\n\r\nWe [index](https://github.com/dmlc/dgl/tree/master/examples) all the DGL examples by their notable tags (e.g. problem domains, tasks, graph characteristics, etc.) and by their publication time. As DGL codebase evolves quickly and may break some examples, we chose to maintain them by branches, i.e., examples on the master branch work with latest nightly build; stable examples are snapshot to the release branch like 0.6.x.\r\n\r\nThe release also brings 13 new examples, adding up to 72 models in total:\r\n\r\n* MixHop: Higher-Order Graph Convolutional Architectures via Sparsified Neighborhood Mixing: https://github.com/dmlc/dgl/tree/master/examples/pytorch/mixhop (Credits to @xnouhz)\r\n* Self-Attention Graph Pooling: https://github.com/dmlc/dgl/tree/master/examples/pytorch/sagpool (Credits to @lygztq )\r\n* GNN-FiLM: Graph Neural Networks with Feature-wise Linear Modulation: https://github.com/dmlc/dgl/tree/master/examples/pytorch/GNN-FiLM (Credits to @KounianhuaDu )\r\n* TensorFlow implementation of Simplifying Graph Convolutional Networks: https://github.com/dmlc/dgl/tree/master/examples/tensorflow/sgc (Credits to @joshcarty)\r\n* Graph Representation Learning via Hard and Channel-Wise Attention Networks: https://github.com/dmlc/dgl/tree/master/examples/pytorch/hardgat (Credits to @Ericcsr )\r\n* Graph Random Neural Network for Semi-Supervised Learning on Graphs: https://github.com/dmlc/dgl/tree/master/examples/pytorch/grand (Credits to @hengruizhang98 )\r\n* Hierarchical Graph Pooling with Structure Learning: https://github.com/dmlc/dgl/tree/master/examples/pytorch/hgp_sl (Credits to @lygztq )\r\n* Towards Deeper Graph Neural Networks: https://github.com/dmlc/dgl/tree/master/examples/pytorch/dagnn (Credits to @lt610)\r\n* PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation/PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space (part segmentation): https://github.com/dmlc/dgl/tree/master/examples/pytorch/pointcloud/pointnet (Credits to @wcyjames )\r\n* Graph Cross Networks with Vertex Infomax Pooling: https://github.com/dmlc/dgl/tree/master/examples/pytorch/gxn (Credits to @lygztq)\r\n* Neural Graph Collaborative Filtering: https://github.com/dmlc/dgl/tree/master/examples/pytorch/NGCF (Credits to @KounianhuaDu )\r\n* Graph Neural Networks with Convolutional ARMA Filters: https://github.com/dmlc/dgl/tree/master/examples/pytorch/arma (Credits to @xnuohz)\r\n* Link Prediction Based on Graph Neural Networks (SEAL): https://github.com/dmlc/dgl/tree/master/examples/pytorch/seal (Credits to @Smilexuhc )\r\n\r\n## New APIs & Features\r\n\r\n* _New API_: `set_batch_num_nodes` and `set_batch_num_edges` for setting batch information manually. They are useful when users want to transform a batched graph into another or construct a new batched graph by themselves (#2430)\r\n* _New API_: `GraphDataLoader`, a data loader wrapper for graph classification tasks. (#2496)\r\n* _New API_: QM9 dataset. (#2521) (Credits to @xnuohz )\r\n* _New API:_ DGL now allows constructing a `DGLBlock` graph from raw data (via `dgl.create_block`) or converting a `DGLBlock` to normal `DGLGraph` (via `dgl.block_to_graph`). They are useful when users wish to transform the `DGLBlock` graph produced by data loaders such as reversing the graph for message diffusion instead of message aggregation. (#2555)\r\n* _New API:_ A new namespace `dgl.nn.functional` for NN related utilities that are functional, much resembling `torch.nn.functional`. `edge_softmax` is moved there. The old `dgl.ops.edge_softmax` is deprecated. (#2442)\r\n* _New Feature:_ Support mixed precision training. DGL now supports training with half precision and thus is compatible with PyTorch\u2019s automatic mixed precision package. See [the user guide chapter](https://docs.dgl.ai/en/latest/guide/mixed_precision.html) for how to use it.\r\n* (Experimental) _New APIs for sparse embedding:_ (#2451)\r\n    * `dgl.nn.NodeEmbedding`: A class for storing node embeddings that is optimized for training on large-scale graphs.\r\n    * `dgl.optim.SparseAdagrad` and `dgl.optim.SparseAdam`: Optimizers to work with `dgl.nn.NodeEmbedding`.\r\n* (Experimental) Distributed heterogeneous support:\r\n    * Enable heterogeneous graph interfaces in `DistGraph` such as `g.nodes['type'].data['feat']`, as well as sampling on distributed heterogeneous graph via `dgl.sample_neighbors` . See [this user guide chapter](https://docs.dgl.ai/en/latest/guide/distributed-hetero.html) for more details.\r\n    * Support distributed graph partitioning on a cluster of machines. See [this user guide chapter](https://docs.dgl.ai/en/latest/guide/distributed-preprocessing.html) for more details.\r\n\r\n## Improvements\r\n\r\n* _API improvement:_ `GraphConv`, `SAGEConv`, `GINConv` now support weighted graph. Users can pass in edge weights via an optional `edge_weight` argument. Also add a new NN module `EdgeWeightNorm` which normalizes edge weights according to Kipf\u2019s GCN paper. (#2557)\r\n* _API improvement_: Add an optional argument device to all dataloaders (e.g., NodeDataLoader , EdgeDataLoader) to indicate the target device of the produced graph minibatches. (#2450)\r\n* _API improvement_: Allow GATConv and DotGATConv to return attention values (#2397).\r\n* _API improvement:_ Allow multiple heads in DotGATConv. (#2549) (Credits to @Ericcsr)\r\n* _API improvement:_ Add an optional flag reverse_edge to CitationGraphDataset to disable adding reverse edges to the graph. (#2588) (Credits to @JuliaSun623 )\r\n* A new implementation for nn.RelGraphConv when low_mem=True. A benchmark on V100 GPU shows it gives a 4.8x boost in training speed on AIFB dataset. (#2468)\r\n* Allow DGL to use PyTorch\u2019s native memory allocator whenever possible. This saves a large number of malloc/free by caching the allocated buffers inside PyTorch (#2328, #2454).\r\n* Speedup DGL by removing unnecessary sorting on CSR structure (#2391) (Credits to @nv-dlasalle )\r\n* Add an option to mini-batch training examples (e.g., GraphSAGE, ClusterGAT, GAT, RGCN) that loads all node features to GPU prior to model training. The option speeds up model training significantly but consumes more GPU memory. (#2453)\r\n* AVX support for faster CPU kernels (#2309) (Credits to @pawelpiotrowicz ).  Enabled in binary releases.\r\n* Add a USE_AVX flag in CMake options to allow disabling AVX optimization on hardware that do not support it. (#2428, #2438)  Enabled in binary releases.\r\n* Change dgl.remove_nodes and dgl.remove_edges to not override the NID and EID feature field by default. (#2465)\r\n* Allow dgl.batch to batch a list of empty graphs. (#2527) (Credits to @noncomputable )\r\n* Speedup HGT example by using DGL built-in functions (2x faster) (#2394). (Credits to @Maybewuss )\r\n* Speedup cuSPARSE SpMM by using another algorithm (4x faster) (#2550). (Credits to @nv-dlasalle )\r\n* Speedup mini-batch generation by removing unnecessary format conversion (#2551). (Credits to @nv-dlasalle )\r\n* Enable in_degrees and out_degrees on DGLGraph with only COO format. (#2565) \r\n* Enable dgl.to_block on CUDA. (#2339) (Credits to @nv-dlasalle )\r\n* Add a compilation option to compile a tailored TVM runtime into DGL. (#2367)  Disabled in binary releases. (Credits to @kira-lin )\r\n\r\n## Bugfixes\r\n\r\n* Fix an issue regarding to OpenMP which causes performance regression when launching multiple training tasks on multi-core machines (#2412).\r\n* Fix a bug where message passing is ignored for empty graph (#2387).\r\n* Fix a bug where citation dataset never loads from cached data. Improve the error message. (#2421)\r\n* Fix a bug in distributed training to allow reusing ports after sockets are closed (#2418)\r\n* Fix a bug in PyTorch backend which produces spontaneous warnings. (#2434)\r\n* Fix a bug that shared memory is not properly deleted when the process is killed by signals. (#2419)\r\n* Fix a bug in knowledge graph dataset which causes undefined variable error. (#2475)\r\n* Fix multiple bugs in metapath2vec (#2491, #2607) (Credits to @pein-hub, @zezhishao )\r\n* Fix a bug in send_and_recv and pull that causes node data writing to wrong places. (#2497)\r\n* Fix a bug in GATConv which changes the model parameters. (#2533)\r\n* Fix a bug that erases empty node features after graph mutation. (#2529) (Credits to @noncomputable )\r\n* Fix an undefined variable bug in LegacyTUDataset. (#2543) (Credits to @lygztq)\r\n* Fix the version check for PyTorch. (#2544)\r\n* Fix a bug in Coauthor datasets that causes duplicate edges (#2569)\r\n* Fix a bug in DGLDataset which prevents it from pickling on Windows (#2572)\r\n* Fix a bug in HeteroGraphConv where node features are not properly handled (#2578)\r\n* Fix a bug in message passing kernels where input data can have different data types. (#2598)\r\n* Fix a boundary bug in segment_sum. (#2610)\r\n* Fix a bug in GINDataset and TUDataset where the node features are in float64 instead of float32. (#2592)\r\n* Fix a bug in dgl.nn.RelGraphConv when the out_feat is one. (#2650)\r\n* Lots of documentation fixes. (Thanks to @Ericcsr, @coco11563 , @cruyffturn , @timgates42 , @standbyme, @Lyken17 , and many others)\r\n\r\n## Breaking changes\r\n\r\n* `HeteroGraphConv` now requires you to give features for all node types even if the computation in the child modules does not use it.  For example, the following code no longer works:\r\n  ```python\r\n  g = dgl.heterograph({('A', 'AB', 'B'):([0,1],[1,2])})\r\n  conv = dglnn.HeteroGraphConv({'AB': GraphConv(5, 10)})\r\n  conv(g, {'A': torch.randn(2, 5)})\r\n  ```\r\n  You will need to give a dummy feature to make it work if you are sure that node type 'B' is not used:\r\n  ```python\r\n  conv(g, {'A': torch.randn(2, 5), 'B': torch.randn(3, 10)})\r\n  ```",
        "dateCreated": "2021-02-25T18:31:32Z",
        "datePublished": "2021-02-26T00:05:08Z",
        "html_url": "https://github.com/dmlc/dgl/releases/tag/v0.6.0",
        "name": "v0.6.0",
        "tag_name": "v0.6.0",
        "tarball_url": "https://api.github.com/repos/dmlc/dgl/tarball/v0.6.0",
        "url": "https://api.github.com/repos/dmlc/dgl/releases/38773140",
        "zipball_url": "https://api.github.com/repos/dmlc/dgl/zipball/v0.6.0"
      },
      {
        "authorType": "User",
        "author_name": "BarclayII",
        "body": "This is a patch release mainly for supporting CUDA 11.0.  Now DGL supports CUDA 11.0 and PyTorch 1.7 on Linux/Windows/Mac.\r\n\r\nOther fixes include:\r\n* Performance fix of graph batching: #2363 \r\n* Speedup on readout: #2361 \r\n* Speedup in CPU SpMM with sum reducer: #2309 (thanks @pawelpiotrowicz )\r\n* Performance optimization that removes redundant copies between CPU and GPU: #2266 #2267 (thanks @nv-dlasalle )\r\n* Fix segment_reduce() ignoring tailing 0 segments (#2228) (thanks @mjwen)\r\n* Fix crash due to unfound attribute (#2262) (thanks @Samiisd )\r\n* Performance optimization in COO-CSR conversion (#2356 ) (thanks @IzabelaMazur )\r\n* Parallelization in heterogeneous graph format conversion (#2148) (thanks @mozga-intel )\r\n* Fix a bug to enable distributed training of RGCN with CPU (#2345) (thanks @mszarma )\r\n* Numerous documentation fixes (kudos to @cafeal , @maqy1995 , @sw32-seo, @157492196 , @chwan-rice , @ZenoTan  )\r\n\r\nNew examples:\r\n* Sparse embedding for GATNE-T for large graphs (#2234 ) (thanks @sangyx )\r\n* LINE (#2195) (thanks @ShawXh )\r\n* SIGN for OGB (#2316 ) (thanks @lingfanyu )\r\n\r\nThe Chinese user guide has been released for chapter 1 to 4 (#2351).  Thanks @zhjwy9343 for coordination and kudos to all the offline contributors!",
        "dateCreated": "2020-11-27T06:43:22Z",
        "datePublished": "2020-11-30T07:39:12Z",
        "html_url": "https://github.com/dmlc/dgl/releases/tag/0.5.3",
        "name": "v0.5.3",
        "tag_name": "0.5.3",
        "tarball_url": "https://api.github.com/repos/dmlc/dgl/tarball/0.5.3",
        "url": "https://api.github.com/repos/dmlc/dgl/releases/34562878",
        "zipball_url": "https://api.github.com/repos/dmlc/dgl/zipball/0.5.3"
      },
      {
        "authorType": "User",
        "author_name": "BarclayII",
        "body": "This is a patch release including the following bug fixes and new models.\r\n\r\n**Documentation fixes**\r\n* #2172 CoraFull dataset remove redundant reference\r\n* #2167 #2177 Fix multiple docstring typos and inconsistencies in `dgl.dataloading` and minibatch training user guide.  (Thanks @ustchhy for reviewing)\r\n* #2131 Update Doc for UDFs\r\n\r\n**Bug fixes**\r\n* #2128 cannot request out_edges() for empty node sets on cuda\r\n* #2098 Context Issue for bfs_edges_generator on GPU Graphs\r\n* #2135 Pickling a subgraph stores the feature of the original graph\r\n* #2137 0.5.x taking too much shared memory during multiprocess training\r\n* #2145 dgl.batch() in 0.5.x is slower than 0.4.x\r\n* #2157 edge_softmax function not working on subgraphs\r\n* #2161 #2165 #2173 TUDataset (Thanks @HenryKenlay)\r\n* #2175 Messages not ordered by edge IDs in degree bucketing\r\n* #2166 Error when call apply_edges for dec_graph\r\n* #2169 Multiprocessing neighbor sampling sometimes have the memory corrupted\r\n* #2106 Bad file descriptor error when saving dgl graph to HDFS\r\n* #2188 Fix dtype mismatch in EdgeDataLoader on Windows\r\n\r\n**Bug fixes in examples**\r\n* #2143 Fix unsupervised graphsage\r\n* #2182 Use DistDataLoader instead of Pytorch\u2019s DataLoader in Distributed GraphSAGE (Thanks @liucw2012 )\r\n* #2187 Fix partition for 0.5.1\r\n\r\n**New examples**\r\n* #2153 GCN on OGB-Arxiv (Thanks @Espylapiza )",
        "dateCreated": "2020-09-13T10:25:29Z",
        "datePublished": "2020-09-14T05:21:24Z",
        "html_url": "https://github.com/dmlc/dgl/releases/tag/0.5.2",
        "name": "v0.5.2",
        "tag_name": "0.5.2",
        "tarball_url": "https://api.github.com/repos/dmlc/dgl/tarball/0.5.2",
        "url": "https://api.github.com/repos/dmlc/dgl/releases/31232659",
        "zipball_url": "https://api.github.com/repos/dmlc/dgl/zipball/0.5.2"
      },
      {
        "authorType": "User",
        "author_name": "BarclayII",
        "body": "This is a patch release including the following bug fixes and minor features.\r\n\r\n**Documentation fixes**\r\n\r\n* #2081 Reorganize user guide and split chapters into multiple pages\r\n* #2085 #2090 Fix links\r\n* #2091 #2114 User guide on distributed training\r\n* #2096 #2097 Other documentation fixes\r\n* #2086\r\n* #2123 Temporarily remove SSE MXNet tutorial\r\n\r\n**Bug fixes**\r\n\r\n* #2100 add_edges() crashes if the input tensor is empty\r\n* #2084 Fix distributed GraphSage running with GPU\r\n* #2107 Building with HDFS previously fails\r\n* #2087 Cannot load the PTC dataset via `dgl.data.GINDataset`\r\n* #2076 Empty cuda graph raise error in create_formats_\r\n* #2121 Disable hypersparse memory optimization due to incomplete COO graph support on GPU\r\n* #2115 Fallback to CPU for graph traversal functions on GPU graphs\r\n* #2108 Force `num_workers` and `num_samplers` to be the same for distributed training\r\n* #2118 (#2127 )\r\n\r\n**Bug fixes in examples**\r\n\r\n* #2119 Bug in using Layer Normalization in RGCN\r\n\r\n**New features**\r\n\r\n* #2102 5 utility functions that handle raw data features\r\n* #1979 #2117 CUDA 11 support - We will not release binary builds of CUDA 11 for now\r\n\r\n**Release changes**\r\n\r\n* Linux now requires GCC 5 to build DGL from source.\r\n* DGL now supports Mac 10.9+.",
        "dateCreated": "2020-08-30T01:52:23Z",
        "datePublished": "2020-08-30T15:40:38Z",
        "html_url": "https://github.com/dmlc/dgl/releases/tag/0.5.1",
        "name": "v0.5.1",
        "tag_name": "0.5.1",
        "tarball_url": "https://api.github.com/repos/dmlc/dgl/tarball/0.5.1",
        "url": "https://api.github.com/repos/dmlc/dgl/releases/30396305",
        "zipball_url": "https://api.github.com/repos/dmlc/dgl/zipball/0.5.1"
      },
      {
        "authorType": "User",
        "author_name": "BarclayII",
        "body": "This is a major release including new documentation, distributed GNN training support, more new features and models, as well as bug fixes and more system performance improvement. Note that this is a huge update and may break some of the existing codes; see the [Migration Guide](https://github.com/dmlc/dgl/blob/master/docs/migrate-guide-0.5.md) for details.\r\n\r\n## New Documentations\r\n\r\n* A new [user guide](https://docs.dgl.ai/en/0.5.x/guide/index.html) that explains the core concepts of DGL including graphs, features, message passing, DGL datasets, full-graph training, stochastic training, and distributed training.\r\n* Re-worked the [API reference manual](https://docs.dgl.ai/en/0.5.x/api/python/index.html).\r\n\r\n## Distributed Training\r\n\r\nDGL now supports training GNNs on large graphs distributed across multiple machines. The new components are under the dgl.distributed package. The [user guide chapter](https://docs.dgl.ai/en/0.5.x/guide/distributed.html) and the [API document page](https://docs.dgl.ai/en/0.5.x/api/python/dgl.distributed.html) describe the usage. New end-to-end examples for distributed training:\r\n\r\n* [An example](https://github.com/dmlc/dgl/tree/master/examples/pytorch/graphsage/experimental) for training GraphSAGE using neighbor sampling on ogbn-product and ogbn-paper100M (100M nodes, 1B edges). Included scripts for both supervised and unsupervised training, and offline inference. The training takes 12 seconds per epoch for ogbn-paper100M on a cluster of 4 m5n.24xlarge instances, and achieves 64% accuracy.\r\n* [An example](https://github.com/dmlc/dgl/tree/master/examples/pytorch/rgcn/experimental) for training R-GCN using neighbor sampling on ogbn-mag. Included scripts for both inductive and transductive modeling. The training takes 841 seconds per epoch on a cluster of 4 m5n.24xlarge CPU machines , and achieves 42.32% accuracy.\r\n\r\n\r\n\r\n## New Features\r\n\r\n**Core data structure**\r\n\r\n* Merged `DGLGraph` and `DGLHeteroGraph`.  `DGLGraph` now supports nodes and edges of different types.\r\n* **All** the APIs on the old `DGLGraph` are now compatible with heterogeneous graphs. They include\r\n    * Mutation operations such as adding or removing nodes and edges.\r\n    * Graph transformation routines such as `dgl.reverse()` `dgl.to_bidirected()`\r\n    * Subgraph extraction routines.\r\n    * `dgl.save_graphs()` and `dgl.load_graphs()`\r\n    * Batching and reading out operators.\r\n* DGL now supports creating graph stored in int32 to further conserve memory. Three new APIs: `DGLGraph.idtype`, `DGLGraph.int`, `DGLGraph.long` for getting or changing the integer type for storing graph.\r\n* DGL now allows performing graph structure relation operations on GPU such as `DGLGraph.in_degrees()`, `DGLGraph.edge_ids()` , `DGLGraph.subgraph` etc. A new API `DGLGraph.to` to copy a graph to different devices. This leads to a breaking change on requiring the graph and feature tensors to always be on the same device. See the [Migration Guide](https://github.com/dmlc/dgl/blob/master/docs/migrate-guide-0.5.md) for more explanations.\r\n* Many graph transformations and subgraph extraction operations in DGL now automatically copy the corresponding node and edge features from the original graph.  The copying happens on-demand, meaning that the copy would not take place until you actually accesses the feature.\r\n    * Before 0.5\r\n    ```python\r\n    >>> g = dgl.graph(([0, 1, 2], [3, 4, 5]))\r\n    >>> g.ndata['x'] = torch.arange(12).view(6, 2)\r\n    >>> sg = g.subgraph([0, 1])    # sg does not have feature 'x'\r\n    >>> 'x' in sg.ndata\r\n    False\r\n    ```\r\n    * From 0.5\r\n    ```python\r\n    >>> g = dgl.graph(([0, 1, 2], [3, 4, 5]))\r\n    >>> g.ndata['x'] = torch.arange(12).view(6, 2)\r\n    >>> sg = g.subgraph([0, 1])    # sg inherits feature 'x' from 'g'\r\n    >>> 'x' in sg.ndata\r\n    True\r\n    >>> print(sg.ndata['x'])       # the actual copy happens at here\r\n    tensor([[0, 1],\r\n            [1, 2]])\r\n    ```\r\n* DGL\u2019s message passing operations (e.g., `DGLGraph.update_all`, `DGLGraph.apply_edges` etc.) now support higher-order gradients when the backend is PyTorch.\r\n* `DGLGraph.subgraph()` and `DGLGraph.edge_subgraph()` now accept boolean tensors or dictionary of boolean tensors as input.\r\n* Min and max aggregators now return 0 instead of a large number for zero-degree nodes to improve training experience.\r\n* DGL kernels and readout functions are now deterministic.\r\n\r\n\r\n**GNN training utilities**\r\n\r\n* New classes: `dgl.dataloading.NodeDataLoader` and `dgl.dataloading.EdgeDataLoader` for stochastic training of node classification, edge classification, and link prediction with neighborhood sampling on a large graph.  Both classes are similar to PyTorch `DataLoader` classes to allow easy customization of the neighborhood sampling strategy.\r\n* DGL neural networks now support feeding in a single tensor together with a block as input.\r\n    * Previously, to perform message passing on a block, you need to always feed in a pair of features as input, representing the features of input and output nodes like the following:\r\n    ```python\r\n    # Assuming that h is a 2D tensor representing the input node features\r\n      def forward(self, blocks, h):\r\n          for layer, block in zip(self.layers, blocks):\r\n              h_dst = h[:block.number_of_dst_nodes()]\r\n              h = layer(block, (h, h_dst))\r\n          return h\r\n    ```\r\n    * Now, you only need to feed in a single tensor if the input graph is a block.\r\n    ```python\r\n    # Assuming that h is a 2D tensor representing the input node features\r\n      def forward(self, blocks, h):\r\n          for layer, block in zip(self.layers, blocks):\r\n              h = layer(block, h)\r\n          return h\r\n    ```\r\n* Added a check for zero-degree nodes to the following modules to prevent potential accuracy degradation. To prevent the error, either fix it by adding self-loops (using `dgl.add_self_loop`) or passing `allow_zero_in_degree=True` to suppress it.\r\n    * GraphConv, GATConv, EdgeConv, SGConv, GMMConv, AGNNConv, DotGatConv\r\n\r\n\r\n## New APIs\r\n\r\n* `dgl.add_reverse_edges()` adds reverse edges for a heterogeneous graph.  It works on all edge types whose source node type is the same as its destination node type.\r\n* `DGLGraph.shared_memory` for copying the graph to shared memory.\r\n\r\n## New Models\r\n\r\n* Hao Xiong @ShawXh has made several DeepWalk submissions to OGB link prediction leaderboard: https://ogb.stanford.edu/docs/leader_linkprop/.  The models are now included in the example directory.\r\n* Zhengdao Chen @zhengdao-chen  has proposed a node classification model which utilizes edge weights in this tech report https://cims.nyu.edu/~chenzh/files/GCN_with_edge_weights.pdf.  The model is included in the example directory and achieved 0.8436 \u00b1 0.0065 ROC-AUC on OGB-proteins: https://ogb.stanford.edu/docs/leader_nodeprop/#ogbn-proteins\r\n* Saurav Manchanda @gurdaspuriya  implemented algorithms for computing graph edit distances for graph matching.  Both exact and approximate algorithms are implemented.  https://github.com/dmlc/dgl/tree/master/examples/pytorch/graph_matching\r\n* We added implementation of Cluster-GCN with GAT and GraphSAGE as the underlying neural network module https://github.com/dmlc/dgl/tree/0.5.x/examples/pytorch/ogb.  They achieved 0.7932 and 0.7830 test accuracy on OGB-products respectively.  The Cluster-GAT implementation is submitted to OGB leaderboard.\r\n* We updated both of our RGCN examples (https://github.com/dmlc/dgl/tree/0.5.x/examples/pytorch/rgcn and https://github.com/dmlc/dgl/tree/0.5.x/examples/pytorch/rgcn-hetero (https://github.com/dmlc/dgl/tree/0.5.x/examples/pytorch/rgcnhttps://github.com/dmlc/dgl/tree/0.5.x/examples/pytorch/rgcn-hetero)) to support minibatch training.  We tested our RGCN implementation on OGB-MAG which achieved 0.4622 accuracy.\r\n* We updated our GraphSAGE example to include the inductive setting, where the training and test graphs are different.\r\n\r\n## Requirement Update\r\n\r\n* For PyTorch users, DGL now requires `torch >= 1.5.0`\r\n* For MXNet users, DGL now requires `mxnet >= 1.6`\r\n* For TensorFlow users, DGL now requires `tensorflow >= 2.3`\r\n* Deprecate support for Python 3.5. Add support for Python 3.8. DGL now supports Python 3.6-3.8.\r\n* Add support for CUDA 10.2\r\n* For users that build DGL from source\r\n    - On Linux: libstdc++.so.6.0.19 or later, or equivalently Ubuntu 14.04 or later, CentOS 7 or later.\r\n    - On Windows: Windows 10 or Windows server 2016 or later\r\n    - On Mac: 10.9 or later\r\n\r\n## Compatibility issues\r\n\r\nPickle files created in versions 0.4.3post2 or earlier cannot be loaded by 0.5.0.  For now, you need to load the graph structure with 0.4.3post2, and save the graph structure as tensors, and reconstruct them with DGL 0.5.\r\n",
        "dateCreated": "2020-08-21T04:39:14Z",
        "datePublished": "2020-08-21T11:52:04Z",
        "html_url": "https://github.com/dmlc/dgl/releases/tag/0.5.0",
        "name": "0.5.0",
        "tag_name": "0.5.0",
        "tarball_url": "https://api.github.com/repos/dmlc/dgl/tarball/0.5.0",
        "url": "https://api.github.com/repos/dmlc/dgl/releases/29977348",
        "zipball_url": "https://api.github.com/repos/dmlc/dgl/zipball/0.5.0"
      },
      {
        "authorType": "User",
        "author_name": "BarclayII",
        "body": "This is a post-release for fixing a few bugs including:\r\n\r\n1. Vertex subgraph on hypersparse graphs crashes #1409\r\n2. Neighbor sampling with replacement returns node ID -1 for 0 degree nodes #1421\r\n3. Rolling back interactive backend selection #1424",
        "dateCreated": "2020-04-07T06:23:36Z",
        "datePublished": "2020-04-07T06:32:36Z",
        "html_url": "https://github.com/dmlc/dgl/releases/tag/0.4.3.post2",
        "name": "v0.4.3.post2",
        "tag_name": "0.4.3.post2",
        "tarball_url": "https://api.github.com/repos/dmlc/dgl/tarball/0.4.3.post2",
        "url": "https://api.github.com/repos/dmlc/dgl/releases/25264777",
        "zipball_url": "https://api.github.com/repos/dmlc/dgl/zipball/0.4.3.post2"
      },
      {
        "authorType": "User",
        "author_name": "jermainewang",
        "body": "We are thrilled to announce DGL v0.4.3, which provides many new features that enhance usability and efficiency.\r\n\r\nMajor Features and Improvements\r\n---\r\n\r\n* TensorFlow backend is now an official feature. Add an environment variable `USE_OFFICIAL_TFDLPACK` for switching to the official TensorFlow DLPack support from tensorflow/tensorflow#36862.\r\n* New graph sampling APIs compatible with `DGLHeteroGraph`:\r\n    * Redesigned `dgl.random_walk` with the support of metapaths.\r\n    * A new API `dgl.random_walk_with_restart` for random walk with restart probability.\r\n    * A new API `dgl.sample_neighbors` for sampling among neighbors with or without replacement.\r\n    * A new API `dgl.sample_neighbors_topk` for picking K neighbors with the largest weight.\r\n    * A new API `dgl.in_subgraph` for extracting all the neighbors connected by the in-edges.\r\n    * A new API `dgl.out_subgraph` for extracting all the neighbors connected by the out-edges.\r\n* Accompany utilities for graph sampling:\r\n    * A new API `dgl.create_from_paths` for creating a graph from sampled random walk traces.\r\n    * A new API `dgl.compact_graphs` for converting a sampled subgraph to a smaller graph with no isolated nodes.\r\n    * A new API `dgl.to_block` for converting a sampled subgraph to a bipartite graph suitable for computation.\r\n    * Reworked `dgl.to_simple_graph` to support heterogeneous graph.\r\n    * Reworked `dgl.remove_edges` to support heterogeneous graph.\r\n* When constructing a `DGLHeteroGraph` to be unidirectional bipartite \u2014 there are two node types and one edge type, where all edges are from one node type to another, the following APIs are enabled:\r\n    * A new API `DGLHeteroGraph.is_unibipartite`\r\n    * New APIs `DGLHeteroGraph.num_src_nodes` and `DGLHeteroGraph.num_dst_nodes`\r\n    * New APIs `DGLHeteroGraph.srcnodes` and `DGLHeteroGraph.dstnodes` for getting a view of source and destination nodes, respectively.\r\n    * New APIs `DGLHeteroGraph.srcdata` and `DGLHeteroGraph.dstdata` for getting the data of source and destination nodes, respectively.\r\n* NN module changes:\r\n    * Users can now directly use `dgl.nn.SomeModule` instead of `dgl.nn.<backend>.SomeModule`.\r\n    * Extend `dgl.nn.GraphConv` to support asymmetric normalizer. It now also accepts an external weight matrix instead of creating its own.\r\n    * Extend all the NN modules to support bipartite graph inputs which enable them for sampling-based GNN training. The input node feature argument now can be a pair of tensors.\r\n    * A new wrapper module `dgl.nn.HeteroGraphConv` for leveraging DGL NN modules on heterogeneous graphs.\r\n* Model examples using the new sampling APIs\r\n    * Train the GraphSAGE model by neighbor sampling and scale it to multiple GPUs ([link](https://github.com/dmlc/dgl/tree/master/examples/pytorch/graphsage)).\r\n    * Train the Relational GCN model on heterogeneous graphs by sampling for both node classification and link prediction ([link](https://github.com/dmlc/dgl/tree/master/examples/pytorch/rgcn-hetero)).\r\n    * Train the PinSAGE model by random walk sampling for item recommendation ([link](https://github.com/dmlc/dgl/pull/1334)).\r\n    * Train the GCMC model by sampling for MovieLens rating prediction ([link](https://github.com/dmlc/dgl/pull/1296)).\r\n    * Implement the variance reduction technique for neighbor sampling ([link](https://github.com/dmlc/dgl/pull/1355)) proposed by [Chen et al.](https://arxiv.org/abs/1710.10568)\r\n* DGL-KE\r\n    * Move to a standalone repository under awslab: https://github.com/awslabs/dgl-ke\r\n    * See the [release note](https://github.com/awslabs/dgl-ke/releases/tag/v0.1.0) for more details.\r\n* DGL-LifeSci\r\n    * Spun off into a standalone package.\r\n    * See the [project page](https://github.com/dmlc/dgl/tree/master/apps/life_sci) for more details.\r\n* A new example for scene graph extraction: https://github.com/dmlc/dgl/tree/master/examples/mxnet/scenegraph\r\n* A new API `dgl.metis_partition` for partitioning a DGLGraph by the Metis algorithm.\r\n* New APIs `dgl.as_immutable_graph` and `dgl.as_heterograph` for casting between `DGLGraph` and `DGLHeteroGraph` efficiently.\r\n* A new API `dgl.rand_graph` for constructing a random graph with specified number of nodes and edges.\r\n* A new API `dgl.random.choice` for more efficient non-uniform random choice.\r\n* Replaced `DGLHeteroGraph.__setitem__` and `DGLHeteroGraph.__getitem__` with a more efficient implementation.\r\n* `dgl.data.save_graphs` and `dgl.data.load_graphs` now support heterogeneous graphs.\r\n* UDFs now have the access to node types and edge types.\r\n\r\nAPI Breaking Changes\r\n---\r\n\r\n* The type of the norm argument in `dgl.nn.GraphConv` is changed from bool to string, with `\"none\"` indicating no normalization, `\"both\"` indicating the original symmetric normalizer proposed by GCN and `\"right\"` indicating normalizing by degrees.\r\n* `DGLSubGraph` and `BatchedDGLGraph` classes are removed and merged to `DGLGraph`. All their methods are ported to `DGLGraph` too, so typical usages will not be affected by this change.\r\n* The multigraph flag in `dgl.DGLGraph` is deprecated and will be removed in the future.\r\n* Rename the card argument in `dgl.graph` and `dgl.bipartite` to `num_nodes`.\r\n\r\nBug Fixes and Others\r\n---\r\n\r\n* Fix a bug in `remove_edges` when the graph has no edge.\r\n* Fix a bug in creating `DGLGraph` from scipy coo matrix that has duplicate entries.\r\n* Improve the speed of sorting a COO format graph.\r\n* Improve the speed of `dgl.to_bidirected` .\r\n* Fix a bug in building DGL on MacOS using clang.\r\n* Fix a bug in `NodeFlow` when `apply_edges` is called.\r\n* Fix a bug in the stack cross-type reducer in `DGLHeteroGraph.multi_update_all`, `DGLHeteroGraph.multi_pull` and `DGLHeteroGraph.multi_recv` to make the stacking order consistent and to remove a redundant dimension.\r\n* Fix a bug in the loss function of the RGCN example.\r\n* Fix a bug in the MXNet backend when using the new Deep NumPy feature.\r\n* Fix a memory leak bug in the PyTorch backend when retain_graph is True.",
        "dateCreated": "2020-03-30T16:21:04Z",
        "datePublished": "2020-04-02T03:43:38Z",
        "html_url": "https://github.com/dmlc/dgl/releases/tag/0.4.3",
        "name": "v0.4.3",
        "tag_name": "0.4.3",
        "tarball_url": "https://api.github.com/repos/dmlc/dgl/tarball/0.4.3",
        "url": "https://api.github.com/repos/dmlc/dgl/releases/25117689",
        "zipball_url": "https://api.github.com/repos/dmlc/dgl/zipball/0.4.3"
      },
      {
        "authorType": "User",
        "author_name": "BarclayII",
        "body": "[Experimental] Tensorflow support\r\n---\r\nTensorflow support is finally live as an experimental feature. To get started, please read the [instructions](https://docs.dgl.ai/install/backend.html) about switching to tensorflow backend. Currently, we released 13 common NN modules and 4 models.\r\n* NN modules: https://docs.dgl.ai/api/python/nn.tensorflow.html\r\n* Examples: https://github.com/dmlc/dgl/tree/master/examples/tensorflow\r\n\r\nMany bug fixes and minor improvements.",
        "dateCreated": "2020-01-20T09:35:24Z",
        "datePublished": "2020-01-23T04:32:44Z",
        "html_url": "https://github.com/dmlc/dgl/releases/tag/0.4.2",
        "name": "v0.4.2",
        "tag_name": "0.4.2",
        "tarball_url": "https://api.github.com/repos/dmlc/dgl/tarball/0.4.2",
        "url": "https://api.github.com/repos/dmlc/dgl/releases/23060014",
        "zipball_url": "https://api.github.com/repos/dmlc/dgl/zipball/0.4.2"
      },
      {
        "authorType": "User",
        "author_name": "BarclayII",
        "body": "0.4.1 is released today!\r\n\r\nThis minor update includes:\r\n\r\n### CUDA 10.1 support (#950 )\r\n\r\nConda and pip users could install with\r\n\r\n```\r\nconda install -c dglteam dgl-cuda10.1\r\npip install dgl-cu101\r\n```\r\n\r\n### MXNet NN modules support and their examples\r\n\r\n(PR #890, @yzh119 )\r\n\r\n* GATConv\r\n* EdgeConv\r\n* SAGEConv\r\n* SGConv\r\n* APPNPConv\r\n* GINConv\r\n* GatedGraphConv\r\n* GMMConv\r\n* ChebConv\r\n* AGNNConv\r\n* NNConv\r\n* DenseGraphConv\r\n* DenseSAGEConv\r\n* DenseChebConv\r\n\r\n### Miscellaneous\r\n\r\n* PyTorch dependency updated to 1.3.0, using BoolTensors in examples (PR #954, @JakeStevens )\r\n* Adapting to API changes in new NetworkX packages (PR #937 @burness , PR #949 @classicsong )\r\n* Bugfix #946 ",
        "dateCreated": "2019-11-04T03:06:05Z",
        "datePublished": "2019-11-05T03:15:33Z",
        "html_url": "https://github.com/dmlc/dgl/releases/tag/0.4.1",
        "name": "v0.4.1",
        "tag_name": "0.4.1",
        "tarball_url": "https://api.github.com/repos/dmlc/dgl/tarball/0.4.1",
        "url": "https://api.github.com/repos/dmlc/dgl/releases/21210043",
        "zipball_url": "https://api.github.com/repos/dmlc/dgl/zipball/0.4.1"
      },
      {
        "authorType": "User",
        "author_name": "BarclayII",
        "body": "We are thrilled to announce the 0.4 release!  This release extends DGL by (i) including support for heterogeneous graphs and (ii) providing a sub-package to efficiently compute embeddings of large knowledge graphs. In addition, it includes numerous performance improvements, contributed models and datasets, and bugfixes.\r\n\r\n# Support for Heterogeneous Graphs\r\n\r\n## What is a heterogeneous graph?\r\n\r\nMany real world data are about relations between different types of entities.  For instance, an E-commerce data may have three types of entities: *customers*, *items*, and *vendors*.  Customers and items may have different types of interactions such as *clicks* or *purchases*.  Customers can also *follow* each other.  Entities and relations may also have their own set of features.\r\n\r\nA heterogeneous graph, whose nodes and edges are typed, could match the scenario accurately:\r\n\r\n![image](https://user-images.githubusercontent.com/2978100/66535860-a7b24c80-eb4d-11e9-97bf-5fdc82a2a04f.png)\r\n\r\n## Models that work on heterogeneous graphs?\r\n\r\nWe provide a few models to demonstrate the use cases of heterogeneous graphs and the corresponding DGL APIs.\r\n\r\n* [Graph Convolutional Matrix Completion](https://arxiv.org/abs/1706.02263) [[Code in MXNet](https://github.com/dmlc/dgl/tree/master/examples/mxnet/gcmc)]\r\n  * On an EC2 p3.2xlarge instance, we obtained a 5x speedup on MovieLens-100K, and 22x on MovieLens-1M compared against official implementation.  We are also able to train on the entire graph without minibatches on MovieLens-10M (the official implementation goes out of memory).\r\n* [R-GCN](https://arxiv.org/abs/1703.06103) [[Code in PyTorch](https://github.com/dmlc/dgl/tree/master/examples/pytorch/rgcn-hetero)]\r\n  * The new code can train the model for the AM dataset (>5M edges) using one GPU, while the original implementation consumes 32GB of memory, thus cannot fit on a single GPU and can only run on CPU.\r\n  * The original implementation takes 51.88s to train one epoch on CPU. The new R-GCN based on heterograph takes only 0.1781s for one epoch on V100 GPU (**291x faster !!**).\r\n* [Heterogeneous Attention Networks](https://arxiv.org/abs/1903.07293) [[Code in PyTorch](https://github.com/dmlc/dgl/tree/master/examples/pytorch/han)]\r\n  * We provide the `dgl.transform.metapath_reachable_graph` that transform a heterogeneous graph into a new graph, where two nodes are connected if the source node can reach the destination node via the given metapath.\r\n* [Metapath2vec](https://dl.acm.org/citation.cfm?id=3098036) [[Code in PyTorch](https://github.com/dmlc/dgl/tree/master/examples/pytorch/metapath2vec)]\r\n  * We implement the metapath sampler in C++, making it twice as fast as the original implementation.\r\n\r\n**Checkout our heterograph tutorial: [Working with Heterogeneous Graphs in DGL](https://docs.dgl.ai/tutorials/hetero/1_basics.html)**\r\n\r\n**Checkout the full [API reference](https://docs.dgl.ai/api/python/heterograph.html).**\r\n\r\n# DGL-KE : A DGL-based Sub-package for Computing Embeddings of Large Knowledge Graphs\r\n\r\nKnowledge graph (KG) embedding is to embed entities and relations of a KG into continuous vector spaces. The embeddings preserve the inherent structure of the KG and can benefit many downstream tasks such as KG completion and relation extraction as well as recommendations.\r\n\r\nWe release DGL-KE that computes embeddings of large KGs efficiently. The package is adapted from the [KnowledgeGraphEmbedding](https://github.com/DeepGraphLearning/KnowledgeGraphEmbedding) package. We extend KnowledgeGraphEmbedding by leveraging DGL's core to achieve high efficiency and scalability. Using a single NVIDIA V100 GPU, DGL-KE can train TransE on FB15k in **6.85 mins**, substantially outperforming existing tools such as GraphVite.  For graphs with hundreds of millions of edges (such as the full Freebase graph), it takes a couple of hours on **one** large EC2 CPU instance such as m5.24xlarge and x1.32xlarge.\r\n\r\nCurrently, the following models are supported:\r\n\r\n* TransE\r\n* DistMult\r\n* ComplEx\r\n\r\nMore models (RESCAL, RotatE, pRotatE, TransH, TransR, TransD, etc) are under development and will be released in the future.\r\n\r\nDGL-KE supports various training methods:\r\n\r\n* CPU training: Graph Embeddings are stored in CPU memory and mini-batches are trained on CPU.\r\n\r\n* GPU training: Graph Embeddings are stored in GPU memory and mini-batches are trained on GPU.\r\n\r\n* Joint CPU & GPU training: Graph Embeddings are stored in CPU memory but mini-batches are trained on GPU. This is designed for training KGE models on large knowledge graphs that cannot fit in GPU.\r\n\r\n* Multiprocessing training on CPUs: Each CPU process train mini-batches independently and use shared memory for communication between processes. This is designed to train KGE models on large knowledge graphs with many CPU cores.\r\n\r\nMulti-GPU training and distributed training will be released in the future.\r\n\r\nFor more information, please refer to [this directory](https://github.com/dmlc/dgl/tree/master/apps/kg)\r\n\r\n# Miscellaneous\r\n\r\n* New builtin message function: dot product (`u_dot_v` etc. #831 @classicsong )\r\n* More efficient data format and serialization (#728 @VoVAllen )\r\n* ClusterGCN (#877 , @Zardinality )\r\n* CoraFull, Amazon, KarateClub, Coauthor datasets (#855 @VoVAllen )\r\n* More performance improvements\r\n* More bugfixes",
        "dateCreated": "2019-10-07T09:26:57Z",
        "datePublished": "2019-10-07T09:45:21Z",
        "html_url": "https://github.com/dmlc/dgl/releases/tag/v0.4.0",
        "name": "v0.4.0",
        "tag_name": "v0.4.0",
        "tarball_url": "https://api.github.com/repos/dmlc/dgl/tarball/v0.4.0",
        "url": "https://api.github.com/repos/dmlc/dgl/releases/20500087",
        "zipball_url": "https://api.github.com/repos/dmlc/dgl/zipball/v0.4.0"
      },
      {
        "authorType": "User",
        "author_name": "BarclayII",
        "body": "We have received many requests from our community for more GNN layers, models and examples. This is the time to respond. In this minor release, we enriched DGL with a ton of common GNN modules. We have also verified their correctness on some popular datasets so feel free to try them out. Another direction we are working on is to build more domain friendly packages based on DGL. As a first step, we released several pretrained GNN models for molecular property prediction and molecule generation (currently grouped under `dgl.model_zoo` namespace). We will continue explore this idea and release more domain specific models and packages.\r\n\r\nNew APIs\r\n===\r\n\r\nNew NN Modules\r\n---\r\n* `GATConv` from [\u201cGraph Attention Network\u201d](https://arxiv.org/pdf/1710.10903.pdf)\r\n* `RelGraphConv` from [\u201cModeling Relational Data with Graph Convolutional Networks\u201d](https://arxiv.org/abs/1703.06103)\r\n* `TAGConv` from [\u201cTopology Adaptive Graph Convolutional Networks\u201d](https://arxiv.org/pdf/1710.10370.pdf)\r\n* `EdgeConv` from [\u201cDynamic Graph CNN for Learning on Point Clouds\u201d](https://arxiv.org/pdf/1801.07829)\r\n* `SAGEConv` from [\u201cInductive Representation Learning on Large Graphs\u201d](https://arxiv.org/pdf/1706.02216.pdf)\r\n* `GatedGraphConv` from [\u201cGated Graph Sequence Neural Networks\u201d](https://arxiv.org/pdf/1511.05493.pdf)\r\n* `GMMConv` from [\u201cGeometric Deep Learning on Graphs and Manifolds using Mixture Model CNNs\u201d](http://openaccess.thecvf.com/content_cvpr_2017/papers/Monti_Geometric_Deep_Learning_CVPR_2017_paper.pdf)\r\n* `GINConv` from [\u201cHow Powerful are Graph Neural Networks?\u201d](https://arxiv.org/pdf/1810.00826.pdf)\r\n* `ChebConv` from [\u201cConvolutional Neural Networks on Graphs with Fast Localized Spectral Filtering\u201d](https://arxiv.org/pdf/1606.09375.pdf)\r\n* `SGConv` from [\u201cSimplifying Graph Convolutional Networks\u201d](https://arxiv.org/pdf/1902.07153.pdf)\r\n* `NNConv` from [\u201cNeural Message Passing for Quantum Chemistry\u201d](https://arxiv.org/pdf/1704.01212.pdf)\r\n* `APPNPConv` from [\u201cPredict then Propagate: Graph Neural Networks meet Personalized PageRank\u201d](https://arxiv.org/pdf/1810.05997.pdf)\r\n* `AGNNConv` from [\u201cAttention-based Graph Neural Network for Semi-Supervised Learning\r\n\u201d](https://arxiv.org/abs/1803.03735)\r\n* `DenseGraphConv` (Dense implementation of `GraphConv`)\r\n* `DenseSAGEConv` (Dense implementation of `SAGEConv`)\r\n* `DenseChebConv` (Dense implementation of `ChebConv`)\r\n\r\nNew global pooling module\r\n---\r\n* `Sum/Avg/MaxPooling`\r\n* `SortPooling`\r\n* `GlobalAttentionPooling` from GGNN model\r\n* `Set2Set` from [\u201cOrder Matters: Sequence to sequence for sets\u201d](https://arxiv.org/pdf/1511.06391.pdf)\r\n* `SetTransformerEncoder` and `SetTransformerDecoder` from [\u201cSet Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks\u201d](https://arxiv.org/pdf/1810.00825.pdf)\r\n\r\nPlease refer to the [API document](https://docs.dgl.ai/api/python/nn.pytorch.html#module-dgl.nn.pytorch.glob) for more details.\r\n\r\nNew graph transformation routines\r\n---\r\n* `dgl.transform.khop_adj`\r\n* `dgl.transform.khop_graph`\r\n* `dgl.transform.laplacian_lambda_max`\r\n* `dgl.transform.knn_graph`\r\n* `dgl.transform.segmented_knn_graph`\r\n\r\nPlease refer to the [API document](https://docs.dgl.ai/api/python/transform.html) for more details.\r\n\r\nModel zoo for chemistry and molecule applications\r\n===\r\nTo make it easy for domain scientists, we are now releasing a model zoo for chemistry, with training scripts and pre-trained models, and focuses on two particular tasks: property prediction and targeted molecular generation/optimization.\r\n\r\n**Credit**: Shout out to @geekinglcq from Tencent Quantum Lab for contributing three models (MGCN, SchNet and MPNN). We also thank WuXi AppTec CADD team for their critical feedback on usability.\r\n\r\nProperty prediction\r\n---\r\nIn practice, the determination of molecular properties is mostly achieved via wet lab experiments. We can cast the problem as a regression or classification problem. \r\n\r\nFeaturization is the beginning of prediction. Traditionally, chemists develop pre-defined rules to convert molecular graphs into binary strings where each bit indicates the presence or absence of a particular substructure. \r\n\r\nGraph neural networks enable a data-driven representation of molecules out of the atoms, bonds and molecular graph topology, which may be viewed as a learned fingerprint. The message passing mechanism allows the model to learn the interactions between atoms in a molecule. \r\n\r\nThe following code script is self-explanatory.\r\n\r\n```python\r\nfrom dgl.data import Tox21\r\nfrom dgl import model_zoo\r\n\r\ndataset = Tox21()\r\nmodel = model_zoo.chem.load_pretrained('GCN_Tox21') # Pretrained model loaded\r\nmodel.eval()\r\n\r\nsmiles, g, label, mask = dataset[0]\r\nfeats = g.ndata.pop('h')\r\nlabel_pred = model(g, feats)\r\nprint(smiles)                   # CCOc1ccc2nc(S(N)(=O)=O)sc2c1\r\nprint(label_pred[:, mask != 0]) # Mask non-existing labels\r\n# tensor([[-0.7956,  0.4054,  0.4288, -0.5565, -0.0911,  \r\n# 0.9981, -0.1663,  0.2311, -0.2376,  0.9196]])\r\n```\r\n\r\nSupported Models\r\n* Graph Convolution\r\n* Graph Attention Networks\r\n* SchNet\r\n* Multilevel Graph Convolutional neural Network\r\n* Message Passing Neural Networks\r\n\r\nGenerative Models\r\n---\r\n\r\nTargeted molecular generation refers to finding new molecules with desired properties. This gives rise to the need for generative models for two purposes:\r\n* **Distribution Learning**: Given a collection of molecules, we want to model their distribution and generate new molecules consistent with the distribution.\r\n* **Goal-directed Optimization**: Find molecules with desired properties.\r\n\r\nFor this model zoo, we provide only graph-based generative models. There are other generative models working with alternative representations like SMILES. \r\n\r\nExample with Pre-trained Models\r\n\r\n```python\r\n# We recommend running the code below with Jupyter notebooks\r\nfrom IPython.display import SVG\r\nfrom rdkit import Chem\r\nfrom rdkit.Chem import Draw\r\n\r\nfrom dgl import model_zoo\r\n\r\nmodel = model_zoo.chem.load_pretrained('DGMG_ZINC_canonical')\r\nmodel.eval()\r\nmols = []\r\nfor i in range(4):\r\n    SMILES = model(rdkit_mol=True)\r\n    mols.append(Chem.MolFromSmiles(SMILES))\r\n# Generating 4 molecules takes less than a second.\r\n\r\nSVG(Draw.MolsToGridImage(mols, molsPerRow=4, subImgSize=(180, 150), useSVG=True))\r\n```\r\n\r\n![](https://s3.us-east-2.amazonaws.com/dgl.ai/model_zoo/drug_discovery/dgmg_model_zoo_example2.png)\r\n\r\nSupported Models\r\n* Learning Deep Generative Models of Graphs\r\n* Junction Tree Variational Autoencoder for Molecular Graph Generation\r\n\r\nAPI break\r\n===\r\nWe refactor the `nn` package to make all APIs more consistent. Thus, there are following changes to the API that breaks the previous behavior:\r\n* Change the argument order of `dgl.nn.pytorch.GraphConv` and `dgl.nn.mxnet.GraphConv`. The argument order is now first `graph` and then `feat`, which follows the convention of all the other new modules.\r\n\r\nNew model example\r\n===\r\n[Recurrent Relational Networks](https://arxiv.org/pdf/1711.08028.pdf) in PyTorch (credit: @HuXiangkun )\r\n\r\nThere are also many bug fixes and minor changes. We will list them in the next 0.4 major release.\r\n",
        "dateCreated": "2019-08-28T05:45:36Z",
        "datePublished": "2019-08-28T05:46:18Z",
        "html_url": "https://github.com/dmlc/dgl/releases/tag/v0.3.1",
        "name": "v0.3.1",
        "tag_name": "v0.3.1",
        "tarball_url": "https://api.github.com/repos/dmlc/dgl/tarball/v0.3.1",
        "url": "https://api.github.com/repos/dmlc/dgl/releases/19581646",
        "zipball_url": "https://api.github.com/repos/dmlc/dgl/zipball/v0.3.1"
      },
      {
        "authorType": "User",
        "author_name": "BarclayII",
        "body": "DGL v0.3 Release Note\r\n===\r\n\r\nV0.3 release includes many crucial updates:\r\n* Fused message passing kernels that greatly boost the training of GNNs on large graphs. Please refer to our [blogpost](https://www.dgl.ai/blog/2019/05/04/kernel.html) for more details.\r\n* Demostration of how to train GNNs on giant graphs by graph sampling.\r\n* New models and NN modules.\r\n* Many other bugfixes and other enhancement.\r\n\r\nAs a result, please be aware of the following changes:\r\n\r\nInstallation\r\n---\r\n\r\nPrevious installation methods with pip and conda, i.e.:\r\n```\r\npip install dgl\r\nconda install -c dglteam dgl\r\n```\r\nnow **only install CPU builds** (works for Linux/MacOS/Windows).\r\n\r\n### July 2nd update\r\n\r\nWe found that the Windows build of DGL v0.3 on PyPI is currently inconsistent with the 0.3.x branch.  Windows pip users, please install it with:\r\n```\r\npip install https://s3.us-east-2.amazonaws.com/dgl.ai/wheels/dgl-0.3-cp35-cp35m-win_amd64.whl\r\npip install https://s3.us-east-2.amazonaws.com/dgl.ai/wheels/dgl-0.3-cp36-cp36m-win_amd64.whl\r\npip install https://s3.us-east-2.amazonaws.com/dgl.ai/wheels/dgl-0.3-cp37-cp37m-win_amd64.whl\r\n```\r\n\r\n### Installing CUDA builds with pip\r\nPip users could install the DGL CUDA builds with the following:\r\n```\r\npip install <package-url>\r\n```\r\nwhere `<package-url>` is one of the following:\r\n\r\n| | CUDA 9.0 | CUDA 10.0 |\r\n|--|--|--|\r\n| Linux + Py35 | `pip install https://s3.us-east-2.amazonaws.com/dgl.ai/wheels/cuda9.0/dgl-0.3-cp35-cp35m-manylinux1_x86_64.whl` | `pip install https://s3.us-east-2.amazonaws.com/dgl.ai/wheels/cuda10.0/dgl-0.3-cp35-cp35m-manylinux1_x86_64.whl` |\r\n| Linux + Py36 | `pip install https://s3.us-east-2.amazonaws.com/dgl.ai/wheels/cuda9.0/dgl-0.3-cp36-cp36m-manylinux1_x86_64.whl` | `pip install https://s3.us-east-2.amazonaws.com/dgl.ai/wheels/cuda10.0/dgl-0.3-cp36-cp36m-manylinux1_x86_64.whl` |\r\n| Linux + Py37 | `pip install https://s3.us-east-2.amazonaws.com/dgl.ai/wheels/cuda9.0/dgl-0.3-cp37-cp37m-manylinux1_x86_64.whl` | `pip install https://s3.us-east-2.amazonaws.com/dgl.ai/wheels/cuda10.0/dgl-0.3-cp37-cp37m-manylinux1_x86_64.whl` |\r\n| Win + Py35 | `pip install https://s3.us-east-2.amazonaws.com/dgl.ai/wheels/cuda9.0/dgl-0.3-cp35-cp35m-win_amd64.whl` | `pip install https://s3.us-east-2.amazonaws.com/dgl.ai/wheels/cuda10.0/dgl-0.3-cp35-cp35m-win_amd64.whl` |\r\n| Win + Py36 | `pip install https://s3.us-east-2.amazonaws.com/dgl.ai/wheels/cuda9.0/dgl-0.3-cp36-cp36m-win_amd64.whl` | `pip install https://s3.us-east-2.amazonaws.com/dgl.ai/wheels/cuda10.0/dgl-0.3-cp36-cp36m-win_amd64.whl` |\r\n| Win + Py37 | `pip install https://s3.us-east-2.amazonaws.com/dgl.ai/wheels/cuda9.0/dgl-0.3-cp37-cp37m-win_amd64.whl` | `pip install https://s3.us-east-2.amazonaws.com/dgl.ai/wheels/cuda10.0/dgl-0.3-cp37-cp37m-win_amd64.whl` |\r\n| MacOS | N/A | N/A |\r\n\r\n### Installing CUDA builds with conda\r\nConda users could install the CUDA builds with\r\n```\r\nconda install -c dglteam dgl-cuda9.0   # For CUDA 9.0\r\nconda install -c dglteam dgl-cuda10.0  # For CUDA 10.0\r\n```\r\n\r\nDGL currently support CUDA 9.0 (dgl-cuda9.0) and 10.0 (dgl-cuda10.0). To find your CUDA version, use `nvcc --version`. To install from source, checkout our [installation guide](https://docs.dgl.ai/install/index.html#install-from-source).\r\n\r\nNew built-in message and reduce functions\r\n---\r\n\r\nWe have expanded the list of built-in message and reduce functions to cover more use cases. Previously, DGL only has `copy_src`, `copy_edge`, `src_mul_edge`. With the v0.3 release, we support more combinations. Here is a demonstration of some of the new builtin functions.\r\n\r\n```python\r\nimport dgl\r\nimport dgl.function as fn\r\nimport torch as th\r\ng = ... # create a DGLGraph\r\ng.ndata['h'] = th.randn((g.number_of_nodes(), 10)) # each node has feature size 10\r\ng.edata['w'] = th.randn((g.number_of_edges(), 1))  # each edge has feature size 1\r\n# collect features from source nodes and aggregate them in destination nodes\r\ng.update_all(fn.copy_u('h', 'm'), fn.sum('m', 'h_sum'))\r\n# multiply source node features with edge weights and aggregate them in destination nodes\r\ng.update_all(fn.u_mul_e('h', 'w', 'm'), fn.max('m', 'h_max'))\r\n# compute edge embedding by multiplying source and destination node embeddings\r\ng.apply_edges(fn.u_mul_v('h', 'h', 'w_new'))\r\n```\r\n\r\nAs you can see, the syntax is quite straight-forward. `u_mul_e` means multiplying the source node data with the edge data; `u_mul_v` means multiplying the source node data with the destination node data, and so on and so forth. Each builtin combination will be mapped to a CPU/CUDA kernel and broadcasting and gradient computation are also supported. Checkout our [document](https://docs.dgl.ai/features/builtin.html) for more details.\r\n\r\nTutorials for training on giant graphs\r\n---\r\n\r\nTwo new tutorials are now live:\r\n* Train GNNs by neighbor sampling and its variants ([link](https://docs.dgl.ai/tutorials/models/5_giant_graph/1_sampling_mx.html)).\r\n* Scale the sampler-trainer architecture to giant graphs using distributed graph store ([link](https://docs.dgl.ai/tutorials/models/5_giant_graph/2_giant.html)).\r\n\r\nWe also provide scripts on how to setup such distributed setting ([link](https://github.com/dmlc/dgl/tree/master/examples/mxnet/sampling/dis_sampling)).\r\n\r\nEnhancement and bugfix\r\n---\r\n* NN modules\r\n    * `dgl.nn.[mxnet|pytorch].edge_softmax` now directly returns the normalized scores on edges.\r\n    * Fix a memory leak bug when graph is passed as the input.\r\n* Graph\r\n    * `DGLGraph` now supports direct conversion from scipy csr matrix rather than conversion to coo matrix first.\r\n    * Readonly graph can now be batched via `dgl.batch`.\r\n    * `DGLGraph` now supports node/edge removal via `DGLGraph.remove_nodes` and `DGLGraph.remove_edges` ([doc](https://docs.dgl.ai/api/python/graph.html#removing-nodes-and-edges)).\r\n    * A new API `DGLGraph.to(device)` that can move all node/edge data to the given device.\r\n    * A new API `dgl.to_simple` that can convert a graph to a simple graph with no multi-edges.\r\n    * A new API `dgl.to_bidirected` that can convert a graph to a bidirectional graph.\r\n    * A new API `dgl.contrib.sampling.random_walk` that can generate random walks from a graph.\r\n    * Allow `DGLGraph` to be constructed from another `DGLGraph`.\r\n* New model examples\r\n    * APPNP\r\n    * GIN\r\n    * PinSage (slow version)\r\n    * DGI\r\n* Bugfix\r\n    * Fix a bug where numpy integer is passed in as the argument.\r\n    * Fix a bug when constructing from a networkx graph that has no edge.\r\n    * Fix a bug in nodeflow where id is not correctly converted sometimes.\r\n    * Fix a bug in MiniGC dataset where the number of nodes is not consistent.\r\n    * Fix a bug in RGCN example when bfs_level=0.\r\n    * Fix a bug where DLContext is not correctly exposed in CFFI.\r\n    * Fix a crash during Cython build.\r\n    * Fix a bug in `send` when the given message function is a builtin.",
        "dateCreated": "2019-06-13T09:31:36Z",
        "datePublished": "2019-06-13T09:46:50Z",
        "html_url": "https://github.com/dmlc/dgl/releases/tag/v0.3",
        "name": "0.3",
        "tag_name": "v0.3",
        "tarball_url": "https://api.github.com/repos/dmlc/dgl/tarball/v0.3",
        "url": "https://api.github.com/repos/dmlc/dgl/releases/17936879",
        "zipball_url": "https://api.github.com/repos/dmlc/dgl/zipball/v0.3"
      },
      {
        "authorType": "User",
        "author_name": "jermainewang",
        "body": "Major release that includes many features, bugfix and performance improvement. Speed of GCN model on Pubmed dataset has been improved by **4.32x**! Speed of RGCN model on Mutag dataset has been improved by **3.59x**! Important new feature: **graph sampling APIs**.\r\n\r\nUpdate details:\r\n\r\n# Model examples\r\n- [x] TreeLSTM w/ MXNet (PR #279 by @szha )\r\n- [x] GraphSage (@ZiyueHuang )\r\n- [x] Improve GAT model speed (PR #348 by @jermainewang )\r\n\r\n# Core system improvement\r\n- [x] SPMM performance improvement (PR #427 by @ylfdq1118 )\r\n  - Replacing `torch.spmm` with `torch.index_select` plus `torch.scatter_add` gives a huge boost in the speed. (inspired by pytorch-geometric project).\r\n- [x] Immutable CSR graph structure (PR #342 by @zheng-da )\r\n  - [x] Finish remaining functionality (Issue #369, PR #404 by @yzh119)\r\n- [x] Nodeflow data structure (PR #361 by @zheng-da )\r\n- [x] Neighbor sampler (PR #322 )\r\n- [x] Layer-wise sampler (PR #362 by @GaiYu0 )\r\n- [x] Multi-GPU support by data parallelism (PR #356 #338 by @ylfdq1118 )\r\n- [x] More dataset:\r\n  - [x] Reddit dataset loader (PR #372 by @ZiyueHuang )\r\n  - [x] PPI dataset loader (PR #395 by @sufeidechabei )\r\n  - [x] Mini graph classification dataset (PR #364 by @mufeili )\r\n- [x] NN modules (PR #406 by @jermainewang @mufeili)\r\n  - [x] GraphConv layer\r\n  - [x] Edge softmax layer\r\n- [x] Edge group apply API (PR #358 by @VoVAllen )\r\n- [x] Reversed graph and transform.py module (PR #331 by @mufeili )\r\n- [x] Max readout (PR #341 by @mufeili )\r\n- [x] Random walk APIs (PR #392 by @BarclayII )\r\n\r\n# Tutorial/Blog\r\n- [x] Batched graph classification in DGL (PR #360 by @mufeili )\r\n- [x] Understanding GAT (@sufeidechabei )\r\n\r\n# Project improvement\r\n- [x] Python lint check (PR #330 by @jermainewang )\r\n- [x] Win CI (PR #324 by @BarclayII )\r\n- [x] Auto doc build (by @VoVAllen )\r\n- [x] Unify tests for different backends (PR #333 by @BarclayII )",
        "dateCreated": "2019-03-08T22:07:42Z",
        "datePublished": "2019-03-08T23:01:02Z",
        "html_url": "https://github.com/dmlc/dgl/releases/tag/v0.2",
        "name": "0.2",
        "tag_name": "v0.2",
        "tarball_url": "https://api.github.com/repos/dmlc/dgl/tarball/v0.2",
        "url": "https://api.github.com/repos/dmlc/dgl/releases/16006835",
        "zipball_url": "https://api.github.com/repos/dmlc/dgl/zipball/v0.2"
      },
      {
        "authorType": "User",
        "author_name": "jermainewang",
        "body": "Patch release mainly for the Pytorch v1.0 update.\r\n---------------\r\n* Bug fix to be compatible with Pytorch v1.0.\r\n* Bug fix in networkx graph conversion.",
        "dateCreated": "2018-12-11T20:40:51Z",
        "datePublished": "2018-12-11T22:30:34Z",
        "html_url": "https://github.com/dmlc/dgl/releases/tag/v0.1.3",
        "name": "0.1.3",
        "tag_name": "v0.1.3",
        "tarball_url": "https://api.github.com/repos/dmlc/dgl/tarball/v0.1.3",
        "url": "https://api.github.com/repos/dmlc/dgl/releases/14478383",
        "zipball_url": "https://api.github.com/repos/dmlc/dgl/zipball/v0.1.3"
      },
      {
        "authorType": "User",
        "author_name": "jermainewang",
        "body": "The first open release includes basically everything in the repository.\r\n* Basic DGL APIs and systems.\r\n* Backend support for Pytorch and MXNet.\r\n* 10 GNN model examples and tutorials.",
        "dateCreated": "2018-12-07T07:22:46Z",
        "datePublished": "2018-12-07T07:50:59Z",
        "html_url": "https://github.com/dmlc/dgl/releases/tag/v0.1.2",
        "name": "First open release",
        "tag_name": "v0.1.2",
        "tarball_url": "https://api.github.com/repos/dmlc/dgl/tarball/v0.1.2",
        "url": "https://api.github.com/repos/dmlc/dgl/releases/14402829",
        "zipball_url": "https://api.github.com/repos/dmlc/dgl/zipball/v0.1.2"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 8686,
      "date": "Thu, 23 Dec 2021 00:52:02 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "deep-learning",
      "graph-neural-networks"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "DGL provides a plenty of learning materials for all kinds of users from ML researcher to domain experts. The [Blitz Introduction to DGL](https://docs.dgl.ai/tutorials/blitz/index.html) is a 120-minute tour of the basics of graph machine learning. The [User Guide](https://docs.dgl.ai/guide/index.html) explains in more details the concepts of graphs as well as the training methodology. All of them include code snippets in DGL that are runnable and ready to be plugged into one\u2019s own pipeline.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Users can install DGL from [pip and conda](https://www.dgl.ai/pages/start.html). Advanced users can follow the [instructions](https://docs.dgl.ai/install/index.html#install-from-source) to install from source.\n\nFor absolute beginners, start with [the Blitz Introduction to DGL](https://docs.dgl.ai/tutorials/blitz/index.html). It covers the basic concepts of common graph machine learning tasks and a step-by-step on building Graph Neural Networks (GNNs) to solve them.\n\nFor acquainted users who wish to learn more,\n\n* Learn DGL by [example implementations](https://www.dgl.ai/) of popular GNN models.\n* Read the [User Guide](https://docs.dgl.ai/guide/index.html) ([\u4e2d\u6587\u7248\u94fe\u63a5](https://docs.dgl.ai/guide_cn/index.html)), which explains the concepts and usage of DGL in much more details.\n* Go through the tutorials for advanced features like [stochastic training of GNNs](https://docs.dgl.ai/tutorials/large/index.html), training on [multi-GPU](https://docs.dgl.ai/tutorials/multi/index.html) or [multi-machine](https://docs.dgl.ai/tutorials/dist/index.html).\n* [Study classical papers](https://docs.dgl.ai/tutorials/models/index.html) on graph machine learning alongside DGL.\n* Search for the usage of a specific API in the [API reference manual](https://docs.dgl.ai/api/python/index.html), which organizes all DGL APIs by their namespace.\n\nAll the learning materials are available at our [documentation site](https://docs.dgl.ai/). If you are new to deep learning in general,\ncheck out the open source book [Dive into Deep Learning](https://d2l.ai/).\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "We provide multiple channels to connect you to the community of the DGL developers, users, and the general GNN academic researchers:\n\n* Our Slack channel, [click to join](https://join.slack.com/t/deep-graph-library/shared_invite/zt-eb4ict1g-xcg3PhZAFAB8p6dtKuP6xQ)\n* Our discussion forum: https://discuss.dgl.ai/\n* Our [Zhihu blog (in Chinese)](https://www.zhihu.com/column/c_1070749881013936128)\n* Monthly GNN User Group online seminar ([event link](https://www.eventbrite.com/e/graph-neural-networks-user-group-tickets-137512275919?utm-medium=discovery&utm-campaign=social&utm-content=attendeeshare&aff=escb&utm-source=cp&utm-term=listing) | [past videos](https://www.youtube.com/channel/UCnmuSDY1pTlaFH1WRQElfTg))\n\nTake the survey [here](https://forms.gle/Ej3jHCocACmb49Gp8) and leave any feedback to make DGL better fit for your needs. Thanks!\n\n",
      "technique": "Header extraction"
    }
  ]
}