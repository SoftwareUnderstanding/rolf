{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1509.06461\n\n<img src=\"Misc/Double Q.png\"><br><br>\n\n<br> It has been proven mathematically and empirically that using Deep Q-Network approximation converges to optimal policy in reasonable amount of time.\n\n\nTraining Result:\n===============\n<br><br>\n**Before training:**<br><br>\n<img src=\"Misc/Initial.gif\">\n\n**After 800 games:**<br><br>\n<img src=\"Misc/NextGen.gif\">\n\n<br><br>\n**Learning curve:**<br><br>\n<img src=\"Misc/Plot.png\"><br>\n\n* The Blue curve shows the reward the agent earned in each episode.\n* The Red curve shows the average reward from the corresponding episode in the x-axis and 100 previous episodes. In other words, it shows the average reward of 100 most current episodes.\n* From the plot, we see that the Blue curve is much noisier due to exploration \u03b5 = 0.1 throughout the training process and due to the imperfect approximation during some first episodes of the training.\n* Averaging 100 most current rewards produces much smoother curve, however.\n* From the curve, we can conclude that the agent has successfully learned a good policy to solve the Lunar Lander problem, according to OpenAI criteria (the average point of any 100 consecutive episodes is at least 200"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.887167692142383
      ],
      "excerpt": "Replay memory size: 10^6 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8511535834563826
      ],
      "excerpt": "<img src=\"Misc/Q-learning.jpg\"><br><br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "<img src=\"Misc/Estimation.jpg\"><br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9912556242864674
      ],
      "excerpt": "Reference: https://arxiv.org/abs/1509.06461 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/anh-nn01/Lunar-Lander-Double-Deep-Q-Networks",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-06-09T03:00:11Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-23T15:23:51Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* The agent has to learn how to land a Lunar Lander to the moon surface safely, quickly and accurately.\n* If the agent just lets the lander fall freely, it is dangerous and thus get a very negative reward from the environment.\n* If the agent does not land quickly enough (after 20 seconds), it fails its objective and receive a negative reward from the environment.\n* If the agent lands the lander safely but in wrong position, it is given either a small negative or small positive reward, depending on how far from the landing zone is the lander.\n* If the AI lands the lander to the landing zone quickly and safely, it is successful and is award very positive reward.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.95748625859457
      ],
      "excerpt": "Since the state space is infinite, traditional Q-value table method does not work on this problem. As a result, we need to integrate Q-learning with Neural Network for value approximation. However, the action space remains discrete. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9690611435155726,
        0.8632046598367357
      ],
      "excerpt": "The equation above based on Bellman equation. You can try creating a sample graph of MDP to see intuitively why the Q-learning method converge to optimal value, thus converging to optimal policy. \nFor Deep Q-learning, we simply use a NN to approximate Q-value in each time step, and then update the NN so that the estimate Q(s,a) approach its target:<br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.936179082469119
      ],
      "excerpt": "Purpose of using Double Deep Q-network:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9484212376933253
      ],
      "excerpt": "<br> It has been proven mathematically and empirically that using Deep Q-Network approximation converges to optimal policy in reasonable amount of time. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8379850560636444,
        0.914980772893374
      ],
      "excerpt": "The Red curve shows the average reward from the corresponding episode in the x-axis and 100 previous episodes. In other words, it shows the average reward of 100 most current episodes. \nFrom the plot, we see that the Blue curve is much noisier due to exploration \u03b5 = 0.1 throughout the training process and due to the imperfect approximation during some first episodes of the training. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.933583014901179
      ],
      "excerpt": "From the curve, we can conclude that the agent has successfully learned a good policy to solve the Lunar Lander problem, according to OpenAI criteria (the average point of any 100 consecutive episodes is at least 200). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "An AI agent that use Double Deep Q-learning to teach itself to land a Lunar Lander on OpenAI universe",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/anh-nn01/Lunar-Lander-Double-Deep-Q-Networks/releases",
    "technique": "GitHub API"
  },
  "faq": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* The agent has to learn how to land a Lunar Lander to the moon surface safely, quickly and accurately.\n* If the agent just lets the lander fall freely, it is dangerous and thus get a very negative reward from the environment.\n* If the agent does not land quickly enough (after 20 seconds), it fails its objective and receive a negative reward from the environment.\n* If the agent lands the lander safely but in wrong position, it is given either a small negative or small positive reward, depending on how far from the landing zone is the lander.\n* If the AI lands the lander to the landing zone quickly and safely, it is successful and is award very positive reward.\n\n",
      "technique": "Header extraction"
    }
  ],
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3,
      "date": "Wed, 29 Dec 2021 14:18:43 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/anh-nn01/Lunar-Lander-Double-Deep-Q-Networks/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "anh-nn01/Lunar-Lander-Double-Deep-Q-Networks",
    "technique": "GitHub API"
  },
  "invocation": [
    {
      "confidence": [
        0.8618422943976106
      ],
      "excerpt": "<img src=\"Misc/Q-learning.jpg\"><br><br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8798481063309972,
        0.8941388543968927,
        0.8923205802291084,
        0.8009284526700153
      ],
      "excerpt": "<img src=\"Misc/Estimation.jpg\"><br> \n<img src=\"Misc/Target.jpg\"><br><br> \n<img src=\"Misc/Loss.jpg\"><br><br> \n<img src=\"Misc/Graph.png\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9202509175650415,
        0.9202509175650415
      ],
      "excerpt": "<img src=\"Misc/Q-table.jpg\"><br><br> \n<img src=\"Misc/Q-NN.jpg\"><br><br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8836704212480256
      ],
      "excerpt": "<img src=\"Misc/Double Q.png\"><br><br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8658507165426992
      ],
      "excerpt": "<img src=\"Misc/Initial.gif\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8647354795712904
      ],
      "excerpt": "<img src=\"Misc/NextGen.gif\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.933967490471044
      ],
      "excerpt": "<img src=\"Misc/Plot.png\"><br> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/anh-nn01/Lunar-Lander-Double-Deep-Q-Networks/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 Nhu Nhat Anh\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Lunar-Lander-Double-Deep-Q-Networks\nAn AI agent that use Double Deep Q-learning to learn by itself how to land a Lunar Lander on OpenAI universe\n# AI-Lunar-Laner-Lander-v2-Keras TF Backend\nA Reinforcement Learning AI Agent that use Deep Q Network to play Lunar Lander\n\n\nAlgorithm Details and Hyperparameters:",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Lunar-Lander-Double-Deep-Q-Networks",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "anh-nn01",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/anh-nn01/Lunar-Lander-Double-Deep-Q-Networks/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 9,
      "date": "Wed, 29 Dec 2021 14:18:43 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "reinforcement-learning",
      "deep-q-network",
      "lunar-lander",
      "artificial-intelligence",
      "machine-learning",
      "double-dqn",
      "openai-universe",
      "keras"
    ],
    "technique": "GitHub API"
  }
}