{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1711.07971>"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9238472809394269
      ],
      "excerpt": "- Perform a stratified split into train, validationn and test (80-10-10). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8538661335395704
      ],
      "excerpt": "    perform whether the wrong answers follow a certain pattern. Indeed, \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jordiae/DeepLearning-MAI",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-02-27T12:14:28Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-02-25T04:54:17Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.994240867133244
      ],
      "excerpt": "This is the repository containing the source code for the assignments of the Deep Learning course at the Master in Artificial Intelligence at UPC-BarcelonaTech. The code is based on PyTorch. Authors: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.937841399093278
      ],
      "excerpt": "    - cnn/: Code for CNNs (mit67). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8201651519994072
      ],
      "excerpt": "    - transfer/: Code for transfer learning (mit67). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9186670816616639
      ],
      "excerpt": "- By default, we resize all images to 256x256, even if our model is input-size-agnostic. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9563731095177103
      ],
      "excerpt": "The default options are the ones that we observed to perfom better in our experiments. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8025679514871208
      ],
      "excerpt": "experiment with the desired configuration and the --autoencoder option. Then, move the generated \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.959961309707981
      ],
      "excerpt": "For evaluating a model: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.833073612175836
      ],
      "excerpt": "The evaluation options are the following (notice that ensembles are implemented by providing more than one checkoint): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8204934568558861
      ],
      "excerpt": "We conducted as many as 150 experiments in the CTE-POWER cluster (V100 GPUs) at the Barcelona Supercomputing Center. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8242841766129188,
        0.9095085075894895,
        0.8580364148750974,
        0.9357669409588669
      ],
      "excerpt": "challening because it only contains 15,620 images, and by manually inspecting the 67 (imbalanced) classes, we can \nobserve that they are not easily distinguishable. Notice that for this assignment we were not supposed to use transfer \nlearning from another dataset (eg. VGG16), which made the problem more difficult. \nBy evaluating the different configurations in the validation set and comparing the results, we highlight the following \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9542638641158361,
        0.8652221150871705,
        0.880211669670238,
        0.860059181823877,
        0.9872778191592435,
        0.8213540532190917
      ],
      "excerpt": "- Since the dataset is tiny, data augmentation was key to improve our results. \n- After performing a grid search on a number of architectural hyperparameters, we found a model with 5 convolutional \nblocks, with 2 convolutional layers per block with a kernel size of 3, and 2 fully-connected layers to be the best \nperformant model. \n- Our models starts with 16 channels and doubles the number of channels for each block, while the image size is \ndivided by 2 with pooling. This is why we call the architecture 'PyramidCNN'. Both 32 and 8 initial channels, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9181292458557814
      ],
      "excerpt": "- Using stride instead of pooling did not lead to any improvement (actually, it performed worse). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9910525917438342,
        0.9044192380499662
      ],
      "excerpt": "- For regularizing, we found that a dropout (only in the convolutional layers) of 0.25 and a weight decay of 0.001 \nwere the best options. We had to add some patience to the early stopping procedure. A label smoothing of 0.1 was \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9361071081503729,
        0.8754725266799306
      ],
      "excerpt": "- Ensembling independently trained models reulted in a gain of about 8 points in accuracy. \n- Since the images present in the dataset have global features, we tried to incorporate Non-Local blocks \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9650224318615738,
        0.9800891389428463
      ],
      "excerpt": "The best found configuration, corresponding to the default values in the train.py script, obtained an accuracy of  58  in the validation set. Then, we build an ensemble of 10 independently trained classifiers with the same configuration, and obtained a validation accuracy of 64. We selected this ensemble as our final model, and obtained a test accuracy of 63. \nThe goal of this work is to build an end-to-end neural system to answer mathematical questioNS. We will do so with a vanilla, character-level Seq2seq. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9665030720151375
      ],
      "excerpt": "We do not apply any preprocessing of the data (apart from uncasing). We provide raw characters to the neural network. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8887655755624762
      ],
      "excerpt": "                                                                      '(considerably less efficient) instead of the' \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9776720531701933
      ],
      "excerpt": "                                                                      'instead of our hand-written ones, for' \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.959961309707981
      ],
      "excerpt": "For evaluating a model: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8428928631126015,
        0.8520870534961787,
        0.995739997909713,
        0.9875855988245382
      ],
      "excerpt": "The accuracies obtained with this model in each subset are the following ones: 78.97% on train, 57.02% on validation and 56.56% on test. \nWe observe that: \nnumbers__place_value is extremely easy for our model, because it \n    is almost a character-level task. The model is asked to return the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8481899179262893
      ],
      "excerpt": "    it refers the question, but this involves no high-level, abstract \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.879413727249967,
        0.9158027849807204
      ],
      "excerpt": "numbers__round_number is the second best-performing task and, \nagain, it basically consists of a character-level task (rounding to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8098603246648142,
        0.8043114132558278
      ],
      "excerpt": "The other problems are all related to number theory and require a \ndeeper understanding, so accuracies are lower. At least, all of them \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8114851728414121,
        0.958980702794384,
        0.9293305435144443,
        0.9196886143103551,
        0.8072898108880958
      ],
      "excerpt": "learning and might be modular up to a certain point. The accuracies \nof each one of this tasks seem to be correlated to the intrinsic \ndifficulty of each task, especially because some of them depend on \nthe skills required for other ones in a composite manner. For \ninstance, just knowing that a given number is a factor of another \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9572990681562501
      ],
      "excerpt": "the greatest common divisor is slightly more difficult (60.61%). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9043090828389583
      ],
      "excerpt": "task, so we hypothesize that the model might be using almost the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.962719157766971
      ],
      "excerpt": "exact remainder (numbers__div_remainder) is the task with the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9115502511966996
      ],
      "excerpt": "zero or not is considerably easier than retreiving the specific \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9486321887093105
      ],
      "excerpt": "but we believe to be the case that the model will generalize better \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.993146456595769
      ],
      "excerpt": "In training, it is interesting to see the evolution of answers to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9761880307751193
      ],
      "excerpt": "it starts by writing either \u2019t\u2019 or \u2019f\u2019, but it is not able of accurately \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8521916938925094,
        0.898114452863402,
        0.8010078801229354,
        0.913613779335266
      ],
      "excerpt": "P(\u2019alse\u2019|\u2019f\u2019) = 1 (in this dataset) is relatively easy, but \nunderstanding the question, or even more, guessing its answer, is not. \nLater in the training, once the model has higher accuracies, these \nartifacts caused by teacher forcing disappear. Once the model is fully \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8893779866741992
      ],
      "excerpt": "In addition, we make the following observations on the selected model in \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.955014897312834,
        0.8487659511519955,
        0.9624481457145334
      ],
      "excerpt": "numbers__base_conversion: In the wrong answers of this problem, we \n    observe that at least the model gives plausible answers. For \n    instance, when is asked to convert a given number to binary, the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9693233393948762
      ],
      "excerpt": "Question: 32 (base 4) to base 2 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9908169296828295
      ],
      "excerpt": "Recall that in our implementation we cut the decoding when the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8372761447096521
      ],
      "excerpt": "numbers__div_remainder: The model does not do well in this task. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9757966734341981
      ],
      "excerpt": "Question: calculate the remainder when 1799 is divided by 1796. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9530610775744349,
        0.8863780097493438
      ],
      "excerpt": "numbers__gcd: In this case, we said that the model performs well. \n    A pattern that we have found in the errors is that at least it gets \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9483805245710437
      ],
      "excerpt": "Question: calculate the greatest common factor of 56 and 20. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9900430831574596
      ],
      "excerpt": "Question: what is the highest common divisor of 300 and 720? \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8531375781168639,
        0.9832523586169352
      ],
      "excerpt": "numbers__is_factor: As we said, all the wrong answers are at least \n    proper boolean answers. We have found that the model is more prone \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9193633936950328
      ],
      "excerpt": "    factorizations. The model could just learn to output that an even \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8582382085813246
      ],
      "excerpt": "    in the dataset there are not many even numbers (the authors of the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9029275119276268
      ],
      "excerpt": "    quantitative analysis that the accuracy in numbers__is_prime in \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9891785691729262
      ],
      "excerpt": "Question: is 20783 composite? \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8993036943685405
      ],
      "excerpt": "The factors of 20783 are 1, 7, 2969, 20783. This particular case \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8997619414835293
      ],
      "excerpt": "the case that the model does not know all the factors, but at least \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8587017639734148
      ],
      "excerpt": "numbers__lcm: In this case, there are some remarkable answers, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8369397252715289
      ],
      "excerpt": "Question: find the common denominator of -115/6 and 5/21. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.853125869071648
      ],
      "excerpt": "numbers__list_prime_factors: The model did not perform well in \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8349637975653459,
        0.9399017895310178
      ],
      "excerpt": "    very common) successes, such as: \nQuestion: list the prime factors of 708. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9190995216845087,
        0.9047501047258553
      ],
      "excerpt": "However, these answers are not anecdotic, because it is difficult to \nguess them just by chance and the accuracy is significantly better \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9219297566791874,
        0.982440174945589,
        0.8800276844705146
      ],
      "excerpt": "numbers__place_value: Recall from the quantitive analysis that for \n    this kind of problems our best model obtained almost an accuracy of \n    100%. We believe to be interesting to perform an error analysis, to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8192403511170405,
        0.9631002683265533,
        0.8440103826385287,
        0.9703107975650164
      ],
      "excerpt": "    they do. Most of the wrong answers have one thing in common: the \n    question asks to retrieve a digit very close to the right of a big a \n    number. This seems to a certain overfitting to the specific digit \n    lengths of the dataset, as we said in the quantitative analysis. For \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9347583408507656
      ],
      "excerpt": "Question: what is the units digit of 39358? \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9089421076522707,
        0.8363575759335045
      ],
      "excerpt": "Notice that the selected model is not bidirectional, which might be \nnegative for these cases (but, empirically, bidirectional models did \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.826589612387074,
        0.9167632723860912,
        0.9693233393948762
      ],
      "excerpt": "numbers__round_number: We observe that there are many correct \nanswers with relatively long sequences of digits: \nQuestion: round 0.0000083 to six dps. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Code for the Deep Learning course (Master in Artificial Intelligence at UPC)",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jordiae/DeepLearning-MAI/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 23 Dec 2021 03:13:22 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jordiae/DeepLearning-MAI/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "jordiae/DeepLearning-MAI",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/jordiae/DeepLearning-MAI/master/move-to-cluster-bsc.sh",
      "https://raw.githubusercontent.com/jordiae/DeepLearning-MAI/master/sample-mathematics.sh",
      "https://raw.githubusercontent.com/jordiae/DeepLearning-MAI/master/setup.sh",
      "https://raw.githubusercontent.com/jordiae/DeepLearning-MAI/master/get-deepmind-mathematics.sh",
      "https://raw.githubusercontent.com/jordiae/DeepLearning-MAI/master/get-mit67.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.821367202096512
      ],
      "excerpt": "The evaluation options are the following (notice that ensembles are implemented by providing more than one checkoint): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8161396334976645
      ],
      "excerpt": "bash sample-mathematics.sh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8106831861091752
      ],
      "excerpt": "Detecting that a question requires a boolean answer or that \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8229176418721967
      ],
      "excerpt": "- data/: Directory where data will be downmloaded. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8217695065332454
      ],
      "excerpt": "   - train.py: Script to train a given model (with a given configuration) on a given dataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9098848181902882
      ],
      "excerpt": "python src/cnn/preprocess.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8308788348364196,
        0.8480032227692669
      ],
      "excerpt": "- Convert a few PNG into JPG. \n- Perform a stratified split into train, validationn and test (80-10-10). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9244684454846165
      ],
      "excerpt": "python src/cnn/create_experiment.py experiment_name [options] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8870264523231494
      ],
      "excerpt": "The text logs (.out, .err and .log) files, as well as the Tensorboard logs, will always be stored in the directory of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9488405792100988
      ],
      "excerpt": "python src/cnn/train.py [options] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8987754728946826,
        0.8286506882282672
      ],
      "excerpt": "parser.add_argument('--data', type=str, help='Dataset', default='256x256-split') \nparser.add_argument('--epochs', type=int, help='Number of epochs', default=100) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8118390656699366
      ],
      "excerpt": "parser.add_argument('--momentum', type=float, help='Momentum', default=0.9) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8545291039303196,
        0.8182781267416042,
        0.8419557020724311
      ],
      "excerpt": "parser.add_argument('--optimizer', type=str, help='Optimizer', default='Adam') \nparser.add_argument('--batch-size', type=int, help='Mini-batch size', default=32) \nparser.add_argument('--criterion', type=str, help='Criterion', default='label-smooth') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8118390656699366,
        0.8452637477778311,
        0.8288915933038944
      ],
      "excerpt": "parser.add_argument('--weight-decay', type=float, help='Weight decay', default=0.001) \nparser.add_argument('--kernel-size', type=int, help='Kernel size', default=3) \nparser.add_argument('--dropout', type=float, help='Dropout in FC layers', default=0.25) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8761039734330478
      ],
      "excerpt": "parser.add_argument('--initial-channels', type=int, help='Channels out in first convolutional layer', default=16) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.85898091474401
      ],
      "excerpt": "checkpoint_best.pt to the experiment directory, change its name to checkpoint_last.pt and start the training \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9129473343930131
      ],
      "excerpt": "python src/cnn/evaluate.py [options] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8776232046225435,
        0.9024546551677984,
        0.8987754728946826,
        0.9194227704628507
      ],
      "excerpt": "parser.add_argument('--models-path', type=str, help='Path to model directory',nargs='+') \nparser.add_argument('--checkpoint', type=str, default='checkpoint_best.pt',  help='Checkpoint name') \nparser.add_argument('--data', type=str, help='Dataset', default='256x256-split') \nparser.add_argument('--subset', type=str, help='Data subset', default='test') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8301827353552413
      ],
      "excerpt": "parser.add_argument('--batch-size', type=int, help='Mini-batch size', default=2) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9633057653407734
      ],
      "excerpt": "python src/RNN/create_experiment.py experiment_name [options] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8870264523231494
      ],
      "excerpt": "The text logs (.out, .err and .log) files, as well as the Tensorboard logs, will always be stored in the directory of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.974212512240273
      ],
      "excerpt": "python src/RNN/train.py [options] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8412893633695965,
        0.8540848934546209,
        0.8286506882282672
      ],
      "excerpt": "    parser.add_argument('--dataset-instances', type=int, default=100000, \n                        help='Number of total instances we want to load from the dataset') \n    parser.add_argument('--epochs', type=int, help='Number of epochs', default=100) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8118390656699366
      ],
      "excerpt": "    parser.add_argument('--momentum', type=float, help='Momentum', default=0.9) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8545291039303196,
        0.8383173209572297,
        0.8545291039303196
      ],
      "excerpt": "    parser.add_argument('--optimizer', type=str, help='Optimizer', default='Adam') \n    parser.add_argument('--batch-size', type=int, help='Mini-batch size', default=64) \n    parser.add_argument('--criterion', type=str, help='Criterion', default='xent') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8118390656699366,
        0.8322472952533219
      ],
      "excerpt": "    parser.add_argument('--weight-decay', type=float, help='Weight decay', default=0.0001) \n    parser.add_argument('--dropout', type=float, help='Dropout in RNN and FC layers', default=0.15) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8431370784094824
      ],
      "excerpt": "    parser.add_argument('--hidden-size', type=int, help='Hidden state size', default=128) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9129473343930131
      ],
      "excerpt": "python src/cnn/evaluate.py [options] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8021261303411427
      ],
      "excerpt": "True/False questions. From very early in the training, the model seems \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8305779873870941
      ],
      "excerpt": "predicting whether they will be true or false. Because of teacher \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8398424885523661
      ],
      "excerpt": "trained, wrong answers to True/False will almost always be either \"true\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8027780623142533
      ],
      "excerpt": "the test set: \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jordiae/DeepLearning-MAI/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'\\nMIT License\\n\\nCopyright (c) 2020 Text Mining Unit at BSC\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Deep Learning assignments (DL-MAI)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "DeepLearning-MAI",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "jordiae",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jordiae/DeepLearning-MAI/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Provided Python3.7 and CUDA are already installed in the system, run:\n\n```\nbash setup.sh\n```\n\nThis script activates the virtual environment, downloads the required dependencies and marks 'src' as the sources root.\n\nFor obtaining the data of the CNNs and transfer learning assignments (mit67), run:\n\n```\nbash get-mit67.sh\npython src/cnn/preprocess.py\n\n```\n\nIn the case of RNNs (DeepMind's Mathematics Dataset):\n\n```\nget-deepmind-mathematics.sh\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 23 Dec 2021 03:13:22 GMT"
    },
    "technique": "GitHub API"
  }
}