{
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rupakdas18/SemEval-2017-Task-4-A-B-C-using-BERT",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-05-07T22:26:26Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-05-29T15:29:27Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9964741436301738
      ],
      "excerpt": "This project describes the BERT model which is a transformer-based architecture.  BERT has been used in task 4(A,B,C), English Language, Sentiment Analysis in Twitter of SemEval2017. BERT is a very powerful model for classification tasks when the number of training data is small. For this experiment, we have used BERTBASE model which has 12 hidden layers. This model provides better accuracy, precision, recall, f1 score than the Naive Bayes baseline model. It performs better in binary classification subtask than the multi-class classification subtasks. We also considered all kinds of ethical issues during this experiment as Twitter data contains personal and sensible information. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8659395375783564
      ],
      "excerpt": "Given a message, classify whether the message is of positive, negative, or neutral sentiment. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.846816363388748
      ],
      "excerpt": "Given a message and a topic classify based on a two-point scale (positive or negative). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.976798738437152,
        0.9055568568591268,
        0.9751931667350313,
        0.9749009836524252
      ],
      "excerpt": "We are going to use the BERT model with the hugging face framework which is available in https://huggingface.co/transformers/model_doc/bert.html. BERT model is actually a multi-layer bidirectional Transformer encoder. The Transformer architecture is described in https://arxiv.org/pdf/1706.03762.pdf. It is an encoder-decoder network that uses self-attention on the encoder side and attention on the decoder side. The Transformer reads entire sequences of tokens at once while LSTMs read sequentially. BERT has 2 model sizes. One is BERT-BASE that contains 12 layers and another one is BERT-LARGE has 24 layers in the encoder stack. In the Transformer, the number of layers in an encoder is 6. After the encoder layer, both BERT model has Feedforward-network with 768 and 1024 hidden layer, respectively. Those two models have more self-attention heads (12 and 16 respectively) than Transformer. BERT-BASE contains 110M parameters while BERT-LARGE contains 340M parameters. \nThis model has 30,000 token vocabularies. It takes ([CLS]) token as input first, then it is followed by a sequence of words as input. Here ([CLS]) is a classification token. It then passes the input to the above layers. Each layer applies self-attention, passes the result through a feedforward network after then it hands off to the next encoder. The model outputs a vector of hidden size (768 for BERT-BASE and 1024 for BERT-LARGE ). If we want to output a classifier from this model, we can take the output corresponding to [CLS] token. \nBERT is pre-trained using Masked Language Modeling (MLM) and Next Sentence Prediction (NSP) on a large corpus from Wikipedia.  BERT was trained by masking 15% of the tokens with the goal to guess those words. For the pre-training corpus authors used the BooksCorpus which contains 800M words and English Wikipedia which contains 2,500M words. \nBERT is helpful for different Natural Language Processing tasks like classification tasks, Name Entity Recognition, Part of Speech tagging, Question Answering, etc. But it is not useful for Language Models, Language Translation or Text Generation. BERT model is large and takes time for fine-tuning and inferencing. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rupakdas18/Research-Project-UMD-CS-5642/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 21 Dec 2021 19:03:33 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/rupakdas18/SemEval-2017-Task-4-A-B-C-using-BERT/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "rupakdas18/SemEval-2017-Task-4-A-B-C-using-BERT",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/rupakdas18/Research-Project-UMD-CS-5642/main/INSTALL.sh",
      "https://raw.githubusercontent.com/rupakdas18/Research-Project-UMD-CS-5642/main/EXPERIMENT.sh"
    ],
    "technique": "File Exploration"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/rupakdas18/SemEval-2017-Task-4-A-B-C-using-BERT/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Research-Project-UMD-CS-5642",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "SemEval-2017-Task-4-A-B-C-using-BERT",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "rupakdas18",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/rupakdas18/SemEval-2017-Task-4-A-B-C-using-BERT/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Tue, 21 Dec 2021 19:03:33 GMT"
    },
    "technique": "GitHub API"
  }
}