{
  "citation": [
    {
      "confidence": [
        0.9372151656983196
      ],
      "excerpt": "(array([   11,     5,     2,    10,     4,     7,     5,     2,     9, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9593299683604384,
        0.9889581433319835,
        0.9593299683604384
      ],
      "excerpt": "[tensor([[11.,  5.,  2., 10.], \n           [12., 11.,  5.,  2.]]), tensor([[ 5.,  2., 10.,  4.], \n           [11.,  5.,  2., 10.]])] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8714162992508173
      ],
      "excerpt": "  trn_sampler = TrainingSampler(data_source=trn_clas_ds.x, key=lambda t: len(trn_clas_ds.x[t]), bs=2) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8714162992508173
      ],
      "excerpt": "  val_sampler = ValidationSampler(val_clas_ds.x, key=lambda t: len(val_clas_ds.x[t])) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8996711421436383
      ],
      "excerpt": "  sched_cos1 = sched_cos(start=lr/10, end=lr*2) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8796522232637305,
        0.9993702886184715,
        0.9698834407712852
      ],
      "excerpt": "Regularizing and Optimizing LSTM Language Models by Stephen Merity et al \n  Article: https://arxiv.org/pdf/1708.02182.pdf \n  Github: https://github.com/salesforce/awd-lstm-lm \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/uchange/ulangel",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-12-19T14:22:46Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-03-05T13:23:53Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9921854707571235,
        0.9877271310496334,
        0.9706976864432908,
        0.9910729123488358,
        0.9715307998061514,
        0.9931010505363365,
        0.9448101269728024,
        0.885587378618858,
        0.8055719010076429,
        0.9670008730293775,
        0.9718536224501663,
        0.891616474459063
      ],
      "excerpt": "Ulangel is a python library for NLP text classification. The idea comes from the article of Jeremy Howard et al. \"Universal Language Model Fine-tuning for Text Classification\" https://arxiv.org/pdf/1801.06146.pdf. The original codes are from the fastai library (https://github.com/fastai/course-v3). We use its NLP part as a source of reference and modify some codes to adapt to our use case. The name ulangel comes from universal language model. It also means the fruit of the research department of company U (a french parisian startup), so called l'ange de U (the angel of U). U aimes to describe the ecosystem established by corporates as well as startups by our product Motherbase. In Motherbase, we have a large quantity of texts concerning company descriptions, communications where we apply this library ulangel to do the Natural Language Processing. \nThis is a LSTM based neural network. To classify the text, we train at first a language model and then fine-tune it into a classifier. That means the whole system is composed of two parts: language model and the text classifier. \nThe language model is trained to predict the next word based on the input text. Its structure is shown below:  \nIt is supposed to treat only texts, because other features won't help to predict the next word. \nThe classifier is adapted from the language model: it keeps all layers except the decoder and then adds a pooling layer and a full connected linear neural network in order to classify. \nDifferent from the language model input, there are two kinds of inputs that this library is able to deal with for the text classification: \n* Text only mode: This input mode means that the input consists of only integers of the text, for exemple: [45, 30, ..., 183]. The classifier structure is shown in the figure below:  \n* Text plus mode: This input mode means that the input consists of not only integers of the text, but also other features for the classification problem, for exemple: [[45, 30, ..., 183], True, 2, 2019, ...]. The list [16, 8, 9, 261, ...] is integers of the text as in the text only mode. True can be a boolean to tell if this text contains a country name, 2 can be the number of presence of the country name, 2019 can be the publication year of this text. You can also add as many features as you want. All features after the integer list can be no matter what you want, as long as they are useful for your classification problem. The classifier structure is shown in the figure below:  \nIn this library you will find all structures needed to process texts, to pack data, to construct your lstm with dropouts, and some evaluation fuctions, optimizers to train your neural network. \nAll classes and methodes are seperated in three different parts of this library: \n* ulangel.data: Preparation of the text classification data, including the text preparation (such as text cleaning) and the data formating (such as creating dataset, creating databunch, padding all texts to have the same length in the same batch, etc.). \n* ulangel.rnn: Create recurrent neural network structures, such as connection dropouts, activation dropouts, LSTM for language model, encoder, etc. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9937845112533328,
        0.9357313166814163
      ],
      "excerpt": "This part is for the data preparation. There are two main groups of functionalities: ulangel.data.text_processor for the text cleaning, and ulangel.data.data_packer for the data gathering. \nIn this part, there are methods to clean the text, including: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9406600639013956,
        0.8711389911102422,
        0.9968054976950226,
        0.9495427977918525,
        0.882428654985064,
        0.8671652325214163
      ],
      "excerpt": "6. Replace tokens with all letters in capitals by their lower case and add xxup ahead: GOOD JOB -> xxup good xxup job \n7. Replace tokens with the first letter in capital by their lower caser and add xxmaj ahead: We -> xxmaj we \nThe method ulangel.data.text_processor.text_proc calls all methods above to clean the text, tokenize it, and add xbos at the beginning and xfld at the end of the text. Here is a notebook to show standard text processing steps, including text cleaning and text numeralization: text_processor \nThere are three types of data objects in our training and validation systems. The default input data are numpy.ndarray objects. \n* Dataset: Divide(or/and shuffle) input data into batches. Each dataset item is a tuple of x and its corresponding y. \n* Dataloader: Here we use the pytorch dataloader, to get dataset item in the way defined by the sampler. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9812069817265002
      ],
      "excerpt": "ulangel.data.data_packer.LanguageModelDataset: For a language model, the input is a bptt-length text and the output is also a text as long as the input with just one word shifted. For a text [w0, w1, w2, w3, w4, ...] (wi is the corresponding integer of a word, in the dictionary of your own text corpus.) If bptt = 4, the input i0 is [w0, w1, w2, w3], then the output o0 will be [w1, w2, w3, w4]. The input and the output are generated from the same text, with the help of the class LanguageModelDataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8561094342118424
      ],
      "excerpt": "ulangel.data.data_packer.TextClassificationDataset: For a text classifier, its dataset is a little bit different. The input is still the same, but the output is an integer representing the corresponding class label. In this case, we use TextClassificationDataset which inherits the pytorch dataset torch.utils.data.Dataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9878623050534885
      ],
      "excerpt": "In this library, we use the pytorch dataloader, but with our own sampler. For the language model, batches are generated by concatenating all texts so they all have the same length. We can use directly the dataloader to pack data. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9392221108780198,
        0.8895026135357433
      ],
      "excerpt": "However, for the text classification, we can not concatenate texts together, because each text has its own class. It doesn't make sense to mix texts to form equilong texts in the batch. In order to train the neural network in an efficient way and at the same time to keep the randomness, we have two different samplers for the training and the validation data. Additionally, for texts in each batch, they should have the same length, so we use a collate function to pad those short texts. \nFor the Classification data, we use the dataloader in this way: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9797151020546134,
        0.9602638194445815
      ],
      "excerpt": "Sampler is an index generator. It returns a list of indexes, which corresponding item is sorted by the attribute key. In this library, TrainingSampler and ValidationSampler inherit the pytorch sampler torch.utils.data.Sampler. \nulangel.data.data_packer.TrainingSampler: TrainingSampler is a sampler for the training data. It sorts the data in the way defined by the given key, the longest at the first, the shortest at the end, random in the middle. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9784692943223013,
        0.9784836638732021
      ],
      "excerpt": "In this exemple, the data source is the x of the training dataset (texts), the key is the length of each text. \nulangel.data.data_packer.ValidationSampler: It sorts the data in the way defined by the given key, in an ascending or a descending way. It is different from the TrainingSampler, there is no randomness, the validation texts will be sorted from the longest to the shortest. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9849809212214903,
        0.9402979312874566,
        0.8979411005071259
      ],
      "excerpt": "In this exemple, the data source is the x of the validation dataset (texts), the key is the length of each text. \nCollate function can be used to manipulate your input data. In this library, our collate function: pad_collate is to pad all texts with padding index pad_idx to have the same length for one whole batch. This pad_collate function is inbuild, we just need to import, so that we can use it in the dataloader. It exists for two different input modes. \n* ulangel.data.data_packer.pad_collate_textonly: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9345529694519963
      ],
      "excerpt": "ulangel.data.data_packer.pad_collate_textplus: is the textplus version. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.855701080368545
      ],
      "excerpt": "ulangel.data.data_packer.DataBunch: Databunch packs your training dataloader and validation dataloader together into a databunch object, so that your can give it to your learner (which will be explained later in README) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.926986471207506,
        0.9907483922029888,
        0.8957732502281938
      ],
      "excerpt": "In this part, there are two main blocks to build a neural network: dropouts and some special neural network structures for our transfer learning. \nThe pytorch dropout are dropouts to zero out some activations with probability p. In the article of Stephen Merity et al. \"Regularizing and Optimizing LSTM Language Models\" https://arxiv.org/pdf/1708.02182.pdf they propose to apply dropouts not only on activations but also on connections. Using pytorch dropouts is not enough. Therefore, we create three different dropout classes: \nulangel.rnn.dropouts.ActivationDropout: as its name, this is a dropout to zero out activations in the layer of the neural network. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8282131267023303,
        0.8506956154098951,
        0.9457800841734173,
        0.9816498276870779
      ],
      "excerpt": "ulangel.rnn.dropouts.EmbeddingDropout: this is a dropout class to zero out embedding activations. \nThese three dropout classes will be used in the AWD_LSTM to build the LSTM for the language model training. Of course, you can also import them to build your own neural network. \nIn this part, we have some structures to build a language model and a text classifier. \nulangel.rnn.nn_block.AWD_LSTM: is the class to build the language model except the decoder layer. This is the commom part for the language model and the text classifier. It is a LSTM neural network inheriting torch.nn.Module proposed by Stephen Merity et al. in the article \"Regularizing and Optimizing LSTM Language Models\" https://arxiv.org/pdf/1708.02182.pdf. Because we use the pretrained language model wikitext-103 from this article, to finetune our own language model on our corpus, we need to keep the same values for some hyperparameters as wikitext-103: embedding_size = 400, number_of_hidden_activation = 1150. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9710598817478955
      ],
      "excerpt": "ulangel.rnn.nn_block.LinearDecoder: This is a decoder inheriting torch.nn.Module, the inverse of an encoder, to transfer the last hidden layer (embedding vector) into its corresponding integer representation of the work, so that we can find comprehensive words for human. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9657436946142823
      ],
      "excerpt": "ulangel.rnn.nn_block.SequentialRNN: This class inherits the pytorch class torch.nn.Sequential, to connect different neural networks, and allows to reset all parameters of substructures with a reset methode (ex: AWD_LSTM) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9111683690758747
      ],
      "excerpt": "<bound method Module.modules of SequentialRNN( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9700372935392262,
        0.9800715715133242
      ],
      "excerpt": "For the classification data, there are two types of input data: text only mode and text plus mode as mentioned in the Background. Therefore all structures concerning classifier are made in two versions for these two different modes of input data. \nulangel.rnn.nn_block.TextOnlySentenceEncoder: it is a class similar to ulangel.rnn.nn_block.AWD_LSTM, but the difference is when the input text length exceeds the value of bptt (we define to train the language model), it divides the text into serval bptt-length sequences at the input and concatenates the results back to one text at the output. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9797853520836721
      ],
      "excerpt": "ulangel.rnn.nn_block.TextPlusSentenceEncoder: is the text plus version of the SentenceEncoder \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9619156527696516
      ],
      "excerpt": "ulangel.rnn.nn_block.TextOnlyPoolingLinearClassifier: different from the language model, we don't need the decoder to read the output. We want to classify the input text. So at the output of the ulangel.rnn.nn_block.AWD_LSTM, we do some pooling to pick the last sequence of the LSTM's output, the max pooling of the LSTM's output, the average pooling of the LSTM's output. We concatenate these three sequences, as input of a linear full connected neural net classifier. The last layer's number of activations should be the same as the number of classes in your classification problem. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9698140183772211
      ],
      "excerpt": "ulangel.rnn.nn_block.TextPlusPoolingLinearClassifier: is the text plus version. The difference from the text only mode is that text plus mode pooling linear classifier has another group of layers. This supplemental group of layers takes nonverbal features into account. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9384830462173054
      ],
      "excerpt": "To build the complete classifier, we use the ulangel.rnn.nn_block.SequentialRNN to connect these two classes, here is an exemple for the text only mode: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9880443029757132
      ],
      "excerpt": "In this part, there are some tools for the training of the neural network. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9195204853378945
      ],
      "excerpt": "ulangel.utils.callbacks.TextPlusCudaCallback: is the textplus version to put the model and the variables on cuda. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9682305573390723
      ],
      "excerpt": "ulangel.utils.callbacks.LR_Find: giving the minimum and the maximum of learning rate and the maximum number of iteration, change linearlly the learning rate (from the minimum value to the maximum value) at every batch. Combine with the Recorder, we can see the evaluation of loss so that we can find an appropriate learning rate for the training. Warning: if there is LR_Find in the callback list, the model is running to go through all learning rates, but not to train the model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8293906790412641
      ],
      "excerpt": "ulangel.utils.callbacks.ParamScheduler: allowing to schedule any hyperparameter during the training, such as learning rate, momentum, weight decay, etc. It takes the hyperparameter's name and its schedule function sched_func. Here we use a combined schedule function combine_scheds, combing two different parts of a cosine function, to have a learning rate low at the beginning and at the end, high in the middle. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9154136418994285
      ],
      "excerpt": "  #: pcts means the percentages taken by the following functions in scheds. In the exemple below means the sched combines the first 0.3 of sched_cos1 and the last 0.7 of sched_cos2. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9487126247709594
      ],
      "excerpt": "For the training process, it's up to the user to choose callbacks to make a callback list. Here it's an exemple: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9832685293804817,
        0.9707776776875989,
        0.9424221554520626,
        0.8795392008274237,
        0.8435593793092078,
        0.866803692121838
      ],
      "excerpt": "Stats contains all classes and functions to compute statistics of the model's performance. There are two classes and some methods. \nmetrics: A metric function takes the outputs of your model, and the target values as inputs, and you can define your own way to evaluate your model's performance by writing your own computation in the function. Functions ulangel.utils.stats.accuracy_flat (calculate the accuracy for the language model) and ulangel.utils.stats.accuracy (calculate the accuracy for the classifier) are two inbuild metrics that we provide. \nWarning: ulangel.utils.stats.cross_entropy_flat is not a metric. It is a loss function, but it is similar to the accuracy_flat metric, so we put them at the same place. \nulangel.utils.stats.AvgStats: calculate loss and statistics defined by input metrics. This class puts the loss value and other performance statistics defined by metrics together into a list. It also has methods to update and print all these performance statistics when called. \nulangel.utils.stats.AvgStatsCallback: Actually the class AvgStatsCallback is also a callback, it uses AvgStats to calculate all performance statistics after every batch, and print these statistics after every epoch. \nWe can add AvgStatsCallback into the callback list, so that we can know the neural network performs after every epoch. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9632974933511432
      ],
      "excerpt": "  #: for a language model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9394449182630016
      ],
      "excerpt": "#: for a classifier \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9879904443700238,
        0.8011321296429157,
        0.8977365231912703,
        0.86943586502737
      ],
      "excerpt": "optimizers: ulangel.utils.optimizer.Optimizer is a class that decides the way to update all parameters of the model by steppers. ulangel.utils.optimizer.StatefulOptimizer is an optimizer with state. It inherits the class Optimizer and adds an attribute state in order to track the history of updates. As we know, when we use an optimizer with momentum, we need to know the last update value to calculate the current one. In this case, we use StatefulOptimizer in this library. \nstepper: functions defining how to update the parameters or the gradient of the parameters. It depends on the current values. In the library we provide several steppers: ulangel.utils.optimizer.sgd_step (stochastic gradient descent stepper), ulangel.utils.optimizer.weight_decay (weight decay stepper), ulangel.utils.optimizer.adam_step (adam stepper). You can also program your own stepper. \nstateupdater: define how to initialize and update state (for exemple, how to update momentum). In the library we provide some inbuild stateupdaters, all of them inherit the class ulangel.utils.optimizer.StateUpdater: ulangel.utils.optimizer.AverageGrad(momentum created by averaging the gradient), ulangel.utils.optimizer.AverageSqrGrad(momentum created by averaging the square of the gradient), ulangel.utils.optimizer.StepCount(step increment). \nIn ulangel, we provide two inbuild optimizer: ulangel.utils.optimizer.sgd_opt (stochastic gradient descent optimizer) and ulangel.utils.optimizer.adam_opt (adam optimizer). Optimizer is an input of the object of the class leaner. We will show you how to use optimizer in the learner part. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9206752908801052,
        0.9260858768807122
      ],
      "excerpt": "This part includes the class Learner and some methods to freeze or unfreeze layers in order to train just a part of or the whole neural network. \nThe class ulangel.utils.learner.Learner: It is a class that takes the RNN model, data for training, the loss function, the optimizer, the learning rate and callbacks that you need. The method Learner.fit(epochs=number of epochs that you want to train) executes all processes in order to train the model. Here is an exemple to build the langage model learner: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877,
        0.8979411005071259
      ],
      "excerpt": "      model=language_model, \n      data=language_model_data, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9885400826306675,
        0.9702039486258138
      ],
      "excerpt": "ulangel.utils.learner.freeze_all is a method that sets requires_grad of all parameters of the neural network as Fasle. \nulangel.utils.learner.unfreeze_all is a method that sets requires_grad of all parameters of the neural network as True. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9166606507246045,
        0.8054228178615603
      ],
      "excerpt": "Here is the notebook to show how to use these methods: freeze_to.ipynb \nRegularizing and Optimizing LSTM Language Models by Stephen Merity et al \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9187038069639567
      ],
      "excerpt": "Universal Language Model Fine-tuning for Text Classification by Jeremy Howard et al \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Python NLP text classification library based on LSTM neural network ULMFiT",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/uchange/ulangel/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Mon, 27 Dec 2021 15:46:43 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/uchange/ulangel/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "uchange/ulangel",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/uchange/ulangel/master/doc/freeze_to.ipynb",
      "https://raw.githubusercontent.com/uchange/ulangel/master/doc/word_embedding_text_processor.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "  ```\n  pip install ulangel\n  ```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8794004579650581
      ],
      "excerpt": "* Dataloader: Here we use the pytorch dataloader, to get dataset item in the way defined by the sampler. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9032837796845528
      ],
      "excerpt": "  language_model.modules \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8359929717815873
      ],
      "excerpt": "  Github: https://github.com/salesforce/awd-lstm-lm \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8042287643504085
      ],
      "excerpt": "* ulangel.data: Preparation of the text classification data, including the text preparation (such as text cleaning) and the data formating (such as creating dataset, creating databunch, padding all texts to have the same length in the same batch, etc.). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8358873607449412
      ],
      "excerpt": "ulangel.data.data_packer.LanguageModelDataset: For a language model, the input is a bptt-length text and the output is also a text as long as the input with just one word shifted. For a text [w0, w1, w2, w3, w4, ...] (wi is the corresponding integer of a word, in the dictionary of your own text corpus.) If bptt = 4, the input i0 is [w0, w1, w2, w3], then the output o0 will be [w1, w2, w3, w4]. The input and the output are generated from the same text, with the help of the class LanguageModelDataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9457175861910134,
        0.9079185768576028,
        0.9023654457511702,
        0.845067557475782,
        0.883570190194724
      ],
      "excerpt": "  import numpy as np \n  from ulangel.data.data_packer import LanguageModelDataset \n  trn_lm = np.load(your_path/'your_file.npy', allow_pickle=True) \n  trn_lm_ds = LanguageModelDataset(data=trn_lm, bs=2, bptt=4, shuffle=False) \n  #: print an item of dataset: (x, y) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8361349576714179
      ],
      "excerpt": "(tensor([1.1000e+01, 5.0000e+00, 2.0000e+00, 1.0000e+01]), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9457175861910134,
        0.9079185768576028,
        0.9023654457511702,
        0.9023654457511702
      ],
      "excerpt": "  import numpy as np \n  from ulangel.data.data_packer import TextClassificationDataset \n  trn_ids = np.load(your_path/'your_text_file.npy', allow_pickle=True) \n  trn_label = np.load(your_path/'your_label_file.npy', allow_pickle=True) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.883570190194724
      ],
      "excerpt": "  #: print an item of dataset: (x, y) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8878193409743138
      ],
      "excerpt": "  from torch.utils.data import DataLoader \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8534451657593075
      ],
      "excerpt": "  #: print an item of dataloader: a batch of dataset \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9079185768576028
      ],
      "excerpt": "  from ulangel.data.data_packer import pad_collate_textonly \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9079185768576028,
        0.8083205525110364
      ],
      "excerpt": "  from ulangel.data.data_packer import TrainingSampler \n  trn_sampler = TrainingSampler(data_source=trn_clas_ds.x, key=lambda t: len(trn_clas_ds.x[t]), bs=2) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9079185768576028
      ],
      "excerpt": "  from ulangel.data.data_packer import ValidationSampler \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9079185768576028
      ],
      "excerpt": "  from ulangel.data.data_packer import pad_collate_textonly \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9079185768576028
      ],
      "excerpt": "  from ulangel.data.data_packer import pad_collate_textplus \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9079185768576028
      ],
      "excerpt": "  from ulangel.data.data_packer import DataBunch \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "  from ulangel.rnn.nn_block import AWD_LSTM \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "  from ulangel.rnn.nn_block import LinearDecoder \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "      bias=True \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "  from ulangel.rnn.nn_block import SequentialRNN \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8085374849674256
      ],
      "excerpt": "ulangel.rnn.nn_block.TextOnlySentenceEncoder: it is a class similar to ulangel.rnn.nn_block.AWD_LSTM, but the difference is when the input text length exceeds the value of bptt (we define to train the language model), it divides the text into serval bptt-length sequences at the input and concatenates the results back to one text at the output. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "  from ulangel.rnn.nn_block import TextOnlySentenceEncoder \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "  from ulangel.rnn.nn_block import TextPlusSentenceEncoder \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "  from ulangel.rnn.nn_block import TextOnlyPoolingLinearClassifier \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8382423390025753
      ],
      "excerpt": "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984,
        0.8685545752676547
      ],
      "excerpt": "      (3): ReLU(inplace=True) \n      (4): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "      (7): ReLU(inplace=True) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "  from ulangel.rnn.nn_block import TextPlusPoolingLinearClassifier \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8382423390025753
      ],
      "excerpt": "          (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984,
        0.8685545752676547
      ],
      "excerpt": "          (3): ReLU(inplace=True) \n          (4): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "          (7): ReLU(inplace=True) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8382423390025753
      ],
      "excerpt": "          (0): BatchNorm1d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "          (3): ReLU(inplace=True) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8382423390025753
      ],
      "excerpt": "        (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984,
        0.8685545752676547
      ],
      "excerpt": "        (3): ReLU(inplace=True) \n        (4): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "        (7): ReLU(inplace=True) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8241781308320554
      ],
      "excerpt": "ulangel.utils.callbacks.TrainEvalCallback: setting if the model is in the training mode or in the validation mode. During the training mode, update the progressing and the number of iteration. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.804148421347362
      ],
      "excerpt": "ulangel.utils.callbacks.Recorder: recording the loss value and the learning rate of every batch, plot the variation of these two values if the methode (recorder.plot_lr() / recorder.plot_loss() / recorder.plot()) is called. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9416522774131079
      ],
      "excerpt": "  from ulangel.utils.callbacks import combine_scheds, ParamScheduler, sched_cos \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9416522774131079
      ],
      "excerpt": "  from ulangel.utils.callbacks import TrainEvalCallback, TextOnlyCudaCallback, Recorder, RNNTrainer \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8695959269526191
      ],
      "excerpt": "  from ulangel.utils.stats import AvgStatsCallback, accuracy, accuracy_flat \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9416522774131079
      ],
      "excerpt": "  from ulangel.utils.optimizer import StatefulOptimizer, StateUpdater, StepCount \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9416522774131079
      ],
      "excerpt": "  from ulangel.utils.learner import Learner \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8237576424847335
      ],
      "excerpt": "  #: load the pretrained model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8442931174286615
      ],
      "excerpt": "  train: [loss value 1 for training set, tensor(metric value 1 for training set, device='cuda:0')] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8442931174286615
      ],
      "excerpt": "  train: [loss value 2 for training set, tensor(metric value 1 for training set, device='cuda:0')] \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/uchange/ulangel/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Apache License 2.0",
      "url": "https://api.github.com/licenses/apache-2.0"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'                                 Apache License\\n                           Version 2.0, January 2004\\n                        http://www.apache.org/licenses/\\n\\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\\n\\n   1. Definitions.\\n\\n      \"License\" shall mean the terms and conditions for use, reproduction,\\n      and distribution as defined by Sections 1 through 9 of this document.\\n\\n      \"Licensor\" shall mean the copyright owner or entity authorized by\\n      the copyright owner that is granting the License.\\n\\n      \"Legal Entity\" shall mean the union of the acting entity and all\\n      other entities that control, are controlled by, or are under common\\n      control with that entity. For the purposes of this definition,\\n      \"control\" means (i) the power, direct or indirect, to cause the\\n      direction or management of such entity, whether by contract or\\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\\n      outstanding shares, or (iii) beneficial ownership of such entity.\\n\\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\\n      exercising permissions granted by this License.\\n\\n      \"Source\" form shall mean the preferred form for making modifications,\\n      including but not limited to software source code, documentation\\n      source, and configuration files.\\n\\n      \"Object\" form shall mean any form resulting from mechanical\\n      transformation or translation of a Source form, including but\\n      not limited to compiled object code, generated documentation,\\n      and conversions to other media types.\\n\\n      \"Work\" shall mean the work of authorship, whether in Source or\\n      Object form, made available under the License, as indicated by a\\n      copyright notice that is included in or attached to the work\\n      (an example is provided in the Appendix below).\\n\\n      \"Derivative Works\" shall mean any work, whether in Source or Object\\n      form, that is based on (or derived from) the Work and for which the\\n      editorial revisions, annotations, elaborations, or other modifications\\n      represent, as a whole, an original work of authorship. For the purposes\\n      of this License, Derivative Works shall not include works that remain\\n      separable from, or merely link (or bind by name) to the interfaces of,\\n      the Work and Derivative Works thereof.\\n\\n      \"Contribution\" shall mean any work of authorship, including\\n      the original version of the Work and any modifications or additions\\n      to that Work or Derivative Works thereof, that is intentionally\\n      submitted to Licensor for inclusion in the Work by the copyright owner\\n      or by an individual or Legal Entity authorized to submit on behalf of\\n      the copyright owner. For the purposes of this definition, \"submitted\"\\n      means any form of electronic, verbal, or written communication sent\\n      to the Licensor or its representatives, including but not limited to\\n      communication on electronic mailing lists, source code control systems,\\n      and issue tracking systems that are managed by, or on behalf of, the\\n      Licensor for the purpose of discussing and improving the Work, but\\n      excluding communication that is conspicuously marked or otherwise\\n      designated in writing by the copyright owner as \"Not a Contribution.\"\\n\\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\\n      on behalf of whom a Contribution has been received by Licensor and\\n      subsequently incorporated within the Work.\\n\\n   2. Grant of Copyright License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      copyright license to reproduce, prepare Derivative Works of,\\n      publicly display, publicly perform, sublicense, and distribute the\\n      Work and such Derivative Works in Source or Object form.\\n\\n   3. Grant of Patent License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      (except as stated in this section) patent license to make, have made,\\n      use, offer to sell, sell, import, and otherwise transfer the Work,\\n      where such license applies only to those patent claims licensable\\n      by such Contributor that are necessarily infringed by their\\n      Contribution(s) alone or by combination of their Contribution(s)\\n      with the Work to which such Contribution(s) was submitted. If You\\n      institute patent litigation against any entity (including a\\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\\n      or a Contribution incorporated within the Work constitutes direct\\n      or contributory patent infringement, then any patent licenses\\n      granted to You under this License for that Work shall terminate\\n      as of the date such litigation is filed.\\n\\n   4. Redistribution. You may reproduce and distribute copies of the\\n      Work or Derivative Works thereof in any medium, with or without\\n      modifications, and in Source or Object form, provided that You\\n      meet the following conditions:\\n\\n      (a) You must give any other recipients of the Work or\\n          Derivative Works a copy of this License; and\\n\\n      (b) You must cause any modified files to carry prominent notices\\n          stating that You changed the files; and\\n\\n      (c) You must retain, in the Source form of any Derivative Works\\n          that You distribute, all copyright, patent, trademark, and\\n          attribution notices from the Source form of the Work,\\n          excluding those notices that do not pertain to any part of\\n          the Derivative Works; and\\n\\n      (d) If the Work includes a \"NOTICE\" text file as part of its\\n          distribution, then any Derivative Works that You distribute must\\n          include a readable copy of the attribution notices contained\\n          within such NOTICE file, excluding those notices that do not\\n          pertain to any part of the Derivative Works, in at least one\\n          of the following places: within a NOTICE text file distributed\\n          as part of the Derivative Works; within the Source form or\\n          documentation, if provided along with the Derivative Works; or,\\n          within a display generated by the Derivative Works, if and\\n          wherever such third-party notices normally appear. The contents\\n          of the NOTICE file are for informational purposes only and\\n          do not modify the License. You may add Your own attribution\\n          notices within Derivative Works that You distribute, alongside\\n          or as an addendum to the NOTICE text from the Work, provided\\n          that such additional attribution notices cannot be construed\\n          as modifying the License.\\n\\n      You may add Your own copyright statement to Your modifications and\\n      may provide additional or different license terms and conditions\\n      for use, reproduction, or distribution of Your modifications, or\\n      for any such Derivative Works as a whole, provided Your use,\\n      reproduction, and distribution of the Work otherwise complies with\\n      the conditions stated in this License.\\n\\n   5. Submission of Contributions. Unless You explicitly state otherwise,\\n      any Contribution intentionally submitted for inclusion in the Work\\n      by You to the Licensor shall be under the terms and conditions of\\n      this License, without any additional terms or conditions.\\n      Notwithstanding the above, nothing herein shall supersede or modify\\n      the terms of any separate license agreement you may have executed\\n      with Licensor regarding such Contributions.\\n\\n   6. Trademarks. This License does not grant permission to use the trade\\n      names, trademarks, service marks, or product names of the Licensor,\\n      except as required for reasonable and customary use in describing the\\n      origin of the Work and reproducing the content of the NOTICE file.\\n\\n   7. Disclaimer of Warranty. Unless required by applicable law or\\n      agreed to in writing, Licensor provides the Work (and each\\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\\n      implied, including, without limitation, any warranties or conditions\\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\\n      PARTICULAR PURPOSE. You are solely responsible for determining the\\n      appropriateness of using or redistributing the Work and assume any\\n      risks associated with Your exercise of permissions under this License.\\n\\n   8. Limitation of Liability. In no event and under no legal theory,\\n      whether in tort (including negligence), contract, or otherwise,\\n      unless required by applicable law (such as deliberate and grossly\\n      negligent acts) or agreed to in writing, shall any Contributor be\\n      liable to You for damages, including any direct, indirect, special,\\n      incidental, or consequential damages of any character arising as a\\n      result of this License or out of the use or inability to use the\\n      Work (including but not limited to damages for loss of goodwill,\\n      work stoppage, computer failure or malfunction, or any and all\\n      other commercial damages or losses), even if such Contributor\\n      has been advised of the possibility of such damages.\\n\\n   9. Accepting Warranty or Additional Liability. While redistributing\\n      the Work or Derivative Works thereof, You may choose to offer,\\n      and charge a fee for, acceptance of support, warranty, indemnity,\\n      or other liability obligations and/or rights consistent with this\\n      License. However, in accepting such obligations, You may act only\\n      on Your own behalf and on Your sole responsibility, not on behalf\\n      of any other Contributor, and only if You agree to indemnify,\\n      defend, and hold each Contributor harmless for any liability\\n      incurred by, or claims asserted against, such Contributor by reason\\n      of your accepting any such warranty or additional liability.\\n\\n   END OF TERMS AND CONDITIONS\\n\\n   APPENDIX: How to apply the Apache License to your work.\\n\\n      To apply the Apache License to your work, attach the following\\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\\n      replaced with your own identifying information. (Don\\'t include\\n      the brackets!)  The text should be enclosed in the appropriate\\n      comment syntax for the file format. We also recommend that a\\n      file or class name and description of purpose be included on the\\n      same \"printed page\" as the copyright notice for easier\\n      identification within third-party archives.\\n\\n   Copyright [yyyy] [name of copyright owner]\\n\\n   Licensed under the Apache License, Version 2.0 (the \"License\");\\n   you may not use this file except in compliance with the License.\\n   You may obtain a copy of the License at\\n\\n       http://www.apache.org/licenses/LICENSE-2.0\\n\\n   Unless required by applicable law or agreed to in writing, software\\n   distributed under the License is distributed on an \"AS IS\" BASIS,\\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n   See the License for the specific language governing permissions and\\n   limitations under the License.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "ulangel",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "ulangel",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "uchange",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/uchange/ulangel/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "debnet",
        "body": "",
        "dateCreated": "2019-12-19T14:30:53Z",
        "datePublished": "2019-12-19T15:12:43Z",
        "html_url": "https://github.com/uchange/ulangel/releases/tag/0.0.1",
        "name": "v0.0.1",
        "tag_name": "0.0.1",
        "tarball_url": "https://api.github.com/repos/uchange/ulangel/tarball/0.0.1",
        "url": "https://api.github.com/repos/uchange/ulangel/releases/22359181",
        "zipball_url": "https://api.github.com/repos/uchange/ulangel/zipball/0.0.1"
      }
    ],
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Python 3.6\ntorch 1.3.1\ntorchvision 0.4.2\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Mon, 27 Dec 2021 15:46:43 GMT"
    },
    "technique": "GitHub API"
  }
}