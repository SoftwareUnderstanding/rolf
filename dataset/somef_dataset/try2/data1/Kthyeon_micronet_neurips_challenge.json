{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This work was supported by Institute for Information & Communications Technology Promotion (IITP) grant funded by the Korea government (MSIT) [No.2018-0-00278,Development of Big Data Edge Analytics SW Technology for Load Balancing and Active Timely Response].\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1810.09102",
      "https://arxiv.org/abs/1905.00397",
      "https://arxiv.org/abs/1709.01507",
      "https://arxiv.org/abs/1801.04381",
      "https://arxiv.org/abs/1905.02244",
      "https://arxiv.org/abs/1905.02244",
      "https://arxiv.org/abs/1905.02244",
      "https://arxiv.org/abs/1502.03167",
      "https://arxiv.org/abs/1810.09102",
      "https://arxiv.org/abs/1608.03983",
      "https://arxiv.org/abs/1905.04899",
      "https://arxiv.org/abs/1803.03635"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Kthyeon/KAIST-AI-NeurIPS2019-MicroNet-2nd-place-solution",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To contact us:\n\nTae hyeon Kim, potter32@kaist.ac.kr\n\n",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-10-02T07:38:14Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-26T08:15:09Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9159540047472625,
        0.9824892806183999
      ],
      "excerpt": "We proceed this work through PMLR Volume123(Proceedings of the NeurIPS 2019 Competition and Demonstration Track, PMLR 123:13-26, 2020). The paper title is Efficient Model for Image Classification With Regularization Tricks. Our main contributions are divided into four parts. \nFrom previous works, dynamic isometric property increases the network performance, but actually the gain from previous orthonormal regularizer is minuscule. We found that attaching the orthonormal regularizer only on 1x1 convolution increases remarkable amounts in the performance, and we argue that this is very related to feature map space. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9776612574362064,
        0.9263329358443493,
        0.9505078154220507,
        0.9442495247328017,
        0.8921057193714176,
        0.9915805962931359,
        0.9672471178425494,
        0.8668867441119023,
        0.9644413377277323,
        0.8776388060538397,
        0.9846420432223549,
        0.9483886347011952
      ],
      "excerpt": "Many networks are initialized with Kaiming initialization or Xavier initialization, but the weights of converged networks are very far from the intial weight distribution. From the empirical results, we found that with our method, trained weight distribution are always certain. Therefore, we initialized  our networks with this obtained weight distribution. \nThe most well-known loss function in convolutional neural networks is Cross-Entropy loss. In the recent, label smooth function can not only enhance the robustness, but also increase the performance so that many replace the loss function as label smooth.  However, this method doesn\u2019t deal with the class-correlation, so sometimes the network is not well-trained when increasing the epsilon. In our loss function called weighted label smooth loss function, this distributes the epsilon with considering class-correlation. The key idea of scaling the class-relativity is to score the weight as the cosine similarity with the class representative feature vector from fully-connected layers\u2019s i-th row vector. \nRecently, iterative pruning with lottery ticket hypothesis is one of the state-of-art pruning method. However, here, we found that keeping pretrained weight, not using random-init, is a bit beneficial for network, and increasing the pruning rate with linear scheduling can preserve the performance. For instance, if you want to prune 50% of the pretrained network weights, then train the pruned network 4 times with pruned rate as 12.5%, 25.0%, 37.5%, and 50%. Then, you could train the 50% pruned network without accuracy loss. \nWe use fast autoaugmentation of CIFAR100, which is made from the networks, wide-resnet and densenet. We can get the CIFAR100 Fast Autoaugmentation strategies from the authors of above paper. \nWe use several blocks, layers, and activation that are known to be efficient in our MicroNet network. These are listed as follow. \nWe attach the Squeeze-and-Excitation (SE) block at the end of the each block in network. Normalizing the input only with batch normalization is a bit hard, but with SE block, it plays a role as scaling factor of covariate shift. \nInverted residual block was first introduced in MobileNetV2 model. This block becomes basic structure of following networks such as MnasNet, EfficientNet, MobileNetV3. Our network is based on MobileNetV3 architecture, so our model, of course, follows inverted residual block structure. \nHard swish (HSwish) activation was introduced in Searching for MobileNetV3 from Google. Hswish function is defined as follow. \nWe incorporated this activation function in our model for better accuracy. \nTo solve the internal covariate shift, we add the batch normalization between each convolution and activation function, and empirically, we found using batch normalization generalizes better. \nWe use the orthonormal regularization (Spectral Restricted Isometry Property Regularization from above work) on pointwise convolution layer, not on depthwise convolution layer. To orthogonalize the weight matrix, one of the efficient way is to regularize the singular value of the matrix. However, this cause a lot of computation costs so that we use an approximated method called SRIP similar with RIP method. In ours, we add this regularizer with $10^{-2}$ coefficient. \nWe use the cosine annealing function as the learning rate scheduler. Converging to local optima is well-known issue in training deep neural network. We found that the periodic function can solve this issue with high probability, and from the empirical results, the output network generalizes better than others. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9913798815953574,
        0.9978156826240394,
        0.9884122072998269,
        0.9815603406552453
      ],
      "excerpt": "For better generalization in data augementation policy, we apply CutMix regularization which outperfroms other state-of-the-art methods. This method enables that the network have the effects from both a regional dropout stategy and a mixup stategy. \nTo prevent the overfitting of deep learning model, we need to use regularization method. One of kind regularization method is the weight decay which is to penalize weights proportionally to their magnitude to prevent overfitting during trainnig. However, when model is quite small like compressed model, weight decay aggravate the training performance. Therfore, we use a little bit small weight decay and for version1 network, we did not apply weight decay on the parameters of batch normalization. \nGradient descent is very important to train deep neural network. However, conventional GD is easily stuck in local optimum. Therefore, there are many gradient descent optimization algorithms to address it. Recently, SGD is commonly used and enough to train deep learning model with momentum. The momentum helps to converge better by preventing stuck to local optima when gradient descent. Therefore, we use SGD optimizer with momentum. \n[Han et al., 2015] suggested deep learning pruning method based on magnitude very well. But, this conventional pruning method has very critical weakness which is the too many re-training process. To address it, [Frankle et al., 2019] defines the lottery ticket hypothesis which is that A randomly-initialized, dense-neural networks contain subnetworks called winning tickets. Here, winning ticket can reach the comparable test acuuracy in at most same iteration of original netwrok through re-initialization right before re-training process. As lottery ticket is a very recent powerful pruning method, we prune the network almost same with this method except for random initialization. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9839455395780655
      ],
      "excerpt": "We made two types of micronet. They are slightly different and the overview of network is like below. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9160320998991637,
        0.8410624271469357
      ],
      "excerpt": "Our network blocks are divided into two, stem block and mobile block. \nWhen downsampling the layer-wise input, we use the depthwise kernel size as 2 and attach the 1x1 convolution block at the shortcut. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8620893746226058
      ],
      "excerpt": "Data: CIFAR100 with fast autoaugmentation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9925538504178024,
        0.9173673456456221
      ],
      "excerpt": "We refer to \u2018thop\u2019 library source from here to count the add operations and multiplication operations. However, to keep the rules of (Neurips 19\u2019s)  micronet challenge, we change many parts of the counting functions. In code, addition is counted 3 and multiplication is counted 1 for the relu6 operations. This is because ReLU6 is only used in hard swish function so that this counting policy is actually for hard swish function when counting the operations of our network. \nDetails about score method, we deal with it in the jupyter notebooke file Score_MicroNet.ipynb. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "NeurIPSCD2019, MicroNet Challenge hosted by Google, Deepmind Researcher, \"Efficient Model for Image Classification With Regularization Tricks\".",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Kthyeon/micronet_neurips_challenge/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 6,
      "date": "Tue, 28 Dec 2021 11:33:40 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Kthyeon/KAIST-AI-NeurIPS2019-MicroNet-2nd-place-solution/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Kthyeon/KAIST-AI-NeurIPS2019-MicroNet-2nd-place-solution",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Kthyeon/micronet_neurips_challenge/master/%28revised%29Score_MicroNet.ipynb",
      "https://raw.githubusercontent.com/Kthyeon/micronet_neurips_challenge/master/Score_MicroNet.ipynb"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.9124837196669503,
        0.9042534665813847,
        0.9042534665813847
      ],
      "excerpt": "<img src=\"./images/Stem.png\" width=\"400\"> \n<img src=\"./images/Block1.png\" width=\"750\"> \n<img src=\"./images/Block2.png\" width=\"750\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "Nesterov: True \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8494747444303936,
        0.823687576256224
      ],
      "excerpt": "Batch weight decay: True \nBatch size: 128 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9516212977561741
      ],
      "excerpt": "python3 micronet_main.py --model=micronet --dataset=CIFAR100 --lr=0.1 --batch_size=128 --lr_type=cos --n_epoch=600 --input_regularize=cutmix --label_regularization=crossentropy --name=micronet_reproduce --ortho_lr=0.7 --model_ver=ver1 --progress_name=reproduce_progress --batch_wd=False \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9445286596726202
      ],
      "excerpt": "python3 micronet_main.py --model=micronet --dataset=CIFAR100 --lr=0.1 --batch_size=128 --lr_type=cos --n_epoch=600 --input_regularize=cutmix --label_regularization=crossentropy --name=micronet_reproduce --ortho_lr=0.7 --model_ver=ver2 --progress_name=reproduce_progress \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9669641556381012,
        0.8429357801000046,
        0.9669641556381012,
        0.9629643349818272,
        0.8429357801000046,
        0.9629643349818272
      ],
      "excerpt": "python3 micronet_main.py --model=micronet_prune --dataset=CIFAR100 --lr=0.1 --batch_size=128 --lr_type=cos --n_epoch=600 --input_regularize=cutmix --label_regularization=crossentropy --name=micronet_reproduce_pr1 --ortho_lr=0.7 --max_prune_rate=45. --load_name=micronet_reproduce --model_ver=ver1 --batch_wd=False \nAnd after training above, run prune-training once again: \npython3 micronet_main.py --model=micronet_prune --dataset=CIFAR100 --lr=0.1 --batch_size=128 --lr_type=cos --n_epoch=600 --input_regularize=cutmix --label_regularization=crossentropy --name=micronet_reproduce_pr2 --ortho_lr=0.7 --min_prune_rate=45. --max_prune_rate=65. --load_name=micronet_reproduce_pr1 --model_ver=ver1 --batch_wd=False \npython3 micronet_main.py --model=micronet_prune --dataset=CIFAR100 --lr=0.1 --batch_size=128 --lr_type=cos --n_epoch=600 --input_regularize=cutmix --label_regularization=crossentropy --name=micronet_reproduce_pr1 --ortho_lr=0.7 --max_prune_rate=45. --load_name=micronet_reproduce --model_ver=ver2 \nAnd after training above, run prune-training once again: \npython3 micronet_main.py --model=micronet_prune --dataset=CIFAR100 --lr=0.1 --batch_size=128 --lr_type=cos --n_epoch=600 --input_regularize=cutmix --label_regularization=crossentropy --name=micronet_reproduce_pr2 --ortho_lr=0.7 --min_prune_rate=45. --max_prune_rate=65. --load_name=micronet_reproduce_pr1 --model_ver=ver2 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Kthyeon/KAIST-AI-NeurIPS2019-MicroNet-2nd-place-solution/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 Kthyeon\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Our Contribution",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "KAIST-AI-NeurIPS2019-MicroNet-2nd-place-solution",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Kthyeon",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Kthyeon/KAIST-AI-NeurIPS2019-MicroNet-2nd-place-solution/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Current code base is tested under following environment:\n\n1. Python 3.7.3\n2. PyTorch 1.1.0\n3. torchvision 0.3.0\n4. Numpy 1.16.2\n5. tqdm\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 15,
      "date": "Tue, 28 Dec 2021 11:33:40 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "efficient-model",
      "orthonormality",
      "regularization",
      "adaptive-label-smoothing",
      "micronet-challenge",
      "cifar100",
      "neurips",
      "neurips-2019",
      "neurips-competition",
      "neurips2019"
    ],
    "technique": "GitHub API"
  }
}