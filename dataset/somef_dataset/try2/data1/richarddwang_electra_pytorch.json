{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2003.10555"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{electra_pytorch,\n  author = {Richard Wang},\n  title = {PyTorch implementation of ELECTRA},\n  year = {2020},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/richarddwang/electra_pytorch}}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{clark2020electra,\n  title = {{ELECTRA}: Pre-training Text Encoders as Discriminators Rather Than Generators},\n  author = {Kevin Clark and Minh-Thang Luong and Quoc V. Le and Christopher D. Manning},\n  booktitle = {ICLR},\n  year = {2020},\n  url = {https://openreview.net/pdf?id=r1xMH1BtvB}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8028046190715653
      ],
      "excerpt": "|ELECTRA-Small-OWT (my)|1.30|0.49|0.7|0.29|0.1|0.15|0.33|1.93 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/richarddwang/electra_pytorch",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-05-03T02:25:55Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-22T12:06:00Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8954806899779658,
        0.9910516134918242
      ],
      "excerpt": "I pretrain ELECTRA-small from scratch and have successfully replicated the paper's results on GLUE.  \n|Model|CoLA|SST|MRPC|STS|QQP|MNLI|QNLI|RTE|Avg. of Avg.| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9498268776441704
      ],
      "excerpt": "Table 3: Both are small models trained on OpenWebText. The official one is from here. You should take the value of training loss with a grain of salt since it doesn't reflect the performance of downstream tasks. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9650507349724108,
        0.8738983397404823
      ],
      "excerpt": "AFAIK, the closest reimplementation to the original one, taking care of many easily overlooked details (described below).  \nAFAIK, the only one successfully validate itself by replicating the results in the paper. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9962657557326284
      ],
      "excerpt": "Tabel 4: Statistics of GLUE devset results for small models. Every model is pretrained from scratch with different seeds and finetuned for 10 random runs for each GLUE task. Score of a model is the average of the best of 10 for each task. (The process is as same as the one described in the paper) As we can see, although ELECTRA is mocking adeversarial training, it has a good training stability. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8172861898975629
      ],
      "excerpt": "Table 5: Standard deviation for each task. This is the same model as Table 1, which finetunes 10 runs for each task. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9497064414780169
      ],
      "excerpt": "You will need a Neptune account and create a neptune project on the website to record GLUE finetuning results. Don't forget to replace richarddwang/electra-glue with your neptune project's name \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9971484881204086,
        0.9772786361487856,
        0.9936733089864987
      ],
      "excerpt": "Below lists the details of the original implementation/paper that are easy to be overlooked and I have taken care of. I found these details are indispensable to successfully replicate the results of the paper. \nUsing Adam optimizer without bias correction (bias correction is default for Adam optimizer in Pytorch and fastai) \nThere is a bug of decaying learning rates through layers in the official implementation , so that when finetuing, lr decays more than the stated in the paper. See _get_layer_lrs. Also see this issue. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9358003550469135,
        0.8366266180426007,
        0.9957355717181963,
        0.9737730815448294,
        0.9833710113454713,
        0.989209766928033,
        0.9193393498917742,
        0.9147606660715939,
        0.9123084761451369
      ],
      "excerpt": "using 0 weight decay when finetuning on GLUE \nIt didn't do warmup and then do linear decay but do them together, which means the learning rate warmups and decays at the same time during the warming up phase. See here \nFor pretraing data preprocessing, it concatenates and truncates setences to fit the max length, and stops concating when it comes to the end of a document. \nFor pretraing data preprocessing, it by chance splits the text into sentence A and sentence B, and also by chance changes the max length \nFor finetuning data preprocessing, it follow BERT's way to truncate the longest one of sentence A and B to fit the max length \nFor MRPC and STS tasks, it augments training data by add the same training data but with swapped sentence A and B. This is called \"double_unordered\" in the official implementation. \nIt didn't mask sentence like BERT, within the mask probability (15% or other value) of tokens,  a token has 85% chance to be replaced with [MASK] and 15% remains the same but no chance to be replaced with a random token. \nInput and output word embeddings of generator, and input word embeddings of discriminator. The three are tied together. \nIt tie not only word/pos/token type embeddings but also layer norm in the embedding layers of both generator and discriminator. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8712860945816823
      ],
      "excerpt": "Using gumbel softmax to sample generations from geneartor as input of discriminator \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9606280763953926,
        0.834574072849805
      ],
      "excerpt": "All public model of ELECTRA checkpoints are actually ++ model. See this issue \nIt downscales generator by hidden_size, number of attention heads, and intermediate size, but not number of layers. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.945645630358837
      ],
      "excerpt": "project root \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Pretrain and finetune ELECTRA with fastai and huggingface. (Results of the paper replicated !)",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/richarddwang/electra_pytorch/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 26,
      "date": "Sat, 25 Dec 2021 07:26:14 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/richarddwang/electra_pytorch/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "richarddwang/electra_pytorch",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/richarddwang/electra_pytorch/master/Pretrain.ipynb",
      "https://raw.githubusercontent.com/richarddwang/electra_pytorch/master/Finetune_GLUE.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "`pip3 install -r requirements.txt`\n\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8873078015675404
      ],
      "excerpt": "Table 2: Results on GLUE test set. My result finetunes the pretrained checkpoint loaded from huggingface. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8411624709648624
      ],
      "excerpt": "python pretrain.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9143624405196366,
        0.821907056763263
      ],
      "excerpt": "python finetune.py (with do_finetune set to True) \nGo to neptune, pick the best run of 10 runs for each task, and set th_runs in finetune.py according to the numbers in the names of runs you picked. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/richarddwang/electra_pytorch/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Replicated Results",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "electra_pytorch",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "richarddwang",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/richarddwang/electra_pytorch/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "`pip3 install -r requirements.txt`\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 230,
      "date": "Sat, 25 Dec 2021 07:26:14 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "electra",
      "pytorch",
      "fastai",
      "huggingface",
      "glue",
      "language-model",
      "deeplearning",
      "nlp"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "> Note: This project is actually for my personal research. So I didn't trying to make it easy to use for all users, but trying to make it easy to read and modify.\n\n",
      "technique": "Header extraction"
    }
  ]
}