{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1312.5602\n\n\n## Project Description \nFor calculating the Q Values we used a Neural Network, rather than just showing the current state to the network, the next 4 possible states(left, right, up, down"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* https://github.com/berkay-dincer/2048-ai-mcts\n* https://towardsdatascience.com/welcome-to-deep-reinforcement-learning-part-1-dqn-c3cab4d41b6b\n* http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html\n* https://papers.nips.cc/paper/3964-double-q-learning\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9944484218006108
      ],
      "excerpt": "- https://arxiv.org/abs/1312.5602 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dsgiitr/rl_2048",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-09-23T22:16:10Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-12T22:48:35Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "For calculating the Q Values we used a Neural Network, rather than just showing the current state to the network, the next 4 possible states(left, right, up, down) were also shown. This intuition was inspired from Monte Carlo Tree Search estimation where game is played till the end to determine the Q-Values. Instead of using a normal Q Network, a Double Q Network was used one for predicting Q values and other for predicting actions. This is done to try and reduce the large overestimations of action values which result form a positive bias introduced in Q Learning. For data preprocessing log2 normalisation, training was done using the Bellman's Equation. The policy used was Epsilon greedy, to allow exploration the value of epsilon was annealed down by 5%. \n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9911272403956165
      ],
      "excerpt": "Implementation of deep Q-network to play the game 2048 using Keras. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9933025337179765
      ],
      "excerpt": "Q-Learning is a reinforcement learning algorithm that seeks to find the best action to take given the current state. The expected reward of a action at that step is known as the Q-Value. It is stored in a table for each state, future state tuple. For most environments keeping track of the number of possible states and all possible combinations of these state is extremely difficult. Hence instead of storing these Q-Values we approximate them using Neural networks, this is known as a Deep Q-Network. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9878835792810277,
        0.9834732841386902,
        0.8785057432528695,
        0.8292753143371395,
        0.9508763885666327,
        0.8100981510901688,
        0.977510395646951
      ],
      "excerpt": "Fig: Max tile obtained on each game as NN training proceeds. Its visible that the model is able to learn the strategy within 600 Epochs as the number of tiles with 512 is much more towards the end. \nAlso from observation the model was able to learn the common heuristic amongst player to keep the maximum numbered tile at one corner and surround it with monotonically increasing tiles. This helps in combining the tiles in a series. \nDQN_Agent_2048.ipynb-Main notebook to train/test the DQN, also contains the definitions of the deep learning models \nGame_Board.py-Module with the game logic of 2048 (uses OPENAI Gym interface) \nDQN_Network is the Architecture of the Q Network. \ntrained_model.model-Model trained for 1000 epochs. \nThis work is in the very elementry stage and we'd like to improve upon the following: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9441733355196331
      ],
      "excerpt": "- Showing the subsequent 4 States of all the immideate next state so as to get better predictions(4 +4*4=16 states in total) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Implementation of Deep Q-network to play the game 2048 using Keras.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dsgiitr/rl_2048/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 6,
      "date": "Sun, 26 Dec 2021 10:40:11 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/dsgiitr/rl_2048/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "dsgiitr/rl_2048",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/dsgiitr/rl_2048/master/DQN_Agent_2048.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This project requires:\n- Python (Python 3.6)\n- Numpy\n- TensorFlow\n- Keras\n- Gym (OpenAI Gym)\n- Matplotlib\n\nOnce everything is installed open the DQN_Agent_2048.ipynb file on a jupyter notebook and run the cells.\n\n",
      "technique": "Header extraction"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/dsgiitr/rl_2048/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Q Learning Agent to Play 2048",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "rl_2048",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "dsgiitr",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/dsgiitr/rl_2048/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This project requires:\n- Python (Python 3.6)\n- Numpy\n- TensorFlow\n- Keras\n- Gym (OpenAI Gym)\n- Matplotlib\n\nOnce everything is installed open the DQN_Agent_2048.ipynb file on a jupyter notebook and run the cells.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 12,
      "date": "Sun, 26 Dec 2021 10:40:11 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "2048-game",
      "reinforcement-learning",
      "openai-gym",
      "keras",
      "dqn"
    ],
    "technique": "GitHub API"
  }
}