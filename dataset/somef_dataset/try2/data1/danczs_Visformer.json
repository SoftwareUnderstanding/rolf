{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2104.12533"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "| Visformer_tiny | 78.6| 1.3 | 10.3 | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/danczs/Visformer",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-04-12T03:21:28Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-22T13:55:22Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This is a pytorch implementation for the Visformer models. This project is based on the training code in [Deit](https://github.com/facebookresearch/deit) and the tools in [timm](https://github.com/rwightman/pytorch-image-models).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8761561829953907
      ],
      "excerpt": "The model performance: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.830683200058274,
        0.8645203502788597,
        0.8953284901405788,
        0.9071427063688384
      ],
      "excerpt": "More information about Visformer V2. \nThe standard self-attention is not efficient for high-reolution inputs,  \nso we simply replace the standard self-attention with Swin-attention for object detection. Therefore, Swin Transformer is our directly baseline.  \nMask R-CNN \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9071427063688384
      ],
      "excerpt": "Cascade Mask R-CNN \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8690934298910911,
        0.9544081101951216
      ],
      "excerpt": "This repo only contains the key files for object detection ('./ObjectDetction'). Swin-Visformer-Object-Detection  is the full detection project. \nBeacause of the policy of our institution, we cannot send the pre-trained models out directly. Thankfully, @hzhang57  and @developer0hye provides Visformer_small and Visformer_tiny models trained by themselves. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9765291930749568
      ],
      "excerpt": "To avoid overflow, we pre-normalize q & k, and, thus, overall normalize 'attn' with 'head_dim' instead of  'head_dim ** 0.5': \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/danczs/Visformer/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 20,
      "date": "Mon, 27 Dec 2021 22:01:49 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/danczs/Visformer/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "danczs/Visformer",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The layout of Imagenet data:\n```bash\n/path/to/imagenet/\n  train/\n    class1/\n      img1.jpeg\n    class2/\n      img2.jpeg\n  val/\n    class1/\n      img1.jpeg\n    class2/\n      img2.jpeg\n```\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8769776062778595
      ],
      "excerpt": "python -m torch.distributed.launch --nproc_per_node=8 --use_env main.py --model visformer_small --batch-size 64 --data-path /path/to/imagenet --output_dir /path/to/save \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8453985745566632
      ],
      "excerpt": "python -m torch.distributed.launch --nproc_per_node=4 --use_env main.py --model visformer_tiny --batch-size 256 --drop-path 0.03 --data-path /path/to/imagenet --output_dir /path/to/save \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8874425165645577,
        0.8585612856201734
      ],
      "excerpt": "python -m torch.distributed.launch --nproc_per_node=8 --use_env main.py --model swin_visformer_small_v2 --batch-size 64 --data-path /path/to/imagenet --output_dir /path/to/save --amp --qk-scale-factor=-0.5 \npython -m torch.distributed.launch --nproc_per_node=4 --use_env main.py --model swin_visformer_tiny_v2 --batch-size 256 --drop-path 0.03 --data-path /path/to/imagenet --output_dir /path/to/save --amp --qk-scale-factor=-0.5 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8174540907975313
      ],
      "excerpt": "Amp training: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8874425165645577,
        0.8585612856201734
      ],
      "excerpt": "python -m torch.distributed.launch --nproc_per_node=8 --use_env main.py --model visformer_small --batch-size 64 --data-path /path/to/imagenet --output_dir /path/to/save --amp --qk-scale-factor=-0.5 \npython -m torch.distributed.launch --nproc_per_node=4 --use_env main.py --model visformer_tiny --batch-size 256 --drop-path 0.03 --data-path /path/to/imagenet --output_dir /path/to/save --amp --qk-scale-factor=-0.5 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/danczs/Visformer/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Visformer",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Visformer",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "danczs",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/danczs/Visformer/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 101,
      "date": "Mon, 27 Dec 2021 22:01:49 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Clone the repository:\n```bash\ngit clone https://github.com/danczs/Visformer.git\n```\nInstall pytorch, timm and einops:\n```bash\npip install -r requirements.txt\n```\n",
      "technique": "Header extraction"
    }
  ]
}