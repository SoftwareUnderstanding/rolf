{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1706.03762",
      "https://arxiv.org/abs/1810.04805"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Other great repositories with this model include: \n - [Google Research's repo](https://github.com/google-research/vision_transformer)\n - [Ross Wightman's repo](https://github.com/rwightman/pytorch-image-models)\n - [Phil Wang's repo](https://github.com/lucidrains/vit-pytorch)\n - [Eunkwang Jeon's repo](https://github.com/jeonsworld/ViT-pytorch)\n - [Luke Melas-Kyriazi repo](https://github.com/lukemelas/PyTorch-Pretrained-ViT)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "configuration.num_classes = 10 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "<div style=\"text-align: center; padding: 10px\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9514562468139621
      ],
      "excerpt": "If you find a bug, create a GitHub issue, or even better, submit a pull request. Similarly, if you have questions, simply post them as GitHub issues. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/arkel23/PyTorch-Pretrained-ViT",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-12-18T03:34:20Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-09-13T05:51:12Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8285146497304466
      ],
      "excerpt": "Forked from Luke Melas-Kyriazi repository. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8269241845551492
      ],
      "excerpt": "Added support for downloading the models directly from Google's cloud storage. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9667911577703932
      ],
      "excerpt": "Modified loading of models by using configurations similar to HuggingFace's Transformers. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8790444523141344
      ],
      "excerpt": "model.load_partial(weights_path=weights_path, pretrained_image_size=configuration.pretrained_image_size,  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "for pretrained_mode in ['full_tokenizer', 'patchprojection', 'posembeddings', 'clstoken',  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "     model.load_partial(weights_path=weights_path,  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8784244240911363
      ],
      "excerpt": "This repository contains an op-for-op PyTorch reimplementation of the Vision Transformer architecture from Google, along with pre-trained models and examples. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9086195866555702,
        0.8863425306443846
      ],
      "excerpt": "The ViT architecture works as follows: (1) it considers an image as a 1-dimensional sequence of patches, (2) it prepends a classification token to the sequence, (3) it passes these patches through a transformer encoder (like BERT), (4) it passes the first token of the output of the transformer through a small MLP to obtain the classification logits.  \nViT is trained on a large-scale dataset (ImageNet-21k) with a huge amount of compute.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Vision Transformer (ViT) in PyTorch",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/arkel23/PyTorch-Pretrained-ViT/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Fri, 24 Dec 2021 06:35:55 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/arkel23/PyTorch-Pretrained-ViT/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "arkel23/PyTorch-Pretrained-ViT",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/arkel23/PyTorch-Pretrained-ViT/master/examples/inference/example.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/arkel23/PyTorch-Pretrained-ViT/master/examples/cifar/train_cifar.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\ngit clone https://github.com/arkel23/PyTorch-Pretrained-ViT.git\ncd PyTorch-Pretrained-ViT\npip install -e .\npython download_convert_models.py #: can modify to download different models, by default it downloads all 5 ViTs pretrained on ImageNet21k\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8924512612366715,
        0.9021820278820686
      ],
      "excerpt": "model = ViT_modified(config=configuration, name='B_16', pretrained=True) \n: for another example see examples/configurations/load_configs.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "pretrained_mode=pretrained_mode, verbose=True) \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/arkel23/PyTorch-Pretrained-ViT/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "ViT PyTorch",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "PyTorch-Pretrained-ViT",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "arkel23",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/arkel23/PyTorch-Pretrained-ViT/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3,
      "date": "Fri, 24 Dec 2021 06:35:55 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\nfrom pytorch_pretrained_vit import ViT, ViTConfigExtended, PRETRAINED_CONFIGS\n\nmodel_name = 'B_16'\ndef_config = PRETRAINED_CONFIGS['{}'.format(model_name)]['config']\nconfiguration = ViTConfigExtended(**def_config)\nmodel = ViT(configuration, name=model_name, pretrained=True, load_repr_layer=False, ret_attn_scores=False)\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}