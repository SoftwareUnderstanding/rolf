{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[1] [Chami, I., Ying, R., R\u00e9, C. and Leskovec, J. Hyperbolic Graph Convolutional Neural Networks. NIPS 2019.](http://web.stanford.edu/~chami/files/hgcn.pdf)\n\n[2] [Nickel, M. and Kiela, D. Poincar\u00e9 embeddings for learning hierarchical representations. NIPS 2017.](https://arxiv.org/pdf/1705.08039.pdf)\n\n[3] [Ganea, O., B\u00e9cigneul, G. and Hofmann, T. Hyperbolic neural networks. NIPS 2017.](https://arxiv.org/pdf/1805.09112.pdf)\n\n[4] [Kipf, T.N. and Welling, M. Semi-supervised classification with graph convolutional networks. ICLR 2017.](https://arxiv.org/pdf/1609.02907.pdf)\n\n[5] [Veli\u010dkovi\u0107, P., Cucurull, G., Casanova, A., Romero, A., Lio, P. and Bengio, Y. Graph attention networks. ICLR 2018.](https://arxiv.org/pdf/1710.10903.pdf)\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find this code useful, please cite the following paper: \n```\n@inproceedings{chami2019hyperbolic,\n  title={Hyperbolic graph convolutional neural networks},\n  author={Chami, Ines and Ying, Zhitao and R{\\'e}, Christopher and Leskovec, Jure},\n  booktitle={Advances in Neural Information Processing Systems},\n  pages={4869--4880},\n  year={2019}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{chami2019hyperbolic,\n  title={Hyperbolic graph convolutional neural networks},\n  author={Chami, Ines and Ying, Zhitao and R{\\'e}, Christopher and Leskovec, Jure},\n  booktitle={Advances in Neural Information Processing Systems},\n  pages={4869--4880},\n  year={2019}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8654671031158477
      ],
      "excerpt": "Link prediction (lp) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/HazyResearch/hgcn",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-09-30T07:45:27Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-19T12:45:59Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9877196278778512
      ],
      "excerpt": "This repository is a graph representation learning library, containing an implementation of Hyperbolic Graph Convolutions [1] in PyTorch, as well as multiple embedding approaches including: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.93867465044829,
        0.93867465044829
      ],
      "excerpt": "Shallow Euclidean + Features (see [1]) \nShallow Hyperbolic + Features (see [1]) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8874520643324144
      ],
      "excerpt": "To run this code on new datasets, please add corresponding data processing and loading in load_data_nc and load_data_lp functions in utils/data_utils.py. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8284327651237399
      ],
      "excerpt": "                        which optimizer to use, can be any of [Adam, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.826570392686923
      ],
      "excerpt": "  --save SAVE           1 to save model and logs and 0 otherwise \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8052587546663049
      ],
      "excerpt": "                        max norm for gradient clipping, or None for no \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8091682614003245,
        0.9052817889274772
      ],
      "excerpt": "  --task TASK           which tasks to train on, can be any of [lp, nc] \n  --model MODEL         which encoder to use, can be any of [Shallow, MLP, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8284327651237399
      ],
      "excerpt": "  --manifold MANIFOLD   which manifold to use, can be any of [Euclidean, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8046553342716138
      ],
      "excerpt": "  --c C                 hyperbolic radius, set to None for trainable curvature \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.916749541752443
      ],
      "excerpt": "  --n-heads N_HEADS     number of attention heads for graph attention \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8114554416963418
      ],
      "excerpt": "                        whether to use node features or not \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8784483891088252
      ],
      "excerpt": "                        whether to normalize input node features \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.908925214220865
      ],
      "excerpt": "Cora and Pubmed:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Hyperbolic Graph Convolutional Networks in PyTorch. ",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/HazyResearch/hgcn/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 80,
      "date": "Tue, 21 Dec 2021 05:19:06 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/HazyResearch/hgcn/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "HazyResearch/hgcn",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/HazyResearch/hgcn/master/set_env.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Alternatively, if you prefer to install dependencies with pip, please follow the instructions below:\n\n```virtualenv -p [PATH to python3.7 binary] hgcn```\n\n```source hgcn/bin/activate```\n\n```pip install -r requirements.txt```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "If you don't have conda installed, please install it following the instructions [here](https://conda.io/projects/conda/en/latest/user-guide/install/index.html).\n\n```git clone https://github.com/HazyResearch/hgcn```\n\n```cd hgcn```\n\n```conda env create -f environment.yml```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9654765604684976
      ],
      "excerpt": "source set_env.sh \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8352432248257089
      ],
      "excerpt": "  --log-freq LOG_FREQ   how often to compute print train/val metrics (in \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8282127504242136
      ],
      "excerpt": "  --save-dir SAVE_DIR   path to save training logs and model weights (defaults \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9157836103422314
      ],
      "excerpt": "  --print-epoch PRINT_EPOCH \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8139181146119242
      ],
      "excerpt": "                        path to pretrained embeddings (.npy file) for Shallow \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8633989807152664
      ],
      "excerpt": "  --test-prop TEST_PROP \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8909156095235873
      ],
      "excerpt": "                        seed for data splits (train/test/val) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9285131990234091
      ],
      "excerpt": "python train.py --task lp --dataset cora --model HGCN --lr 0.01 --dim 16 --num-layers 2 --act relu --bias 1 --dropout 0.5 --weight-decay 0.001 --manifold PoincareBall --log-freq 5 --cuda 0 --c None \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9248163947275149
      ],
      "excerpt": "python train.py --task lp --dataset pubmed --model HGCN --lr 0.01 --dim 16 --num-layers 2 --act relu --bias 1 --dropout 0.4 --weight-decay 0.0001 --manifold PoincareBall --log-freq 5 --cuda 0  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9363635604372955
      ],
      "excerpt": "python train.py --task lp --dataset disease_lp --model HGCN --lr 0.01 --dim 16 --num-layers 2 --num-layers 2 --act relu --bias 1 --dropout 0 --weight-decay 0 --manifold PoincareBall --normalize-feats 0 --log-freq 5 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9285131990234091
      ],
      "excerpt": "python train.py --task lp --dataset airport --model HGCN --lr 0.01 --dim 16 --num-layers 2 --act relu --bias 1 --dropout 0.0 --weight-decay 0 --manifold PoincareBall --log-freq 5 --cuda 0 --c None \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.879887354535674,
        0.9250117574790861
      ],
      "excerpt": "To train train a HGCN node classification model on Cora and Pubmed datasets, pre-train embeddings for link prediction as decribed in the previous section. Then train a MLP classifier using the pre-trained embeddings (embeddings.npy file saved in the save-dir directory). For instance for the Pubmed dataset: \npython train.py --task nc --dataset pubmed --model Shallow --lr 0.01 --dim 16 --num-layers 2 --act relu --bias 1 --dropout 0.2 --weight-decay 0.0005 --manifold Euclidean --log-freq 5 --cuda 0 --use-feats 0 --pretrained-embeddings [PATH_TO_EMBEDDINGS] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9182599489841892
      ],
      "excerpt": "python train.py --task nc --dataset disease_nc --model HGCN --dim 16 --lr 0.01 --dim 16 --num-layers 2 --act relu --bias 1 --dropout 0 --weight-decay 0 --manifold PoincareBall --log-freq 5 --cuda 0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9109691317037258
      ],
      "excerpt": "python train.py --task lp --dataset cora --model Shallow --manifold Euclidean --lr 0.01 --weight-decay 0.0005 --dim 16 --num-layers 0 --use-feats 0 --dropout 0.2 --act None --bias 0 --optimizer Adam --cuda 0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9109691317037258
      ],
      "excerpt": "python train.py --task lp --dataset cora --model Shallow --manifold PoincareBall --lr 0.01 --weight-decay 0.0005 --dim 16 --num-layers 0 --use-feats 0 --dropout 0.2 --act None --bias 0 --optimizer RiemannianAdam --cuda 0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9248163947275149,
        0.8059405042806759,
        0.9285131990234091
      ],
      "excerpt": "python train.py --task lp --dataset cora --model GCN --lr 0.01 --dim 16 --num-layers 2 --act relu --bias 1 --dropout 0.2 --weight-decay 0 --manifold Euclidean --log-freq 5 --cuda 0 \nHNN (Test ROC-AUC=90.79):  \npython train.py --task lp --dataset cora --model HNN --lr 0.01 --dim 16 --num-layers 2 --act None --bias 1 --dropout 0.2 --weight-decay 0.001 --manifold PoincareBall --log-freq 5 --cuda 0 --c 1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9285131990234091
      ],
      "excerpt": "python train.py --task nc --dataset pubmed --model HNN --lr 0.01 --dim 16 --num-layers 2 --act None --bias 1 --dropout 0.5 --weight-decay 0 --manifold PoincareBall --log-freq 5 --cuda 0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9285131990234091
      ],
      "excerpt": "python train.py --task nc --dataset pubmed --model MLP --lr 0.01 --dim 16 --num-layers 2 --act None --bias 0 --dropout 0.2 --weight-decay 0.001 --manifold Euclidean --log-freq 5 --cuda 0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9248163947275149
      ],
      "excerpt": "python train.py --task nc --dataset pubmed --model GCN --lr 0.01 --dim 16 --num-layers 2 --act relu --bias 1 --dropout 0.7 --weight-decay 0.0005 --manifold Euclidean --log-freq 5 --cuda 0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9079809839042586
      ],
      "excerpt": "python train.py --task nc --dataset pubmed --model GAT --lr 0.01 --dim 16 --num-layers 2 --act elu --bias 1 --dropout 0.5 --weight-decay 0.0005 --alpha 0.2 --n-heads 4 --manifold Euclidean --log-freq 5 --cuda 0 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/HazyResearch/hgcn/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Hyperbolic Graph Convolutional Networks in PyTorch",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "hgcn",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "HazyResearch",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/HazyResearch/hgcn/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 330,
      "date": "Tue, 21 Dec 2021 05:19:06 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We provide examples of training commands used to train HGCN and other graph embedding models for link prediction and node classification. In the examples below, we used a fixed random seed set to 1234 for reproducibility purposes. Note that results might slightly vary based on the machine used. To reproduce results in the paper, run each commad for 10 random seeds and average the results.\n\n",
      "technique": "Header extraction"
    }
  ]
}