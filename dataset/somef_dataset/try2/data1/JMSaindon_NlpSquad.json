{
  "citation": [
    {
      "confidence": [
        0.9105368110547479,
        0.8715509475085587
      ],
      "excerpt": "HuggingFace/transformers: https://github.com/huggingface/transformers  \nSQuAD 2.0: https://rajpurkar.github.io/SQuAD-explorer/ \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/JMSaindon/NlpSquad",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-01-15T13:22:01Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-09-21T01:29:38Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "**BERT** (Bidirectional Encoder Representations from Transformers) is a NLP model developed by Google that follows the structure of a transformer. That structure was used to create models that NLP practicioners can then download and use for free such as RoBerta, XLNet or AlBert. You can either use these models to extract high quality language features from your text data, or you can fine-tune these models on a specific task (classification, entity recognition, question answering, etc.) with your own data to produce state of the art predictions.\n\nIn this repository, we will try to modify and fine-tune BERT to create a powerful NLP model for Question Answering, which means giving a text and a question, the model will be able to find the answer to the question in the text.\n\nThe dataset that we will be using is **SQuAD 2.0**. The dataset consist in a series of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage (sometimes the question might be unanswerable).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9185578257832995,
        0.8439688942193608,
        0.9809224704188088
      ],
      "excerpt": "We first attempted to run the scripts provided by the transformers library by HuggingFace that fine-tune the base model of BERT in order to train it for Question Answering. We have used a small portion of the SQuAD dataset because our environment's performance didn't allow us to run the training on the whole dataset. (Both the colab and local environment we tried unfortunately ended by crashing) \nThe script that fine-tunes BERT using the library transformer and run_squad is: Nlp-squad-runner.ipynb \nAfter getting an insight into the challenges that we have to face, we've tried to implement the training and evaluation loops to fine-tune BERT with the SQuAD 2.0 dataset. We've analyzed the script given by the library transformers (run_squad.py) and we've attempted to re-create a fine-tuning more adapted to our needs. These are the steps that we've followed to fine-tune BERT. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9965155910990043,
        0.9133153674549584,
        0.9034608331907262,
        0.8330464337683816,
        0.9952844011538444
      ],
      "excerpt": "Apply the BERT tokenizer to our data. \nAdd special tokens to the start and end of each sentence. \nPadd & truncate all sentences to a single constant length. \nMap the tokens to their IDs. (the last 3 steps are done by a transformers function provided in order to extract features) \nLoad the base model of BERT. In our case, we chosed to use the BertForQuestionAnswering structures with some bert pretrained weights on the classic Bert part of it. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9562508973274699,
        0.9330912796035205,
        0.8424019090954447,
        0.8182316889955386,
        0.9791900581487828
      ],
      "excerpt": "Evaluate the model with the dev file provided by the website of Squad V2. \nBy using 10 000 text-question-answer, the precision of our model was: 49% \nA save of this model can be found in the directory finetuned_squad_saved_acc_49 with our prediction files too \nThe script that fine-tunes BERT using the library transformer is: Bert_fine_tune.ipynb \nWe have tried to launch the fine tuning with all the data but it was too long (16-17 hours) and our computer was literaly burning after an hour of it (the train interrupted after an hour resulted in an accuracy of 56%). As our script seem to train well, we think that a full training should give satisfying performances. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8435213286327863
      ],
      "excerpt": "Painless Fine-Tuning of BERT in Pytorch: https://medium.com/swlh/painless-fine-tuning-of-bert-in-pytorch-b91c14912caa \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/JMSaindon/NlpSquad/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Tue, 28 Dec 2021 22:08:59 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/JMSaindon/NlpSquad/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "JMSaindon/NlpSquad",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/JMSaindon/NlpSquad/master/Bert_fine_tune.ipynb",
      "https://raw.githubusercontent.com/JMSaindon/NlpSquad/master/Nlp_squad_runner.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8918974083095406
      ],
      "excerpt": "HuggingFace/transformers: https://github.com/huggingface/transformers  \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8884699223466979
      ],
      "excerpt": "Download the SQuAD 2.0 dataset (json files) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8545189135462047
      ],
      "excerpt": "Train the BERT model with our SQuAD dataset (A reduced version in a first place). \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/JMSaindon/NlpSquad/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Introduction",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "NlpSquad",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "JMSaindon",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/JMSaindon/NlpSquad/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Tue, 28 Dec 2021 22:08:59 GMT"
    },
    "technique": "GitHub API"
  }
}