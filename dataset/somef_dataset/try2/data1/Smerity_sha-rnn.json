{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1911.11423"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Smerity/sha-rnn",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-09-16T19:52:29Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-19T06:45:29Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9799523098688843,
        0.8307880040140854,
        0.8235101762360842
      ],
      "excerpt": "For full details see the paper Single Headed Attention RNN: Stop Thinking With Your Head. \nIn summary, \"stop thinking with your (attention) head\". \nObtain strong results on a byte level language modeling dataset (enwik8) in under 24 hours on a single GPU (12GB Titan V) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9775099241672908
      ],
      "excerpt": "Provide a smaller model that features only standard components such as the LSTM, single headed attention, and feed-forward modules such that they can easily be productionized using existing optimized tools and exported to various formats (i.e. ONNX) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9845441645794841,
        0.9294232617362796,
        0.9176359821244641,
        0.8384453124669361
      ],
      "excerpt": "Whilst the model is still quite some way away from state of the art (~0.98 bpc) the model is low resource and high efficiency without having yet been optimized to be so. \nThe model was trained in under 24 hours on a single GPU with the Adaptive Span Transformer (small) being the only recent Transformer model to achieve similar levels of training efficiency. \nBy default the model trains the minimal single headed attention model from the paper, inserting a lone attention mechanism in the second last layer of a four layer LSTM. \nThis takes only half an hour per epoch on a Titan V or V100. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9113407095367724
      ],
      "excerpt": "The code is not kind. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9449462993687925,
        0.8328181134861526
      ],
      "excerpt": "Note: still shaking out bugs from the commands below. We have near third party replication but still a fix or two out. Feel free to run and note any discrepancies! If you fiddle with hyper-parameters (which I've done very little of - it's a treasure chest of opportunity to get a lower than expected BPC as your reward!) do report that too :) \nWhen running the training command below continue until the validation bpc stops improving. Don't worry about letting it run longer as the code will only save the model with the best validation bpc. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9380144891298825
      ],
      "excerpt": "When the training slows down a second pass with a halved learning rate until validation bpc stops improving will get a few more bpc off. A smart learning rate decay is likely the correct way to go here but that's not what I did for my experiments. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Single Headed Attention RNN - \"Stop thinking with your head\"",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Smerity/sha-rnn/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 132,
      "date": "Sat, 25 Dec 2021 22:01:13 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Smerity/sha-rnn/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Smerity/sha-rnn",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Smerity/sha-rnn/master/getdata.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To get started:\n\n- Retrieve the data with `./getdata.sh`\n- Install PyTorch version 1.2+\n- Install Nvidia's [AMP](https://github.com/NVIDIA/apex)\n- Install the minimum trust variant of LAMB from [Smerity's PyTorch-LAMB](https://github.com/Smerity/pytorch-lamb)\n\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8094492324621507
      ],
      "excerpt": "| Model                             | Test BPC | Params | LSTM Based | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8705782464609969
      ],
      "excerpt": "If you want slightly better results but a longer training time (an hour per epoch) set use_attn to True for all layers in model.py and decrease batch size until it fits in memory (i.e. 8). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9333002120337289
      ],
      "excerpt": "python -u main.py --epochs 32 --dropouth 0.1 --dropouti 0.1 --dropout 0.1 --data data/enwik8/ --save ENWIK8.pt --log-interval 10 --seed 5512 --optimizer lamb --bptt 1024 --warmup 800 --lr 2e-3 --emsize 1024 --nhid 4096 --nlayers 4 --batch_size 16 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9280056812772178
      ],
      "excerpt": "python -u main.py --epochs 5 --dropouth 0.1 --dropouti 0.1 --dropout 0.1 --data data/enwik8/ --save ENWIK8.pt --log-interval 10 --seed 5512 --optimizer lamb --bptt 1024 --warmup 800 --lr 2e-3 --emsize 1024 --nhid 4096 --nlayers 4 --batch_size 16 --resume ENWIK8.pt --lr 1e-3 --seed 125 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Smerity/sha-rnn/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Single Headed Attention RNN",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "sha-rnn",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Smerity",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Smerity/sha-rnn/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1144,
      "date": "Sat, 25 Dec 2021 22:01:13 GMT"
    },
    "technique": "GitHub API"
  }
}