{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "https://github.com/tramphero/kaldi\n\nThis is now the official location of the Kaldi project. http://kaldi-asr.org\n\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1810.04805.\n\n---\n\n### \u4e09\u3001\u5b66\u4e60\u8d44\u6e90\n\n#### Oxford NLP Lectures\n\nhttps://github.com/oxford-cs-deepnlp-2017/lectures\n\nThis repository contains the lecture slides and course description for the Deep Natural Language Processing course offered in Hilary Term 2017 at the University of Oxford.\n\n#### stat212b\n\nhttps://github.com/joanbruna/stat212b\n\nTopics Course on Deep Learning for Spring 2016, by Joan Bruna, UC Berkeley, Statistics Department\n\n#### Deep-Learning-101\n\nhttps://github.com/sjchoi86/Deep-Learning-101\n\nDeep Learning Tutorials These tutorials are for deep learning beginners which have been used in a six week Deep Learning and Computer Vision course. Hope these to be helpful for understanding what deep learning is and how it can be applied to various fields including computer vision, robotics, natural language processings, and so forth.\n\n#### Stanford Machine Learning course exercises\n\nhttps://github.com/krasserm/machine-learning-notebooks\n\nStanford Machine Learning course exercises implemented with scikit-learn\n\n#### CMU 10703: Deep Reinforcement Learning and Control, Spring 2017\n\nhttps://katefvision.github.io/\n\n+ Implement and experiment with existing algorithms for learning control policies guided by reinforcement, expert demonstrations or self-trials.\n+ Evaluate the sample complexity, generalization and generality of these algorithms.\n+ Be able to understand research papers in the field of robotic learning.\n+ Try out some ideas/extensions of your own. Particular focus on incorporating true sensory signal from vision or tactile sensing, and exploring the synergy between learning from simulation versus learning from real experience.\n\n#### MIT 6.S099: Artificial General Intelligence\n\nhttps://agi.mit.edu/\n\nThis class takes an engineering approach to exploring possible research paths toward building human-level intelligence. The lectures will introduce our current understanding of computational intelligence and ways in which strong AI could possibly be achieved, with insights from deep learning, reinforcement learning, computational neuroscience, robotics, cognitive modeling, psychology, and more. Additional topics will include AI safety and ethics. Projects will seek to build intuition about the limitations of state-of-the-art machine learning approaches and how those limitations may be overcome. The course will include several guest talks. Listeners are welcome.\n\n#### The Human Brain\n\nhttps://nancysbraintalks.mit.edu/course/9-11-the-human-brain\n\nMIT\u8ba4\u77e5\u795e\u7ecf\u79d1\u5b66\u6559\u6388Nancy Kanwisher\uff0c\u653e\u51fa\u4e86\u4e00\u5927\u6ce2\u672c\u5b66\u671f\uff082018\u5e74\u6625\u5b63\uff09MIT\u672c\u79d1\u751f\u8bfe\u7a0b\u4eba\u7c7b\u5927\u8111\uff08The Human Brain\uff09\u7684\u89c6\u9891\uff0c\u8bfe\u7a0b\u4ee3\u53f7MIT 9.11\u3002\n\n#### Berkeley \u4eba\u5de5\u667a\u80fd\u76f8\u5173\u8bfe\u7a0b\n\nhttp://bair.berkeley.edu/courses.html\n\n#### Berkeley CS 294: Deep Reinforcement Learning, Fall 2017\n\nhttp://rll.berkeley.edu/deeprlcourse/#syllabus\n\n#### Tensorflow-101\n\nhttps://github.com/sjchoi86/Tensorflow-101\n\nTensorflow Tutorials using Jupyter Notebook\n\nTensorFlow tutorials written in Python (of course"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "https://github.com/mlperf/reference\n\nReference implementations of MLPerf benchmarks\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9138274863291145
      ],
      "excerpt": "Microsoft Cognitive Toolkit (CNTK), an open source deep-learning toolkit \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8053302913910552
      ],
      "excerpt": "Topics Course on Deep Learning for Spring 2016, by Joan Bruna, UC Berkeley, Statistics Department \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8109194328925066
      ],
      "excerpt": "\u4e54\u6cbb\u4e9a\u7406\u5de5\u5927\u5b66 Jacob Eisenstein \u6559\u6388\u5f00\u653e\u4e86\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\u7684\u6700\u65b0\u6559\u6750\u300aNatural Language Processing\u300b \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9105843271952955
      ],
      "excerpt": "A curated list of automated machine learning papers, articles, tutorials, slides and projects. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9992066935433416
      ],
      "excerpt": "UC Berkeley \u53d1\u5e03\u4e86\u8fc4\u4eca\u4e3a\u6b62\u89c4\u6a21\u6700\u5927\u3001\u6700\u591a\u6837\u5316\u7684\u5f00\u653e\u9a7e\u9a76\u89c6\u9891\u6570\u636e\u96c6\u2014\u2014BDD100K\u3002\u8be5\u6570\u636e\u96c6\u5171\u5305\u542b 10 \u4e07\u4e2a\u89c6\u9891\uff0cBAIR \u7814\u7a76\u8005\u5728\u89c6\u9891\u4e0a\u91c7\u6837\u5173\u952e\u5e27\uff0c\u5e76\u4e3a\u8fd9\u4e9b\u5173\u952e\u5e27\u63d0\u4f9b\u6ce8\u91ca\u3002\u6b64\u5916\uff0cBAIR \u8fd8\u5c06\u5728 CVPR 2018 \u81ea\u52a8\u9a7e\u9a76 Workshop \u4e0a\u57fa\u4e8e\u5176\u6570\u636e\u4e3e\u529e\u4e09\u9879\u6311\u6218\u8d5b\u3002 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jiyuan/ainote",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2014-05-17T11:36:30Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-02-28T13:09:39Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9691152962111884
      ],
      "excerpt": "A list of resources releated to federated learning and privacy in machine learning. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.993820609939688,
        0.9683852237311356
      ],
      "excerpt": "Open Neural Network Exchange (ONNX) is the first step toward an open ecosystem that empowers AI developers to choose the right tools as their project evolves. ONNX provides an open source format for AI models. It defines an extensible computation graph model, as well as definitions of built-in operators and standard data types. Initially we focus on the capabilities needed for inferencing (evaluation). \nCaffe2, PyTorch, Microsoft Cognitive Toolkit, Apache MXNet and other tools are developing ONNX support. Enabling interoperability between different frameworks and streamlining the path from research to production will increase the speed of innovation in the AI community. We are an early stage and we invite the community to submit feedback and help us further evolve ONNX. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9793551178381222,
        0.995630627392967
      ],
      "excerpt": "Apache MXNet (incubating) is a deep learning framework designed for both efficiency and flexibility. It allows you to mix symbolic and imperative programming to maximize efficiency and productivity. At its core, MXNet contains a dynamic dependency scheduler that automatically parallelizes both symbolic and imperative operations on the fly. A graph optimization layer on top of that makes symbolic execution fast and memory efficient. MXNet is portable and lightweight, scaling effectively to multiple GPUs and multiple machines. \nMXNet is also more than a deep learning project. It is also a collection of blue prints and guidelines for building deep learning systems, and interesting insights of DL systems for hackers. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9452989201363019,
        0.8399016034800437
      ],
      "excerpt": "Apache MXNet Model Server (MMS) is a flexible and easy to use tool for serving deep learning models exported from MXNet or the Open Neural Network Exchange (ONNX). \nUse the MMS Server CLI, or the pre-configured Docker images, to start a service that sets up HTTP endpoints to handle model inference requests. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9910876504927265
      ],
      "excerpt": "Lucid is a collection of infrastructure and tools for research in neural network interpretability. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9809016707049674
      ],
      "excerpt": "Sonnet is a library built on top of TensorFlow for building complex neural networks. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8158582489363686,
        0.899787859431642
      ],
      "excerpt": "This package contains: \n- A set of Python Reinforcement Learning environments powered by the MuJoCo physics engine. See the suite subdirectory. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9619618154188011
      ],
      "excerpt": "Graph Nets is DeepMind's library for building graph networks in Tensorflow and Sonnet. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9931313472396587
      ],
      "excerpt": "MMLSpark provides a number of deep learning and data science tools for Apache Spark, including seamless integration of Spark Machine Learning pipelines with Microsoft Cognitive Toolkit (CNTK) and OpenCV, enabling you to quickly create powerful, highly-scalable predictive and analytical models for large image and text datasets. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9954762581894648
      ],
      "excerpt": "MMdnn is a set of tools to help users inter-operate among different deep learning frameworks. E.g. model conversion and visualization. Convert models between Caffe, Keras, MXNet, Tensorflow, CNTK, PyTorch and CoreML. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.903740768293056
      ],
      "excerpt": "BigDL: Distributed Deep Learning on Apache Spark \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9857872021845694
      ],
      "excerpt": "Edward is a Python library for probabilistic modeling, inference, and criticism. It is a testbed for fast experimentation and research with probabilistic models, ranging from classical hierarchical models on small data sets to complex deep probabilistic models on large data sets. Edward fuses three fields: Bayesian statistics and machine learning, deep learning, and probabilistic programming. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9481800883295562
      ],
      "excerpt": "Python codes in Machine Learning, NLP, Deep Learning and Reinforcement Learning with Keras and Theano \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8741339592337491
      ],
      "excerpt": "Researches for Natural Language Processing for Financial Domain. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9737832696448857
      ],
      "excerpt": "This document aims to track the progress in Natural Language Processing (NLP) and give an overview of the state-of-the-art across the most common NLP tasks and their corresponding datasets. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8345712876419826,
        0.9795819407265713
      ],
      "excerpt": "Simple, Strong Deep-Learning Baselines for NLP in several frameworks \nBaseline algorithms and data support implemented with multiple deep learning tools, including sentence classification, tagging, seq2seq, and language modeling. Can be used as stand-alone command line tools or as a Python library. The library attempts to provide a common interface for several common deep learning tasks, as well as easy-to-use file loaders to make it easy to publish standard results, compare against strong baselines without concern for mistakes and to support rapid experiments to try and beat these baselines. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9295175493966392
      ],
      "excerpt": "NLTK -- the Natural Language Toolkit -- is a suite of open source Python modules, data sets, and tutorials supporting research and development in Natural Language Processing. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8518252817022505
      ],
      "excerpt": "Its goal is to provide researchers: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8926282805342335,
        0.9542091331929271
      ],
      "excerpt": "- many popular datasets available all in one place, with the ability to multi-task over them \n- seamless integration of Amazon Mechanical Turk for data collection and human evaluation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9951917637202249
      ],
      "excerpt": "DeepQA is a library for doing high-level NLP tasks with deep learning, particularly focused on various kinds of question answering. DeepQA is built on top of Keras and TensorFlow, and can be thought of as an interface to these systems that makes NLP easier. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.987786766702565
      ],
      "excerpt": "This repository contains Tensorflow implementations of various deep learning models, with a focus on problems in Natural Language Processing. Each individual subdirectory is self-contained, addressing one specific model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9238772965536204
      ],
      "excerpt": "we provide details of a newly created dataset of Chinese text with about 1 million Chinese characters annotated by experts in over 30 thousand street view images. This is a challenging dataset with good diversity. It contains planar text, raised text, text in cities, text in rural areas, text under poor illumination, distant text, partially occluded text, etc. For each character in the dataset, the annotation includes its underlying character, its bounding box, and 6 attributes. The attributes indicate whether it has complex background, whether it is raised, whether it is handwritten or printed, etc. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9522926224678561
      ],
      "excerpt": "NLP Architect by Intel AI Lab: Python library for exploring the state-of-the-art deep learning topologies and techniques for natural language processing and natural language understanding http://nlp_architect.nervanasys.com/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9873589437433746,
        0.8863308201829363
      ],
      "excerpt": "BERT, or Bidirectional Encoder Representations from Transformers, is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks. \nOur academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: https://arxiv.org/abs/1810.04805. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9842636504803511
      ],
      "excerpt": "This repository contains the lecture slides and course description for the Deep Natural Language Processing course offered in Hilary Term 2017 at the University of Oxford. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8537966625936576
      ],
      "excerpt": "Topics Course on Deep Learning for Spring 2016, by Joan Bruna, UC Berkeley, Statistics Department \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9472765567057858
      ],
      "excerpt": "Deep Learning Tutorials These tutorials are for deep learning beginners which have been used in a six week Deep Learning and Computer Vision course. Hope these to be helpful for understanding what deep learning is and how it can be applied to various fields including computer vision, robotics, natural language processings, and so forth. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8172068868513803,
        0.8833244179172987,
        0.9085916862682566
      ],
      "excerpt": "Implement and experiment with existing algorithms for learning control policies guided by reinforcement, expert demonstrations or self-trials. \nEvaluate the sample complexity, generalization and generality of these algorithms. \nBe able to understand research papers in the field of robotic learning. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9703097006783119
      ],
      "excerpt": "This class takes an engineering approach to exploring possible research paths toward building human-level intelligence. The lectures will introduce our current understanding of computational intelligence and ways in which strong AI could possibly be achieved, with insights from deep learning, reinforcement learning, computational neuroscience, robotics, cognitive modeling, psychology, and more. Additional topics will include AI safety and ethics. Projects will seek to build intuition about the limitations of state-of-the-art machine learning approaches and how those limitations may be overcome. The course will include several guest talks. Listeners are welcome. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665427636179162
      ],
      "excerpt": "MIT\u8ba4\u77e5\u795e\u7ecf\u79d1\u5b66\u6559\u6388Nancy Kanwisher\uff0c\u653e\u51fa\u4e86\u4e00\u5927\u6ce2\u672c\u5b66\u671f\uff082018\u5e74\u6625\u5b63\uff09MIT\u672c\u79d1\u751f\u8bfe\u7a0b\u4eba\u7c7b\u5927\u8111\uff08The Human Brain\uff09\u7684\u89c6\u9891\uff0c\u8bfe\u7a0b\u4ee3\u53f7MIT 9.11\u3002 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9718625438130825
      ],
      "excerpt": "TensorFlow tutorials written in Python (of course) with Jupyter Notebook. Tried to explain as kindly as possible, as these tutorials are intended for TensorFlow beginners. Hope these tutorials to be a useful recipe book for your deep learning projects. Enjoy coding! :) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9822506784729547
      ],
      "excerpt": "Here is a reading roadmap of Deep Learning papers! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9181419570341666,
        0.9910443116708554
      ],
      "excerpt": "A curated list of the most cited deep learning papers (since 2012) \nWe believe that there exist classic deep learning papers which are worth reading regardless of their application domain. Rather than providing overwhelming amount of papers, We would like to provide a curated list of the awesome deep learning papers which are considered as must-reads in certain research domains. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9207725069469421
      ],
      "excerpt": "These notes are the basis for the readings in CS4650 and CS7650 (\"Natural Language\") at Georgia Tech. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9196369915282755
      ],
      "excerpt": "A curated list of automated machine learning papers, articles, tutorials, slides and projects. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9863741300327339
      ],
      "excerpt": "TransmogrifAI (pronounced tr\u0103ns-m\u014fg\u02c8r\u0259-f\u012b) is an AutoML library written in Scala that runs on top of Spark. It was developed with a focus on accelerating machine learning developer productivity through machine learning automation, and an API that enforces compile-time type-safety, modularity, and reuse. Through automation, it achieves accuracies close to hand-tuned models with almost 100x reduction in time. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9684765169860944
      ],
      "excerpt": "NNI (Neural Network Intelligence) is a toolkit to help users run automated machine learning experiments. The tool dispatches and runs trial jobs that generated by tuning algorithms to search the best neural architecture and/or hyper-parameters in different environments (e.g. local machine, remote servers and cloud). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9140912764507327
      ],
      "excerpt": "AdaNet is a lightweight and scalable TensorFlow AutoML framework for training and deploying adaptive neural networks using the AdaNet algorithm [Cortes et al. ICML 2017]. AdaNet combines several learned subnetworks in order to mitigate the complexity inherent in designing effective neural networks. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9177742653294126
      ],
      "excerpt": "AceKG describes 114.30 million academic entities based on a consistent ontology, including 61,704,089 papers, 52,498,428 authors, 50,233 research fields, 19,843 academic institutes, 22,744 journals, 1,278 conferences and 3 special affiliations. In total, AceKG consists of 2.2 billion pieces of relationship information. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Resource list for AI.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jiyuan/ainote/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Thu, 23 Dec 2021 22:03:53 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jiyuan/ainote/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "jiyuan/ainote",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/jiyuan/ainote/master/scoreLog_S23_V_1_0.ipynb",
      "https://raw.githubusercontent.com/jiyuan/ainote/master/note/bayesian.ipynb",
      "https://raw.githubusercontent.com/jiyuan/ainote/master/note/Credit%20Card%20Fraud%20Detection.ipynb",
      "https://raw.githubusercontent.com/jiyuan/ainote/master/note/Probability.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8556547851109332
      ],
      "excerpt": "Tensorflow Tutorials using Jupyter Notebook \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jiyuan/ainote/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Resource List",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "ainote",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "jiyuan",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jiyuan/ainote/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Thu, 23 Dec 2021 22:03:53 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "https://github.com/krasserm/machine-learning-notebooks\n\nStanford Machine Learning course exercises implemented with scikit-learn\n\n",
      "technique": "Header extraction"
    }
  ]
}