{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Thanks to Alex Rogozhnikov [@arogozhnikov](https://github.com/arogozhnikov) for the awesome einops package. \nFor my re-implementations I have studied and borrowed code from many repositories of Phil Wang [@lucidrains](https://github.com/lucidrains). \nBy studying  his code I have managed to grasp self-attention, discover nlp stuff that are never\nreferred in the papers, and learn from his clean coding style.\n\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1706.03762.\n2. Wang, H., Zhu, Y., Green, B., Adam, H., Yuille, A., & Chen, L. C. (2020, August). Axial-deeplab: Stand-alone axial-attention for panoptic segmentation. In European Conference on Computer Vision (pp. 108-126). Springer, Cham.\n3. Srinivas, A., Lin, T. Y., Parmar, N., Shlens, J., Abbeel, P., & Vaswani, A. (2021). Bottleneck Transformers for Visual Recognition. arXiv preprint https://arxiv.org/abs/2101.11605.  \n4. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... & Houlsby, N. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint https://arxiv.org/abs/2010.11929.\n5. Ramachandran, P., Parmar, N., Vaswani, A., Bello, I., Levskaya, A., & Shlens, J. (2019). Stand-alone self-attention in vision models. arXiv preprint https://arxiv.org/abs/1906.05909.\n6. Chen, J., Lu, Y., Yu, Q., Luo, X., Adeli, E., Wang, Y., ... & Zhou, Y. (2021). Transunet: Transformers make strong encoders for medical image segmentation. arXiv preprint https://arxiv.org/abs/2102.04306.\n7. Wang, S., Li, B., Khabsa, M., Fang, H., & Ma, H. (2020). Linformer: Self-attention with linear complexity. arXiv preprint https://arxiv.org/abs/2006.04768.\n8. Bertasius, G., Wang, H., & Torresani, L. (2021). Is Space-Time Attention All You Need for Video Understanding?. arXiv preprint https://arxiv.org/abs/2102.05095.\n9. Shaw, P., Uszkoreit, J., & Vaswani, A. (2018). Self-attention with relative position representations. arXiv preprint https://arxiv.org/abs/1803.02155.\n\n## Support\nIf you really like this repository and find it useful, please consider (\u2605) starring it, so that it can reach a broader audience of like-minded people. It would be highly appreciated :) !\n\n\n",
      "https://arxiv.org/abs/2101.11605.  \n4. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... & Houlsby, N. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint https://arxiv.org/abs/2010.11929.\n5. Ramachandran, P., Parmar, N., Vaswani, A., Bello, I., Levskaya, A., & Shlens, J. (2019). Stand-alone self-attention in vision models. arXiv preprint https://arxiv.org/abs/1906.05909.\n6. Chen, J., Lu, Y., Yu, Q., Luo, X., Adeli, E., Wang, Y., ... & Zhou, Y. (2021). Transunet: Transformers make strong encoders for medical image segmentation. arXiv preprint https://arxiv.org/abs/2102.04306.\n7. Wang, S., Li, B., Khabsa, M., Fang, H., & Ma, H. (2020). Linformer: Self-attention with linear complexity. arXiv preprint https://arxiv.org/abs/2006.04768.\n8. Bertasius, G., Wang, H., & Torresani, L. (2021). Is Space-Time Attention All You Need for Video Understanding?. arXiv preprint https://arxiv.org/abs/2102.05095.\n9. Shaw, P., Uszkoreit, J., & Vaswani, A. (2018). Self-attention with relative position representations. arXiv preprint https://arxiv.org/abs/1803.02155.\n\n## Support\nIf you really like this repository and find it useful, please consider (\u2605) starring it, so that it can reach a broader audience of like-minded people. It would be highly appreciated :) !\n\n\n",
      "https://arxiv.org/abs/2010.11929.\n5. Ramachandran, P., Parmar, N., Vaswani, A., Bello, I., Levskaya, A., & Shlens, J. (2019). Stand-alone self-attention in vision models. arXiv preprint https://arxiv.org/abs/1906.05909.\n6. Chen, J., Lu, Y., Yu, Q., Luo, X., Adeli, E., Wang, Y., ... & Zhou, Y. (2021). Transunet: Transformers make strong encoders for medical image segmentation. arXiv preprint https://arxiv.org/abs/2102.04306.\n7. Wang, S., Li, B., Khabsa, M., Fang, H., & Ma, H. (2020). Linformer: Self-attention with linear complexity. arXiv preprint https://arxiv.org/abs/2006.04768.\n8. Bertasius, G., Wang, H., & Torresani, L. (2021). Is Space-Time Attention All You Need for Video Understanding?. arXiv preprint https://arxiv.org/abs/2102.05095.\n9. Shaw, P., Uszkoreit, J., & Vaswani, A. (2018). Self-attention with relative position representations. arXiv preprint https://arxiv.org/abs/1803.02155.\n\n## Support\nIf you really like this repository and find it useful, please consider (\u2605) starring it, so that it can reach a broader audience of like-minded people. It would be highly appreciated :) !\n\n\n",
      "https://arxiv.org/abs/1906.05909.\n6. Chen, J., Lu, Y., Yu, Q., Luo, X., Adeli, E., Wang, Y., ... & Zhou, Y. (2021). Transunet: Transformers make strong encoders for medical image segmentation. arXiv preprint https://arxiv.org/abs/2102.04306.\n7. Wang, S., Li, B., Khabsa, M., Fang, H., & Ma, H. (2020). Linformer: Self-attention with linear complexity. arXiv preprint https://arxiv.org/abs/2006.04768.\n8. Bertasius, G., Wang, H., & Torresani, L. (2021). Is Space-Time Attention All You Need for Video Understanding?. arXiv preprint https://arxiv.org/abs/2102.05095.\n9. Shaw, P., Uszkoreit, J., & Vaswani, A. (2018). Self-attention with relative position representations. arXiv preprint https://arxiv.org/abs/1803.02155.\n\n## Support\nIf you really like this repository and find it useful, please consider (\u2605) starring it, so that it can reach a broader audience of like-minded people. It would be highly appreciated :) !\n\n\n",
      "https://arxiv.org/abs/2102.04306.\n7. Wang, S., Li, B., Khabsa, M., Fang, H., & Ma, H. (2020). Linformer: Self-attention with linear complexity. arXiv preprint https://arxiv.org/abs/2006.04768.\n8. Bertasius, G., Wang, H., & Torresani, L. (2021). Is Space-Time Attention All You Need for Video Understanding?. arXiv preprint https://arxiv.org/abs/2102.05095.\n9. Shaw, P., Uszkoreit, J., & Vaswani, A. (2018). Self-attention with relative position representations. arXiv preprint https://arxiv.org/abs/1803.02155.\n\n## Support\nIf you really like this repository and find it useful, please consider (\u2605) starring it, so that it can reach a broader audience of like-minded people. It would be highly appreciated :) !\n\n\n",
      "https://arxiv.org/abs/2006.04768.\n8. Bertasius, G., Wang, H., & Torresani, L. (2021). Is Space-Time Attention All You Need for Video Understanding?. arXiv preprint https://arxiv.org/abs/2102.05095.\n9. Shaw, P., Uszkoreit, J., & Vaswani, A. (2018). Self-attention with relative position representations. arXiv preprint https://arxiv.org/abs/1803.02155.\n\n## Support\nIf you really like this repository and find it useful, please consider (\u2605) starring it, so that it can reach a broader audience of like-minded people. It would be highly appreciated :) !\n\n\n",
      "https://arxiv.org/abs/2102.05095.\n9. Shaw, P., Uszkoreit, J., & Vaswani, A. (2018). Self-attention with relative position representations. arXiv preprint https://arxiv.org/abs/1803.02155.\n\n## Support\nIf you really like this repository and find it useful, please consider (\u2605) starring it, so that it can reach a broader audience of like-minded people. It would be highly appreciated :) !\n\n\n",
      "https://arxiv.org/abs/1803.02155.\n\n## Support\nIf you really like this repository and find it useful, please consider (\u2605) starring it, so that it can reach a broader audience of like-minded people. It would be highly appreciated :) !\n\n\n"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. arXiv preprint arXiv:1706.03762.\n2. Wang, H., Zhu, Y., Green, B., Adam, H., Yuille, A., & Chen, L. C. (2020, August). Axial-deeplab: Stand-alone axial-attention for panoptic segmentation. In European Conference on Computer Vision (pp. 108-126). Springer, Cham.\n3. Srinivas, A., Lin, T. Y., Parmar, N., Shlens, J., Abbeel, P., & Vaswani, A. (2021). Bottleneck Transformers for Visual Recognition. arXiv preprint arXiv:2101.11605.  \n4. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... & Houlsby, N. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.\n5. Ramachandran, P., Parmar, N., Vaswani, A., Bello, I., Levskaya, A., & Shlens, J. (2019). Stand-alone self-attention in vision models. arXiv preprint arXiv:1906.05909.\n6. Chen, J., Lu, Y., Yu, Q., Luo, X., Adeli, E., Wang, Y., ... & Zhou, Y. (2021). Transunet: Transformers make strong encoders for medical image segmentation. arXiv preprint arXiv:2102.04306.\n7. Wang, S., Li, B., Khabsa, M., Fang, H., & Ma, H. (2020). Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768.\n8. Bertasius, G., Wang, H., & Torresani, L. (2021). Is Space-Time Attention All You Need for Video Understanding?. arXiv preprint arXiv:2102.05095.\n9. Shaw, P., Uszkoreit, J., & Vaswani, A. (2018). Self-attention with relative position representations. arXiv preprint arXiv:1803.02155.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```\n@article{adaloglou2021transformer,\n    title   = \"Transformers in Computer Vision\",\n    author  = \"Adaloglou, Nikolas\",\n    journal = \"https://theaisummer.com/\",\n    year    = \"2021\",\n    howpublished = {https://github.com/The-AI-Summer/self-attention-cv},\n  }\n```\n \n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9797704256806975,
        0.8014766508353657
      ],
      "excerpt": "Focused on computer vision self-attention modules. \nHow Attention works in Deep Learning \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9920934132822626
      ],
      "excerpt": "Why multi-head self attention works: math, intuitions and 10+1 hidden insights \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9285156578933775
      ],
      "excerpt": "mask = torch.zeros(10, 10)  #: tokens X tokens \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9285156578933775
      ],
      "excerpt": "mask = torch.zeros(10, 10)  #: tokens X tokens \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "                        blocks=6, num_classes=10,  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "y = model2(x) #: [2,10] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8444342525991423,
        0.8955886365383559
      ],
      "excerpt": "inp = torch.rand(1, 512, 32, 32) \nbottleneck_block = BottleneckBlock(in_channels=512, fmap_size=(32, 32), heads=4, out_channels=1024, pooling=True) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/The-AI-Summer/self-attention-cv",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-01-31T21:28:59Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-24T13:57:24Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9295678227075863
      ],
      "excerpt": "Implementation of self attention mechanisms for computer vision in PyTorch with einsum and einops. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8534526170950241,
        0.9513253207369936
      ],
      "excerpt": "How Attention works in Deep Learning \nHow Transformers work in deep learning and NLP \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8361852053724047,
        0.8473151619807399
      ],
      "excerpt": "Understanding einsum for Deep learning: implement a transformer with multi-head self-attention from scratch \nHow Positional Embeddings work in Self-Attention \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = MultiHeadSelfAttention(dim=64) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = AxialAttentionBlock(in_channels=256, dim=64, heads=8) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "y = model(x) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = TransUnet(in_channels=3, img_dim=128, vit_blocks=8, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "y = model(a) #: [2, 5, 128, 128] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = AbsPosEmb1D(tokens=20, dim_head=64) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877,
        0.860059181823877
      ],
      "excerpt": "y1 = model(q) \nmodel = RelPosEmb1D(tokens=20, dim_head=64, heads=3) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "y2 = model(q) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9838020811471998,
        0.860059181823877
      ],
      "excerpt": "dim = 32  #: spatial dim of the feat map \nmodel = RelPosEmb2D( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "y = model(q) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Implementation of various self-attention mechanisms focused on computer vision. Ongoing repository. ",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/The-AI-Summer/self_attention/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 90,
      "date": "Sun, 26 Dec 2021 16:03:09 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/The-AI-Summer/self-attention-cv/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "The-AI-Summer/self-attention-cv",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/The-AI-Summer/self_attention/main/notebooks/torch_io_tutorial_for_transf3d_seg.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```$ pip install self-attention-cv``` \n\nIt would be nice to pre-install pytorch in your environment, in case you don't have a GPU. To run the tests from the terminal \n```$ pytest``` you may need to run ``` export PYTHONPATH=$PATHONPATH:`pwd` ``` before.\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "from self_attention_cv import MultiHeadSelfAttention \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "from self_attention_cv import AxialAttentionBlock \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "from self_attention_cv import TransformerEncoder \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516,
        0.8246611022973888
      ],
      "excerpt": "from self_attention_cv import ViT, ResNet50ViT \nmodel1 = ResNet50ViT(img_dim=128, pretrained_resnet=False,  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "from self_attention_cv.transunet import TransUnet \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8132816484822137
      ],
      "excerpt": "model = TransUnet(in_channels=3, img_dim=128, vit_blocks=8, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8002443583192153
      ],
      "excerpt": "y = model(a) #: [2, 5, 128, 128] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "from self_attention_cv.bottleneck_transformer import BottleneckBlock \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "from self_attention_cv.pos_embeddings import AbsPosEmb1D,RelPosEmb1D \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "from self_attention_cv.pos_embeddings import RelPosEmb2D \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/The-AI-Summer/self-attention-cv/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2021 AI Summer\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Self-attention building blocks for computer vision applications in PyTorch",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "self-attention-cv",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "The-AI-Summer",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/The-AI-Summer/self-attention-cv/blob/main/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "black0017",
        "body": "Update pip package Unetr fix v2",
        "dateCreated": "2021-07-26T07:21:18Z",
        "datePublished": "2021-07-26T09:50:13Z",
        "html_url": "https://github.com/The-AI-Summer/self-attention-cv/releases/tag/1.2.3",
        "name": "Update pip package Unetr fix v2",
        "tag_name": "1.2.3",
        "tarball_url": "https://api.github.com/repos/The-AI-Summer/self-attention-cv/tarball/1.2.3",
        "url": "https://api.github.com/repos/The-AI-Summer/self-attention-cv/releases/46756968",
        "zipball_url": "https://api.github.com/repos/The-AI-Summer/self-attention-cv/zipball/1.2.3"
      },
      {
        "authorType": "User",
        "author_name": "black0017",
        "body": "Experimentation with the UneTR architecture to get an architecture that can be trained on 3d segmentation",
        "dateCreated": "2021-07-26T07:21:18Z",
        "datePublished": "2021-07-26T09:36:49Z",
        "html_url": "https://github.com/The-AI-Summer/self-attention-cv/releases/tag/1.2.2",
        "name": "Fix UneTR v2 to be closer to the MONAI implementation. To be tested on brats datasets along with tutorial blog",
        "tag_name": "1.2.2",
        "tarball_url": "https://api.github.com/repos/The-AI-Summer/self-attention-cv/tarball/1.2.2",
        "url": "https://api.github.com/repos/The-AI-Summer/self-attention-cv/releases/46756308",
        "zipball_url": "https://api.github.com/repos/The-AI-Summer/self-attention-cv/zipball/1.2.2"
      },
      {
        "authorType": "User",
        "author_name": "black0017",
        "body": "Experimentation with the UneTR architecture to get an architecture that can be trained on 3d segmentation",
        "dateCreated": "2021-07-22T14:40:57Z",
        "datePublished": "2021-07-22T14:50:00Z",
        "html_url": "https://github.com/The-AI-Summer/self-attention-cv/releases/tag/1.2.1",
        "name": "Fix UneTR",
        "tag_name": "1.2.1",
        "tarball_url": "https://api.github.com/repos/The-AI-Summer/self-attention-cv/tarball/1.2.1",
        "url": "https://api.github.com/repos/The-AI-Summer/self-attention-cv/releases/46611404",
        "zipball_url": "https://api.github.com/repos/The-AI-Summer/self-attention-cv/zipball/1.2.1"
      },
      {
        "authorType": "User",
        "author_name": "black0017",
        "body": " add tests on cuda if available, UNETTR paper, absolute_positional_encodings_1d, update readme for running pytests",
        "dateCreated": "2021-06-30T10:34:08Z",
        "datePublished": "2021-06-30T10:43:37Z",
        "html_url": "https://github.com/The-AI-Summer/self-attention-cv/releases/tag/1.2",
        "name": "v1.2",
        "tag_name": "1.2",
        "tarball_url": "https://api.github.com/repos/The-AI-Summer/self-attention-cv/tarball/1.2",
        "url": "https://api.github.com/repos/The-AI-Summer/self-attention-cv/releases/45481254",
        "zipball_url": "https://api.github.com/repos/The-AI-Summer/self-attention-cv/zipball/1.2"
      },
      {
        "authorType": "User",
        "author_name": "black0017",
        "body": "Added pos. enc",
        "dateCreated": "2021-03-02T19:16:45Z",
        "datePublished": "2021-03-02T19:18:01Z",
        "html_url": "https://github.com/The-AI-Summer/self-attention-cv/releases/tag/1.1.0",
        "name": "v1.1.0",
        "tag_name": "1.1.0",
        "tarball_url": "https://api.github.com/repos/The-AI-Summer/self-attention-cv/tarball/1.1.0",
        "url": "https://api.github.com/repos/The-AI-Summer/self-attention-cv/releases/39138090",
        "zipball_url": "https://api.github.com/repos/The-AI-Summer/self-attention-cv/zipball/1.1.0"
      },
      {
        "authorType": "User",
        "author_name": "SergiosKar",
        "body": "https://pypi.org/project/self-attention-cv/",
        "dateCreated": "2021-02-15T00:16:19Z",
        "datePublished": "2021-02-15T13:42:08Z",
        "html_url": "https://github.com/The-AI-Summer/self-attention-cv/releases/tag/1.0.0",
        "name": "",
        "tag_name": "1.0.0",
        "tarball_url": "https://api.github.com/repos/The-AI-Summer/self-attention-cv/tarball/1.0.0",
        "url": "https://api.github.com/repos/The-AI-Summer/self-attention-cv/releases/38061376",
        "zipball_url": "https://api.github.com/repos/The-AI-Summer/self-attention-cv/zipball/1.0.0"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 682,
      "date": "Sun, 26 Dec 2021 16:03:09 GMT"
    },
    "technique": "GitHub API"
  },
  "support": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you really like this repository and find it useful, please consider (\u2605) starring it, so that it can reach a broader audience of like-minded people. It would be highly appreciated :) !\n\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "deep-learning",
      "transformer",
      "transformers",
      "self-attention",
      "attention-mechanism",
      "attention",
      "machine-learning",
      "machine-learning-algorithms",
      "artificial-intelligence"
    ],
    "technique": "GitHub API"
  }
}