{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2101.11986*.\n\n- Tensorflow documentation tutorial \"Transformer model for language understanding.\" I found this after fully completing the model and found the attention mask was incorrect. My use of \"tf.linalg.band_part\" (only"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- \"Attention is All You Need.\" \n - Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. NIPS (2017). *https://research.google/pubs/pub46201/*\n\n- \"Convolutional Sequence to Sequence Learning.\"\n \n  - Gehring, J., Auli, M., Grangier, D., Yarats, D. & Dauphin, Y.N.. (2017). Convolutional Sequence to Sequence Learning. Proceedings of the 34th International Conference on Machine Learning, in Proceedings of Machine Learning Research 70:1243-1252, *http://proceedings.mlr.press/v70/gehring17a.html.*\n\n\n- \"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.\"\n \n  - Mingxing Tan, Quoc V. Le (2019). Convolutional Sequence to Sequence Learning. International Conference on Machine Learning. *http://arxiv.org/abs/1905.11946.*\n\n\n-  \"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention.\"\n  -  Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., Zemel, R. & Bengio, Y.. (2015). Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. Proceedings of the 32nd International Conference on Machine Learning, in Proceedings of Machine Learning Research 37:2048-2057. *http://proceedings.mlr.press/v37/xuc15.html.* \n            \n\n- \"Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet\"\n\n  - Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH Tay, Jiashi Feng, Shuicheng Yan. Preprint (2021). *https://arxiv.org/abs/2101.11986*.\n\n- Tensorflow documentation tutorial \"Transformer model for language understanding.\" I found this after fully completing the model and found the attention mask was incorrect. My use of \"tf.linalg.band_part\" (only) is due to this tutorial. *www.tensorflow.org/text/tutorials/transformer#masking*\n\n- Special thanks to [Darien Schettler](https://www.kaggle.com/dschettler8845/bms-efficientnetv2-tpu-e2e-pipeline-in-3hrs/notebook.) for leading readers to the \"Show\" and \"Attention\" papers cited above, using *session.run()* to improve inference speed in distributed settings and providing detailed info on creating TF Records. This work is otherwise derived independently from his.\n\n- It is possible my idea of a Beam Search Alternative is based on a lecture video from DeepLearning.ai's [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning)  on Coursera.\n\n- **Dataset / Kaggle Competition:** \"Bristol-Myers Squibb \u2013 Molecular Translation\" competition on Kaggle (2021). *https://www.kaggle.com/c/bms-molecular-translation*\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mvenouziou/Project-Attention-Is-What-You-Get",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-06-03T00:14:27Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-07T16:57:24Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9069836298972459,
        0.8031459223148889
      ],
      "excerpt": "International Chemical Identifiers (\"InChI values\") are a standardized encoding to describe chemical compounds. They take the form of a string of letters, numbers and deliminators, often between 100 - 400 characters long.  \nThe chemical diagrams are provided as PNG files, often of such low quality that it may take a human several seconds to decipher.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.871096316822838,
        0.8977526981365604,
        0.9256266259680668
      ],
      "excerpt": "Image CNN + Attention Features encoder --> text Attention + (optional )CNN feature layer decoder. \nThis is a hybrid approach with: \nImage Encoder from Show, Attend and Tell: Neural Image Caption Generation with Visual Attention.  Generate image feature vectors using intermediate layer outputs from a pretrained CNN. (Here I use the more modern EfficientNet model (recommended by Darien Schettler) with fixed weights and a trainable Dense layer for customization.) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.873697686016785,
        0.8872425100364966,
        0.8509037347944834
      ],
      "excerpt": "PLUS (optional): Decoder Output Blocks placed in Series (not stacked). Increase the number of trainable parameters without adding inference computational complexity, while also allowing decoders to specialize on different regions of the output. \nPLUS (optional): Is attention really all you need? Add a convolutional layer to enhance text features before decoder self-attention to experiment with performance differences with and without extra convolutional layer(s). Use of CNN's in NLP comes from Convolutional Sequence to Sequence Learning \nPLUS (optional): Beam-Search Alternative, an extra decoding layer applied after the full logits prediction has been made. This takes the form of a bidirectional RNN with attention, applied to the full logits sequence. Because a full (initial) prediction has already been made, computations can be parallelized using statefull RNNs. (See more details below.) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8731772906043802,
        0.9206589293682377
      ],
      "excerpt": "(Low priority, specific to Kaggle's TPU implementation.) Fix \"session.run()\" TPU calls on Kaggle. (It works correctly on Colab.) This severely impacts inference speed on Kaggle. \nExperiment with \"Tokens-to-Token ViT\" in place of the image CNN. (Technique from Training Vision Transformers from Scratch on ImageNet \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9990895343302063,
        0.9249990516818581,
        0.8121219124350133
      ],
      "excerpt": "Beam search is a technique to modify model predictions to reflect the (local) maximum likelihood estimate. However, it is very local in that computation expense increases quickly with the number of character steps taken into account. This is also a hard-coded algorithm, which is somewhat contrary to the philosophy of deep learning. \nA Beam-search Alternative would be an extra decoding layer applied after the full logits prediction has been made. This might be in the form of a stateful, bidirectional RNN that is computationally parallizable because it is applied to the full logits sequence. \nNeed to revamp code to accept main model changes made for TPU support. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mvenouziou/Project-Attention-Is-What-You-Get/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 09 Dec 2021 12:01:17 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/mvenouziou/Project-Attention-Is-What-You-Get/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "mvenouziou/Project-Attention-Is-What-You-Get",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/mvenouziou/Project-Attention-Is-What-You-Get/main/bms_molecular_translation_AttentionIsWhatYouGet.ipynb"
    ],
    "technique": "File Exploration"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/mvenouziou/Project-Attention-Is-What-You-Get/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Attention is What You Get",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Project-Attention-Is-What-You-Get",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "mvenouziou",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mvenouziou/Project-Attention-Is-What-You-Get/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 09 Dec 2021 12:01:17 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "*This is my entry into the [Bristol-Myers Squibb Molecular Translation](https://www.kaggle.com/c/bms-molecular-translation)  Kaggle competition. The notebook is publicly available at https://www.kaggle.com/c/bms-molecular-translation/discussion/url.*\n\nKagglers have coalesced around \"Attention is What You Need\" models, so I ask, *Is attention really all you need?*  This notebook include features to test that out: Enable/Disable CNN text feature extraction before the decoder self-attention; Increase model parameters without harming inference speed using decoder heads in series; and Experiment with my trainable & parallelizable alternative to beam search.\n\n----\n",
      "technique": "Header extraction"
    }
  ]
}