{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1803.07416](http://arxiv.org/abs/1803.07416)\n\n[4] Dai et al. \"Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context\". 2018. [https://arxiv.org/abs/1901.02860](http://arxiv.org/abs/1901.02860)",
      "https://arxiv.org/abs/1901.02860](http://arxiv.org/abs/1901.02860)"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[1] The Unreasonable Effectiveness of Recurrent Neural Networks\nhttp://karpathy.github.io/2015/05/21/rnn-effectiveness/\n\n[2] Vaswani, et al. \"Attention is all you need.\" Advances in Neural Information Processing Systems. 2017.\n\n[3] Vaswani et al. \"Tensor2Tensor for Neural Machine Translation\". 2018. [arXiv:1803.07416](http://arxiv.org/abs/1803.07416)\n\n[4] Dai et al. \"Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context\". 2018. [arXiv:1901.02860](http://arxiv.org/abs/1901.02860)\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8153821062667099
      ],
      "excerpt": "inzva AI Projects #2 - Fake Academic Paper Generation Project \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/inzva/fake-academic-paper-generation",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-03-19T08:36:41Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-09-19T16:27:11Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9926757424283952,
        0.8071897008699597
      ],
      "excerpt": "In this work, we tackle the problem of structured text generation, specifically academic paper generation in LaTeX, inspired by the surprisingly good results of basic character-level language models. Our motivation is using more recent and advanced methods of language modeling on a more complex dataset of LaTeX source files to generate realistic academic papers. Our first contribution is preparing a dataset with LaTeX source files on recent open-source computer vision papers. Our second contribution is experimenting with recent methods of language modeling and text generation such as Transformer and Transformer-XL to generate consistent LaTeX code. We report cross-entropy and bits-per-character (BPC) results of the trained models, and we also discuss interesting points on some examples of the generated LaTeX code. \nNote: We decided not to share the dataset because of ethical concerns. However, the code can be used to recreate the dataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9339335173498448
      ],
      "excerpt": "dataset_generation/complete_dataset.py (kind of combination of all these scripts which finds problematic source files and replaces them with other papers from the paperlinks.txt) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9139198882310565,
        0.9829490709302274
      ],
      "excerpt": "Using this specified process, we downloaded 4-5 GB source files for papers since source files include images etc. which are not need for our purpose. At the end, we have 799 latex files each for an academic paper. Before preprocessing, this is approximately equal to 46 MB of latex. \nDataset is needed to be preprocessed because of noise such as created by comments and non-UTF characters. Therefore, we used preprocess_char.py to delete comments and characters that used below a certain threshold, in our experiments it is 100.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9862375813367508
      ],
      "excerpt": "For our baseline model, we decided to use character level embedding. The details of the preprocessed char-based dataset is given below. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "inzva AI Projects #2 - Fake Academic Paper Generation Project",
      "technique": "GitHub API"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We downloaded the source files as tar files for the selected papers and untar/unzip them.\n\nrelated script: **[dataset_generation/downloader.py](dataset_generation/downloader.py)** (reads selected papers from **selected_papers.txt**, downloads the source files and untar/unzip them)\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/inzva/fake-academic-paper-generation/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 8,
      "date": "Mon, 13 Dec 2021 13:56:04 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/inzva/fake-academic-paper-generation/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "inzva/fake-academic-paper-generation",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/inzva/fake-academic-paper-generation/master/transformer-xl/pytorch/run_papers_base_2gpus.sh",
      "https://raw.githubusercontent.com/inzva/fake-academic-paper-generation/master/transformer-xl/pytorch/generate2.sh",
      "https://raw.githubusercontent.com/inzva/fake-academic-paper-generation/master/transformer-xl/pytorch/run_papers_base.sh",
      "https://raw.githubusercontent.com/inzva/fake-academic-paper-generation/master/transformer-xl/pytorch/slurm.sh",
      "https://raw.githubusercontent.com/inzva/fake-academic-paper-generation/master/transformer-xl/pytorch/slurm_2gpus.sh",
      "https://raw.githubusercontent.com/inzva/fake-academic-paper-generation/master/transformer-xl/pytorch/generate.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To the best of our knowledge there was no available dataset compiled from academic papers. Therefore we decided to prepare a dataset from academic papers on arxiv.org. \n\nAll scripts related to the dataset preparation can be found in the **[dataset_generation](dataset_generation)** directory.\n\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8881972322573726
      ],
      "excerpt": "* dataset_generation/random_paper_sampler.py (samples examples from paperlinks.txt and writes the result to selected_papers.txt) \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/inzva/fake-academic-paper-generation/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 inzva\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Neural Academic Paper Generation",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "fake-academic-paper-generation",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "inzva",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/inzva/fake-academic-paper-generation/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- NumPy\n- TexSoup (for dataset preparation)\n- BeautifulSoup (for dataset preparation)\n- Tensorflow 1.12 (for RNN)\n- Tensor2Tensor 1.13.4 (for Transformer)\n- PyTorch (for Transformer-XL)\n\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "After preparing the dataset, run **[char-rnn.py](char-rnn.py)** to train the model.\n\nWhen training is over, run **[generate_text.py](generate_text.py)**. This script will load the last\ncheckpoint and generate a number of characters using the learned parameters.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "We use [Tensor2Tensor](https://github.com/tensorflow/tensor2tensor) [3] for Transformer model. See **[t2t_paper_generation_problem](t2t_paper_generation_problem)** directory for details.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "We use the original code shared by the authors who propose Transformer-XL. See **[transformer-xl](transformer-xl)** directory for details.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 34,
      "date": "Mon, 13 Dec 2021 13:56:04 GMT"
    },
    "technique": "GitHub API"
  }
}