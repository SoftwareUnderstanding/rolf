{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* [A Code-First Introduction to NLP](https://github.com/fastai/course-nlp)\n* [Universal Language Model Fine-Tuning (ULMFiT)](https://humboldt-wi.github.io/blog/research/information_systems_1819/group4_ulmfit)  \n* [NLP & fastai | MultiFiT](https://mc.ai/nlp-fastai-multifit/)\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1801.06146",
      "https://arxiv.org/abs/1801.06146",
      "https://arxiv.org/abs/1909.04761"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9901087270882659
      ],
      "excerpt": "\u2192 :page_with_curl: See arxiv.org/abs/1801.06146 for paper  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9979803913522193
      ],
      "excerpt": "Reference: Efficient multi-lingual language model fine-tuning by Sebastian Ruder and Julian Eisenschlos (http://nlp.fast.ai/classification/2019/09/10/multifit.html) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lukexyz/Language-Models",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-12-03T13:00:17Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-02-25T07:15:00Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.904157852767677,
        0.8184629537852376
      ],
      "excerpt": "Sentiment analysis via prediction of restaurant reviews using ULMFiT (2018), a state-of-the-art method (for 2018) which provides a framework for NLP transfer learning. (https://arxiv.org/abs/1801.06146) \nTo build the text classification model, there are three stages:   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9530022122773476
      ],
      "excerpt": "A pretrained AWD-LSTM SequentialRNN is imported, which works as a sequence generator (i.e. predicts the next word) for a general-domain corpus, in our case the WikiText103 dataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9774419403477393
      ],
      "excerpt": "The AWD-LSTM Language Model is fine-tuned on the domain-specific corpus (Yelp reviews), to be able to generate fake restaurant reviews. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9687739786659914
      ],
      "excerpt": "The embeddings learnt from these first two steps are imported into a new classifier model, which is then fine-tuned on the target task (star ratings) with gradual unfreezing of the final layers. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9810955183904586
      ],
      "excerpt": "After stage 2 of the process is complete, the AWD-LSTM RNN language model can now be used for synthetic text generation. The original RNN model was trained to predict the next word in the WikiText103 dataset, and we have fine-tuned this with our yelp corpus to predict the next word in a restaurant review. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8274182925633837,
        0.8490037945672047,
        0.9746869939145882
      ],
      "excerpt": "learn.predict(\"I hated the restaurant, the food tasted\") \nI hated the restaurant, the food tasted bad \nYou can generate reviews of any length. The output generally has a believable sentence structure, but they tend to lack higher-order coherency within a paragraph. This is because the RNN has no memory of the start of the sentence by the time it reaches the end of it. Larger transformer attention models like OpenAI GPT-2 or BERT do a better job at this. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8818191553924458,
        0.9886301177913067,
        0.9608002747359198
      ],
      "excerpt": "learn.predict(\"The food is good and the staff\", words=30, temperature=0.75) \nThe food is good and the staff is very friendly. We had the full menu and the Big Lots of Vegas. The food was ok, but there was nothing and this isn't a Chinese place. \nThe overall accuracy of the trained classifier was 0.665, which means that giving the model and un-seen restaurant review it can predict its rating (1-5 stars) correctly 66.5% of the time. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9225066771010385
      ],
      "excerpt": "(INPUT 25816) You can count on excellent quality and fresh baked goods daily. The patisseries are refined and always delicious. I am addicted to their home made salads and strong coffee. \\nYou can order customized cakes and impress your guests. Everything here is made with the finest ingredients. It never disappoints. \\n\\nThe service is formal. You are always treated with respect. Sometimes I don't mind when they call me Madame but I always correct them and ask to be called \\\"Mademoiselle, SVP!\\\"\\n\\nI guarantee you will return here many times.   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9685289887940381
      ],
      "excerpt": "(INPUT 28342) 8 of us just finished eating here.  Service was very friendly, prices were definitely reasonable, and we all really enjoyed our meals. \\n\\nI would come back again for sure!\\n\\nUnfortunately I didn't snap any photos of our food, but here are a few of the place.   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9183938955246174,
        0.9678872746343821
      ],
      "excerpt": "(INPUT 43756) The food was not all that.  The customer service was just okay. Don't get what all the rave is about?? \nPlotting an Actual vs. Predicted matrix gives us a visual representation of the accuracy of the model. True positives are highlighted on the diagonal. So even when it makes the prediction wrong - the error usually is only off by only 1 star.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9717205717117033,
        0.9874469949610054
      ],
      "excerpt": "In the paper MultiFiT: Efficient Multi-lingual Language Model Fine-tuning (2019), the transfer learning language model is improved using \n1. Subword Tokenization, which uses a mixture of character, subword and word tokens, depending on how common they are. These properties allow it to fit much better to multilingual models (non-english languages). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8571420046700927,
        0.9189411556960261,
        0.9128533183770037
      ],
      "excerpt": "Updates the AWD-LSTM base RNN network with a Quasi-Recurrent Neural Network (QRNN). The QRNN benefits from attributes from both a CNN and an LSTM: \nIt can be parallelized across time and minibatch dimensions like a CNN (for performance boost)  \nIt retains the LSTM\u2019s sequential bias (the output depends on the order of elements in the sequence). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9661257034970826
      ],
      "excerpt": "\"We find that our monolingual language models fine-tuned only on 100 labeled examples of the corresponding task in the target language outperform zero-shot inference (trained on 1000 examples in the source language) with multilingual BERT and LASER. MultiFit also outperforms the other methods when all models are fine-tuned on 1000 target language examples.\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": ":earth_africa::book::speech_balloon: Sentiment analysis and text generation using BERT and ULMFiT (2018)",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lukexyz/Language-Models/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sun, 05 Dec 2021 17:00:08 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/lukexyz/Language-Models/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "lukexyz/Language-Models",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/lukexyz/Language-Models/master/notebooks/02-ULMFiT-Yelp-Full-Train.ipynb",
      "https://raw.githubusercontent.com/lukexyz/Language-Models/master/notebooks/01-ULMFiT-Yelp.ipynb",
      "https://raw.githubusercontent.com/lukexyz/Language-Models/master/notebooks/03-BERT-DocumentClassification.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "    $ python -m fastai.utils.show_install\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "`Deep Learning AMI (Ubuntu 16.04) Version 25.3`, GPU `p2.xlarge` for training :ballot_box_with_check:, `120 GB`\n\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8838148168639296
      ],
      "excerpt": "Examples   \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/lukexyz/Language-Models/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 Luke\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "ULMFiT NLP Transfer Learning :earth_africa::book::speech_balloon:",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Language-Models",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "lukexyz",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lukexyz/Language-Models/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "    $ conda install jupyter notebook -y\n    $ conda install -c conda-forge jupyter_contrib_nbextensions\n    $ conda install fastai pytorch=1.0.0 -c fastai -c pytorch -c conda-forge\n\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "    $ jupyter notebook --ip=0.0.0.0 --no-browser\n    ",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Sun, 05 Dec 2021 17:00:08 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "language-models",
      "transformer",
      "bert",
      "ulm-fit"
    ],
    "technique": "GitHub API"
  }
}