{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "---\n\n```bibtex\n@article{DBLP:journals/corr/abs-1810-04805,\n  author    = {Jacob Devlin and\n               Ming{-}Wei Chang and\n               Kenton Lee and\n               Kristina Toutanova},\n  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language\n               Understanding},\n  journal   = {CoRR},\n  volume    = {abs/1810.04805},\n  year      = {2018},\n  url       = {http://arxiv.org/abs/1810.04805},\n  archivePrefix = {arXiv},\n  eprint    = {1810.04805},\n  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n\n```\n\n```bibtex\n@article{DBLP:journals/corr/abs-1907-11692,\n  author    = {Yinhan Liu and\n               Myle Ott and\n               Naman Goyal and\n               Jingfei Du and\n               Mandar Joshi and\n               Danqi Chen and\n               Omer Levy and\n               Mike Lewis and\n               Luke Zettlemoyer and\n               Veselin Stoyanov},\n  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},\n  journal   = {CoRR},\n  volume    = {abs/1907.11692},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1907.11692},\n  archivePrefix = {arXiv},\n  eprint    = {1907.11692},\n  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n\n```bibtex\n@article{DBLP:journals/corr/abs-1910-01108,\n  author    = {Victor Sanh and\n               Lysandre Debut and\n               Julien Chaumond and\n               Thomas Wolf},\n  title     = {DistilBERT, a distilled version of {BERT:} smaller, faster, cheaper\n               and lighter},\n  journal   = {CoRR},\n  volume    = {abs/1910.01108},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1910.01108},\n  archivePrefix = {arXiv},\n  eprint    = {1910.01108},\n  timestamp = {Tue, 02 Jun 2020 12:48:59 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-01108.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n\n```bibtex\n@article{DBLP:journals/corr/abs-1911-03894,\n  author    = {Louis Martin and\n               Benjamin M{\\\"{u}}ller and\n               Pedro Javier Ortiz Su{\\'{a}}rez and\n               Yoann Dupont and\n               Laurent Romary and\n               {\\'{E}}ric Villemonte de la Clergerie and\n               Djam{\\'{e}} Seddah and\n               Beno{\\^{\\i}}t Sagot},\n  title     = {CamemBERT: a Tasty French Language Model},\n  journal   = {CoRR},\n  volume    = {abs/1911.03894},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1911.03894},\n  archivePrefix = {arXiv},\n  eprint    = {1911.03894},\n  timestamp = {Sun, 01 Dec 2019 20:31:34 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-03894.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n\n```bibtex\n@article{dHoffschmidt2020FQuADFQ,\n  title={FQuAD: French Question Answering Dataset},\n  author={Martin d'Hoffschmidt and Maxime Vidal and Wacim Belblidia and Tom Brendl'e and Quentin Heinrich},\n  journal={ArXiv},\n  year={2020},\n  volume={abs/2002.06071}\n}\n```\n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "---\n\n - [Haystack](https://github.com/deepset-ai/haystack) ([deepset](https://deepset.ai))\n - [Transformers](https://github.com/huggingface/transformers) ([Hugging Face](https://huggingface.co))\n - Pre-trained Transformers Models on [Hugging Face's model hub](https://huggingface.co/models) \t\n    - [roberta-base-squad2](https://huggingface.co/deepset/roberta-base-squad2) (deepset)\n\t- [bert-large-uncased-whole-word-masking-squad2](https://huggingface.co/deepset/bert-large-uncased-whole-word-masking-squad2)(deepset)\n\t- [distilbert-base-cased-distilled-squad](https://huggingface.co/distilbert-base-cased-distilled-squad)\n\t- [camembert-base-fquad](https://huggingface.co/illuin/camembert-base-fquad) (illuin)\n- [Elasticsearch and Kibana](https://www.elastic.co)\n- [Flask](https://flask.palletsprojects.com/en/1.1.x/)\n- [Gunicorn](https://gunicorn.org)\n- [Bootstrap](https://getbootstrap.com)\n- [DropzoneJS](https://www.dropzonejs.com)\n- [Docker](https://www.docker.com)\n- [Sanjay R Kamath, PhD (Research Scientist, NLP)](https://twitter.com/sanjaykamath)\n- Open-Source Community \u2764\ufe0f\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{dHoffschmidt2020FQuADFQ,\n  title={FQuAD: French Question Answering Dataset},\n  author={Martin d'Hoffschmidt and Maxime Vidal and Wacim Belblidia and Tom Brendl'e and Quentin Heinrich},\n  journal={ArXiv},\n  year={2020},\n  volume={abs/2002.06071}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{DBLP:journals/corr/abs-1911-03894,\n  author    = {Louis Martin and\n               Benjamin M{\\\"{u}}ller and\n               Pedro Javier Ortiz Su{\\'{a}}rez and\n               Yoann Dupont and\n               Laurent Romary and\n               {\\'{E}}ric Villemonte de la Clergerie and\n               Djam{\\'{e}} Seddah and\n               Beno{\\^{\\i}}t Sagot},\n  title     = {CamemBERT: a Tasty French Language Model},\n  journal   = {CoRR},\n  volume    = {abs/1911.03894},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1911.03894},\n  archivePrefix = {arXiv},\n  eprint    = {1911.03894},\n  timestamp = {Sun, 01 Dec 2019 20:31:34 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-03894.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{DBLP:journals/corr/abs-1910-01108,\n  author    = {Victor Sanh and\n               Lysandre Debut and\n               Julien Chaumond and\n               Thomas Wolf},\n  title     = {DistilBERT, a distilled version of {BERT:} smaller, faster, cheaper\n               and lighter},\n  journal   = {CoRR},\n  volume    = {abs/1910.01108},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1910.01108},\n  archivePrefix = {arXiv},\n  eprint    = {1910.01108},\n  timestamp = {Tue, 02 Jun 2020 12:48:59 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-01108.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{DBLP:journals/corr/abs-1907-11692,\n  author    = {Yinhan Liu and\n               Myle Ott and\n               Naman Goyal and\n               Jingfei Du and\n               Mandar Joshi and\n               Danqi Chen and\n               Omer Levy and\n               Mike Lewis and\n               Luke Zettlemoyer and\n               Veselin Stoyanov},\n  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},\n  journal   = {CoRR},\n  volume    = {abs/1907.11692},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1907.11692},\n  archivePrefix = {arXiv},\n  eprint    = {1907.11692},\n  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{DBLP:journals/corr/abs-1810-04805,\n  author    = {Jacob Devlin and\n               Ming{-}Wei Chang and\n               Kenton Lee and\n               Kristina Toutanova},\n  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language\n               Understanding},\n  journal   = {CoRR},\n  volume    = {abs/1810.04805},\n  year      = {2018},\n  url       = {http://arxiv.org/abs/1810.04805},\n  archivePrefix = {arXiv},\n  eprint    = {1810.04805},\n  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.887167692142383
      ],
      "excerpt": "Elasticsearch (version 7.10) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeOfConduct": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://raw.githubusercontent.com/Karthik-Bhaskar/Context-Based-Question-Answering/main/CODE_OF_CONDUCT.md",
    "technique": "File Exploration"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Karthik-Bhaskar/Context-Based-Question-Answering",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "---\n\n [Karthik Bhaskar](https://www.karthikbhaskar.in/) - Feel free to contact me. \n\n <a href=\"https://github.com/Karthik-Bhaskar\" target=\"blank\"><img align=\"center\" src=\"https://cdn.jsdelivr.net/npm/simple-icons@3.0.1/icons/github.svg\" alt=\"Karthik-Bhaskar\" height=\"25\" width=\"30\" /></a>  <a href=\"https://twitter.com/karthik_bhaskar\" target=\"blank\"><img align=\"center\" src=\"https://cdn.jsdelivr.net/npm/simple-icons@3.0.1/icons/twitter.svg\" alt=\"karthik_bhaskar\" height=\"25\" width=\"30\" /></a>  <a href=\"https://linkedin.com/in/contactkarthikbhaskar\" target=\"blank\"><img align=\"center\" src=\"https://cdn.jsdelivr.net/npm/simple-icons@3.0.1/icons/linkedin.svg\" alt=\"contactkarthikbhaskar\" height=\"25\" width=\"30\" /></a>  \n  \n",
      "technique": "Header extraction"
    }
  ],
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "You are most welcome to contribute to this project. Be it a small typo correction or a feature enhancement \ud83d\ude42\nCurrently, there is no structure to follow for contribution, but you can open an Issue under Community contribution or contact me to discuss it before getting started.\nThe guidelines for contribution will be updated soon!",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-12-16T15:28:31Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-04T11:45:30Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "---\n\n>**Question answering**  (**QA**) is a computer science discipline within the fields of  [information retrieval](https://en.wikipedia.org/wiki/Information_retrieval \"Information retrieval\")  and  [natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing \"Natural language processing\")  (NLP), which is concerned with building systems that automatically answer questions posed by humans in a  [natural language](https://en.wikipedia.org/wiki/Natural_language \"Natural language\").\n>\n>Source: [Wikipedia](https://en.wikipedia.org/wiki/Question_answering)\n\n>**Extractive QA** is a popular task for natural language processing (NLP) research, where models must extract a short snippet from a document in order to answer a natural language question.\n>\n>  Source: [Facebook AI](https://ai.facebook.com/blog/research-in-brief-unsupervised-question-answering-by-cloze-translation/)\n\n\n\n**Context-Based Question Answering (CBQA)** is an inference web-based Extractive QA search engine, mainly dependent on [Haystack](https://github.com/deepset-ai/haystack) and [Transformers](https://github.com/huggingface/transformers) library.\n\nThe CBQA application allows the user to add context and perform Question Answering(QA) in that context.\n\nThe main components in this application use [Haystack's](https://github.com/deepset-ai/haystack) core components,\n\n> - **FileConverter**: Extracts pure text from files (pdf, docx, pptx, html and many more).\n> -  **PreProcessor**: Cleans and splits texts into smaller chunks.\n> -   **DocumentStore**: Database storing the documents, metadata and vectors for our search. We recommend Elasticsearch or FAISS, but have also more light-weight options for fast prototyping (SQL or In-Memory).\n>   - **Retriever**: Fast algorithms that identify candidate documents for a given query from a large collection of documents. Retrievers narrow down the search space significantly and are therefore key for scalable QA. Haystack supports sparse methods (TF-IDF, BM25, custom Elasticsearch queries) and state of the art dense methods (e.g. sentence-transformers and Dense Passage Retrieval)\n> - **Reader**: Neural network (e.g. BERT or RoBERTA) that reads through texts in detail to find an answer. The Reader takes multiple passages of text as input and returns top-n answers. Models are trained via  [FARM](https://github.com/deepset-ai/FARM)  or [Transformers](https://github.com/huggingface/transformers)  on SQuAD like tasks. You can just load a pretrained model from  [Hugging Face's model hub](https://huggingface.co/models)  or fine-tune it on your own domain data.\n>\n>Source: [Haystack's Key Components docs](https://github.com/deepset-ai/haystack/#key-components)\n\nIn CBQA the allowed formats for adding the context are,\n\n - Textual Context (Using the TextBox field)\n - File Uploads (.pdf, .txt, and .docx)\n\nThese contexts are uploaded to a temporary directory for each user for pre-processing and deletes after uploading them to **Elasticsearch,** which is the only **DocumentStore type** used in this system.\n\nEach user has a separate Elasticsearch index to store the context documents. Using the **PreProcessor** and the **FileConverter** modules from [Haystack](https://github.com/deepset-ai/haystack), the pre-processing and the extraction of text from context files is done.\n\n**Elasticsearcher Retriever** is used in CBQA to retrieve relevant documents based on the search query.\n\n**Transformers-based Readers** are used to extracting answers from the retrieved documents. The Readers used in CBQA are the pre-trained Transformers models hosted on the [Hugging Face's model hub](https://huggingface.co/models) .\n\nCurrently, CBQA provides the interface to perform QA in **English** and **French**, using four Transformers based models,\n\n - BERT\n - RoBERTa\n - DistilBERT\n - CamemBERT\n\nThe interface provides an option to choose the inference device between CPU and GPU.\n\nThe output is in a tabular form containing the following headers,\n\n-   **Answers** (Extracted answers based on the question and context)\n-   **Context** (Specifies the context window, related to the answer)\n-   **Document Title** (Specifies the title of context file, related to the answer)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8867430207897808,
        0.8906128909950556
      ],
      "excerpt": "Context-Based Question Answering is an easy-to-use Extractive QA search engine, which extracts answers to the question based on the provided context. \nIntroduction  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9872973363077815
      ],
      "excerpt": "The motivation to implement the CBQA project is to make an easy-to-use interface to perform Question Answering (QA) in multiple languages, with the option for using different pre-trained models.  Also, to enable organizations to use this project locally with minimal modifications or none. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8535989768210706,
        0.8243668675033635
      ],
      "excerpt": "* QA Language support (English and French) \n* Simultaneous user handling support (While using WSGI server) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.868797160903444
      ],
      "excerpt": "* Ability to plugin large scale document corpus for the QA engine \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Context-Based-Question-Answering",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Karthik-Bhaskar/Context-Based-Question-Answering/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 6,
      "date": "Sun, 12 Dec 2021 19:01:33 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Karthik-Bhaskar/Context-Based-Question-Answering/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Karthik-Bhaskar/Context-Based-Question-Answering",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Karthik-Bhaskar/Context-Based-Question-Answering/main/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "---\n\n\nThe main dependencies required to run the CBQA application is in the  [requirements.txt](./requirements.txt)\n\nTo install the dependencies,\n\n1.  Create a Python virtual environment with Python version 3.7 and activate it\n2.  Install dependency libraries using the following command in the terminal.\n```console  \n$ pip install -r requirements.txt\n```\n\nBefore executing the CBQA application, please start the Elasticsearch server. Elasticsearch is the DocumentStore type used in this application. To download and install the Elasticsearch, please check [here](https://www.elastic.co/downloads/elasticsearch).\n\nIn case you are using the docker environment, run Elasticsearch on docker using the following commands in the terminal. If you want to install the docker engine on your machine, please check [here](https://docs.docker.com/get-docker/).\n```console\n$ docker pull docker.elastic.co/elasticsearch/elasticsearch:7.10.0\n````\n```console\n$ docker run -p 9200:9200 -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:7.10.0\n```\n\nMake sure the Elasticsearch server is running using the following command in the new terminal.\n```console\n$ curl http://localhost:9200\n```\n You should get a response like the one below.\n```console\n{\n\"name\" : \"facddac422e8\",\n\"cluster_name\" : \"docker-cluster\",\n\"cluster_uuid\" : \"a4A3-yBhQdKBlpSDkpRelg\",\n\"version\" : {\n\"number\" : \"7.10.0\",\n\"build_flavor\" : \"default\",\n\"build_type\" : \"docker\",\n\"build_hash\" : \"51e9d6f22758d0374a0f3f5c6e8f3a7997850f96\",\n\"build_date\" : \"2020-11-09T21:30:33.964949Z\",\n\"build_snapshot\" : false,\n\"lucene_version\" : \"8.7.0\",\n\"minimum_wire_compatibility_version\" : \"6.8.0\",\n\"minimum_index_compatibility_version\" : \"6.0.0-beta1\"\n},\n\"tagline\" : \"You Know, for Search\"\n}\n```\n\n\nAfter installing the dependency libraries and starting the Elasticsearch server, you are good to go.\n\n- _To run the application using a WSGI server like [Gunicorn](https://gunicorn.org)._ Use the following command in the new terminal.\n  ```console\n  $ gunicorn -w 1 --threads 4 app:app\n  ```\n\n  This runs the application in Gunicorn server on [http://localhost:8000](http://localhost:8000/) with a single worker and 4 threads.\n\n- _To run the application in the [flask](https://flask.palletsprojects.com/en/1.1.x/) development server (Not recommended using this in production)_. Use the following command in the new terminal.\n  ```console\n  $ python app.py\n  ```\n\n  Now the application will be running in the flask development server on [http://localhost/5000](http://localhost/5000).\n\nIn the application execution cases above, you should see the below statement (date and time will be different) in the terminal after the application has started.\n```console\nUser tracker thread started @  2021-03-04 18:25:20.803277\n```\n\nThe above statement means that the thread handling the user connection has started, and the application is ready to accept users.\n\n\n**Note**: When performing QA using pre-trained models, at first use, the selected model gets downloaded from the [Hugging Face's model hub](https://huggingface.co/models) this may take a while depending on your internet speed. If you are re-starting the application while you are testing, make sure to remove the auto-created temporary user directories in the project and the user index on Elasticsearch (Will be fixed soon)\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "---\n\nBefore starting the installation, clone this repository using the following commands in the terminal.\n\n```console\n$ git clone https://github.com/Karthik-Bhaskar/Context-Based-Question-Answering.git\n````\n\n```console\n$ cd Context-Based-Question-Answering/\n```\n\nYou can get started using one of the two options,\n\n 1. [Installation using pip](#installation-using-pip)\n 2. [Running as a docker container ](#running-as-a-docker-container )\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9857659135807597,
        0.9717106327039013,
        0.9717106327039013
      ],
      "excerpt": "Python (version 3.7) \nHaystack  (version 0.7.0) \nTransformers (version 4.3.1) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9717106327039013,
        0.9473898066245818,
        0.9132793380272162,
        0.9717106327039013
      ],
      "excerpt": "Flask  (version 1.1.2) \nGunicorn (version 20.0.4) \nBootstrap (version 4.5.3) \njQuery (version 3.5.1) \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Karthik-Bhaskar/Context-Based-Question-Answering/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "JavaScript",
      "HTML",
      "Python",
      "CSS",
      "Dockerfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Apache License 2.0",
      "url": "https://api.github.com/licenses/apache-2.0"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'                                 Apache License\\n                           Version 2.0, January 2004\\n                        http://www.apache.org/licenses/\\n\\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\\n\\n   1. Definitions.\\n\\n      \"License\" shall mean the terms and conditions for use, reproduction,\\n      and distribution as defined by Sections 1 through 9 of this document.\\n\\n      \"Licensor\" shall mean the copyright owner or entity authorized by\\n      the copyright owner that is granting the License.\\n\\n      \"Legal Entity\" shall mean the union of the acting entity and all\\n      other entities that control, are controlled by, or are under common\\n      control with that entity. For the purposes of this definition,\\n      \"control\" means (i) the power, direct or indirect, to cause the\\n      direction or management of such entity, whether by contract or\\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\\n      outstanding shares, or (iii) beneficial ownership of such entity.\\n\\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\\n      exercising permissions granted by this License.\\n\\n      \"Source\" form shall mean the preferred form for making modifications,\\n      including but not limited to software source code, documentation\\n      source, and configuration files.\\n\\n      \"Object\" form shall mean any form resulting from mechanical\\n      transformation or translation of a Source form, including but\\n      not limited to compiled object code, generated documentation,\\n      and conversions to other media types.\\n\\n      \"Work\" shall mean the work of authorship, whether in Source or\\n      Object form, made available under the License, as indicated by a\\n      copyright notice that is included in or attached to the work\\n      (an example is provided in the Appendix below).\\n\\n      \"Derivative Works\" shall mean any work, whether in Source or Object\\n      form, that is based on (or derived from) the Work and for which the\\n      editorial revisions, annotations, elaborations, or other modifications\\n      represent, as a whole, an original work of authorship. For the purposes\\n      of this License, Derivative Works shall not include works that remain\\n      separable from, or merely link (or bind by name) to the interfaces of,\\n      the Work and Derivative Works thereof.\\n\\n      \"Contribution\" shall mean any work of authorship, including\\n      the original version of the Work and any modifications or additions\\n      to that Work or Derivative Works thereof, that is intentionally\\n      submitted to Licensor for inclusion in the Work by the copyright owner\\n      or by an individual or Legal Entity authorized to submit on behalf of\\n      the copyright owner. For the purposes of this definition, \"submitted\"\\n      means any form of electronic, verbal, or written communication sent\\n      to the Licensor or its representatives, including but not limited to\\n      communication on electronic mailing lists, source code control systems,\\n      and issue tracking systems that are managed by, or on behalf of, the\\n      Licensor for the purpose of discussing and improving the Work, but\\n      excluding communication that is conspicuously marked or otherwise\\n      designated in writing by the copyright owner as \"Not a Contribution.\"\\n\\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\\n      on behalf of whom a Contribution has been received by Licensor and\\n      subsequently incorporated within the Work.\\n\\n   2. Grant of Copyright License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      copyright license to reproduce, prepare Derivative Works of,\\n      publicly display, publicly perform, sublicense, and distribute the\\n      Work and such Derivative Works in Source or Object form.\\n\\n   3. Grant of Patent License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      (except as stated in this section) patent license to make, have made,\\n      use, offer to sell, sell, import, and otherwise transfer the Work,\\n      where such license applies only to those patent claims licensable\\n      by such Contributor that are necessarily infringed by their\\n      Contribution(s) alone or by combination of their Contribution(s)\\n      with the Work to which such Contribution(s) was submitted. If You\\n      institute patent litigation against any entity (including a\\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\\n      or a Contribution incorporated within the Work constitutes direct\\n      or contributory patent infringement, then any patent licenses\\n      granted to You under this License for that Work shall terminate\\n      as of the date such litigation is filed.\\n\\n   4. Redistribution. You may reproduce and distribute copies of the\\n      Work or Derivative Works thereof in any medium, with or without\\n      modifications, and in Source or Object form, provided that You\\n      meet the following conditions:\\n\\n      (a) You must give any other recipients of the Work or\\n          Derivative Works a copy of this License; and\\n\\n      (b) You must cause any modified files to carry prominent notices\\n          stating that You changed the files; and\\n\\n      (c) You must retain, in the Source form of any Derivative Works\\n          that You distribute, all copyright, patent, trademark, and\\n          attribution notices from the Source form of the Work,\\n          excluding those notices that do not pertain to any part of\\n          the Derivative Works; and\\n\\n      (d) If the Work includes a \"NOTICE\" text file as part of its\\n          distribution, then any Derivative Works that You distribute must\\n          include a readable copy of the attribution notices contained\\n          within such NOTICE file, excluding those notices that do not\\n          pertain to any part of the Derivative Works, in at least one\\n          of the following places: within a NOTICE text file distributed\\n          as part of the Derivative Works; within the Source form or\\n          documentation, if provided along with the Derivative Works; or,\\n          within a display generated by the Derivative Works, if and\\n          wherever such third-party notices normally appear. The contents\\n          of the NOTICE file are for informational purposes only and\\n          do not modify the License. You may add Your own attribution\\n          notices within Derivative Works that You distribute, alongside\\n          or as an addendum to the NOTICE text from the Work, provided\\n          that such additional attribution notices cannot be construed\\n          as modifying the License.\\n\\n      You may add Your own copyright statement to Your modifications and\\n      may provide additional or different license terms and conditions\\n      for use, reproduction, or distribution of Your modifications, or\\n      for any such Derivative Works as a whole, provided Your use,\\n      reproduction, and distribution of the Work otherwise complies with\\n      the conditions stated in this License.\\n\\n   5. Submission of Contributions. Unless You explicitly state otherwise,\\n      any Contribution intentionally submitted for inclusion in the Work\\n      by You to the Licensor shall be under the terms and conditions of\\n      this License, without any additional terms or conditions.\\n      Notwithstanding the above, nothing herein shall supersede or modify\\n      the terms of any separate license agreement you may have executed\\n      with Licensor regarding such Contributions.\\n\\n   6. Trademarks. This License does not grant permission to use the trade\\n      names, trademarks, service marks, or product names of the Licensor,\\n      except as required for reasonable and customary use in describing the\\n      origin of the Work and reproducing the content of the NOTICE file.\\n\\n   7. Disclaimer of Warranty. Unless required by applicable law or\\n      agreed to in writing, Licensor provides the Work (and each\\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\\n      implied, including, without limitation, any warranties or conditions\\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\\n      PARTICULAR PURPOSE. You are solely responsible for determining the\\n      appropriateness of using or redistributing the Work and assume any\\n      risks associated with Your exercise of permissions under this License.\\n\\n   8. Limitation of Liability. In no event and under no legal theory,\\n      whether in tort (including negligence), contract, or otherwise,\\n      unless required by applicable law (such as deliberate and grossly\\n      negligent acts) or agreed to in writing, shall any Contributor be\\n      liable to You for damages, including any direct, indirect, special,\\n      incidental, or consequential damages of any character arising as a\\n      result of this License or out of the use or inability to use the\\n      Work (including but not limited to damages for loss of goodwill,\\n      work stoppage, computer failure or malfunction, or any and all\\n      other commercial damages or losses), even if such Contributor\\n      has been advised of the possibility of such damages.\\n\\n   9. Accepting Warranty or Additional Liability. While redistributing\\n      the Work or Derivative Works thereof, You may choose to offer,\\n      and charge a fee for, acceptance of support, warranty, indemnity,\\n      or other liability obligations and/or rights consistent with this\\n      License. However, in accepting such obligations, You may act only\\n      on Your own behalf and on Your sole responsibility, not on behalf\\n      of any other Contributor, and only if You agree to indemnify,\\n      defend, and hold each Contributor harmless for any liability\\n      incurred by, or claims asserted against, such Contributor by reason\\n      of your accepting any such warranty or additional liability.\\n\\n   END OF TERMS AND CONDITIONS\\n\\n   APPENDIX: How to apply the Apache License to your work.\\n\\n      To apply the Apache License to your work, attach the following\\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\\n      replaced with your own identifying information. (Don\\'t include\\n      the brackets!)  The text should be enclosed in the appropriate\\n      comment syntax for the file format. We also recommend that a\\n      file or class name and description of purpose be included on the\\n      same \"printed page\" as the copyright notice for easier\\n      identification within third-party archives.\\n\\n   Copyright [yyyy] [name of copyright owner]\\n\\n   Licensed under the Apache License, Version 2.0 (the \"License\");\\n   you may not use this file except in compliance with the License.\\n   You may obtain a copy of the License at\\n\\n       http://www.apache.org/licenses/LICENSE-2.0\\n\\n   Unless required by applicable law or agreed to in writing, software\\n   distributed under the License is distributed on an \"AS IS\" BASIS,\\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n   See the License for the specific language governing permissions and\\n   limitations under the License.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Context Based Question Answering",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Context-Based-Question-Answering",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Karthik-Bhaskar",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Karthik-Bhaskar/Context-Based-Question-Answering/blob/main/README.md",
    "technique": "GitHub API"
  },
  "repo_status": {
    "description": "Active \u2013 The project has reached a stable, usable state and is being actively developed.",
    "excerpt": "https://www.repostatus.org/#active",
    "technique": "Regular expression"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "---\n\n\n\nTo install the docker engine on your machine, please check [here](https://docs.docker.com/get-docker/).\n\nThis project includes the [docker-compose.yml](./docker-compose.yml) file describing the services to get started quickly. The specified services in the file are Elasticsearch, Kibana, and the CBQA application.\n\n**Note**: Make sure to increase the resources (Memory) in your docker engine in case you are facing 137 exit code (out of memory).\n\nThe [docker image](https://hub.docker.com/repository/docker/karthikbhaskar/context-based-question-answering) for the CBQA application has already been built and hosted on [the docker hub](https://hub.docker.com/).\n\nTo start the complete package that includes Elasticsearch, Kibana, and CBQA application as a docker container.\n\nUse the following command in the terminal from the root of this project directory to start the container.\n```\n$ docker-compose up\n```\n\nPlease wait until all the services have started after pulling the images.\n\nMake sure the Elasticsearch server is running using the following command in the new terminal.\n```console\n$ curl http://localhost:9200\n```\n\nYou should get a response like the one below.\n```console\n{\n\"name\" : \"facddac422e8\",\n\"cluster_name\" : \"docker-cluster\",\n\"cluster_uuid\" : \"a4A3-yBhQdKBlpSDkpRelg\",\n\"version\" : {\n\"number\" : \"7.10.0\",\n\"build_flavor\" : \"default\",\n\"build_type\" : \"docker\",\n\"build_hash\" : \"51e9d6f22758d0374a0f3f5c6e8f3a7997850f96\",\n\"build_date\" : \"2020-11-09T21:30:33.964949Z\",\n\"build_snapshot\" : false,\n\"lucene_version\" : \"8.7.0\",\n\"minimum_wire_compatibility_version\" : \"6.8.0\",\n\"minimum_index_compatibility_version\" : \"6.0.0-beta1\"\n},\n\"tagline\" : \"You Know, for Search\"\n}\n```\n\nThe CBQA application service in the docker container runs on [http://localhost:5000](http://localhost:5000/), the container name of the application will be **web-app**.\n\nElasticsearch service will be running on [http://localhost:9200](http://localhost:9200/), and Kibana service will be running on [http://localhost:5601](http://localhost:5601/).\n\n**Note**: When performing QA using pre-trained models, at first use, the selected model gets downloaded from the [Hugging Face's model hub](https://huggingface.co/models) this may take a while depending on your internet speed. If you are re-starting the application while you are testing, make sure to remove the auto-created temporary user directories in the project and the user index on Elasticsearch (Will be fixed soon)\n\nTo stop the container, use the following command in the new terminal.\n```console\n$ docker-compose down\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 17,
      "date": "Sun, 12 Dec 2021 19:01:33 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "question-answering",
      "natural-language-processing",
      "nlp",
      "web-application",
      "flask-application",
      "transformers",
      "haystack"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "---\n*Full demo video on YouTube.*\n\n[![Alt text](https://img.youtube.com/vi/vqy5XmDwKMQ/0.jpg)](https://youtu.be/vqy5XmDwKMQ)\n\n",
      "technique": "Header extraction"
    }
  ]
}