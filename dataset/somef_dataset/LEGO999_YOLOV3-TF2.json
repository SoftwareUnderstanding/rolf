{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1804.02767"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "320320 | 30.1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8645786864232722
      ],
      "excerpt": "COCO original | [(10, 13), (16, 30), (33, 23), (30, 61), (62, 45), (59, 119), (116, 90), (156, 198), (373, 326)] \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/LEGO999/YOLOV3-TF2",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-08-04T13:00:38Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-08-06T15:05:10Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you want to go somewhere regarding implementation, please skip this part.  \n\nYOLOv3 is a light-weight but powerful one-stage object detector, which means it regresses the positions of objects and predict the probability of objects directly from the feature maps of CNN. Typical example of one-state detector will be YOLO and SSD series.On the contrary,  two stage detector like R-CNN, Fast R-CNN and Faster R-CNN may include\nSelective Search, Support Vector Machine (SVM) and Region Proposal Network (RPN) besides CNN. Two-stage detectors will be sightly more accurate but much slower.\n \nYOLOv3 consists of 2 parts: feature extractor and detector. Feature extractor is a Darknet-53 without its fully connected layer, which is originally designed for classification task on ImageNet dataset.   \n![darknet](/fig/Darknet.png)  \n*Darknet-53 architecture(Source: YOLOv3: An Incremental Improvement https://arxiv.org/abs/1804.02767)*\n\nDetector uses multi-scale fused features to predict the position and the class of the corresponding object.  \n![yolov3](/fig/yolo.png)*(Source: https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b)*\n\nAs you can see from the picture above, there are 3 prediction scales in total. For example, if the spatial resolution of an input image is 32N X 32N, the output of the first prediction convolution layer(strides32) will be N X N X (B X (C+5)). B indicates amount of anchors at this scale and C stands for probabilities of different classes. 5 represents 5 different regressions, the  horizontal offset t_x, the vertical offset t_y, resizing factor of the given anchor height t_hand width t_wand objectness score o (whether an object exists in this square of the checkerboard). The second prediction layer will output feature maps of 2N X 2N X (B X (C+5)). And the third prediction output will be much finer, which is 4N X 4N X (B X (C+5).\n\nReading papers of YOLO, YOLOv2 and YOLOv3, I summarize the loss function of YOLOv3 as follows:  \n![](/fig/loss1.PNG)\n<!-- $$\nL_{Localization} = \\lambda_1\\sum_{i=0}^{N^2}\\sum_{j=0}^{B}1_{ij}^{obj}[(t_{x} - t_{\\hat{x}})^2 + (t_{y} - t_{\\hat{y}})^2]\n\\\\L_{Shaping} =\\lambda_2\\sum_{i=0}^{N^2}\\sum_{j=0}^{B}1_{ij}^{obj}[(t_{w} - t_{\\hat{w}})^2 + (t_{h} - t_{\\hat{h}})^2]\\\\\nL_{objectness-obj} =\\lambda_3\\sum_{i=0}^{N^2}\\sum_{j=0}^{B}1_{ij}^{obj}\\log(o_{ij})$$ $$L_{objectness-noobj} =\\lambda_4\\sum_{i=0}^{N^2}\\sum_{j=0}^{B}1_{ij}^{obj}\\log(1-o_{ij})\n\\\\L_{class} =\\lambda_5\\sum_{i=0}^{N^2}\\sum_{j=0}^{B}1_{ij}^{obj}\\sum_{c\\in classes}[p_{\\hat{ij}}(c)\\log(p_{ij}(c))+ (1-p_{\\hat{ij}}(c))\\log(1-p_{ij}(c))])\n\\\\ L_{Scale_{1}} = L_{Localization} + L_{Shaping} + L_{objectness-obj} + L_{objectness-noobj} + L_{class}\n\\\\ L_{total} = L_{Scale_{1}}+L_{Scale_{2}}+L_{Scale_{3}}$$ -->\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "This is my implementation of YOLOv3 using TensorFlow 2.0 backend. The main purpose of this project is to get me familiar with deep learning and specific concepts in domain object detection. Two usages are provided:\n* Object detection based on official pre-trained weights in COCO\n* Object detection of optic nerve on Indian Diabetic Retinopathy Image Dataset (IDRiD) using fine tuning. \n![nerve](/fig/optics_nerve.png)\n*Fundus and the corresponding optic nerve*\n\nThe following content will be provided in this repo:\n* Introduction of YOLOv3\n* Object detection based on the official pre-trained weights\n* Object detection - fine tuning on IDRiD  \n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9630560192241839,
        0.8783195453194543
      ],
      "excerpt": "Model_size: Video frames will be resized to this size and be put into CNN model, higher resolution leads to more accurate detection (especially for small objects) and slower speed. \nIou_threshold: Threshold of non-max suppression. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8727765390886534,
        0.927320916661205
      ],
      "excerpt": "Confid_threshold: neglect the detected objects under the certain confidence. \n@tf.function is enabled by default to improve performance, namely, no eager execution. No batching is applied here. Performance is measured on platform Intel i7 9750H, GTX 1660 Ti 6GB, DDR4 2666 8GB2, Sabrent Rocket NVME 2TB \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "Data augmentation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8013457552368276
      ],
      "excerpt": "First, enter python3 k-means.py to generate new anchors on our IDRiD dataset. Copy the new anchors into trainer_main.py. In the terminal, enter python3 trainer_main.py to begin fine tuning. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8752791720898575,
        0.8919622415131365
      ],
      "excerpt": "Except for part of the previous flags, there might be the following flags needing to be noticed. \n* lr: learning rate of the Adam optimizer, by default 10e-3 for fine tuning. If full transfer learning(including feature extractor) is needed, please further reduce the learning rate. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9894527603366323,
        0.9888875098395794
      ],
      "excerpt": "The original image size is 4288 X 2848. The images are cropped and padded to ratio 1:1. After that, they are all resized to 608 X 608 and written into TFRecord. In this way, data could be loaded efficiently. \nFrom the original papers of YOLOv3 and its predecessor YOLOv2, one of the trick to gain more performance on a specific dataset is to use a better set of anchors priors. Since our dataset is a customized dataset different from COCO. It might be beneficial to use the new priors. Here I use K-Means++ instead of K-Means to make clustering less sensitive to initialization. For practice, I don't use any existing package but implement the algorithm by myself. I tried two different setups, one is to generate 9 different sizes of anchors, another is to generate 3 anchors. Results with non-changed COCO anchors will also be reported later. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9902369464847738,
        0.8719659125468645
      ],
      "excerpt": "NEW_CENTRIOD_THRESHOLD: K-Means is sensitive to initialization. Theoretically, it should be set as 1.0. In this situation, the furthest point will be chosen as the centriod of the new cluster at the initialization. Pro: larger coverage of data points;  con: sensitive to outliers. By default, threshold is set as 0.98. \nSince our dataset is a rather small dataset, after train-validation-split, there are about 360 images for training, I use data augmentation to mitigate over-fitting. Data augmentation contains:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8442892922248184
      ],
      "excerpt": "Further data augmentation methods could be considered: as flipping, shearing and shifting. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/LEGO999/YOLOV3-TF2/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sun, 12 Dec 2021 03:39:39 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/LEGO999/YOLOV3-TF2/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "LEGO999/YOLOV3-TF2",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8661176197453521
      ],
      "excerpt": "Name | anchors \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9220451905096577
      ],
      "excerpt": "Following YOLOv2: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8039555011569357
      ],
      "excerpt": "Model size | Average FPS \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8388018165440712
      ],
      "excerpt": "First, enter python3 k-means.py to generate new anchors on our IDRiD dataset. Copy the new anchors into trainer_main.py. In the terminal, enter python3 trainer_main.py to begin fine tuning. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8148383852264717
      ],
      "excerpt": "* epoch: Training will be stopped only until this number. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8421074476017179
      ],
      "excerpt": "Name | anchors \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8003502145739053
      ],
      "excerpt": "COCO original | [(10, 13), (16, 30), (33, 23), (30, 61), (62, 45), (59, 119), (116, 90), (156, 198), (373, 326)] \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/LEGO999/YOLOV3-TF2/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "YOLOv3-TF2",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "YOLOV3-TF2",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "LEGO999",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/LEGO999/YOLOV3-TF2/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": " * Python 3.6\n * Opencv-python 4.1.2.30\n * TensorFlow 2.0.0\n * Numpy 1.17.3\n * Seaborn 0.10.0\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sun, 12 Dec 2021 03:39:39 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[![](http://img.youtube.com/vi/6mWNgng6CfY/0.jpg)](http://www.youtube.com/watch?v=6mWNgng6CfY \"\")   \nhttps://www.youtube.com/watch?v=6mWNgng6CfY&t=3s\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "![](/fig/k-means.gif)\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "What in the red frame line is our ground truth.\n![](/fig/detection1.gif)\n",
      "technique": "Header extraction"
    }
  ]
}