{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Our code is inspired by [pytorch-DCGAN](https://github.com/pytorch/examples/tree/master/dcgan).\n",
      "technique": "Header extraction"
    }
  ],
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you use this code for your research, please cite our papers.\n```\n@inproceedings{CycleGAN2017,\n  title={Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks},\n  author={Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A},\n  booktitle={Computer Vision (ICCV), 2017 IEEE International Conference on},\n  year={2017}\n}\n\n\n@inproceedings{isola2017image,\n  title={Image-to-Image Translation with Conditional Adversarial Networks},\n  author={Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},\n  booktitle={Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on},\n  year={2017}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{isola2017image,\n  title={Image-to-Image Translation with Conditional Adversarial Networks},\n  author={Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},\n  booktitle={Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on},\n  year={2017}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{CycleGAN2017,\n  title={Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks},\n  author={Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A},\n  booktitle={Computer Vision (ICCV), 2017 IEEE International Conference on},\n  year={2017}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9575615154108722
      ],
      "excerpt": "The code was written by Jun-Yan Zhu and Taesung Park, and supported by Tongzhou Wang. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9997516492998463
      ],
      "excerpt": "If you use this code for your research, please cite: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9998522751534391
      ],
      "excerpt": "Jun-Yan Zhu*,  Taesung Park*, Phillip Isola, Alexei A. Efros. In ICCV 2017. (* equal contributions) [Bibtex] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999439035149589
      ],
      "excerpt": "Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros. In CVPR 2017. [Bibtex] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9820653880687046
      ],
      "excerpt": "CycleGAN course assignment code and handout designed by Prof. Roger Grosse for CSC321 \"Intro to Neural Networks and Machine Learning\" at University of Toronto. Please contact the instructor if you would like to adopt it in your course. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8227893355900447
      ],
      "excerpt": "<p><a href=\"https://github.com/leehomyc/cyclegan-1\"> [Tensorflow]</a> (by Harry Yang), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9027805018660631
      ],
      "excerpt": "<a href=\"https://github.com/LynnHo/CycleGAN-Tensorflow-2\"> [Tensorflow2]</a> (by Zhenliang He), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8227893355900447,
        0.9432788912470228
      ],
      "excerpt": "<a href=\"https://github.com/kaonashi-tyc/zi2zi\">[Tensorflow (zi2zi)]</a> (by Yuchen Tian), \n<a href=\"https://github.com/pfnet-research/chainer-pix2pix\">[Chainer]</a> (by mattya), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9245998068629447
      ],
      "excerpt": "Before you post a new question, please first look at the above Q & A and existing GitHub issues. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-04-18T10:33:05Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-08T01:49:24Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9264525518212987,
        0.8932420525641758
      ],
      "excerpt": "New:  Please check out contrastive-unpaired-translation (CUT), our new unpaired image-to-image translation model that enables fast and memory-efficient training. \nWe provide PyTorch implementations for both unpaired and paired image-to-image translation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9264982603468344,
        0.9634036303230676,
        0.9783004463747682,
        0.8810593587994024,
        0.8497433546409732
      ],
      "excerpt": "This PyTorch implementation produces results comparable to or better than our original Torch software. If you would like to reproduce the same results as in the papers, check out the original CycleGAN Torch and pix2pix Torch code in Lua/Torch. \nNote: The current software works well with PyTorch 1.4. Check out the older branch that supports PyTorch 0.1-0.3. \nYou may find useful information in training/test tips and frequently asked questions. To implement custom models and datasets, check out our templates. To help users better understand and adapt our codebase, we provide an overview of the code structure of this repository. \nCycleGAN: Project |  Paper |  Torch | \nTensorflow Core Tutorial | PyTorch Colab \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8810593587994024,
        0.8497433546409732
      ],
      "excerpt": "Pix2pix:  Project |  Paper |  Torch | \nTensorflow Core Tutorial | PyTorch Colab \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8099033291246154
      ],
      "excerpt": "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks.<br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8552078246538597
      ],
      "excerpt": "Image-to-Image Translation with Conditional Adversarial Networks.<br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9485887571971563,
        0.8959236469373937,
        0.8959236469373937,
        0.870491297724489,
        0.8400228363860333
      ],
      "excerpt": "CycleGAN course assignment code and handout designed by Prof. Roger Grosse for CSC321 \"Intro to Neural Networks and Machine Learning\" at University of Toronto. Please contact the instructor if you would like to adopt it in your course. \nTensorFlow Core CycleGAN Tutorial: Google Colab | Code \nTensorFlow Core pix2pix Tutorial: Google Colab | Code \nPyTorch Colab notebook: CycleGAN and pix2pix \nZeroCostDL4Mic Colab notebook: CycleGAN and pix2pix \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.879629236041527
      ],
      "excerpt": "To see more intermediate results, check out `./checkpoints/maps_cyclegan/web/index.html`. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.879629236041527
      ],
      "excerpt": "To see more intermediate results, check out./checkpoints/facades_pix2pix/web/index.html`. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9651404336974972
      ],
      "excerpt": "- To train and test pix2pix-based colorization models, please add--model colorizationand--dataset_mode colorization`. See our training tips for more details. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.852714024668955
      ],
      "excerpt": "The pretrained model is saved at ./checkpoints/{name}_pretrained/latest_net_G.pth. Check here for all the available CycleGAN models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8679700002105912,
        0.9636642855365062
      ],
      "excerpt": "The option --model test is used for generating results of CycleGAN only for one side. This option will automatically set --dataset_mode single, which only loads the images from one set. On the contrary, using --model cycle_gan requires loading and generating results in both directions, which is sometimes unnecessary. The results will be saved at ./results/. Use --results_dir {directory_path_to_save_result} to specify the results directory. \nFor pix2pix and your own models, you need to explicitly specify --netG, --norm, --no_dropout to match the generator architecture of the trained model. See this FAQ for more details. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8341549856015136
      ],
      "excerpt": "Note that we specified --direction BtoA as Facades dataset's A to B direction is photos to labels. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8867044843876927,
        0.8795572088224289
      ],
      "excerpt": "See a list of currently available models at ./scripts/download_pix2pix_model.sh \nWe provide the pre-built Docker image and Dockerfile that can run this code repo. See docker. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8204183127188566,
        0.9715193104088529,
        0.9161113314069189
      ],
      "excerpt": "If you plan to implement custom models and dataset for your new applications, we provide a dataset template and a model template as a starting point. \nTo help users better understand and use our code, we briefly overview the functionality and implementation of each package and each module. \nYou are always welcome to contribute to this repository by sending a pull request. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Image-to-Image Translation in PyTorch",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 4933,
      "date": "Wed, 08 Dec 2021 02:49:04 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/junyanz/pytorch-CycleGAN-and-pix2pix/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "junyanz/pytorch-CycleGAN-and-pix2pix",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/junyanz/pytorch-CycleGAN-and-pix2pix/master/docs/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/junyanz/pytorch-CycleGAN-and-pix2pix/master/CycleGAN.ipynb",
      "https://raw.githubusercontent.com/junyanz/pytorch-CycleGAN-and-pix2pix/master/pix2pix.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/junyanz/pytorch-CycleGAN-and-pix2pix/master/scripts/test_cyclegan.sh",
      "https://raw.githubusercontent.com/junyanz/pytorch-CycleGAN-and-pix2pix/master/scripts/download_pix2pix_model.sh",
      "https://raw.githubusercontent.com/junyanz/pytorch-CycleGAN-and-pix2pix/master/scripts/conda_deps.sh",
      "https://raw.githubusercontent.com/junyanz/pytorch-CycleGAN-and-pix2pix/master/scripts/download_cyclegan_model.sh",
      "https://raw.githubusercontent.com/junyanz/pytorch-CycleGAN-and-pix2pix/master/scripts/install_deps.sh",
      "https://raw.githubusercontent.com/junyanz/pytorch-CycleGAN-and-pix2pix/master/scripts/test_pix2pix.sh",
      "https://raw.githubusercontent.com/junyanz/pytorch-CycleGAN-and-pix2pix/master/scripts/test_single.sh",
      "https://raw.githubusercontent.com/junyanz/pytorch-CycleGAN-and-pix2pix/master/scripts/test_colorization.sh",
      "https://raw.githubusercontent.com/junyanz/pytorch-CycleGAN-and-pix2pix/master/scripts/train_colorization.sh",
      "https://raw.githubusercontent.com/junyanz/pytorch-CycleGAN-and-pix2pix/master/scripts/train_cyclegan.sh",
      "https://raw.githubusercontent.com/junyanz/pytorch-CycleGAN-and-pix2pix/master/scripts/train_pix2pix.sh",
      "https://raw.githubusercontent.com/junyanz/pytorch-CycleGAN-and-pix2pix/master/scripts/eval_cityscapes/download_fcn8s.sh",
      "https://raw.githubusercontent.com/junyanz/pytorch-CycleGAN-and-pix2pix/master/datasets/download_cyclegan_dataset.sh",
      "https://raw.githubusercontent.com/junyanz/pytorch-CycleGAN-and-pix2pix/master/datasets/download_pix2pix_dataset.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Clone this repo:\n```bash\ngit clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\ncd pytorch-CycleGAN-and-pix2pix\n```\n\n- Install [PyTorch](http://pytorch.org) and 0.4+ and other dependencies (e.g., torchvision, [visdom](https://github.com/facebookresearch/visdom) and [dominate](https://github.com/Knio/dominate)).\n  - For pip users, please type the command `pip install -r requirements.txt`.\n  - For Conda users, you can create a new Conda environment using `conda env create -f environment.yml`.\n  - For Docker users, we provide the pre-built Docker image and Dockerfile. Please refer to our [Docker](docs/docker.md) page.\n  - For Repl users, please click [![Run on Repl.it](https://repl.it/badge/github/junyanz/pytorch-CycleGAN-and-pix2pix)](https://repl.it/github/junyanz/pytorch-CycleGAN-and-pix2pix).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8547506928230144,
        0.9259408091992843
      ],
      "excerpt": "<a href=\"https://github.com/yunjey/mnist-svhn-transfer\">[Minimal PyTorch]</a> (by yunjey), \n<a href=\"https://github.com/Ldpe2G/DeepLearningForFun/tree/master/Mxnet-Scala/CycleGAN\">[Mxnet]</a> (by Ldpe2G), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8951048829289392
      ],
      "excerpt": "<a href=\"https://github.com/Ldpe2G/DeepLearningForFun/tree/master/Oneflow-Python/CycleGAN\">[OneFlow]</a> (by Ldpe2G) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.946109459978882
      ],
      "excerpt": "<a href=\"https://github.com/taey16/pix2pixBEGAN.pytorch\">[Pytorch]</a> (by taey16) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8109284348123712
      ],
      "excerpt": "Test the model (bash ./scripts/test_pix2pix.sh): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8998272975939812,
        0.9414119019616536
      ],
      "excerpt": "You can download a pretrained model (e.g. horse2zebra) with the following script: \nbash ./scripts/download_cyclegan_model.sh horse2zebra \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8917133644880719
      ],
      "excerpt": "bash ./datasets/download_cyclegan_dataset.sh horse2zebra \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9414119019616536
      ],
      "excerpt": "bash ./scripts/download_pix2pix_model.sh facades_label2photo \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8260262293211895
      ],
      "excerpt": "<img src=\"https://junyanz.github.io/CycleGAN/images/teaser_high_res.jpg\" width=\"800\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9154330685897901
      ],
      "excerpt": "<img src='imgs/edges2cats.jpg' width=\"400px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8988665351184281,
        0.8737873112808705
      ],
      "excerpt": "To log training progress and test images to W&B dashboard, set the --use_wandb flag with train and test script \nTrain a model: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9373628356670933
      ],
      "excerpt": "python train.py --dataroot ./datasets/maps --name maps_cyclegan --model cycle_gan \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8574618856000782
      ],
      "excerpt": "- Test the model:bash \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9379080685614798
      ],
      "excerpt": "python test.py --dataroot ./datasets/maps --name maps_cyclegan --model cycle_gan \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8094468517814178,
        0.8640668950388781
      ],
      "excerpt": "- The test results will be saved to a html file here:./results/maps_cyclegan/latest_test/index.html`. \nDownload a pix2pix dataset (e.g.facades): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8988665351184281,
        0.8737873112808705
      ],
      "excerpt": "To log training progress and test images to W&B dashboard, set the --use_wandb flag with train and test script \nTrain a model: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9436610090033062
      ],
      "excerpt": "python train.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --direction BtoA \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9440788375899131
      ],
      "excerpt": "python test.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --direction BtoA \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.85924277383721
      ],
      "excerpt": "The pretrained model is saved at ./checkpoints/{name}_pretrained/latest_net_G.pth. Check here for all the available CycleGAN models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.962599325391566
      ],
      "excerpt": "python test.py --dataroot datasets/horse2zebra/testA --name horse2zebra_pretrained --model test --no_dropout \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8207576516613929
      ],
      "excerpt": "Check here for all the available pix2pix models. For example, if you would like to download label2photo model on the Facades dataset, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8413266574487004
      ],
      "excerpt": "Download the pix2pix facades datasets: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9440788375899131
      ],
      "excerpt": "python test.py --dataroot ./datasets/facades/ --direction BtoA --model pix2pix --name facades_label2photo_pretrained \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/junyanz/pytorch-CycleGAN-and-pix2pix/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook",
      "Shell",
      "MATLAB",
      "TeX"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other",
      "url": "https://raw.githubusercontent.com/junyanz/pytorch-CycleGAN-and-pix2pix/master/LICENSE"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'Copyright (c) 2017, Jun-Yan Zhu and Taesung Park\\nAll rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met:\\n\\n Redistributions of source code must retain the above copyright notice, this\\n  list of conditions and the following disclaimer.\\n\\n Redistributions in binary form must reproduce the above copyright notice,\\n  this list of conditions and the following disclaimer in the documentation\\n  and/or other materials provided with the distribution.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n\\n\\n--------------------------- LICENSE FOR pix2pix --------------------------------\\nBSD License\\n\\nFor pix2pix software\\nCopyright (c) 2016, Phillip Isola and Jun-Yan Zhu\\nAll rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met:\\n\\n Redistributions of source code must retain the above copyright notice, this\\n  list of conditions and the following disclaimer.\\n\\n Redistributions in binary form must reproduce the above copyright notice,\\n  this list of conditions and the following disclaimer in the documentation\\n  and/or other materials provided with the distribution.\\n\\n----------------------------- LICENSE FOR DCGAN --------------------------------\\nBSD License\\n\\nFor dcgan.torch software\\n\\nCopyright (c) 2015, Facebook, Inc. All rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\\n\\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\\n\\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\\n\\nNeither the name Facebook nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "CycleGAN and pix2pix in PyTorch",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "pytorch-CycleGAN-and-pix2pix",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "junyanz",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Linux or macOS\n- Python 3\n- CPU or NVIDIA GPU + CUDA CuDNN\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 16401,
      "date": "Wed, 08 Dec 2021 02:49:04 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "pytorch",
      "gan",
      "cyclegan",
      "pix2pix",
      "deep-learning",
      "computer-vision",
      "computer-graphics",
      "image-manipulation",
      "image-generation",
      "generative-adversarial-network",
      "gans"
    ],
    "technique": "GitHub API"
  }
}