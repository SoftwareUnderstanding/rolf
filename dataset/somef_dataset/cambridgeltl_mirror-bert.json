{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```bibtex\n@inproceedings{liu-etal-2021-fast,\n    title = \"Fast, Effective, and Self-Supervised: Transforming Masked Language Models into Universal Lexical and Sentence Encoders\",\n    author = \"Liu, Fangyu  and\n      Vuli{\\'c}, Ivan  and\n      Korhonen, Anna  and\n      Collier, Nigel\",\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.emnlp-main.109\",\n    pages = \"1442--1459\",\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{liu-etal-2021-fast,\n    title = \"Fast, Effective, and Self-Supervised: Transforming Masked Language Models into Universal Lexical and Sentence Encoders\",\n    author = \"Liu, Fangyu  and\n      Vuli{\\'c}, Ivan  and\n      Korhonen, Anna  and\n      Collier, Nigel\",\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.emnlp-main.109\",\n    pages = \"1442--1459\",\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8422862053358849
      ],
      "excerpt": "by Fangyu Liu, Ivan Vuli\u0107, Anna Korhonen, and Nigel Collier.  \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/cambridgeltl/mirror-bert",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-09-06T13:29:41Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-06T14:02:16Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9374680734812779,
        0.9065356207190821,
        0.8619860342205848,
        0.8192505441185383,
        0.9897250638905096
      ],
      "excerpt": "UPDATE: see a follow-up work Trans-Encoder, a SotA unsupervised model for STS. \nCode repo for the EMNLP 2021 paper: <br> \nFast, Effective, and Self-Supervised: Transforming Masked Language Models into Universal Lexical and Sentence Encoders<br> \nby Fangyu Liu, Ivan Vuli\u0107, Anna Korhonen, and Nigel Collier.  \nMirror-BERT is an unsupervised contrastive learning method that converts pretrained language models (PLMs) into universal text encoders. It takes a PLM and a txt file containing raw text as input, and output a strong text embedding model, in just 20-30 seconds. It works well for not only sentence, but also word and phrase representation learning. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "|model | STS avg. | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8786166433310099
      ],
      "excerpt": "(Note that the released models would not replicate the exact numbers in the paper, since the reported numbers in the paper are average of three runs.) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8525637632059119
      ],
      "excerpt": "It's easy to compute your own sentence embeddings: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8072880351303955
      ],
      "excerpt": "    \"I transform pre-trained language models into universal text encoders.\", \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": " [EMNLP 2021] Mirror-BERT: Converting Pretrained Language Models to universal text encoders without labels.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/cambridgeltl/mirror-bert/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Fri, 10 Dec 2021 12:48:41 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/cambridgeltl/mirror-bert/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "cambridgeltl/mirror-bert",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/cambridgeltl/mirror-bert/main/mirror_scripts/mirror_sentence_bert.sh",
      "https://raw.githubusercontent.com/cambridgeltl/mirror-bert/main/mirror_scripts/mirror_sentence_roberta_supervised_amazon_qa.sh",
      "https://raw.githubusercontent.com/cambridgeltl/mirror-bert/main/mirror_scripts/mirror_sentence_bert_drophead.sh",
      "https://raw.githubusercontent.com/cambridgeltl/mirror-bert/main/mirror_scripts/mirror_sentence_roberta_drophead.sh",
      "https://raw.githubusercontent.com/cambridgeltl/mirror-bert/main/mirror_scripts/mirror_sentence_roberta.sh",
      "https://raw.githubusercontent.com/cambridgeltl/mirror-bert/main/mirror_scripts/mirror_word_bert.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9023697225149864
      ],
      "excerpt": "./mirror_scripts/mirror_sentence_bert.sh 0,1 \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.950560466673748
      ],
      "excerpt": "from src.mirror_bert import MirrorBERT \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8131897700978834
      ],
      "excerpt": "mirror_bert.load_model(path=model_name, use_cuda=True) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9312650322969277
      ],
      "excerpt": "print (embeddings.shape) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8538438913059458
      ],
      "excerpt": "python evaluation/eval.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8538438913059458
      ],
      "excerpt": "python evaluation/eval.py \\ \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/cambridgeltl/mirror-bert/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2021 Cambridge Language Technology Lab\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Mirror-BERT",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "mirror-bert",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "cambridgeltl",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/cambridgeltl/mirror-bert/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 37,
      "date": "Fri, 10 Dec 2021 12:48:41 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "bert",
      "nlp",
      "contrastive-learning",
      "emnlp2021",
      "unsupervised-learning"
    ],
    "technique": "GitHub API"
  }
}