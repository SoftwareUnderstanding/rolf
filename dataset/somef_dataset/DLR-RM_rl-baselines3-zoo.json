{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2005.05719"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To cite this repository in publications:\n\n```bibtex\n@misc{rl-zoo3,\n  author = {Raffin, Antonin},\n  title = {RL Baselines3 Zoo},\n  year = {2020},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/DLR-RM/rl-baselines3-zoo}},\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{rl-zoo3,\n  author = {Raffin, Antonin},\n  title = {RL Baselines3 Zoo},\n  year = {2020},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/DLR-RM/rl-baselines3-zoo}},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9762830854544107
      ],
      "excerpt": "See https://gym.openai.com/envs/#robotics and https://github.com/DLR-RM/rl-baselines3-zoo/pull/71 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8656070203791273
      ],
      "excerpt": "See https://github.com/qgallouedec/panda-gym/. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8656070203791273
      ],
      "excerpt": "See https://github.com/maximecb/gym-minigrid \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/DLR-RM/rl-baselines3-zoo",
    "technique": "GitHub API"
  },
  "contributor": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We would like to thanks our contributors: [@iandanforth](https://github.com/iandanforth), [@tatsubori](https://github.com/tatsubori) [@Shade5](https://github.com/Shade5) [@mcres](https://github.com/mcres)\n",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-05-05T05:53:27Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-09T13:09:54Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9298200434264403
      ],
      "excerpt": "RL Baselines3 Zoo is a training framework for Reinforcement Learning (RL), using Stable Baselines3. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9712676365497585,
        0.881772435147659,
        0.9808772988276458
      ],
      "excerpt": "In addition, it includes a collection of tuned hyperparameters for common environments and RL algorithms, and agents trained with those settings. \nWe are looking for contributors to complete the collection! \nGoals of this repository: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9703403428881437
      ],
      "excerpt": "This is the SB3 version of the original SB2 rl-zoo. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9050004038016073
      ],
      "excerpt": "Save a checkpoint of the agent every 100000 steps: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8945133247042758
      ],
      "excerpt": "Plot training success (y-axis) w.r.t. timesteps (x-axis) with a moving window of 500 episodes for all the Fetch environment with HER algorithm: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9568030429748006,
        0.8082125625980153
      ],
      "excerpt": "The RL zoo integrates some of rliable library features. \nYou can find a visual explanation of the tools used by rliable in this blog post. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8198659955463234
      ],
      "excerpt": "The easiest way to add support for a custom environment is to edit utils/import_envs.py and register your environment here. Then, you need to add a section for it in the hyperparameters file (hyperparams/algo.yml). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8841831669017519
      ],
      "excerpt": "Not all hyperparameters are tuned, and tuning enforces certain default hyperparameter settings that may be different from the official defaults. See utils/hyperparams_opt.py for the current settings for each agent. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.877061365203219
      ],
      "excerpt": "Note: hyperparameters search is not implemented for DQN for now. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9857336245815235
      ],
      "excerpt": "Budget of 1000 trials with a maximum of 50000 steps: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9311739274725181
      ],
      "excerpt": "Distributed optimization using a shared database is also possible (see the corresponding Optuna documentation): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801663439762109
      ],
      "excerpt": "Note that the default hyperparameters used in the zoo when tuning are not always the same as the defaults provided in stable-baselines3. Consult the latest source code to be sure of these settings. For example: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9718111743705132
      ],
      "excerpt": "Non-episodic rollout in TD3 and DDPG assumes gradient_steps = train_freq and so tunes only train_freq to reduce the search space.   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9906443500622181
      ],
      "excerpt": "Normalization uses the default parameters of VecNormalize, with the exception of gamma which is set to match that of the agent.  This can be overridden using the appropriate hyperparameters/algo_name.yml, e.g. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8659241231165004
      ],
      "excerpt": "Apart from recording videos of specific saved models, it is also possible to record a video of a training experiment where checkpoints have been saved. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.958653500149026
      ],
      "excerpt": "NOTE: this is not a quantitative benchmark as it corresponds to only one run (cf issue #38). This benchmark is meant to check algorithm (maximal) performance, find potential bugs and also allow users to have access to pretrained agents. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8917406377870095
      ],
      "excerpt": "Similar to MuJoCo Envs but with a ~free~ (MuJoCo 2.1.0+ is now free!) easy to install simulator: pybullet. We are using BulletEnv-v0 version. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8058381501513693
      ],
      "excerpt": "Similar to MuJoCo Robotics Envs but with a ~free~ easy to install simulator: pybullet. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9389755676725645
      ],
      "excerpt": "A simple, lightweight and fast Gym environments implementation of the famous gridworld. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.800426790923918
      ],
      "excerpt": "Please see Stable Baselines3 documentation for alternatives. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included.",
      "technique": "GitHub API"
    }
  ],
  "documentation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "https://optuna.readthedocs.io/",
      "technique": "Regular expression"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/DLR-RM/rl-baselines3-zoo/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 141,
      "date": "Fri, 10 Dec 2021 02:18:11 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/DLR-RM/rl-baselines3-zoo/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "DLR-RM/rl-baselines3-zoo",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/DLR-RM/rl-baselines3-zoo/master/docker/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/DLR-RM/rl-baselines3-zoo/master/scripts/run_docker_cpu.sh",
      "https://raw.githubusercontent.com/DLR-RM/rl-baselines3-zoo/master/scripts/run_docker_gpu.sh",
      "https://raw.githubusercontent.com/DLR-RM/rl-baselines3-zoo/master/scripts/build_docker.sh",
      "https://raw.githubusercontent.com/DLR-RM/rl-baselines3-zoo/master/scripts/run_tests.sh",
      "https://raw.githubusercontent.com/DLR-RM/rl-baselines3-zoo/master/docker/entrypoint.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8506371989489071
      ],
      "excerpt": "python train.py --algo sac --env Pendulum-v0 --save-replay-buffer \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9616642520648009,
        0.8565427520982913
      ],
      "excerpt": "First, you need to install rliable. \nNote: Python 3.7+ is required in that case. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9741263536260352
      ],
      "excerpt": "Note: to download the repo with the trained agents, you must use git clone --recursive https://github.com/DLR-RM/rl-baselines3-zoo in order to clone the submodule too. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8764112293207336
      ],
      "excerpt": "python train.py --algo ppo --env MountainCar-v0 -optimize --study-name test --storage sqlite:///example.db \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9653080982686493
      ],
      "excerpt": "You can specify in the hyperparameter config one or more wrapper to use around the environment: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8392226997288225
      ],
      "excerpt": "Note that you can easily specify parameters too. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8678686242456772
      ],
      "excerpt": "Note: if you want to pass a string, you need to escape it like that: my_string:\"'value'\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.824212544715757,
        0.9152135146171199,
        0.8161761020197926
      ],
      "excerpt": "See https://github.com/bulletphysics/bullet3/tree/master/examples/pybullet/gym/pybullet_envs. \nSimilar to MuJoCo Envs but with a ~free~ (MuJoCo 2.1.0+ is now free!) easy to install simulator: pybullet. We are using BulletEnv-v0 version. \nNote: those environments are derived from Roboschool and are harder than the Mujoco version (see Pybullet issue) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8144926137182862,
        0.8787119201734519,
        0.9154264580393926
      ],
      "excerpt": "See https://gym.openai.com/envs/#robotics and https://github.com/DLR-RM/rl-baselines3-zoo/pull/71 \nMuJoCo version: 1.50.1.0 \nGym version: 0.18.0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9028667846288516
      ],
      "excerpt": "See https://github.com/qgallouedec/panda-gym/. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9028667846288516
      ],
      "excerpt": "See https://github.com/maximecb/gym-minigrid \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9068912466327751,
        0.999746712887969,
        0.8839480811484454
      ],
      "excerpt": "Note that you need to specify --gym-packages gym_minigrid with enjoy.py and train.py as it is not a standard Gym environment, as well as installing the custom Gym package module or putting it in python path. \npip install gym-minigrid \npython train.py --algo ppo --env MiniGrid-DoorKey-5x5-v0 --gym-packages gym_minigrid \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9184470084965942
      ],
      "excerpt": "You can train agents online using colab notebook. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9915158754495621,
        0.9979947896609701
      ],
      "excerpt": "apt-get install swig cmake ffmpeg \npip install -r requirements.txt \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8533241409712514
      ],
      "excerpt": "GPU image: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8808279034764968,
        0.9208012637310985,
        0.9040437757486015
      ],
      "excerpt": "./scripts/run_docker_cpu.sh python train.py --algo ppo --env CartPole-v1 \nTo run tests, first install pytest, then: \nmake pytest \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8536206762893198
      ],
      "excerpt": "<img src=\"images/panda_pick.gif\" align=\"right\" width=\"35%\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8906829375720374
      ],
      "excerpt": "python train.py --algo algo_name --env env_id \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8545975810706216
      ],
      "excerpt": "python train.py --algo ppo --env CartPole-v1 --tensorboard-log /tmp/stable-baselines/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8631212974369468
      ],
      "excerpt": "python train.py --algo sac --env HalfCheetahBulletEnv-v0 --eval-freq 10000 --eval-episodes 10 --n-eval-envs 1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955347578582923,
        0.8223977027399393,
        0.9214040904976778
      ],
      "excerpt": "python train.py --algo td3 --env HalfCheetahBulletEnv-v0 --save-freq 100000 \nContinue training (here, load pretrained agent for Breakout and continue training for 5000 steps): \npython train.py --algo a2c --env BreakoutNoFrameskip-v4 -i rl-trained-agents/a2c/BreakoutNoFrameskip-v4_1/BreakoutNoFrameskip-v4.zip -n 5000 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9203791186539343
      ],
      "excerpt": "python train.py --algo sac --env Pendulum-v0 --save-replay-buffer \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8166537326501488
      ],
      "excerpt": "python scripts/plot_from_file.py -i logs/offpolicy.pkl --skip-timesteps --rliable --versus -l SAC TD3 TQC \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8216196964127496
      ],
      "excerpt": "python enjoy.py --algo algo_name --env env_id \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8838835258600217
      ],
      "excerpt": "python enjoy.py --algo a2c --env BreakoutNoFrameskip-v4 --folder rl-trained-agents/ -n 5000 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8461945561218402
      ],
      "excerpt": "python enjoy.py --algo algo_name --env env_id -f logs/ --exp-id 0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8265441744662457,
        0.8778769774812997,
        0.8792003947381468
      ],
      "excerpt": "python enjoy.py --algo algo_name --env env_id -f logs/ --exp-id 1 --load-best \nTo load a checkpoint (here the checkpoint name is rl_model_10000_steps.zip): \npython enjoy.py --algo algo_name --env env_id -f logs/ --exp-id 1 --load-checkpoint 10000 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8691320315157621
      ],
      "excerpt": "python enjoy.py --algo algo_name --env env_id -f logs/ --exp-id 1 --load-last-checkpoint \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8583468705850437
      ],
      "excerpt": "Hyperparameters not specified in utils/hyperparams_opt.py are taken from the associated YAML file and fallback to the default values of SB3 if not present. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8906829375720374
      ],
      "excerpt": "python train.py --algo ppo --env MountainCar-v0 -n 50000 -optimize --n-trials 1000 --n-jobs 2 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9490102656591566
      ],
      "excerpt": "python train.py --algo ppo --env MountainCar-v0 -optimize --study-name test --storage sqlite:///example.db \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8435875075253022
      ],
      "excerpt": "python scripts/parse_study.py -i path/to/study.pkl --print-n-best-trials 10 --save-n-best-hyperparameters 10 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8768801849289117
      ],
      "excerpt": "  normalize: \"{'norm_obs': True, 'norm_reward': False}\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8924976426181745
      ],
      "excerpt": "    - utils.wrappers.DoneOnSuccessWrapper: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8924976426181745
      ],
      "excerpt": "  - utils.callbacks.ParallelTrainCallback: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8506610094451736
      ],
      "excerpt": "python train.py --algo a2c --env MountainCarContinuous-v0 --hyperparams learning_rate:0.001 policy_kwargs:\"dict(net_arch=[64, 64])\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.81930139706216
      ],
      "excerpt": "python -m utils.record_video --algo ppo --env BipedalWalkerHardcore-v3 -n 1000 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8573193905885919,
        0.8576022926967136
      ],
      "excerpt": "Record a video of a checkpoint saved during training (here the checkpoint name is rl_model_10000_steps.zip): \npython -m utils.record_video --algo ppo --env BipedalWalkerHardcore-v3 -n 1000 --load-checkpoint 10000 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.822818836623903,
        0.8804890012980218,
        0.822818836623903
      ],
      "excerpt": "python -m utils.record_training --algo ppo --env CartPole-v1 -n 1000 -f logs --deterministic \nThe previous command will create a mp4 file. To convert this file to gif format as well: \npython -m utils.record_training --algo ppo --env CartPole-v1 -n 1000 -f logs --deterministic --gif \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9133368656218674
      ],
      "excerpt": "import gym_minigrid \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8148157936650192
      ],
      "excerpt": "USE_GPU=True make docker-gpu \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8477793186370367
      ],
      "excerpt": "To run tests, first install pytest, then: \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/DLR-RM/rl-baselines3-zoo/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell",
      "Makefile",
      "Dockerfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 Antonin RAFFIN\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "RL Baselines3 Zoo: A Training Framework for Stable Baselines3 Reinforcement Learning Agents",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "rl-baselines3-zoo",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "DLR-RM",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "araffin",
        "body": "\r\n**WARNING: This version will be the last one supporting Python 3.6 (end of life in Dec 2021). We highly recommended you to upgrade to Python >= 3.7.**\r\n\r\n### Breaking Changes\r\n- Upgrade to panda-gym 1.1.1\r\n- Upgrade to Stable-Baselines3 (SB3) >= 1.3.0\r\n- Upgrade to sb3-contrib >= 1.3.0\r\n\r\n### New Features\r\n- Added support for using rliable for performance comparison\r\n\r\n### Bug fixes\r\n- Fix training with Dict obs and channel last images\r\n\r\n### Other\r\n- Updated docker image\r\n- constrained gym version: gym>=0.17,<0.20\r\n- Better hyperparameters for A2C/PPO on Pendulum",
        "dateCreated": "2021-10-23T15:59:57Z",
        "datePublished": "2021-10-23T16:00:58Z",
        "html_url": "https://github.com/DLR-RM/rl-baselines3-zoo/releases/tag/v1.3.0",
        "name": "SB3 v1.3.0: rliable plots and bug fixes",
        "tag_name": "v1.3.0",
        "tarball_url": "https://api.github.com/repos/DLR-RM/rl-baselines3-zoo/tarball/v1.3.0",
        "url": "https://api.github.com/repos/DLR-RM/rl-baselines3-zoo/releases/51913323",
        "zipball_url": "https://api.github.com/repos/DLR-RM/rl-baselines3-zoo/zipball/v1.3.0"
      },
      {
        "authorType": "User",
        "author_name": "araffin",
        "body": "\r\n### Breaking Changes\r\n- Upgrade to Stable-Baselines3 (SB3) >= 1.2.0\r\n- Upgrade to sb3-contrib >= 1.2.0\r\n\r\n### Bug fixes\r\n- Fix `--load-last-checkpoint` (@SammyRamone)\r\n- Fix `TypeError` for `gym.Env` class entry points in `ExperimentManager` (@schuderer)\r\n- Fix usage of callbacks during hyperparameter optimization (@SammyRamone)\r\n\r\n\r\n### Other\r\n- Added python 3.9 to Github CI\r\n- Increased DQN replay buffer size for Atari games (@nikhilrayaprolu)\r\n",
        "dateCreated": "2021-09-08T11:39:32Z",
        "datePublished": "2021-09-08T11:40:24Z",
        "html_url": "https://github.com/DLR-RM/rl-baselines3-zoo/releases/tag/v1.2.0",
        "name": "SB3 v1.2.0",
        "tag_name": "v1.2.0",
        "tarball_url": "https://api.github.com/repos/DLR-RM/rl-baselines3-zoo/tarball/v1.2.0",
        "url": "https://api.github.com/repos/DLR-RM/rl-baselines3-zoo/releases/49191930",
        "zipball_url": "https://api.github.com/repos/DLR-RM/rl-baselines3-zoo/zipball/v1.2.0"
      },
      {
        "authorType": "User",
        "author_name": "araffin",
        "body": "\r\n### Breaking Changes\r\n- Upgrade to Stable-Baselines3 (SB3) >= 1.1.0\r\n- Upgrade to sb3-contrib >= 1.1.0\r\n- Add timeout handling (cf SB3 doc)\r\n- `HER` is now a replay buffer class and no more an algorithm\r\n- Removed `PlotNoiseRatioCallback`\r\n- Removed `PlotActionWrapper`\r\n- Changed `'lr'` key in Optuna param dict to `'learning_rate'` so the dict can be directly passed to SB3 methods (@justinkterry)\r\n\r\n### New Features\r\n- Add support for recording videos of best models and checkpoints (@mcres)\r\n- Add support for recording videos of training experiments (@mcres)\r\n- Add support for dictionary observations\r\n- Added experimental parallel training (with `utils.callbacks.ParallelTrainCallback`)\r\n- Added support for using multiple envs for evaluation\r\n- Added `--load-last-checkpoint` option for the enjoy script\r\n- Save Optuna study object at the end of hyperparameter optimization and plot the results (`plotly` package required)\r\n- Allow to pass multiple folders to `scripts/plot_train.py`\r\n- Flag to save logs and optimal policies from each training run (@justinkterry)\r\n\r\n### Bug fixes\r\n- Fixed video rendering for PyBullet envs on Linux\r\n- Fixed `get_latest_run_id()` so it works in Windows too (@NicolasHaeffner)\r\n- Fixed video record when using `HER` replay buffer\r\n\r\n### Documentation\r\n- Updated README (dict obs are now supported)\r\n\r\n### Other\r\n- Added `is_bullet()` to `ExperimentManager`\r\n- Simplify `close()` for the enjoy script\r\n- Updated docker image to include latest black version\r\n- Updated TD3 Walker2D model (thanks @modanesh)\r\n- Fixed typo in plot title (@scottemmons)\r\n- Minimum cloudpickle version added to `requirements.txt` (@amy12xx)\r\n- Fixed atari-py version (ROM missing in newest release)\r\n- Updated `SAC` and `TD3` search spaces\r\n- Cleanup eval_freq documentation and variable name changes (@justinkterry)\r\n- Add clarifying print statement when printing saved hyperparameters during optimization (@justinkterry)\r\n- Clarify n_evaluations help text (@justinkterry)\r\n- Simplified hyperparameters files making use of defaults\r\n- Added new TQC+HER agents\r\n- Add `panda-gym`environments (@qgallouedec)",
        "dateCreated": "2021-07-02T10:07:05Z",
        "datePublished": "2021-07-02T10:08:44Z",
        "html_url": "https://github.com/DLR-RM/rl-baselines3-zoo/releases/tag/v1.1.0",
        "name": "SB3 v1.1.0",
        "tag_name": "v1.1.0",
        "tarball_url": "https://api.github.com/repos/DLR-RM/rl-baselines3-zoo/tarball/v1.1.0",
        "url": "https://api.github.com/repos/DLR-RM/rl-baselines3-zoo/releases/45613394",
        "zipball_url": "https://api.github.com/repos/DLR-RM/rl-baselines3-zoo/zipball/v1.1.0"
      },
      {
        "authorType": "User",
        "author_name": "araffin",
        "body": "Blog post: https://araffin.github.io/post/sb3/\r\n\r\n\r\n### Breaking Changes\r\n- Upgrade to SB3 >= 1.0\r\n- Upgrade to sb3-contrib >= 1.0\r\n\r\n### New Features\r\n- Added 100+ trained agents + benchmark file\r\n- Add support for loading saved model under python 3.8+ (no retraining possible)\r\n- Added Robotics pre-trained agents (@sgillen)\r\n\r\n### Bug fixes\r\n- Bug fixes for `HER` handling action noise\r\n- Fixed double reset bug with `HER` and enjoy script\r\n\r\n### Documentation\r\n- Added doc about plotting scripts\r\n\r\n### Other\r\n- Updated `HER` hyperparameters",
        "dateCreated": "2021-03-17T13:56:30Z",
        "datePublished": "2021-03-17T14:28:38Z",
        "html_url": "https://github.com/DLR-RM/rl-baselines3-zoo/releases/tag/v1.0",
        "name": "Stable-Baselines3 v1.0 - 100+ pre-trained models",
        "tag_name": "v1.0",
        "tarball_url": "https://api.github.com/repos/DLR-RM/rl-baselines3-zoo/tarball/v1.0",
        "url": "https://api.github.com/repos/DLR-RM/rl-baselines3-zoo/releases/39945710",
        "zipball_url": "https://api.github.com/repos/DLR-RM/rl-baselines3-zoo/zipball/v1.0"
      },
      {
        "authorType": "User",
        "author_name": "araffin",
        "body": "\r\n### Breaking Changes\r\n- Removed `LinearNormalActionNoise`\r\n- Evaluation is now deterministic by default, except for Atari games\r\n- `sb3_contrib` is now required\r\n- `TimeFeatureWrapper` was moved to the contrib repo\r\n- Replaced old `plot_train.py` script with updated `plot_training_success.py`\r\n- Renamed ``n_episodes_rollout`` to ``train_freq`` tuple to match latest version of SB3\r\n\r\n### New Features\r\n- Added option to choose which `VecEnv` class to use for multiprocessing\r\n- Added hyperparameter optimization support for `TQC`\r\n- Added support for `QR-DQN` from SB3 contrib\r\n\r\n### Bug fixes\r\n- Improved detection of Atari games\r\n- Fix potential bug in plotting script when there is not enough timesteps\r\n- Fixed a bug when using HER + DQN/TQC for hyperparam optimization\r\n\r\n### Documentation\r\n- Improved documentation (@cboettig)\r\n\r\n### Other\r\n- Refactored train script, now uses a `ExperimentManager` class\r\n- Replaced `make_env` with SB3 built-in `make_vec_env`\r\n- Add more type hints (`utils/utils.py` done)\r\n- Use f-strings when possible\r\n- Changed `PPO` atari hyperparameters (removed vf clipping)\r\n- Changed `A2C` atari hyperparameters (eps value of the optimizer)\r\n- Updated benchmark script\r\n- Updated hyperparameter optim search space (commented gSDE for A2C/PPO)\r\n- Updated `DQN` hyperparameters for CartPole\r\n- Do not wrap channel-first image env (now natively supported by SB3)\r\n- Removed hack to log success rate\r\n- Simplify plot script",
        "dateCreated": "2021-02-27T19:28:23Z",
        "datePublished": "2021-02-27T19:33:15Z",
        "html_url": "https://github.com/DLR-RM/rl-baselines3-zoo/releases/tag/v0.11.1",
        "name": "Big refactor - SB3 upgrade - Last before v1.0",
        "tag_name": "v0.11.1",
        "tarball_url": "https://api.github.com/repos/DLR-RM/rl-baselines3-zoo/tarball/v0.11.1",
        "url": "https://api.github.com/repos/DLR-RM/rl-baselines3-zoo/releases/39000534",
        "zipball_url": "https://api.github.com/repos/DLR-RM/rl-baselines3-zoo/zipball/v0.11.1"
      },
      {
        "authorType": "User",
        "author_name": "araffin",
        "body": "",
        "dateCreated": "2020-10-28T14:44:19Z",
        "datePublished": "2020-10-28T16:40:37Z",
        "html_url": "https://github.com/DLR-RM/rl-baselines3-zoo/releases/tag/v0.10.0",
        "name": "",
        "tag_name": "v0.10.0",
        "tarball_url": "https://api.github.com/repos/DLR-RM/rl-baselines3-zoo/tarball/v0.10.0",
        "url": "https://api.github.com/repos/DLR-RM/rl-baselines3-zoo/releases/33175324",
        "zipball_url": "https://api.github.com/repos/DLR-RM/rl-baselines3-zoo/zipball/v0.10.0"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 463,
      "date": "Fri, 10 Dec 2021 02:18:11 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "rl",
      "reinforcement-learning",
      "stable-baselines",
      "openai",
      "gym",
      "pybullet",
      "hyperparameter-optimization",
      "hyperparameter-tuning",
      "hyperparameter-search",
      "optimization",
      "sde",
      "robotics",
      "lab",
      "pybullet-environments",
      "tuning-hyperparameters"
    ],
    "technique": "GitHub API"
  }
}