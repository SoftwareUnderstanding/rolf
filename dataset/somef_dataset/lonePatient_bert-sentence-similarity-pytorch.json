{
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lonePatient/bert-sentence-similarity-pytorch",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-02-14T13:32:20Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-08-26T13:22:37Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9882871813568004,
        0.9652771208610523
      ],
      "excerpt": "This repo contains a PyTorch implementation of a pretrained BERT model  for sentence similarity task. \nAt the root of the project, you will see: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "|  \u2514\u2500\u2500 model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8596982069667689
      ],
      "excerpt": "|  \u2514\u2500\u2500 output #:save the ouput of model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.974376041305329,
        0.9528725823047942
      ],
      "excerpt": "When converting the tensorflow checkpoint into the pytorch, it's expected to choice the \"bert_model.ckpt\", instead of \"bert_model.ckpt.index\", as the input file. Otherwise, you will see that the model can learn nothing and give almost same random outputs for any inputs. This means, in fact, you have not loaded the true ckpt for your model \nWhen using multiple GPUs, the non-tensor calculations, such as accuracy and f1_score, are not supported by DataParallel instance \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "This repo contains a PyTorch implementation of a pretrained BERT model  for sentence similarity task.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lonePatient/bert-sentence-similarity-pytorch/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 5,
      "date": "Sat, 11 Dec 2021 11:11:29 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/lonePatient/bert-sentence-similarity-pytorch/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "lonePatient/bert-sentence-similarity-pytorch",
    "technique": "GitHub API"
  },
  "invocation": [
    {
      "confidence": [
        0.9336801098518991,
        0.9336801098518991
      ],
      "excerpt": "|  |  \u2514\u2500\u2500 lrscheduler.py\u3000\u3000 \n|  |  \u2514\u2500\u2500 trainingmonitor.py\u3000 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8369245672576114
      ],
      "excerpt": "|  |  \u2514\u2500\u2500 basic_config.py #:a configuration file for storing model parameters \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9152750849795331,
        0.9336801098518991
      ],
      "excerpt": "|  |  \u2514\u2500\u2500 dataset.py\u3000\u3000 \n|  |  \u2514\u2500\u2500 data_transformer.py\u3000\u3000 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8277380791614312
      ],
      "excerpt": "|  \u2514\u2500\u2500 output #:save the ouput of model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8653523522410159,
        0.828520898273279
      ],
      "excerpt": "|  \u2514\u2500\u2500 train #:used for training a model \n|  |  \u2514\u2500\u2500 trainer.py  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8197328191806175,
        0.9336801098518991,
        0.9336801098518991,
        0.9336801098518991
      ],
      "excerpt": "|  \u2514\u2500\u2500 utils #: a set of utility functions \n\u251c\u2500\u2500 convert_tf_checkpoint_to_pytorch.py \n\u251c\u2500\u2500 train_bert_atec_nlp.py \n\u251c\u2500\u2500 data_join.py \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/lonePatient/bert-sentence-similarity-pytorch/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Bert sentence similarity by PyTorch",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "bert-sentence-similarity-pytorch",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "lonePatient",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lonePatient/bert-sentence-similarity-pytorch/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\n- csv\r\n- tqdm\r\n- numpy\r\n- pickle\r\n- scikit-learn\r\n- PyTorch 1.0\r\n- matplotlib\r\n- pandas\r\n- pytorch_pretrained_bert (load bert model)\r\n\r\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 45,
      "date": "Sat, 11 Dec 2021 11:11:29 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "sentence-similarity",
      "nlp",
      "pytorch",
      "bert",
      "text-classification"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\nyou need download pretrained chinese bert model (`chinese_L-12_H-768_A-12.zip`)\r\n\r\n1. Download the Bert pretrained model from [Google](https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip) and place it into the `/pybert/model/pretrain` directory.\r\n2. `pip install pytorch-pretrained-bert` from [github](https://github.com/huggingface/pytorch-pretrained-BERT).\r\n3. Run `python convert_tf_checkpoint_to_pytorch.py` to transfer the pretrained model(tensorflow version)  into pytorch form .\r\n4. Prepare [ATEC NLP data](https://dc.cloud.alipay.com/index#/topic/data?id=8), you can modify the `io.data_transformer.py` to adapt your data.\r\n5. Modify configuration information in `pybert/config/basic_config.py`(the path of data,...).\r\n6. Run `python data_join.py` \r\n7. Run `python train_bert_atec_nlp.py`.\r\n\r\n",
      "technique": "Header extraction"
    }
  ]
}