{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Code for LXMERT Model was adapted from [LXMERT](https://github.com/airsplay/lxmert) repo.\n- Entmax autograd function implementation was adapted from [entmax repo](https://github.com/deep-spin/entmax)\n\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2005.07486"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you use this work in any form, please cite the paper:\n```\n@inproceedings{bhargava-2020-adaptive,\n    title = \"Adaptive Transformers for Learning Multimodal Representations\",\n    author = \"Bhargava, Prajjwal\",\n    booktitle = \"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop\",\n    month = jul,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.acl-srw.1\",\n    doi = \"10.18653/v1/2020.acl-srw.1\",\n    pages = \"1--7\",\n    abstract = \"The usage of transformers has grown from learning about language semantics to forming meaningful visiolinguistic representations. These architectures are often over-parametrized, requiring large amounts of computation. In this work, we extend adaptive approaches to learn more about model interpretability and computational efficiency. Specifically, we study attention spans, sparse, and structured dropout methods to help understand how their attention mechanism extends for vision and language tasks. We further show that these approaches can help us learn more about how the network perceives the complexity of input sequences, sparsity preferences for different modalities, and other related phenomena.\",\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{bhargava-2020-adaptive,\n    title = \"Adaptive Transformers for Learning Multimodal Representations\",\n    author = \"Bhargava, Prajjwal\",\n    booktitle = \"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop\",\n    month = jul,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.acl-srw.1\",\n    doi = \"10.18653/v1/2020.acl-srw.1\",\n    pages = \"1--7\",\n    abstract = \"The usage of transformers has grown from learning about language semantics to forming meaningful visiolinguistic representations. These architectures are often over-parametrized, requiring large amounts of computation. In this work, we extend adaptive approaches to learn more about model interpretability and computational efficiency. Specifically, we study attention spans, sparse, and structured dropout methods to help understand how their attention mechanism extends for vision and language tasks. We further show that these approaches can help us learn more about how the network perceives the complexity of input sequences, sparsity preferences for different modalities, and other related phenomena.\",\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9792428879788975
      ],
      "excerpt": "Paper: arXiv \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "    \"nb_heads\": 12, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9056674988540252
      ],
      "excerpt": "    \"layer_sizes\": {\"lang\": 9, \"cross\": 5, \"vision\": 5}, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266,
        0.9593299683604384
      ],
      "excerpt": "| w/ Layerdrop (10-6-6, p=1)            | 66.4     | 66.72    | \n| w/ Layerdrop (10-6-6, p=0)            | 66.35    | 66.57    | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/prajjwal1/adaptive_transformer",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-01-11T11:13:45Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-05T13:00:27Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8576214973909165
      ],
      "excerpt": "Please check the params dict when starting training to see the configurations. Config should match with the config used in loaded model.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9755684086875477
      ],
      "excerpt": "NOTE: Number of layers params['layer_sizes'] have to match with number of layers in the model checkpoint. To perform pruning during inference, default learn.load method is not suitable as it loads all the layers. Please refer to this fairseq issue to perform pruning during inference. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8529186850782022
      ],
      "excerpt": "models: Contains implmentation of adaptive mechanisms and LXMERT \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9839480391484309,
        0.923948005372906
      ],
      "excerpt": "optimizers: implementation of LAMB and Lookahead optimizer \npretrain: utility tools \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8492015425907283
      ],
      "excerpt": "learner.py: Implements a Learner class to control all functionalities of this codebase. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8623361257294204
      ],
      "excerpt": "Please refer to nbs/inference.ipynb to load your trained model, obtain predictions and visualize the results. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9530063577566298
      ],
      "excerpt": "Our model achives the following performance on the VQA 2.0 benchmark: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Code for the paper \"Adaptive Transformers for Learning Multimodal Representations\" (ACL SRW 2020)",
      "technique": "GitHub API"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please download the pretrained models from this [Google drive link](https://drive.google.com/drive/folders/1V1SjSfGCqBJZi2INzCmNKxnoXI4FnVeP?usp=sharing)\n\nAlternatively, if you want to train (finetune) the model yourself, download the pretrained weights from [here](http://nlp1.cs.unc.edu/data/model_LXRT.pth). Skip this step if you're using my weights.\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/prajjwal1/adaptive_transformer/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 7,
      "date": "Fri, 10 Dec 2021 15:14:42 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/prajjwal1/adaptive_transformer/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "prajjwal1/adaptive_transformer",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/prajjwal1/adaptive_transformer/master/nbs/attention_analysis.ipynb",
      "https://raw.githubusercontent.com/prajjwal1/adaptive_transformer/master/nbs/adaptive_train.ipynb",
      "https://raw.githubusercontent.com/prajjwal1/adaptive_transformer/master/nbs/attention_analysis_v2.ipynb",
      "https://raw.githubusercontent.com/prajjwal1/adaptive_transformer/master/nbs/nan.ipynb",
      "https://raw.githubusercontent.com/prajjwal1/adaptive_transformer/master/nbs/graph_plots.ipynb",
      "https://raw.githubusercontent.com/prajjwal1/adaptive_transformer/master/nbs/flops.ipynb",
      "https://raw.githubusercontent.com/prajjwal1/adaptive_transformer/master/nbs/data_vqa.ipynb",
      "https://raw.githubusercontent.com/prajjwal1/adaptive_transformer/master/nbs/lxmert_arch.ipynb",
      "https://raw.githubusercontent.com/prajjwal1/adaptive_transformer/master/nbs/data_scratch.ipynb",
      "https://raw.githubusercontent.com/prajjwal1/adaptive_transformer/master/nbs/span.ipynb",
      "https://raw.githubusercontent.com/prajjwal1/adaptive_transformer/master/nbs/train.ipynb",
      "https://raw.githubusercontent.com/prajjwal1/adaptive_transformer/master/nbs/layerdrop.ipynb",
      "https://raw.githubusercontent.com/prajjwal1/adaptive_transformer/master/nbs/inference.ipynb",
      "https://raw.githubusercontent.com/prajjwal1/adaptive_transformer/master/nbs/alpha_extract.ipynb",
      "https://raw.githubusercontent.com/prajjwal1/adaptive_transformer/master/nbs/Spase%20entmax.ipynb",
      "https://raw.githubusercontent.com/prajjwal1/adaptive_transformer/master/nbs/interpretability.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/prajjwal1/adaptive_transformer/master/run_test.sh",
      "https://raw.githubusercontent.com/prajjwal1/adaptive_transformer/master/run_train.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\npython train.py --bs=128 --test --adaptive --load_model=adaptive_6910\n```\nWhen `test` flag is passed, only inference is performed on the test set. Ground truths for test set for VQA are not publicly available. This command will dump the JSON file in the `/snap` directory. Submit the JSON file through the [EvalAI competition page](https://evalai.cloudcv.org/web/challenges/challenge-page/514/overview).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "- Download the raw VQA 2.0 dataset from the [official website](https://visualqa.org/download.html).\n\nMake sure that your data directory looks similar to the following structure (you can change the paths if you want a different structure in `train.py`).\n\n- These instructions are from [LXMERT repo]((https://github.com/airsplay/lxmert#vqa)). Download the re-distributed JSON files.\n```\nmkdir -p data/vqa\nwget --no-check-certificate https://nlp1.cs.unc.edu/data/lxmert_data/vqa/train.json -P data/vqa/\nwget --no-check-certificate https://nlp1.cs.unc.edu/data/lxmert_data/vqa/nominival.json -P  data/vqa/\nwget --no-check-certificate https://nlp1.cs.unc.edu/data/lxmert_data/vqa/minival.json -P data/vqa/\n```\nFor downloading FasterRCNN features, use these instructions:\n```\nmkdir -p data/mscoco_imgfeat\nwget --no-check-certificate https://nlp1.cs.unc.edu/data/lxmert_data/mscoco_imgfeat/train2014_obj36.zip -P data/mscoco_imgfeat\nunzip data/mscoco_imgfeat/train2014_obj36.zip -d data/mscoco_imgfeat && rm data/mscoco_imgfeat/train2014_obj36.zip\nwget --no-check-certificate https://nlp1.cs.unc.edu/data/lxmert_data/mscoco_imgfeat/val2014_obj36.zip -P data/mscoco_imgfeat\nunzip data/mscoco_imgfeat/val2014_obj36.zip -d data && rm data/mscoco_imgfeat/val2014_obj36.zip\n```\nIf the links don't work, you can use [Google drive link](https://drive.google.com/drive/folders/1Gq1uLUk6NdD0CcJOptXjxE6ssY5XAuat?usp=sharing) to get access. For more details, please refer [LXMERT repo](https://github.com/airsplay/lxmert).\n\nSetup the directory structure like this:\nIn `/home/user/`\n```\n+-- data\n|   +-- lxmert\n|   +-- mscoco_imgfeat\n|   +-- vqa\n+-- adaptive_transformer\n+-- snap\n.......\n```\nCreate a directory snap, that's where checkpoints will be store by default.\nAll of this structure can be changed but suitable modifications will be needed in `train.py`.\n\nFasterRCNN features are loaded all at once in the RAM, so you'd require an instance with >48 GB of RAM. For training, I used a single P100 Nvidia GPU. \n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9879863063452118,
        0.9906248903846466
      ],
      "excerpt": "$ git clone https://github.com/prajjwal1/adaptive_transformer \n$ cd adaptive_transformer \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9416612099375058
      ],
      "excerpt": "$ python3 train.py --bs=128 --epochs=1 --sparse --tiny #:test script \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9503189345333785,
        0.802422095963417
      ],
      "excerpt": "python train.py \n    [--bs]            #: Specify the batch size \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8708376126950123,
        0.834175144906982
      ],
      "excerpt": "    [--test]          #: Dumps a JSON file for submission to VQA servers. \nMore customizations can be done by modifying the params and config dict in train.py.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8060372108626078
      ],
      "excerpt": "params = { \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "    \"adapt_span_cache\": True, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8405210537621255
      ],
      "excerpt": "    \"sparse_enabled\": args.sparse, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8405210537621255
      ],
      "excerpt": "    \"sparse_enable\": args.sparse, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8807557722099326
      ],
      "excerpt": "python train.py --bs=128 --epochs=1 --adaptive --tiny \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9256763787118937,
        0.8969477472584729,
        0.85362235383073,
        0.8060372108626078
      ],
      "excerpt": "python train.py --bs=128 --epochs=1 --sparse --tiny \npython train.py --bs=128 --epochs=1 --layerdrop --tiny \nSpecify the following as per use case in train.py: \n- params['layerdrop_num_layers'] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8078119643681451,
        0.8807557722099326,
        0.9256763787118937,
        0.8969477472584729
      ],
      "excerpt": "To load a model trained with adaptive or sparse or layerdrop flag: \npython train.py --bs=128 --epochs=1 --adaptive --tiny --load_model=adaptive_6910 \npython train.py --bs=128 --epochs=1 --sparse --tiny --load_model=sparse_7 \npython train.py --bs=128 --epochs=1 --layerdrop --load_model=layerdrop_1066_ldrop_1 --tiny \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8296166570005067
      ],
      "excerpt": "| Model                                 | test-dev | test-std | \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/prajjwal1/adaptive_transformer/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Adaptive Transformers for Learning Multimodal Representations",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "adaptive_transformer",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "prajjwal1",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/prajjwal1/adaptive_transformer/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please refer `requirements.txt`.\nTo install,\n```\n$ pip install -r requirements.txt \n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\npython train.py --bs=128 --test --adaptive --load_model=adaptive_6910\n```\nWhen `test` flag is passed, only inference is performed on the test set. Ground truths for test set for VQA are not publicly available. This command will dump the JSON file in the `/snap` directory. Submit the JSON file through the [EvalAI competition page](https://evalai.cloudcv.org/web/challenges/challenge-page/514/overview).\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 38,
      "date": "Fri, 10 Dec 2021 15:14:42 GMT"
    },
    "technique": "GitHub API"
  }
}