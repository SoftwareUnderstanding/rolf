{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1505.04597"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "4. Person/animal - 0.151 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "10. Truck - 0.238 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "12. Vehicle Fallback - 0.246 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/zhiggins11/Semantic-Segmentation",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-02-25T10:16:14Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-04-01T17:54:26Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9968659835150195,
        0.8021159422192181
      ],
      "excerpt": "This is work that I did as part of a group project (with Lingxi Li, Yejun Li, Jiawen Zeng, and Yunyi Zhang) for CSE 251B (Neural Networks).  I benefited from discussions with my partners, but all code here was either given by the instructor or written by me.  Specifically, I created the model (basic_fcn.py) and wrote the code for training the model and evaluating it on the validation and tests sets (starter.py and util.py), and the instructor gave us all the code for loading the data. \nThis project has a basic fully convolutional neural network that can be used for semantic segmentation.  It uses a subset of the India Driving Dataset to train, validate, and test the models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8088313626598957
      ],
      "excerpt": "get_weights.py contains code for computing weights for each of the classes.  These weights can be used to train a model using weighted cross entropy loss.\\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9346840184160476,
        0.8807714519910266,
        0.9775738332311185,
        0.846452611050068
      ],
      "excerpt": "Here are some test images where the model performs well.  Each strip contains the actual image, ground truth labels for that image, and model predictions, in that order. \nHere are some test images where the model doesn't perform very well.  Each strip contains the actual image, groud truth labels, and model predictions, in that order. \nClearly, the model does not do very well on classes like 'people', 'billboard', and 'motorcycle' which only account for a small portion of the pixels in the training dataset.  I'm hoping that using dice loss (or weighted cross entropy) should fix this issue. \nThe trained model, which can be loaded from latest_model.pt was evaluated on the test set, and gave a pixel accuracy of 0.8134 as well as the following intersection-over-union (IoU) values on each category\\ \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/zhiggins11/Semantic-Segmentation/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sun, 12 Dec 2021 17:51:43 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/zhiggins11/Semantic-Segmentation/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "zhiggins11/Semantic-Segmentation",
    "technique": "GitHub API"
  },
  "invocation": [
    {
      "confidence": [
        0.8654344252583838
      ],
      "excerpt": "dataloader.py contains code for loading training, validation, and test datasets.\\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8750750002691753
      ],
      "excerpt": "starter.py contains code needed to train a model.  If you would like to run this, you'll need to download (some portion of) the India Driving Dataset and save links to each of the training, validation, and test images to files train.csv, val.csv, and test.csv, respectively, in your working directory.\\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.811854372964597
      ],
      "excerpt": "15. Fence - 0.065 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.855327623807029
      ],
      "excerpt": "21. Obs-str-bar-fallback - 0.130 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/zhiggins11/Semantic-Segmentation/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Semantic Segmentation Using a Fully Convolutional Network",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Semantic-Segmentation",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "zhiggins11",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/zhiggins11/Semantic-Segmentation/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sun, 12 Dec 2021 17:51:43 GMT"
    },
    "technique": "GitHub API"
  }
}