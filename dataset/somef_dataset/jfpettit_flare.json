{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1707.06347",
      "https://arxiv.org/abs/1602.01783",
      "https://arxiv.org/abs/1707.06347",
      "https://arxiv.org/abs/1509.02971",
      "https://arxiv.org/abs/1802.09477",
      "https://arxiv.org/abs/1801.01290",
      "https://arxiv.org/abs/1707.06347",
      "https://arxiv.org/abs/1602.01783"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- [OpenAI SpinningUp](https://spinningup.openai.com/en/latest/)\n- [FiredUp](https://github.com/kashif/firedup)\n- [PPO paper](https://arxiv.org/abs/1707.06347)\n- [A3C paper](https://arxiv.org/abs/1602.01783)\n- [Pytorch RL examples](https://github.com/pytorch/examples/tree/master/reinforcement_learning)\n- [PyTorch Lightning RL example](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pl_examples/domain_templates/reinforce_learn_Qnet.py)\n\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jfpettit/flare",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contributing to flare\nTable of Contents\n\nMain Design Values\nContribution types\nDocumentation\nCoding guidelines\nTesting\nOther questions\n\nThank you for taking the time to contribute! :+1:\nThe aim of flare is to provide a reliable implementation of many common reinforcement learning algorithms (and eventually some not so common ones) and to unify those algorithms under a common API as much as possible. There are two efforts to do this. The first is a very \"plug and play\" style set-up, where a user can plug whatever environment they want to train on into an algorithm and get decent results. The second, the flare.kindling submodule, aims to expose building blocks of RL algorithms so that when a user wants to work on a custom algorithm, they can use the pre-built bits provided and only have to worry about creating their custom/new algorithm from scratch.\nMain Design values: Clarity, Performance, and Composability\nWe want to accelerate the rate of progress for RL researchers and reduce the time from idea to experimentation. This means that everything written needs to be understandable, must work well, and must be highly composable.\nPytorchLightning has excellent values and contributor guidelines. We take much inspiration from them, and any overlap between their guidelines and ours is probably intentional on our part. :smile:\n\nClarity:\nThings like clear, concise variable names, thorough commenting and documentation, and consistency in code structure.\nAs PytorchLightning says, many users won't be engineers. Clear code is much more important than fanciness and slick moves.\nWe want to keep the API as simple as possible. The external API should allow a user to import an algorithm, set up their RL environment, and train that algorithm simply by calling a learn() function. In flare.kindling, the API should be consistent and flat, without obfuscating division between functional groups.\n\n\nPerformance:\nHigh quality performance without compromising clarity, and faithful implementations of algorithms.\nEstablished best-practice hyperparameters so that a user can get good results without worrying about their hyperparameter setup.\n\n\nComposability:\nThe components in flare.kindling must be easily composed into full algorithms.\n\n\n\nThese values are by no means things that have already been achieved, but are rather goals to work towards and strive for, moving forward.\nContribution types\nPresently, we are looking for people to help with: \n    - Implementing new algorithms \n    - Bug fixes\n    - Unifying the codebase API\n    - Writing unit tests and continuous integration\n    - Writing documentation\n    - General mechanics of the project\nFor most things:\n    1. Submit an issue on GitHub and we can discuss the bug fix/new feature/new algorithm/contribution.\n    2. For bugs, try fixing it yourself or recommend a fix!\n    3. After discussion, submit a pull request! Of course please update the docs and tests.\nUse your best judgment to add relevant tags to your issues/PRs.\nDocumentation\nHas yet to be set up... Currently working on this.\nCoding guidelines\n\nUse f-strings for output formatting.\nUse black or flake8 to format the code. The code so far has just been formatted with black on default settings, so it is OK to do the same.\n\nTesting\nWe need to (and need help with!) build unit tests and implement continuous integration for the code.\nOther questions\nFeel free to open an issue with your question! Maybe use a tag like general-question or something similar.",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-03-05T19:28:40Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-02-02T14:51:11Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8133140753893953,
        0.9901389409884188,
        0.9335475521141142
      ],
      "excerpt": "More to come \nflare is a small reinforcement learning library. Currently, the use case for this library is small-scale RL experimentation/research. Much of the code is refactored from and built off of SpinningUp, so massive thanks to them for writing quality, understandable, and performant code. \n(old) Blog post about this repository here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9522934887021445
      ],
      "excerpt": "This repository is intended to be a lightweight and simple to use RL framework, while still getting good performance. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9328483275600182
      ],
      "excerpt": "REINFORCE Policy Gradient (this link is to a PDF) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9815173703105142
      ],
      "excerpt": "We'd love for you to contribute! Any help is welcome. See CONTRIBUTING.md for contributor guidelines and info. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Modular Reinforcement Learning in PyTorch.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jfpettit/flare/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Mon, 06 Dec 2021 03:39:12 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jfpettit/flare/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "jfpettit/flare",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/jfpettit/flare/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/jfpettit/flare/master/Untitled.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "It is recommended to use a virtual env before installing this, to avoid conflicting with other installed packages. Anaconda and Python offer virtual environment systems.\n\nClone the repository and cd into it: \n\n```\ngit clone https://github.com/jfpettit/flare.git\ncd flare\n```\n\n**The next step depends on your package manager.**\n\nIf you are using pip, pip install the ```requirements``` file:\n\n```\npip install -r requirements.txt\n```\n\nAlternatively, if you're using Anaconda, create a new Anaconda env from the ```environments.yml``` file, and activate your new conda environment:\n\n```\nconda env create -f environment.yml\nconda activate flare\n```\n\nA third option, if you don't want to clone a custom environment or run through the ```requirements.txt``` file, is to simply pip install the repository via:\n\n```\npip install -e git+https://github.com/jfpettit/flare.git@98d6d3e74dfadc458b1197d995f6d60ef516f1ee#:egg=flare\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "**MPI parallelization will soon be removed. Work is being done to rebase the code using [PyTorch Lightning](https://pytorch-lightning.readthedocs.io/en/latest/) which uses PyTorch's multiprocessing under the hood.**\n\n**Flare supports parallelization via MPI!** So, you'll need to install [OpenMPI](https://www.open-mpi.org/) to run this code. [SpinningUp](https://spinningup.openai.com/en/latest/user/installation.html#installing-openmpi) provides the following installation instructions:\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9877246626844249,
        0.9858636962450815,
        0.8341801901494347,
        0.9419724122170332
      ],
      "excerpt": "sudo apt-get update &amp;&amp; sudo apt-get install libopenmpi-dev \nbrew install openmpi \nIf using homebrew doesn't work for you, consider these instructions. \nIf you're on Windows, here is a link to some instructions. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8175266100350335
      ],
      "excerpt": "If you wish to build your own actor-critic from scratch, then it is recommended to use the FireActorCritic as a template. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jfpettit/flare/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 Jacob Pettit\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "flare",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "flare",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "jfpettit",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jfpettit/flare/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Each algorithm implemented can be run from the command line. A good way to test your installation is to do the following:\n\n```\npython -m flare.run\n```\n\nThis will run [PPO](https://arxiv.org/abs/1707.06347) on [LunarLander-v2](https://gym.openai.com/envs/LunarLander-v2/) with default arguments. If you want to change the algorithm to A2C, run on a different env, or otherwise change some defaults with this command line interface, then do ```python -m flare.run -h``` to see the available optional arguments.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Import required packages:\n\n```python\nimport gym\nfrom flare.polgrad import a2c \n\nenv = gym.make('CartPole-v0') #: or other gym env\nepochs = 100\na2c.learn(env, epochs)\n```\n\nThe above snippet will train an agent on the [CartPole environment](http://gym.openai.com/envs/CartPole-v1/) for 100 epochs. \n\nYou may alter the architecture of your actor-critic network by passing in a tuple of hidden layer sizes to your agent initialization. i.e.:\n\n```python\nfrom flare.polgrad import ppo \nhidden_sizes = (64, 32)\nppo.learn(env, epochs=100, hidden_sizes=hidden_sizes)\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Mon, 06 Dec 2021 03:39:12 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "reinforcement-learning-algorithms",
      "reinforcement-learning",
      "neural-networks",
      "ppo-agent",
      "gym",
      "deep-reinforcement-learning",
      "machine-learning",
      "machine-learning-algorithms",
      "deep-learning",
      "pytorch",
      "rew",
      "leng",
      "a2c"
    ],
    "technique": "GitHub API"
  }
}