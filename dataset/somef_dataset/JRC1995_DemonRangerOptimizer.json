{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1703.04782, https://github.com/gbaydin/hypergradient-descent",
      "https://arxiv.org/abs/1910.04209v1",
      "https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980",
      "https://arxiv.org/abs/1412.6980",
      "https://arxiv.org/abs/1904.09237\n * QHAdam: https://arxiv.org/abs/1810.06801\n * Gradient Noise: https://arxiv.org/abs/1511.06807\n * AdamW: https://arxiv.org/abs/1711.05101\n * RAdam: https://arxiv.org/abs/1908.03265, https://github.com/LiyuanLucasLiu/RAdam\n * More on RAdam: https://arxiv.org/abs/1910.04209v1\n * Lookahead: https://arxiv.org/abs/1907.08610\n * Ranger: https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer\n * Gradient Centralization: https://arxiv.org/abs/2004.01461v2\n * DEMON (Decaying Momentum",
      "https://arxiv.org/abs/1810.06801\n * Gradient Noise: https://arxiv.org/abs/1511.06807\n * AdamW: https://arxiv.org/abs/1711.05101\n * RAdam: https://arxiv.org/abs/1908.03265, https://github.com/LiyuanLucasLiu/RAdam\n * More on RAdam: https://arxiv.org/abs/1910.04209v1\n * Lookahead: https://arxiv.org/abs/1907.08610\n * Ranger: https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer\n * Gradient Centralization: https://arxiv.org/abs/2004.01461v2\n * DEMON (Decaying Momentum",
      "https://arxiv.org/abs/1511.06807\n * AdamW: https://arxiv.org/abs/1711.05101\n * RAdam: https://arxiv.org/abs/1908.03265, https://github.com/LiyuanLucasLiu/RAdam\n * More on RAdam: https://arxiv.org/abs/1910.04209v1\n * Lookahead: https://arxiv.org/abs/1907.08610\n * Ranger: https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer\n * Gradient Centralization: https://arxiv.org/abs/2004.01461v2\n * DEMON (Decaying Momentum",
      "https://arxiv.org/abs/1711.05101\n * RAdam: https://arxiv.org/abs/1908.03265, https://github.com/LiyuanLucasLiu/RAdam\n * More on RAdam: https://arxiv.org/abs/1910.04209v1\n * Lookahead: https://arxiv.org/abs/1907.08610\n * Ranger: https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer\n * Gradient Centralization: https://arxiv.org/abs/2004.01461v2\n * DEMON (Decaying Momentum",
      "https://arxiv.org/abs/1908.03265, https://github.com/LiyuanLucasLiu/RAdam\n * More on RAdam: https://arxiv.org/abs/1910.04209v1\n * Lookahead: https://arxiv.org/abs/1907.08610\n * Ranger: https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer\n * Gradient Centralization: https://arxiv.org/abs/2004.01461v2\n * DEMON (Decaying Momentum",
      "https://arxiv.org/abs/1910.04209v1\n * Lookahead: https://arxiv.org/abs/1907.08610\n * Ranger: https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer\n * Gradient Centralization: https://arxiv.org/abs/2004.01461v2\n * DEMON (Decaying Momentum",
      "https://arxiv.org/abs/1907.08610\n * Ranger: https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer\n * Gradient Centralization: https://arxiv.org/abs/2004.01461v2\n * DEMON (Decaying Momentum",
      "https://arxiv.org/abs/2004.01461v2\n * DEMON (Decaying Momentum",
      "https://arxiv.org/abs/1910.04952\n * AdaMod: https://arxiv.org/abs/1910.12249\n * GAdam (Iterate Averaging",
      "https://arxiv.org/abs/1910.12249\n * GAdam (Iterate Averaging",
      "https://arxiv.org/abs/2003.01247, https://github.com/diegogranziol/Gadam\n * Hypergradient Descent: https://arxiv.org/abs/1703.04782, https://github.com/gbaydin/hypergradient-descent\n * Nostalgic Adam: https://arxiv.org/abs/1805.07557, https://github.com/andrehuang/NostalgicAdam-NosAdam\n * PAdam: https://arxiv.org/abs/1806.06763, https://github.com/uclaml/Padam, https://arxiv.org/pdf/1901.09517.pdf\n * LaProp: https://arxiv.org/abs/2002.04839",
      "https://arxiv.org/abs/1703.04782, https://github.com/gbaydin/hypergradient-descent\n * Nostalgic Adam: https://arxiv.org/abs/1805.07557, https://github.com/andrehuang/NostalgicAdam-NosAdam\n * PAdam: https://arxiv.org/abs/1806.06763, https://github.com/uclaml/Padam, https://arxiv.org/pdf/1901.09517.pdf\n * LaProp: https://arxiv.org/abs/2002.04839",
      "https://arxiv.org/abs/1805.07557, https://github.com/andrehuang/NostalgicAdam-NosAdam\n * PAdam: https://arxiv.org/abs/1806.06763, https://github.com/uclaml/Padam, https://arxiv.org/pdf/1901.09517.pdf\n * LaProp: https://arxiv.org/abs/2002.04839",
      "https://arxiv.org/abs/1806.06763, https://github.com/uclaml/Padam, https://arxiv.org/pdf/1901.09517.pdf\n * LaProp: https://arxiv.org/abs/2002.04839",
      "https://arxiv.org/abs/2002.04839"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": " \n * Adam: [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)\n * AMSGrad: https://arxiv.org/abs/1904.09237\n * QHAdam: https://arxiv.org/abs/1810.06801\n * Gradient Noise: https://arxiv.org/abs/1511.06807\n * AdamW: https://arxiv.org/abs/1711.05101\n * RAdam: https://arxiv.org/abs/1908.03265, https://github.com/LiyuanLucasLiu/RAdam\n * More on RAdam: https://arxiv.org/abs/1910.04209v1\n * Lookahead: https://arxiv.org/abs/1907.08610\n * Ranger: https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer\n * Gradient Centralization: https://arxiv.org/abs/2004.01461v2\n * DEMON (Decaying Momentum): https://arxiv.org/abs/1910.04952\n * AdaMod: https://arxiv.org/abs/1910.12249\n * GAdam (Iterate Averaging): https://arxiv.org/abs/2003.01247, https://github.com/diegogranziol/Gadam\n * Hypergradient Descent: https://arxiv.org/abs/1703.04782, https://github.com/gbaydin/hypergradient-descent\n * Nostalgic Adam: https://arxiv.org/abs/1805.07557, https://github.com/andrehuang/NostalgicAdam-NosAdam\n * PAdam: https://arxiv.org/abs/1806.06763, https://github.com/uclaml/Padam, https://arxiv.org/pdf/1901.09517.pdf\n * LaProp: https://arxiv.org/abs/2002.04839\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9906895360417073
      ],
      "excerpt": "Dense-sparse-Dense Training: https://arxiv.org/pdf/1607.04381.pdf \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/JRC1995/DemonRangerOptimizer",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-04-18T23:24:41Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-09T02:37:11Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8513716651519515,
        0.9584223863975574,
        0.9523323803501657,
        0.9106719665638855
      ],
      "excerpt": "Quasi Hyperbolic Rectified DEMON (Decaying Momentum) Adam/Amsgrad with AdaMod,  Lookahead, iterate averaging, and decorrelated weight decay. \nAlso, other variants with Nostalgia (NosAdam), P (from PAdam), LaProp, and Hypergradient Descent (see HyperRanger and HyperRangerMod and others in optimizers.py) \nHyperxxx series optimizers implements hypergradient descent for dynamic learning rate updates. Some optimizers like  HDQHSGDW implements hypergradient descent for all hyperparameters - beta, nu, lr. Unlike the original implementation (https://arxiv.org/abs/1703.04782, https://github.com/gbaydin/hypergradient-descent) they take care of the gradients due to the weight decay and other things. (I also implement state level lr so that lr for each parameters will be hypertuned through hypergradient descent separately instead of in the group level like in the original implementation) \nLRangerMod uses Linear Warmup within Adam/AMSGrad based on the rule of thumb as in (https://arxiv.org/abs/1910.04209v1). Note Rectified Adam boils down to a fixed (not dynamic) form of learning rate scheduling similar to a linear warmup.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9480395739825721
      ],
      "excerpt": ": just do optimizer.step(IA_activate=IA_activate) when necessary (change IA_activate to True near the end of training based on some scheduling scheme or tuned hyperparameter--- alternative to learning rate scheduling) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9480395739825721
      ],
      "excerpt": ": just do optimizer.step(IA_activate=IA_activate) when necessary (change IA_activate to True near the end of training based on some scheduling scheme or tuned hyperparameter--- alternative to learning rate scheduling) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Quasi Hyperbolic Rectified DEMON Adam/Amsgrad with AdaMod, Gradient Centralization, Lookahead, iterative averaging and decorrelated Weight Decay",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/JRC1995/DemonRangerOptimizer/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 5,
      "date": "Mon, 13 Dec 2021 06:50:15 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/JRC1995/DemonRangerOptimizer/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "JRC1995/DemonRangerOptimizer",
    "technique": "GitHub API"
  },
  "invocation": [
    {
      "confidence": [
        0.8107737471051991
      ],
      "excerpt": "optimizer = DemonRanger(params=model.parameters(), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8107737471051991
      ],
      "excerpt": "optimizer = DemonRanger(params=model.parameters(), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "                        amsgrad=True #: disables amsgrad \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8107737471051991
      ],
      "excerpt": "optimizer = DemonRanger(params=model.parameters(), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8107737471051991
      ],
      "excerpt": "optimizer = DemonRanger(params=model.parameters(), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8107737471051991
      ],
      "excerpt": "optimizer = DemonRanger(params=model.parameters(), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8107737471051991
      ],
      "excerpt": "optimizer = DemonRanger(params=model.parameters(), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8107737471051991
      ],
      "excerpt": "optimizer = DemonRanger(params=model.parameters(), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8107737471051991
      ],
      "excerpt": "optimizer = DemonRanger(params=model.parameters(), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8107737471051991
      ],
      "excerpt": "optimizer = DemonRanger(params=model.parameters(), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8107737471051991
      ],
      "excerpt": "optimizer = DemonRanger(params=model.parameters(), \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/JRC1995/DemonRangerOptimizer/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "DemonRangerOptimizer",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "DemonRangerOptimizer",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "JRC1995",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/JRC1995/DemonRangerOptimizer/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 13,
      "date": "Mon, 13 Dec 2021 06:50:15 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "qhadam",
      "demon",
      "decay-momentum",
      "adamod",
      "radam",
      "adam",
      "gradient-centralization",
      "lookahead",
      "amsgrad",
      "iterate-averaging",
      "adamw",
      "ranger",
      "qhranger",
      "optimizer",
      "adaptive-optimizer",
      "hypergradient-descent",
      "hd-adam",
      "hd-sgd",
      "nosadam",
      "nostalgic-adam"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\nfrom optimizers import DemonRanger\nfrom dataloader import batcher #: some random function to batch data\n\nclass config:\n   def __init__(self):\n       self.batch_size = ...\n       self.wd = ...\n       self.lr = ...\n       self.epochs = ...\n       \n       \nconfig = config()\n   \n\ntrain_data = ...\nstep_per_epoch = count_step_per_epoch(train_data,config.batch_size)\n\nmodel = module(stuff)\n\noptimizer = DemonRanger(params=model.parameters(),\n                        lr=config.lr,\n                        weight_decay=config.wd,\n                        epochs=config.epochs,\n                        step_per_epoch=step_per_epoch,\n                        IA_cycle=step_per_epoch)\nIA_activate = False                      \nfor epoch in range(config.epochs):\n    batches = batcher(train_data, config.batch_size)\n    \n    for batch in batches:\n        loss = do stuff\n        loss.backward()\n        optimizer.step(IA_activate=IA_activate)\n    \n    #: automatically enable IA (Iterate Averaging) near the end of training (when metric of your choice not improving for a while)\n    if (IA_patience running low) and IA_activate is False:\n        IA_activate = True \n        \n\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```\noptimizer = DemonRanger(params=model.parameters(),\n                        lr=config.lr,\n                        weight_decay=config.wd,\n                        epochs=config.epochs,\n                        step_per_epoch=step_per_epoch,\n                        IA_cycle=step_per_epoch)\n #: just do optimizer.step(IA_activate=IA_activate) when necessary (change IA_activate to True near the end of training based on some scheduling scheme or tuned hyperparameter--- alternative to learning rate scheduling)\n ```\n \n",
      "technique": "Header extraction"
    }
  ]
}