{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2102.11582",
      "https://arxiv.org/abs/2102.11582",
      "https://arxiv.org/abs/2102.11582",
      "https://arxiv.org/abs/2010.03759",
      "https://arxiv.org/abs/1612.01474",
      "https://arxiv.org/abs/2102.11582",
      "https://arxiv.org/abs/2102.11582"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{mukhoti2021deterministic,\n  title={Deterministic Neural Networks with Appropriate Inductive Biases Capture Epistemic and Aleatoric Uncertainty},\n  author={Mukhoti, Jishnu and Kirsch, Andreas and van Amersfoort, Joost and Torr, Philip HS and Gal, Yarin},\n  journal={arXiv preprint arXiv:2102.11582},\n  year={2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9474209111726714
      ],
      "excerpt": "If the code or the paper has been useful in your research, please add a citation to our work: \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/omegafragger/DDU",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-05-20T12:05:40Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-03T04:25:20Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9813509384027278
      ],
      "excerpt": "This repository contains the code for Deterministic Neural Networks with Appropriate Inductive Biases Capture Epistemic and Aleatoric Uncertainty. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9394449182630016
      ],
      "excerpt": "--seed: seed for initialization \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8861472275184861,
        0.8789634547645613
      ],
      "excerpt": "--model: model to train (wide_resnet/vgg16/resnet18/resnet50/lenet) \n-sn: whether to use spectral normalization (available for wide_resnet, vgg16 and resnets) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "       --model wide_resnet \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "       --model resnet18 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "\u2502\u00a0\u00a0 \u2514\u2500\u2500 wide_resnet_1_350.model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "\u2502\u00a0\u00a0 \u2514\u2500\u2500 wide_resnet_2_350.model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "\u2502\u00a0\u00a0 \u2514\u2500\u2500 wide_resnet_3_350.model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "\u2502\u00a0\u00a0 \u2514\u2500\u2500 wide_resnet_4_350.model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "    \u2514\u2500\u2500 wide_resnet_5_350.model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877,
        0.860059181823877,
        0.860059181823877,
        0.860059181823877,
        0.860059181823877
      ],
      "excerpt": "\u2502\u00a0\u00a0 \u251c\u2500\u2500 wide_resnet_1_350.model \n\u2502\u00a0\u00a0 \u251c\u2500\u2500 wide_resnet_2_350.model \n\u2502\u00a0\u00a0 \u251c\u2500\u2500 wide_resnet_3_350.model \n\u2502\u00a0\u00a0 \u251c\u2500\u2500 wide_resnet_4_350.model \n\u2502\u00a0\u00a0 \u2514\u2500\u2500 wide_resnet_5_350.model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877,
        0.860059181823877,
        0.860059181823877,
        0.860059181823877,
        0.860059181823877
      ],
      "excerpt": "\u2502\u00a0\u00a0 \u251c\u2500\u2500 wide_resnet_10_350.model \n\u2502\u00a0\u00a0 \u251c\u2500\u2500 wide_resnet_6_350.model \n\u2502\u00a0\u00a0 \u251c\u2500\u2500 wide_resnet_7_350.model \n\u2502\u00a0\u00a0 \u251c\u2500\u2500 wide_resnet_8_350.model \n\u2502\u00a0\u00a0 \u2514\u2500\u2500 wide_resnet_9_350.model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877,
        0.860059181823877,
        0.860059181823877,
        0.860059181823877,
        0.860059181823877
      ],
      "excerpt": "\u2502\u00a0\u00a0 \u251c\u2500\u2500 wide_resnet_11_350.model \n\u2502\u00a0\u00a0 \u251c\u2500\u2500 wide_resnet_12_350.model \n\u2502\u00a0\u00a0 \u251c\u2500\u2500 wide_resnet_13_350.model \n\u2502\u00a0\u00a0 \u251c\u2500\u2500 wide_resnet_14_350.model \n\u2502\u00a0\u00a0 \u2514\u2500\u2500 wide_resnet_15_350.model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877,
        0.860059181823877,
        0.860059181823877,
        0.860059181823877,
        0.860059181823877
      ],
      "excerpt": "\u2502\u00a0\u00a0 \u251c\u2500\u2500 wide_resnet_16_350.model \n\u2502\u00a0\u00a0 \u251c\u2500\u2500 wide_resnet_17_350.model \n\u2502\u00a0\u00a0 \u251c\u2500\u2500 wide_resnet_18_350.model \n\u2502\u00a0\u00a0 \u251c\u2500\u2500 wide_resnet_19_350.model \n\u2502\u00a0\u00a0 \u2514\u2500\u2500 wide_resnet_20_350.model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877,
        0.860059181823877,
        0.860059181823877,
        0.860059181823877,
        0.860059181823877
      ],
      "excerpt": "    \u251c\u2500\u2500 wide_resnet_21_350.model \n    \u251c\u2500\u2500 wide_resnet_22_350.model \n    \u251c\u2500\u2500 wide_resnet_23_350.model \n    \u251c\u2500\u2500 wide_resnet_24_350.model \n    \u2514\u2500\u2500 wide_resnet_25_350.model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8846922562801878
      ],
      "excerpt": "--model: model architecture to load (wide_resnet/vgg16) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8907822233659366,
        0.8816290315189673
      ],
      "excerpt": "--ensemble: number of models in the ensemble \n--model-type: type of model to load for evaluation (softmax/ensemble/gmm) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "       --model wide_resnet \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9420537160873325
      ],
      "excerpt": "Similarly, to evaluate the above model using feature density, set --model-type gmm. The evaluation script assumes that the seeds of models trained in consecutive runs differ by 1. The script stores the results in a json file with the following structure:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8447514338274712,
        0.8447514338274712
      ],
      "excerpt": "        \"m2_auroc\": mean AUROC using entropy / PE for ensembles, \n        \"m2_auprc\": mean AUPRC using entropy / PE for ensembles, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8449394653472176,
        0.8449394653472176
      ],
      "excerpt": "        \"t_m2_auroc\": mean AUROC using entropy / PE for ensembles (post temp scaling), \n        \"t_m2_auprc\": mean AUPRC using entropy / PE for ensembles (post temp scaling) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8002750431393366,
        0.8002750431393366
      ],
      "excerpt": "        \"t_m2_auroc\": std error AUROC using entropy / PE for ensembles (post temp scaling), \n        \"t_m2_auprc\": std error AUPRC using entropy / PE for ensembles (post temp scaling) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8537903955762541
      ],
      "excerpt": "        \"m2_auroc\": AUROC list using entropy / PE for ensembles, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8496555503878951,
        0.8496555503878951
      ],
      "excerpt": "        \"t_m2_auroc\": AUROC list using entropy / PE for ensembles (post temp scaling), \n        \"t_m2_auprc\": AUPRC list using entropy / PE for ensembles (post temp scaling) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9968029537584643
      ],
      "excerpt": "    \"info\": {dictionary of args} \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9892440705861549
      ],
      "excerpt": "To visualise DDU's performance on Dirty-MNIST (i.e., Fig. 1 of the paper), use fig_1_plot.ipynb. The notebook requires a pretrained LeNet, VGG-16 and ResNet-18 with spectral normalization trained on Dirty-MNIST and visualises the softmax entropy and feature density for Dirty-MNIST (iD) samples vs Fashion-MNIST (OoD) samples. The notebook also visualises the softmax entropies of MNIST vs Ambiguous-MNIST samples for the ResNet-18+SN model (Fig. 2 of the paper). The following figure shows the output of the notebook for the LeNet, VGG-16 and ResNet18+SN model we trained on Dirty-MNIST. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9071019836076069
      ],
      "excerpt": "The following table presents results for a Wide-ResNet-28-10 architecture trained on CIFAR-10 with SVHN as the OoD dataset. For the full set of results, refer to the paper. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9124398045924318
      ],
      "excerpt": "--trained-model: model architecture of pretrained model to distinguish clean and ambiguous MNIST samples \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9496696209374089
      ],
      "excerpt": "--tcoeff: coefficient of spectral normalization used on pretrained model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8136268068700454
      ],
      "excerpt": "--al-type: type of active learning acquisition model (softmax/ensemble/gmm) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "       --model resnet18 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8701980730066341
      ],
      "excerpt": "Similarly, to run the active learning experiment on Dirty-MNIST using the DDU baseline, with a pretrained ResNet-18 with SN to distinguish clean and ambiguous MNIST samples, use the following: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "       --model resnet18 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9392910280211558
      ],
      "excerpt": "When using ambiguous samples in the pool set, the script also stores the fraction of ambiguous samples acquired in each step in the following json: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9207920348660362
      ],
      "excerpt": "To visualise results from the above json files, use the al_plot.ipynb notebook. The following diagram shows the performance of different baselines (softmax, ensemble PE, ensemble MI and DDU) on MNIST and Dirty-MNIST. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Code for Deterministic Neural Networks with Appropriate Inductive Biases Capture Epistemic and Aleatoric Uncertainty",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/omegafragger/DDU/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 5,
      "date": "Thu, 09 Dec 2021 21:34:25 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/omegafragger/DDU/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "omegafragger/DDU",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/omegafragger/DDU/main/notebooks/two_moons.ipynb",
      "https://raw.githubusercontent.com/omegafragger/DDU/main/notebooks/fig_1_plot.ipynb",
      "https://raw.githubusercontent.com/omegafragger/DDU/main/notebooks/al_plot.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8281072865442075
      ],
      "excerpt": "For OoD detection, you can train on CIFAR-10/100. You can also train on Dirty-MNIST by downloading Ambiguous-MNIST (amnist_labels.pt and amnist_samples.pt) from here and using the following training instructions. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8589951010888837
      ],
      "excerpt": "In order to train a model for the OoD detection task, use the train.py script. Following are the main parameters for training: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8245841289091004
      ],
      "excerpt": "--dataset: dataset used for training (cifar10/cifar100/dirty_mnist) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9503189345333785
      ],
      "excerpt": "python train.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9503189345333785
      ],
      "excerpt": "python train.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8299221304057071
      ],
      "excerpt": "To evaluate trained models, use evaluate.py. This script can evaluate and aggregate results over multiple experimental runs. For example, if the pretrained models are stored in a directory path /home/user/models, store them using the following directory structure: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.819287248311883,
        0.8245841289091004
      ],
      "excerpt": "--seed: seed used for initializing the first trained model \n--dataset: dataset used for training (cifar10/cifar100) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8020395991939573
      ],
      "excerpt": "--load-path: /path/to/pretrained/models/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8708148446064828
      ],
      "excerpt": "python evaluate.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8020395991939573
      ],
      "excerpt": "       --load-path /path/to/pretrained/models/ \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8226763253166589
      ],
      "excerpt": "       --model-type softmax \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8890818307099057
      ],
      "excerpt": "  <img src=\"vis/dirty_mnist_vis.png\" width=\"500\" /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8580853791002399
      ],
      "excerpt": "To run active learning experiments, use active_learning_script.py. You can run active learning experiments on both MNIST as well as Dirty-MNIST. When running with Dirty-MNIST, you will need to provide a pretrained model on Dirty-MNIST to distinguish between clean MNIST and Ambiguous-MNIST samples. The following are the main command line arguments for active_learning_script.py. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8485774454627109
      ],
      "excerpt": "-ambiguous: whether to use ambiguous MNIST during training. If this is set to True, the models will be trained on Dirty-MNIST, otherwise they will train on MNIST. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.813029018715359
      ],
      "excerpt": "--trained-model: model architecture of pretrained model to distinguish clean and ambiguous MNIST samples \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8499133111105738
      ],
      "excerpt": "--saved-model-name: name of the saved pretrained model file \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8528098075985725,
        0.8432457292949819
      ],
      "excerpt": "--num-initial-samples: number of initial samples in the training set \n--max-training-samples: maximum number of training samples \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python active_learning_script.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python active_learning_script.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8228997515786973
      ],
      "excerpt": "The active learning script stores all results in json files. The MNIST test set accuracy is stored in a json file with the following structure: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8023588515952818
      ],
      "excerpt": "    \"experiment run\": list of MNIST test set accuracies one per acquisition step \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8890818307099057
      ],
      "excerpt": "  <img src=\"vis/al_plots.png\" width=\"700\" /> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/omegafragger/DDU/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 Jishnu Mukhoti\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Deep Deterministic Uncertainty",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "DDU",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "omegafragger",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/omegafragger/DDU/blob/main/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The code is based on PyTorch and requires a few further dependencies, listed in [environment.yml](environment.yml). It should work with newer versions as well.\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 36,
      "date": "Thu, 09 Dec 2021 21:34:25 GMT"
    },
    "technique": "GitHub API"
  }
}