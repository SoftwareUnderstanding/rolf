{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1607.06450"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find the codes useful, please consider cite the following paper:\n> Biao Zhang; Rico Sennrich (2019). Root Mean Square Layer Normalization. In Advances in Neural Information Processing Systems 32. Vancouver, Canada.\n```\n@inproceedings{zhang-sennrich-neurips19,\n    address = \"Vancouver, Canada\",\n    author = \"Zhang, Biao and Sennrich, Rico\",\n    booktitle = \"Advances in Neural Information Processing Systems 32\",\n    url = \"https://openreview.net/references/pdf?id=S1qBAf6rr\",\n    title = \"{Root Mean Square Layer Normalization}\",\n    year = \"2019\"\n}\n```\n\nPlease feel free to contact [me](mailto:B.Zhang@ed.ac.uk) for any questions about our paper.",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{zhang-sennrich-neurips19,\n    address = \"Vancouver, Canada\",\n    author = \"Zhang, Biao and Sennrich, Rico\",\n    booktitle = \"Advances in Neural Information Processing Systems 32\",\n    url = \"https://openreview.net/references/pdf?id=S1qBAf6rr\",\n    title = \"{Root Mean Square Layer Normalization}\",\n    year = \"2019\"\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8090016440670298
      ],
      "excerpt": "    class LayerNormLayer(object): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8665716475375693
      ],
      "excerpt": "if x.ndim == 3: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8043073075947367
      ],
      "excerpt": "    if len(self.input_shape)==4: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8043073075947367
      ],
      "excerpt": "    if hasattr(self, 'g'): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8043073075947367
      ],
      "excerpt": "    if hasattr(self, 'b'): \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/bzhangGo/rmsnorm",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-09-24T10:03:55Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-18T13:48:14Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9106005260951305
      ],
      "excerpt": "RMSNorm is a simplification of the original layer normalization  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.888655227513245
      ],
      "excerpt": "LayerNorm is a regularization technique that might handle the internal covariate shift issue so as to  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9561352740038945,
        0.974326689784207
      ],
      "excerpt": "LayerNorm has become an essential component to enable model optimization, such as in the SOTA NMT model Transformer. \nOne application of LayerNorm is on recurrent neural networks. Nonetheless, we observe that  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9080842191682177
      ],
      "excerpt": "which diminishes the net efficiency gain from faster and more stable training, as shown in the Figure below. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8845523189657335
      ],
      "excerpt": "  model without any normalization. When the Baseline training loss arrives at 7.0, the loss of LayerNorm  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9197437965853065,
        0.990263408416073,
        0.8547945590132129,
        0.9039986135828114
      ],
      "excerpt": "<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\begin{align}&space;\\begin{split}&space;&&space;\\bar{a}_i&space;=&space;\\frac{a_i}{\\text{RMS}(\\mathbf{a})}&space;g_i,&space;\\quad&space;\\text{where}~~&space;\\text{RMS}(\\mathbf{a})&space;=&space;\\sqrt{\\frac{1}{n}&space;\\sum_{i=1}^{n}&space;a_i^2}.&space;\\end{split}\\nonumber&space;\\end{align}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\begin{align}&space;\\begin{split}&space;&&space;\\bar{a}_i&space;=&space;\\frac{a_i}{\\text{RMS}(\\mathbf{a})}&space;g_i,&space;\\quad&space;\\text{where}~~&space;\\text{RMS}(\\mathbf{a})&space;=&space;\\sqrt{\\frac{1}{n}&space;\\sum_{i=1}^{n}&space;a_i^2}.&space;\\end{split}\\nonumber&space;\\end{align}\" title=\"\\begin{align} \\begin{split} & \\bar{a}_i = \\frac{a_i}{\\text{RMS}(\\mathbf{a})} g_i, \\quad \\text{where}~~ \\text{RMS}(\\mathbf{a}) = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} a_i^2}. \\end{split}\\nonumber \\end{align}\" /></a> \nWhen the mean of the inputs is exactly 0, then LayerNorm equals to RMSNorm. We also observe that the RMS statistic  \ncan be estimated from partial inputs, based on the iid assumption. Below shows the comparision of LayerNorm  \n and RMSNorm in different properties. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8671685280115903
      ],
      "excerpt": "As RMSNorm does not consider the mean of the inputs, it's not re-centering invariant. This is the main difference \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9174169495824087,
        0.9641960913273474
      ],
      "excerpt": "improve the robustness of LayerNorm? We did an experiment on RNNSearch with Nematus, \nwhere we initialize the weights with a center of about 0.2. The figure below suggests that removing re-centering operation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9813582681935004,
        0.9891785691729262
      ],
      "excerpt": "  <em>SacreBLEU score curve of LayerNorm and RMSNorm on newstest2013 (devset) when the initialization center  \n  is 0.2.</em> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9337617119361109
      ],
      "excerpt": "We did experiments on four different tasks, including different neural models (RNN/CNN/Transformer),  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9394112745197206,
        0.9524119186512484
      ],
      "excerpt": "and different deep learning frameworks (Theano/Pytorch/Tensorflow). Our experiments involve NLP-related and Image-related tasks. \nMost of the settings follows those in LayerNorm paper. But from our view, we put more focus on machine translation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8215719205400105
      ],
      "excerpt": "- clone the github repository and checkout the specific version.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.859060902393745
      ],
      "excerpt": "To ease the training of RNNSearch, we also provide the used/preprocessed dataset & training script & pretrained model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8015058893433946,
        0.9863261444612311,
        0.8166494473037054,
        0.8254584705992885
      ],
      "excerpt": "``b` here is deletable. The training follows a similar way as above. \nThe RMSNorm for Transformer is implemented in zero. \nWe experiment with the bidirectional attentive reader model proposed by Hermann et al.  \nWe use the attentive reader model from the repository given by Tim Coojimans et al.. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9095989852825405
      ],
      "excerpt": "Clone the above repository and obtain the data: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9001248609052479
      ],
      "excerpt": "Follow the instructions for training a new model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8967028536517441
      ],
      "excerpt": "We experiment with order-embedding model proposed by Vendro et al. The code used is available here.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.802436023219978
      ],
      "excerpt": "Add the rlngru_layer and param_init_rlngru functions to layers.py in the order-embeddings repo \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9284900040193205
      ],
      "excerpt": "Available below is a download to the model used to report results in the paper: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.928000882212665,
        0.9474330226828717
      ],
      "excerpt": "please change the prefix for rlngru model to lngru to use the saved models. \nWe experiment with the ConvPool-CNN-C architecture proposed by Krizhevsky and Hinton, and follow the settings in WeightNorm. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8409952393201047
      ],
      "excerpt": "Running los of our model can be downloaded as below: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Root Mean Square Layer Normalization",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/bzhangGo/rmsnorm/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Wed, 08 Dec 2021 23:00:59 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/bzhangGo/rmsnorm/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "bzhangGo/rmsnorm",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8468034016054617,
        0.9893272198983933,
        0.9906248903846466,
        0.8579005295786547
      ],
      "excerpt": "- clone the github repository and checkout the specific version.  \n    git clone https://github.com/EdinburghNLP/nematus.git \n    cd nematus \n    git checkout tags/v0.3 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8953947298193995
      ],
      "excerpt": "About the Theano-experiments: You can download the Theano-version Nematus \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8656085564333076
      ],
      "excerpt": "We train the model using the following command \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8774771813377704
      ],
      "excerpt": "Clone the above repository \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9012535140424993
      ],
      "excerpt": "Once downloaded, follow the instructions on the main page for evaluating models. Notice that \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8774771813377704
      ],
      "excerpt": "Clone the above repository \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.829186355849132
      ],
      "excerpt": "We use the following command to train the model: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8818747549410019
      ],
      "excerpt": "  <img src=\"./rnn_layernorm.svg\" height=\"300\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9085205418536756
      ],
      "excerpt": "  <img src=\"./ininmt.svg\" height=\"300\" width=\"100%\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123763140827432
      ],
      "excerpt": "                                           initializer=tf.constant_initializer(1)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8234607265914139
      ],
      "excerpt": "        #: m, v = tf.nn.moments(x, axes=[-1], keep_dims=True) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8812031492970367
      ],
      "excerpt": "        ms = tf.reduce_sum(tf.square(x), axis=-1, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8227809396803554
      ],
      "excerpt": "        norm_inputs = x * tf.rsqrt(ms + self.eps) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8777635561683621,
        0.8289669050403863
      ],
      "excerpt": "        norm_x = tensor.mean(x * x, axis=-1, keepdims=True) \n        output = x / tensor.sqrt(norm_x + _eps) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.818550718661506,
        0.8113404775112109
      ],
      "excerpt": "    output = s[None, :] * output + b[None,:] \nreturn output \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8553516679151805,
        0.8070508988721181
      ],
      "excerpt": "Add the RMSNorm function to layers.py in codes/att_reader/layers.py \nAdd the rlnlstm_layer and param_init_rlnlstm functions to layers.py in codes/att_reader/layers.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9375074424019093,
        0.8072826656280974
      ],
      "excerpt": "GPUARRAY_FORCE_CUDA_DRIVER_LOAD=True THEANO_FLAGS=mode=FAST_RUN,floatX=float32,device=$device,gpuarray.preallocate=0.8 python -u train_attentive_reader.py \\ \n    --use_dq_sims 1 --use_desc_skip_c_g 0 --dim 240 --learn_h0 1 --lr 8e-5 --truncate -1 --model \"lstm_s1.npz\" --batch_size 64 --optimizer \"adam\" --validFreq 1000 --model_dir $MDIR --use_desc_skip_c_g 1  --unit_type rlnlstm --use_bidir 1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8026420641312261
      ],
      "excerpt": "Add the RMSNorm function to layers.py in the order-embeddings repo \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8586665136296852
      ],
      "excerpt": "Add RMSNorm function to nn.py as follows: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8451115096790174
      ],
      "excerpt": "    meanS = T.mean(input ** 2,axis=self.axes_to_sum,keepdims=True) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8198087311497705
      ],
      "excerpt": "        del layer.params[layer.b] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8947828450917619
      ],
      "excerpt": "* Add the option into `train.py`: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8569382081398033
      ],
      "excerpt": "* Download the ciFar dataset and set the train/test_data direction intrain.py` to your ciFar directory \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9580690189614289
      ],
      "excerpt": "GPUARRAY_FORCE_CUDA_DRIVER_LOAD=True THEANO_FLAGS=mode=FAST_RUN,floatX=float32,device=$device,gpuarray.preallocate=0.4 python train.py --norm_type rms_norm --learning_rate 0.003 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/bzhangGo/rmsnorm/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "BSD 3-Clause \"New\" or \"Revised\" License",
      "url": "https://api.github.com/licenses/bsd-3-clause"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'BSD 3-Clause License\\n\\nCopyright (c) 2019, Biao Zhang\\nAll rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met:\\n\\n1. Redistributions of source code must retain the above copyright notice, this\\n   list of conditions and the following disclaimer.\\n\\n2. Redistributions in binary form must reproduce the above copyright notice,\\n   this list of conditions and the following disclaimer in the documentation\\n   and/or other materials provided with the distribution.\\n\\n3. Neither the name of the copyright holder nor the names of its\\n   contributors may be used to endorse or promote products derived from\\n   this software without specific prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "RMSNorm",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "rmsnorm",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "bzhangGo",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/bzhangGo/rmsnorm/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The codes rely on the following packages:\n* Python2.7\n* Numpy\n* Tensorflow\n* Theano\n* Scipy\n* lasagne\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 40,
      "date": "Wed, 08 Dec 2021 23:00:59 GMT"
    },
    "technique": "GitHub API"
  }
}