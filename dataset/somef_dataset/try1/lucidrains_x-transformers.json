{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1907.01470\n\nProposes adding learned memory key / values prior to attention. They were able to remove feedforwards altogether and attain similar performance to the original transformers. I have found that keeping the feedforwards and adding the memory key / values leads to even better performance.\n\n```python\nfrom x_transformers import Decoder, Encoder\n\nenc = Encoder(\n    dim = 512,\n    depth = 6,\n    heads = 8,\n    attn_num_mem_kv = 16 # 16 memory key / values\n",
      "https://arxiv.org/abs/2006.11527\n\nProposes adding learned tokens, akin to CLS tokens, named memory tokens, that is passed through the attention layers alongside the input tokens.\n\n```python\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder, Encoder\n\nmodel = TransformerWrapper(\n    num_tokens = 20000,\n    max_seq_len = 1024,\n    num_memory_tokens = 20, # 20 memory tokens\n    attn_layers = Encoder(\n        dim = 512,\n        depth = 6,\n        heads = 8\n    ",
      "https://arxiv.org/abs/1910.05895\n\nThey experiment with alternatives to Layer normalization and found one that is both effective and simpler. Researchers have shared with me this leads to faster convergence.\n\n```python\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder, Encoder\n\nmodel = TransformerWrapper(\n    num_tokens = 20000,\n    max_seq_len = 1024,\n    attn_layers = Decoder(\n        dim = 512,\n        depth = 6,\n        heads = 8,\n        use_scalenorm = True # set to true to use for all layers\n    ",
      "https://arxiv.org/abs/2002.05202\n\nNoam Shazeer paper that explores gating in the feedforward, finding that simple gating with GELU leads to significant improvements. This variant also showed up in the latest mT5 architecture. You should always turn this on (I may eventually turn it on by default",
      "https://arxiv.org/abs/2109.08668\n\nThis paper used neural architecture search and found an activation, Relu Squared, that is both simpler and performs better than GELU, in the autoregressive language model setting. I have confirmed this in my independent experiments. However, if one were using the GLU variant from above, GELU still performs better. Pending further corroboration.\n\n```python\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder, Encoder\n\nmodel = TransformerWrapper(\n    num_tokens = 20000,\n    max_seq_len = 1024,\n    attn_layers = Decoder(\n        dim = 512,\n        depth = 6,\n        heads = 8,\n        ff_relu_squared = True\n    ",
      "https://arxiv.org/abs/2003.04887\n\nThis paper proposes to do away with normalization altogether, and instead gate the output of each branch with a single learned scalar, initialized at zero. They demonstrate convergence for very deep networks, convolution or attention, all without normalization.\n\nI have had good results on usual datasets, but had met trouble with convergence on large datasets (GPT3 sized datasets",
      "https://arxiv.org/abs/1912.11637\n\nThis paper proposes an efficient way to sparsify attention by zeroing all dot-product query/key values not within the top k values. The show that this cheap method was as effective as other more expensive operations like sparsemax or entmax15. This technique comes with the cost of an extra hyperparameter (the top k values to keep",
      "https://arxiv.org/abs/2003.02436\n\nA Noam Shazeer paper that proposes mixing information between heads pre and post attention (softmax",
      "https://arxiv.org/abs/2006.16362\n\nShare redundent learned key/query projections accross heads. Collaborative attention reduces the number of parameters but requires slightly more memory and computation. A good compression factor to match the performance of the vanilla multi-head attention is between 0.25 and 0.5.\n\n```python\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel = TransformerWrapper(\n    num_tokens = 20000,\n    max_seq_len = 1024,\n    attn_layers = Decoder(\n        dim = 512,\n        depth = 6,\n        heads = 8,\n        attn_collab_heads = True,\n        attn_collab_compression = .3,\n    ",
      "https://arxiv.org/abs/1908.06954\n\nThis paper proposes to add a gated linear unit at the end of the attention layer, further gated by the original queries. Although this is not widely used outside of visual question / answering, I suspect it should lead to improvements after seeing the success of the feedforward GLU variant.\n\nUpdate: After some experimentation, I found this variant actually performs worse, but if it were to be modified to not concatenate the queries before gating, it performs much better. That is what we will be using in this repository.\n\n```python\nimport torch\nfrom x_transformers import TransformerWrapper, Encoder\n\nmodel = TransformerWrapper(\n    num_tokens = 20000,\n    max_seq_len = 1024,\n    attn_layers = Encoder(\n        dim = 512,\n        depth = 6,\n        heads = 8,\n        attn_on_attn = True  # gate output of attention layer, by queries\n    ",
      "https://arxiv.org/abs/1911.03864\n\nThis paper proposes to break from the normal fixed pattern of alternating attention and feedforwards, but to have blocks of only attention at the beginning followed by blocks of feedforwards at the end. This was further corroborated by a paper by Nvidia that reduces the number of attention layers to be 1/3rd of the feedforwards without loss in performance.\n\nThe amount of interleaving is controlled by a \"sandwich coefficient\", which they found to be optimal at a value of `6`.\n\nYou can experiment with this feature as shown below\n\n```python\nimport torch\nfrom x_transformers import TransformerWrapper, Encoder\n\nmodel = TransformerWrapper(\n    num_tokens = 20000,\n    max_seq_len = 1024,\n    attn_layers = Encoder(\n        dim = 512,\n        depth = 6,\n        heads = 8,\n        sandwich_coef = 6  # interleave attention and feedforwards with sandwich coefficient of 6\n    ",
      "https://arxiv.org/abs/1906.02762\n\nThe authors propose to view the success of transformers from a dynamical systems point of view, and then proposes an improvement based on mathematics of that POV. Specifically, they propose to place the attention layer in between two feedforward layers. This was adopted by a paper using transformers for speech recognition, the <a href=\"https://arxiv.org/abs/2005.08100\">Conformer</a>.\n\n```python\nimport torch\nfrom x_transformers import TransformerWrapper, Encoder\n\nmodel = TransformerWrapper(\n    num_tokens = 20000,\n    max_seq_len = 1024,\n    attn_layers = Encoder(\n        dim = 512,\n        depth = 6,\n        heads = 8,\n        macaron = True  # use macaron configuration\n    ",
      "https://arxiv.org/abs/2005.08100\">Conformer</a>.\n\n```python\nimport torch\nfrom x_transformers import TransformerWrapper, Encoder\n\nmodel = TransformerWrapper(\n    num_tokens = 20000,\n    max_seq_len = 1024,\n    attn_layers = Encoder(\n        dim = 512,\n        depth = 6,\n        heads = 8,\n        macaron = True  # use macaron configuration\n    ",
      "https://arxiv.org/abs/1910.10683\n\nT5 is one of the most successful encoder / decoder transformer architectures trained to date. They invented a new simplified relative positional encoding based on learned bias values that are added to the attention matrix pre-softmax. This bias is shared and injected into each attention layer. I have decided to include this because it offers a cheap way to have relative positional encoding (superior to absolute positional",
      "https://arxiv.org/abs/2005.12872\n\nhttps://ofir.io/shortformer.pdf\n\nIn these two papers, the authors independently figured out a new technique where fixed sinusoidal positional embeddings are injected into the input prior to the queries and keys projection for all layers, leading to \"position infused\" attention, but leaving the actual tokens (values",
      "https://arxiv.org/abs/2012.11747\n\nThis paper from Google proposes residualizing the pre-attention scores across all layers. At the cost of no extra parameters, they show improvement on top of regular attention networks. If you turn on this setting, be aware that the best results in the paper used post-normalization, in which case a learning warmup will be needed. The authors also reported that they could use a higher learning rate and get even better gains in the same amount of steps. (In the paper they use `2e-4` vs `1e-4` for vanilla transformer",
      "https://arxiv.org/abs/2012.15688\">This paper</a> proposes a simple technique to enhance the range of Transformer-XL. They simply route the memory segment of a layer to the layer below it, for the next recurrent step. You can enable this by setting `shift_mem_down = 1`. You can also shift down arbitrary number of layers by setting this value to `> 1`.\n\n```python\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel_xl = TransformerWrapper(\n    num_tokens = 20000,\n    max_seq_len = 512,\n    max_mem_len = 2048,\n    shift_mem_down = 1,\n    attn_layers = Decoder(\n        dim = 512,\n        depth = 6,\n        heads = 8,\n        rotary_pos_emb = True\n    ",
      "https://arxiv.org/abs/1910.06764\n\nThe authors propose gating the residual connections in the transformer network and demonstrate increased stability and performance for Transformer-XL in a variety of reinforcement learning tasks.\n\n```python\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel = TransformerWrapper(\n    num_tokens = 20000,\n    max_seq_len = 1024,\n    max_mem_len = 2048,\n    attn_layers = Decoder(\n        dim = 512,\n        depth = 6,\n        heads = 16,\n        gate_residual = True\n    ",
      "https://arxiv.org/abs/2106.07477\">the results</a> of some papers in the vision domain.\n\nTo use it, simply set `shift_tokens = 1` (or to whatever number of shifts you desire",
      "https://arxiv.org/abs/2105.13290\">the CoqView paper</a>, a Chinese version of the famous text-to-image transformer DALL-E. They propose, when using pre-layernorm, to add an extra layernorm to all the branch outputs. I have found this to be very effective for a number of projects, when facing instability during training.\n\n```python\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel = TransformerWrapper(\n    num_tokens = 20000,\n    max_seq_len = 1024,\n    attn_layers = Decoder(\n        dim = 512,\n        depth = 6,\n        heads = 8,\n        sandwich_norm = True # set this to True\n    ",
      "https://arxiv.org/abs/2010.04245\">paper</a> proposes to l2 normalize the queries and keys along the head dimension before the dot product (cosine similarity",
      "https://arxiv.org/abs/2111.09883\">a 3B parameter vision transformer</a>. The SwinV2 paper also proposes to change the pre-layernorm to a post-layernorm for further stability.\n\nI have validated that this works just as well as dot product attention in an autoregressive setting, if one were to initialize the temperature as proposed in the QK-norm paper (as a function of the sequence length",
      "https://arxiv.org/abs/2111.05498\">a connection</a> to sparse distributed memory. <a href=\"https://www.youtube.com/watch?v=THIIk7LR9_8\">[youtube talk]</a>\n\nYou can use it as follows\n\n```python\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel = TransformerWrapper(\n    num_tokens = 20000,\n    max_seq_len = 1024,\n    attn_layers = Decoder(\n        dim = 512,\n        depth = 6,\n        heads = 8,\n        use_qk_norm_attn = True, # set this to True\n        qk_norm_attn_seq_len = 1024 # set this to max_seq_len from above\n    ",
      "https://arxiv.org/abs/2002.05202}    \n}\n```\n\n```bibtex\n@misc{bachlechner2020rezero,\n    title   = {ReZero is All You Need: Fast Convergence at Large Depth},\n    author  = {Thomas Bachlechner and Bodhisattwa Prasad Majumder and Huanru Henry Mao and Garrison W. Cottrell and Julian McAuley},\n    year    = {2020},\n    url     = {https://arxiv.org/abs/2003.04887}\n}\n```\n\n```bibtex\n@misc{bhojanapalli2020lowrank,\n    title   = {Low-Rank Bottleneck in Multi-head Attention Models},\n    author  = {Srinadh Bhojanapalli and Chulhee Yun and Ankit Singh Rawat and Sashank J. Reddi and Sanjiv Kumar},\n    year    = {2020},\n    eprint  = {2002.07028}\n}\n```\n\n```bibtex\n@misc{burtsev2020memory,\n    title   = {Memory Transformer}, \n    author  = {Mikhail S. Burtsev and Grigory V. Sapunov},\n    year    = {2020},\n    eprint  = {2006.11527},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}\n```\n\n```bibtex\n@misc{zhao2019explicit,\n    title   = {Explicit Sparse Transformer: Concentrated Attention Through Explicit Selection}, \n    author  = {Guangxiang Zhao and Junyang Lin and Zhiyuan Zhang and Xuancheng Ren and Qi Su and Xu Sun},\n    year    = {2019},\n    eprint  = {1912.11637},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}\n```\n\n```bibtex\n@misc{correia2019adaptively,\n    title   = {Adaptively Sparse Transformers},\n    author  = {Gon\u00e7alo M. Correia and Vlad Niculae and Andr\u00e9 F. T. Martins},\n    year    = {2019},\n    eprint  = {1909.00015},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}\n```\n\n```bibtex\n@misc{shazeer2020talkingheads,\n    title   = {Talking-Heads Attention}, \n    author  = {Noam Shazeer and Zhenzhong Lan and Youlong Cheng and Nan Ding and Le Hou},\n    year    = {2020},\n    eprint  = {2003.02436},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG}\n}\n```\n\n```bibtex\n@misc{cordonnier2020multihead,\n    title   = {Multi-Head Attention: Collaborate Instead of Concatenate},\n    author  = {Jean-Baptiste Cordonnier and Andreas Loukas and Martin Jaggi},\n    year    = {2020},\n    eprint  = {2006.16362},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG}\n}\n```\n\n```bibtex\n@misc{press2020improving,\n    title   = {Improving Transformer Models by Reordering their Sublayers}, \n    author  = {Ofir Press and Noah A. Smith and Omer Levy},\n    year    = {2020},\n    eprint  = {1911.03864},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}\n```\n\n```bibtex\n@misc{lu2019understanding,\n    title   = {Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View}, \n    author  = {Yiping Lu and Zhuohan Li and Di He and Zhiqing Sun and Bin Dong and Tao Qin and Liwei Wang and Tie-Yan Liu},\n    year    = {2019},\n    eprint  = {1906.02762},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG}\n}\n```\n\n```bibtex\n@misc{ke2020rethinking,\n    title     = {Rethinking Positional Encoding in Language Pre-training},\n    author    = {Guolin Ke and Di He and Tie-Yan Liu},\n    year      = {2020},\n    eprint    = {2006.15595},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}\n```\n\n```bibtex\n@misc{dosovitskiy2020image,\n    title   = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},\n    author  = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},\n    year    = {2020},\n    eprint  = {2010.11929},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n```bibtex\n@misc{huang2019attention,\n    title   = {Attention on Attention for Image Captioning},\n    author  = {Lun Huang and Wenmin Wang and Jie Chen and Xiao-Yong Wei},\n    year    = {2019},\n    eprint  = {1908.06954},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n```bibtex\n@misc{raffel2020exploring,\n    title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}, \n    author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},\n    year    = {2020},\n    eprint  = {1910.10683},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG}\n}\n```\n\n```bibtex\n@inproceedings{martins-etal-2020-sparse,\n    title   = \"Sparse Text Generation\",\n    author  = \"Martins, Pedro Henrique  and\n        Marinho, Zita  and\n        Martins, Andr{\\'e} F. T.\",\n    booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP",
      "https://arxiv.org/abs/2003.04887}\n}\n```\n\n```bibtex\n@misc{bhojanapalli2020lowrank,\n    title   = {Low-Rank Bottleneck in Multi-head Attention Models},\n    author  = {Srinadh Bhojanapalli and Chulhee Yun and Ankit Singh Rawat and Sashank J. Reddi and Sanjiv Kumar},\n    year    = {2020},\n    eprint  = {2002.07028}\n}\n```\n\n```bibtex\n@misc{burtsev2020memory,\n    title   = {Memory Transformer}, \n    author  = {Mikhail S. Burtsev and Grigory V. Sapunov},\n    year    = {2020},\n    eprint  = {2006.11527},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}\n```\n\n```bibtex\n@misc{zhao2019explicit,\n    title   = {Explicit Sparse Transformer: Concentrated Attention Through Explicit Selection}, \n    author  = {Guangxiang Zhao and Junyang Lin and Zhiyuan Zhang and Xuancheng Ren and Qi Su and Xu Sun},\n    year    = {2019},\n    eprint  = {1912.11637},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}\n```\n\n```bibtex\n@misc{correia2019adaptively,\n    title   = {Adaptively Sparse Transformers},\n    author  = {Gon\u00e7alo M. Correia and Vlad Niculae and Andr\u00e9 F. T. Martins},\n    year    = {2019},\n    eprint  = {1909.00015},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}\n```\n\n```bibtex\n@misc{shazeer2020talkingheads,\n    title   = {Talking-Heads Attention}, \n    author  = {Noam Shazeer and Zhenzhong Lan and Youlong Cheng and Nan Ding and Le Hou},\n    year    = {2020},\n    eprint  = {2003.02436},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG}\n}\n```\n\n```bibtex\n@misc{cordonnier2020multihead,\n    title   = {Multi-Head Attention: Collaborate Instead of Concatenate},\n    author  = {Jean-Baptiste Cordonnier and Andreas Loukas and Martin Jaggi},\n    year    = {2020},\n    eprint  = {2006.16362},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG}\n}\n```\n\n```bibtex\n@misc{press2020improving,\n    title   = {Improving Transformer Models by Reordering their Sublayers}, \n    author  = {Ofir Press and Noah A. Smith and Omer Levy},\n    year    = {2020},\n    eprint  = {1911.03864},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}\n```\n\n```bibtex\n@misc{lu2019understanding,\n    title   = {Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View}, \n    author  = {Yiping Lu and Zhuohan Li and Di He and Zhiqing Sun and Bin Dong and Tao Qin and Liwei Wang and Tie-Yan Liu},\n    year    = {2019},\n    eprint  = {1906.02762},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG}\n}\n```\n\n```bibtex\n@misc{ke2020rethinking,\n    title     = {Rethinking Positional Encoding in Language Pre-training},\n    author    = {Guolin Ke and Di He and Tie-Yan Liu},\n    year      = {2020},\n    eprint    = {2006.15595},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}\n```\n\n```bibtex\n@misc{dosovitskiy2020image,\n    title   = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},\n    author  = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},\n    year    = {2020},\n    eprint  = {2010.11929},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n```bibtex\n@misc{huang2019attention,\n    title   = {Attention on Attention for Image Captioning},\n    author  = {Lun Huang and Wenmin Wang and Jie Chen and Xiao-Yong Wei},\n    year    = {2019},\n    eprint  = {1908.06954},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n```bibtex\n@misc{raffel2020exploring,\n    title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}, \n    author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},\n    year    = {2020},\n    eprint  = {1910.10683},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG}\n}\n```\n\n```bibtex\n@inproceedings{martins-etal-2020-sparse,\n    title   = \"Sparse Text Generation\",\n    author  = \"Martins, Pedro Henrique  and\n        Marinho, Zita  and\n        Martins, Andr{\\'e} F. T.\",\n    booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP",
      "https://arxiv.org/abs/1910.05895"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```bibtex\n@misc{vaswani2017attention,\n    title   = {Attention Is All You Need},\n    author  = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},\n    year    = {2017},\n    eprint  = {1706.03762},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}\n```\n\n```bibtex\n@article{DBLP:journals/corr/abs-1907-01470,\n    author    = {Sainbayar Sukhbaatar and\n               Edouard Grave and\n               Guillaume Lample and\n               Herv{\\'{e}} J{\\'{e}}gou and\n               Armand Joulin},\n    title     = {Augmenting Self-attention with Persistent Memory},\n    journal   = {CoRR},\n    volume    = {abs/1907.01470},\n    year      = {2019},\n    url       = {http://arxiv.org/abs/1907.01470}\n}\n```\n\n```bibtex\n@article{1910.05895,\n    author  = {Toan Q. Nguyen and Julian Salazar},\n    title   = {Transformers without Tears: Improving the Normalization of Self-Attention},\n    year    = {2019},\n    eprint  = {arXiv:1910.05895},\n    doi     = {10.5281/zenodo.3525484},\n}\n```\n\n```bibtex\n@misc{shazeer2020glu,\n    title   = {GLU Variants Improve Transformer},\n    author  = {Noam Shazeer},\n    year    = {2020},\n    url     = {https://arxiv.org/abs/2002.05202}    \n}\n```\n\n```bibtex\n@misc{bachlechner2020rezero,\n    title   = {ReZero is All You Need: Fast Convergence at Large Depth},\n    author  = {Thomas Bachlechner and Bodhisattwa Prasad Majumder and Huanru Henry Mao and Garrison W. Cottrell and Julian McAuley},\n    year    = {2020},\n    url     = {https://arxiv.org/abs/2003.04887}\n}\n```\n\n```bibtex\n@misc{bhojanapalli2020lowrank,\n    title   = {Low-Rank Bottleneck in Multi-head Attention Models},\n    author  = {Srinadh Bhojanapalli and Chulhee Yun and Ankit Singh Rawat and Sashank J. Reddi and Sanjiv Kumar},\n    year    = {2020},\n    eprint  = {2002.07028}\n}\n```\n\n```bibtex\n@misc{burtsev2020memory,\n    title   = {Memory Transformer}, \n    author  = {Mikhail S. Burtsev and Grigory V. Sapunov},\n    year    = {2020},\n    eprint  = {2006.11527},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}\n```\n\n```bibtex\n@misc{zhao2019explicit,\n    title   = {Explicit Sparse Transformer: Concentrated Attention Through Explicit Selection}, \n    author  = {Guangxiang Zhao and Junyang Lin and Zhiyuan Zhang and Xuancheng Ren and Qi Su and Xu Sun},\n    year    = {2019},\n    eprint  = {1912.11637},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}\n```\n\n```bibtex\n@misc{correia2019adaptively,\n    title   = {Adaptively Sparse Transformers},\n    author  = {Gon\u00e7alo M. Correia and Vlad Niculae and Andr\u00e9 F. T. Martins},\n    year    = {2019},\n    eprint  = {1909.00015},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}\n```\n\n```bibtex\n@misc{shazeer2020talkingheads,\n    title   = {Talking-Heads Attention}, \n    author  = {Noam Shazeer and Zhenzhong Lan and Youlong Cheng and Nan Ding and Le Hou},\n    year    = {2020},\n    eprint  = {2003.02436},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG}\n}\n```\n\n```bibtex\n@misc{cordonnier2020multihead,\n    title   = {Multi-Head Attention: Collaborate Instead of Concatenate},\n    author  = {Jean-Baptiste Cordonnier and Andreas Loukas and Martin Jaggi},\n    year    = {2020},\n    eprint  = {2006.16362},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG}\n}\n```\n\n```bibtex\n@misc{press2020improving,\n    title   = {Improving Transformer Models by Reordering their Sublayers}, \n    author  = {Ofir Press and Noah A. Smith and Omer Levy},\n    year    = {2020},\n    eprint  = {1911.03864},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}\n```\n\n```bibtex\n@misc{lu2019understanding,\n    title   = {Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View}, \n    author  = {Yiping Lu and Zhuohan Li and Di He and Zhiqing Sun and Bin Dong and Tao Qin and Liwei Wang and Tie-Yan Liu},\n    year    = {2019},\n    eprint  = {1906.02762},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG}\n}\n```\n\n```bibtex\n@misc{ke2020rethinking,\n    title     = {Rethinking Positional Encoding in Language Pre-training},\n    author    = {Guolin Ke and Di He and Tie-Yan Liu},\n    year      = {2020},\n    eprint    = {2006.15595},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}\n```\n\n```bibtex\n@misc{dosovitskiy2020image,\n    title   = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},\n    author  = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},\n    year    = {2020},\n    eprint  = {2010.11929},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n```bibtex\n@misc{huang2019attention,\n    title   = {Attention on Attention for Image Captioning},\n    author  = {Lun Huang and Wenmin Wang and Jie Chen and Xiao-Yong Wei},\n    year    = {2019},\n    eprint  = {1908.06954},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n```bibtex\n@misc{raffel2020exploring,\n    title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}, \n    author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},\n    year    = {2020},\n    eprint  = {1910.10683},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG}\n}\n```\n\n```bibtex\n@inproceedings{martins-etal-2020-sparse,\n    title   = \"Sparse Text Generation\",\n    author  = \"Martins, Pedro Henrique  and\n        Marinho, Zita  and\n        Martins, Andr{\\'e} F. T.\",\n    booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)\",\n    month   = nov,\n    year    = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url     = \"https://www.aclweb.org/anthology/2020.emnlp-main.348\"\n}\n```\n\n```bibtex\n@misc{he2020realformer,\n    title   = {RealFormer: Transformer Likes Residual Attention},\n    author  = {Ruining He and Anirudh Ravula and Bhargav Kanagal and Joshua Ainslie},\n    year    = {2020},\n    eprint  = {2012.11747},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG}\n}\n```\n\n```bibtex\n@misc{carion2020endtoend,\n    title   = {End-to-End Object Detection with Transformers},\n    author  = {Nicolas Carion and Francisco Massa and Gabriel Synnaeve and Nicolas Usunier and Alexander Kirillov and Sergey Zagoruyko},\n    year    = {2020},\n    eprint  = {2005.12872},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n```bibtex\n@misc{press2020shortformer,\n    title   = {Shortformer: Better Language Modeling using Shorter Inputs},\n    author  = {Ofir Press and Noah A. Smith and Mike Lewis},\n    year    = {2020}\n}\n```\n\n```bibtex\n@misc{press2021ALiBi,\n    title   = {Train Short, Test Long: Attention with Linear Biases Enable Input Length Extrapolation},\n    author  = {Ofir Press and Noah A. Smith and Mike Lewis},\n    year    = {2021},\n    url     = {https://ofir.io/train_short_test_long.pdf}\n}\n```\n\n```bibtex\n@misc{parisotto2019stabilizing,\n    title     = {Stabilizing Transformers for Reinforcement Learning},\n    author    = {Emilio Parisotto and H. Francis Song and Jack W. Rae and Razvan Pascanu and Caglar Gulcehre and Siddhant M. Jayakumar and Max Jaderberg and Raphael Lopez Kaufman and Aidan Clark and Seb Noury and Matthew M. Botvinick and Nicolas Heess and Raia Hadsell},\n    year      = {2019},\n    eprint    = {1910.06764},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG}\n}\n```\n\n```bibtex\n@misc{narang2021transformer,\n    title       = {Do Transformer Modifications Transfer Across Implementations and Applications?},\n    author      = {Sharan Narang and Hyung Won Chung and Yi Tay and William Fedus and Thibault Fevry and Michael Matena and Karishma Malkan and Noah Fiedel and Noam Shazeer and Zhenzhong Lan and Yanqi Zhou and Wei Li and Nan Ding and Jake Marcus and Adam Roberts and Colin Raffel},\n    year        = {2021},\n    eprint      = {2102.11972},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG}\n}\n```\n\n```bibtex\n@misc{zhang2019root,\n    title   = {Root Mean Square Layer Normalization},\n    author  = {Biao Zhang and Rico Sennrich},\n    year    = {2019},\n    eprint  = {1910.07467},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG}\n}\n```\n\n```bibtex\n@misc{su2021roformer,\n    title   = {RoFormer: Enhanced Transformer with Rotary Position Embedding},\n    author  = {Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu},\n    year    = {2021},\n    eprint  = {2104.09864},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}\n```\n\n```bibtex\n@Article{AlphaFold2021,\n    author  = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\\v{Z}}{\\'\\i}dek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A A and Ballard, Andrew J and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},\n    journal = {Nature},\n    title   = {Highly accurate protein structure prediction with {AlphaFold}},\n    year    = {2021},\n    doi     = {10.1038/s41586-021-03819-2},\n    note    = {(Accelerated article preview)},\n}\n```\n\n```bibtex\n@software{peng_bo_2021_5196578,\n    author       = {PENG Bo},\n    title        = {BlinkDL/RWKV-LM: 0.01},\n    month        = {aug},\n    year         = {2021},\n    publisher    = {Zenodo},\n    version      = {0.01},\n    doi          = {10.5281/zenodo.5196578},\n    url          = {https://doi.org/10.5281/zenodo.5196578}\n}\n```\n\n```bibtex\n@misc{csord\u00e1s2021devil,\n    title   = {The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers},\n    author  = {R\u00f3bert Csord\u00e1s and Kazuki Irie and J\u00fcrgen Schmidhuber},\n    year    = {2021},\n    eprint  = {2108.12284},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG}\n}\n```\n\n```bibtex\n@misc{so2021primer,\n    title   = {Primer: Searching for Efficient Transformers for Language Modeling}, \n    author  = {David R. So and Wojciech Ma\u0144ke and Hanxiao Liu and Zihang Dai and Noam Shazeer and Quoc V. Le},\n    year    = {2021},\n    eprint  = {2109.08668},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG}\n}\n```\n\n```bibtex\n@misc{ding2021erniedoc,\n    title   = {ERNIE-Doc: A Retrospective Long-Document Modeling Transformer}, \n    author  = {Siyu Ding and Junyuan Shang and Shuohuan Wang and Yu Sun and Hao Tian and Hua Wu and Haifeng Wang},\n    year    = {2021},\n    eprint  = {2012.15688},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}\n```\n\n```bibtex\n@misc{ding2021cogview,\n    title   = {CogView: Mastering Text-to-Image Generation via Transformers},\n    author  = {Ming Ding and Zhuoyi Yang and Wenyi Hong and Wendi Zheng and Chang Zhou and Da Yin and Junyang Lin and Xu Zou and Zhou Shao and Hongxia Yang and Jie Tang},\n    year    = {2021},\n    eprint  = {2105.13290},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n```bibtex\n@inproceedings{anonymous2022normformer,\n    title   = {NormFormer: Improved Transformer Pretraining with Extra Normalization},\n    author  = {Anonymous},\n    booktitle = {Submitted to The Tenth International Conference on Learning Representations },\n    year    = {2022},\n    url     = {https://openreview.net/forum?id=GMYWzWztDx5},\n    note    = {under review}\n}\n```\n\n```bibtex\n@misc{henry2020querykey,\n    title   = {Query-Key Normalization for Transformers},\n    author  = {Alex Henry and Prudhvi Raj Dachapally and Shubham Pawar and Yuxuan Chen},\n    year    = {2020},\n    eprint  = {2010.04245},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}\n```\n\n```bibtex\n@misc{liu2021swin,\n    title   = {Swin Transformer V2: Scaling Up Capacity and Resolution},\n    author  = {Ze Liu and Han Hu and Yutong Lin and Zhuliang Yao and Zhenda Xie and Yixuan Wei and Jia Ning and Yue Cao and Zheng Zhang and Li Dong and Furu Wei and Baining Guo},\n    year    = {2021},\n    eprint  = {2111.09883},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n*solve intelligence... then use that to solve everything else.* - Demis Hassabis\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{liu2021swin,\n    title   = {Swin Transformer V2: Scaling Up Capacity and Resolution},\n    author  = {Ze Liu and Han Hu and Yutong Lin and Zhuliang Yao and Zhenda Xie and Yixuan Wei and Jia Ning and Yue Cao and Zheng Zhang and Li Dong and Furu Wei and Baining Guo},\n    year    = {2021},\n    eprint  = {2111.09883},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{henry2020querykey,\n    title   = {Query-Key Normalization for Transformers},\n    author  = {Alex Henry and Prudhvi Raj Dachapally and Shubham Pawar and Yuxuan Chen},\n    year    = {2020},\n    eprint  = {2010.04245},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{anonymous2022normformer,\n    title   = {NormFormer: Improved Transformer Pretraining with Extra Normalization},\n    author  = {Anonymous},\n    booktitle = {Submitted to The Tenth International Conference on Learning Representations },\n    year    = {2022},\n    url     = {https://openreview.net/forum?id=GMYWzWztDx5},\n    note    = {under review}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{ding2021cogview,\n    title   = {CogView: Mastering Text-to-Image Generation via Transformers},\n    author  = {Ming Ding and Zhuoyi Yang and Wenyi Hong and Wendi Zheng and Chang Zhou and Da Yin and Junyang Lin and Xu Zou and Zhou Shao and Hongxia Yang and Jie Tang},\n    year    = {2021},\n    eprint  = {2105.13290},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{ding2021erniedoc,\n    title   = {ERNIE-Doc: A Retrospective Long-Document Modeling Transformer}, \n    author  = {Siyu Ding and Junyuan Shang and Shuohuan Wang and Yu Sun and Hao Tian and Hua Wu and Haifeng Wang},\n    year    = {2021},\n    eprint  = {2012.15688},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{so2021primer,\n    title   = {Primer: Searching for Efficient Transformers for Language Modeling}, \n    author  = {David R. So and Wojciech Ma\u0144ke and Hanxiao Liu and Zihang Dai and Noam Shazeer and Quoc V. Le},\n    year    = {2021},\n    eprint  = {2109.08668},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{csord\u00e1s2021devil,\n    title   = {The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers},\n    author  = {R\u00f3bert Csord\u00e1s and Kazuki Irie and J\u00fcrgen Schmidhuber},\n    year    = {2021},\n    eprint  = {2108.12284},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@software{peng_bo_2021_5196578,\n    author       = {PENG Bo},\n    title        = {BlinkDL/RWKV-LM: 0.01},\n    month        = {aug},\n    year         = {2021},\n    publisher    = {Zenodo},\n    version      = {0.01},\n    doi          = {10.5281/zenodo.5196578},\n    url          = {https://doi.org/10.5281/zenodo.5196578}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@Article{AlphaFold2021,\n    author  = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\\v{Z}}{\\'\\i}dek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A A and Ballard, Andrew J and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},\n    journal = {Nature},\n    title   = {Highly accurate protein structure prediction with {AlphaFold}},\n    year    = {2021},\n    doi     = {10.1038/s41586-021-03819-2},\n    note    = {(Accelerated article preview)},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{su2021roformer,\n    title   = {RoFormer: Enhanced Transformer with Rotary Position Embedding},\n    author  = {Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu},\n    year    = {2021},\n    eprint  = {2104.09864},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{zhang2019root,\n    title   = {Root Mean Square Layer Normalization},\n    author  = {Biao Zhang and Rico Sennrich},\n    year    = {2019},\n    eprint  = {1910.07467},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{narang2021transformer,\n    title       = {Do Transformer Modifications Transfer Across Implementations and Applications?},\n    author      = {Sharan Narang and Hyung Won Chung and Yi Tay and William Fedus and Thibault Fevry and Michael Matena and Karishma Malkan and Noah Fiedel and Noam Shazeer and Zhenzhong Lan and Yanqi Zhou and Wei Li and Nan Ding and Jake Marcus and Adam Roberts and Colin Raffel},\n    year        = {2021},\n    eprint      = {2102.11972},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{parisotto2019stabilizing,\n    title     = {Stabilizing Transformers for Reinforcement Learning},\n    author    = {Emilio Parisotto and H. Francis Song and Jack W. Rae and Razvan Pascanu and Caglar Gulcehre and Siddhant M. Jayakumar and Max Jaderberg and Raphael Lopez Kaufman and Aidan Clark and Seb Noury and Matthew M. Botvinick and Nicolas Heess and Raia Hadsell},\n    year      = {2019},\n    eprint    = {1910.06764},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{press2021ALiBi,\n    title   = {Train Short, Test Long: Attention with Linear Biases Enable Input Length Extrapolation},\n    author  = {Ofir Press and Noah A. Smith and Mike Lewis},\n    year    = {2021},\n    url     = {https://ofir.io/train_short_test_long.pdf}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{press2020shortformer,\n    title   = {Shortformer: Better Language Modeling using Shorter Inputs},\n    author  = {Ofir Press and Noah A. Smith and Mike Lewis},\n    year    = {2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{carion2020endtoend,\n    title   = {End-to-End Object Detection with Transformers},\n    author  = {Nicolas Carion and Francisco Massa and Gabriel Synnaeve and Nicolas Usunier and Alexander Kirillov and Sergey Zagoruyko},\n    year    = {2020},\n    eprint  = {2005.12872},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{he2020realformer,\n    title   = {RealFormer: Transformer Likes Residual Attention},\n    author  = {Ruining He and Anirudh Ravula and Bhargav Kanagal and Joshua Ainslie},\n    year    = {2020},\n    eprint  = {2012.11747},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{martins-etal-2020-sparse,\n    title   = \"Sparse Text Generation\",\n    author  = \"Martins, Pedro Henrique  and\n        Marinho, Zita  and\n        Martins, Andr{\\'e} F. T.\",\n    booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)\",\n    month   = nov,\n    year    = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url     = \"https://www.aclweb.org/anthology/2020.emnlp-main.348\"\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{raffel2020exploring,\n    title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}, \n    author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},\n    year    = {2020},\n    eprint  = {1910.10683},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{huang2019attention,\n    title   = {Attention on Attention for Image Captioning},\n    author  = {Lun Huang and Wenmin Wang and Jie Chen and Xiao-Yong Wei},\n    year    = {2019},\n    eprint  = {1908.06954},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{dosovitskiy2020image,\n    title   = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},\n    author  = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},\n    year    = {2020},\n    eprint  = {2010.11929},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{ke2020rethinking,\n    title     = {Rethinking Positional Encoding in Language Pre-training},\n    author    = {Guolin Ke and Di He and Tie-Yan Liu},\n    year      = {2020},\n    eprint    = {2006.15595},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{lu2019understanding,\n    title   = {Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View}, \n    author  = {Yiping Lu and Zhuohan Li and Di He and Zhiqing Sun and Bin Dong and Tao Qin and Liwei Wang and Tie-Yan Liu},\n    year    = {2019},\n    eprint  = {1906.02762},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{press2020improving,\n    title   = {Improving Transformer Models by Reordering their Sublayers}, \n    author  = {Ofir Press and Noah A. Smith and Omer Levy},\n    year    = {2020},\n    eprint  = {1911.03864},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{cordonnier2020multihead,\n    title   = {Multi-Head Attention: Collaborate Instead of Concatenate},\n    author  = {Jean-Baptiste Cordonnier and Andreas Loukas and Martin Jaggi},\n    year    = {2020},\n    eprint  = {2006.16362},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{shazeer2020talkingheads,\n    title   = {Talking-Heads Attention}, \n    author  = {Noam Shazeer and Zhenzhong Lan and Youlong Cheng and Nan Ding and Le Hou},\n    year    = {2020},\n    eprint  = {2003.02436},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{correia2019adaptively,\n    title   = {Adaptively Sparse Transformers},\n    author  = {Gon\u00e7alo M. Correia and Vlad Niculae and Andr\u00e9 F. T. Martins},\n    year    = {2019},\n    eprint  = {1909.00015},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{zhao2019explicit,\n    title   = {Explicit Sparse Transformer: Concentrated Attention Through Explicit Selection}, \n    author  = {Guangxiang Zhao and Junyang Lin and Zhiyuan Zhang and Xuancheng Ren and Qi Su and Xu Sun},\n    year    = {2019},\n    eprint  = {1912.11637},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{burtsev2020memory,\n    title   = {Memory Transformer}, \n    author  = {Mikhail S. Burtsev and Grigory V. Sapunov},\n    year    = {2020},\n    eprint  = {2006.11527},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{bhojanapalli2020lowrank,\n    title   = {Low-Rank Bottleneck in Multi-head Attention Models},\n    author  = {Srinadh Bhojanapalli and Chulhee Yun and Ankit Singh Rawat and Sashank J. Reddi and Sanjiv Kumar},\n    year    = {2020},\n    eprint  = {2002.07028}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{bachlechner2020rezero,\n    title   = {ReZero is All You Need: Fast Convergence at Large Depth},\n    author  = {Thomas Bachlechner and Bodhisattwa Prasad Majumder and Huanru Henry Mao and Garrison W. Cottrell and Julian McAuley},\n    year    = {2020},\n    url     = {https://arxiv.org/abs/2003.04887}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{shazeer2020glu,\n    title   = {GLU Variants Improve Transformer},\n    author  = {Noam Shazeer},\n    year    = {2020},\n    url     = {https://arxiv.org/abs/2002.05202}    \n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{1910.05895,\n    author  = {Toan Q. Nguyen and Julian Salazar},\n    title   = {Transformers without Tears: Improving the Normalization of Self-Attention},\n    year    = {2019},\n    eprint  = {arXiv:1910.05895},\n    doi     = {10.5281/zenodo.3525484},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{DBLP:journals/corr/abs-1907-01470,\n    author    = {Sainbayar Sukhbaatar and\n               Edouard Grave and\n               Guillaume Lample and\n               Herv{\\'{e}} J{\\'{e}}gou and\n               Armand Joulin},\n    title     = {Augmenting Self-attention with Persistent Memory},\n    journal   = {CoRR},\n    volume    = {abs/1907.01470},\n    year      = {2019},\n    url       = {http://arxiv.org/abs/1907.01470}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{vaswani2017attention,\n    title   = {Attention Is All You Need},\n    author  = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},\n    year    = {2017},\n    eprint  = {1706.03762},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "        depth = 12, \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lucidrains/x-transformers",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-10-24T22:13:25Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-09T14:33:57Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.901734534968967
      ],
      "excerpt": "A concise but fully-featured transformer, complete with a set of promising experimental features from various papers. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = TransformerWrapper( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9362248052008364
      ],
      "excerpt": "Proposes adding learned memory key / values prior to attention. They were able to remove feedforwards altogether and attain similar performance to the original transformers. I have found that keeping the feedforwards and adding the memory key / values leads to even better performance. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8063801170561898
      ],
      "excerpt": "Proposes adding learned tokens, akin to CLS tokens, named memory tokens, that is passed through the attention layers alongside the input tokens. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = TransformerWrapper( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9313425853426271
      ],
      "excerpt": "They experiment with alternatives to Layer normalization and found one that is both effective and simpler. Researchers have shared with me this leads to faster convergence. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = TransformerWrapper( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8916273984276659
      ],
      "excerpt": "Noam Shazeer paper that explores gating in the feedforward, finding that simple gating with GELU leads to significant improvements. This variant also showed up in the latest mT5 architecture. You should always turn this on (I may eventually turn it on by default). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = TransformerWrapper( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8987316672830127
      ],
      "excerpt": "This paper used neural architecture search and found an activation, Relu Squared, that is both simpler and performs better than GELU, in the autoregressive language model setting. I have confirmed this in my independent experiments. However, if one were using the GLU variant from above, GELU still performs better. Pending further corroboration. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = TransformerWrapper( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9798658138165183
      ],
      "excerpt": "This paper proposes to do away with normalization altogether, and instead gate the output of each branch with a single learned scalar, initialized at zero. They demonstrate convergence for very deep networks, convolution or attention, all without normalization. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = TransformerWrapper( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9823509367341463
      ],
      "excerpt": "This paper proposes an efficient way to sparsify attention by zeroing all dot-product query/key values not within the top k values. The show that this cheap method was as effective as other more expensive operations like sparsemax or entmax15. This technique comes with the cost of an extra hyperparameter (the top k values to keep). The paper recommends a value of k = 8 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = TransformerWrapper( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = TransformerWrapper( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9269886264501026
      ],
      "excerpt": "A Noam Shazeer paper that proposes mixing information between heads pre and post attention (softmax). This comes with the cost of extra memory and compute. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = TransformerWrapper( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9676079068476413
      ],
      "excerpt": "Share redundent learned key/query projections accross heads. Collaborative attention reduces the number of parameters but requires slightly more memory and computation. A good compression factor to match the performance of the vanilla multi-head attention is between 0.25 and 0.5. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = TransformerWrapper( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9791120511724402,
        0.9618165093305965
      ],
      "excerpt": "This paper proposes to add a gated linear unit at the end of the attention layer, further gated by the original queries. Although this is not widely used outside of visual question / answering, I suspect it should lead to improvements after seeing the success of the feedforward GLU variant. \nUpdate: After some experimentation, I found this variant actually performs worse, but if it were to be modified to not concatenate the queries before gating, it performs much better. That is what we will be using in this repository. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = TransformerWrapper( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9447604709582774
      ],
      "excerpt": "<a href=\"https://github.com/deepmind/alphafold\">Alphafold2</a> had a peculiar variant of attention where they gate the aggregated values with the input, presumably to have the block have more control over the update. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = TransformerWrapper( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9870629994779414,
        0.978858775965193
      ],
      "excerpt": "This paper proposes to break from the normal fixed pattern of alternating attention and feedforwards, but to have blocks of only attention at the beginning followed by blocks of feedforwards at the end. This was further corroborated by a paper by Nvidia that reduces the number of attention layers to be 1/3rd of the feedforwards without loss in performance. \nThe amount of interleaving is controlled by a \"sandwich coefficient\", which they found to be optimal at a value of 6. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = TransformerWrapper( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9988587109987077
      ],
      "excerpt": "        sandwich_coef = 6  #: interleave attention and feedforwards with sandwich coefficient of 6 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.928329439107758
      ],
      "excerpt": "The authors propose to view the success of transformers from a dynamical systems point of view, and then proposes an improvement based on mathematics of that POV. Specifically, they propose to place the attention layer in between two feedforward layers. This was adopted by a paper using transformers for speech recognition, the <a href=\"https://arxiv.org/abs/2005.08100\">Conformer</a>. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = TransformerWrapper( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9703471212084376
      ],
      "excerpt": "T5 is one of the most successful encoder / decoder transformer architectures trained to date. They invented a new simplified relative positional encoding based on learned bias values that are added to the attention matrix pre-softmax. This bias is shared and injected into each attention layer. I have decided to include this because it offers a cheap way to have relative positional encoding (superior to absolute positional), and I have read papers that suggest having positional encoding added to each layer (vs only before the first) is beneficial. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = TransformerWrapper( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9933086355429216,
        0.9373303097206834
      ],
      "excerpt": "In these two papers, the authors independently figured out a new technique where fixed sinusoidal positional embeddings are injected into the input prior to the queries and keys projection for all layers, leading to \"position infused\" attention, but leaving the actual tokens (values) uncolored by positional embedding. The Shortformer paper uses this property to cache the tokens for simplified recurrent type of transformer that bested Transformer-XL. \nI have tested this, and found that it produces better results than plain absolute positional encoding, even in the absence of recurrence. However, I have found that the T5 relative positional bias (also injected into all layers and has the same properties as PIA) performs even better. So given the option, you should just go with T5's rel_pos_bias above. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = TransformerWrapper( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9704268947903335
      ],
      "excerpt": "This paper from Google proposes residualizing the pre-attention scores across all layers. At the cost of no extra parameters, they show improvement on top of regular attention networks. If you turn on this setting, be aware that the best results in the paper used post-normalization, in which case a learning warmup will be needed. The authors also reported that they could use a higher learning rate and get even better gains in the same amount of steps. (In the paper they use 2e-4 vs 1e-4 for vanilla transformer) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = TransformerWrapper( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8241452104844125
      ],
      "excerpt": "        pre_norm = False,       #: in the paper, residual attention had best results with post-layernorm \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = XTransformer( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8334691918865046
      ],
      "excerpt": "Then, you can retrieve the memories at each step with the return_mems keyword and pass it to the next iteration. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8339987841664035
      ],
      "excerpt": "<a href=\"https://arxiv.org/abs/2012.15688\">This paper</a> proposes a simple technique to enhance the range of Transformer-XL. They simply route the memory segment of a layer to the layer below it, for the next recurrent step. You can enable this by setting shift_mem_down = 1. You can also shift down arbitrary number of layers by setting this value to &gt; 1. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9752061778177051
      ],
      "excerpt": "The authors propose gating the residual connections in the transformer network and demonstrate increased stability and performance for Transformer-XL in a variety of reinforcement learning tasks. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = TransformerWrapper( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.969820428614119
      ],
      "excerpt": "Developed in Beijing, this new technique quickly gained interest in the NLP circles. In short, it allows you to endow the transformer with relative positional embeddings at the cost of no learned parameters. You apply a rotary operation to the queries and keys prior to their dot product in attention. The big idea is injecting positions through rotations. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = TransformerWrapper( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9886868828282322,
        0.8820715248739425
      ],
      "excerpt": "<a href=\"https://ofir.io/train_short_test_long.pdf\">This paper</a> proposes to simply apply a static linear bias to the attention matrix. The authors show this is not only effective as a relative positional encoding, but also allows the attention net to extrapolate to greater sequences length than what it was trained on, for autoregressive language models. \nUpdate: It may be that ALiBi enforces a strong local attention across the heads, and may hinder it from attending at distances greater than 1k. To avoid any issues with global message passing, I've decided to introduce another hyperparameter alibi_num_heads, so one can specify less heads for the ALiBi bias \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = TransformerWrapper( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8522552023710642
      ],
      "excerpt": "        alibi_num_heads = 4    #: only use ALiBi for 4 out of the 8 heads, so other 4 heads can still attend far distances \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8609979391583038
      ],
      "excerpt": "An <a href=\"https://github.com/BlinkDL\">independent researcher</a> has found that shifting a subset of the feature dimension along the sequence dimension by 1 token helps with convergence (<a href=\"https://zhuanlan.zhihu.com/p/191393788\">Time-mixing</a>). I have tested this for the autoregressive case and can confirm that it leads to greatly improved convergence. This also lines up with <a href=\"https://arxiv.org/abs/2106.07477\">the results</a> of some papers in the vision domain. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8452521396884909,
        0.9581797295448452,
        0.9005573743303128
      ],
      "excerpt": "Update: new experiments by @sdtblck suggests this may only work for character-level training \nUpdate: after more experiments, it seems that in the context of BPE encoding, with rotary turned on, there is no benefit to shifting. for character-level training, shifting may still improve a tiny bit \nUpdate: When doing BPE encoded tokens, it seems that shift of 2 will bottleneck the dimensions (divided by 5). It is recommended you always do a shift of 1, unless if you are working with character level. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = TransformerWrapper( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9420080649831446
      ],
      "excerpt": "If you want finer control over how much is shifted per block (whether attention or feedforward), simply pass in a tuple of size that is equal to the number of layers. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = TransformerWrapper( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8512635417422002
      ],
      "excerpt": "        shift_tokens = (1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0) #: 12 blocks, attention and feedforward alternating, with progressively less shifting \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9645066318443712
      ],
      "excerpt": "This technique first made an appearance in <a href=\"https://arxiv.org/abs/2105.13290\">the CoqView paper</a>, a Chinese version of the famous text-to-image transformer DALL-E. They propose, when using pre-layernorm, to add an extra layernorm to all the branch outputs. I have found this to be very effective for a number of projects, when facing instability during training. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = TransformerWrapper( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8968064349585209
      ],
      "excerpt": "This <a href=\"https://openreview.net/forum?id=GMYWzWztDx5\">paper</a> uncovers an issue with pre-norm transformers where gradients are mismatched between the early and later layers. They propose 4 changes, of which I will be offering 3. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = TransformerWrapper( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8930717507369081
      ],
      "excerpt": "The second change is an extra layernorm right after the activation in the feedforward. I have also verified a slight improvement, at the cost of extra compute. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = TransformerWrapper( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = TransformerWrapper( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8937859747469259
      ],
      "excerpt": "The last change is a layernorm right after the outwards projection in attention. This is actually identical to the sandwich norm proposed by the Coqview paper, so you can use this by simply setting sandwich_norm = True, although it would also add it to the feedforward layer. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8705045792253795,
        0.837804640533704,
        0.9013181113678639
      ],
      "excerpt": "This <a href=\"https://arxiv.org/abs/2010.04245\">paper</a> proposes to l2 normalize the queries and keys along the head dimension before the dot product (cosine similarity), with the additional change of the scale being learned rather than static. The normalization prevents the attention operation from overflowing, a perennial problem when training transformers. \nThis was validated at scale recently by the training of <a href=\"https://arxiv.org/abs/2111.09883\">a 3B parameter vision transformer</a>. The SwinV2 paper also proposes to change the pre-layernorm to a post-layernorm for further stability. \nI have validated that this works just as well as dot product attention in an autoregressive setting, if one were to initialize the temperature as proposed in the QK-norm paper (as a function of the sequence length). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = TransformerWrapper( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = ContinuousTransformerWrapper( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A simple but complete full-attention transformer with a set of promising experimental features from various papers",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lucidrains/x-transformers/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 90,
      "date": "Thu, 09 Dec 2021 19:09:57 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/lucidrains/x-transformers/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "lucidrains/x-transformers",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```bash\n$ pip install x-transformers\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9482083101194309
      ],
      "excerpt": "Alternatively, if you would like to use entmax15, you can also do so with one setting as shown below. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8632552915565684
      ],
      "excerpt": "Update: It may be that ALiBi enforces a strong local attention across the heads, and may hinder it from attending at distances greater than 1k. To avoid any issues with global message passing, I've decided to introduce another hyperparameter alibi_num_heads, so one can specify less heads for the ALiBi bias \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8639258741734444
      ],
      "excerpt": "        depth = 6, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8668671021142615
      ],
      "excerpt": "<img src=\"./images/all-attention.png\" width=\"500px\"></img> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639258741734444
      ],
      "excerpt": "    depth = 6, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8564348382753311
      ],
      "excerpt": "<img src=\"./images/memory-transformer.png\" width=\"500px\"></img> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639258741734444
      ],
      "excerpt": "        depth = 6, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8536753758407865
      ],
      "excerpt": "<img src=\"./images/scalenorm.png\"></img> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639258741734444
      ],
      "excerpt": "        depth = 6, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8536753758407865
      ],
      "excerpt": "<img src=\"./images/ffglu.png\"></img> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639258741734444
      ],
      "excerpt": "        depth = 6, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639258741734444
      ],
      "excerpt": "        depth = 6, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "        ff_relu_squared = True \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8536753758407865
      ],
      "excerpt": "<img src=\"./images/rezero.png\"></img> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639258741734444
      ],
      "excerpt": "        depth = 6, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8671904599658984
      ],
      "excerpt": "<img src=\"./images/topk-attention.png\" width=\"500px\"></img> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639258741734444
      ],
      "excerpt": "        depth = 6, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639258741734444
      ],
      "excerpt": "        depth = 6, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8465776260348857
      ],
      "excerpt": "<img src=\"./images/talking-heads.png\" width=\"500px\"></img> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639258741734444
      ],
      "excerpt": "        depth = 6, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8671904599658984
      ],
      "excerpt": "<img src=\"./images/collaborative-attention.png\" width=\"500px\"></img> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639258741734444
      ],
      "excerpt": "        depth = 6, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "        attn_collab_heads = True, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8386173948664303
      ],
      "excerpt": "<img src=\"./images/attention-on-attention.png\"></img> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639258741734444
      ],
      "excerpt": "        depth = 6, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8633537525994954
      ],
      "excerpt": "        attn_on_attn = True  #: gate output of attention layer, by queries \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8671904599658984
      ],
      "excerpt": "<img src=\"./images/gate_values.png\" width=\"400px\"></img> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639258741734444
      ],
      "excerpt": "        depth = 6, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8536753758407865,
        0.8536753758407865
      ],
      "excerpt": "<img src=\"./images/sandwich.png\"></img> \n<img src=\"./images/sandwich-2.png\"></img> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639258741734444
      ],
      "excerpt": "        depth = 6, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8536753758407865,
        0.8536753758407865
      ],
      "excerpt": "<img src=\"./images/macaron-1.png\"></img> \n<img src=\"./images/macaron-2.png\"></img> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639258741734444
      ],
      "excerpt": "        depth = 6, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639258741734444
      ],
      "excerpt": "        depth = 6, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8671904599658984
      ],
      "excerpt": "<img src=\"./images/pia.png\" width=\"500px\"></img> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639258741734444
      ],
      "excerpt": "        depth = 6, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8671904599658984
      ],
      "excerpt": "<img src=\"./images/residual_attn.png\" width=\"500px\"></img> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639258741734444
      ],
      "excerpt": "        depth = 6, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "from x_transformers import XTransformer \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639258741734444
      ],
      "excerpt": "        depth = 6, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "        rel_pos_bias = True \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984,
        0.8594142235991984,
        0.8594142235991984
      ],
      "excerpt": "logits1, mems1  = model_xl(seg1, return_mems = True) \nlogits2, mems2  = model_xl(seg2, mems = mems1, return_mems = True) \nlogits3, mems3  = model_xl(seg3, mems = mems2, return_mems = True) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8611311387238124
      ],
      "excerpt": "<img src=\"./images/enhanced-recurrence.png\" width=\"400px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639258741734444
      ],
      "excerpt": "        depth = 6, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "        rotary_pos_emb = True \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "logits1, mems1  = model_xl(seg1, return_mems = True) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8671904599658984
      ],
      "excerpt": "<img src=\"./images/gating.png\" width=\"500px\"></img> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639258741734444
      ],
      "excerpt": "        depth = 6, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "        gate_residual = True \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8671904599658984
      ],
      "excerpt": "<img src=\"./images/rotary.png\" width=\"500px\"></img> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639258741734444
      ],
      "excerpt": "        depth = 6, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639258741734444
      ],
      "excerpt": "        depth = 6, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639258741734444
      ],
      "excerpt": "        depth = 6, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639258741734444
      ],
      "excerpt": "        depth = 6, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9042534665813847
      ],
      "excerpt": "<img src=\"./images/sandwich_norm.png\" width=\"400px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639258741734444
      ],
      "excerpt": "        depth = 6, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.821964837177328
      ],
      "excerpt": "        sandwich_norm = True #: set this to True \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9042534665813847
      ],
      "excerpt": "<img src=\"./images/normformer.png\" width=\"400px\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639258741734444
      ],
      "excerpt": "        depth = 6, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.821964837177328
      ],
      "excerpt": "        attn_head_scale = True  #: set this to True \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639258741734444
      ],
      "excerpt": "        depth = 6, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.821964837177328
      ],
      "excerpt": "        ff_post_act_ln = True #: set this to True \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639258741734444
      ],
      "excerpt": "        depth = 6, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.821964837177328
      ],
      "excerpt": "        scale_residual = True #: set this to True \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8671904599658984
      ],
      "excerpt": "<img src=\"./images/cosine-sim-attention.png\" width=\"400px\"></img> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639258741734444
      ],
      "excerpt": "        depth = 6, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.821964837177328
      ],
      "excerpt": "        use_qk_norm_attn = True, #: set this to True \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8802237935578744
      ],
      "excerpt": "model = CrossAttender(dim = 512, depth = 6) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8307133686518146
      ],
      "excerpt": "model(x, mask = mask) #: (1, 1024, 100) \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/lucidrains/x-transformers/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 Phil Wang\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# x-transformers",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "x-transformers",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "lucidrains",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/lucidrains/x-transformers/blob/main/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-12-03T06:09:56Z",
        "datePublished": "2021-12-03T06:10:27Z",
        "html_url": "https://github.com/lucidrains/x-transformers/releases/tag/0.21.1",
        "name": "0.21.1",
        "tag_name": "0.21.1",
        "tarball_url": "https://api.github.com/repos/lucidrains/x-transformers/tarball/0.21.1",
        "url": "https://api.github.com/repos/lucidrains/x-transformers/releases/54553300",
        "zipball_url": "https://api.github.com/repos/lucidrains/x-transformers/zipball/0.21.1"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-12-02T18:53:17Z",
        "datePublished": "2021-12-02T18:54:04Z",
        "html_url": "https://github.com/lucidrains/x-transformers/releases/tag/0.21.0",
        "name": "0.21.0",
        "tag_name": "0.21.0",
        "tarball_url": "https://api.github.com/repos/lucidrains/x-transformers/tarball/0.21.0",
        "url": "https://api.github.com/repos/lucidrains/x-transformers/releases/54523939",
        "zipball_url": "https://api.github.com/repos/lucidrains/x-transformers/zipball/0.21.0"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-11-26T01:03:33Z",
        "datePublished": "2021-11-26T01:03:51Z",
        "html_url": "https://github.com/lucidrains/x-transformers/releases/tag/0.20.9",
        "name": "0.20.9",
        "tag_name": "0.20.9",
        "tarball_url": "https://api.github.com/repos/lucidrains/x-transformers/tarball/0.20.9",
        "url": "https://api.github.com/repos/lucidrains/x-transformers/releases/54105066",
        "zipball_url": "https://api.github.com/repos/lucidrains/x-transformers/zipball/0.20.9"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-11-26T00:21:41Z",
        "datePublished": "2021-11-26T00:22:01Z",
        "html_url": "https://github.com/lucidrains/x-transformers/releases/tag/0.20.8",
        "name": "0.20.8",
        "tag_name": "0.20.8",
        "tarball_url": "https://api.github.com/repos/lucidrains/x-transformers/tarball/0.20.8",
        "url": "https://api.github.com/repos/lucidrains/x-transformers/releases/54104113",
        "zipball_url": "https://api.github.com/repos/lucidrains/x-transformers/zipball/0.20.8"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-11-25T21:39:20Z",
        "datePublished": "2021-11-25T21:39:36Z",
        "html_url": "https://github.com/lucidrains/x-transformers/releases/tag/0.20.7",
        "name": "0.20.7",
        "tag_name": "0.20.7",
        "tarball_url": "https://api.github.com/repos/lucidrains/x-transformers/tarball/0.20.7",
        "url": "https://api.github.com/repos/lucidrains/x-transformers/releases/54100398",
        "zipball_url": "https://api.github.com/repos/lucidrains/x-transformers/zipball/0.20.7"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-11-06T17:36:57Z",
        "datePublished": "2021-11-06T17:37:25Z",
        "html_url": "https://github.com/lucidrains/x-transformers/releases/tag/0.20.6",
        "name": "0.20.6",
        "tag_name": "0.20.6",
        "tarball_url": "https://api.github.com/repos/lucidrains/x-transformers/tarball/0.20.6",
        "url": "https://api.github.com/repos/lucidrains/x-transformers/releases/52850380",
        "zipball_url": "https://api.github.com/repos/lucidrains/x-transformers/zipball/0.20.6"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-11-01T23:08:31Z",
        "datePublished": "2021-11-01T23:08:53Z",
        "html_url": "https://github.com/lucidrains/x-transformers/releases/tag/0.20.5",
        "name": "0.20.5",
        "tag_name": "0.20.5",
        "tarball_url": "https://api.github.com/repos/lucidrains/x-transformers/tarball/0.20.5",
        "url": "https://api.github.com/repos/lucidrains/x-transformers/releases/52481227",
        "zipball_url": "https://api.github.com/repos/lucidrains/x-transformers/zipball/0.20.5"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-10-23T19:01:52Z",
        "datePublished": "2021-10-23T19:02:09Z",
        "html_url": "https://github.com/lucidrains/x-transformers/releases/tag/0.20.4",
        "name": "0.20.4",
        "tag_name": "0.20.4",
        "tarball_url": "https://api.github.com/repos/lucidrains/x-transformers/tarball/0.20.4",
        "url": "https://api.github.com/repos/lucidrains/x-transformers/releases/51916061",
        "zipball_url": "https://api.github.com/repos/lucidrains/x-transformers/zipball/0.20.4"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-10-20T01:06:08Z",
        "datePublished": "2021-10-20T01:06:29Z",
        "html_url": "https://github.com/lucidrains/x-transformers/releases/tag/0.20.2",
        "name": "0.20.2",
        "tag_name": "0.20.2",
        "tarball_url": "https://api.github.com/repos/lucidrains/x-transformers/tarball/0.20.2",
        "url": "https://api.github.com/repos/lucidrains/x-transformers/releases/51664286",
        "zipball_url": "https://api.github.com/repos/lucidrains/x-transformers/zipball/0.20.2"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-10-20T00:53:14Z",
        "datePublished": "2021-10-20T00:53:32Z",
        "html_url": "https://github.com/lucidrains/x-transformers/releases/tag/0.20.1",
        "name": "0.20.1",
        "tag_name": "0.20.1",
        "tarball_url": "https://api.github.com/repos/lucidrains/x-transformers/tarball/0.20.1",
        "url": "https://api.github.com/repos/lucidrains/x-transformers/releases/51663868",
        "zipball_url": "https://api.github.com/repos/lucidrains/x-transformers/zipball/0.20.1"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-10-19T21:18:04Z",
        "datePublished": "2021-10-19T21:18:18Z",
        "html_url": "https://github.com/lucidrains/x-transformers/releases/tag/0.20.0",
        "name": "0.20.0",
        "tag_name": "0.20.0",
        "tarball_url": "https://api.github.com/repos/lucidrains/x-transformers/tarball/0.20.0",
        "url": "https://api.github.com/repos/lucidrains/x-transformers/releases/51654839",
        "zipball_url": "https://api.github.com/repos/lucidrains/x-transformers/zipball/0.20.0"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-10-04T14:00:44Z",
        "datePublished": "2021-10-04T14:01:02Z",
        "html_url": "https://github.com/lucidrains/x-transformers/releases/tag/0.19.1",
        "name": "0.19.1",
        "tag_name": "0.19.1",
        "tarball_url": "https://api.github.com/repos/lucidrains/x-transformers/tarball/0.19.1",
        "url": "https://api.github.com/repos/lucidrains/x-transformers/releases/50745098",
        "zipball_url": "https://api.github.com/repos/lucidrains/x-transformers/zipball/0.19.1"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-10-03T17:58:14Z",
        "datePublished": "2021-10-03T17:58:29Z",
        "html_url": "https://github.com/lucidrains/x-transformers/releases/tag/0.19.0",
        "name": "0.19.0",
        "tag_name": "0.19.0",
        "tarball_url": "https://api.github.com/repos/lucidrains/x-transformers/tarball/0.19.0",
        "url": "https://api.github.com/repos/lucidrains/x-transformers/releases/50703741",
        "zipball_url": "https://api.github.com/repos/lucidrains/x-transformers/zipball/0.19.0"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-09-20T14:32:14Z",
        "datePublished": "2021-09-20T14:32:32Z",
        "html_url": "https://github.com/lucidrains/x-transformers/releases/tag/0.18.0",
        "name": "0.18.0",
        "tag_name": "0.18.0",
        "tarball_url": "https://api.github.com/repos/lucidrains/x-transformers/tarball/0.18.0",
        "url": "https://api.github.com/repos/lucidrains/x-transformers/releases/49858895",
        "zipball_url": "https://api.github.com/repos/lucidrains/x-transformers/zipball/0.18.0"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-09-18T19:14:38Z",
        "datePublished": "2021-09-18T19:14:53Z",
        "html_url": "https://github.com/lucidrains/x-transformers/releases/tag/0.17.13",
        "name": "0.17.13",
        "tag_name": "0.17.13",
        "tarball_url": "https://api.github.com/repos/lucidrains/x-transformers/tarball/0.17.13",
        "url": "https://api.github.com/repos/lucidrains/x-transformers/releases/49799953",
        "zipball_url": "https://api.github.com/repos/lucidrains/x-transformers/zipball/0.17.13"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-09-06T16:35:07Z",
        "datePublished": "2021-09-06T16:35:33Z",
        "html_url": "https://github.com/lucidrains/x-transformers/releases/tag/0.17.12",
        "name": "0.17.12",
        "tag_name": "0.17.12",
        "tarball_url": "https://api.github.com/repos/lucidrains/x-transformers/tarball/0.17.12",
        "url": "https://api.github.com/repos/lucidrains/x-transformers/releases/49083359",
        "zipball_url": "https://api.github.com/repos/lucidrains/x-transformers/zipball/0.17.12"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-09-06T16:29:44Z",
        "datePublished": "2021-09-06T16:30:02Z",
        "html_url": "https://github.com/lucidrains/x-transformers/releases/tag/0.17.11",
        "name": "0.17.11",
        "tag_name": "0.17.11",
        "tarball_url": "https://api.github.com/repos/lucidrains/x-transformers/tarball/0.17.11",
        "url": "https://api.github.com/repos/lucidrains/x-transformers/releases/49083138",
        "zipball_url": "https://api.github.com/repos/lucidrains/x-transformers/zipball/0.17.11"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-09-03T19:37:32Z",
        "datePublished": "2021-09-03T19:37:56Z",
        "html_url": "https://github.com/lucidrains/x-transformers/releases/tag/0.17.10",
        "name": "0.17.10",
        "tag_name": "0.17.10",
        "tarball_url": "https://api.github.com/repos/lucidrains/x-transformers/tarball/0.17.10",
        "url": "https://api.github.com/repos/lucidrains/x-transformers/releases/48990048",
        "zipball_url": "https://api.github.com/repos/lucidrains/x-transformers/zipball/0.17.10"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-08-31T15:53:33Z",
        "datePublished": "2021-08-31T15:53:51Z",
        "html_url": "https://github.com/lucidrains/x-transformers/releases/tag/0.17.9",
        "name": "0.17.9",
        "tag_name": "0.17.9",
        "tarball_url": "https://api.github.com/repos/lucidrains/x-transformers/tarball/0.17.9",
        "url": "https://api.github.com/repos/lucidrains/x-transformers/releases/48757035",
        "zipball_url": "https://api.github.com/repos/lucidrains/x-transformers/zipball/0.17.9"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-08-31T15:45:51Z",
        "datePublished": "2021-08-31T15:46:28Z",
        "html_url": "https://github.com/lucidrains/x-transformers/releases/tag/0.17.8",
        "name": "0.17.8",
        "tag_name": "0.17.8",
        "tarball_url": "https://api.github.com/repos/lucidrains/x-transformers/tarball/0.17.8",
        "url": "https://api.github.com/repos/lucidrains/x-transformers/releases/48756519",
        "zipball_url": "https://api.github.com/repos/lucidrains/x-transformers/zipball/0.17.8"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-08-30T16:07:20Z",
        "datePublished": "2021-08-30T16:07:36Z",
        "html_url": "https://github.com/lucidrains/x-transformers/releases/tag/0.17.7",
        "name": "0.17.7",
        "tag_name": "0.17.7",
        "tarball_url": "https://api.github.com/repos/lucidrains/x-transformers/tarball/0.17.7",
        "url": "https://api.github.com/repos/lucidrains/x-transformers/releases/48685287",
        "zipball_url": "https://api.github.com/repos/lucidrains/x-transformers/zipball/0.17.7"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-08-28T14:18:39Z",
        "datePublished": "2021-08-28T14:19:00Z",
        "html_url": "https://github.com/lucidrains/x-transformers/releases/tag/0.17.6",
        "name": "0.17.6",
        "tag_name": "0.17.6",
        "tarball_url": "https://api.github.com/repos/lucidrains/x-transformers/tarball/0.17.6",
        "url": "https://api.github.com/repos/lucidrains/x-transformers/releases/48614253",
        "zipball_url": "https://api.github.com/repos/lucidrains/x-transformers/zipball/0.17.6"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-08-28T14:17:14Z",
        "datePublished": "2021-08-28T14:17:35Z",
        "html_url": "https://github.com/lucidrains/x-transformers/releases/tag/0.17.5",
        "name": "0.17.5",
        "tag_name": "0.17.5",
        "tarball_url": "https://api.github.com/repos/lucidrains/x-transformers/tarball/0.17.5",
        "url": "https://api.github.com/repos/lucidrains/x-transformers/releases/48614234",
        "zipball_url": "https://api.github.com/repos/lucidrains/x-transformers/zipball/0.17.5"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-08-27T22:17:11Z",
        "datePublished": "2021-08-27T22:17:30Z",
        "html_url": "https://github.com/lucidrains/x-transformers/releases/tag/0.17.4",
        "name": "0.17.4",
        "tag_name": "0.17.4",
        "tarball_url": "https://api.github.com/repos/lucidrains/x-transformers/tarball/0.17.4",
        "url": "https://api.github.com/repos/lucidrains/x-transformers/releases/48599923",
        "zipball_url": "https://api.github.com/repos/lucidrains/x-transformers/zipball/0.17.4"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-08-27T19:41:00Z",
        "datePublished": "2021-08-27T19:41:21Z",
        "html_url": "https://github.com/lucidrains/x-transformers/releases/tag/0.17.3",
        "name": "0.17.3",
        "tag_name": "0.17.3",
        "tarball_url": "https://api.github.com/repos/lucidrains/x-transformers/tarball/0.17.3",
        "url": "https://api.github.com/repos/lucidrains/x-transformers/releases/48593929",
        "zipball_url": "https://api.github.com/repos/lucidrains/x-transformers/zipball/0.17.3"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-08-27T18:51:53Z",
        "datePublished": "2021-08-27T18:52:10Z",
        "html_url": "https://github.com/lucidrains/x-transformers/releases/tag/0.17.2",
        "name": "0.17.2",
        "tag_name": "0.17.2",
        "tarball_url": "https://api.github.com/repos/lucidrains/x-transformers/tarball/0.17.2",
        "url": "https://api.github.com/repos/lucidrains/x-transformers/releases/48591430",
        "zipball_url": "https://api.github.com/repos/lucidrains/x-transformers/zipball/0.17.2"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-08-27T17:31:19Z",
        "datePublished": "2021-08-27T18:06:54Z",
        "html_url": "https://github.com/lucidrains/x-transformers/releases/tag/0.17.1",
        "name": "0.17.1",
        "tag_name": "0.17.1",
        "tarball_url": "https://api.github.com/repos/lucidrains/x-transformers/tarball/0.17.1",
        "url": "https://api.github.com/repos/lucidrains/x-transformers/releases/48589105",
        "zipball_url": "https://api.github.com/repos/lucidrains/x-transformers/zipball/0.17.1"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-08-27T03:58:12Z",
        "datePublished": "2021-08-27T03:58:32Z",
        "html_url": "https://github.com/lucidrains/x-transformers/releases/tag/0.17.0",
        "name": "0.17.0",
        "tag_name": "0.17.0",
        "tarball_url": "https://api.github.com/repos/lucidrains/x-transformers/tarball/0.17.0",
        "url": "https://api.github.com/repos/lucidrains/x-transformers/releases/48544631",
        "zipball_url": "https://api.github.com/repos/lucidrains/x-transformers/zipball/0.17.0"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-08-14T17:48:32Z",
        "datePublished": "2021-08-14T17:49:35Z",
        "html_url": "https://github.com/lucidrains/x-transformers/releases/tag/0.16.1",
        "name": "0.16.1",
        "tag_name": "0.16.1",
        "tarball_url": "https://api.github.com/repos/lucidrains/x-transformers/tarball/0.16.1",
        "url": "https://api.github.com/repos/lucidrains/x-transformers/releases/47855521",
        "zipball_url": "https://api.github.com/repos/lucidrains/x-transformers/zipball/0.16.1"
      },
      {
        "authorType": "User",
        "author_name": "lucidrains",
        "body": "",
        "dateCreated": "2021-08-13T22:39:42Z",
        "datePublished": "2021-08-13T22:39:56Z",
        "html_url": "https://github.com/lucidrains/x-transformers/releases/tag/0.16.0",
        "name": "0.16.0",
        "tag_name": "0.16.0",
        "tarball_url": "https://api.github.com/repos/lucidrains/x-transformers/tarball/0.16.0",
        "url": "https://api.github.com/repos/lucidrains/x-transformers/releases/47839046",
        "zipball_url": "https://api.github.com/repos/lucidrains/x-transformers/zipball/0.16.0"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1247,
      "date": "Thu, 09 Dec 2021 19:09:57 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "artificial-intelligence",
      "deep-learning",
      "attention-mechanism",
      "transformers"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Full encoder / decoder\n\n```python\nimport torch\nfrom x_transformers import XTransformer\n\nmodel = XTransformer(\n    dim = 512,\n    enc_num_tokens = 256,\n    enc_depth = 6,\n    enc_heads = 8,\n    enc_max_seq_len = 1024,\n    dec_num_tokens = 256,\n    dec_depth = 6,\n    dec_heads = 8,\n    dec_max_seq_len = 1024,\n    tie_token_emb = True      #: tie embeddings of encoder and decoder\n)\n\nsrc = torch.randint(0, 256, (1, 1024))\nsrc_mask = torch.ones_like(src).bool()\ntgt = torch.randint(0, 256, (1, 1024))\ntgt_mask = torch.ones_like(tgt).bool()\n\nloss = model(src, tgt, src_mask = src_mask, tgt_mask = tgt_mask) #: (1, 1024, 512)\nloss.backward()\n```\n\nDecoder-only (GPT-like)\n\n```python\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel = TransformerWrapper(\n    num_tokens = 20000,\n    max_seq_len = 1024,\n    attn_layers = Decoder(\n        dim = 512,\n        depth = 12,\n        heads = 8\n    )\n).cuda()\n\nx = torch.randint(0, 256, (1, 1024)).cuda()\n\nmodel(x) #: (1, 1024, 20000)\n```\n\nGPT3 would be approximately the following (but you wouldn't be able to run it anyways)\n\n```python\n\ngpt3 = TransformerWrapper(\n    num_tokens = 50000,\n    max_seq_len = 2048,\n    attn_layers = Decoder(\n        dim = 12288,\n        depth = 96,\n        heads = 96,\n        attn_dim_head = 128\n    )\n).cuda()\n```\n\nEncoder-only (BERT-like)\n\n```python\nimport torch\nfrom x_transformers import TransformerWrapper, Encoder\n\nmodel = TransformerWrapper(\n    num_tokens = 20000,\n    max_seq_len = 1024,\n    attn_layers = Encoder(\n        dim = 512,\n        depth = 12,\n        heads = 8\n    )\n).cuda()\n\nx = torch.randint(0, 256, (1, 1024)).cuda()\nmask = torch.ones_like(x).bool()\n\nmodel(x, mask = mask) #: (1, 1024, 20000)\n```\n\nState of the art image classification\n\n```python\nimport torch\nfrom x_transformers import ViTransformerWrapper, Encoder\n\nmodel = ViTransformerWrapper(\n    image_size = 256,\n    patch_size = 32,\n    num_classes = 1000,\n    attn_layers = Encoder(\n        dim = 512,\n        depth = 6,\n        heads = 8,\n    )\n)\n\nimg = torch.randn(1, 3, 256, 256)\nmodel(img) #: (1, 1000)\n```\n\nImage -> caption\n\n```python\nimport torch\nfrom x_transformers import ViTransformerWrapper, TransformerWrapper, Encoder, Decoder\n\nencoder = ViTransformerWrapper(\n    image_size = 256,\n    patch_size = 32,\n    attn_layers = Encoder(\n        dim = 512,\n        depth = 6,\n        heads = 8\n    )\n)\n\ndecoder = TransformerWrapper(\n    num_tokens = 20000,\n    max_seq_len = 1024,\n    attn_layers = Decoder(\n        dim = 512,\n        depth = 6,\n        heads = 8,\n        cross_attend = True\n    )\n)\n\nimg = torch.randn(1, 3, 256, 256)\ncaption = torch.randint(0, 20000, (1, 1024))\n\nencoded = encoder(img, return_embeddings = True)\ndecoder(caption, context = encoded) #: (1, 1024, 20000)\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}