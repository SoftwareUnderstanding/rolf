{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1703.04908"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you used this environment for your experiments or found it helpful, consider citing the following papers:\n\nEnvironments in this repo:\n<pre>\n@article{lowe2017multi,\n  title={Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments},\n  author={Lowe, Ryan and Wu, Yi and Tamar, Aviv and Harb, Jean and Abbeel, Pieter and Mordatch, Igor},\n  journal={Neural Information Processing Systems (NIPS)},\n  year={2017}\n}\n</pre>\n\nOriginal particle world environment:\n<pre>\n@article{mordatch2017emergence,\n  title={Emergence of Grounded Compositional Language in Multi-Agent Populations},\n  author={Mordatch, Igor and Abbeel, Pieter},\n  journal={arXiv preprint arXiv:1703.04908},\n  year={2017}\n}\n</pre>\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{mordatch2017emergence,\n  title={Emergence of Grounded Compositional Language in Multi-Agent Populations},\n  author={Mordatch, Igor and Abbeel, Pieter},\n  journal={arXiv preprint arXiv:1703.04908},\n  year={2017}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{lowe2017multi,\n  title={Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments},\n  author={Lowe, Ryan and Wu, Yi and Tamar, Aviv and Harb, Jean and Abbeel, Pieter and Mordatch, Igor},\n  journal={Neural Information Processing Systems (NIPS)},\n  year={2017}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/SintolRTOS/multi-agent_Example",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-07-27T20:21:07Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-07-27T20:21:57Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8377287841544716,
        0.8024544426600417
      ],
      "excerpt": "A simple multi-agent particle world with a continuous observation and discrete action space, along with some basic simulated physics. \nUsed in the paper Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8985991911119664
      ],
      "excerpt": "./multiagent/core.py: contains classes for various objects (Entities, Landmarks, Agents, etc.) that are used throughout the code. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9186136692352519,
        0.9008513089457102
      ],
      "excerpt": "./multiagent/policy.py: contains code for interactive policy based on keyboard input. \n./multiagent/scenario.py: contains base scenario object that is extended for all scenarios. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8528618469733519
      ],
      "excerpt": "    1) make_world(): creates all of the entities that inhabit the world (landmarks, agents, etc.), assigns their capabilities (whether they can communicate, or move, or both). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8025689979366682
      ],
      "excerpt": "    2) reset_world(): resets the world by assigning properties (position, color, etc.) to all entities in the world \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9604236543201415
      ],
      "excerpt": "    4) observation(): defines the observation space of a given agent \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8967308336571312,
        0.994273563173841,
        0.9281133163275297,
        0.9726274360573318,
        0.9961727273804327,
        0.9413984383534142,
        0.9765304745721692
      ],
      "excerpt": "| simple.py | N | N | Single agent sees landmark position, rewarded based on how close it gets to landmark. Not a multiagent environment -- used for debugging policies. | \n| simple_adversary.py (Physical deception) | N | Y | 1 adversary (red), N good agents (green), N landmarks (usually N=2). All agents observe position of landmarks and other agents. One landmark is the \u2018target landmark\u2019 (colored green). Good agents rewarded based on how close one of them is to the target landmark, but negatively rewarded if the adversary is close to target landmark. Adversary is rewarded based on how close it is to the target, but it doesn\u2019t know which landmark is the target landmark. So good agents have to learn to \u2018split up\u2019 and cover all landmarks to deceive the adversary. | \n| simple_crypto.py (Covert communication) | Y | Y | Two good agents (alice and bob), one adversary (eve). Alice must sent a private message to bob over a public channel. Alice and bob are rewarded based on how well bob reconstructs the message, but negatively rewarded if eve can reconstruct the message. Alice and bob have a private key (randomly generated at beginning of each episode), which they must learn to use to encrypt the message. | \n| simple_push.py (Keep-away) | N |Y  | 1 agent, 1 adversary, 1 landmark. Agent is rewarded based on distance to landmark. Adversary is rewarded if it is close to the landmark, and if the agent is far from the landmark. So the adversary learns to push agent away from the landmark. | \n| simple_reference.py | Y | N | 2 agents, 3 landmarks of different colors. Each agent wants to get to their target landmark, which is known only by other agent. Reward is collective. So agents have to learn to communicate the goal of the other agent, and navigate to their landmark. This is the same as the simple_speaker_listener scenario where both agents are simultaneous speakers and listeners. | \n| simple_speaker_listener.py (Cooperative communication) | Y | N | Same as simple_reference, except one agent is the \u2018speaker\u2019 (gray) that does not move (observes goal of other agent), and other agent is the listener (cannot speak, but must navigate to correct landmark).| \n| simple_spread.py (Cooperative navigation) | N | N | N agents, N landmarks. Agents are rewarded based on how far any agent is from each landmark. Agents are penalized if they collide with other agents. So, agents have to learn to cover all the landmarks while avoiding collisions. | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "It's the example of the multi-agent",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/SintolRTOS/multi-agent_Example/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Mon, 06 Dec 2021 20:10:11 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/SintolRTOS/multi-agent_Example/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "SintolRTOS/multi-agent_Example",
    "technique": "GitHub API"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/SintolRTOS/multi-agent_Example/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Multi-Agent Particle Environment",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "multi-agent_Example",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "SintolRTOS",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/SintolRTOS/multi-agent_Example/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Mon, 06 Dec 2021 20:10:11 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- To install, `cd` into the root directory and type `pip install -e .`\n\n- To interactively view moving to landmark scenario (see others in ./scenarios/):\n`bin/interactive.py --scenario simple.py`\n\n- Known dependencies: Python (3.5.4), OpenAI gym (0.10.5), numpy (1.14.5)\n\n- To use the environments, look at the code for importing them in `make_env.py`.\n\n",
      "technique": "Header extraction"
    }
  ]
}