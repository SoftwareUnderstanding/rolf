{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This dataset was created for testing out the [ULMFiT](https://arxiv.org/abs/1801.06146) (by Jeremy Howard and Sebastian Ruder) deep learning algorithm for text classification. It is implemented in the [FastAI](https://github.com/fastai/fastai) Python library that has taught me a lot. I'd also like to thank [Timo Block](https://github.com/tblock) for making his [10kGNAD](https://github.com/tblock/10kGNAD) dataset publicly available and giving me a starting point for this dataset. The dataset structure based on the [Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/) by Andrew L. Maas et al. Thanks to [Andreas van Cranenburg](https://github.com/andreasvc) for pointing out a problem with the dataset.\n\nAnd of course I'd like to thank all the reviewers on [Hebban](https://www.hebban.nl) for having taken the time to write all these reviews. You've made both book enthousiast and NLP researchers very happy :)\n\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1801.06146"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please use the following citation when making use of this dataset in your work.\n\n```\n@article{DBLP:journals/corr/abs-1910-00896,\n  author    = {Benjamin van der Burgh and\n               Suzan Verberne},\n  title     = {The merits of Universal Language Model Fine-tuning for Small Datasets\n               - a case with Dutch book reviews},\n  journal   = {CoRR},\n  volume    = {abs/1910.00896},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1910.00896},\n  archivePrefix = {arXiv},\n  eprint    = {1910.00896},\n  timestamp = {Fri, 04 Oct 2019 12:28:06 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-00896.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{DBLP:journals/corr/abs-1910-00896,\n  author    = {Benjamin van der Burgh and\n               Suzan Verberne},\n  title     = {The merits of Universal Language Model Fine-tuning for Small Datasets\n               - a case with Dutch book reviews},\n  journal   = {CoRR},\n  volume    = {abs/1910.00896},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1910.00896},\n  archivePrefix = {arXiv},\n  eprint    = {1910.00896},\n  timestamp = {Fri, 04 Oct 2019 12:28:06 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-00896.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8490817347094297
      ],
      "excerpt": "\u2514\u2500\u2500 unsup         // unbalanced positive and neutral \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/benjaminvdb/DBRD",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-02-02T10:17:31Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-13T18:01:08Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9766828825012615
      ],
      "excerpt": "The DBRD (pronounced dee-bird) dataset contains over 110k book reviews along with associated binary sentiment polarity labels. It is greatly influenced by the Large Movie Review Dataset and intended as a benchmark for sentiment classification in Dutch. The scripts that were used to scrape the reviews from Hebban can be found in the DBRD GitHub repository. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9327430522608352
      ],
      "excerpt": "The urls.txt file contains on line L the URL of the book review on Hebban for the book review with that ID, i.e., the URL of the book review in 48091_5.txt can be found on line 48091 of urls.txt. It cannot be guaranteed that these pages still exist. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8211213547181805
      ],
      "excerpt": "\u2514\u2500\u2500 urls.txt      // urls to reviews on Hebban \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9184500838871439,
        0.9622997039842721
      ],
      "excerpt": "Since scraping Hebban induces a load on their servers, it's best to download the prepared dataset instead. This also makes sure your results can be compared to those of others. The scripts and instructions should be used mostly as a starting point for building a scraper for another website. \nI'm making using of Selenium for automating user actions such as clicks. This library requires a browser driver that provides the rendering backend. I've made use of ChromeDriver. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8488774965944276
      ],
      "excerpt": "  --help            Show this message and exit. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9555447699876823
      ],
      "excerpt": "The second step is to scrape the URLs for review data. Run scrape_reviews.py to iterate over the review URLs and save the scraped data to a JSON file. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8377585688139164
      ],
      "excerpt": "Iterate over review urls in INFILE text file, scrape review data and \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8488774965944276
      ],
      "excerpt": "  --help            Show this message and exit. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9483637189057732
      ],
      "excerpt": "The third and final step is to prepare the dataset using the scraped reviews. By default, we limit the number of reviews to 110k, filter out some reviews and prepare train and test sets of 0.9 and 0.1 the total amount, respectively. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259,
        0.8488774965944276
      ],
      "excerpt": "  --shuffle TEXT               Shuffle data before saving. \n  --help                       Show this message and exit. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "110k Dutch Book Reviews Dataset for Sentiment Analysis",
      "technique": "GitHub API"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The dataset is ~79MB compressed and can be downloaded from here:\n\n**[Dutch Book Reviews Dataset](https://github.com/benjaminvdb/DBRD/releases/download/v3.0/DBRD_v3.tgz)**\n\n\nA language model trained with [FastAI](https://github.com/fastai/fastai) on Dutch Wikipedia can be downloaded from here:\n\n**[Dutch language model trained on Wikipedia](http://bit.ly/2trOhzq)**\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/benjaminvdb/110kDBRD/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3,
      "date": "Sun, 05 Dec 2021 21:49:44 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/benjaminvdb/DBRD/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "benjaminvdb/DBRD",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/benjaminvdb/110kDBRD/master/run.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9773752951054073,
        0.9858636962450815,
        0.8171325637789909,
        0.9791257006465891,
        0.9935459261120722
      ],
      "excerpt": "If you're on macOS and you have Homebrew installed, you can install ChromeDriver by running: \nbrew install chromedriver \nYou can download ChromeDriver from the official download page. \nThe scripts are written for Python 3. To install the Python dependencies, run:      \npip3 install -r ./requirements.txt \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8861843383154835
      ],
      "excerpt": "The dataset includes three folders with data: test (test split), train (train split) and unsup (remaining reviews). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8889249476777551
      ],
      "excerpt": "\u251c\u2500\u2500 test          // balanced 10% test split \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8636687469624337
      ],
      "excerpt": "\u251c\u2500\u2500 train:        // balanced 90% train split \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8174540907975313
      ],
      "excerpt": "  #:training:       20028 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9097179062581762
      ],
      "excerpt": "The first step is to gather all review URLs from Hebban. Run gather_urls.py to fetch them and save them to a text file. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9462684835635092
      ],
      "excerpt": "Usage: gather_urls.py [OPTIONS] OUTFILE \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8341682996944508
      ],
      "excerpt": "  --offset INTEGER  Review offset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8262751496746716
      ],
      "excerpt": "The second step is to scrape the URLs for review data. Run scrape_reviews.py to iterate over the review URLs and save the scraped data to a JSON file. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9501397960037816,
        0.8642534566948284,
        0.8462526564705545
      ],
      "excerpt": "Usage: scrape_reviews.py [OPTIONS] INFILE OUTFILE \nIterate over review urls in INFILE text file, scrape review data and \n  output to OUTFILE. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9329077521860516,
        0.8755697498585697
      ],
      "excerpt": "  --encoding TEXT   Output file encoding. \n  --indent INTEGER  Output JSON file with scraped data. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9462684835635109
      ],
      "excerpt": "Usage: post_process.py [OPTIONS] INFILE OUTDIR \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9084903747648088
      ],
      "excerpt": "  --encoding TEXT              Input file encoding \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8404519485311828
      ],
      "excerpt": "  --valid-size-fraction FLOAT  Fraction of total to set aside as validation. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/benjaminvdb/DBRD/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 Benjamin van der Burgh\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "DBRD: Dutch Book Reviews Dataset",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "DBRD",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "benjaminvdb",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/benjaminvdb/DBRD/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "benjaminvdb",
        "body": "Changed name of the dataset from 110kDBRD to DBRD. The dataset itself remains unchanged.",
        "dateCreated": "2020-12-07T17:00:32Z",
        "datePublished": "2020-12-07T17:30:52Z",
        "html_url": "https://github.com/benjaminvdb/DBRD/releases/tag/v3.0",
        "name": "v3.0",
        "tag_name": "v3.0",
        "tarball_url": "https://api.github.com/repos/benjaminvdb/DBRD/tarball/v3.0",
        "url": "https://api.github.com/repos/benjaminvdb/DBRD/releases/34937666",
        "zipball_url": "https://api.github.com/repos/benjaminvdb/DBRD/zipball/v3.0"
      },
      {
        "authorType": "User",
        "author_name": "benjaminvdb",
        "body": "Removed advertisements from reviews and increased dataset size to 118,516.",
        "dateCreated": "2019-06-24T14:04:53Z",
        "datePublished": "2019-06-24T15:01:32Z",
        "html_url": "https://github.com/benjaminvdb/DBRD/releases/tag/v2.0",
        "name": "",
        "tag_name": "v2.0",
        "tarball_url": "https://api.github.com/repos/benjaminvdb/DBRD/tarball/v2.0",
        "url": "https://api.github.com/repos/benjaminvdb/DBRD/releases/18184628",
        "zipball_url": "https://api.github.com/repos/benjaminvdb/DBRD/zipball/v2.0"
      },
      {
        "authorType": "User",
        "author_name": "benjaminvdb",
        "body": "Initial release",
        "dateCreated": "2019-06-24T14:04:53Z",
        "datePublished": "2019-06-24T14:55:18Z",
        "html_url": "https://github.com/benjaminvdb/DBRD/releases/tag/v1.0",
        "name": "",
        "tag_name": "v1.0",
        "tarball_url": "https://api.github.com/repos/benjaminvdb/DBRD/tarball/v1.0",
        "url": "https://api.github.com/repos/benjaminvdb/DBRD/releases/18184176",
        "zipball_url": "https://api.github.com/repos/benjaminvdb/DBRD/zipball/v1.0"
      }
    ],
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Two scripts are provided that can be run in sequence. You can also run `run.sh` to run all scripts with defaults.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 25,
      "date": "Sun, 05 Dec 2021 21:49:44 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "dataset",
      "dataset-creation",
      "nlp",
      "nlp-machine-learning",
      "python",
      "dutch",
      "python3",
      "scraper",
      "scraped-data"
    ],
    "technique": "GitHub API"
  }
}