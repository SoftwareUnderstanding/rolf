{
  "citation": [
    {
      "confidence": [
        0.9881086225049991
      ],
      "excerpt": "Title: Generative Adversarial Networks \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9941533744942845
      ],
      "excerpt": "Link: http://arxiv.org/abs/1406.2661 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9931888241155523
      ],
      "excerpt": "Year: 2014 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "Adversarial Nets \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8964261972746612
      ],
      "excerpt": "They tested on MNIST, Toronto Face Database (TFD) and CIFAR-10. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ashishmurali/Generative-Adversarial-Network",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-05-17T10:57:27Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-05-17T10:59:11Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* What\n  * GANs are based on adversarial training.\n  * Adversarial training is a basic technique to train generative models (so here primarily models that create new images).\n  * In an adversarial training one model (G, Generator) generates things (e.g. images). Another model (D, discriminator) sees real things (e.g. real images) as well as fake things (e.g. images from G) and has to learn how to differentiate the two.\n  * Neural Networks are models that can be trained in an adversarial way (and are the only models discussed here).\n\n* How\n  * G is a simple neural net (e.g. just one fully connected hidden layer). It takes a vector as input (e.g. 100 dimensions) and produces an image as output.\n  * D is a simple neural net (e.g. just one fully connected hidden layer). It takes an image as input and produces a quality rating as output (0-1, so sigmoid).\n  * You need a training set of things to be generated, e.g. images of human faces.\n  * Let the batch size be B.\n  * G is trained the following way:\n    * Create B vectors of 100 random values each, e.g. sampled uniformly from [-1, +1]. (Number of values per components depends on the chosen input size of G.)\n    * Feed forward the vectors through G to create new images.\n    * Feed forward the images through D to create ratings.\n    * Use a cross entropy loss on these ratings. All of these (fake) images should be viewed as label=0 by D. If D gives them label=1, the error will be low (G did a good job).\n    * Perform a backward pass of the errors through D (without training D). That generates gradients/errors per image and pixel.\n    * Perform a backward pass of these errors through G to train G.\n  * D is trained the following way:\n    * Create B/2 images using G (again, B/2 random vectors, feed forward through G).\n    * Chose B/2 images from the training set. Real images get label=1.\n    * Merge the fake and real images to one batch. Fake images get label=0.\n    * Feed forward the batch through D.\n    * Measure the error using cross entropy.\n    * Perform a backward pass with the error through D.\n  * Train G for one batch, then D for one (or more) batches. Sometimes D can be too slow to catch up with D, then you need more iterations of D per batch of G.\n\n* Results\n  * Good looking images MNIST-numbers and human faces. (Grayscale, rather homogeneous datasets.)\n  * Not so good looking images of CIFAR-10. (Color, rather heterogeneous datasets.)\n\n\n![Generated Faces](images/Generative_Adversarial_Networks__faces.jpg?raw=true \"Generated Faces\")\n\n*Faces generated by MLP GANs. (Rightmost column shows examples from the training set.)*\n\n-------------------------\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8522034581809397,
        0.9464561502379915,
        0.8631935683272658
      ],
      "excerpt": "The generator learns to create content (e.g. images), the discriminator learns to differentiate between real content and generated content. \nAnalogy: Generator produces counterfeit art, discriminator's job is to judge whether a piece of art is a counterfeit. \nThis principle could be used with many techniques, but they use neural nets (MLPs) for both the generator as well as the discriminator. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8357401167958124
      ],
      "excerpt": "G takes a random vector as input (e.g. vector of 100 random values between -1 and +1). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8467048237225155
      ],
      "excerpt": "D can also be trained multiple times in a row. That allows it to catch up with G. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.988861345199946,
        0.9553281891324058,
        0.8902022840425877
      ],
      "excerpt": "It is proofable that there is only one global optimum for GANs, which is reached when G perfectly replicates the training set probability distribution. (Assuming unlimited capacity of the models and unlimited training time.) \nIt is proofable that G and D will converge to the global optimum, so long as D gets enough steps per training iteration to model the distribution generated by G. (Again, assuming unlimited capacity/time.) \nNote that these things are proofed for the general principle for GANs. Implementing GANs with neural nets can then introduce problems typical for neural nets (e.g. getting stuck in saddle points). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.827929378797664,
        0.908925214220865
      ],
      "excerpt": "They used MLPs for G and D. \nG contained ReLUs and Sigmoids. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.865272864578902
      ],
      "excerpt": "They note that KDE is not really a great technique for such high dimensional spaces, but its the only one known. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.967005895896403,
        0.908925214220865
      ],
      "excerpt": "Their KDE score for MNIST and TFD is competitive or better than other approaches. \nAdvantages and Disadvantages \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8691888848122002
      ],
      "excerpt": "Wide variety of functions can be incorporated into the model (?) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9040503674723963
      ],
      "excerpt": "Can represent a wide variety of distributions, including sharp ones (Markov chains only work with blurry images). \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ashishmurali/Generative-Adversarial-Network/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 09 Dec 2021 12:00:33 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ashishmurali/Generative-Adversarial-Network/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "ashishmurali/Generative-Adversarial-Network",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8025675783265361
      ],
      "excerpt": "Both are trained simultaneously, i.e. one batch for G, then one batch for D, then one batch for G... \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8016697906207164
      ],
      "excerpt": "D and G must be well synchronized during training \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ashishmurali/Generative-Adversarial-Network/issues{/number}",
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Generative-Adversarial-Network",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Generative-Adversarial-Network",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "ashishmurali",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ashishmurali/Generative-Adversarial-Network/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 09 Dec 2021 12:00:33 GMT"
    },
    "technique": "GitHub API"
  }
}