{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1712.00559\n[2]:https://arxiv.org/abs/1602.02830\n[3]:https://github.com/yashkant/PNAS-Binarized-Neural-Networks/blob/master/binarized/binary_ops.py\n[4]:https://www.tensorflow.org/install/\n[5]:https://keras.io/#installation\n[6]:https://github.com/yashkant/PNAS-Binarized-Neural-Networks/blob/master/pnas/model.py\n[7]:https://github.com/yashkant/PNAS-Binarized-Neural-Networks/blob/master/train.py\n\n\nThanks to \n---------\n\nThis work wouldn't have been possible without the help from the following repos:\n\n1. https://github.com/titu1994/progressive-neural-architecture-search\n2. https://github.com/DingKe/nn_playground/",
      "https://arxiv.org/abs/1602.02830\n[3]:https://github.com/yashkant/PNAS-Binarized-Neural-Networks/blob/master/binarized/binary_ops.py\n[4]:https://www.tensorflow.org/install/\n[5]:https://keras.io/#installation\n[6]:https://github.com/yashkant/PNAS-Binarized-Neural-Networks/blob/master/pnas/model.py\n[7]:https://github.com/yashkant/PNAS-Binarized-Neural-Networks/blob/master/train.py\n\n\nThanks to \n---------\n\nThis work wouldn't have been possible without the help from the following repos:\n\n1. https://github.com/titu1994/progressive-neural-architecture-search\n2. https://github.com/DingKe/nn_playground/",
      "https://arxiv.org/abs/1712.00559"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{hubara2016binarized,\n  title={Binarized neural networks},\n  author={Hubara, Itay and Courbariaux, Matthieu and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},\n  booktitle={Advances in neural information processing systems},\n  pages={4107--4115},\n  year={2016}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{liu2017progressive,\n  title={Progressive neural architecture search},\n  author={Liu, Chenxi and Zoph, Barret and Shlens, Jonathon and Hua, Wei and Li, Li-Jia and Fei-Fei, Li and Yuille, Alan and Huang, Jonathan and Murphy, Kevin},\n  journal={arXiv preprint arXiv:1712.00559},\n  year={2017}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9992658107293803,
        0.9859379936648053,
        0.9975788273939303,
        0.988536099279406,
        0.8944178096468923,
        0.9954488832581693
      ],
      "excerpt": "If you find this code useful, please consider citing the original work by the authors: \n  title={Binarized neural networks}, \n  author={Hubara, Itay and Courbariaux, Matthieu and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua}, \n  booktitle={Advances in neural information processing systems}, \n  pages={4107--4115}, \n  year={2016} \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/yashkant/PNAS-Binarized-Neural-Networks",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-09-07T08:21:43Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-06-09T11:54:05Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9303258048928685,
        0.9872024051077801,
        0.9508618035413643
      ],
      "excerpt": "This project combines the architecture search strategy from [Progressive Neural Architecture Search][1] with the search space of [Binarized Neural Networks][2].  \nNeural Architecture Search is a sub-field of AutoML which has recently gained popularity for generating state-of-the-art architectures on various tasks of Image Processing and Natural Language Processing.  \nProgressive Neural Architecture Search searches through the space in a sequential fashion starting with simplest models and increasing the complexity as it proceeds. It learns a surrogate reward predictor implemented as a RNN to reduce the overhead of training every proposed architecture.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9343074585431008,
        0.8933910800927701
      ],
      "excerpt": "Binarized Neural Networks with binary weights and activations at run-time drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations which substantially improve power-efficiency. Both the weights and the activations are constrained to either +1 or -1.  \nBinarization function used in the experiment is deterministic binary-tanh which is placed in [binary_ops.py][3].  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.853134726109167,
        0.9901254270621448
      ],
      "excerpt": "Project Structure \nThe skeletal overview of the project is as follows:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8110291785265838
      ],
      "excerpt": "\u251c\u2500\u2500 architectures/        #: Stores the architectures evaluated and their corresponding rewards \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8934846032014275
      ],
      "excerpt": "B = 3   #: Maximum number of block in the cell \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9047555159717714,
        0.9510152367827914
      ],
      "excerpt": "DROP_HIDDEN = 0.5  #: Dropout parameter for the hidden dense layers \nDROPOUT= (False, DROP_INPUT, DROP_HIDDEN) #: Dropout only applied to the dense layers and the input \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8876330234231155
      ],
      "excerpt": "              '3x3 bconv']  #: Defines set of possible operations in the search space \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8580451586836978
      ],
      "excerpt": "The architecture with highest reward needs to be trained till convergence, follow the steps below for it. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9693233393948762
      ],
      "excerpt": "Thanks to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Progressive Neural Architecture Search coupled with Binarized CNNs to search for resource efficient and accurate architectures.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/yashkant/PNAS-Binarized-Neural-Networks/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sun, 05 Dec 2021 02:47:53 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/yashkant/PNAS-Binarized-Neural-Networks/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "yashkant/PNAS-Binarized-Neural-Networks",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.9461387272693286
      ],
      "excerpt": "Setup Dependencies \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9824891837724392,
        0.9771149093065467,
        0.9333063750587288,
        0.833114516308531
      ],
      "excerpt": "Follow the installation guide on [Tensorflow Homepage][4] for installing Tensorflow-GPU or Tensorflow-CPU.  \nFollow instructions outlined on [Keras Homepage][5] for installing Keras. \nRun a vanilla experiment using the following command at the directory root folder.  \nbash  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.833114516308531
      ],
      "excerpt": "bash  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8921493482303118,
        0.833114516308531
      ],
      "excerpt": "Use the following command to run the experiment finally.  \nbash  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8012528779582535
      ],
      "excerpt": "After replacing theREPRESENTATION_STRING``` run the following command: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9503189345333785
      ],
      "excerpt": "python train.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8561358885313097
      ],
      "excerpt": "\u251c\u2500\u2500 train.py              #: Defines the experiment settings \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8286572695472
      ],
      "excerpt": ": -------Controller Training Settings------- \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8252232618938141,
        0.8534623307507299,
        0.8724865001215014
      ],
      "excerpt": "CONTROLLER_CELLS = 100  #: Number of cells in RNN controller \nRNN_TRAINING_EPOCHS = 15 #: Number of training epochs during each run of the encoder training \nRESTORE_CONTROLLER = True  #: Restore a pre-trained controller from earlier run  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8166534998056648
      ],
      "excerpt": "USE_EXPANSION = False #: If true uses expanded MNIST with data augmentation and rotation  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9503189345333785
      ],
      "excerpt": "python train.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9649911558180978
      ],
      "excerpt": "python train.py -ta True \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/yashkant/PNAS-Binarized-Neural-Networks/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Progressive Neural Architecture Search with Binarized Neural Networks",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "PNAS-Binarized-Neural-Networks",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "yashkant",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/yashkant/PNAS-Binarized-Neural-Networks/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Sun, 05 Dec 2021 02:47:53 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "binarized-neural-networks",
      "neural-architecture-search",
      "tensorflow-experiments",
      "keras",
      "automl"
    ],
    "technique": "GitHub API"
  }
}