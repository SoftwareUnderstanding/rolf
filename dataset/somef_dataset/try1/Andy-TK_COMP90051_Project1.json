{
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/YEY11/COMP90051_Project1",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-09-18T12:17:24Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-06-01T15:47:02Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9928489340353567,
        0.970990337897935
      ],
      "excerpt": "This is the Project 1 for COMP90051 (Statistical Machine Learning) from the University of Melbourne. \nAuthorship attribution is a common task in Natural Language Processing (NLP) applications, such as academic plagiarism detection and potential terrorist suspects identification on social media. As for the traditional author classification task, the training dataset usually includes the entire corpus of the author\u2019s published work, which contains a large number of examples of standard sentences that might reflect the writing style of the author. However, when it comes to the limited text on social media like Twitter, it brings some challenging problems, such as informal expressions, a huge number of labels, unbalanced dataset and extremely limited information related to identity. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9975966310379624,
        0.863203628819834
      ],
      "excerpt": "In this project, the task is to predict authors of test tweets from among a very large number of authors found in training tweets, which comes from an in-class Kaggle Competition. Our works include data preprocessing, feature engineering, model selection and ensemble models etc. For more details, please check the project specifications and project report. \nThe Data folder contains both original data and processed data. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8301718919256255
      ],
      "excerpt": "The original training dataset which contains 328932 tweets posted by 9297 users. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8518170278942312
      ],
      "excerpt": "The preprocess.py in the Code folder transfered the original data into processed data. For example: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9966314524582487
      ],
      "excerpt": "is used for data preprocessing including removing non-English characters (e.g. emoticons and punctuations) and stopwords, as well as word tokenization and lemmatization based on nltk package. Also, it provides some distribution plots for data based on matplotlib package. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9929365768500602,
        0.9877556954854363
      ],
      "excerpt": "Before entering data into the models\uff0cusing TF-IDF to transfer clean tweets text into a vector or matrix. This process is implemented by CountVectorizer and TfidfTransformer modules from scikit-learn package. \nFive machine learning/deep learning models based on scikit-learn and Keras are implemented in this part, including the Multinomial Naive Bayes, KNN, Multiple Logistic Regression, Linear SVC and LSTM. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.99582866464585,
        0.9021595064230692
      ],
      "excerpt": "Ensemble learning is a powerful technique to increase accuracy on a most of machine learning tasks. In this project, we try a simple ensemble approach called weighted voting to avoid overfitting and improve performance. The basic thought of this method is quite simple. For each prediction from the results of different models, we give them a weight corresponding to their individual accuracy in the previous stage. If the predicted labels of two models are the same, we just add their weight together. Then we select the prediction with highest weight as the final prediction. \nConsidering the individual performance of the previous models, we try three different combinations:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "This is the Project 1 for COMP90051 (Statistical Machine Learning) from the University of Melbourne.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Andy-TK/COMP90051_Project1/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 09 Dec 2021 01:24:04 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/YEY11/COMP90051_Project1/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "YEY11/COMP90051_Project1",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Andy-TK/COMP90051_Project1/master/PreProcess.ipynb",
      "https://raw.githubusercontent.com/Andy-TK/COMP90051_Project1/master/NaiveBayers.ipynb"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.8451112722228072
      ],
      "excerpt": "The preprocess.py in the Code folder transfered the original data into processed data. For example: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8192765396158481,
        0.8395583997764666
      ],
      "excerpt": "The random 9/10 processed training dataset used for partial training dataset. \nThe random 1/10 processed training dataset used for partial test dataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8068795640171285,
        0.91892912920148
      ],
      "excerpt": "nb.py - Multinomial Naive Bayes Model. \nknn1.py - KNN Model. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/YEY11/COMP90051_Project1/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "## COMP90051 Project 1: Authorship Attribution with Limited Text on Twitter",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "COMP90051_Project1",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "YEY11",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/YEY11/COMP90051_Project1/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Thu, 09 Dec 2021 01:24:04 GMT"
    },
    "technique": "GitHub API"
  }
}