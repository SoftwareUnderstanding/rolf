{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1707.06347\n\nPlease note the original paper described the PPO with Gaussain distribution but Beta is commonly accepted as a superior for RL learning. \n\nThe environment used to present the algorithm is Multi agent Tennis from Unity-ML\nYou don\u2019t have to build the environment yourself the prebuilt one included in the project will work fine - please note it\u2019s only compatible with Unity-ML 0.4.0b NOT the current newest version. I don\u2019t have access to the source of the environment as it was prebuilt by Udacity. \n\n## Environment details:\nA reward of -0.01 is returned when ball touches the ground, and a reward of 0.1 if the agent hits the ball over the net. Thus, the goal of the agents is to keep the ball in play for as long as possible.\nEach racket is controlled by it's own agent - because the observations of the agents overlap I decided to use a single agent in code but it consumes stacked observations from both and outputs actions for both.  \n\nThe state space has 8 dimensions:\n* Vector Observation space: 8 variables corresponding to the ball and agent positions\n* Vector Action space: (Continuous"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9554441738822752
      ],
      "excerpt": "* Visual Observations: None. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jsztompka/MultiAgent-PPO",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-01-09T17:57:08Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-21T08:40:43Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9251392629608739,
        0.9877752832561406
      ],
      "excerpt": "Repository uses Unity-ML Tennis as environment for Proximal Policy Optimization agent  \nThis project is my sample implementation of Proximal Policy Optimization with Beta distribution algorithm described in detail: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.975322536136832,
        0.8054042935659782,
        0.9047055451320629,
        0.9944699631372712,
        0.9545081881275556
      ],
      "excerpt": "Please note the original paper described the PPO with Gaussain distribution but Beta is commonly accepted as a superior for RL learning.  \nThe environment used to present the algorithm is Multi agent Tennis from Unity-ML \nYou don\u2019t have to build the environment yourself the prebuilt one included in the project will work fine - please note it\u2019s only compatible with Unity-ML 0.4.0b NOT the current newest version. I don\u2019t have access to the source of the environment as it was prebuilt by Udacity. \nA reward of -0.01 is returned when ball touches the ground, and a reward of 0.1 if the agent hits the ball over the net. Thus, the goal of the agents is to keep the ball in play for as long as possible. \nEach racket is controlled by it's own agent - because the observations of the agents overlap I decided to use a single agent in code but it consumes stacked observations from both and outputs actions for both.   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.967567141643019,
        0.906586905398107
      ],
      "excerpt": "* Vector Observation space: 8 variables corresponding to the ball and agent positions \n* Vector Action space: (Continuous) Size of 2, corresponding to agent movement (forward / back / up / down). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Proximal Policy Optimization with Beta distribution - uses multi agent Unity ML Tennis",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jsztompka/MultiAgent-PPO/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Fri, 10 Dec 2021 21:06:25 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jsztompka/MultiAgent-PPO/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "jsztompka/MultiAgent-PPO",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please run pip install . in order to ensure you got all dependencies needed\n\nTo start up the project:\npython -m train.py \n\nAll hyper-paramters are in: \nconfig.py \n\nThe config includes PLAY_ONLY argument which decides whether to start Agent with pre-trained weights or spend a few hours and train it from scratch :) \n\nMore details on the project can be found in:  \n[Report](/Report.md)\n\n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9198707510100148
      ],
      "excerpt": "You don\u2019t have to build the environment yourself the prebuilt one included in the project will work fine - please note it\u2019s only compatible with Unity-ML 0.4.0b NOT the current newest version. I don\u2019t have access to the source of the environment as it was prebuilt by Udacity. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jsztompka/MultiAgent-PPO/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "ASP",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 Jedrzej Sztompka\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "PPO-demo",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "MultiAgent-PPO",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "jsztompka",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jsztompka/MultiAgent-PPO/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 16,
      "date": "Fri, 10 Dec 2021 21:06:25 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "reinforcement-learning",
      "ppo",
      "python3",
      "unity-ml-agents"
    ],
    "technique": "GitHub API"
  }
}