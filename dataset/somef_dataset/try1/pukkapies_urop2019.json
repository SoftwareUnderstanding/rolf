{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1803.09820](https://arxiv.org/pdf/1803.09820.pdf) (2018)\n\n## Contacts / Getting Help\n\ncalle.sonne18@imperial.ac.uk\n\nchon.ho17@imperial.ac.uk\n\ndavide.gallo18@imperial.ac.uk / davide.gallo@pm.me\n\nkevin.webster@imperial.ac.uk"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Pons, J., Nieto, O., Prockup, M., Schmidt, E., Ehmann, A., Serra, X.: END-TO-END LEARNING FOR MUSIC AUDIO TAGGING AT SCALE. Proc. of the 19th International Society for Music Information Retrieval Conference (ISMIR). Paris, France (2018)\n\nSmith, L. N.: A DISCIPLINED APPROACH TO NEURAL-NETWORK HYPER-PARAMETERS: PART 1 \u2013 LEARNING RATE, BATCH SIZE, MOMENTUM, AND WEIGHT DECAY. arXiv preprint [arXiv:1803.09820](https://arxiv.org/pdf/1803.09820.pdf) (2018)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "top_tags = lf.popularity()['tags'][:10].tolist() \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9911959320294913,
        0.9661397119947687
      ],
      "excerpt": "| Waveform (Pons, et al., 2018)              | 87.41   | 28.53   | \n| Log-mel-spectrogram (Pons, et al., 2018)   | 88.75   | 31.24   | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9661397119947687
      ],
      "excerpt": "| Log-mel-spectrogram (Pons, et al., 2018)   | 88.75   | 31.24   | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/pukkapies/urop2019",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "calle.sonne18@imperial.ac.uk\n\nchon.ho17@imperial.ac.uk\n\ndavide.gallo18@imperial.ac.uk / davide.gallo@pm.me\n\nkevin.webster@imperial.ac.uk\n",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-06-22T14:54:59Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-07-08T17:20:14Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This project makes use of the freely-available [Million Song Dataset](http://millionsongdataset.com/), and its integration with the [Last.fm Dataset](http://millionsongdataset.com/lastfm/). The former provides a link between all the useful information about the tracks (such as title, artist or year) and the audio track themselves, whereas the latter contains tags information on some of the tracks. A preview of the audio tracks can be fetched from services such as 7Digital, but this is allegedly not an easy task. \n\nIf you are only interested in our final results, click [here](https://github.com/pukkapies/urop2019#results).\n\nIf you want to use some of our code, or try to re-train our model on your own, read on. We will assume you have access to the actual songs in the dataset. Here is the outline of the approach we followed:\n\n1. Extracte all the useful information from the Million Song Dataset and clean both the audio tracks and the Last.fm tags database to produce our final 'clean' data;\n\n2. Prepare a flexible data input pipeline and transform the data in a format which is easy to consume by the training algorithm;\n\n3. Prepare a flexible training script which would allow for multiple experiments (such as slightly different architectures, slightly different versions of the tags database, or different training parameters);\n\n4. Train our model and use it to make sensible tag predictions from a given input audio.\n\nIn the following sections, we will provide a brief tutorial of how you may use this repository to make genre predictions of your own, or carry out some further experiments.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9957371070768758,
        0.9795518041384732
      ],
      "excerpt": "This is the repository of an Imperial College UROP 2019 project in deep learning for music tagging. We aimed to develop an end-to-end music auto-tagger competitive with the state-of-the-art. We replicated the convolutional neural network architecture proposed by (Pons, et al., 2018) in this paper, and reproduced the results they obtained on the Million Song Dataset.  \nSince our model learned to predict some audio features quite accurately, we decided to call it \"Orpheus\", like the legendary ancient Greek poet and musician. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "Data Cleaning \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.881909667137859
      ],
      "excerpt": "Model and JSON Configuration \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9942942636645477
      ],
      "excerpt": "The lastfm.py module contains three classes, LastFm, LastFm2Pandas and Matrix. The first two classes contain all the basic tools for querying the Lastfm database. The former directly queries the database by SQL, whereas the latter converts the database into dataframes and performs queries using Pandas. In some of the functions in later sections, an input parameter may require you to pass an instance of one of these two classes. The last one, on the other hand, contains some advanced tools to perform a thorough analysis of the tags distribution in the tags database. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "fm = lastfm.LastFm('/srv/data/msd/lastfm/SQLITE/lastfm_tags.db') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "fm = lastfm.LastFm2Pandas('/srv/data/msd/lastfm/SQLITE/lastfm_tags.db') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9433770338922444,
        0.8754578999286987,
        0.9960307639332274,
        0.9790473553321571,
        0.9791742739956023,
        0.9442465430818099
      ],
      "excerpt": "The major difference between the two classes is that LastFm is quicker to initiate, but some queries might take some time to perform, whereas LastFm2Pandas may take longer to initiate (due to the whole dataset being loaded into the memory). However, LastFm2Pandas contains some more advanced methods, and it is reasonably quick to initialise if database is converted into .csv files in advance. Moreover, you will need to use LastFm2Pandas if you want to perform advanced an advanced analysis of the tags distribution using the Matrix class. \nFinally, metadata.py contains some basic tools to explore the msd_summary_file.h5 file. \nIn the L<span>ast.f</span>m database there are more than 500,000 different tags. Such a high number of tags is clearly useless, and the tags need to be cleaned in order for the training algorithm to learn to make some sensible predictions. The tags are cleaned using lastfm_cleaning_utils.py and lastfm_cleaning.py, and the exact mechanisms of how they work can be found in the documentation of the scripts. \nIn brief, the tags are divided into two categories: genre tags, and vocal tags (which in our case are 'male vocalist', 'female vocalist', 'rap' and 'instrumental'). \nFor genre tags, we first obtained a list of tags from the L<span>ast.f</span>m database which have appeared for more than 2000 times, then we manually filtered out the tags that we considered rubbish (such as 'cool' or 'favourite song'). We fed the remaining 'good' tags into generate_genre_df(), which searched for similar spelling of the same tag within a 500,000+ tags pool (tags with occurrence \u2265 10), and we produced a handy dataframe with manually chosen tags in one column, and similar matching tags from the pool in the other. \nFor vocal tags, we first obtained a long list of potentially matching tags for each of the four vocal tags, then we manually separated the 'real' matching tags from the rest, for each of the tag lists. We fed the tag lists into generate_vocal_df(), and we produced a dataframe with the structure previously outlined. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9140981333616062,
        0.8161808846974069,
        0.9691133085226253,
        0.8954049422455939,
        0.8862015269478803,
        0.9366914388586577
      ],
      "excerpt": "To search for similar tags, we did the following: \nRemove all the non-alphabet and non-number characters and any single trailing 's' from the raw tags with occurance \u2265 10 and the target tags (the classified genre tags and vocal tags). If any transformed raw tag is identical to any target tag, the raw tag is merged into target tag; \nRepeat the same merging mechanism as 1, but replace '&' with 'n', '&' with 'and' and 'n' with 'and'; \nRepeat the same merging mechanism as 1, but replace 'x0s' with '19x0s' and 'x0s' with '20x0' (without removing the trailing 's'; x denotes a number character here). \nSee here for more details on how you may tailor the merging mechanism by defining a new filtering function. \nIf you want to actually see the dataframe which contains all the clean tag info (which will then be used by lastfm_cleaning.py to produce the new .db file), you can generate it using the generate_final_df() functions, which combines all the tools mentioned above, and which allows a lot of room for customization and fine-tuning. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.832927172792452,
        0.888721401077035,
        0.920130041755245
      ],
      "excerpt": "Finally, the .txt files containing the lists of tags we used in our experiment can be found in this folder. \nTo store the necessary information we needed for training, we used the TFRecords format. The preprocessing.py script does exactly this. In each entry of the .tfrecord file, it stores the audio as an array in either waveform or log-mel-spectrogram format. It will also store the track ID to identify each track, and the tags from the clean tags database in a one-hot vector format. It will accept audio as either .mp3 files, or as .npz files where each entry contains the audio as an array and the sample rate. The user can choose the sample rate to store the data in as well as the number of mel bins (when storing the audios as log-mel-spectrogram). The user can also specify the number of  .tfrecord files to split the data between. \nIn our case, we used 96 mel bins, a sample rate of 16kHz and split the data into 100 .tfrecord files. We also had the data stored as .npz files, since we had previously converted the .mp3 files into NumPy format for silence analysis. We would once again recommend users to convert directly from .mp3 files, as the .npz files need a lot of storage.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8992634432169456,
        0.900614324717613,
        0.986863447418187
      ],
      "excerpt": "It is recommended to use tmux split screens to speed up the process. \nThe projectname_input.py module was used to create ready-to-use TensorFlow datasets from the .tfrecord files. Its main feature is to create three datasets for train/validating/testing by parsing the .tfrecord files and extracting a 15 sec window from the audio, then normalizing the data. If waveform is used, we normalized the batch, but if log mel-spectrogram is used, we normalized with respect to the spectrograms themselves (Pons, et al., 2018). The module will also create mini-batches of a chosen size. \nThe projectname_input.py module again leaves a lot of room for customisation. There are functions to exclude certain track IDs from the dataset, to merge certain tags (e.g. 'rap' and 'hip hop'), or to only include some tags. The user can also choose the size of the audio window and whether the window is to be extracted at random, or centred on the audio array.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "lf = lastfm.LastFm('/srv/data/urop/clean_lastfm.db') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9443023922994759,
        0.9841981074799072,
        0.9310061088912235,
        0.8941417438631594,
        0.9807465587847334,
        0.8307813746456064,
        0.8938695832730252,
        0.9241391478356565,
        0.8973516610634953
      ],
      "excerpt": "Finally, this data input pipeline is optimised following the official guidelines for TensorFlow 2.0. \nThe model we used was designed by (Pons, et al., 2018). See their GitHub repository for more details. In our experiment, as mentioned above, we have followed their approach and compared the performance when training with waveform or log mel-spectrogram audio format. Since the model they provide is written using TensorFlow 1.x syntax, we have rewritten the same model using TensorFlow 2.0. You can find the 'upgraded' model in projectname.py. \nIn brief, projectname.py contains two frontend architectures (one for waveform and one for log mel-spectrogram) and a single backend architecture, with a build_model() function which combines the two to produce the complete model that will be used for training. \nIn order to avoid having to manually tinker with the training code every time a training parameter has to be changed, all the training parameters are set through a handy JSON file. You can create an empty config.json file by using the create_config_json() function. Here is an outline of how the JSON file is structured: \nmodel: contains parameters to set the number of dense units and convolutional filters in the model; \nmodel-training: contains important training parameters such as batch size or window length and allow you to fully specify the optimizer to use; \ntags: contains parameters to specify which tags to use when parsing the TFRecords; \ntfrecords: contains parameters to specify how the audio tracks were encoded in the TFRecords such as sample rate or the number of frequency bands in the mel scale. \nSee the inline comments for the create_config_json() function within projectname.py for more details.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8634492062340966
      ],
      "excerpt": ": to create an empty .json and manually enter some parameters (equivalent to editing the file after creation) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9717218969180523,
        0.882968402609794,
        0.9004006025124215
      ],
      "excerpt": "and 'training_custom.py. The main difference between the two is that the former makes use of the built-in Keras model.fit, whereas the latter makes use of a custom training loop (as described in the official guidelines for TensorFlow 2.0) where each training step is performed manually. While training.py only allows the introduction of advanced training features through Keras callbacks, training_custom.py allows total flexibility in the features you could introduce.  \nBoth scripts assume you have one or more GPUs available, and make use of a MirroredStrategy to distribute training. Both scripts write (train and validation) summaries on TensorBoard and save checkpoints at the end of each epoch, and they also have the option to enable early stopping or learning rate reduction on plateau. Only the custom loop implements cyclical learning rate and the one-cycle policy, as described by (N. Smith, 2018) in this paper. \nFor ease of use, projectname_train.py is wrapper of the two scripts. By default, the custom loop is selected, unless a different choice is specified. You may control all the training parameters by tweaking the config.json file. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9805864379876168,
        0.8232906798980543
      ],
      "excerpt": "Furthermore, it is possible to stop the scripts in the middle of training by keyboard interrupt and recover from a saved checkpoint using the --resume-time parameter. \nThe projectname_train.py script makes use of projectname_input.py to generate training and validation datasets. If you want to perform the model training with more flexibility in choosing your own datasets, you may generate your own datasets using the tf.data API and then do the following: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9226724495496211
      ],
      "excerpt": "The evaluation tools are contained in the script projectname.py. There is a test() function which simply tests the model's performance on the test dataset from a certain checkpoint. There is also a predict() function which takes an audio array (in waveform or log mel-spectrogram format) and uses the model to return the most confident tag predicitons for that track. Optionally, the audio array might be sliced in n_slices sliding windows of length window_length, and the final tag predictions will average out the tag predictions for each single slice. In either case, you will need to pass a threshold to determine which tags are shown, based on their prediction confidence. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8451448127246369
      ],
      "excerpt": "To use the same model to predict tags with threshold 0.1 for a single audio track (or multiple tracks in the same folder): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9102201909593935
      ],
      "excerpt": "To use the same model to predict tags with threshold 0.1 for a 30 sec recording: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9575751444334502
      ],
      "excerpt": "Here are our results when performing different experiments using both waveform and log-mel-spectrogram. We always trained on the top 50 tags from our clean L<span>ast.f</span>m database.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.95074527391284
      ],
      "excerpt": "This experiment was used to try to replicate the results by (Pons, et al., 2018), and compare the performance obtained on our dataset using waveform and log-mel-spectrogram. We ran this experiments using a constant learning rate of 0.001. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9228427502515466
      ],
      "excerpt": "This experiment was used to test the effectiveness of cyclic learning rate (Smith, 2018) as well as an attempt to try and improve the model. We ran this experiment on an identical run of the log-mel-spectrogram above, using cyclic learning rate varying linearly between 0.0014/4 instead of a constant learning rate of 0.001. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Summer UROP 2019 project repository.",
      "technique": "GitHub API"
    }
  ],
  "documentation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "https://mutagen.readthedocs.io/",
      "technique": "Regular expression"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/pukkapies/urop2019/releases",
    "technique": "GitHub API"
  },
  "faq": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You can use `fetcher.py` to scan the directory which contains your audio files and store their file path, file size, duration and number of channels in a Pandas dataframe. As it turns out, some files cannot be opened, and some others *can* be opened but are completely (or mostly) silent. To tackle the first issue, `fetcher.py` can automatically purge the faulty tracks from the final output (most of them have either zero or extremely small file size). To tackle the second issue, we made use of LibROSA and its `librosa.effects.split()` function, which splits an audio signal into non-silent intervals.\n\nIn order to do so, LibROSA first needs to convert an audio file into a waveform array, stored in NumPy format. The information on audio silence will then be processed by `wrangler_silence.py`, which will remove from the original Pandas dataframe all the tracks which do not satisfy certain user-set criteria. You have two options here: either you use `mp3_to_numpy.py` to create in advance `.npz` files for your entire dataset, or you generate and process the audio arrays on the fly when using `wrangler_silence.py`. \n\nWe originally created `.npz` files for the entire dataset, as we did not have the whole audio cleaning path outlined yet, and we needed to have the data easily accessible for frequent experimentations. We would however recommend against this approach, as creating `.npz` files often requires a huge amounts of storage space.\n\nYou will then have a Pandas dataframe (saved into a `.csv` file) containing only the good audio files.\n\n*Example:*\n\n```bash\n#: save a .csv file containing audio tracks and tracks info\npython fetch.py --root-dir /srv/data/msd/7digital /path/to/output/fetcher.csv\n```\n```bash\n#: convert audio tracks into .npz\npython mp3_to_numpy.py /path/to/output/fetcher.csv --root-dir-npz /srv/data/urop2019/npz --root-dir-mp3 /srv/data/msd/7digital\n```\n```bash\n#: save a .csv file containing audio silence info, optionally also discard 'silent' tracks\npython wrangler_silence.py /path/to/output/fetcher.csv /path/to/output/wrangler_silence.csv --root-dir-npz /srv/data/urop2019/npz --root-dir-mp3 /srv/data/msd/7digital --min-size 100000 --filter-tot-silence 15 --filter-max-silence 3\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "The raw HDF5 Million Song Dataset file, which contains three smaller datasets, are converted into multiple Pandas dataframes. The relevant information is then extracted and merged. According to the MSD website, there are [mismatches](http://millionsongdataset.com/blog/12-2-12-fixing-matching-errors/) between these datasets. To deal with the issue, `wrangler.py` takes a `.txt` file with a list of tids which could not be trusted, and remove the corresponding rows in the dataframe. Furthermore, MSD also provides a `.txt` file with a list of tracks which have [duplicates](http://millionsongdataset.com/blog/11-3-15-921810-song-dataset-duplicates/). To deal with this other issue, `wrangler.py` by default only keeps one version of each duplicate (picked randomly), and removes the rest.\n\nThe dataframe from the above paragraph is merged with the dataframe produced by the above audio section followed by removing unnecessary columns to produce the 'ultimate' dataframe, which contains essential information about the tracks that will be used throughout the project.\n\n*Example:*\n\n```\npython wrangler.py /path/to/output/fetcher.csv /path/to/ultimate.csv --path-h5 /srv/data/msd/entp/msd_summary_file.h5 --path-db /srv/data/msd/lastfm/lastfm_tags.db --path-txt-dupl /path/to/duplicates.txt --path-txt-mism /path/to/mismatches.txt\n```\n\nIn order to save storage space and time, a different order of code execution was instead used though.\n\n*Example:*\n\n```\npython fetcher.py --root-dir /srv/data/msd/7digital /path/to/output/fetcher.csv\n```\n```\npython wrangler.py /path/to/output/fetcher.csv /path/to/output/wrangler.csv --path-h5 /srv/data/msd/entp/msd_summary_file.h5 --path-db /srv/data/msd/lastfm/lastfm_tags.db --path-txt-dupl /path/to/duplicates.txt --path-txt-mism /path/to/mismatches.txt --discard-dupl --discard-no-tag\n```\n```\npython mp3_to_numpy.py /path/to/output/wrangler.csv --root-dir-npz /output/dir/npz --root-dir-mp3 /srv/data/msd/7digital\n```\n```\npython wrangler_silence.py /path/to/output/wrangler.csv /path/to/output/wrangler_silence.csv --root-dir-npz /output/dir/npz/ --root-dir-mp3 /srv/data/msd/7digital/\n```\n```\npython wrangler_silence.py /path/to/output/wrangler_silence.csv /path/to/ultimate.csv --min-size 200000 --filter-trim-length 15 --filter-tot-silence 3 --filter-max-silence 1\n```\n\nThis reduces the number of useful tracks from 1,000,000 to ~300,000.\n\n",
      "technique": "Header extraction"
    }
  ],
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Sun, 05 Dec 2021 19:22:47 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/pukkapies/urop2019/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "pukkapies/urop2019",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/pukkapies/urop2019/master/orpheus-code/preprocessing-tmux.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9272102707687195
      ],
      "excerpt": "System Requirements \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8517199685332766
      ],
      "excerpt": "TFRecords into a tf.data.Dataset \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8361090768493878
      ],
      "excerpt": ": generate .csv's \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8525447642357499,
        0.8525447642357499,
        0.8525447642357499
      ],
      "excerpt": "tags = pd.read_csv('/srv/data/urop/lastfm_tags.csv') \ntids = pd.read_csv('/srv/data/urop/lastfm_tids.csv') \ntid_tag = pd.read_csv('/srv/data/urop/lastfm_tid_tag.csv') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "from lastfm_cleaning_utils import generate_final_df \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.922738784878987
      ],
      "excerpt": "python lastfm_cleaning.py /srv/data/msd/lastfm/SQLITE/lastfm_tags.db /srv/data/urop/clean_lastfm.db \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8019479270790463
      ],
      "excerpt": "To store the necessary information we needed for training, we used the TFRecords format. The preprocessing.py script does exactly this. In each entry of the .tfrecord file, it stores the audio as an array in either waveform or log-mel-spectrogram format. It will also store the track ID to identify each track, and the tags from the clean tags database in a one-hot vector format. It will accept audio as either .mp3 files, or as .npz files where each entry contains the audio as an array and the sample rate. The user can choose the sample rate to store the data in as well as the number of mel bins (when storing the audios as log-mel-spectrogram). The user can also specify the number of  .tfrecord files to split the data between. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9369586042580907
      ],
      "excerpt": "python preprocessing.py waveform /output/dir/ --root-dir /srv/data/npz --tag-path /srv/data/urop/clean_lastfm.db --csv-path /srv/data/urop/ultimate.csv --sr 16000 --num-files 100 --start-stop 1 10 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9133368656218674
      ],
      "excerpt": "import lastfm \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8151869418446529
      ],
      "excerpt": "files = ['/srv/data/urop/tfrecords-waveform/waveform_1.tfrecord', '/srv/data/urop/tfrecords-waveform/waveform_2.tfrecord'] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8277247269550299
      ],
      "excerpt": "In order to avoid having to manually tinker with the training code every time a training parameter has to be changed, all the training parameters are set through a handy JSON file. You can create an empty config.json file by using the create_config_json() function. Here is an outline of how the JSON file is structured: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9133368656218674
      ],
      "excerpt": "import projectname \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8302488976403029
      ],
      "excerpt": "projectname.create_config_json('/srv/data/urop/config.json', 'batch_size'=32) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8169370275756846
      ],
      "excerpt": "python waveform --epochs 10 --root-dir /srv/data/urop/tfrecords-waveform --config-path /srv/data/urop/config.json --lastfm-path /srv/data/urop/clean_lastfm.db --cuda 0 1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8191841272993289
      ],
      "excerpt": "python waveform --epochs 10 --root-dir /srv/data/urop/tfrecords-waveform --config-path /srv/data/urop/config.json --lastfm-path /srv/data/urop/clean_lastfm.db --cuda 0 1 --custom \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8401558704798054,
        0.925671696398174,
        0.9293871285963212,
        0.9133368656218674
      ],
      "excerpt": "import os \nimport tensorflow as tf \nimport training \nimport projectname_train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.827550492655979
      ],
      "excerpt": "config = projectname_train.parse_config('/srv/data/urop/config.json', '/srv/data/urop/clean_lastfm.db') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.901336716501716,
        0.9206708573173158
      ],
      "excerpt": "To test a log-mel-spectrogram model on the test dataset (as specified by split in the config JSON): \npython projectname.py test log-mel-spectrogram --checkpoint /path/to/model/checkpoint --config /path/to/config.json --lastfm /path/to/clean/lastfm.db --tfrecords-dir /srv/data/urop/tfrecords-log-mel-spectrogram \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8896753023957631
      ],
      "excerpt": "python projectname.py predict log-mel-spectrogram --checkpoint /path/to/model/checkpoint --config /path/to/config.json --lastfm /path/to/clean/lastfm.db -t 0.1 --mp3 /path/to/your/song.mp3 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9196470962009372
      ],
      "excerpt": "python projectname.py predict log-mel-spectrogram --checkpoint /path/to/model/checkpoint --config /path/to/config.json --lastfm /path/to/clean/lastfm.db -t 0.1 --record --record-length 30 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/pukkapies/urop2019/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Deep Learning for Music Tagging (aka \"Orpheus\")",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "urop2019",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "pukkapies",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/pukkapies/urop2019/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* [Python](https://www.python.org/)* 3.6 or above\n* One or more CUDA-enabled GPUs\n* Mac or Linux environment\n* [TensorFlow](https://www.tensorflow.org/beta)* 2.0.0 RC0 or above (GPU version)\n* [H5Py](https://www.h5py.org/) 2.3.1 -- to read the the HDF5 MSD summary \n* [LibROSA](https://librosa.github.io/librosa/)* 0.7.0 + [FFmpeg](https://www.ffmpeg.org/)* -- to read, load and analyse audio files\n* [mutagen](https://mutagen.readthedocs.io/en/latest/) 1.42.0 -- to read audio files\n* [sounddevice](https://python-sounddevice.readthedocs.io/en/latest/)* 0.3.12 -- to record audio from your microphone through terminal\n* [sparse](https://sparse.pydata.org/en/latest/) 0.8.9 -- to perform advanced operations on the tags database (and process data using sparse matrices)\n* Other common Python libraries such as [Pandas](https://pandas.pydata.org/) or [NumPy](https://numpy.org/)\n\nIf you are just running the lite version of our prediction tool, all you need are the packages marked with *.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3,
      "date": "Sun, 05 Dec 2021 19:22:47 GMT"
    },
    "technique": "GitHub API"
  },
  "support": [
    {
      "confidence": [
        1
      ],
      "excerpt": "calle.sonne18@imperial.ac.uk\n\nchon.ho17@imperial.ac.uk\n\ndavide.gallo18@imperial.ac.uk / davide.gallo@pm.me\n\nkevin.webster@imperial.ac.uk\n",
      "technique": "Header extraction"
    }
  ],
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "calle.sonne18@imperial.ac.uk\n\nchon.ho17@imperial.ac.uk\n\ndavide.gallo18@imperial.ac.uk / davide.gallo@pm.me\n\nkevin.webster@imperial.ac.uk\n",
      "technique": "Header extraction"
    }
  ]
}