{
  "citation": [
    {
      "confidence": [
        0.9230320744509319
      ],
      "excerpt": "Ideas for Improving your Score \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8444342525991423,
        0.8444342525991423
      ],
      "excerpt": "<p align=\"center\"> <img src=\"./docs/misc/CNN_Architecture.png\"> </p> \n<p align=\"center\"> <img src=\"./docs/misc/CNN_Classifications.png\"> </p> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9321597499656605
      ],
      "excerpt": "<p align=\"center\"> <img src=\"./docs/misc/FCN.png\"> </p> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8444342525991423
      ],
      "excerpt": "<p align=\"center\"> <img src=\"./docs/misc/inception_1x1.png\"> </p> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8444342525991423
      ],
      "excerpt": "<p align=\"center\"> <img src=\"./docs/misc/FirstResultFCN_No_Skips.png\"> </p> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8444342525991423
      ],
      "excerpt": "<p align=\"center\"> <img src=\"./docs/misc/Skip_Layers_FCN.png\"> </p> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8444342525991423
      ],
      "excerpt": "<p align=\"center\"> <img src=\"./docs/misc/AllSkips_FCN.png\"> </p> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9321597499656605
      ],
      "excerpt": "<p align=\"center\"> <img src=\"./docs/misc/FCN Design.jpg\"> </p> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8207940084462922
      ],
      "excerpt": "<p align=\"center\"> <img src=\"./docs/misc/Training.png\"> </p> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8444342525991423,
        0.8444342525991423,
        0.8444342525991423,
        0.8444342525991423,
        0.8444342525991423,
        0.8444342525991423,
        0.8444342525991423,
        0.8444342525991423,
        0.8444342525991423
      ],
      "excerpt": "<p align=\"center\"> <img src=\"./docs/misc/pwt1.png\"> </p> \n<p align=\"center\"> <img src=\"./docs/misc/pwt2.png\"> </p> \n<p align=\"center\"> <img src=\"./docs/misc/pwt3.png\"> </p> \n<p align=\"center\"> <img src=\"./docs/misc/pwot1.png\"> </p> \n<p align=\"center\"> <img src=\"./docs/misc/pwot2.png\"> </p> \n<p align=\"center\"> <img src=\"./docs/misc/pwot3.png\"> </p> \n<p align=\"center\"> <img src=\"./docs/misc/fi1.png\"> </p> \n<p align=\"center\"> <img src=\"./docs/misc/fi2.png\"> </p> \n<p align=\"center\"> <img src=\"./docs/misc/fi3.png\"> </p> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.842790493796475
      ],
      "excerpt": "[Simulation Video] \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/oktantod/RoboND-DeepLearning-Project",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-06-20T05:15:09Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-12-02T02:27:48Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9833491482168105
      ],
      "excerpt": "In this project we will train a deep neural network, especially Fully Convolutional Neural Network (FCN) to identify and track a target in simulation. So-called \u201cfollow me\u201d applications like this are key to many fields of robotics and the very same techniques you apply here could be extended to scenarios like advanced cruise control in autonomous vehicles or human-robot collaboration in industry. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8226525502478987
      ],
      "excerpt": "Intel(R) Core(TM) i7 - 3630QM \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.802873322160555
      ],
      "excerpt": "NVIDIA GeForce 650M (2 GB with 384 core) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8769915989615059
      ],
      "excerpt": "Once you are comfortable with performance on the training dataset, see how it performs in live simulation! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.932069199485605
      ],
      "excerpt": "data/runs - contains the results of prediction runs \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.880777198942255
      ],
      "excerpt": "Before the network is trained, the images first need to be undergo a preprocessing step. The preprocessing step transforms the depth masks from the sim, into binary masks suitable for training a neural network. It also converts the images from .png to .jpeg to create a reduced sized dataset, suitable for uploading to AWS.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8549896399492699
      ],
      "excerpt": "Running preprocess_ims.py does not delete files in the processed_data folder. This means if you leave images in processed data and collect a new dataset, some of the data in processed_data will be overwritten some will be left as is. It is recommended to delete the train and validation folders inside processed_data(or the entire folder) before running preprocess_ims.py with a new set of collected data. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9769427065944016
      ],
      "excerpt": "With your training and validation data having been generated or downloaded from the above section of this repository, you are free to begin working with the neural net. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8333306495136495
      ],
      "excerpt": "- Validation data is in the data directory \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9616225908439635,
        0.9586983724168477,
        0.8926463595197562,
        0.9581293850230448,
        0.8660612584910234,
        0.8920369565575414,
        0.8718310708717206
      ],
      "excerpt": "Important Note the validation directory is used to store data that will be used during training to produce the plots of the loss, and help determine when the network is overfitting your data.  \nThe sample_evalution_data directory contains data specifically designed to test the networks performance on the FollowME task. In sample_evaluation data are three directories each generated using a different sampling method. The structure of these directories is exactly the same as validation, and train datasets provided to you. For instance patrol_with_targ contains an images and masks subdirectory. If you would like to the evaluation code on your validation data a copy of the it should be moved into sample_evaluation_data, and then the appropriate arguments changed to the function calls in the model_training.ipynb notebook. \nThe notebook has examples of how to evaulate your model once you finish training. Think about the sourcing methods, and how the information provided in the evaluation sections relates to the final score. Then try things out that seem like they may work. \nTo score the network on the Follow Me task, two types of error are measured. First the intersection over the union for the pixelwise classifications is computed for the target channel.  \nIn addition to this we determine whether the network detected the target person or not. If more then 3 pixels have probability greater then 0.5 of being the target person then this counts as the network guessing the target is in the image.  \nWe determine whether the target is actually in the image by whether there are more then 3 pixels containing the target in the label mask.  \nUsing the above the number of detection true_positives, false positives, false negatives are counted.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9671912790276808
      ],
      "excerpt": "The final score is the pixelwise average_IoU*(n_true_positive/(n_true_positive+n_false_positive+n_false_negative)) on data similar to that provided in sample_evaulation_data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9619086449651051
      ],
      "excerpt": "Collect more data from the sim. Look at the predictions think about what the network is getting wrong, then collect data to counteract this. Or improve your network architecture and hyperparameters.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8986602875301211
      ],
      "excerpt": "Note: If you'd like to see an overlay of the detected region on each camera frame from the drone, simply pass the --pred_viz parameter to follower.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.972093648691899,
        0.8828436890714909,
        0.9839903176834149,
        0.9977622499355513
      ],
      "excerpt": "A Convolution Neural Network may have several layer which is each layer might capture a different level in the hierarchy of object. The first layer called as lowest level hierarchy, where CNN may classifies small parth of the image into simple shapes like horizontal and vertical linea and simple blobs of colors. The last layers tend to be highest level in the hierarchy and may classify more complex ideas like shapes and eventually full object like cars. All this convolutional layer also called as feature learning. \nThe highest level of hierarchy or the last convolutional layer then connected with classification layer which is consist with fully connected layer and softmax . From this layer, the input would be classified as which object. CNN are usually used to classify object inside an image. \nCNN very useful for tackling tasks such as image classification, which just want to determine 'what' is the object in a image. But when we want to know 'where' is in the image a certain object, CNN would not work since fully connected layers remove any sense of spacial information. Therefore Fully Convolutional Network (FCN), will perform this task. \nA FCN is a CNN, which is the classification layer is replace with 1x1 convolution layer with a large \"receptive field\" and add with upscale layer which called as decoder.  The purpose in here is to get the global context of the scene and enable us to get what are the object on image and their spatial information. The output of this network not only contain object classification but also the scene of segmentation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9360007666850166,
        0.9693049053889192,
        0.9863082695011827
      ],
      "excerpt": "The structure of FCN is divide by two part that is encoder layer part which will extract feature from the image and decoder layer part which will upscale the output of the encoder so the output will have the original size of the image. This two part connected with 1x1 convolution layer. \na 1x1 convolution simply maps an input pixel with all its channel to an output pixel, not looking at anything around itself. It is often used to reduce the number of depth channels, since it is often very slow to multiply volumes with extremely large depths. \nWhen we convert our last fully connected (FC) layer of the CNN to a 1x1 convolutional layer we choose our new conv layer to be big enough so that it will enable us to have this localization effect scaled up to our original input image size then activate pixels to indicate objects and their approximate locations in the scene as shown in above figure. replacement of fully-connected layers with convolutional layers presents an added advantage that during inference (testing your model), you can feed images of any size into your trained network. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8650130063560221
      ],
      "excerpt": "Here is the screenshot from the paper, which elucidates above points: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9695074312851834
      ],
      "excerpt": "It can be seen from the image on the right, that 1x1 convolutions (in yellow), are specially used before 3x3 and 5x5 convolution to reduce the dimensions. It should be noted that a two step convolution operation can always to combined into one, but in this case and in most other deep learning networks, convolutions are followed by non-linear activation and hence convolutions are no longer linear operators and cannot be combined. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.976151558671623,
        0.948720746154612,
        0.8866819289885607
      ],
      "excerpt": "Everytime we do convolution (down sampling), we are facing one problem with this approach that is we lose some information; we keep the smaller picture (the local context) and lose the bigger picture (the global context) for example if we are using max-pooling to reduce the size of the input, and allow the neural network to focus on only the most important elements. Max pooling does this by only retaining the maximum value for each filtered area, and removing the remaining values. \nTo solve this problem we also get some activation from previous layers and sum/interpolate them together. This process is called \"skip\" from the creators of this algorithm. \nThose up-sampling operations used on skip are also learn-able. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9717886221326802
      ],
      "excerpt": "Below we show the effects of this \"skip\" process, notice how the resolution of the segmentation improves after some \"skips\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9077541129752358
      ],
      "excerpt": "In this learning project, I didn't record train, validation and sample_evaluation_data data from quadcopter simulator. I used train and validation data from link above to get weight from the network model that I have design.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8005846578083659,
        0.8564057958721433
      ],
      "excerpt": "In this project, there are seven layers to build a fully convolutional networks (FCN). Three layers for encoder, one layers as one by one convolutional matrix and another three layers as decoder block. See image below: \n<p align=\"center\"> <img src=\"./docs/misc/FCN Design.jpg\"> </p> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9237903541385124,
        0.8694174889132203
      ],
      "excerpt": "Explanations of how to build the code for FCN Design above would be explain in Build the Model section below \nThe Encoder for FCN require separable convolution layers. The 1x1 convolution layer in the FCN, however, is a regular convolution. Implementations for both are provided below for your use. Each includes batch normalization with the ReLU activation function applied to the layers. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9558359752759511
      ],
      "excerpt": "The following helper function implements the bilinear upsampling layer. Upsampling by a factor of 2 is generally recommended, but you can try out different factors as well. Upsampling is used in the decoder block of the FCN. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9068047872754496
      ],
      "excerpt": "Create an encoder block that includes a separable convolution layer using the separable_conv2d_batchnorm() function. The filters parameter defines the size or depth of the output layer. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.952343165638383,
        0.8041804105499879,
        0.8120231774532914
      ],
      "excerpt": "The decoder block is comprised of three parts: \n*    A bilinear upsampling layer using the upsample_bilinear() function. The current recommended factor for upsampling is set to 2. \n*    A layer concatenation step. This step is similar to skip connections. You will concatenate the upsampled small_ip_layer and the large_ip_layer. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8243542055905074
      ],
      "excerpt": "Add a 1x1 Convolution layer using the conv2d_batchnorm() function. Remember that 1x1 Convolutions require a kernel and stride of 1. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9613156204379512
      ],
      "excerpt": "#: Remember that with each encoder layer, the depth of your model (the number of filters) increases. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8144897333295629
      ],
      "excerpt": "#: The function returns the output layer of your model. \"layer07\" is the final layer obtained from the last decoder_block() \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9456618933848937,
        0.8720245544523968
      ],
      "excerpt": "Training is my bigest problems. I have facing two problem in AWS account, the first one is AWS reject my request increasing EC2 instance p2.xlarge and the second ones is AWS facing problem when send my promotion code for initial balance by mail. Thanks for support dashboard in AWS, my complain had been approved at 18 June for initial credit. And at 20 June AWS approved for increasing limit in p2.xlarge when I reopen the case. Sadly when all my request had been approved, I moved to my village with lower internet connectivity speed therefore I used my laptop for trained the model. Spesification of my laptop you can see at above explanations. \nTo increase the speed of training the model in my laptop, I install the following software and library: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8044325519217305
      ],
      "excerpt": "Number of training samples/images that get propagated through the network in a single pass. In this training we used batch_size with value 32. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8837529248881487
      ],
      "excerpt": "Now that you have your model trained and saved, you can make predictions on your validation dataset. These predictions can be compared to the mask images, which are the ground truth labels, to evaluate how well your model is doing under different conditions. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9475759872195062,
        0.9067504390052318,
        0.8901891029310368,
        0.9113518254330505,
        0.9592740889281454
      ],
      "excerpt": "Evaluate our model! The following cells include several different scores to help you evaluate your model under the different conditions discussed during the Prediction step. \nnumber of validation samples intersection over the union evaulated on 542 \naverage intersection over union for background is 0.9944914007764788 \naverage intersection over union for other people is 0.3256942366738677 \naverage intersection over union for the hero is 0.9125996469040777 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9067504390052318,
        0.8901891029310368,
        0.9113518254330505,
        0.9592740889281454
      ],
      "excerpt": "number of validation samples intersection over the union evaulated on 270 \naverage intersection over union for background is 0.981193497537517 \naverage intersection over union for other people is 0.6976223997700709 \naverage intersection over union for the hero is 0.0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9067504390052318,
        0.8901891029310368,
        0.9113518254330505,
        0.9592740889281454
      ],
      "excerpt": "number of validation samples intersection over the union evaulated on 322 \naverage intersection over union for background is 0.995441234028656 \naverage intersection over union for other people is 0.40741392661423764 \naverage intersection over union for the hero is 0.19374283779449622 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8886028841997774
      ],
      "excerpt": "The model weights selected is model_weights_new that have final score 40.74, to run this model weight \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9740928823194837
      ],
      "excerpt": "For future enhancement, there are several thing that need to be improved to increased final model score and accuracy in simulator that is: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8114129098844153
      ],
      "excerpt": "I have training the model using my laptop which have a standard graphical card which give me about 22.8 hour to finish model training. Because my EC2 Instances limit increase request have been approved by AWS, I will train my model using AWS Services. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9680300006451583
      ],
      "excerpt": "In this project, I used epochs and steps_per_epoch limited to get passing required scores. I need to increase the number of epochs to increase my model final scores. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/oktantod/RoboND-DeepLearning-Project/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Tue, 07 Dec 2021 18:21:00 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/oktantod/RoboND-DeepLearning-Project/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "oktantod/RoboND-DeepLearning-Project",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/oktantod/RoboND-DeepLearning-Project/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/oktantod/RoboND-DeepLearning-Project/master/code/model_training.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To collect the validation set, repeat both sets of steps above, except using the directory `data/raw_sim_data/validation` instead rather than `data/raw_sim_data/train`.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Run QuadSim\n2. Click the `DL Training` button\n3. Set patrol points, path points, and spawn points. **TODO** add link to data collection doc\n3. With the simulator running, press \"r\" to begin recording.\n4. In the file selection menu navigate to the `data/raw_sim_data/train/run1` directory\n5. **optional** to speed up data collection, press \"9\" (1-9 will slow down collection speed)\n6. When you have finished collecting data, hit \"r\" to stop recording.\n7. To reset the simulator, hit \"`<esc>`\"\n8. To collect multiple runs create directories `data/raw_sim_data/train/run2`, `data/raw_sim_data/train/run3` and repeat the above steps.\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "**Clone the repository**\n```\n$ git clone https://github.com/udacity/RoboND-DeepLearning.git\n```\n\n**Download the data**\n\nSave the following three files into the data folder of the cloned repository. \n\n[Training Data](https://s3-us-west-1.amazonaws.com/udacity-robotics/Deep+Learning+Data/Lab/train.zip) \n\n[Validation Data](https://s3-us-west-1.amazonaws.com/udacity-robotics/Deep+Learning+Data/Lab/validation.zip)\n\n[Sample Evaluation Data](https://s3-us-west-1.amazonaws.com/udacity-robotics/Deep+Learning+Data/Project/sample_evaluation_data.zip)\n\nWe used above data training and validation for train weight for FCN.\n\n**Download the QuadSim binary**\n\nTo interface your neural net with the QuadSim simulator, you must use a version QuadSim that has been custom tailored for this project. The previous version that you might have used for the Controls lab will not work.\n\nThe simulator binary can be downloaded [here](https://github.com/udacity/RoboND-DeepLearning/releases/latest)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8419785735712633,
        0.8837680365796365
      ],
      "excerpt": "Windows 8.1 64bit \nPython 3.x \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8708439851414099,
        0.8564268077837719
      ],
      "excerpt": "NumPy 1.11 \nSciPy 0.17.0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8130198600172718,
        0.8411004553040458
      ],
      "excerpt": "* Latest NVIDIA Driver 398.11 \n* CUDA v9.0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8851454362724194
      ],
      "excerpt": "* https://iamaaditya.github.io/2016/03/one-by-one-convolution/ \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8701686947275162
      ],
      "excerpt": "Download the training dataset from above and extract to the project data directory. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8569375374835275,
        0.809588831165426
      ],
      "excerpt": "data/train/images - contains images for the training set \ndata/train/masks - contains masked (labeled) images for the training set \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "$ python preprocess_ims.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8114648371512049
      ],
      "excerpt": "Running preprocess_ims.py does not delete files in the processed_data folder. This means if you leave images in processed data and collect a new dataset, some of the data in processed_data will be overwritten some will be left as is. It is recommended to delete the train and validation folders inside processed_data(or the entire folder) before running preprocess_ims.py with a new set of collected data. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.850528747589133,
        0.8469758981582196
      ],
      "excerpt": "The notebook, and supporting code assume your data for training/validation is in data/train, and data/validation. After you run preprocess_ims.py you will have new train, and possibly validation folders in the processed_ims. \nRename or move data/train, and data/validation, then move data/processed_ims/train, into data/, and  data/processed_ims/validationalso into data/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8480949644936816,
        0.807402658134823,
        0.8306350328409582
      ],
      "excerpt": "- Training data is in data directory \n- Validation data is in the data directory \n- The folders data/train/images/, data/train/masks/, data/validation/images/, and data/validation/masks/ should exist and contain the appropriate data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "$ python follower.py my_amazing_model.h5 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8131189859691188,
        0.8131189859691188
      ],
      "excerpt": "<p align=\"center\"> <img src=\"./docs/misc/CNN_Architecture.png\"> </p> \n<p align=\"center\"> <img src=\"./docs/misc/CNN_Classifications.png\"> </p> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8131189859691188
      ],
      "excerpt": "<p align=\"center\"> <img src=\"./docs/misc/inception_1x1.png\"> </p> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8131189859691188
      ],
      "excerpt": "<p align=\"center\"> <img src=\"./docs/misc/FirstResultFCN_No_Skips.png\"> </p> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8131189859691188
      ],
      "excerpt": "<p align=\"center\"> <img src=\"./docs/misc/Skip_Layers_FCN.png\"> </p> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8131189859691188
      ],
      "excerpt": "<p align=\"center\"> <img src=\"./docs/misc/AllSkips_FCN.png\"> </p> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8134023751690825
      ],
      "excerpt": "    <tr><td align=\"left\">/data/train</td><td align=\"left\">4,131 images + 4,131 masks</td></tr> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8325007702651301
      ],
      "excerpt": "print(\"Outputs shape:\",outputs.shape, \"\\tOutput Size in Pixel\") \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8542189592944772
      ],
      "excerpt": "Number of batches of training images that go through the network in 1 epoch. One recommended value to try would be based on the total number of images in training dataset divided by the batch_size. Total number in training data set is 4131 images divided by 32 with the result is 129. We select step each epoch is 200. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.857655476265086
      ],
      "excerpt": "<p align=\"center\"> <img src=\"./docs/misc/Training.png\"> </p> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8131189859691188,
        0.8131189859691188,
        0.8131189859691188,
        0.8131189859691188,
        0.8131189859691188,
        0.8131189859691188,
        0.8131189859691188,
        0.8131189859691188,
        0.8131189859691188
      ],
      "excerpt": "<p align=\"center\"> <img src=\"./docs/misc/pwt1.png\"> </p> \n<p align=\"center\"> <img src=\"./docs/misc/pwt2.png\"> </p> \n<p align=\"center\"> <img src=\"./docs/misc/pwt3.png\"> </p> \n<p align=\"center\"> <img src=\"./docs/misc/pwot1.png\"> </p> \n<p align=\"center\"> <img src=\"./docs/misc/pwot2.png\"> </p> \n<p align=\"center\"> <img src=\"./docs/misc/pwot3.png\"> </p> \n<p align=\"center\"> <img src=\"./docs/misc/fi1.png\"> </p> \n<p align=\"center\"> <img src=\"./docs/misc/fi2.png\"> </p> \n<p align=\"center\"> <img src=\"./docs/misc/fi3.png\"> </p> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8510471226461515
      ],
      "excerpt": "number true positives: 539, number false positives: 0, number false negatives: 0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8510471226461515
      ],
      "excerpt": "number true positives: 0, number false positives: 52, number false negatives: 0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8510471226461515
      ],
      "excerpt": "number true positives: 118, number false positives: 0, number false negatives: 183 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python follower.py model_weights_new \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8668937975765546
      ],
      "excerpt": "1. Increased Data Training. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/oktantod/RoboND-DeepLearning-Project/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "JavaScript",
      "HTML",
      "Jupyter Notebook",
      "CSS",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2017 Udacity\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Deep Learning Project - Follow Me ##",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "RoboND-DeepLearning-Project",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "oktantod",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/oktantod/RoboND-DeepLearning-Project/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Tue, 07 Dec 2021 18:21:00 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "udacity",
      "nanodegree",
      "course"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\n0.7365470852017937\n```\n",
      "technique": "Header extraction"
    }
  ]
}