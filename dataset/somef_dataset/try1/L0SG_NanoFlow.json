{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2006.06280",
      "https://arxiv.org/abs/1807.03039\n[WaveFlow]: https://arxiv.org/abs/1912.01219\n[LJ Speech Data]: https://keithito.com/LJ-Speech-Dataset\n[Apex]: https://github.com/nvidia/apex\n[official implementation]: https://github.com/PaddlePaddle/Parakeet\n[Glow-PyTorch]: https://github.com/y0ast/Glow-PyTorch\n[\"NanoFlow: Scalable Normalizing Flows with Sublinear Parameter Complexity\"]: https://arxiv.org/abs/2006.06280\n[here]: https://github.com/r9y9/wavenet_vocoder/issues/67\n[Our other WaveFlow repo]: https://github.com/L0SG/WaveFlow\n[other open-source implementation of WaveNet]: https://github.com/r9y9/wavenet_vocoder/issues/67\n[convolution queue]: https://arxiv.org/abs/1611.0948",
      "https://arxiv.org/abs/1912.01219\n[LJ Speech Data]: https://keithito.com/LJ-Speech-Dataset\n[Apex]: https://github.com/nvidia/apex\n[official implementation]: https://github.com/PaddlePaddle/Parakeet\n[Glow-PyTorch]: https://github.com/y0ast/Glow-PyTorch\n[\"NanoFlow: Scalable Normalizing Flows with Sublinear Parameter Complexity\"]: https://arxiv.org/abs/2006.06280\n[here]: https://github.com/r9y9/wavenet_vocoder/issues/67\n[Our other WaveFlow repo]: https://github.com/L0SG/WaveFlow\n[other open-source implementation of WaveNet]: https://github.com/r9y9/wavenet_vocoder/issues/67\n[convolution queue]: https://arxiv.org/abs/1611.0948",
      "https://arxiv.org/abs/2006.06280\n[here]: https://github.com/r9y9/wavenet_vocoder/issues/67\n[Our other WaveFlow repo]: https://github.com/L0SG/WaveFlow\n[other open-source implementation of WaveNet]: https://github.com/r9y9/wavenet_vocoder/issues/67\n[convolution queue]: https://arxiv.org/abs/1611.0948",
      "https://arxiv.org/abs/1611.0948"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "NVIDIA Tacotron2: https://github.com/NVIDIA/tacotron2\n\nNVIDIA WaveGlow: https://github.com/NVIDIA/waveglow\n\nr9y9 wavenet-vocoder: https://github.com/r9y9/wavenet_vocoder\n\nFloWaveNet: https://github.com/ksw0306/FloWaveNet\n\nParakeet: https://github.com/PaddlePaddle/Parakeet\n\nWaveFlow (unofficial): https://github.com/L0SG/WaveFlow\n\nGlow-PyTorch: https://github.com/y0ast/Glow-PyTorch\n\nNeural Spline Flows (nsf): https://github.com/bayesiains/nsf\n\n[Tacotron2]: https://github.com/NVIDIA/tacotron2\n[DataParallel]: https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html\n[DistributedDataParallel]: https://pytorch.og/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html\n[Glow]: https://arxiv.org/abs/1807.03039\n[WaveFlow]: https://arxiv.org/abs/1912.01219\n[LJ Speech Data]: https://keithito.com/LJ-Speech-Dataset\n[Apex]: https://github.com/nvidia/apex\n[official implementation]: https://github.com/PaddlePaddle/Parakeet\n[Glow-PyTorch]: https://github.com/y0ast/Glow-PyTorch\n[\"NanoFlow: Scalable Normalizing Flows with Sublinear Parameter Complexity\"]: https://arxiv.org/abs/2006.06280\n[here]: https://github.com/r9y9/wavenet_vocoder/issues/67\n[Our other WaveFlow repo]: https://github.com/L0SG/WaveFlow\n[other open-source implementation of WaveNet]: https://github.com/r9y9/wavenet_vocoder/issues/67\n[convolution queue]: https://arxiv.org/abs/1611.09482",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8727473632394707,
        0.996779633287069
      ],
      "excerpt": "Waveform synthesis model (i.e. neural vocoder) based on [WaveFlow] (Ping et al., ICML 2020). See below for a detailed description. \nImage density estimation based on [Glow] (Kingma et al., NIPS 2018), hosted in a separate image_density_experiments subdirectory. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/L0SG/NanoFlow",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-10-20T01:36:29Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-18T06:13:58Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9963725023085989
      ],
      "excerpt": "This repository is an official PyTorch implementation of the paper: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9973363383249743,
        0.9873792613226829,
        0.9170359976898982
      ],
      "excerpt": "A flow-based network is considered to be inefficient in parameter complexity because of reduced expressiveness of bijective mapping, which renders the models unfeasibly expensive in terms of parameters. We present an alternative parameterization scheme called NanoFlow, which uses a single neural density estimator to model multiple transformation stages. \nThe codebase provides two real-world applications of flow-based models with our method: \nWaveform synthesis model (i.e. neural vocoder) based on [WaveFlow] (Ping et al., ICML 2020). See below for a detailed description. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9143614608213073
      ],
      "excerpt": "   Single-node multi-GPU training is automatically enabled with [DataParallel] (instead of [DistributedDataParallel] for simplicity). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.961596398601625,
        0.993421609238842,
        0.9875233244898406,
        0.9834147582858052,
        0.9059428501610673,
        0.8501718759067614,
        0.9668638535207167,
        0.9509035208109836,
        0.9837619466112372,
        0.9295823730597407,
        0.9498591860687243
      ],
      "excerpt": "We used row-wise autoregressive coupling transformation for the entire data point for each flow. This is implemented by shifting the data point down by one and padding the first upper row with zeros (see shift_1d). The network uses the shifted input for the transformation. \nFor example, with h=16, all 16 rows are transformed, whereas the [official implementation] transforms 15 rows by splitting the input into the upper 1 and remaining 15 rows and performing transformation on 15 rows. The difference in performance is marginal. [Our other WaveFlow repo] provides more faithful details following the [official implementation].   \nWe used math.sqrt(0.5) as a constant multiplier for fused_res_skip similar to [other open-source implementation of WaveNet]. Later we found that the difference is negligible. \nThere exists a tiny fraction of unused network parameter (half of the last res_skip_conv layer) for the simplicity of implementation.  \nWe initialized multgate for NanoFlow with ones (self.multgate = nn.Parameter(torch.ones(num_layer, filter_size))) for WaveFlow-based experiments, instead of using zero-init (self.multgate = nn.Parameter(torch.zeros((6, hidden_channels, 1, 1)))) accompanied by multgate = torch.exp(multgate) from Glow-based experiments. \nLater we found no meaningful difference between the two, but the latter assures the positive value range to be interpreted as gating. \nreverse_fast implements an edge case version of the [convolution queue] mechanism without a proper queue system for simplicity. It is only correct up to \"n_height\": 16 with \"n_layer_per_cycle\": 1. \nWe provide pretrained weights via Google Drive. The models are further fine-tuned for additional 2.5 M steps with a constant \"learning_rate\": 2e-4 from the checkpoint used in the paper, then we averaged weights over 20 last checkpoints with -a 20.  \nPlease note that these models are not based on the best-performing vocoder configuration of the WaveFlow paper and serve as a comparative study. Specifically, \nThe models are trained on the 90 % of the LJSpeech clips and the remaining 10 % clips are used only for evaluation. \nWe have not applied the bipartized permutation method in these models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "PyTorch implementation of the paper \"NanoFlow: Scalable Normalizing Flows with Sublinear Parameter Complexity.\"",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/L0SG/NanoFlow/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 3,
      "date": "Sun, 12 Dec 2021 14:13:17 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/L0SG/NanoFlow/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "L0SG/NanoFlow",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/L0SG/NanoFlow/master/tacotron2_custom/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/L0SG/NanoFlow/master/tacotron2_custom/inference.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Clone this repo and install requirements\n\n   ```command\n   git clone https://github.com/L0SG/NanoFlow.git\n   cd NanoFlow\n   pip install -r requirements.txt\n   ```\n\n2. Install [Apex] for mixed-precision training\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8160988559529309
      ],
      "excerpt": "Below are the example commands using nanoflow-h16-r128-emb512.json \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8386040774764527
      ],
      "excerpt": "Download [LJ Speech Data]. In this example it's in data/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8474031600365725
      ],
      "excerpt": "    -n+1310 and -n1310 indicates that this example reserves the first 1310 audio clips (10 % of the dataset) for model testing. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9280120806792806
      ],
      "excerpt": "   python train.py -c configs/nanoflow-h16-r128-emb512.json \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8181312107475086
      ],
      "excerpt": "For mixed precision training, set \"fp16_run\": true on the configuration file. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8924791493079317
      ],
      "excerpt": "| Models        | Test set LL (gain)| Params (M) | Download | \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/L0SG/NanoFlow/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook",
      "Dockerfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "BSD 3-Clause \"New\" or \"Revised\" License",
      "url": "https://api.github.com/licenses/bsd-3-clause"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 Joost van Amersfoort\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n\\nMIT License\\n\\nCopyright (c) 2019 Yuki-Chai\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# NanoFlow: Scalable Normalizing Flows with Sublinear Parameter Complexity",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "NanoFlow",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "L0SG",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/L0SG/NanoFlow/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 62,
      "date": "Sun, 12 Dec 2021 14:13:17 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "normalizing-flows",
      "deep-generative-model",
      "density-estimation",
      "speech-synthesis",
      "pytorch",
      "probabilistic-models",
      "generative-models"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "   insert `checkpoint_path: \"experiments/nanoflow-h16-r128-emb512/waveflow_5000\"` in the config file then run\n   ```command\n   python train.py -c configs/nanoflow-h16-r128-emb512.json\n   ```\n\n   for loading averaged weights over 10 recent checkpoints, insert `checkpoint_path: \"experiments/nanoflow-h16-r128-emb512\"` in the config file then run\n   ```command\n   python train.py -a 10 -c configs/nanoflow-h16-r128-emb512.json\n   ```\n\n   you can reset the optimizer and training scheduler (and keep the weights) by providing `--warm_start`\n   ```command\n   python train.py --warm_start -c configs/nanoflow-h16-r128-emb512.json\n   ```\n   \n4. Synthesize waveform from the trained model.\n\n   insert `checkpoint_path` in the config file and use `--synthesize` to `train.py`. The model generates waveform by looping over `test_files.txt`.\n   ```command\n   python train.py --synthesize -c configs/nanoflow-h16-r128-emb512.json\n   ```\n   if `fp16_run: true`, the model uses FP16 (half-precision) arithmetic for faster performance (on GPUs equipped with Tensor Cores).\n\n\n",
      "technique": "Header extraction"
    }
  ]
}