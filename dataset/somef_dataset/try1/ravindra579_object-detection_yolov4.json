{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2004.10934 and go through the paper to unsterstood better\n \n The neck part of yolov4 will be fpn,spp,panet..... if we use \"SPP(spatial pyramid pooling"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9023140952808582
      ],
      "excerpt": "You can download offical paper from this link https://arxiv.org/abs/2004.10934 and go through the paper to unsterstood better \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ravindra579/object-detection_yolov4",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-10-11T09:03:15Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-09-11T13:39:55Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9795736975036736,
        0.9906283989278901
      ],
      "excerpt": "YOLOV4's backbone arcitecture can be vgg16,resnet-50,resnet16-101,darknet53.... as said in official paper it is better to use \"CSPdarknet53\" you can check the architecture the flow chart is shown in \"yolov4_model.pdf\" and the code is shown in \"yolov4_model.py\" \n\"cspdarknet53\" is a novel backbone that can enhance the learning capability of cnn  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9725696407078133,
        0.8595276472258975,
        0.9913219642945198,
        0.8458117486595027
      ],
      "excerpt": "The neck part of yolov4 will be fpn,spp,panet..... if we use \"SPP(spatial pyramid pooling)\" it gives more accuracy the spp block is added over \"cspdarknet53\" to increase the receptive field and seperate out most signitficant features  \nYOLOV4 is twice as fast as efficiendet with comparable performance and fps increased by 10% to 12% compared to YOLOV3 \nHigher input network size (resolution) \u2013 for detecting multiple small-sized objects ,More layers \u2013 for a higher receptive field to cover the increased size of input network , More parameters \u2013 for greater capacity of a model to detect multiple objects of different sizes in a single image \nI have used \"412 x 412\" as input image shape for model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9322700849331249
      ],
      "excerpt": "\"model.txt\" consist all labels  as it is a cocodataset it contains 0 classes or labels  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8674782255729199
      ],
      "excerpt": "\"yolov4_.py\" is just like a dense prediction contains \"NMS(non max supression)\" ,IOU(intersection over union)\" and many functions used for images and for videos we can use \"yolov4_video.py\"  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9129776972456475
      ],
      "excerpt": "Backbones: VGG16 , ResNet-50 , SpineNet, EfficientNet-B0/B7 ,CSPResNeXt50,CSPDarknet53 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9448396535952216,
        0.9264846128106652
      ],
      "excerpt": "       Dense Prediction (one-stage):\u25e6 RPN , SSD, YOLO , RetinaNet (anchor based)\u25e6 CornerNet, CenterNet, MatrixNet, FCOS  (anchor free) \n       Sparse Prediction (two-stage):\u25e6 Faster R-CNN , R-FCN, Mask RCNN (anchor based)\u25e6 RepPoints(anchor free) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.928575013425916,
        0.9907539151038556
      ],
      "excerpt": "One stage or two stage models. A one stage model is capable of detecting objects without the need for a preliminary step. On the contrary, a two stage detector uses a preliminary stage where regions of importance are detected and then classified to see if an object has been detected in these areas. The advantage of a one stage detector is the speed it is able to make predictions quickly allowing a real time use. \nA modern detector is usually composed of two parts,a backbone which is pre-trained on ImageNet and a head which is used to predict classes and bounding boxes of objects. For those detectors running on GPU platform, their backbone could be VGG , ResNet , ResNeXt,or DenseNet. For those detectors running on CPU platform, their backbone could be SqueezeNet, MobileNet, or ShuffleNet. As to the head part,it is usually categorized into two kinds, i.e., one-stage object detector and two-stage object detector. The most representative two-stage object detector is the R-CNN series,including fast R-CNN , faster R-CNN , R-FCN,and Libra R-CNN. It is also possible to make a twostage object detector an anchor-free object detector, such as RepPoints. As for one-stage object detector, the most representative models are YOLO , SSD ,and RetinaNet. In recent years,anchor-free one-stage object detectors are developed. The detectors of this sort are CenterNet, CornerNet, FCOS \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9770008333066788
      ],
      "excerpt": "YoloV4 is an important improvement of YoloV3, the implementation of a new architecture in the Backbone and the modifications in the Neck have improved the mAP(mean Average Precision) by 10% and the number of FPS(Frame per Second) by 12%. In addition, it has become easier to train this neural network on a single GPU. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9831496454405035,
        0.9429773348789374,
        0.9968029537584643,
        0.9968029537584643
      ],
      "excerpt": "Deep neural network composed mainly of convolution layers. The main objective of the backbone is to extract the essential features, the selection of the backbone is a key step it will improve the performance of object detection. Often pre-trained neural networks are used to train the backbone. \nThe YoloV4 backbone architecture is composed of three parts: \nBag of freebies \nBag of specials \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9968029537584643,
        0.9389284648751481,
        0.8979411005071259,
        0.9969318775940341
      ],
      "excerpt": "Bag of freebies: \nwhich can make the object detector receive better accuracy without increasing the inference cost. We call these methods that only change the training strategy or only increase the training cost as bag of freebies . \n=> Data augmentation:- \n                The main objective of data augmentation methods is to increase the variability of an image in order to improve the generalization of the model training. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9290766678937148
      ],
      "excerpt": "               Photometric distortion creates new images by adjusting brightness, hue, contrast, saturation and noise to display more varieties of the same image. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9326722884150903
      ],
      "excerpt": "                 The geometric distortion methods are all the techniques used to rotate the image, flipping, random scaling or cropping. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9669982031213313
      ],
      "excerpt": "        Mixup augmentation is a type of augmentation where in we form a new image through weighted linear interpolation of two existing images. We take two images and do a linear combination of them in terms of tensors of those images. Mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9616105670324135
      ],
      "excerpt": "       CutMix augmentation strategy: patches are cut and pasted among training images where the ground truth labels are also mixed proportionally to the area of the patches. CutMix improves the model robustness against input corruptions and its out-of-distribution detection performances \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9511106648145888
      ],
      "excerpt": "       The Focal Loss is designed to address the one-stage object detection scenario in which there is an extreme imbalance between foreground and background classes during training . The new Focal loss function is based on the cross entropy by introducing a (1-pt)^gamma coefficient. This coefficient allows to focus the importance on the correction of misclassified examples. at gamma =0 focal loss= cross entropy \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9259846703945787
      ],
      "excerpt": "         Whenever you feel absolutely right, you may be plainly wrong. A 100% confidence in a prediction may reveal that the model is memorizing the data instead of learning. Label smoothing adjusts the target upper bound of the prediction to a lower value say 0.9. And it will use this value instead of 1.0 in calculating the loss. This concept mitigates overfitting. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8124624276670543,
        0.965811778784565,
        0.8506019077535971,
        0.8808219410726246,
        0.9856911134497687,
        0.9968029537584643,
        0.9652123813619577
      ],
      "excerpt": "        Most object detection models use bounding box to predict the location of an object. To evaluate the quality of a model the L2 standard is used, to calculate the difference in position and size of the predicted bounding box and the real bounding box. \nThe disadvantage of this L2 standard is that it minimizes errors on small objects and tries to minimize errors on large bounding boxes. \nTo address this problem we use IoU loss for the YoloV4 model. \nCompared to the l2 loss, we can see that instead of optimizing four coordinates independently, the IoU loss considers the bounding box as a unit. Thus the IoU loss could provide more accurate bounding box prediction than the l2 loss. Moreover, the definition naturally norms the IoU to [0, 1] regardless of the scales of bounding boxes \nRecently, some Improved IoU loss are For example, GIoU loss  is to include the shape and orientation of object in addition to the coverage area. They proposed to find the smallest area BBox that can simultaneously cover the predicted BBox and ground truth BBox, and use this BBox as the denominator to replace the denominator originally used in IoU loss. As for DIoU loss , it additionally considers the distance of the center of an object, and CIOU loss , on the other hand simultaneously considers the overlapping area, the distance between center points, and the aspect ratio. CIoU can achieve better convergence speed and accuracy on the BBox regression problem. \nBag of specials \n    Bag of special methods are the set of methods which increase inference cost by a small amount but can significantly improve the accuracy of object detection. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9845335959013979
      ],
      "excerpt": "   Why Mish activation :Due to the preservation of a small amount of negative information, Mish eliminated by design the preconditions necessary for the Dying ReLU phenomenon. A large negative bias can cause saturation of the ReLu function and causes the weights not to be updated during the backpropagation phase making the neurons inoperative for prediction. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9903606116745927,
        0.9903946083457891
      ],
      "excerpt": "               SPP module was originated from Spatial Pyramid Matching (SPM) [39], and SPMs original method was to split feature map into several d \u00d7 d equal blocks, where d can be{1, 2, 3, ...}, thus forming spatial pyramid, and then extracting bag-of-word features. SPP integrates SPM into CNN and use max-pooling operation instead of bag-of-word operation. Since the SPP will output one dimensional feature vector, it is infeasible to be applied in Fully Convolutional Network (FCN). \n  The post-processing method commonly used in deeplearning-based object detection is NMS, which can be used to filter those BBoxes that badly predict the same object, and only retain the candidate BBoxes with higher response. The way NMS tries to improve is consistent with the method of optimizing an objective function \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8950477797477869,
        0.9600138550193346
      ],
      "excerpt": "The Cross Stage Partial architecture is derived from the DenseNet architecture which uses the previous input and concatenates it with the current input before moving into the dense layer. \nEach stage layer of a DenseNet contains a dense block and a transition layer, and each dense block is composed of k dense layers. The output of the ith dense layer will be concatenated with the input of the ith dense layer, and the concatenated outcome will become the input of the (i + 1)th dense layer. The equations showing the above-mentioned mechanism can be expressed as: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9643788867644036
      ],
      "excerpt": "where x is the convolution [x0,x1,x2 ....] concatenate of x0,x1,x2,.... \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9590575129989665
      ],
      "excerpt": "The CSP is based on the same principle except that instead of concatenating the ith output with the ith input, we divided the input ith in two parts x0' and x0'\u2019, one part will pass through the dense layer x0'\u2019, the second part x0' will be concatenated at the end with the result at the output of the dense layer of x0'\u2019. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9642046779065025
      ],
      "excerpt": "The essential role of the neck is to collect feature maps from different stages. Usually, a neck is composed of several bottom-up paths and several top-down paths. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9434882930352301,
        0.955860651416752,
        0.9936905555768387
      ],
      "excerpt": "What is the problem caused by CNN and fully connected network ? \nThe fully connected network requires a fixed size so we need to have a fixed size image, when detecting objects we don\u2019t necessarily have fixed size images. This problem forces us to scale the images, this method can remove a part of the object we want to detect and therefore decrease the accuracy of our model. \nThe second problem caused by CNN is that the size of the sliding window is fixed. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9932545955125686,
        0.9530358981023598
      ],
      "excerpt": "     At the output of the convolution neural networks, we have the features map, these are features generated by our different filters. To make it simple, we can have a filter able to detect circular geometric shapes, this filter will produce a feature map highlighting these shapes while keeping the location of the shape in the image. \nSpatial Pyramid Pooling Layer will allow to generate fixed size features whatever the size of our feature maps. To generate a fixed size it will use pooling layers like Max Pooling for example, and generate different representations of our feature maps. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8232140053251502,
        0.9296476983138716,
        0.9162513504233846,
        0.8759440945611422,
        0.9524508895612396,
        0.9187690097894642
      ],
      "excerpt": "2)First, each feature map is pooled to become a one value. Then the size of the vector is (1, 256) \n3)Then, each feature map is pooled to have 4 values. Then the size of the vector is (4, 256) \nOn the same way, each feature is pooled to have 16 values. Then the size of the vector is (16, 256) \nThe 3 vectors created in the previous 3 steps are then concatenated to form a fixed size vector which will be the input of the fully connected network. \nWhat are the benefits of SPP ? \n      SPP is able to generate a fixed- length output regardless of the input size \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8953482472456261,
        0.8537492997415006
      ],
      "excerpt": "SPP can pool features extracted at variable scales thanks to the flexibility of input scales \nPaNet: for aggregate different backbone levels \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9287739609965968,
        0.9640205655950159
      ],
      "excerpt": "To correct this problem, PaNet has introduced an architecture that allows better propagation of layer information from bottom to top or top to bottom.that the information of the first layer is added in layer p5, and propagated in layer N5 . This is a shortcut to propagate low level information to the top. \nIn the original implementation of PaNet, the current layer and information from a previous layer is added together to form a new vector. In the YoloV4 implementation, a modified version is used where the new vector is created by concatenating the input and the vector from a previous layer. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9874837389532278
      ],
      "excerpt": "The role of the head in the case of a one stage detector is to perform dense prediction. The dense prediction is the final prediction which is composed of a vector containing the coordinates of the predicted bounding box (center, height, width), the confidence score of the prediction and the label. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9941229597083713
      ],
      "excerpt": "where b and bgt denote the central points of B and Bgt,p(.) is the Euclidean distance, c is the diagonal length of the smallest enclosing box covering the two boxes, \u03b1 is a positive trade-off parameter, and v measures the consistency of aspect ratio. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9333040391322568
      ],
      "excerpt": "where h is the height of the bounding box and w is the width. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9819319044056993
      ],
      "excerpt": "Batch Normalization does not perform when the batch size becomes small. The estimate of the standard deviation and mean is biased by the sample size. The smaller the sample size, the more likely it is not to represent the completeness of the distribution. To solve this problem, Cross mini Batch Normalization is used, which uses estimates from recent batches to improve the quality of each batch\u2019s estimate. A challenge of computing statistics over multiple iterations is that the network activations from different iterations are not comparable to each other due to changes in network weights. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9459403368480582,
        0.9418922972391787
      ],
      "excerpt": "where \u03b5 is a small constant added for numerical stability, and \u03bct(\u03b8t) and \u03c3t(\u03b8t) are the mean and variance computed for all the examples from the current mini-batch. \nwhereas in Cross mini Batch Normalization the mean and variance are calculated from the previous N means and variances and approximated using Taylor formulae to express them as a function of the parameters \u03b8t rather than \u03b8t-N. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9098907650865481,
        0.9410979006631593,
        0.985907897607567,
        0.8979411005071259,
        0.9930596094045095
      ],
      "excerpt": "Neural networks work better if they are able to generalize better, to do this we use regularization techniques such as dropout which consists in deactivating certain neurons during training. These methods generally improve accuracy during the test phase. \nNevertheless the dropout drops features randomly, this method works well for fully connected layers but is not efficient for convoluted layers where features are spatially correlated. \nIn DropBlock, features in a block (i.e. a contiguous region of a feature map), are dropped together. As DropBlock discards features in a correlated area, the networks must look elsewhere for evidence to fit the data. \nMosaic data augmentation \nMosaic data augmentation combines 4 training images into one in certain ratios. This allows for the model to learn how to identify objects at a smaller scale than normal. It also encourages the model to localize different types of images in different portions of the frame. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9804935505163066
      ],
      "excerpt": "Self-Adversarial Training (SAT) represents a new data augmentation technique that operates in 2 forward backward stages. In the 1st stage the neural network alters the original image instead of the network weights. In this way the neural network executes an adversarial attack on itself, altering the original image to create the deception that there is no desired object on the image. In the 2nd stage, the neural network is trained to detect an object on this modified image in the normal way with original label before add noise to the image. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9263782497439761,
        0.9027168381659547,
        0.9825967109537662,
        0.9905111455674334
      ],
      "excerpt": "Eliminate grid sensitivity the equation bx = \u03c3(tx)+ cx,by =\u03c3(ty)+cy, where cx and cy a real ways whole numbers, is used in YOLOv3 for evaluating the object coordinates, therefore, extremely high tx absolute values are required for the bx value approaching the cx or cx + 1 values. We solve this problem through multiplying the sigmoid by a factor exceeding 1.0, so eliminating the effect of grid on which the object is undetectable. \nUsing multiple anchors for a single ground truth \nWe predict several boxes, because it is difficult for a convolution network to predict directly a set of boxes associated with objects of different ratio, that\u2019s why we use anchors that divide the image space according to different strategies. \nFrom the features map created by the convolution layers, we create many anchor boxes of different ratios in order to be able to represent objects of any size, we then decide thanks to the IOU to assign some boxes to an object or a background according to the threshold below. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9968961816973724
      ],
      "excerpt": "A cosine function is used to update the learning rate, the advantage of the cosine function is that it is cyclic allowing to get out of the local minima more easily than the step method or SGD. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8921856437827359
      ],
      "excerpt": "SAM simply consists of applying two separate transforms to the output feature map of a convolutional layer, a Max Pooling and an Avg Pooling. The two features are concatenated and then passed in a convoluted layer, before applying a sigmoid function that will highlight where the most important features are located. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8825735497207557
      ],
      "excerpt": "NMS (Non-Maximum Suppression) is used to remove the boxes that represent the same object while keeping the one that is the most precise compared to the real bounding box. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9855257020960005
      ],
      "excerpt": "where b and bgt denote the central points of B and Bgt, \u03c1(\u00b7) is the Euclidean distance, and c is the diagonal length of the smallest enclosing box covering the two boxes. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ravindra579/object-detection_yolov4/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Fri, 10 Dec 2021 12:47:27 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ravindra579/object-detection_yolov4/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "ravindra579/object-detection_yolov4",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/ravindra579/object-detection_yolov4/main/video.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8108508467517774
      ],
      "excerpt": "If you want to download weights the link in \"weights.txt\" file you can check that otherwise if you want to train on your own coustom dataset you can use man losses like ciou losses giou loss if you want reference you can check it in \"losses.py\" file \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8981524067900627
      ],
      "excerpt": "Eliminate grid sensitivity \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ravindra579/object-detection_yolov4/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "object-detection_yolov4",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "object-detection_yolov4",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "ravindra579",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ravindra579/object-detection_yolov4/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Fri, 10 Dec 2021 12:47:27 GMT"
    },
    "technique": "GitHub API"
  }
}