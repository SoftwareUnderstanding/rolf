{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- I would like to thank the authors of the paper for the amazing public dataset found [here](http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/).\n\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1611.07004",
      "https://arxiv.org/abs/1505.04597",
      "https://arxiv.org/abs/1611.07004](https://arxiv.org/abs/1611.07004))\n\nIt was made as the final project for CS 763 - **Computer Vision** course in Spring 2019 at Indian Institute of Technology (IIT) Bombay, India.\n\n## Abstract\n\npix2pix uses a conditional generative adversarial network to efficiently design a general-purpose image-to-\nimage translation system. Image-to-image translation involves learning a mapping from images from one\ndistribution to corresponding images in another distribution. Many kinds of problems can be viewed as an\nimage-to-image translation problem, including image colorization, edges to object visualization, style transfer *etc*.\n\nFor example, an output for Satellite-to-Maps view would be\n\n![1.png](/assets/results/satellite-to-map/1.png)\n\n### Note\n\nAll the image output files in this project will be of the above format *i.e.*\n\n<p align=\"center\">[Source - Target_Ground_Truth - Target_Generated]</p>\n\n## Datasets\n\nI had tested this project with the following datasets released public by the authors (link in [Acknowledgements](#acknowledgements) section)\n\n- Facades\n- Maps (satellite-to-map)\n- Maps (map-to-satellite)\n\n## Getting Started\n\nFollow the instructions below to get our project running on your local machine.\n\n1. Clone the repository and make sure you have prerequisites below to run the code.\n2. Run `python src/main.py --help` to see the various options available to specify.\n3. To train the model, run the command `python src/main.py ...` along with the flags. For example, to run on the maps (map-to-satellite) dataset, you may run\n\n```bash\npython src/main.py --mode train --data_root '../datasets/maps' --num_epochs 100 --data_invert\n```\n\n4. All the outputs will be saved to `src/output/[timestamp]` where `[timestamp]` is the time of start of training.\n\n### Prerequisites\n\n- Python 3.7.1 or above\n\n- [PyTorch](https://pytorch.org/) 1.0.0 or above\n- CUDA 9.1 (or other version corresponding to PyTorch) to utilize any compatible GPU present for faster training\n\n[The code is tested to be working with the above versions on a Windows 10 machine with GTX 1070. It may also work for other lower versions.]\n\n## Architecture\n\nCode of the various modules can be found in the [modules.py](/src/modules.py) file.\n\n- **Generator**\n  - I had used a `U-Net` ([https://arxiv.org/abs/1505.04597](https://arxiv.org/abs/1505.04597)) like architecture for the generator, which is simply an encoder-decoder architecture with skip connections in between them.\n\n![U-Net](/assets/architecture/U-Net.png)\n\n<p align=\"center\">[Image Courtesy: Author's paper]</p>\n\n  - Precisely, the encoder channels vary as  `in_channels -> 64 -> 128 -> 256 -> 512 -> 512 -> 512 -> 512` and the decoder's channel sizes vary accordingly.\n- **Discriminator**\n  - For the discriminator, a `PatchGAN` is used. A `PatchGAN` is similar to a common discriminator, except that it tries to classify each patch of N \u00d7 N size whether it is real or fake.\n  - In our case, we take N = 70\u200b. This is in our code achieved by using a Convolutional network whose receptive field is 70 on the input image to the discriminator. Mathematically, this can be checked to be equivalent to what has been described in the paper.\n  - The channel sizes in our `PatchGAN ` vary as `in_channels -> 64 -> 128 -> 256 -> 512 -> out_channels`.\n\n- **Hyperparameters**\n\n  - I had used the default parameters mentioned in the code of `main.py`. You may easily test on other values by suitably changing the flags.\n\n## Results\n\nAll the results shown here are on <u>test data</u>.\n\n### Map-to-Satellite\n\n| ![1.png](/assets/results/maps-to-satellite/1.png) | ![2.png](/assets/results/maps-to-satellite/2.png) |\n| ------------------------------------------------- | ------------------------------------------------- |\n| ![3.png](/assets/results/maps-to-satellite/3.png) | ![4.png](/assets/results/maps-to-satellite/4.png) |\n| ![5.png](/assets/results/maps-to-satellite/5.png) | ![6.png](/assets/results/maps-to-satellite/6.png) |\n\n### Satellite-to-Map\n\n| ![1.png](/assets/results/satellite-to-map/1.png) | ![2.png](/assets/results/satellite-to-map/2.png) |\n| ------------------------------------------------- | ------------------------------------------------- |\n| ![3.png](/assets/results/satellite-to-map/3.png) | ![4.png](/assets/results/satellite-to-map/4.png) |\n| ![5.png](/assets/results/satellite-to-map/5.png) | ![6.png](/assets/results/satellite-to-map/6.png) |\n\n### Facades\n\n| ![1.png](/assets/results/facades/1.png) | ![2.png](/assets/results/facades/2.png) |\n| ------------------------------------------------- | ------------------------------------------------- |\n| ![3.png](/assets/results/facades/3.png) | ![4.png](/assets/results/facades/4.png) |\n| ![5.png](/assets/results/facades/5.png) | ![6.png](/assets/results/facades/6.png) |\n\n\n\nAs a sanity check, I would like to point out that on the training set, the model was able to give good outputs as shown below, indicating that it's capacity was quite sufficient.\n\n|      |      |\n| ---- | ---- |\n| ![train_1.png](/assets/results/facades/train_1.png) | ![train_2.png](/assets/results/facades/train_2.png) |\n\n### Plots\n\nFor the Facades dataset,\n\n|             Generator Loss [Training]             |           Discriminator Loss [Training]           |\n| :-----------------------------------------------: | :-----------------------------------------------: |\n| ![g_loss.png](/assets/results/facades/g_loss.png) | ![d_loss.png](/assets/results/facades/d_loss.png) |\n\n## Authors\n\n* **Vamsi Krishna Reddy Satti** - [vamsi3](https://github.com/vamsi3)\n\n## Acknowledgements\n\n- I would like to thank the authors of the paper for the amazing public dataset found [here](http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/).\n\n## License\n\nThis project is licensed under MIT License - please see the [LICENSE](LICENSE) file for details.\n",
      "https://arxiv.org/abs/1505.04597](https://arxiv.org/abs/1505.04597)) like architecture for the generator, which is simply an encoder-decoder architecture with skip connections in between them.\n\n![U-Net](/assets/architecture/U-Net.png)\n\n<p align=\"center\">[Image Courtesy: Author's paper]</p>\n\n  - Precisely, the encoder channels vary as  `in_channels -> 64 -> 128 -> 256 -> 512 -> 512 -> 512 -> 512` and the decoder's channel sizes vary accordingly.\n- **Discriminator**\n  - For the discriminator, a `PatchGAN` is used. A `PatchGAN` is similar to a common discriminator, except that it tries to classify each patch of N \u00d7 N size whether it is real or fake.\n  - In our case, we take N = 70\u200b. This is in our code achieved by using a Convolutional network whose receptive field is 70 on the input image to the discriminator. Mathematically, this can be checked to be equivalent to what has been described in the paper.\n  - The channel sizes in our `PatchGAN ` vary as `in_channels -> 64 -> 128 -> 256 -> 512 -> out_channels`.\n\n- **Hyperparameters**\n\n  - I had used the default parameters mentioned in the code of `main.py`. You may easily test on other values by suitably changing the flags.\n\n## Results\n\nAll the results shown here are on <u>test data</u>.\n\n### Map-to-Satellite\n\n| ![1.png](/assets/results/maps-to-satellite/1.png) | ![2.png](/assets/results/maps-to-satellite/2.png) |\n| ------------------------------------------------- | ------------------------------------------------- |\n| ![3.png](/assets/results/maps-to-satellite/3.png) | ![4.png](/assets/results/maps-to-satellite/4.png) |\n| ![5.png](/assets/results/maps-to-satellite/5.png) | ![6.png](/assets/results/maps-to-satellite/6.png) |\n\n### Satellite-to-Map\n\n| ![1.png](/assets/results/satellite-to-map/1.png) | ![2.png](/assets/results/satellite-to-map/2.png) |\n| ------------------------------------------------- | ------------------------------------------------- |\n| ![3.png](/assets/results/satellite-to-map/3.png) | ![4.png](/assets/results/satellite-to-map/4.png) |\n| ![5.png](/assets/results/satellite-to-map/5.png) | ![6.png](/assets/results/satellite-to-map/6.png) |\n\n### Facades\n\n| ![1.png](/assets/results/facades/1.png) | ![2.png](/assets/results/facades/2.png) |\n| ------------------------------------------------- | ------------------------------------------------- |\n| ![3.png](/assets/results/facades/3.png) | ![4.png](/assets/results/facades/4.png) |\n| ![5.png](/assets/results/facades/5.png) | ![6.png](/assets/results/facades/6.png) |\n\n\n\nAs a sanity check, I would like to point out that on the training set, the model was able to give good outputs as shown below, indicating that it's capacity was quite sufficient.\n\n|      |      |\n| ---- | ---- |\n| ![train_1.png](/assets/results/facades/train_1.png) | ![train_2.png](/assets/results/facades/train_2.png) |\n\n### Plots\n\nFor the Facades dataset,\n\n|             Generator Loss [Training]             |           Discriminator Loss [Training]           |\n| :-----------------------------------------------: | :-----------------------------------------------: |\n| ![g_loss.png](/assets/results/facades/g_loss.png) | ![d_loss.png](/assets/results/facades/d_loss.png) |\n\n## Authors\n\n* **Vamsi Krishna Reddy Satti** - [vamsi3](https://github.com/vamsi3)\n\n## Acknowledgements\n\n- I would like to thank the authors of the paper for the amazing public dataset found [here](http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/).\n\n## License\n\nThis project is licensed under MIT License - please see the [LICENSE](LICENSE) file for details.\n"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8728059224385537
      ],
      "excerpt": "<p align=\"center\">[Source - Target_Ground_Truth - Target_Generated]</p> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9971137948962174
      ],
      "excerpt": "<p align=\"center\">[Image Courtesy: Author's paper]</p> \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/vamsi3/pix2pix",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-05-05T18:15:20Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-12-04T11:03:08Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.938120730582906,
        0.8436202567948828,
        0.8024141473305035
      ],
      "excerpt": "This project implements a image-to-image translation method as described in the paper - Image-to-Image Translation with Conditional Adversarial Networks by Phillip Isola et al. (arXiv:1611.07004) \nIt was made as the final project for CS 763 - Computer Vision course in Spring 2019 at Indian Institute of Technology (IIT) Bombay, India. \npix2pix uses a conditional generative adversarial network to efficiently design a general-purpose image-to- \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8877652775186881
      ],
      "excerpt": "All the image output files in this project will be of the above format i.e. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8941634811054324
      ],
      "excerpt": "I had tested this project with the following datasets released public by the authors (link in Acknowledgements section) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8525447369347303,
        0.8525447369347303
      ],
      "excerpt": "Maps (satellite-to-map) \nMaps (map-to-satellite) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9750058908575099,
        0.9826335290808225
      ],
      "excerpt": "For the discriminator, a PatchGAN is used. A PatchGAN is similar to a common discriminator, except that it tries to classify each patch of N \u00d7 N size whether it is real or fake. \nIn our case, we take N = 70\u200b. This is in our code achieved by using a Convolutional network whose receptive field is 70 on the input image to the discriminator. Mathematically, this can be checked to be equivalent to what has been described in the paper. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8571687007368816
      ],
      "excerpt": "All the results shown here are on <u>test data</u>. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9470999456013204
      ],
      "excerpt": "As a sanity check, I would like to point out that on the training set, the model was able to give good outputs as shown below, indicating that it's capacity was quite sufficient. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "An implementation of the the paper \"Image-to-Image Translation with Conditional Adversarial Networks\" by Isola et al.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/vamsi3/pix2pix/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Mon, 13 Dec 2021 21:06:16 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/vamsi3/pix2pix/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "vamsi3/pix2pix",
    "technique": "GitHub API"
  },
  "invocation": [
    {
      "confidence": [
        0.8257715355050855
      ],
      "excerpt": "All the results shown here are on <u>test data</u>. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/vamsi3/pix2pix/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 Vamsi Krishna\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "pix2pix: Image-to-Image Translation with Conditional Adversarial Networks",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "pix2pix",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "vamsi3",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/vamsi3/pix2pix/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Python 3.7.1 or above\n\n- [PyTorch](https://pytorch.org/) 1.0.0 or above\n- CUDA 9.1 (or other version corresponding to PyTorch) to utilize any compatible GPU present for faster training\n\n[The code is tested to be working with the above versions on a Windows 10 machine with GTX 1070. It may also work for other lower versions.]\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Mon, 13 Dec 2021 21:06:16 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "pix2pix",
      "image-to-image-translation",
      "gan",
      "computer-vision",
      "image-generation",
      "deep-learning",
      "pytorch",
      "implementation-of-research-paper"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Follow the instructions below to get our project running on your local machine.\n\n1. Clone the repository and make sure you have prerequisites below to run the code.\n2. Run `python src/main.py --help` to see the various options available to specify.\n3. To train the model, run the command `python src/main.py ...` along with the flags. For example, to run on the maps (map-to-satellite) dataset, you may run\n\n```bash\npython src/main.py --mode train --data_root '../datasets/maps' --num_epochs 100 --data_invert\n```\n\n4. All the outputs will be saved to `src/output/[timestamp]` where `[timestamp]` is the time of start of training.\n\n",
      "technique": "Header extraction"
    }
  ]
}