{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1406.1078>`_.\n\nImplementation\n--------------\n\nThe autoencoder is implemented with `Tensorflow <http://tensorflow.org>`_. Specifically, it uses a bidirectional LSTM (but it can be configured to use a simple LSTM instead"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/erickrf/autoencoder",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-02-01T19:43:23Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-01T05:44:16Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9503293012189127,
        0.9901557254411704,
        0.9029726735006319,
        0.8468807713951713
      ],
      "excerpt": "This is an implementation of a recurrent neural network that reads an input text, encodes it in its memory cell, and then reconstructs the inputs. This is basically the idea presented by Sutskever et al. (2014) &lt;https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf&gt;_ \nWhy? The point of training an autoencoder is to make an RNN learn how to compress a relatively long sequence into a limited, dense vector. Once we have a fixed-size representation of a sentence, there's a lot we can do with it. \nWe can work with single sentences (classifying them with respect to sentiment, topic, authorship, etc), or more than one at a time (checking for similarities, contradiction, question/answer pairs, etc.) Another successful application is to encode one sentence in one language and use a different autoencoder to decode it into another language, e.g. Cho et al. (2014) &lt;https://arxiv.org/abs/1406.1078&gt;_. \nThe autoencoder is implemented with Tensorflow &lt;http://tensorflow.org&gt;_. Specifically, it uses a bidirectional LSTM (but it can be configured to use a simple LSTM instead). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9902479263995774,
        0.9049189337297576
      ],
      "excerpt": "Then, in the decoder step, a special symbol GO is read, and the output of the LSTM is fed to a linear layer with the size of the vocabulary. The chosen word (i.e., the one with the highest score) is the next input to the decoder. This goes on until a special symbol EOS is produced. \nThe weights of the encoder and decoder are shared. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9617036211881373,
        0.9759887697411636
      ],
      "excerpt": "Even for small vocabularies (a few thousand words), training the network over all possible outputs at each time step is very expensive computationally. Instead, we just sample the weights of 100 possible words. During inference time, there is no way around it, but the computational cost is much lesser. \nFor better decoder performance, a beam search is preferable to the currently used greedy choice. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Text autoencoder with LSTMs",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/erickrf/autoencoder/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 88,
      "date": "Wed, 08 Dec 2021 07:33:31 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/erickrf/autoencoder/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "erickrf/autoencoder",
    "technique": "GitHub API"
  },
  "invocation": [
    {
      "confidence": [
        0.8809961465950163,
        0.9159080205449199,
        0.8351028095241182,
        0.9117089022415626
      ],
      "excerpt": "prepare-data.py: reads a text file and create numpy files that can be used to train an autoencoder \ntrain-autoencoder.py: train a new autoencoder model \ninteractive.py: run a trained autoencoder that reads input from stdin. It can be fun to test the boundaries of your trained model :) \ncodify-sentences.py: run the encoder part of a trained autoencoder on sentences read from a text file. The encoded representation is saved as a numpy file \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/erickrf/autoencoder/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Text Autoencoder",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "autoencoder",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "erickrf",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/erickrf/autoencoder/blob/master/README.rst",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 235,
      "date": "Wed, 08 Dec 2021 07:33:31 GMT"
    },
    "technique": "GitHub API"
  }
}