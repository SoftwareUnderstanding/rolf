{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* [JayParks/tf-seq2seq](https://github.com/JayParks/tf-seq2seq): Example sequence-to-sequence implementation in tensorflow\n* [Omorfi](https://github.com/flammie/omorfi): Finnish open source morphology tool\n* [FinnTreeBank](http://www.ling.helsinki.fi/kieliteknologia/tutkimus/treebank/): Source for datasets\n* [Finnish Dependency Parser](http://bionlp.utu.fi/finnish-parser.html): Source for datasets\n\t\t\n---\nJesse Myrberg (jesse.myrberg@gmail.com)",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1406.1078",
      "https://arxiv.org/abs/1409.0473",
      "https://arxiv.org/abs/1508.04025"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* [JayParks/tf-seq2seq](https://github.com/JayParks/tf-seq2seq): Example sequence-to-sequence implementation in tensorflow\n* [Omorfi](https://github.com/flammie/omorfi): Finnish open source morphology tool\n* [FinnTreeBank](http://www.ling.helsinki.fi/kieliteknologia/tutkimus/treebank/): Source for datasets\n* [Finnish Dependency Parser](http://bionlp.utu.fi/finnish-parser.html): Source for datasets\n\t\t\n---\nJesse Myrberg (jesse.myrberg@gmail.com)",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8511535834563826
      ],
      "excerpt": "multimediaopetusmateriaalia --&gt; multi#:media#:opetus#:materiaali \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jmyrberg/finnlem",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-07-27T21:02:12Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-11-05T10:56:12Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9864389622930777
      ],
      "excerpt": "finnlem is a neural network based lemmatizer model for Finnish language. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9747907574127315,
        0.9882583231544912
      ],
      "excerpt": "The model is a tensorflow implementation of a sequence-to-sequence (Seq2Seq) recurrent neural network model.  \nThis repository contains the code and data needed for training and making predictions with the model. The datasets contain over 2M samples in total. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8871734156278073,
        0.9846890846306543,
        0.9225628040109568
      ],
      "excerpt": "* Sequence-to-sequence model features: Bahdanau and Luong attention, residual connections, dropout, beamsearch decoding, ... \nThe following is a simple example of using some of the features in the Python API. \nSee more detailed descriptions of functions and parameters available from the source code documentation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9718252791980313
      ],
      "excerpt": ": Documents to fit in dictionary \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8202818278554551
      ],
      "excerpt": ": Fit characters of each document \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8747930897866267,
        0.9387632038353994
      ],
      "excerpt": ": Create a new model \nmodel = Seq2Seq(model_dir='./data/models/lemmatizer, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "pred_docs = model.decode(test_docs) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9387632038353994
      ],
      "excerpt": "        --model-dir ./data/models/lemmatizer \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9387632038353994
      ],
      "excerpt": "        --model-dir ./data/models/lemmatizer \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8637756697427481,
        0.907975351198766,
        0.9343157981301738,
        0.9364196615367272,
        0.9767140608619151,
        0.9594533813064795,
        0.9762052372736639,
        0.8175171525928424
      ],
      "excerpt": "where model_dir is the Seq2Seq model checkpoint folder. \nThe model was originally created for summarizing the Finnish news, by using news contents as the sources, and news titles as the targets. \nThis proved to be quite a difficult task due to rich morphology of Finnish language, and lack of computational resources. My first \napproach for tackling the morphology was to use the base forms for each word, which is what the model in this package does by default. However,  \nusing this model to convert every word to their base form ended up being too slow to be used as an input for the second model in real time. \nIn the end, I decided to try the Finnish SnowballStemmer from nltk in order to get the \"base words\",  \nand started training the model with 100k vocabulary. After 36 hours of training with loss decreasing very slowly, I decided to stop, and keep this package as a character-level lemmatizer. \nHowever, in model_wrappers.py, there is a global variable DOC_HANDLER_FUNC, which enables one to change the preprocessing method easily from \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Neural network based lemmatizer for Finnish language",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jmyrberg/finnlem/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Sat, 11 Dec 2021 00:39:14 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jmyrberg/finnlem/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "jmyrberg/finnlem",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You should have the latest versions for (as of 7/2017):\n* keras\n* nltk\n* numpy\n* pandas\n* tensorflow (1.3.0 or greater, with CUDA 8.0 and cuDNN 6.0 or greater)\n* unidecode\n* sacremoses ([see issue regarding this](https://github.com/jmyrberg/finnlem/issues/1))\n\nAfter this, clone this repository to your local machine.\n\nUpdate 10.9.2020: You could also try to first clone and then run `pip install -r requirements.txt` at the root of this repository. This will install the latest versions of the required packages automatically, but notice that the very latest versions of some of the packages might nowadays be incompatible with the source code provided here. Feel free to make a pull request with fixed versions of the packages, in case you manage to run the source code successfully :)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8837680365796365
      ],
      "excerpt": "python -m dict_train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8837680365796365
      ],
      "excerpt": "python -m model_train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8837680365796365
      ],
      "excerpt": "python -m model_decode \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8768786811016962
      ],
      "excerpt": "* one source document per line, or \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9117802601181192
      ],
      "excerpt": "To use tensorboard, run command python -m tensorflow.tensorboard --logdir=model_dir,  \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.803994963397163
      ],
      "excerpt": "from dictionary import Dictionary \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "from model_wrappers import Seq2Seq \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8501856166027086
      ],
      "excerpt": "model = Seq2Seq(model_dir='./data/models/lemmatizer, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8138146952991508
      ],
      "excerpt": ": Train 100 batches, save checkpoint every 25th batch \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8366907471965758
      ],
      "excerpt": "    loss,global_step = model.train(source_docs, target_docs, save_every_n_batch=25) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8366710481601165
      ],
      "excerpt": "The following demonstrates the usage of command line for training and predicting from files. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8444002201815901,
        0.9460448278710993
      ],
      "excerpt": "        --dict-train-path ./data/dictionaries/lemmatizer.vocab \nThe dictionary train path file(s) should contain one document per line (example). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8393717063612749
      ],
      "excerpt": "        --model-dir ./data/models/lemmatizer \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9107179357870361,
        0.8959237548604942
      ],
      "excerpt": "        --train-data-path ./data/datasets/lemmatizer_train.csv \nThe model train and validation data path file(s) should contain one source and target document per line,  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8393717063612749,
        0.9115477394732091,
        0.8532107804144106,
        0.8995889829381496
      ],
      "excerpt": "        --model-dir ./data/models/lemmatizer \n        --test-data-path ./data/datasets/lemmatizer_test.csv \n        --decoded-data-path ./data/decoded/lemmatizer_decoded.csv \nThe model test data path file(s) should contain either: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8075602766905654
      ],
      "excerpt": "* one source and target document per line, separated by a comma (example) \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jmyrberg/finnlem/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "finnlem",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "finnlem",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "jmyrberg",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jmyrberg/finnlem/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 9,
      "date": "Sat, 11 Dec 2021 00:39:14 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "neural-network",
      "seq2seq",
      "finnish",
      "natural-language-processing",
      "nlp",
      "news",
      "tensorflow",
      "lemmatization"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Three-steps are required in order to get from zero to making predictions with a trained model:\n\n1. **Dictionary training**: Dictionary is created from training documents, which are processed the same way as the Seq2Seq model inputs later on.\n\tDictionary handles vocabulary/integer mappings required by Seq2Seq.\n2. **Model training**: Seq2Seq model is trained in batches with training documents that contain source and target.\n3. **Model decoding**: Unseen source documents are fed into Seq2Seq model, which makes predictions on the target.\n\n",
      "technique": "Header extraction"
    }
  ]
}