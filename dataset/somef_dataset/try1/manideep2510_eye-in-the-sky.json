{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[1] [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/pdf/1505.04597.pdf), Olaf Ronneberger, Philipp Fischer, and Thomas Brox\n\n[2] [Pyramid Scene Parsing Network](https://arxiv.org/pdf/1612.01105.pdf), Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, Jiaya Jia\n\n[3] [A 2017 Guide to Semantic Segmentation with Deep Learning](http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review), Sasank Chilamkurthy\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9182501781808255
      ],
      "excerpt": "Satellite Image Classification, InterIIT Techmeet 2018, IIT Bombay. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "Modified U-Net: \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/manideep2510/eye-in-the-sky",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-12-04T14:08:22Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-18T14:46:03Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[Remote sensing](https://www.usgs.gov/faqs/what-remote-sensing-and-what-it-used) is the science of obtaining information about objects or areas from a distance, typically from aircraft or satellites.\n\nWe realized the problem of satellite image classification as a [semantic segmentation](https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf) problem and built semantic segmentation algorithms in deep learning to tackle this.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9786521021487204,
        0.8742532163434806,
        0.9058494518498849,
        0.9352971656115979,
        0.8338051337885494,
        0.9007002949089897,
        0.9601816985715046
      ],
      "excerpt": "This repository contains the implementation of two algorithms namely U-Net: Convolutional Networks for Biomedical \nImage Segmentation and Pyramid Scene Parsing Network modified for the problem of satellite image classification. \nmain_unet.py : Python code for training the algorithm with U-Net architecture including the encoding of the ground truths. \nunet.py : Contains our implementation of U-Net layers. \ntest_unet.py : Code for Testing, calculating accuracies, calculating confusion matrices for training and validation and saving predictions by the U-Net model on training, validation and testing images. \nInter-IIT-CSRE : Contains all the training, validation ad testing data. \nComparison_Test.pdf : Side by side comparison of the test data with the U-Net model predictions on the data. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8055470213484217
      ],
      "excerpt": "plots : Accuracy and loss plots for training and validation for U-Net architecture. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9397966207209176,
        0.9706121789736457
      ],
      "excerpt": "PSPNet : Contains training files for implementation of PSPNet algorithm to satellite image classification. \nYou might get an error xrange is not defined while running our code. This error is not due to errors in our code but due to not up to date python package named libtiff (some parts of the source code of the package are in python2 and some are in python3) which we used to read the dataset which in which the images are in .tif format. We were not able to use other libraries like openCV or PIL to read the images as they are not properly supporting to read the 4-channel .tif images. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8763200269852336
      ],
      "excerpt": "Go to the file in the source code of the library from where the error arises (the file name will be displayed in the terminal when it is showing the error) and replace all the xrange() (python2) functions in the file to range() (python3). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9786947927791603,
        0.9619968259719671,
        0.9303401826428322
      ],
      "excerpt": "1. What this project is about,  \n2. Architectures we have used and experimented with and  \n3. Some novel training strategies we have used in the project \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8001765575794383,
        0.9249964677408222
      ],
      "excerpt": "The ground truths provided are 3 channel RGB images. In the current dataset, there are only 9 unique RGB values in the ground truths as there are 9 classes that are to be classified. These 9 different RGB values are one-hot encoded to generate a 9 channel encoded ground truth with each channel representing a particular class. \nBelow is the encoding scheme \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9764067679175656,
        0.9944886222739518
      ],
      "excerpt": "So instead of training on the RGB values of the ground truth we have converted them into the one-hot values of different classes. This approach yielded us a validation accuracy of 85% and training accuracy of 92% compared to 71% validation accuracy and 65% training accuracy when we were using the RGB ground truth values for training. \nThis might be due to decrease in variance and mean of the ground truth of training data as it acts as an effective normalization technique. The better performance of this training technique is also because the model is giving an output with 9 feature maps each map indicating a class, i.e, this training technique is acting as if the model is trained on each of the 9 classes separately for some extent(but here definitely the prediction on one channel which corresponds to a particular class depends on others). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.873982399751785
      ],
      "excerpt": "Best suitable for cases where the data is comparatively less, especially in the current case, training on 14 images \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9582864839267956,
        0.922306896871969
      ],
      "excerpt": "Batch Normalization before every downsampling and upsampling layers to decrease the variance and mean of the feature maps. \nUsed deconvolution layers instead of conv layers in the upsampling part of the UNet, but the results weren't good \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9521157844798888,
        0.8490037945672047,
        0.9946458402334475,
        0.9673171292166411
      ],
      "excerpt": "The reason we have considered only one image (14th image) as validation set is because it is one of the smallest images in the dataset and we do not want to leave less data fo training as the dataset is pretty small. The validation set (14th image) we have considered does not have 3 classes (Bare soil, Rail, Swimmimg poll) in it which have pretty high training accuracies. The validation accuracy would have been better if we would have considered a image with all the classes in it(No image in the dataset contains all the classs, there is atleast one class missing in all the images). \nThe Strided Cropping: \nTo have sufficient training data from the given high definition images cropping is required to train the classifier which has about 31M parameters of our U-Net implementation. The crop size of 64x64 we find under-representation of the individual classes and the geometry and continuity of the objects is lost, decreasing the field of view of the convolutions. \nUsing a cropping window of 128x128 pixels with a stride of 32 resultant of 15887 training 414 validation images. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9426829139661539,
        0.9798517168672641,
        0.9735228191953563
      ],
      "excerpt": "Before cropping, the dimensions of training images are converted into multiples of stride for convenience during strided cropping. \nFor the cases where the no. of crops is not the multiple of the image dimensions we initially tried zero padding , we realised that adding padding will add unwanted artefacts in the form of black pixels in training and test images leading to training on false data and image boundary. \nAlternatively we have correctly changed the image dimensions by adding extra pixels in the right most side and bottom of the image. So we padded the difference from the left most part of the image to it\u2019s right deficit end and similarly for the top and bottom of the image. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9909003612076035,
        0.9734297679813824
      ],
      "excerpt": "Our model is able to predict some classes which a human annotator wasn't able to. The un-identifiable classes in the images are labeled as white pixels by the human annotator. Our model is able to predict some of these white pixels correctly as some class, but this caused a decrease in the overall accuracy as the white pixels are considered as a seperate class by the model. \nHere the model is able to predict the white pixels as a building which is correct and can be clearly seen in the input image \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8489995837267176
      ],
      "excerpt": "Kappa Coefficients With and Without considering the unclassified pixels \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8635516211844516
      ],
      "excerpt": "Overall Accuracy With and Without considering the unclassified pixels \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": " Satellite Image Classification using semantic segmentation methods in deep learning",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/manideep2510/eye-in-the-sky/releases",
    "technique": "GitHub API"
  },
  "faq": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Our results on PSPNet for Satellite image classification:\n\nTraining Accuracy - 49%\nValidation Accuracy - 60%\n\n*Reasons:*\n\n- Huge number of parameters (46M trainable params)\n- Contains a Resnet which demands more data to learn the features properly.\n- Under-fitting - We currently have very less data even after the strided cropping (15k images), so the PSPNet could not learn to segment (classify) the satellite images\n\n",
      "technique": "Header extraction"
    }
  ],
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 69,
      "date": "Sun, 05 Dec 2021 13:27:32 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/manideep2510/eye-in-the-sky/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "manideep2510/eye-in-the-sky",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8147577243198572
      ],
      "excerpt": "We are providing some reasonably good pre-trained weights here so that the users don't need to train from scratch. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8348356355815393,
        0.8401216664690632
      ],
      "excerpt": "test_unet.py : Code for Testing, calculating accuracies, calculating confusion matrices for training and validation and saving predictions by the U-Net model on training, validation and testing images. \nInter-IIT-CSRE : Contains all the training, validation ad testing data. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8131251227909405
      ],
      "excerpt": "train_predictions : U-Net Model predictions on training and validation images. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8890818307099057
      ],
      "excerpt": "    <img src=\"images_for_doc/unet.png\" width=\"640\" /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8890818307099057
      ],
      "excerpt": "    <img src=\"images_for_doc/pspnet.png\" width=\"850\" /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8890818307099057
      ],
      "excerpt": "    <img src=\"images_for_doc/table_onehot.png\" width=\"640\" /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8890818307099057
      ],
      "excerpt": "    <img src=\"images_for_doc/channel_classes.png\" width=\"900\" /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8181342644212009
      ],
      "excerpt": "Each satelite images in the folder sat contains 4 channels namely R (Band 1),G (Band 2),B (Band 3) and NIR (Band 4). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9102880956794563
      ],
      "excerpt": "Training Example 1: Image '2.tif' from training data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8890818307099057
      ],
      "excerpt": "    <img src=\"images_for_doc/train1.png\" width=\"825\" /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9102880956794563
      ],
      "excerpt": "Training Example 2: Image '4.tif' from training data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8890818307099057
      ],
      "excerpt": "    <img src=\"images_for_doc/train2.png\" width=\"825\" /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8144953211259158
      ],
      "excerpt": "Validation Example: Image '14.tif' from dataset \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8890818307099057
      ],
      "excerpt": "    <img src=\"images_for_doc/val1.png\" width=\"825\" /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8890818307099057
      ],
      "excerpt": "    <img src=\"images_for_doc/unclass_pred.png\" width=\"750\" /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.894321322127165,
        0.9204464347411074,
        0.894321322127165,
        0.9204464347411074
      ],
      "excerpt": "  <img src=\"Test_images/1.png\" width=\"200\" /> \n  <img src=\"Test_outputs/1.jpg\" width=\"200\" />  \n  <img src=\"Test_images/2.png\" width=\"200\" /> \n  <img src=\"Test_outputs/2.jpg\" width=\"200\" /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.894321322127165,
        0.9204464347411074,
        0.9079094081920868,
        0.929129221372287
      ],
      "excerpt": "  <img src=\"Test_images/3.png\" width=\"200\" /> \n  <img src=\"Test_outputs/3.jpg\" width=\"200\" /> \n  <img src=\"Test_images/4.png\" height=\"220\" width=\"200\" /> \n  <img src=\"Test_outputs/4.jpg\" height=\"220\" width=\"200\" />  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.894321322127165,
        0.9204464347411074,
        0.9079094081920868,
        0.929129221372287
      ],
      "excerpt": "  <img src=\"Test_images/5.png\" width=\"200\" /> \n  <img src=\"Test_outputs/5.jpg\" width=\"200\" /> \n  <img src=\"Test_images/6.png\" height=\"250\" width=\"200\" /> \n  <img src=\"Test_outputs/6.jpg\" height=\"250\" width=\"200\" /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.874045737975337
      ],
      "excerpt": "    <img src=\"plots.png\" width=\"800\" /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8890818307099057
      ],
      "excerpt": "    <img src=\"images_for_doc/conf.png\" width=\"800\" /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8890818307099057
      ],
      "excerpt": "    <img src=\"images_for_doc/kappa.png\" width=\"600\" /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8890818307099057
      ],
      "excerpt": "    <img src=\"images_for_doc/overall.png\" width=\"600\" /> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/manideep2510/eye-in-the-sky/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Apache License 2.0",
      "url": "https://api.github.com/licenses/apache-2.0"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'                                 Apache License\\n                           Version 2.0, January 2004\\n                        http://www.apache.org/licenses/\\n\\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\\n\\n   1. Definitions.\\n\\n      \"License\" shall mean the terms and conditions for use, reproduction,\\n      and distribution as defined by Sections 1 through 9 of this document.\\n\\n      \"Licensor\" shall mean the copyright owner or entity authorized by\\n      the copyright owner that is granting the License.\\n\\n      \"Legal Entity\" shall mean the union of the acting entity and all\\n      other entities that control, are controlled by, or are under common\\n      control with that entity. For the purposes of this definition,\\n      \"control\" means (i) the power, direct or indirect, to cause the\\n      direction or management of such entity, whether by contract or\\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\\n      outstanding shares, or (iii) beneficial ownership of such entity.\\n\\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\\n      exercising permissions granted by this License.\\n\\n      \"Source\" form shall mean the preferred form for making modifications,\\n      including but not limited to software source code, documentation\\n      source, and configuration files.\\n\\n      \"Object\" form shall mean any form resulting from mechanical\\n      transformation or translation of a Source form, including but\\n      not limited to compiled object code, generated documentation,\\n      and conversions to other media types.\\n\\n      \"Work\" shall mean the work of authorship, whether in Source or\\n      Object form, made available under the License, as indicated by a\\n      copyright notice that is included in or attached to the work\\n      (an example is provided in the Appendix below).\\n\\n      \"Derivative Works\" shall mean any work, whether in Source or Object\\n      form, that is based on (or derived from) the Work and for which the\\n      editorial revisions, annotations, elaborations, or other modifications\\n      represent, as a whole, an original work of authorship. For the purposes\\n      of this License, Derivative Works shall not include works that remain\\n      separable from, or merely link (or bind by name) to the interfaces of,\\n      the Work and Derivative Works thereof.\\n\\n      \"Contribution\" shall mean any work of authorship, including\\n      the original version of the Work and any modifications or additions\\n      to that Work or Derivative Works thereof, that is intentionally\\n      submitted to Licensor for inclusion in the Work by the copyright owner\\n      or by an individual or Legal Entity authorized to submit on behalf of\\n      the copyright owner. For the purposes of this definition, \"submitted\"\\n      means any form of electronic, verbal, or written communication sent\\n      to the Licensor or its representatives, including but not limited to\\n      communication on electronic mailing lists, source code control systems,\\n      and issue tracking systems that are managed by, or on behalf of, the\\n      Licensor for the purpose of discussing and improving the Work, but\\n      excluding communication that is conspicuously marked or otherwise\\n      designated in writing by the copyright owner as \"Not a Contribution.\"\\n\\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\\n      on behalf of whom a Contribution has been received by Licensor and\\n      subsequently incorporated within the Work.\\n\\n   2. Grant of Copyright License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      copyright license to reproduce, prepare Derivative Works of,\\n      publicly display, publicly perform, sublicense, and distribute the\\n      Work and such Derivative Works in Source or Object form.\\n\\n   3. Grant of Patent License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      (except as stated in this section) patent license to make, have made,\\n      use, offer to sell, sell, import, and otherwise transfer the Work,\\n      where such license applies only to those patent claims licensable\\n      by such Contributor that are necessarily infringed by their\\n      Contribution(s) alone or by combination of their Contribution(s)\\n      with the Work to which such Contribution(s) was submitted. If You\\n      institute patent litigation against any entity (including a\\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\\n      or a Contribution incorporated within the Work constitutes direct\\n      or contributory patent infringement, then any patent licenses\\n      granted to You under this License for that Work shall terminate\\n      as of the date such litigation is filed.\\n\\n   4. Redistribution. You may reproduce and distribute copies of the\\n      Work or Derivative Works thereof in any medium, with or without\\n      modifications, and in Source or Object form, provided that You\\n      meet the following conditions:\\n\\n      (a) You must give any other recipients of the Work or\\n          Derivative Works a copy of this License; and\\n\\n      (b) You must cause any modified files to carry prominent notices\\n          stating that You changed the files; and\\n\\n      (c) You must retain, in the Source form of any Derivative Works\\n          that You distribute, all copyright, patent, trademark, and\\n          attribution notices from the Source form of the Work,\\n          excluding those notices that do not pertain to any part of\\n          the Derivative Works; and\\n\\n      (d) If the Work includes a \"NOTICE\" text file as part of its\\n          distribution, then any Derivative Works that You distribute must\\n          include a readable copy of the attribution notices contained\\n          within such NOTICE file, excluding those notices that do not\\n          pertain to any part of the Derivative Works, in at least one\\n          of the following places: within a NOTICE text file distributed\\n          as part of the Derivative Works; within the Source form or\\n          documentation, if provided along with the Derivative Works; or,\\n          within a display generated by the Derivative Works, if and\\n          wherever such third-party notices normally appear. The contents\\n          of the NOTICE file are for informational purposes only and\\n          do not modify the License. You may add Your own attribution\\n          notices within Derivative Works that You distribute, alongside\\n          or as an addendum to the NOTICE text from the Work, provided\\n          that such additional attribution notices cannot be construed\\n          as modifying the License.\\n\\n      You may add Your own copyright statement to Your modifications and\\n      may provide additional or different license terms and conditions\\n      for use, reproduction, or distribution of Your modifications, or\\n      for any such Derivative Works as a whole, provided Your use,\\n      reproduction, and distribution of the Work otherwise complies with\\n      the conditions stated in this License.\\n\\n   5. Submission of Contributions. Unless You explicitly state otherwise,\\n      any Contribution intentionally submitted for inclusion in the Work\\n      by You to the Licensor shall be under the terms and conditions of\\n      this License, without any additional terms or conditions.\\n      Notwithstanding the above, nothing herein shall supersede or modify\\n      the terms of any separate license agreement you may have executed\\n      with Licensor regarding such Contributions.\\n\\n   6. Trademarks. This License does not grant permission to use the trade\\n      names, trademarks, service marks, or product names of the Licensor,\\n      except as required for reasonable and customary use in describing the\\n      origin of the Work and reproducing the content of the NOTICE file.\\n\\n   7. Disclaimer of Warranty. Unless required by applicable law or\\n      agreed to in writing, Licensor provides the Work (and each\\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\\n      implied, including, without limitation, any warranties or conditions\\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\\n      PARTICULAR PURPOSE. You are solely responsible for determining the\\n      appropriateness of using or redistributing the Work and assume any\\n      risks associated with Your exercise of permissions under this License.\\n\\n   8. Limitation of Liability. In no event and under no legal theory,\\n      whether in tort (including negligence), contract, or otherwise,\\n      unless required by applicable law (such as deliberate and grossly\\n      negligent acts) or agreed to in writing, shall any Contributor be\\n      liable to You for damages, including any direct, indirect, special,\\n      incidental, or consequential damages of any character arising as a\\n      result of this License or out of the use or inability to use the\\n      Work (including but not limited to damages for loss of goodwill,\\n      work stoppage, computer failure or malfunction, or any and all\\n      other commercial damages or losses), even if such Contributor\\n      has been advised of the possibility of such damages.\\n\\n   9. Accepting Warranty or Additional Liability. While redistributing\\n      the Work or Derivative Works thereof, You may choose to offer,\\n      and charge a fee for, acceptance of support, warranty, indemnity,\\n      or other liability obligations and/or rights consistent with this\\n      License. However, in accepting such obligations, You may act only\\n      on Your own behalf and on Your sole responsibility, not on behalf\\n      of any other Contributor, and only if You agree to indemnify,\\n      defend, and hold each Contributor harmless for any liability\\n      incurred by, or claims asserted against, such Contributor by reason\\n      of your accepting any such warranty or additional liability.\\n\\n   END OF TERMS AND CONDITIONS\\n\\n   APPENDIX: How to apply the Apache License to your work.\\n\\n      To apply the Apache License to your work, attach the following\\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\\n      replaced with your own identifying information. (Don\\'t include\\n      the brackets!)  The text should be enclosed in the appropriate\\n      comment syntax for the file format. We also recommend that a\\n      file or class name and description of purpose be included on the\\n      same \"printed page\" as the copyright notice for easier\\n      identification within third-party archives.\\n\\n   Copyright [yyyy] [name of copyright owner]\\n\\n   Licensed under the Apache License, Version 2.0 (the \"License\");\\n   you may not use this file except in compliance with the License.\\n   You may obtain a copy of the License at\\n\\n       http://www.apache.org/licenses/LICENSE-2.0\\n\\n   Unless required by applicable law or agreed to in writing, software\\n   distributed under the License is distributed on an \"AS IS\" BASIS,\\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n   See the License for the specific language governing permissions and\\n   limitations under the License.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Eye In The Sky",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "eye-in-the-sky",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "manideep2510",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/manideep2510/eye-in-the-sky/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 211,
      "date": "Sun, 05 Dec 2021 13:27:32 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "remote-sensing",
      "deep-learning",
      "computer-vision",
      "machine-learning",
      "keras",
      "tensorflow",
      "unet",
      "pspnet",
      "semantic-segmentation",
      "artificial-intelligence",
      "satellite-images",
      "satellite-image-classification"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Clone the repository, change your present working directory to the cloned directory.\nCreate folders with names `train_predictions` and `test_outputs` to save model predicted outputs on training and testing images (Not required now as the repo already contains these folders)\n\n```\n$ git clone https://github.com/manideep2510/eye-in-the-sky.git\n$ cd eye-in-the-sky\n$ mkdir train_predictions\n$ mkdir test_outputs\n```\n\nFor training the U-Net model and saving weights, run the below command\n\n```\n$ python3 main_unet.py\n```\n\nTo test the U-Net model, calculating accuracies, calculating confusion matrices for training and validation and saving predictions by the model on training, validation and testing images.\n\n```\n$ python3 test_unet.py\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}