{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2102.12924",
      "https://arxiv.org/abs/1911.08265\n- Silver, David et al. (Dec. 2018). \u201cA general reinforcement learning algorithm that masters chess, shogi, and Go through self-play\u201d. In:Science 362.6419, pp. 1140\u20131144.DOI:10.1126/science.aar6404\n\n\n"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Schrittwieser, Julian et al. (Feb. 21, 2020). \u201cMastering Atari, Go, Chess and Shogi by Planning with a Learned Model\u201d. [cs, stat]. arXiv:1911.08265\n- Silver, David et al. (Dec. 2018). \u201cA general reinforcement learning algorithm that masters chess, shogi, and Go through self-play\u201d. In:Science 362.6419, pp. 1140\u20131144.DOI:10.1126/science.aar6404\n\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kaesve/muzero",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-09-12T13:48:53Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-17T00:42:24Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9758978755337264,
        0.9846511641016448,
        0.9516296442896591,
        0.9804740036066776,
        0.9525245369529096,
        0.9390053551195494
      ],
      "excerpt": "We provide a readable, commented, well documented, and conceptually easy implementation of the AlphaZero and MuZero algorithms based on the popular AlphaZero-General implementation.  \nOur implementation extends AlphaZero to work with singleplayer domains, like its successor MuZero. \nThe codebase provides a modular framework to design your own AlphaZero and MuZero models and an API to pit the two algorithms against each other.  \nThis API also allows MuZero agents to more strongly rely on their learned model during interaction with the environment; the programmer can e.g., specify the sparsity of observations to a learned MuZero agent during a trial.  \nOur interface also provides sufficient abstraction to extend the MuZero or AlphaZero algorithm for research purposes. \nNote that we did not perform extensive testing on the boardgames, we experienced that this was very time intensive and difficult to tune.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8780114956183761
      ],
      "excerpt": "There are already a variety of MuZero and AlphaZero implementations available: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A clean implementation of MuZero and AlphaZero following the AlphaZero General framework. Train and Pit both algorithms against each other, and investigate reliability of learned MuZero MDP models.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kaesve/muzero/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 16,
      "date": "Sat, 11 Dec 2021 10:02:07 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/kaesve/muzero/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "kaesve/muzero",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/kaesve/muzero/master/publish/MDP%20Abstraction%20Analysis.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8837680365796365
      ],
      "excerpt": "Python 3.7.9 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/kaesve/muzero/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 Joery de Vries and Ken Voskuil\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "MuZero Vs. AlphaZero in Tensorflow",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "muzero",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "kaesve",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kaesve/muzero/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* Python 3.7+\n - tensorflow\n - keras standalone (until tensorflow 2.3 is available on anaconda windows)\n - tqdm\n\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "In order to run experiments/ train agents, you first need a .json configuration file (see [Configurations/ModelConfigs](Configurations/ModelConfigs)) for specifying the agent's parameters.\nWithin this .json file you also need to specify a neural network architectures (see [Agents/__init__.py](Agents/__init__.py) for existing architectures).\nThen run Main.py with the following flags to train an agent:\n```shell\npython Main.py train -c my/config/file.json --game gym_Cartpole-v1 --gpu [INT]\n```\nSee the [wiki](https://github.com/kaesve/muzero/wiki) for a more elaborate overview of the hyperparameters and how to create new agents or games.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 94,
      "date": "Sat, 11 Dec 2021 10:02:07 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "muzero",
      "alphazero",
      "reinforcement-learning",
      "tensorflow",
      "tensorflow2",
      "mcts",
      "tf2",
      "deep-learning",
      "deep-reinforcement-learning"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This codebase was designed for a Masters Course at Leiden University, we utilized the code to create visualizations of the learned MDP model within MuZero. \nWe did this exclusively for MountainCar, the visualization tool can be viewed here: https://kaesve.nl/projects/muzero-model-inspector/#/; an example illustration of this is shown below.\nThis figure illustrates the entire state-space from the MountainCar being embedded by MuZero's encoding network projected to the 3-PC space of the embedding's neural activation values. \n\n![example](publish/figures/MC_MDP_l8_illustration.png)\n\nWe quantified the efficacy of our MuZero and AlphaZero implementations also on the CartPole environment over numerous hyperparameters. \nThe canonical MuZero can be quite unstable depending on the hyperparameters, the figure shows this through median and mean training rewards over 8 training runs.\n\n![example2](publish/figures/CP_NumericalResults.png)\n\nThe figure below illustrates the efficacy of learned models on MountainCar, when we only provide the MuZero agent observations every n'th environment step along with the agent's learning progress with dense observations.\n\n![example3](publish/figures/MC_NumericalResultsCombinedUpdated.png)\n\nNo boardgames were tested for MuZero as computation time quickly became an issue for us, even on smaller boardsizes.\nWe did find that AlphaZero could learn good policies on boardgames, we found that it depends on the observation encoding. \nHeuristic encoding as used in AlphaZero seemed less effective to the canonicalBoard representation used in AlphaZero-General.\n\nOur paper can be read for more details here: [arxiv:2102.12924](https://arxiv.org/abs/2102.12924).\n \n",
      "technique": "Header extraction"
    }
  ]
}