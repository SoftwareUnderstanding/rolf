{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/ 1910.03771. [Online]. Available: http://arxiv.org/abs/1910.03771.\n\n[3] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen,\nO. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, \u201cRoberta: A robustly optimized BERT pretraining approach,\u201d CoRR, vol. abs/1907.11692,\n2019. [Online]. Available: http://arxiv.org/abs/1907.11692.1908.\n\n[4] D. Q. Nguyen, T. Vu, and A. T. Nguyen,\n\u201cBERTweet: A pre-trained language model for\nEnglish Tweets,\u201d in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, 2020,pp. 9\u201314.\n\n[5] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C.\nDelangue, A. Moi, P. Cistac, T. Rault, R. Louf, M.\nFuntowicz, J. Davison, S. Shleifer, P. von Platen,\nC. Ma, Y. Jernite, J. Plu, C. Xu, T. L. Scao,\nS. Gugger, M. Drame, Q. Lhoest, and A. M. Rush, \u201cTransformers: State-of-the-art natural language processing,\u201d in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Online: Association for Computational Linguistics, Oct. 2020, pp. 38\u201345. [Online]. Available: https://www.aclweb.org/anthology/2020.emnlp-demos.6.\n\n[6] S. Rosenthal, N. Farra, and P. Nakov, \u201cSemEval-\n2017 task 4: Sentiment analysis in Twitter,\u201d in\nProceedings of the 11th International Workshop\non Semantic Evaluation (SemEval-2017), Vancouver, Canada: Association for Computational Linguistics, Aug. 2017, pp. 502\u2013518. DOI: 10.18653/v1/S17- 2088. [Online]. Available: https://www.aclweb.org/anthology/S17-2088.\n\n[7] C. Van Hee, E. Lefever, and V. Hoste, \u201cSemEval-\n2018 task 3: Irony detection in English tweets,\u201d in\nProceedings of The 12th International Workshop\non Semantic Evaluation, New Orleans, Louisiana:\nAssociation for Computational Linguistics, Jun.\n2018, pp. 39\u201350. DOI: 10.18653/v1/S18- 1005.\n[Online]. Available: https : //www.aclweb.org/anthology/S18-1005.\n\n[8] S. Mohammad, S. Kiritchenko, P. Sobhani, X.\nZhu, and C. Cherry, \u201cSemEval-2016 task 6: Detecting stance in tweets,\u201d in Proceedings of the\n10th International Workshop on Semantic Evaluation (SemEval-2016), San Diego, California:\nAssociation for Computational Linguistics, Jun.\n2016, pp. 31\u201341. DOI: 10.18653/v1/S16- 1003.\n[Online]. Available: https : www.aclweb.org/anthology/S16-1003.\n\n[9] V. Basile, C. Bosco, E. Fersini, D. Nozza, V. Patti,\nF. M. Rangel Pardo, P. Rosso, and M. Sanguinetti,\n\u201cSemEval-2019 task 5: Multilingual detection of\nhate speech against immigrants and women in\nTwitter,\u201d in Proceedings of the 13th International\nWorkshop on Semantic Evaluation, Minneapolis,\nMinnesota, USA: Association for Computational\nLinguistics, Jun. 2019, pp. 54\u201363. DOI: 10.18653/v1/S19- 2007. \n[Online]. Available: https://www.aclweb.org/anthology/S19-2007.\n\n[10] M. Zampieri, S. Malmasi, P. Nakov, S. Rosenthal,\nN. Farra, and R. Kumar, \u201cSemEval-2019 task 6:\nIdentifying and categorizing offensive language\nin social media (OffensEval),\u201d in Proceedings\nof the 13th International Workshop on Semantic\nEvaluation, Minneapolis, Minnesota, USA: Association for Computational Linguistics, Jun. 2019,\npp. 75\u201386. DOI: 10.18653/v1/S19-2010. [Online].\nAvailable: https://www.aclweb.org/anthology/S19-2010\n\n\n[11] P. Patwa, G. Aguilar, S. Kar, S. Pandey, S. PYKL,\nB. Gamback, T. Chakraborty, T. Solorio, and A. \u00a8\n7Das, \u201cSemEval-2020 task 9: Overview of sentiment analysis of code-mixed tweets,\u201d in Proceedings of the Fourteenth Workshop on Semantic Evaluation, Barcelona (online): International\nCommittee for Computational Linguistics, Dec.\n2020, pp. 774\u2013790. [Online]. Available: https://www.aclweb.org/anthology/2020.semeval-1.100.\n\n[12] M. Zampieri, P. Nakov, S. Rosenthal, P.\nAtanasova, G. Karadzhov, H. Mubarak, L. Derczynski, Z. Pitenis, and C\u00b8 . C\u00b8 oltekin, \u201cSemEval- \u00a8\n2020 task 12: Multilingual offensive language\nidentification in social media (OffensEval 2020),\u201d\nin Proceedings of the Fourteenth Workshop on\nSemantic Evaluation, Barcelona (online): International Committee for Computational Linguistics,\nDec. 2020, pp. 1425\u20131447. [Online]. Available:https://www.aclweb.org/anthology/2020.semeval-1.188.\n\n\n\n\n\n\n\n\n"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[1] J. Devlin, M.-W. Chang, K. Lee, and K.\nToutanova, \u201cBERT: Pre-training of deep bidirectional transformers for language understanding,\u201d in Proceedings of the 2019 Conference ofthe North American Chapter of the Associationfor Computational Linguistics: Human LanguageTechnologies, Volume 1 (Long and Short Papers), Minneapolis, Minnesota: Association for Computational Linguistics, Jun. 2019, pp. 4171\u20134186. DOI: 10.18653/v1/N19-1423. [Online]. Available: https://www.aclweb.org/anthology/N19-1423.\n\n[2] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C.Delangue, A. Moi, P. Cistac, T. Rault, R. Louf,M. Funtowicz, and J. Brew, \u201cHuggingface\u2019s transformers: State-of-the-art natural language processing,\u201d CoRR, vol. abs/1910.03771, 2019. arXiv: 1910.03771. [Online]. Available: http://arxiv.org/abs/1910.03771.\n\n[3] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen,\nO. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, \u201cRoberta: A robustly optimized BERT pretraining approach,\u201d CoRR, vol. abs/1907.11692,\n2019. [Online]. Available: http://arxiv.org/abs/1907.11692.1908.\n\n[4] D. Q. Nguyen, T. Vu, and A. T. Nguyen,\n\u201cBERTweet: A pre-trained language model for\nEnglish Tweets,\u201d in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, 2020,pp. 9\u201314.\n\n[5] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C.\nDelangue, A. Moi, P. Cistac, T. Rault, R. Louf, M.\nFuntowicz, J. Davison, S. Shleifer, P. von Platen,\nC. Ma, Y. Jernite, J. Plu, C. Xu, T. L. Scao,\nS. Gugger, M. Drame, Q. Lhoest, and A. M. Rush, \u201cTransformers: State-of-the-art natural language processing,\u201d in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Online: Association for Computational Linguistics, Oct. 2020, pp. 38\u201345. [Online]. Available: https://www.aclweb.org/anthology/2020.emnlp-demos.6.\n\n[6] S. Rosenthal, N. Farra, and P. Nakov, \u201cSemEval-\n2017 task 4: Sentiment analysis in Twitter,\u201d in\nProceedings of the 11th International Workshop\non Semantic Evaluation (SemEval-2017), Vancouver, Canada: Association for Computational Linguistics, Aug. 2017, pp. 502\u2013518. DOI: 10.18653/v1/S17- 2088. [Online]. Available: https://www.aclweb.org/anthology/S17-2088.\n\n[7] C. Van Hee, E. Lefever, and V. Hoste, \u201cSemEval-\n2018 task 3: Irony detection in English tweets,\u201d in\nProceedings of The 12th International Workshop\non Semantic Evaluation, New Orleans, Louisiana:\nAssociation for Computational Linguistics, Jun.\n2018, pp. 39\u201350. DOI: 10.18653/v1/S18- 1005.\n[Online]. Available: https : //www.aclweb.org/anthology/S18-1005.\n\n[8] S. Mohammad, S. Kiritchenko, P. Sobhani, X.\nZhu, and C. Cherry, \u201cSemEval-2016 task 6: Detecting stance in tweets,\u201d in Proceedings of the\n10th International Workshop on Semantic Evaluation (SemEval-2016), San Diego, California:\nAssociation for Computational Linguistics, Jun.\n2016, pp. 31\u201341. DOI: 10.18653/v1/S16- 1003.\n[Online]. Available: https : www.aclweb.org/anthology/S16-1003.\n\n[9] V. Basile, C. Bosco, E. Fersini, D. Nozza, V. Patti,\nF. M. Rangel Pardo, P. Rosso, and M. Sanguinetti,\n\u201cSemEval-2019 task 5: Multilingual detection of\nhate speech against immigrants and women in\nTwitter,\u201d in Proceedings of the 13th International\nWorkshop on Semantic Evaluation, Minneapolis,\nMinnesota, USA: Association for Computational\nLinguistics, Jun. 2019, pp. 54\u201363. DOI: 10.18653/v1/S19- 2007. \n[Online]. Available: https://www.aclweb.org/anthology/S19-2007.\n\n[10] M. Zampieri, S. Malmasi, P. Nakov, S. Rosenthal,\nN. Farra, and R. Kumar, \u201cSemEval-2019 task 6:\nIdentifying and categorizing offensive language\nin social media (OffensEval),\u201d in Proceedings\nof the 13th International Workshop on Semantic\nEvaluation, Minneapolis, Minnesota, USA: Association for Computational Linguistics, Jun. 2019,\npp. 75\u201386. DOI: 10.18653/v1/S19-2010. [Online].\nAvailable: https://www.aclweb.org/anthology/S19-2010\n\n\n[11] P. Patwa, G. Aguilar, S. Kar, S. Pandey, S. PYKL,\nB. Gamback, T. Chakraborty, T. Solorio, and A. \u00a8\n7Das, \u201cSemEval-2020 task 9: Overview of sentiment analysis of code-mixed tweets,\u201d in Proceedings of the Fourteenth Workshop on Semantic Evaluation, Barcelona (online): International\nCommittee for Computational Linguistics, Dec.\n2020, pp. 774\u2013790. [Online]. Available: https://www.aclweb.org/anthology/2020.semeval-1.100.\n\n[12] M. Zampieri, P. Nakov, S. Rosenthal, P.\nAtanasova, G. Karadzhov, H. Mubarak, L. Derczynski, Z. Pitenis, and C\u00b8 . C\u00b8 oltekin, \u201cSemEval- \u00a8\n2020 task 12: Multilingual offensive language\nidentification in social media (OffensEval 2020),\u201d\nin Proceedings of the Fourteenth Workshop on\nSemantic Evaluation, Barcelona (online): International Committee for Computational Linguistics,\nDec. 2020, pp. 1425\u20131447. [Online]. Available:https://www.aclweb.org/anthology/2020.semeval-1.188.\n\n\n\n\n\n\n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9297291156825539,
        0.9726505065617498
      ],
      "excerpt": "from the SemEval2017 Task 4A (Rosenthal et al., 2017). \nand the 2-class irony detection dataset from the SemEval2018 Task 3A (Van Hee et al., 2018). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.996021929372819,
        0.9949993519458928,
        0.9889963980842408
      ],
      "excerpt": "- SemEval-2016 Task 6: Detecting Stance in Tweets (Mohammad et al, 2016). \n- SemEval-2019 Task 5, subtask A : Hate Speech Detection against immigrants and women (Basile et al).  \n- SemEval-2019 Task 6, Sub-task A: Offensive language identification (Zampieri et al).  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.993551667681701
      ],
      "excerpt": "- SemEval-2020 Task 12: Offensive Language Detection (Zampieri et al). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8927402726880329
      ],
      "excerpt": "entry if it doesn\u2019t exceed 128 tokens. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/pyhemza/BERTweet_sentiment_analysis",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-04-30T23:55:47Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-13T20:40:20Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9558617362815234,
        0.9615136291075248,
        0.9776058492199934,
        0.9865027689476096,
        0.9464251774182697
      ],
      "excerpt": "The language model BERT, the Bidirectional Encoder Representations from transformers and its variants have helped produce the state of the art performance results for various NLP tasks.  \nThese models are trained on the common English domains such as Wikipedia, news and books. The idea behind BERTweet is to train a model using the BERT architecture on a specific domain,  \nwhich is twitter one of the most popular micro-blogging platforms, where users can share real time information related to all kind of topics events.  \nNote that the characteristics of Tweets are generally different from those traditional written text such as Wikipedia and news articles, due to the typical short length of Tweets and frequent use of informal grammar as well as irregular vocabulary. \nBERTweet authors has decided to train a language model for English Tweets using a 80Gb corpus of 850M English Tweets. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9715101615964504
      ],
      "excerpt": "we evaluate and compare the performance of BERTweet, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9389091607498957
      ],
      "excerpt": "- SemEval-2020 Task 9: Overview of Sentiment Analysis of Code-Mixed Tweets (Patwa et al). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8598038182023018,
        0.8374431010065915
      ],
      "excerpt": "We used the same procedure as in the paper, for each \ndataset, we merge the training and the validation data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9109343619214926,
        0.9609657260310295,
        0.9214036151020232,
        0.8485156995458883,
        0.9167659112120837,
        0.9389105506670395,
        0.8601308075584717
      ],
      "excerpt": "We use \u201csoft\u201d normalization strategy to all of the \nexperimental datasets, by translating word to tokens of \nuser mentions and web/url links into special tokens \n@USER, and HTTP/URL, respectively, and the emoji \npackage to translate emotion icons into text strings \nWe employ the transformers library to preprocess the \ndata, the tokenizer has an option to normalize data before \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8554206513626055,
        0.8728545774466198
      ],
      "excerpt": "sequences. To choose the maximum length, we tokenize \nthe training set and take the length of the maximum \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.935995252902982
      ],
      "excerpt": "paper we append a linear prediction layer on the top of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9567588029116127,
        0.9650312919583437
      ],
      "excerpt": "Optimizer : AdamW (Adam with L2 regularization \nand weight decay) with a fixed learning rate of 1e-5. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.936935802885353
      ],
      "excerpt": "On average BERTweet outperforms, the generic language models BERT and RoBERTa, by around 4% and \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9291573474851512
      ],
      "excerpt": "effectiveness of the large scale BERTweet for the tweet \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9792816931464104
      ],
      "excerpt": "On average BERTweet outperforms, the generic language models BERT and RoBERTa, by around 4% and 5%, respectively. The results reported above, confirm the effectiveness of the large scale BERTweet for the tweet \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/devhemza/BERTweet_sentiment_analysis/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Fri, 10 Dec 2021 10:28:22 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/pyhemza/BERTweet_sentiment_analysis/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "pyhemza/BERTweet_sentiment_analysis",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/devhemza/BERTweet_sentiment_analysis/main/Bert_base_vs_BERTweet.ipynb",
      "https://raw.githubusercontent.com/devhemza/BERTweet_sentiment_analysis/main/BERTweet.ipynb",
      "https://raw.githubusercontent.com/devhemza/BERTweet_sentiment_analysis/main/Roberta_vs_BERTweet.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8455263979448202,
        0.999833231880651,
        0.9893272198983933,
        0.9906248903846466,
        0.979515180465076,
        0.9569145373273186
      ],
      "excerpt": "Python 3.6+, and PyTorch 1.1.0+. \nInstall transformers:  \ngit clone https://github.com/huggingface/transformers.git \ncd transformers \npip3 install --upgrade. \nInstall emoji: pip3 install emoji \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/pyhemza/BERTweet_sentiment_analysis/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "BERTweet_sentiment_analysis",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "BERTweet_sentiment_analysis",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "pyhemza",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/pyhemza/BERTweet_sentiment_analysis/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Fri, 10 Dec 2021 10:28:22 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Our results show that BERT model slightly outperform\nRoBERTa on average, The results of BERTweet confirms\nthe effectiveness of a large scale specific pre-trained\nlanguage model for English tweets. To further understand\nwhy BERTweet have better result.\n\nwe analysed the tweets correctly classified by BERTweet\nand wrongly classified by the other two models\n\n- Example well classified by BERTweet\n![image](https://user-images.githubusercontent.com/56854458/116765945-3a645600-aa28-11eb-8192-63471f63f062.png)\n\n- Definition of shweet in the Urban dictionnary :\n![image](https://user-images.githubusercontent.com/56854458/116765971-58ca5180-aa28-11eb-92a8-b8aba38e2348.png)\n\n",
      "technique": "Header extraction"
    }
  ]
}