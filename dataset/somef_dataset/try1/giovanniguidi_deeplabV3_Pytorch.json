{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1802.02611"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\\[1\\] [Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation](https://arxiv.org/pdf/1802.02611.pdf)\n\n\\[2\\] [Rethinking Atrous Convolution for Semantic Image Segmentation](https://arxiv.org/pdf/1706.05587.pdf)\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9954573200654647
      ],
      "excerpt": "  <i>Fig. 1: DeepLabV3+ model (source Chen et al. 2018)</i> \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/giovanniguidi/deeplabV3-PyTorch",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-01-29T09:39:47Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-09-09T14:56:01Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9863486648707509
      ],
      "excerpt": "This project is based on one of the state-of-the-art algorithms for semantic segmentation, DeepLabV3+ by the Google research group (Chen et al. 2018, https://arxiv.org/abs/1802.02611). Semantic segmentation is the task of predicting for each pixel of an image a \"semantic\" label, such as tree, street, sky, car (and of course background).  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9864559767202147,
        0.971060590198343
      ],
      "excerpt": "DeepLabV3+ model is very complex, but the biggest difference compared to other models is the use of \"atrous convolutions\" in the encoder (which was already suggested in the first DeepLab model by Chen et al. 2016), in a configuration called Atrous Spatial Pyramid Pooling (ASPP). ASPP is composed by different atrous convolution layers in parallel with a different atrous rate, allowing to capture information at multiple scales and extract denser  \nfeature maps (see the image below and the paper for details).   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9020527548870979
      ],
      "excerpt": "All the parameters of the model are in configs/config.yml. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9511135028522149
      ],
      "excerpt": "Here is an example of the results: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Implementation of the DeepLabV3+ model in PyTorch for semantic segmentation, trained on DeepFashion2 dataset ",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/giovanniguidi/deeplabV3_Pytorch/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 5,
      "date": "Sun, 05 Dec 2021 22:58:57 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/giovanniguidi/deeplabV3-PyTorch/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "giovanniguidi/deeplabV3-PyTorch",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/giovanniguidi/deeplabV3_Pytorch/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/giovanniguidi/deeplabV3_Pytorch/master/notebooks/inference.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/giovanniguidi/deeplabV3_Pytorch/master/run.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9123908655963883,
        0.9558322409057513,
        0.9772227696564274,
        0.9770335174395833,
        0.841721949119684
      ],
      "excerpt": "First you need to create a virtual environment.  \nUsing Conda you can type: \nconda create --name deeplab --python==3.7.1 \nconda activate deeplab \nDownload the dataset from:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8093019340199852
      ],
      "excerpt": "The model can be trained with different backbones (resnet, xception, drn, mobilenet). The weights on the Drive has been trained with the ResNet backbone, so if you want to use another backbone you need to train from scratch (although the backbone weights are always pre-trained on ImageNet). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8410943602067414
      ],
      "excerpt": "You can also check the \"inference.ipynb\" notebook for visual assessing the predictions. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8180607541817051
      ],
      "excerpt": "Download the dataset from:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8208048665108875
      ],
      "excerpt": "This folder contains also the train/val/test split json in case you want to use the same split I used. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955237133889473,
        0.918352196404126
      ],
      "excerpt": "To train a model run: \npython main.py -c configs/config.yml --train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8837601967653921
      ],
      "excerpt": "python main.py -c configs/config.yml --predict_on_test \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9591314615929807
      ],
      "excerpt": "python main.py -c configs/config.yml --predict --filename test_images/068834.jpg \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8022097415075592
      ],
      "excerpt": "Here is an example of the results: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8017168178864091
      ],
      "excerpt": "mean IoU: 0.34 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/giovanniguidi/deeplabV3-PyTorch/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "DeepLab V3+ Network for Semantic Segmentation",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "deeplabV3-PyTorch",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "giovanniguidi",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/giovanniguidi/deeplabV3-PyTorch/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This project is based on the PyTorch Deep Learning library. \n\nInstall the dependencies by:\n```\npip install -r requirements.txt \n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 18,
      "date": "Sun, 05 Dec 2021 22:58:57 GMT"
    },
    "technique": "GitHub API"
  }
}