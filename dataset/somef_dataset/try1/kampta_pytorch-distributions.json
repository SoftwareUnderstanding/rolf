{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1711.10604",
      "https://arxiv.org/abs/1312.6114",
      "https://arxiv.org/abs/1611.01144\n\n[2] \"The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables\" -\n Chris J. Maddison, Andriy Mnih, Yee Whye Teh - https://arxiv.org/abs/1611.0071",
      "https://arxiv.org/abs/1611.0071"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kampta/pytorch-distributions",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-12-24T23:13:33Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-29T06:32:16Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8956107631504954,
        0.93646390437797,
        0.8914978211496273
      ],
      "excerpt": "This repository contains basic examples of pytorch distributions package. \nThe distributions package is pytorch adaptation of tensorflow distributions which implements \nbuilding blocks for Bayesian Deep Learning. To read more about the examples in this \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Basic VAE flow using pytorch distributions",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kampta/pytorch-distributions/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 09 Dec 2021 10:49:18 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/kampta/pytorch-distributions/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "kampta/pytorch-distributions",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Borrowed as it is from [pytorch repo](https://github.com/pytorch/examples/tree/master/vae).\nIt is implementation of the paper [Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114) by Kingma and Welling.\n\n\n**Usage**\n```\npython vae.py\n...\n...\n...\n====> Epoch: 10 Average loss: 106.3110\n====> Test set loss: 105.5890\n```\n\n**Reconstructed samples**\n\nSome sample reconstructions from the basic VAE (trained for 10 epochs on MNIST)  \n\n![alt text](imgs/vae_recon_sample.png \"Reconstructions from Standard VAE\")\n\n**Generated Samples**\n\nWe can also generate some sample digits from the basic VAE by providing random numbers\ngenerated from a normal distribution as input.\n\n![alt text](imgs/vae_sample.png \"Samples from Standard VAE\")\n\n\nWe will use this example as template for rest of the code.\n\n",
      "technique": "Header extraction"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/kampta/pytorch-distributions/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Bayesian Deep Learning with torch distributions",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "pytorch-distributions",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "kampta",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/kampta/pytorch-distributions/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 11,
      "date": "Thu, 09 Dec 2021 10:49:18 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We'll construct the exact same example using using `distributions` package now.\nWe'll need to modify very little code. Notice the changes in `forward` and `loss_function`. \n\n```\npython gaussian_vae.py\n...\n...\n...\n====> Epoch: 10 Average loss: 106.3209\n====> Test set loss: 105.6140\n```\n\n**Reconstructed samples**\n\nSome sample reconstructions from the basic VAE (trained for 10 epochs on MNIST)  \n\n![alt text](imgs/gaussian_recon_sample.png \"Reconstructions from Standard VAE\")\n\n**Generated Samples**\n\n![alt text](imgs/gaussian_vae_sample.png \"Samples from Standard VAE\")\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "We can make our latent representation bernoulli by using [relaxed bernoulli](https://pytorch.org/docs/stable/_modules/torch/distributions/relaxed_bernoulli.html#RelaxedBernoulli)\ndistribution. The file `binconcrete.py` contains implementation with bottleneck layer of size 20.\n\n```\npython binconcrete.py\n...\n...\n...\n====> Epoch: 10 Average loss: 126.6666\n====> Test set loss: 125.3123\n```\n\n**Reconstructed samples**\n\nSome sample reconstructions (trained for 10 epochs on MNIST)  \n\n![alt text](imgs/binconcrete_recon_sample.png \"Reconstructions from Bernoulli VAE\")\n\n**Generated Samples**\n\n![alt text](imgs/binconcrete_sample.png \"Samples from Bernoulli VAE\")\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Similar to the bernoulli example, a more general usecase is when the latent dimension is\ncategorical. \n\n\n```\npython concrete.py\n...\n...\n...\n====> Epoch: 10 Average loss: 110.2161\n====> Test set loss: 109.1930\n```\n\n**Reconstructed samples**\n\nSome sample reconstructions (trained for 10 epochs on MNIST)  \n\n![alt text](imgs/concrete_recon_sample.png \"Reconstructions from Categorical VAE\")\n\n**Generated Samples**\n\n![alt text](imgs/concrete_sample.png \"Samples from Categorical VAE\")\n\nFor more details on relaxed bernoulli or relaxed categorical distributions, please refer\nto the following papers\n\n[1] \"Categorical Reparameterization with Gumbel-Softmax\" - \nEric Jang, Shixiang Gu, Ben Poole - https://arxiv.org/abs/1611.01144\n\n[2] \"The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables\" -\n Chris J. Maddison, Andriy Mnih, Yee Whye Teh - https://arxiv.org/abs/1611.00712",
      "technique": "Header extraction"
    }
  ]
}