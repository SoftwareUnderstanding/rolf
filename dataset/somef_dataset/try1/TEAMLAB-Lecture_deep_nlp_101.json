{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1802.05365",
      "https://arxiv.org/abs/1607.01759\n\n### Network Architecture\n- \\[SEQ2SEQ_2014\\]Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. \"Sequence to sequence learning with neural networks.\" In Advances in neural information processing systems, pp. 3104-3112. 2014. Available at - https://arxiv.org/abs/1409.3215\n\n## Datasets\n- [The Best 25 Datasets for Natural Language Processing](https://gengo.ai/articles/the-best-25-datasets-for-natural-language-processing/",
      "https://arxiv.org/abs/1409.3215\n\n## Datasets\n- [The Best 25 Datasets for Natural Language Processing](https://gengo.ai/articles/the-best-25-datasets-for-natural-language-processing/",
      "https://arxiv.org/abs/1802.05365.\n\n\n#### sentece or paragraph embeddings\n- \\[[DOC2VEC_2014]()\\] Le, Quoc, and Tomas Mikolov. \"Distributed representations of sentences and documents.\" In International Conference on Machine Learning, pp. 1188-1196. 2014. Available - https://cs.stanford.edu/~quocle/paragraph_vector.pdf\n- \\[[SIM_SEN_2016]()\\] Arora, S., Liang, Y. and Ma, T., 2016. A simple but tough-to-beat baseline for sentence embeddings. Available - https://openreview.net/forum?id=SyK00v5xx\n- \\[[DEEP_SEN_2016]()\\] Palangi, H., Deng, L., Shen, Y., Gao, J., He, X., Chen, J., Song, X. and Ward, R., 2016. Deep sentence embedding using long short-term memory networks: Analysis and application to information retrieval. IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP), 24(4), pp.694-707. Available - https://dl.acm.org/citation.cfm?id=2992457\n\n\n### Text Classification\n- \\[[CHAR_CNN_2015]()\\] Zhang, Xiang, Junbo Zhao, and Yann LeCun. \"Character-level convolutional networks for text classification.\" In Advances in neural information processing systems, pp. 649-657. 2015. Aaailalble at: http://papers.nips.cc/paper/5782-character-level-convolutional-networks-fo\n- \\[[FASTTEXT_2016()]()\\]Joulin, Armand, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. \"Bag of tricks for efficient text classification.\" arXiv preprint https://arxiv.org/abs/1607.01759 (2016). Available at - https://arxiv.org/abs/1607.01759\n\n### Network Architecture\n- \\[SEQ2SEQ_2014\\]Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. \"Sequence to sequence learning with neural networks.\" In Advances in neural information processing systems, pp. 3104-3112. 2014. Available at - https://arxiv.org/abs/1409.3215\n\n## Datasets\n- [The Best 25 Datasets for Natural Language Processing](https://gengo.ai/articles/the-best-25-datasets-for-natural-language-processing/)\n- [Data Sets and Corpora](https://natemccoy.github.io/2016/11/28/datasetsandcorpora.html)\n\n\nhttp://www.sciencedirect.com/science/article/pii/S0893608005001206",
      "https://arxiv.org/abs/1607.01759 (2016). Available at - https://arxiv.org/abs/1607.01759\n\n### Network Architecture\n- \\[SEQ2SEQ_2014\\]Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. \"Sequence to sequence learning with neural networks.\" In Advances in neural information processing systems, pp. 3104-3112. 2014. Available at - https://arxiv.org/abs/1409.3215\n\n## Datasets\n- [The Best 25 Datasets for Natural Language Processing](https://gengo.ai/articles/the-best-25-datasets-for-natural-language-processing/)\n- [Data Sets and Corpora](https://natemccoy.github.io/2016/11/28/datasetsandcorpora.html)\n\n\nhttp://www.sciencedirect.com/science/article/pii/S0893608005001206"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9875077906061815
      ],
      "excerpt": "[2018] NLP with DL, graduate cource, IME at Gachon University \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8511535834563841
      ],
      "excerpt": "Neural machine translation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336221125029482
      ],
      "excerpt": "Visual question and answering \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.927521773993594,
        0.9615894215234206
      ],
      "excerpt": "Neural Networks for NLP from Carnegie Mellon University link, youtube \nDeep Learning for Natural Language Processing from University of Oxford and DeepMind link, youtube \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8596843120704137
      ],
      "excerpt": "CS224n 2018 Project \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9748627453040746,
        0.9773282912211774,
        0.9987575966597765,
        0.9999342979171256,
        0.9999718927681124
      ],
      "excerpt": "[NLM_2003] Bengio, Y., Ducharme, R., Vincent, P., & Jauvin, C. (2003). A neural probabilistic language model. Journal of machine learning research, 3(Feb), 1137-1155. Available - http://www.jmlr.org/papers/v3/bengio03a.html \n[WORD2VEC_2013] Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. \"Distributed representations of words and phrases and their compositionality.\" In Advances in neural information processing systems, pp. 3111-3119. 2013.  Available - https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf \n[GLOVE_2014] Pennington, Jeffrey, Richard Socher, and Christopher Manning. \"Glove: Global vectors for word representation.\" In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp. 1532-1543. 2014. Available - http://www.aclweb.org/anthology/D14-1162, PR12, ratsgoBlog #1, ratsgoBlog #2 \n[ELMO_2018] Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., & Zettlemoyer, L. (2018). Deep contextualized word representations. arXiv preprint arXiv:1802.05365. \n[DOC2VEC_2014] Le, Quoc, and Tomas Mikolov. \"Distributed representations of sentences and documents.\" In International Conference on Machine Learning, pp. 1188-1196. 2014. Available - https://cs.stanford.edu/~quocle/paragraph_vector.pdf \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999995295031822,
        0.9856437491274895,
        0.9999996534826497,
        0.9948851960405067
      ],
      "excerpt": "[DEEP_SEN_2016] Palangi, H., Deng, L., Shen, Y., Gao, J., He, X., Chen, J., Song, X. and Ward, R., 2016. Deep sentence embedding using long short-term memory networks: Analysis and application to information retrieval. IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP), 24(4), pp.694-707. Available - https://dl.acm.org/citation.cfm?id=2992457 \n[CHAR_CNN_2015] Zhang, Xiang, Junbo Zhao, and Yann LeCun. \"Character-level convolutional networks for text classification.\" In Advances in neural information processing systems, pp. 649-657. 2015. Aaailalble at: http://papers.nips.cc/paper/5782-character-level-convolutional-networks-fo \n[FASTTEXT_2016()]Joulin, Armand, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. \"Bag of tricks for efficient text classification.\" arXiv preprint arXiv:1607.01759 (2016). Available at - https://arxiv.org/abs/1607.01759 \n[SEQ2SEQ_2014]Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. \"Sequence to sequence learning with neural networks.\" In Advances in neural information processing systems, pp. 3104-3112. 2014. Available at - https://arxiv.org/abs/1409.3215 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/TEAMLAB-Lecture/deep_nlp_101",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-07-08T09:35:26Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-11-03T14:39:03Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9788231669156202,
        0.9917028674218457,
        0.9679744946979856
      ],
      "excerpt": "Todays, Natural Language Processing (NLP) plays a significant role in building intelligent information systems. Traditionally, applications of NLP are everywhere in a variety of areas including web searching, email processing, e-commerce, translation, and automatic generation of reports.  Human languages are complex and unstructured. NLP is recognized as a tough field because various preprocessing is required for a computer to understand human words. \nIn recent years, the explosion of text data and advancement of Deep Learning technology have resulted in a dramatic increase in the performance of existing NLP applications. In particular, neural networks, unlike traditional models, find the appropriate features for text information on their own, minimizing human involvement. Besides, by developing appropriate models for the features, we have seen dramatically increasing performance and practicality. \nIn this course, students will take the advanced learning to develop NLP applications with cutting-edge deep learning techniques. You will study the design, implementation, debugging, and visualization techniques of neural network models to handle textual information. Through the final project, students will also have the opportunity to organize and train their neural net for text processing problems in specific areas they want to handle. It will be an arduous journey, but I wish you an enjoyable walk with your friends. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8686090370258049,
        0.8308795652155618
      ],
      "excerpt": "Language modeling - techniques of embeddings \nNeural net arichtecutre for NLP: Memory, Attention and Transformer Models \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9809901337832813
      ],
      "excerpt": "Deep Learning for Natural Language Processing from University of Oxford and DeepMind link, youtube \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8265179166960581,
        0.9166923571940664
      ],
      "excerpt": "[NLM_2003] Bengio, Y., Ducharme, R., Vincent, P., & Jauvin, C. (2003). A neural probabilistic language model. Journal of machine learning research, 3(Feb), 1137-1155. Available - http://www.jmlr.org/papers/v3/bengio03a.html \n[WORD2VEC_2013] Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. \"Distributed representations of words and phrases and their compositionality.\" In Advances in neural information processing systems, pp. 3111-3119. 2013.  Available - https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9020293014996936,
        0.9232468424046326
      ],
      "excerpt": "The Best 25 Datasets for Natural Language Processing \nData Sets and Corpora \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/TEAMLAB-Lecture/deep_nlp_101/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Fri, 10 Dec 2021 23:27:55 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/TEAMLAB-Lecture/deep_nlp_101/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "TEAMLAB-Lecture/deep_nlp_101",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/TEAMLAB-Lecture/deep_nlp_101/master/2018/tutorial/Lecture01/Lecture_01.ipynb",
      "https://raw.githubusercontent.com/TEAMLAB-Lecture/deep_nlp_101/master/2018/tutorial/Lecture03/Lecture03.ipynb",
      "https://raw.githubusercontent.com/TEAMLAB-Lecture/deep_nlp_101/master/2018/tutorial/Lecture09/Lecture09.ipynb",
      "https://raw.githubusercontent.com/TEAMLAB-Lecture/deep_nlp_101/master/2018/tutorial/pytorch_tuorial/2_pytorch_autograd.ipynb",
      "https://raw.githubusercontent.com/TEAMLAB-Lecture/deep_nlp_101/master/2018/tutorial/pytorch_tuorial/1_pytorch_examples_with_tensors.ipynb",
      "https://raw.githubusercontent.com/TEAMLAB-Lecture/deep_nlp_101/master/2018/tutorial/Lecture02/Lecture02.ipynb",
      "https://raw.githubusercontent.com/TEAMLAB-Lecture/deep_nlp_101/master/2018/tutorial/Lecture04/Lecture04.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9322609392449874
      ],
      "excerpt": "PyTorch book \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/TEAMLAB-Lecture/deep_nlp_101/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Natural Language Process with Deep Learning",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "deep_nlp_101",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "TEAMLAB-Lecture",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/TEAMLAB-Lecture/deep_nlp_101/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- \ud30c\uc774\uc36c \ucf54\ub529 \ub2a5\ub825\n  - [\ub370\uc774\ud130 \uacfc\ud559\uc744 \uc704\ud55c \ud30c\uc774\uc36c \uc785\ubb38](https://www.inflearn.com/course/python-%ED%8C%8C%EC%9D%B4%EC%8D%AC-%EC%9E%85%EB%AC%B8-%EA%B0%95%EC%A2%8C/)\n- \uba38\uc2e0 \ub7ec\ub2dd \uae30\ucd08 \uc774\ud574\n  - [\ubc11\ubc14\ub2e5 \ubd80\ud130 \uc2dc\uc791\ud558\ub294 \uba38\uc2e0\ub7ec\ub2dd \uc785\ubb38](https://www.inflearn.com/course/%ED%8C%8C%EC%9D%B4%EC%8D%AC-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8-%EA%B0%95%EC%A2%8C/)\n- \ub525\ub7ec\ub2dd \uae30\ucd08 \uc774\ud574\n  - [\ubaa8\ub450\ub97c \uc704\ud55c \uba38\uc2e0\ub7ec\ub2dd\uacfc \ub525\ub7ec\ub2dd](http://hunkim.github.io/ml/)\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Fri, 10 Dec 2021 23:27:55 GMT"
    },
    "technique": "GitHub API"
  }
}