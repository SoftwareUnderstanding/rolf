{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1606.08415",
      "https://arxiv.org/abs/1706.03762v5",
      "https://arxiv.org/abs/1904.09751",
      "https://arxiv.org/abs/2002.04745"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* [Hendrycks+, 2016] *Gaussian Error Linear Units (GELUs)* by Dan Hendrycks and Kevin Gimpel. (https://arxiv.org/abs/1606.08415)\n* [Vaswani+, 2017] *Attention Is All You Need* by Ashish Vaswani et al. (https://arxiv.org/abs/1706.03762v5)\n* [Radford+, 2018] *Improving Language Understanding by Generative Pre-Training* by Alec Radford et al. (https://openai.com/blog/language-unsupervised/)\n* [Radford+, 2019] *Language Models are Unsupervised Multitask Learners* by Alec Radford et al. (https://openai.com/blog/better-language-models/)\n* [Holtzman+, 2019] *The Curious Case of Neural Text Degeneration* by Ari Holtzman et al. (https://arxiv.org/abs/1904.09751)\n* [Xiong+, 2020] *On Layer Normalization in the Transformer Architecture* by Ruibin Xiong et al. (https://arxiv.org/abs/2002.04745)\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "Epoch 1/10 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "Epoch 2/10 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "Epoch 3/10 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "Epoch 4/10 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "Epoch 5/10 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "Epoch 6/10 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "Epoch 7/10 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "Epoch 8/10 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "Epoch 9/10 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9848179726092176
      ],
      "excerpt": "Epoch 10/10 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/colorfulscoop/tfdlg",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-10-21T10:29:28Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-07-23T13:44:46Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9728074682480061,
        0.8930901044020226,
        0.9147295605271616
      ],
      "excerpt": "tfDlg is a Python library for transformer-based language models and dialog models with TensorFlow. \n:sparkles: Features :sparkles: \nSimple models: tfDlg adopts simple and easy-to-understand model implementation to enable users to customize models for their research and interests. You can find the model implementation in tfdlg/models.py. You can utilize these models in the usual manner of tf.keras (e.g. you can call compile and build method for them). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8178612036112177
      ],
      "excerpt": "tfdlg.data provides dataset builders to input them to your model. They generate tf.data.Dataset object \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9078821403878278
      ],
      "excerpt": "tfdlg.dialog.data provides a dataset builder which considers context of the dialog. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9329482940327538
      ],
      "excerpt": ":memo: If you train a tokenizer for languages which do not separate words with white spaces, consider to use --add_dummy_prefix=False option to avoid adding a dummy white space at the beginnin of a text (detault is True to add a white space at the beginning of a text). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9783367659824918
      ],
      "excerpt": "The server is launched by FastAPI. Therefore a Swagger document is also available on http://localhost:8080/docs. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9266825591643847
      ],
      "excerpt": "It is the decoder side implementation of [Vaswani+, 2017] . \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9774288106911729
      ],
      "excerpt": "Weight is initialized by Grolo's uniform distribution except for layers which uses ReLU. For those which uses the ReLU activation function, He's initialization is used. (The weight initialization method is not mentioned in the paper.) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "tfDlg is a Python library for transformer-based language models and dialog models with TensorFlow.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/colorfulscoop/tfdlg/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sun, 12 Dec 2021 17:51:36 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/colorfulscoop/tfdlg/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "colorfulscoop/tfdlg",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/colorfulscoop/tfdlg/master/examples/overview.ipynb",
      "https://raw.githubusercontent.com/colorfulscoop/tfdlg/master/examples/usage.ipynb",
      "https://raw.githubusercontent.com/colorfulscoop/tfdlg/master/examples/transformers-gpt2-ja/train_tokenizer.ipynb",
      "https://raw.githubusercontent.com/colorfulscoop/tfdlg/master/examples/transformers-gpt2-ja/convert_to_pytorch.ipynb",
      "https://raw.githubusercontent.com/colorfulscoop/tfdlg/master/examples/transformers-gpt2-ja/train_model.ipynb",
      "https://raw.githubusercontent.com/colorfulscoop/tfdlg/master/examples/transformers-gpt2-ja/output/train_tokenizer.ipynb",
      "https://raw.githubusercontent.com/colorfulscoop/tfdlg/master/examples/transformers-gpt2-ja/output/convert_to_pytorch.ipynb",
      "https://raw.githubusercontent.com/colorfulscoop/tfdlg/master/examples/transformers-gpt2-ja/output/train_model.ipynb",
      "https://raw.githubusercontent.com/colorfulscoop/tfdlg/master/examples/benchmark-wikitext-v0.1.0/transformers_train_scratch.ipynb",
      "https://raw.githubusercontent.com/colorfulscoop/tfdlg/master/examples/benchmark-wikitext-v0.1.0/tfmodel_train_scratch.ipynb",
      "https://raw.githubusercontent.com/colorfulscoop/tfdlg/master/examples/benchmark-wikitext-v0.1.0/output/tfmodel_train_scratch-wikitext_103_raw-post_ln-lr_e4.ipynb",
      "https://raw.githubusercontent.com/colorfulscoop/tfdlg/master/examples/benchmark-wikitext-v0.1.0/output/tfmodel_train_scratch-wikitext_103_raw-pre_ln-gelu-lr_e4.ipynb",
      "https://raw.githubusercontent.com/colorfulscoop/tfdlg/master/examples/benchmark-wikitext-v0.1.0/output/tfmodel_train_scratch-wikitext_103_raw-min_gpt-lr_e4.ipynb",
      "https://raw.githubusercontent.com/colorfulscoop/tfdlg/master/examples/benchmark-wikitext-v0.1.0/output/tfmodel_train_scratch-wikitext_103_raw-pre_ln-gelu-lr_e4-clipnorm_none-fp16.ipynb",
      "https://raw.githubusercontent.com/colorfulscoop/tfdlg/master/examples/benchmark-wikitext-v0.1.0/output/tfmodel_train_scratch-wikitext_103_raw-transformers-lr_e4.ipynb",
      "https://raw.githubusercontent.com/colorfulscoop/tfdlg/master/examples/benchmark-wikitext-v0.1.0/output/transformers_train_scratch-wikitext_103_raw-lr_e3.ipynb",
      "https://raw.githubusercontent.com/colorfulscoop/tfdlg/master/examples/benchmark-wikitext-v0.1.0/output/transformers_train_scratch-wikitext_103_raw-lr_e5.ipynb",
      "https://raw.githubusercontent.com/colorfulscoop/tfdlg/master/examples/benchmark-wikitext-v0.1.0/output/tfmodel_train_scratch-wikitext_103_raw-pre_ln-lr_e4.ipynb",
      "https://raw.githubusercontent.com/colorfulscoop/tfdlg/master/examples/benchmark-wikitext-v0.1.0/output/transformers_train_scratch-wikitext_103_raw-lr_e4.ipynb",
      "https://raw.githubusercontent.com/colorfulscoop/tfdlg/master/examples/benchmark-wikitext-v0.1.0/output/tfmodel_train_scratch-wikitext_103_raw-pre_ln-gelu-lr_e4-clipnorm_none-fp16-batch_size_4.ipynb",
      "https://raw.githubusercontent.com/colorfulscoop/tfdlg/master/examples/benchmark-wikitext-v0.1.0/output/tfmodel_train_scratch-wikitext_103_raw-pre_ln-gelu-lr_e4-clipnorm_none.ipynb",
      "https://raw.githubusercontent.com/colorfulscoop/tfdlg/master/examples/benchmark-wikitext-v0.1.0/output/tfmodel_train_scratch-wikitext_103_raw-pre_ln-unshare-lr_e4.ipynb",
      "https://raw.githubusercontent.com/colorfulscoop/tfdlg/master/examples/benchmark-wikitext/tfdlg_train.ipynb",
      "https://raw.githubusercontent.com/colorfulscoop/tfdlg/master/examples/benchmark-wikitext/output/tfmodel_train-pre_ln.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/colorfulscoop/tfdlg/master/examples/transformers-gpt2-ja/get_jawiki.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "In this example, we will use [WikiText-2](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/) to train and evaluate a language model.\ntfDlg provides a simple script to download the corpus from the official web site in the [example/wikitext](example/benchmark-wikitext) directory.\nUse the script [examples/benchmark-wikitext/get_wikitext.py](tfdlg/examples/benchmark-wikitext/get_wikitext.py) to download the data first.\n\n```sh\n$ python ../examples/benchmark-wikitext/get_wikitext.py 2_raw\n```\n\nThis command downloads WikiText-2 consisting of raw level tokens. You can find the corpus under the `wikitext-2-raw` directory.\n\n```sh\n$ ls wikitext-2-raw\nwiki.test.raw  wiki.train.raw wiki.valid.raw\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Prepare your environment with Python >= 3.8, < 3.9 first.\n\nThen run `pip` to install this package from GitHub.\n\n```sh\n$ pip install git+https://github.com/colorfulscoop/tfdlg\n```\n\nYou can run tests with [pytest](https://docs.pytest.org/en/stable/) to make sure your installtion succeeds.\n\n```sh\n$ pip install pytest==6.1.1\n$ pytest tests/\n```\n\n:memo: If you install tfDlg in a container environment, use the corresponded container.\n\n| GPU use | Container | Command example |\n| --- | --- | --- |\n| Yes | tensorflow/tensorflow:2.4.1-gpu bash | `docker container run --gpus all -v $(pwd):/work -w /work --rm -it tensorflow/tensorflow:2.4.1-gpu bash` |\n| No | python:3.8.7-buster | `docker container run -v $(pwd):/work -w /work --rm -it python:3.8.7-buster bash` |\n\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8578618941159004
      ],
      "excerpt": "tfdlg.data provides dataset builders to input them to your model. They generate tf.data.Dataset object \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8709315157984365
      ],
      "excerpt": "$ python train_tokenizer.py tokenizer_model wikitext-2-raw/wiki.train.raw --vocab_size=5000 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.874831946235871
      ],
      "excerpt": "Use the train_model.py script to train your model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.852631247742974,
        0.906614234580968
      ],
      "excerpt": "$ python train_model.py --train_file wikitext-2-raw/wiki.train.raw --valid_file wikitext-2-raw/wiki.valid.raw --tokenizer_model_dir tokenizer_model --save_model_dir=model --epochs=10 --batch_size=4 --fp16 --memory_growth \nLayer (type)                 Output Shape              Param #: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8934745846565956
      ],
      "excerpt": "Total params: 88,857,600 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8112735652505316
      ],
      "excerpt": "Epoch 1/10 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8112735652505316
      ],
      "excerpt": "Epoch 2/10 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8112735652505316
      ],
      "excerpt": "Epoch 3/10 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8112735652505316
      ],
      "excerpt": "Epoch 4/10 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8112735652505316
      ],
      "excerpt": "Epoch 5/10 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8112735652505316
      ],
      "excerpt": "Epoch 6/10 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8112735652505316
      ],
      "excerpt": "Epoch 7/10 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8112735652505316
      ],
      "excerpt": "Epoch 8/10 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8112735652505316
      ],
      "excerpt": "Epoch 9/10 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8145558915392348
      ],
      "excerpt": "Epoch 10/10 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8213210641400173,
        0.8900486270063179
      ],
      "excerpt": "from tfdlg.configs import GPT2SmallConfig \nfrom tfdlg.models import PostLNDecoder \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8213210641400173,
        0.8900486270063179
      ],
      "excerpt": "from tfdlg.configs import GPT2SmallConfig \nfrom tfdlg.models import PreLNDecoder \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/colorfulscoop/tfdlg/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'The MIT License (MIT)\\n\\nCopyright (c) 2020-2021 Noriyuki Abe\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy of\\nthis software and associated documentation files (the \"Software\"), to deal in\\nthe Software without restriction, including without limitation the rights to\\nuse, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\\nthe Software, and to permit persons to whom the Software is furnished to do so,\\nsubject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\\nFOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\\nCOPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\\nIN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\\nCONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# tfDlg\n\n![](https://github.com/colorfulscoop/tfdlg/workflows/unittest/badge.svg)\n\n**tfDlg** is a Python library for transformer-based language models and dialog models with TensorFlow.\n\n:sparkles: Features :sparkles:\n\n* **Simple models:** tfDlg adopts simple and easy-to-understand model implementation to enable users to customize models for their research and interests. You can find the model implementation in [tfdlg/models.py](tfdlg/models.py). You can utilize these models in the usual manner of tf.keras (e.g. you can call compile and build method for them).\n* **Useful utilities:** tfDlg provides several useful utilities. For example,\n  * [tfdlg.data](tfdlg/data.py) provides dataset builders to input them to your model. They generate tf.data.Dataset object\n  * [tfdlg.schedules](tfdlg/schedules.py) provides learning rate schedules to consider warmup steps as well as linear decay.\n  * [tfdlg.losses](tfdlg/losses.py) provides loss function which considers padding.\n  * [tfdlg.eval](tfdlg/eval.py) provides function to calculate perplexity.\n  * [tfdlg.tokenizers](tfdlg/tokenizers.py) provides SentencePiece tokenizer.\n  * [tfdlg.generations](tfdlg/generations.py) provides top-k top-p generator .\n* **Utilities for dialog modeling:** Useful utilities for dialog modeling are provided under the `tfdlg.dialog` namespace.\n  * [tfdlg.dialog.data](tfdlg/dialog/data.py) provides a dataset builder which considers context of the dialog.\n\n## Installation\n\nPrepare your environment with Python >= 3.8, < 3.9 first.\n\nThen run `pip` to install this package from GitHub.\n\n```sh\n$ pip install git+https://github.com/colorfulscoop/tfdlg\n```\n\nYou can run tests with [pytest](https://docs.pytest.org/en/stable/) to make sure your installtion succeeds.\n\n```sh\n$ pip install pytest==6.1.1\n$ pytest tests/\n```\n\n:memo: If you install tfDlg in a container environment, use the corresponded container.\n\n| GPU use | Container | Command example |\n| --- | --- | --- |\n| Yes | tensorflow/tensorflow:2.4.1-gpu bash | `docker container run --gpus all -v $(pwd):/work -w /work --rm -it tensorflow/tensorflow:2.4.1-gpu bash` |\n| No | python:3.8.7-buster | `docker container run -v $(pwd):/work -w /work --rm -it python:3.8.7-buster bash` |\n\n## Usage\n\ntfDlg provides two ways to use in ways of **script-based** and **package-based**.\n\ntfDlg is a Python package to enable you to use all the functionalities from your Python scripts. This usual way to use tfDlg as a Python package is called a package-based usage here.\n\nOn the other hand, script-based utilizes the pacakge to provide fundamental scripts for training, evaluation and serving your models.\nIn this viewpoint, script-based can be considered as examples of how to use tfDlg as a Python package.\n\nTake a look at the script-based usage first.\n\n### Script-based usage\n\nGet scripts from GitHub first. Then install dependencies.\n\n```sh\n$ git clone https://github.com/colorfulscoop/tfdlg\n$ cd scripts\n$ pip install -r requirements.txt\n```\n\n#### Prepare corpus\n\nIn this example, we will use [WikiText-2](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/) to train and evaluate a language model.\ntfDlg provides a simple script to download the corpus from the official web site in the [example/wikitext](example/benchmark-wikitext) directory.\nUse the script [examples/benchmark-wikitext/get_wikitext.py](tfdlg/examples/benchmark-wikitext/get_wikitext.py) to download the data first.\n\n```sh\n$ python ../examples/benchmark-wikitext/get_wikitext.py 2_raw\n```\n\nThis command downloads WikiText-2 consisting of raw level tokens. You can find the corpus under the `wikitext-2-raw` directory.\n\n```sh\n$ ls wikitext-2-raw\nwiki.test.raw  wiki.train.raw wiki.valid.raw\n```\n\n#### Train tokenizer\n\nFirst of all, you need train your tokenizer. Currently, only [SentencePiece](https://github.com/google/sentencepiece) tokenizer is available.\n\n```sh\n$ python train_tokenizer.py tokenizer_model wikitext-2-raw/wiki.train.raw --vocab_size=5000\n```\n\n:memo: If you train a tokenizer for languages which do not separate words with white spaces, consider to use `--add_dummy_prefix=False` option to avoid adding a dummy white space at the beginnin of a text (detault is `True` to add a white space at the beginning of a text).\n\n\n#### Train model\n\nUse the `train_model.py` script to train your model.\n\n```sh\n$ python train_model.py --train_file wikitext-2-raw/wiki.train.raw --valid_file wikitext-2-raw/wiki.valid.raw --tokenizer_model_dir tokenizer_model --save_model_dir=model --epochs=10 --batch_size=4 --fp16 --memory_growth\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "tfdlg",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "colorfulscoop",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/colorfulscoop/tfdlg/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Sun, 12 Dec 2021 17:51:36 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "tensorflow",
      "conversational-ai",
      "transformer",
      "gpt",
      "gpt-2",
      "gpt2"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "tfDlg provides two ways to use in ways of **script-based** and **package-based**.\n\ntfDlg is a Python package to enable you to use all the functionalities from your Python scripts. This usual way to use tfDlg as a Python package is called a package-based usage here.\n\nOn the other hand, script-based utilizes the pacakge to provide fundamental scripts for training, evaluation and serving your models.\nIn this viewpoint, script-based can be considered as examples of how to use tfDlg as a Python package.\n\nTake a look at the script-based usage first.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Get scripts from GitHub first. Then install dependencies.\n\n```sh\n$ git clone https://github.com/colorfulscoop/tfdlg\n$ cd scripts\n$ pip install -r requirements.txt\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "As a basic usage, you first need to import config files and models from the package.\n\n```py\nfrom tfdlg.configs import GPT2SmallConfig\nfrom tfdlg.models import PreLNDecoder\n\nconfig = GPT2SmallConfig()\nmodel = PreLNDecoder(config)\n```\n\nThen you can train here in the usual manner of training Tensorflow Keras models by `fit` method.\n\nAfter training your model, `save_model` saves the model parameter as well as the model hyper parameters which are specified in `tfdlg.configs.Config` class.\n\n```py\nfrom tfdlg.utils import save_model\nsave_model(\"path/to/save/dir\", model, config)\n```\n\n`load_model` can be used to load your model from the directory where you saved it.\n\n```py\nfrom tfdlg.utils import load_model\nmodel = load_model(\"path/to/save/dir\")\n```\n\nCheck more details in the scripts which are used for script-based usage.\n\n",
      "technique": "Header extraction"
    }
  ]
}