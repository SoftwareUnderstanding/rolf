{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Our work builds on and uses code from [timm](https://github.com/rwightman/pytorch-image-models). \nWe'd like to thank the author for making these libraries available.\n\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1907.01341v3",
      "https://arxiv.org/abs/2103.13413",
      "https://arxiv.org/abs/2103.13413"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please cite our paper if you use this code or any of the models:\n```\n@article{Ranftl2020,\n\tauthor    = {Ren\\'{e} Ranftl and Katrin Lasinger and David Hafner and Konrad Schindler and Vladlen Koltun},\n\ttitle     = {Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer},\n\tjournal   = {IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},\n\tyear      = {2020},\n}\n```\n\nIf you use a DPT-based model, please also cite:\n\n```\n@article{Ranftl2021,\n\tauthor    = {Ren\\'{e} Ranftl and Alexey Bochkovskiy and Vladlen Koltun},\n\ttitle     = {Vision Transformers for Dense Prediction},\n\tjournal   = {ArXiv preprint},\n\tyear      = {2021},\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{Ranftl2021,\n    author    = {Ren\\'{e} Ranftl and Alexey Bochkovskiy and Vladlen Koltun},\n    title     = {Vision Transformers for Dense Prediction},\n    journal   = {ArXiv preprint},\n    year      = {2021},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{Ranftl2020,\n    author    = {Ren\\'{e} Ranftl and Katrin Lasinger and David Hafner and Konrad Schindler and Vladlen Koltun},\n    title     = {Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer},\n    journal   = {IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},\n    year      = {2020},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.904711075158732,
        0.9443700328633208
      ],
      "excerpt": "and our preprint: \nVision Transformers for Dense Prediction \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8109194328925066
      ],
      "excerpt": "multi-objective optimization.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9450382385238081
      ],
      "excerpt": "[Jul 2019] Initial release of MiDaS (Link) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9072564759784449
      ],
      "excerpt": "| MiDaS v2.1 small URL | 0.1344 | 0.1344 | 0.3370 | 29.27 | 13.43 | 14.53 | 30 | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ahmedmostafa0x61/Depth_Estimation",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-07-06T12:47:06Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-09-02T20:36:59Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9760034837476667
      ],
      "excerpt": "This repository contains code to compute depth from a single image. It accompanies our paper: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8388545815678734
      ],
      "excerpt": "and our preprint: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.874575693899046
      ],
      "excerpt": "The original model that was trained on 5 datasets  (MIX 5 in the paper) can be found here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9692961491641598
      ],
      "excerpt": "New models based on Dense Prediction Transformers are on average 21% more accurate than MiDaS v2.1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9109479855828322,
        0.933809934830376
      ],
      "excerpt": "New model that was trained on 10 datasets and is on average about 10% more accurate than MiDaS v2.0 \nNew light-weight model that achieves real-time performance on mobile platforms. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9174252196914411
      ],
      "excerpt": "ROS package for easy deployment on robots \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.969156697809594
      ],
      "excerpt": "[Dec 2019] Released new version of MiDaS - the new model is significantly more accurate and robust \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9818997859888628,
        0.936563705618703,
        0.9681025967257751,
        0.936563705618703,
        0.936563705618703,
        0.9681025967257751
      ],
      "excerpt": "The pretrained model is also available on PyTorch Hub \nSee README in the tf subdirectory. \nCurrently only supports MiDaS v2.1. DPT-based models to be added. \nSee README in the mobile subdirectory. \nSee README in the ros subdirectory. \nCurrently only supports MiDaS v2.1. DPT-based models to be added. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Simple Desktop app for Single Image Depth Estimation using Midas",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ahmedmostafa0x61/Depth_Estimation/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Fri, 10 Dec 2021 05:48:18 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ahmedmostafa0x61/Depth_Estimation/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "ahmedmostafa0x61/Depth_Estimation",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1) Pick one or more models and download corresponding weights to the `weights` folder:\n\n- For highest quality: [dpt_large](https://github.com/intel-isl/DPT/releases/download/1_0/dpt_large-midas-2f21e586.pt)\n- For moderately less quality, but better speed on CPU and slower GPUs: [dpt_hybrid](https://github.com/intel-isl/DPT/releases/download/1_0/dpt_hybrid-midas-501f0c75.pt)\n- For real-time applications on resource-constrained devices: [midas_v21_small](https://github.com/AlexeyAB/MiDaS/releases/download/midas_dpt/midas_v21_small-70d6b9c8.pt)\n- Legacy convolutional model: [midas_v21](https://github.com/AlexeyAB/MiDaS/releases/download/midas_dpt/midas_v21-f6b98070.pt)\n\n2) Set up dependencies: \n\n    ```shell\n    conda install pytorch torchvision opencv\n    pip install timm\n    ```\n\n   The code was tested with Python 3.7, PyTorch 1.8.0, OpenCV 4.5.1, and timm 0.4.5.\n\n    \n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9566457327786118
      ],
      "excerpt": "| Big models: | | | | | | | GPU RTX 3090 | \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.867297245828706
      ],
      "excerpt": "docker run --rm --gpus all -v $PWD/input:/opt/MiDaS/input -v $PWD/output:/opt/MiDaS/output midas \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8012863039498485
      ],
      "excerpt": "| Model |  DIW, WHDR | Eth3d, AbsRel | Sintel, AbsRel | Kitti, \u03b4>1.25 | NyuDepthV2, \u03b4>1.25 | TUM, \u03b4>1.25 | Speed, FPS | \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ahmedmostafa0x61/Depth_Estimation/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 Intel ISL (Intel Intelligent Systems Lab)\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Depth_Estimation",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "ahmedmostafa0x61",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ahmedmostafa0x61/Depth_Estimation/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Fri, 10 Dec 2021 05:48:18 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1) Place one or more input images in the folder `input`.\n\n2) Run the model:\n\n    ```shell\n    python run.py --model_type dpt_large\n    python run.py --model_type dpt_hybrid \n    python run.py --model_type midas_v21_small\n    python run.py --model_type midas_v21\n    ```\n\n3) The resulting inverse depth maps are written to the `output` folder.\n\n\n",
      "technique": "Header extraction"
    }
  ]
}