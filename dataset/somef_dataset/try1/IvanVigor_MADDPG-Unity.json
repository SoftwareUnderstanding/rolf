{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1509.02971\nReacher Challenge - https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#reacher\n\n## Author\n\nIvan Vigorito\n\n##  License\nThe code is provided with MIT license \n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Deep Deterministic Policy Gradient - https://arxiv.org/abs/1509.02971\nReacher Challenge - https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#reacher\n\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/IvanVigor/MADDPG-Unity",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-03-13T20:57:44Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-11-14T16:18:29Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9951457417691238,
        0.9887586092487681,
        0.9874232373116522,
        0.9476680339258932,
        0.8990420647077751
      ],
      "excerpt": "In this project, I adopted a Multi-Agent Deep Deterministic Policy Gradien for creating two agents with are in charge of collaborate and compete for playing a tennis match. The environment is the similar to the Unity Tennis one.  \nIn this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1. If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01. Thus, the goal of each agent is to keep the ball in play. \nThe observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Each agent receives its own, local observation. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \nThe task is episodic, and in order to solve the environment, your agents must get an average score of +0.5 (over 100 consecutive episodes, after taking the maximum over both agents). Specifically, \nAfter each episode, we add up the rewards that each agent received (without discounting), to get a score for each agent. This yields 2 (potentially different) scores. We then take the maximum of these 2 scores. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.846125212482974
      ],
      "excerpt": "scripts/model.py: topology of PyTorch networks \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Multi-Agent Deep Deterministic Policy Gradient applied in Unity Tennis environment",
      "technique": "GitHub API"
    }
  ],
  "documentation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "http://ipython.readthedocs.io/",
      "technique": "Regular expression"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/IvanVigor/MADDPG-Unity/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sun, 05 Dec 2021 11:02:23 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/IvanVigor/MADDPG-Unity/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "IvanVigor/MADDPG-Unity",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/IvanVigor/MADDPG-Unity/master/Tennis.ipynb",
      "https://raw.githubusercontent.com/IvanVigor/MADDPG-Unity/master/.ipynb_checkpoints/Tennis-checkpoint.ipynb",
      "https://raw.githubusercontent.com/IvanVigor/MADDPG-Unity/master/weights/Tennis_load.ipynb",
      "https://raw.githubusercontent.com/IvanVigor/MADDPG-Unity/master/weights/.ipynb_checkpoints/Tennis_load-checkpoint.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8062965854842815
      ],
      "excerpt": "The model has been developed using PyTorch library. The Pytorch library is available over the main page: https://pytorch.org/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9951750829147036,
        0.8523398070052144
      ],
      "excerpt": "conda install pytorch torchvision -c pytorch \nIn addition to PyTorch, in this repository has been used also Numpy. Numpy is already installed in Anaconda, otherwise you can use: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9322609392449874,
        0.8233588558014837
      ],
      "excerpt": "PyTorch  \nNumpy  \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/IvanVigor/MADDPG-Unity/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 Ivan Vigorito\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "MADDPG-Unity-Env",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "MADDPG-Unity",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "IvanVigor",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/IvanVigor/MADDPG-Unity/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Sun, 05 Dec 2021 11:02:23 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To set up your python environment to run the code in this repository, follow the instructions below.\n\n1. Create (and activate) a new environment with Python 3.6.\n\n    - __Linux__ or __Mac__: \n    ```bash\n    conda create --name drlnd python=3.6\n    source activate drlnd\n    ```\n    - __Windows__: \n    ```bash\n    conda create --name drlnd python=3.6 \n    activate drlnd\n    ```\n    \n2. Follow the instructions in [this repository](https://github.com/openai/gym) to perform a minimal install of OpenAI gym.  \n    - Next, install the **classic control** environment group by following the instructions [here](https://github.com/openai/gym#classic-control).\n    - Then, install the **box2d** environment group by following the instructions [here](https://github.com/openai/gym#box2d).\n    \n3. Clone the repository (if you haven't already!), and navigate to the `python/` folder.  Then, install several dependencies.\n\n```bash\ngit clone https://github.com/udacity/deep-reinforcement-learning.git\ncd deep-reinforcement-learning/python\npip install .\n```\n4. Create an [IPython kernel](http://ipython.readthedocs.io/en/stable/install/kernel_install.html) for the `drlnd` environment.  \n```bash\npython -m ipykernel install --user --name drlnd --display-name \"drlnd\"\n```\n\n5. Before running code in a notebook, change the kernel to match the `drlnd` environment by using the drop-down `Kernel` menu. \n\n",
      "technique": "Header extraction"
    }
  ]
}