{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Thanks to the deep learning community and especially to the contributers of the pytorch ecosystem.\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1712.00559"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.8885280170037176
      ],
      "excerpt": "- 17/07/2017: BNInception pretrained on Imagenet \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Cadene/pretrained-models.pytorch",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-04-09T15:54:23Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-08T13:27:31Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- [Installation](https://github.com/Cadene/pretrained-models.pytorch#installation)\n- [Quick examples](https://github.com/Cadene/pretrained-models.pytorch#quick-examples)\n- [Few use cases](https://github.com/Cadene/pretrained-models.pytorch#few-use-cases)\n    - [Compute imagenet logits](https://github.com/Cadene/pretrained-models.pytorch#compute-imagenet-logits)\n    - [Compute imagenet validation metrics](https://github.com/Cadene/pretrained-models.pytorch#compute-imagenet-validation-metrics)\n- [Evaluation on ImageNet](https://github.com/Cadene/pretrained-models.pytorch#evaluation-on-imagenet)\n    - [Accuracy on valset](https://github.com/Cadene/pretrained-models.pytorch#accuracy-on-validation-set)\n    - [Reproducing results](https://github.com/Cadene/pretrained-models.pytorch#reproducing-results)\n- [Documentation](https://github.com/Cadene/pretrained-models.pytorch#documentation)\n    - [Available models](https://github.com/Cadene/pretrained-models.pytorch#available-models)\n        - [AlexNet](https://github.com/Cadene/pretrained-models.pytorch#torchvision)\n        - [BNInception](https://github.com/Cadene/pretrained-models.pytorch#bninception)\n        - [CaffeResNet101](https://github.com/Cadene/pretrained-models.pytorch#caffe-resnet)\n        - [DenseNet121](https://github.com/Cadene/pretrained-models.pytorch#torchvision)\n        - [DenseNet161](https://github.com/Cadene/pretrained-models.pytorch#torchvision)\n        - [DenseNet169](https://github.com/Cadene/pretrained-models.pytorch#torchvision)\n        - [DenseNet201](https://github.com/Cadene/pretrained-models.pytorch#torchvision)\n        - [DenseNet201](https://github.com/Cadene/pretrained-models.pytorch#torchvision)\n        - [DualPathNet68](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks)\n        - [DualPathNet92](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks)\n        - [DualPathNet98](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks)\n        - [DualPathNet107](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks)\n        - [DualPathNet113](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks)\n        - [FBResNet152](https://github.com/Cadene/pretrained-models.pytorch#facebook-resnet)\n        - [InceptionResNetV2](https://github.com/Cadene/pretrained-models.pytorch#inception)\n        - [InceptionV3](https://github.com/Cadene/pretrained-models.pytorch#inception)\n        - [InceptionV4](https://github.com/Cadene/pretrained-models.pytorch#inception)\n        - [NASNet-A-Large](https://github.com/Cadene/pretrained-models.pytorch#nasnet)\n        - [NASNet-A-Mobile](https://github.com/Cadene/pretrained-models.pytorch#nasnet)\n        - [PNASNet-5-Large](https://github.com/Cadene/pretrained-models.pytorch#pnasnet)\n        - [PolyNet](https://github.com/Cadene/pretrained-models.pytorch#polynet)\n        - [ResNeXt101_32x4d](https://github.com/Cadene/pretrained-models.pytorch#resnext)\n        - [ResNeXt101_64x4d](https://github.com/Cadene/pretrained-models.pytorch#resnext)\n        - [ResNet101](https://github.com/Cadene/pretrained-models.pytorch#torchvision)\n        - [ResNet152](https://github.com/Cadene/pretrained-models.pytorch#torchvision)\n        - [ResNet18](https://github.com/Cadene/pretrained-models.pytorch#torchvision)\n        - [ResNet34](https://github.com/Cadene/pretrained-models.pytorch#torchvision)\n        - [ResNet50](https://github.com/Cadene/pretrained-models.pytorch#torchvision)\n        - [SENet154](https://github.com/Cadene/pretrained-models.pytorch#senet)\n        - [SE-ResNet50](https://github.com/Cadene/pretrained-models.pytorch#senet)\n        - [SE-ResNet101](https://github.com/Cadene/pretrained-models.pytorch#senet)\n        - [SE-ResNet152](https://github.com/Cadene/pretrained-models.pytorch#senet)\n        - [SE-ResNeXt50_32x4d](https://github.com/Cadene/pretrained-models.pytorch#senet)\n        - [SE-ResNeXt101_32x4d](https://github.com/Cadene/pretrained-models.pytorch#senet)\n        - [SqueezeNet1_0](https://github.com/Cadene/pretrained-models.pytorch#torchvision)\n        - [SqueezeNet1_1](https://github.com/Cadene/pretrained-models.pytorch#torchvision)\n        - [VGG11](https://github.com/Cadene/pretrained-models.pytorch#torchvision)\n        - [VGG13](https://github.com/Cadene/pretrained-models.pytorch#torchvision)\n        - [VGG16](https://github.com/Cadene/pretrained-models.pytorch#torchvision)\n        - [VGG19](https://github.com/Cadene/pretrained-models.pytorch#torchvision)\n        - [VGG11_BN](https://github.com/Cadene/pretrained-models.pytorch#torchvision)\n        - [VGG13_BN](https://github.com/Cadene/pretrained-models.pytorch#torchvision)\n        - [VGG16_BN](https://github.com/Cadene/pretrained-models.pytorch#torchvision)\n        - [VGG19_BN](https://github.com/Cadene/pretrained-models.pytorch#torchvision)\n        - [Xception](https://github.com/Cadene/pretrained-models.pytorch#xception)\n    - [Model API](https://github.com/Cadene/pretrained-models.pytorch#model-api)\n        - [model.input_size](https://github.com/Cadene/pretrained-models.pytorch#modelinput_size)\n        - [model.input_space](https://github.com/Cadene/pretrained-models.pytorch#modelinput_space)\n        - [model.input_range](https://github.com/Cadene/pretrained-models.pytorch#modelinput_range)\n        - [model.mean](https://github.com/Cadene/pretrained-models.pytorch#modelmean)\n        - [model.std](https://github.com/Cadene/pretrained-models.pytorch#modelstd)\n        - [model.features](https://github.com/Cadene/pretrained-models.pytorch#modelfeatures)\n        - [model.logits](https://github.com/Cadene/pretrained-models.pytorch#modellogits)\n        - [model.forward](https://github.com/Cadene/pretrained-models.pytorch#modelforward)\n- [Reproducing porting](https://github.com/Cadene/pretrained-models.pytorch#reproducing)\n    - [ResNet*](https://github.com/Cadene/pretrained-models.pytorch#hand-porting-of-resnet152)\n    - [ResNeXt*](https://github.com/Cadene/pretrained-models.pytorch#automatic-porting-of-resnext)\n    - [Inception*](https://github.com/Cadene/pretrained-models.pytorch#hand-porting-of-inceptionv4-and-inceptionresnetv2)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9770643560044698,
        0.9334724141754942
      ],
      "excerpt": "The goal of this repo is: \nto help to reproduce research papers results (transfer learning setups for instance), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8825336052138127
      ],
      "excerpt": "- 30/11/2017: improve API (model.features(input), model.logits(features), model.forward(input), model.last_linear) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8485520954664574
      ],
      "excerpt": "- 22/07/2017: momentum in inceptionv4 and inceptionresnetv2 to 0.1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9551878526686561
      ],
      "excerpt": "See examples/imagenet_logits.py to compute logits of classes appearance over a single image with a pretrained model on imagenet. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.991399381191609
      ],
      "excerpt": "'nasnetalarge': data/cat.jpg' is a 'tiger cat'  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8339155295296953,
        0.9536460350782316
      ],
      "excerpt": "Source: Torch7 repo of FaceBook \nThere are a bit different from the ResNet* of torchvision. ResNet152 is currently the only one available. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9106567041018739
      ],
      "excerpt": "Source: ResNeXt repo of FaceBook \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9298171266392449
      ],
      "excerpt": "As you can see here DualPathNetworks allows you to try different scales. The default one in this repo is 0.875 meaning that the original input size is 256 before croping to 224. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8514474425974831
      ],
      "excerpt": "Source: Caffe repo of the CUHK Multimedia Lab \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8191818563765993
      ],
      "excerpt": "vgg16(num_classes=1000, pretrained='imagenet') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8093951690693796,
        0.8514768458546002
      ],
      "excerpt": "Important note: All image must be loaded using PIL which scales the pixel values between 0 and 1. \nAttribut of type list composed of 3 numbers: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9342843419373491,
        0.8605324733834179
      ],
      "excerpt": "height of the input image, \nwidth of the input image. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8567476229121087,
        0.9360335127811564,
        0.8514768458546002
      ],
      "excerpt": "[3, 224, 224] for resnet* networks. \nAttribut of type str representating the color space of the image. Can be RGB or BGR. \nAttribut of type list composed of 2 numbers: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.868129373909536
      ],
      "excerpt": "[0, 1] for resnet and inception networks, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8723444579108433
      ],
      "excerpt": "Attribut of type list composed of 3 numbers which are used to normalize the input image (substract \"color-channel-wise\"). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8567476229121087,
        0.8723444579108433
      ],
      "excerpt": "[0.485, 0.456, 0.406] for resnet* networks. \nAttribut of type list composed of 3 numbers which are used to normalize the input image (divide \"color-channel-wise\"). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8567476229121087
      ],
      "excerpt": "[0.229, 0.224, 0.225] for resnet* networks. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9808086021742689,
        0.959245354812591
      ],
      "excerpt": "Method which is used to extract the features from the image. \nExample when the model is loaded using fbresnet152: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9907272014107006,
        0.959245354812591
      ],
      "excerpt": "Method which is used to classify the features from the image. \nExample when the model is loaded using fbresnet152: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9649808750582173,
        0.962120441388205
      ],
      "excerpt": "Method used to call model.features and model.logits. It can be overwritten as desired. \nNote: A good practice is to use model.__call__ as your function of choice to forward an input to your model. See the example bellow. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8739313833129171
      ],
      "excerpt": ": With model.call \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9515561417492426
      ],
      "excerpt": "Attribut of type nn.Linear. This module is the last one to be called during the forward pass. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.959245354812591
      ],
      "excerpt": "Example when the model is loaded using fbresnet152: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "dim_feats = model.last_linear.in_features #: =2048 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8640898813428294
      ],
      "excerpt": "model.last_linear = nn.Linear(dim_feats, nb_classes) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Pretrained ConvNets for pytorch: NASNet, ResNeXt, ResNet, InceptionV4, InceptionResnetV2, Xception, DPN, etc.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Cadene/pretrained-models.pytorch/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1767,
      "date": "Thu, 09 Dec 2021 07:24:01 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Cadene/pretrained-models.pytorch/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Cadene/pretrained-models.pytorch",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Results were obtained using (center cropped) images of the same size than during the training process.\n\nModel | Version | Acc@1 | Acc@5\n--- | --- | --- | ---\nPNASNet-5-Large | [Tensorflow](https://github.com/tensorflow/models/tree/master/research/slim) | 82.858 | 96.182\n[PNASNet-5-Large](https://github.com/Cadene/pretrained-models.pytorch#pnasnet) | Our porting | 82.736 | 95.992\nNASNet-A-Large | [Tensorflow](https://github.com/tensorflow/models/tree/master/research/slim) | 82.693 | 96.163\n[NASNet-A-Large](https://github.com/Cadene/pretrained-models.pytorch#nasnet) | Our porting | 82.566 | 96.086\nSENet154 | [Caffe](https://github.com/hujie-frank/SENet) | 81.32 | 95.53\n[SENet154](https://github.com/Cadene/pretrained-models.pytorch#senet) | Our porting | 81.304 | 95.498\nPolyNet | [Caffe](https://github.com/CUHK-MMLAB/polynet) | 81.29 | 95.75\n[PolyNet](https://github.com/Cadene/pretrained-models.pytorch#polynet) | Our porting | 81.002 | 95.624\nInceptionResNetV2 | [Tensorflow](https://github.com/tensorflow/models/tree/master/slim) | 80.4 | 95.3\nInceptionV4 | [Tensorflow](https://github.com/tensorflow/models/tree/master/slim) | 80.2 | 95.3\n[SE-ResNeXt101_32x4d](https://github.com/Cadene/pretrained-models.pytorch#senet) | Our porting | 80.236 | 95.028\nSE-ResNeXt101_32x4d | [Caffe](https://github.com/hujie-frank/SENet) | 80.19 | 95.04\n[InceptionResNetV2](https://github.com/Cadene/pretrained-models.pytorch#inception) | Our porting | 80.170 | 95.234\n[InceptionV4](https://github.com/Cadene/pretrained-models.pytorch#inception) | Our porting | 80.062 | 94.926\n[DualPathNet107_5k](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks) | Our porting | 79.746 | 94.684\nResNeXt101_64x4d | [Torch7](https://github.com/facebookresearch/ResNeXt) | 79.6 | 94.7\n[DualPathNet131](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks) | Our porting | 79.432 | 94.574\n[DualPathNet92_5k](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks) | Our porting | 79.400 | 94.620\n[DualPathNet98](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks) | Our porting | 79.224 | 94.488\n[SE-ResNeXt50_32x4d](https://github.com/Cadene/pretrained-models.pytorch#senet) | Our porting | 79.076 | 94.434\nSE-ResNeXt50_32x4d | [Caffe](https://github.com/hujie-frank/SENet) | 79.03 | 94.46\n[Xception](https://github.com/Cadene/pretrained-models.pytorch#xception) | [Keras](https://github.com/keras-team/keras/blob/master/keras/applications/xception.py) | 79.000 | 94.500\n[ResNeXt101_64x4d](https://github.com/Cadene/pretrained-models.pytorch#resnext) | Our porting | 78.956 | 94.252\n[Xception](https://github.com/Cadene/pretrained-models.pytorch#xception) | Our porting | 78.888 | 94.292\nResNeXt101_32x4d | [Torch7](https://github.com/facebookresearch/ResNeXt) | 78.8 | 94.4\nSE-ResNet152 | [Caffe](https://github.com/hujie-frank/SENet) | 78.66 | 94.46\n[SE-ResNet152](https://github.com/Cadene/pretrained-models.pytorch#senet) | Our porting | 78.658 | 94.374\nResNet152 | [Pytorch](https://github.com/pytorch/vision#models) | 78.428 | 94.110\n[SE-ResNet101](https://github.com/Cadene/pretrained-models.pytorch#senet) | Our porting | 78.396 | 94.258\nSE-ResNet101 | [Caffe](https://github.com/hujie-frank/SENet) | 78.25 | 94.28\n[ResNeXt101_32x4d](https://github.com/Cadene/pretrained-models.pytorch#resnext) | Our porting | 78.188 | 93.886\nFBResNet152 | [Torch7](https://github.com/facebook/fb.resnet.torch) | 77.84 | 93.84\nSE-ResNet50 | [Caffe](https://github.com/hujie-frank/SENet) | 77.63 | 93.64\n[SE-ResNet50](https://github.com/Cadene/pretrained-models.pytorch#senet) | Our porting | 77.636 | 93.752\n[DenseNet161](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 77.560 | 93.798\n[ResNet101](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 77.438 | 93.672\n[FBResNet152](https://github.com/Cadene/pretrained-models.pytorch#facebook-resnet) | Our porting | 77.386 | 93.594\n[InceptionV3](https://github.com/Cadene/pretrained-models.pytorch#inception) | [Pytorch](https://github.com/pytorch/vision#models) | 77.294 | 93.454\n[DenseNet201](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 77.152 | 93.548\n[DualPathNet68b_5k](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks) | Our porting | 77.034 | 93.590\n[CaffeResnet101](https://github.com/Cadene/pretrained-models.pytorch#caffe-resnet) | [Caffe](https://github.com/KaimingHe/deep-residual-networks) | 76.400 | 92.900\n[CaffeResnet101](https://github.com/Cadene/pretrained-models.pytorch#caffe-resnet) | Our porting | 76.200 | 92.766\n[DenseNet169](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 76.026 | 92.992\n[ResNet50](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 76.002 | 92.980\n[DualPathNet68](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks) | Our porting | 75.868 | 92.774\n[DenseNet121](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 74.646 | 92.136\n[VGG19_BN](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 74.266 | 92.066\nNASNet-A-Mobile | [Tensorflow](https://github.com/tensorflow/models/tree/master/research/slim) | 74.0 | 91.6\n[NASNet-A-Mobile](https://github.com/veronikayurchuk/pretrained-models.pytorch/blob/master/pretrainedmodels/models/nasnet_mobile.py) | Our porting | 74.080 | 91.740\n[ResNet34](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 73.554 | 91.456\n[BNInception](https://github.com/Cadene/pretrained-models.pytorch#bninception) | Our porting | 73.524 | 91.562\n[VGG16_BN](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 73.518 | 91.608\n[VGG19](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 72.080 | 90.822\n[VGG16](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 71.636 | 90.354\n[VGG13_BN](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 71.508 | 90.494\n[VGG11_BN](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 70.452 | 89.818\n[ResNet18](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 70.142 | 89.274\n[VGG13](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 69.662 | 89.264\n[VGG11](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 68.970 | 88.746\n[SqueezeNet1_1](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 58.250 | 80.800\n[SqueezeNet1_0](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 58.108 | 80.428\n[Alexnet](https://github.com/Cadene/pretrained-models.pytorch#torchvision) | [Pytorch](https://github.com/pytorch/vision#models) | 56.432 | 79.194\n\nNotes:\n- the Pytorch version of ResNet152 is not a porting of the Torch7 but has been retrained by facebook.\n- For the PolyNet evaluation each image was resized to 378x378 without preserving the aspect ratio and then the central 331\u00d7331 patch from the resulting image was used.\n\nBeware, the accuracy reported here is not always representative of the transferable capacity of the network on other tasks and datasets. You must try them all! :P\n    \n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "3. `git clone https://github.com/Cadene/pretrained-models.pytorch.git`\n4. `cd pretrained-models.pytorch`\n5. `python setup.py install`\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "3. `pip install pretrainedmodels`\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "1. [python3 with anaconda](https://www.continuum.io/downloads)\n2. [pytorch with/out CUDA](http://pytorch.org)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8357527010818722
      ],
      "excerpt": "<a href=\"https://travis-ci.org/Cadene/pretrained-models.pytorch\"><img src=\"https://api.travis-ci.org/Cadene/pretrained-models.pytorch.svg?branch=master\"/></a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8988905465892447
      ],
      "excerpt": "- 13/01/2018: pip install pretrainedmodels, pretrainedmodels.model_names, pretrainedmodels.pretrained_settings \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9057824142888429
      ],
      "excerpt": "Source: TensorFlow Slim repo \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8773226935081564
      ],
      "excerpt": "Source: TensorFlow Slim repo and Pytorch/Vision repo for inceptionv3 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8204891912088107,
        0.8798373593354343
      ],
      "excerpt": "Source: MXNET repo of Chen Yunpeng \nThe porting has been made possible by Ross Wightman in his PyTorch repo. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9193363101701191
      ],
      "excerpt": "Source: Keras repo \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9057824142888429
      ],
      "excerpt": "Source: TensorFlow Slim repo \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9504830231550649
      ],
      "excerpt": "Source: Pytorch/Vision repo \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9563277988753164
      ],
      "excerpt": "$ python examples/imagenet_logits.py -h \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9739026222428298,
        0.8505301117208606
      ],
      "excerpt": "$ python examples/imagenet_logits.py -a nasnetalarge --path_img data/cat.jpg \n'nasnetalarge': data/cat.jpg' is a 'tiger cat'  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8959795319824979
      ],
      "excerpt": "See examples/imagenet_eval.py to evaluate pretrained models on imagenet valset.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8842782298427034
      ],
      "excerpt": "$ python examples/imagenet_eval.py /local/common-data/imagenet_2012/images -a nasnetalarge -b 20 -e \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8264968400319402
      ],
      "excerpt": "nasnetalarge(num_classes=1001, pretrained='imagenet+background') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8264968400319402
      ],
      "excerpt": "inceptionresnetv2(num_classes=1001, pretrained='imagenet+background') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8264968400319402
      ],
      "excerpt": "inceptionv4(num_classes=1001, pretrained='imagenet+background') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8264968400319402
      ],
      "excerpt": "pnasnet5large(num_classes=1001, pretrained='imagenet+background') \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8323213690260282,
        0.8731562459058029
      ],
      "excerpt": "min pixel value, \nmax pixel value. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.919363037574928
      ],
      "excerpt": "print(input_224.size())            #: (1,3,224,224) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9414325779374935,
        0.919363037574928
      ],
      "excerpt": "print(output.size())               #: (1,2048,1,1) \n: print(input_448.size())          #: (1,3,448,448) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9414325779374935
      ],
      "excerpt": ": print(output.size())             #: (1,2048,7,7) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9414325779374935,
        0.8556702004934098,
        0.9414325779374935
      ],
      "excerpt": "print(output.size())               #: (1,2048, 1, 1) \noutput = model.logits(output) \nprint(output.size())               #: (1,1000) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.804920205996709,
        0.9414325779374935
      ],
      "excerpt": "output = model.forward(input_224) \nprint(output.size())      #: (1,1000) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.857129346874625,
        0.9414325779374935
      ],
      "excerpt": "output = model(input_224) \nprint(output.size())      #: (1,1000) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.919363037574928
      ],
      "excerpt": "print(input_224.size())            #: (1,3,224,224) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9414325779374935,
        0.8556702004934098,
        0.9414325779374935
      ],
      "excerpt": "print(output.size())               #: (1,2048,1,1) \noutput = model.logits(output) \nprint(output.size())               #: (1,1000) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.857129346874625,
        0.9414325779374935
      ],
      "excerpt": "output = model(input_224) \nprint(output.size())               #: (1,4) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8649779996320799,
        0.857129346874625,
        0.9414325779374935
      ],
      "excerpt": "model.last_linear = pretrained.utils.Identity() \noutput = model(input_224) \nprint(output.size())               #: (1,2048) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python pretrainedmodels/fbresnet/resnet152_load.py \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Cadene/pretrained-models.pytorch/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Lua"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "BSD 3-Clause \"New\" or \"Revised\" License",
      "url": "https://api.github.com/licenses/bsd-3-clause"
    },
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Pretrained models for Pytorch (Work in progress)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "pretrained-models.pytorch",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Cadene",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Cadene/pretrained-models.pytorch/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 8299,
      "date": "Thu, 09 Dec 2021 07:24:01 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "imagenet",
      "resnet",
      "resnext",
      "pretrained",
      "pytorch",
      "inception"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- To import `pretrainedmodels`:\n\n```python\nimport pretrainedmodels\n```\n\n- To print the available pretrained models:\n\n```python\nprint(pretrainedmodels.model_names)\n> ['fbresnet152', 'bninception', 'resnext101_32x4d', 'resnext101_64x4d', 'inceptionv4', 'inceptionresnetv2', 'alexnet', 'densenet121', 'densenet169', 'densenet201', 'densenet161', 'resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152', 'inceptionv3', 'squeezenet1_0', 'squeezenet1_1', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19_bn', 'vgg19', 'nasnetalarge', 'nasnetamobile', 'cafferesnet101', 'senet154',  'se_resnet50', 'se_resnet101', 'se_resnet152', 'se_resnext50_32x4d', 'se_resnext101_32x4d', 'cafferesnet101', 'polynet', 'pnasnet5large']\n```\n\n- To print the available pretrained settings for a chosen model:\n\n```python\nprint(pretrainedmodels.pretrained_settings['nasnetalarge'])\n> {'imagenet': {'url': 'http://data.lip6.fr/cadene/pretrainedmodels/nasnetalarge-a1897284.pth', 'input_space': 'RGB', 'input_size': [3, 331, 331], 'input_range': [0, 1], 'mean': [0.5, 0.5, 0.5], 'std': [0.5, 0.5, 0.5], 'num_classes': 1000}, 'imagenet+background': {'url': 'http://data.lip6.fr/cadene/pretrainedmodels/nasnetalarge-a1897284.pth', 'input_space': 'RGB', 'input_size': [3, 331, 331], 'input_range': [0, 1], 'mean': [0.5, 0.5, 0.5], 'std': [0.5, 0.5, 0.5], 'num_classes': 1001}}\n```\n\n- To load a pretrained models from imagenet:\n\n```python\nmodel_name = 'nasnetalarge' #: could be fbresnet152 or inceptionresnetv2\nmodel = pretrainedmodels.__dict__[model_name](num_classes=1000, pretrained='imagenet')\nmodel.eval()\n```\n\n**Note**: By default, models will be downloaded to your `$HOME/.torch` folder. You can modify this behavior using the `$TORCH_HOME` variable as follow: `export TORCH_HOME=\"/local/pretrainedmodels\"`\n\n- To load an image and do a complete forward pass:\n\n```python\nimport torch\nimport pretrainedmodels.utils as utils\n\nload_img = utils.LoadImage()\n\n#: transformations depending on the model\n#:\u00a0rescale, center crop, normalize, and others (ex: ToBGR, ToRange255)\ntf_img = utils.TransformImage(model) \n\npath_img = 'data/cat.jpg'\n\ninput_img = load_img(path_img)\ninput_tensor = tf_img(input_img)         #: 3x400x225 -> 3x299x299 size may differ\ninput_tensor = input_tensor.unsqueeze(0) #: 3x299x299 -> 1x3x299x299\ninput = torch.autograd.Variable(input_tensor,\n    requires_grad=False)\n\noutput_logits = model(input) #: 1x1000\n```\n\n- To extract features (beware this API is not available for all networks):\n\n```python\noutput_features = model.features(input) #: 1x14x14x2048 size may differ\noutput_logits = model.logits(output_features) #: 1x1000\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}