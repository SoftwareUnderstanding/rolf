{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1808.07962",
      "https://arxiv.org/abs/1808.10437",
      "https://arxiv.org/abs/1811.08264",
      "https://arxiv.org/abs/2003.05541",
      "https://arxiv.org/abs/1808.07962",
      "https://arxiv.org/abs/1808.10437",
      "https://arxiv.org/abs/1811.08264",
      "https://arxiv.org/abs/2003.05541",
      "https://arxiv.org/abs/2003.05541"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find this work useful, please consider our paper to cite:\n\n\t @InProceedings{Ulutan_2020_CVPR,\n\tauthor = {Ulutan, Oytun and Iftekhar, A S M and Manjunath, B. S.},\n\ttitle = {VSGNet: Spatial Attention Network for Detecting Human Object Interactions Using Graph Convolutions},\n\tbooktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n\tmonth = {June},\n\tyear = {2020}\n\t}\n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@InProceedings{Ulutan_2020_CVPR,\nauthor = {Ulutan, Oytun and Iftekhar, A S M and Manjunath, B. S.},\ntitle = {VSGNet: Spatial Attention Network for Detecting Human Object Interactions Using Graph Convolutions},\nbooktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\nmonth = {June},\nyear = {2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9020694875806334
      ],
      "excerpt": "Official repository of our CVPR 2020 paper. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9302785107180478
      ],
      "excerpt": "|Kolesnikov et al.| 41.0| \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9934635484831749
      ],
      "excerpt": "|Li et al.| 47.8 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9086892148066392
      ],
      "excerpt": "|GPNN| 10.61  | 7.78 | 11.45 |  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9983800825961148
      ],
      "excerpt": "|Li et al.| 17.03   | 13.42 | 18.11 |  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8488563248724261
      ],
      "excerpt": "Object Detector Fine-Tuned on HICO \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9507374082549614,
        0.8550101043698384
      ],
      "excerpt": "|IP-Net | 19.56 |12.79| 21.58 | \n|PPDM |21.10 |14.46| 23.09|  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "|VSGNet| 26.54| 21.26 | 28.12 | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ASMIftekhar/VSGNet",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-02-27T06:43:39Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-26T09:03:25Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9856001682284669
      ],
      "excerpt": "Official repository of our CVPR 2020 paper. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8999289671392283
      ],
      "excerpt": "To see the results in original v-coco scheme: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.812006153172241
      ],
      "excerpt": "The evaluation code has been adapted from the No-Frills repository.Here, 20 indicates the number of cpu cores to be used for evaluation, this can be changed to any number based on the system. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "VSGNet:Spatial Attention Network for Detecting Human Object Interactions Using Graph Convolutions.",
      "technique": "GitHub API"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To download the pre-trained models for the results reported in the paper:\n```Shell\nbash download_res.sh\n```\nThis will store the model for v-coco in 'soa_paper' folder and the model for HICO_DET in 'soa_paper_hico'. Alternatively you can download the models from [here](https://drive.google.com/drive/folders/1J8mN63bNIrTdBQzq7Lpjp4qxMXgYI-yF?usp=sharing).\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ASMIftekhar/VSGNet/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 19,
      "date": "Mon, 06 Dec 2021 09:30:59 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ASMIftekhar/VSGNet/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "ASMIftekhar/VSGNet",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/ASMIftekhar/VSGNet/master/download_data.sh",
      "https://raw.githubusercontent.com/ASMIftekhar/VSGNet/master/download_res.sh",
      "https://raw.githubusercontent.com/ASMIftekhar/VSGNet/master/scripts_hico/HICO_eval/compute_map.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Clone repository (recursively):\n```Shell\ngit clone --recursive https://github.com/ASMIftekhar/VSGNet.git\n```\n2. Download data,annotations,object detection results:\n```Shell\nbash download_data.sh\n```\nYou need to have wget and unzip packages to execute this script. Alternatively you can download the data from [here](https://drive.google.com/drive/folders/1J8mN63bNIrTdBQzq7Lpjp4qxMXgYI-yF?usp=sharing).\nIf you execute the script then there will be two folders in the directory \"All\\_data\" and \"infos\". This will take close to 10GB space. This contains both of the datasets and all the essential files. Also, if you just want to work with v-coco, download \"All_data_vcoco\" from the link.  \n\nInside the All\\_data folder you will find the following subdirectories.\n\n**a.Data_vcoco**: It will contain all training and validation images of v-coco inside train2014 subdirectory and all test images of v-coco inside val2014 subdirectory.\n\n**b.Annotations\\_vcoco**: It will contain all annotations of training, validation and testing set in three json files. The annotations are taken from v-coco API and converted into our convenient format. For example, lets consider there is only one single image annotated with two verbs \"smile\" and \"hold\" along with two person and object bounding boxes. The annotation for this image will be arranged as follows:\n\n```\n\t{image_id:[{'Verbs': 'hold',\n  \t'object': {'obj_bbx': [305.84, 59.12, 362.34, 205.22]},\n  \t'person_bbx': [0.0, 0.63, 441.03, 368.86]},\n \t{'Verbs': 'smile',\n  \t'object': {'obj_bbx': []},\n  \tperson_bbx': [0.0, 0.63, 441.03, 368.86]}]}\n```\n**c.Object\\_Detections\\_vcoco**: It will contain all object detection results for v-coco. \n\n**d.v-coco**: It will contain original v-coco API. This is needed for doing evaluations.\n\n**e.Data_hico**: It will contain all the training images of HICO-DET inside train2015 subdirectory and all test images of HICO_DET inside test2015 subdirectory.\n\n**f.Annotations\\_hico**: same as folder (b) but for HICO_DET dataset.\n\n**g.Object\\_Detections\\_hico**: same as folder (c) but for HICO_DET dataset.\n\n**h.bad\\_Detections\\_hico**: It will contain the list of images in HICO_DET dataset where our object detector fails to detect any person or object.\n\n**j.hico\\_infos**: It will contain additional files required to run training and testing in HICO_DET.\n\n3. To install all packages (preferable to run in a python2 virtual environment):\n```\npip2 install -r requirements.txt\n```\nFor HICO_DET evaluation we will use python3 environment, to install those packages (preferable to run in a python3 virtual environment):\n```\npip3 install -r requirements3.txt\n```\nRun only compute_map.sh in a python 3 enviornment. For all other use python 2 environment.\n\n4. If you do not wish to move \"All\\_data\" folder from the main directory then you dont need to do anything else to setup the repo. Otherwise you need to run setup.py with the location of All\\_data. If you put it in /media/ssd2 with a new name of \"data\" then you need to execute the following command:\n```\npython2 setup.py -d /media/ssd2/data/\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9531766919803467
      ],
      "excerpt": "bash compute_map.sh soa_paper_hico 20 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9531766919803467
      ],
      "excerpt": "bash compute_map.sh new_test 20 \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8702815774991463
      ],
      "excerpt": "|iCAN| 45.3  | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8508586540743793
      ],
      "excerpt": "|iCAN| 14.84  | 10.45 | 16.15 |  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.852508133578685,
        0.8164934614472162
      ],
      "excerpt": "python2 calculate_map_vcoco.py -fw soa_paper -sa 34 -t test \nTo store the best result in HICO_DET format run (inside \"scripts_hico/\"): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8368349691136163
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES=0 python2 main.py -fw new_test -ba 8 -l 0.001 -e 80 -sa 20 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8395969488037187
      ],
      "excerpt": "-fw: Name of the folder in which the result will be stored. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8218590029124793,
        0.824583481343483,
        0.8368349691136163,
        0.8178602427810775,
        0.8224898580403758
      ],
      "excerpt": "python2 calculate_map_vcoco.py -fw new_test -sa 30 -t test \nTo train the model from scratch (inside \"scripts_hico/\"): \nCUDA_VISIBLE_DEVICES=0 python2 main.py -fw new_test -ba 8 -l 0.001 -e 80 -sa 20 \nThe flags are same as v-coco. The model converges normally within 30 epochs. Again,you can use as many gpus as you wish. Just add the necessary gpu ids in the given command. We have used 4 2080Tis to train HICO_DET with a batch size of 8 per gpu. It takes around 40 minutes per epoch. \nAfter running the model, to store the results in HICO_DET format (inside \"scripts_hico/\"): \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/ASMIftekhar/VSGNet/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 A S M Iftekhar\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "VSGNet",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "VSGNet",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "ASMIftekhar",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/ASMIftekhar/VSGNet/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 91,
      "date": "Mon, 06 Dec 2021 09:30:59 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "human-object-interaction",
      "action-recognition",
      "scene-understanding"
    ],
    "technique": "GitHub API"
  }
}