{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1902.02476",
      "https://arxiv.org/abs/1803.05407"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Stochastic weight averaging: [Pytorch repo](https://github.com/timgaripov/swa/); most of the base methods and model definitions are built off of this repo.\n\nModel implementations:\n  - VGG: https://github.com/pytorch/vision/\n  - PreResNet: https://github.com/bearpaw/pytorch-classification\n  - WideResNet: https://github.com/meliketoy/wide-resnet.pytorch\n  - FCDensenet67: https://github.com/bfortuner/pytorch_tiramisu\n\nHessian eigenvalue computation: [PyTorch repo](https://github.com/tomgoldstein/loss-landscape), but we ultimately ended up using [GPyTorch](https://gpytorch.ai) as it allows calculation of more eigenvalues.\n\nSegmentation evaluation metrics: [Lasagne repo](https://github.com/SimJeg/FC-DenseNet/blob/master/metrics.py)\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{maddox_2019_simple,\n  title={A simple baseline for bayesian uncertainty in deep learning},\n  author={Maddox, Wesley J and Izmailov, Pavel and Garipov, Timur and Vetrov, Dmitry P and Wilson, Andrew Gordon},\n  booktitle={Advances in Neural Information Processing Systems},\n  pages={13153--13164},\n  year={2019}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/wjmaddox/swa_gaussian",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-06-07T20:58:52Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-07T20:29:44Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "SWA-Gaussian (SWAG) is a convenient method for uncertainty representation and calibration in Bayesian deep learning.\nThe key idea of SWAG is that the SGD iterates, with a modified learning rate schedule, act like samples from a Gaussian distribution; SWAG fits this Gaussian distribution by capturing the [SWA](https://arxiv.org/abs/1803.05407) mean and a covariance matrix, representing the first two moments of SGD iterates. We use this Gaussian distribution as a posterior over neural network weights, and then perform a Bayesian model average, for uncertainty representation and calibration.\n\n<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/14368801/52224039-09ab0b80-2875-11e9-9c12-c72b88abf4a9.png\" width=350>\n  <img src=\"https://user-images.githubusercontent.com/14368801/52224049-0dd72900-2875-11e9-9de8-540ceaae60b3.png\" width=350>\n</p>\n\n\nIn this repo, we implement SWAG for image classification with several different architectures on both CIFAR datasets and ImageNet. We also implement SWAG for semantic segmentation on CamVid using our implementation of a FCDenseNet67.\nWe additionally include several other experiments on exploring the covariance of the gradients of the SGD iterates, the eigenvalues of the Hessian, and width/PCA decompositions of the SWAG approximate posterior.\n\nCIFAR10 -> STL10             |  CIFAR100\n:-------------------------:|:-------------------------:\n![](plots/stl_wrn.jpg)  |  ![](plots/c100_resnet110.jpg)\n\nPlease cite our work if you find it useful:\n```bibtex\n@inproceedings{maddox_2019_simple,\n  title={A simple baseline for bayesian uncertainty in deep learning},\n  author={Maddox, Wesley J and Izmailov, Pavel and Garipov, Timur and Vetrov, Dmitry P and Wilson, Andrew Gordon},\n  booktitle={Advances in Neural Information Processing Systems},\n  pages={13153--13164},\n  year={2019}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9804707978105576,
        0.8049472091217347,
        0.8192505441185383
      ],
      "excerpt": "This repository contains a PyTorch implementation of Stochastic Weight Averaging-Gaussian (SWAG) from the paper \nA Simple Baseline for Bayesian Uncertainty in Deep Learning \nby Wesley Maddox, Timur Garipov, Pavel Izmailov, Dmitry Vetrov, and Andrew Gordon Wilson \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8828182805334649
      ],
      "excerpt": "|   +-- models/ (Folder with all model definitions) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8527613100774696
      ],
      "excerpt": "|   +-- hessian_eigs/ (folder for eigenvalues of hessian) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Code repo for \"A Simple Baseline for Bayesian Uncertainty in Deep Learning\"",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/wjmaddox/swa_gaussian/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 55,
      "date": "Fri, 10 Dec 2021 18:45:49 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/wjmaddox/swa_gaussian/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "wjmaddox/swa_gaussian",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/wjmaddox/swa_gaussian/master/experiments/segmentation/secondary_calibration_checks.ipynb",
      "https://raw.githubusercontent.com/wjmaddox/swa_gaussian/master/experiments/segmentation/model_results.ipynb",
      "https://raw.githubusercontent.com/wjmaddox/swa_gaussian/master/experiments/uncertainty/out_of_dist_plots.ipynb",
      "https://raw.githubusercontent.com/wjmaddox/swa_gaussian/master/experiments/ensembling/bma_rank.ipynb",
      "https://raw.githubusercontent.com/wjmaddox/swa_gaussian/master/experiments/ensembling/bma_samples_and_scaling.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```bash\npython setup.py develop\n```\n\nSee requirements.txt file for requirements that came from our setup. We use Pytorch 1.0.0 in our experiments.\n\nUnless otherwise described, all experiments were run on a single GPU. Note that if you are using CUDA 10 you may need to manually install Pytorch with the correct CUDA toolkit.\n\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8432721506552504,
        0.9586232994076559
      ],
      "excerpt": "|   +-- models/ (Folder with all model definitions) \n|   +-- utils.py (utility functions) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8232917117895253
      ],
      "excerpt": "|   +-- train/ (folder containing standard training scripts for non-ImageNet data) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8305038024478723
      ],
      "excerpt": "+-- tests/ (folder containing tests for SWAG sampling and SWAG log-likelihood calculation.) \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/wjmaddox/swa_gaussian/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "BSD 2-Clause \"Simplified\" License",
      "url": "https://api.github.com/licenses/bsd-2-clause"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'BSD 2-Clause License\\n\\nCopyright (c) 2019, Wesley Maddox, Timur Garipov, Pavel Izmailov,  Dmitry Vetrov, Andrew Gordon Wilson\\nAll rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met:\\n\\n Redistributions of source code must retain the above copyright notice, this\\n  list of conditions and the following disclaimer.\\n\\n Redistributions in binary form must reproduce the above copyright notice,\\n  this list of conditions and the following disclaimer in the documentation\\n  and/or other materials provided with the distribution.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "A Simple Baseline for Bayesian Deep Learning",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "swa_gaussian",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "wjmaddox",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/wjmaddox/swa_gaussian/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 311,
      "date": "Fri, 10 Dec 2021 18:45:49 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "**See experiments/* for particular READMEs**\n\n[Image Classification](experiments/train/README.md)\n\n[Segmentation](experiments/segmentation/README.md)\n\n[Uncertainty](experiments/uncertainty/README.md)\n\nSome other commands are listed here:\n\n*Hessian eigenvalues*\n\n```cd experiments/hessian_eigs; python run_hess_eigs.py --dataset CIFAR100 --data_path [data_path] --model PreResNet110 --use_test --file [ckpt] --save_path [output.npz] ```\n\n*Gradient covariances*\n\n```cd experiments/grad_cov; python run_grad_cov.py --dataset CIFAR100 --data_path [data_path] --model VGG16 --use_test --epochs=300 --lr_init=0.05 --wd=5e-4 --swa --swa_start 161 --swa_lr=0.01 --grad_cov_start 251 --dir [dir] ```\n\nNote that this will output the gradient covariances onto the console, so you ought to write these into a log file and retrieve them afterwards.\n\n",
      "technique": "Header extraction"
    }
  ]
}