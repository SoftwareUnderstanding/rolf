{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1505.05424",
      "https://arxiv.org/abs/1506.02557",
      "https://arxiv.org/abs/1506.02142",
      "https://arxiv.org/abs/1512.07666",
      "https://arxiv.org/abs/1505.05424"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9863714424608271
      ],
      "excerpt": "<!--* [Regression Results](#homoscedastic-regression)--> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8307186591810036,
        0.8356013927728488
      ],
      "excerpt": "| Log Like  | -572.9    |       -496.54         |    -1100.29       |  -1008.28     |    -892.85        |       -1086.43        |  -435.458     | -828.29   | -661.25   | \n|    Error \\%       |   1.58    |         1.53          |      2.60         |    2.38       |      2.28         |         2.61          |    1.37       |   1.76    |   1.76    | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/MayarLotfy/bayesianNN",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-06-14T09:07:56Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-06-14T09:22:11Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8663394725644362
      ],
      "excerpt": "Pytorch implementations for the following approximate inference methods: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.90689542689224
      ],
      "excerpt": "We also provide code for: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9435473664035061,
        0.9220900399577103
      ],
      "excerpt": "  experiements on toy datasets, generated with (Gaussian Process ground truth), \n    as well as on real data (six UCI datasets).  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9254749566248652
      ],
      "excerpt": " on (ExperimentType), i.e. homoscedastic/heteroscedastic. The heteroscedastic \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.818357963559477
      ],
      "excerpt": "We also provide Google Colab notebooks. This means that \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8058490425366022
      ],
      "excerpt": "src/: General utilities and model definitions. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8352859721901275
      ],
      "excerpt": "running of digit rotation uncertainty experiments. They also allow for weight \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9394449182630016
      ],
      "excerpt": "for experimentation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9513591569589528
      ],
      "excerpt": "For an explanation of the script's arguments: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8494486085311579
      ],
      "excerpt": "Best results are obtained with a Laplace prior. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9527733477180743,
        0.8387784319827133,
        0.8840170773391658,
        0.8656496768294036
      ],
      "excerpt": "Bayes By Backprop inference where the mean and variance of activations \n are calculated in closed form. Activations are sampled instead of \n weights. This makes the variance of the Monte Carlo ELBO estimator scale \n as 1/M, where M is the minibatch size. Sampling weights scales (M-1)/M. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9118746944543304
      ],
      "excerpt": " further reducing variance. Computation of each epoch is faster and so is convergence. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8862538870433946
      ],
      "excerpt": "A fixed dropout rate of 0.5 is set. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9513591569589528
      ],
      "excerpt": "For an explanation of the script's arguments: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8305301376609747
      ],
      "excerpt": "In order to converge to the true posterior over w, the learning rate \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9513591569589528
      ],
      "excerpt": "For an explanation of the script's arguments: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8068930525486822
      ],
      "excerpt": "than for vanilla SGLD. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8267438618432947
      ],
      "excerpt": "Multiple networks are trained on subsamples of the dataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9513591569589528
      ],
      "excerpt": "For an explanation of the script's arguments: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9029867207652978
      ],
      "excerpt": " to the curvature around a mode of the posterior. A block diagonal Hessian \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9868783155328522,
        0.9763692516483558
      ],
      "excerpt": " for. The Hessian is further approximated as the kronecker product of the  \n expectation of a single datapoint's Hessian factors. Approximating the Hessian \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9513591569589528
      ],
      "excerpt": "For an explanation of the script's arguments: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9663279169496634
      ],
      "excerpt": "allow for computationally cheap changes to the prior at inference time as the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9286402163926443,
        0.9052594331412861
      ],
      "excerpt": "Map inference provides a point estimate of parameter values. When provided with \nout of distribution inputs, such as rotated digits, these models then to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8879779805044943
      ],
      "excerpt": "We can decompose this term in order to distinguish between 2 types of uncertainty. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9514867521385801
      ],
      "excerpt": " quantified as the expected entropy of model predictions. Model uncertainty \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.908925214220865,
        0.9692401441548719
      ],
      "excerpt": " and aleatoric entropy. \nToy homoscedastic regression task. Data is generated by a GP with a RBF \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9692401441548719
      ],
      "excerpt": "Toy heteroscedastic regression task. Data is generated by a GP with a RBF \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8928392230433202
      ],
      "excerpt": "We performed heteroscedastic regression on the six UCI datasets \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8415591833446245
      ],
      "excerpt": "     red wine and yacht datasets), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8170935788942726
      ],
      "excerpt": "      Note that results depend heavily on hyperparameter selection. Plots below show log-likelihoods and RMSEs  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9847501818901284,
        0.891258232091717
      ],
      "excerpt": "W is marginalised with 100 samples of the weights for all models except \nMAP, where only one set of weights is used. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8311270093542309,
        0.8715273913618686
      ],
      "excerpt": "   Gaussian with std=0.1. P-SGLD uses RMSprop preconditioning. \nThe original paper for Bayes By Backprop \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8535935897025945,
        0.9328177655953985
      ],
      "excerpt": " However, when initialising the variances to match the prior (BBP Gauss 1), we obtain the above results. \n The training curves for both of these hyperparameter configuration schemes \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.936279710997694
      ],
      "excerpt": "feeding our models with adversarial samples (fgsm). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8734927645743327
      ],
      "excerpt": "Histograms of weights sampled from each model trained on MNIST. We draw 10 samples of w for each model. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/MayarLotfy/bayesianNN/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Mon, 06 Dec 2021 23:42:55 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/MayarLotfy/bayesianNN/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "MayarLotfy/bayesianNN",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/MayarLotfy/bayesianNN/master/notebooks/regression/map_homo.ipynb",
      "https://raw.githubusercontent.com/MayarLotfy/bayesianNN/master/notebooks/regression/mc_dropout_hetero.ipynb",
      "https://raw.githubusercontent.com/MayarLotfy/bayesianNN/master/notebooks/regression/bbp_homo.ipynb",
      "https://raw.githubusercontent.com/MayarLotfy/bayesianNN/master/notebooks/regression/bbp_hetero.ipynb",
      "https://raw.githubusercontent.com/MayarLotfy/bayesianNN/master/notebooks/regression/map_hetero.ipynb",
      "https://raw.githubusercontent.com/MayarLotfy/bayesianNN/master/notebooks/regression/gp_homo_hetero.ipynb",
      "https://raw.githubusercontent.com/MayarLotfy/bayesianNN/master/notebooks/regression/mc_dropout_homo.ipynb",
      "https://raw.githubusercontent.com/MayarLotfy/bayesianNN/master/notebooks/regression/langevin_homo.ipynb",
      "https://raw.githubusercontent.com/MayarLotfy/bayesianNN/master/notebooks/regression/langevin_hetero.ipynb",
      "https://raw.githubusercontent.com/MayarLotfy/bayesianNN/master/notebooks/classification/Bootstrap_ensemble_NN_MNIST.ipynb",
      "https://raw.githubusercontent.com/MayarLotfy/bayesianNN/master/notebooks/classification/weight_uncertainty_local_reparam_MNIST.ipynb",
      "https://raw.githubusercontent.com/MayarLotfy/bayesianNN/master/notebooks/classification/BayesByBackprop_MNIST_gaussian.ipynb",
      "https://raw.githubusercontent.com/MayarLotfy/bayesianNN/master/notebooks/classification/weight_uncertainty_MC_MNIST_laplace.ipynb",
      "https://raw.githubusercontent.com/MayarLotfy/bayesianNN/master/notebooks/classification/weight_uncertainty_MC_MNIST_importance_weigh.ipynb",
      "https://raw.githubusercontent.com/MayarLotfy/bayesianNN/master/notebooks/classification/MC_dropout_MNIST.ipynb",
      "https://raw.githubusercontent.com/MayarLotfy/bayesianNN/master/notebooks/classification/BayesByBackprop_MNIST_GMM.ipynb",
      "https://raw.githubusercontent.com/MayarLotfy/bayesianNN/master/notebooks/classification/KFAC_Laplace_MNIST.ipynb",
      "https://raw.githubusercontent.com/MayarLotfy/bayesianNN/master/notebooks/classification/MAP_NN_MNIST.ipynb",
      "https://raw.githubusercontent.com/MayarLotfy/bayesianNN/master/notebooks/classification/PLOTS.ipynb",
      "https://raw.githubusercontent.com/MayarLotfy/bayesianNN/master/notebooks/classification/pSGLD_MNIST.ipynb",
      "https://raw.githubusercontent.com/MayarLotfy/bayesianNN/master/notebooks/classification/SGLD_MNIST.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.986064667927924
      ],
      "excerpt": "  you can run on a GPU (for free!). No modifications required - all dependencies  \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8871747144400938
      ],
      "excerpt": "train_(ModelName)_(Dataset).py: Trains (ModelName) on (Dataset). Training \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8209895949072734
      ],
      "excerpt": "Train a model on MNIST: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8261716260857435
      ],
      "excerpt": "python train_BayesByBackprop_MNIST.py [--model [MODEL]] [--prior_sig [PRIOR_SIG]] [--epochs [EPOCHS]] [--lr [LR]] [--n_samples [N_SAMPLES]] [--models_dir [MODELS_DIR]] [--results_dir [RESULTS_DIR]] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python train_BayesByBackprop_MNIST.py -h \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8209895949072734
      ],
      "excerpt": "Train a model on MNIST: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8135973209763124
      ],
      "excerpt": "python train_BayesByBackprop_MNIST.py --model Local_Reparam [--prior_sig [PRIOR_SIG]] [--epochs [EPOCHS]] [--lr [LR]] [--n_samples [N_SAMPLES]] [--models_dir [MODELS_DIR]] [--results_dir [RESULTS_DIR]] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8209895949072734
      ],
      "excerpt": "Train a model on MNIST: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python train_MCDropout_MNIST.py -h \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8209895949072734
      ],
      "excerpt": "Train a model on MNIST: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python train_SGLD_MNIST.py -h \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8209895949072734
      ],
      "excerpt": "Train a model on MNIST: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8447759704096607
      ],
      "excerpt": "python train_SGLD_MNIST.py --use_preconditioning True [--prior_sig [PRIOR_SIG]] [--epochs [EPOCHS]] [--lr [LR]] [--models_dir [MODELS_DIR]] [--results_dir [RESULTS_DIR]] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python train_Bootrap_Ensemble_MNIST.py -h \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python train_KFLaplace_MNIST.py -h \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8758774699081027
      ],
      "excerpt": "<img src=\"images/MAP.png\" width=\"430\" height=\"270\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8705172726853898,
        0.8861257091281016,
        0.8861257091281016,
        0.8861257091281016,
        0.8861257091281016
      ],
      "excerpt": "  <img src=\"images/map_homo.png\" width=\"170\" /> \n  <img src=\"images/bbp_homo.png\" width=\"150\" />  \n  <img src=\"images/mc_dropout_homo.png\" width=\"150\" /> \n  <img src=\"images/sgld_homo.png\" width=\"150\" />  \n  <img src=\"images/gp_homo.png\" width=\"150\" /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8705172726853898,
        0.8861257091281016,
        0.8861257091281016,
        0.8861257091281016,
        0.8861257091281016
      ],
      "excerpt": "  <img src=\"images/map_hetero.png\" width=\"170\" /> \n  <img src=\"images/bbp_hetero.png\" width=\"150\" />  \n  <img src=\"images/mc_dropout_hetero.png\" width=\"150\" /> \n  <img src=\"images/sgld_hetero.png\" width=\"150\" />  \n  <img src=\"images/gp_hetero.png\" width=\"150\" /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.877864006043759,
        0.877864006043759
      ],
      "excerpt": "  <img src=\"images/uci_regression_log_liks.png\" float=\"center\" width=\"400\" /> \n  <img src=\"images/uci_regression_rmses.png\" float=\"center\" width=\"400\" /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9156781332708049
      ],
      "excerpt": "<img src=\"images/BBP_train.png\" width=\"500\" height=\"420\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9156781332708049
      ],
      "excerpt": "<img src=\"images/all_rotations.png\" width=\"900\" height=\"420\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9156781332708049
      ],
      "excerpt": "<img src=\"images/KMNIST_entropies.png\" width=\"770\" height=\"240\"/> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9059821652011756
      ],
      "excerpt": "<img src=\"images/Wdistribution.png\" width=\"450\" height=\"270\"/> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/MayarLotfy/bayesianNN/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 Javier Antoran\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Bayesian Neural Networks",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "bayesianNN",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "MayarLotfy",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/MayarLotfy/bayesianNN/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* PyTorch\n* Numpy\n* Matplotlib\n\nThe project is written in python 2.7 and Pytorch 1.0.1. If CUDA is available, it will be\nused automatically. The models can also run on CPU as they are not excessively big.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Mon, 06 Dec 2021 23:42:55 GMT"
    },
    "technique": "GitHub API"
  }
}