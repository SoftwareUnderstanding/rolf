{
  "citation": [
    {
      "confidence": [
        0.9944484218006108
      ],
      "excerpt": "- UNet: https://arxiv.org/pdf/1505.04597.pdf \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9554441738822752,
        0.9819107666668762
      ],
      "excerpt": "- https://aihpc.ipages.nist.gov/pages/ \n- https://gitlab.nist.gov/gitlab/aihpc/pages/wikis/home \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/usnistgov/semantic-segmentation-fc-densenet",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-03-23T22:39:40Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-04-28T17:28:03Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9886168122364347,
        0.8838354824877431
      ],
      "excerpt": "This codebase is designed to work with Python3 and Tensorflow 2.x \nThere is example input data included in the repo under the data folder. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8009371785030491
      ],
      "excerpt": "- mask type: grayscale image with one of these pixel types: uint8, uint16, int32 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955840416664728,
        0.9536441138363216
      ],
      "excerpt": "This training code uses lmdb databases to store the image and mask data to enable parallel memory-mapped file reader to keep the GPUs fed.  \nThe input folder of images and masks needs to be split into train and test. Train to update the model parameters, and test to estimate the generalization accuracy of the resulting model. By default 80% of the data is used for training, 20% for test. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8952104988397119
      ],
      "excerpt": "Script which converts two folders of images and masks into a pair of lmdb \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8995886921616592
      ],
      "excerpt": "                        Whether to shard the image into tiles [0 = False, 1 = \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9782878104865321
      ],
      "excerpt": "                        The size of the tiles to crop out of the source \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9183694671991841
      ],
      "excerpt": "The full help for the training script is: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8318697480577522
      ],
      "excerpt": "                        how many threads to use for disk I/O and augmentation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9177911624435448,
        0.9184650248926527
      ],
      "excerpt": "number_classes: you need to specify the number of classes being segmented so the network knows how to format the output. The input labels are integers indicating the classes. However, under the hood tensorflow needs a one-hot encoding of the class, so this tells the model how to expand the input label into a one-hot encoding of the class id. \ntest_every_n_steps: typically, you run test/validation every epoch. However, I am often building models with very small amounts of data (e.g. 500 images). With an actual batch size of 32, that allows me 15 gradient updates per epoch. The model does not change that fast, so I impose a fixed global step count between test so that I don't spend all of my GPU time running the test data. A good value for this is typically 1000. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9537254010326064
      ],
      "excerpt": "One of the defining features of this codebase is the parallel (python multiprocess) image reading from lightning memory mapped databases.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8971901197191089,
        0.9596506358555309,
        0.9691816872609298,
        0.9945212280430878
      ],
      "excerpt": "- applies the augmentation transformation to the image and mask pair \n- add the augmented image to the batch that reader is building \n- once a batch is constructed, the imagereader adds it to the output queue shared among all of the imagereaders \nThe training script setups of python generators which just get a reference to the output batch queue data and pass it into tensorflow. One of the largest bottlenecks in deep learning is keeping the GPUs fed. By performing the image reading and data augmentation asynchronously all the main python training thread has to do is get a reference to the next batch (which is waiting in memory) and pass it to tensorflow to be copied to the GPUs. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9233283637261012
      ],
      "excerpt": "You will know whether the image readers are keeping up with the GPUs. When the imagereader output queue is getting empty a warning is printed to the log: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9095989214785583
      ],
      "excerpt": "| jitter (x, y)  | Percent of Image Size  | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8216071058889584
      ],
      "excerpt": "| blur  | Uniform Selection of Kernel Size | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8358985203126529,
        0.9502012260702014
      ],
      "excerpt": "These augmentation transformations are generally configured based on domain expertise and stay fixed per dataset. \nCurrently the only method for modifying them is to open the imagereader.py file and edit the augmentation parameters contained within the code block within the imagereader __init__: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8698949587553885,
        0.8387836596614869
      ],
      "excerpt": "self._jitter_augmentation_severity = 0.1  #: x% of a FOV \nself._noise_augmentation_severity = 0.02  #: vary noise by x% of the dynamic range present in the image \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8719849158610605
      ],
      "excerpt": "Script to detect stars with the selected unet model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9241301514650458
      ],
      "excerpt": "                        SavedModel filepath to the model to use \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "FC-DenseNet semantic segmentation model codebase",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/usnistgov/semantic-segmentation-fc-densenet/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 09 Dec 2021 02:36:48 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/usnistgov/semantic-segmentation-fc-densenet/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "usnistgov/semantic-segmentation-fc-densenet",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/usnistgov/semantic-segmentation-fc-densenet/master/FC-DenseNet/train.sh",
      "https://raw.githubusercontent.com/usnistgov/semantic-segmentation-fc-densenet/master/FC-DenseNet/build_lmdb.sh",
      "https://raw.githubusercontent.com/usnistgov/semantic-segmentation-fc-densenet/master/FC-DenseNet/inference.sh",
      "https://raw.githubusercontent.com/usnistgov/semantic-segmentation-fc-densenet/master/FC-DenseNet/sbatch_train.sh",
      "https://raw.githubusercontent.com/usnistgov/semantic-segmentation-fc-densenet/master/FC-DenseNet/setup_python_environment.sh",
      "https://raw.githubusercontent.com/usnistgov/semantic-segmentation-fc-densenet/master/FC-DenseNet/setup_enki_environment.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8980582709505841
      ],
      "excerpt": "                        lmdb database to use for (Required) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8043492247211779
      ],
      "excerpt": "                        lmdb database to use for testing (Required) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9748709027320682
      ],
      "excerpt": "                        per gpu \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8660851044974243
      ],
      "excerpt": "- mask pixel value 0 indicates background/no-class \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8579411795380598
      ],
      "excerpt": "The input folder of images and masks needs to be split into train and test. Train to update the model parameters, and test to estimate the generalization accuracy of the resulting model. By default 80% of the data is used for training, 20% for test. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091,
        0.8587476985249702
      ],
      "excerpt": "python build_lmdb.py -h \nusage: build_lmdb [-h] [--image_folder IMAGE_FOLDER] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8576356149034564
      ],
      "excerpt": "                        format (extension) of the input images. E.g {tif, jpg, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8594142235991984
      ],
      "excerpt": "                        True] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091,
        0.8587476985249702
      ],
      "excerpt": "python train_unet.py -h \nusage: train_unet [-h] --train_database TRAIN_DATABASE_FILEPATH \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.843422915301725
      ],
      "excerpt": "                        training batch size \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8153013484632672
      ],
      "excerpt": "                        whether to balance classes [0 = false, 1 = true] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8366840868128748
      ],
      "excerpt": "                        whether to use data augmentation [0 = false, 1 = true] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8087175695762129
      ],
      "excerpt": "                        per gpu \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8736586251589697
      ],
      "excerpt": "Once you have a trained model, the script inference_unet.py will take the saved_model from the training run and use it to inference all of the images in a specified folder. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python inference_unet.py -h \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8515563575856843
      ],
      "excerpt": "                        filepath to the folder containing tif images to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8576356149034564
      ],
      "excerpt": "                        format (extension) of the input images. E.g {tif, jpg, \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/usnistgov/semantic-segmentation-fc-densenet/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other"
    },
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "SemanticSegmentation",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "semantic-segmentation-fc-densenet",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "usnistgov",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/usnistgov/semantic-segmentation-fc-densenet/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Thu, 09 Dec 2021 02:36:48 GMT"
    },
    "technique": "GitHub API"
  }
}