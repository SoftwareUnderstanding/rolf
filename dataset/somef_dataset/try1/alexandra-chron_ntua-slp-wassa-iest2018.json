{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1801.06146\n\n```Regularizing and Optimizing LSTM Language Models``` https://arxiv.org/abs/1708.02182\n\n```Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm``` http://arxiv.org/abs/1708.00524",
      "https://arxiv.org/abs/1708.02182\n\n```Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm``` http://arxiv.org/abs/1708.00524"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@InProceedings{W18-6209,\n  author =  \"Chronopoulou, Alexandra\n        and Margatina, Aikaterini\n        and Baziotis, Christos\n        and Potamianos, Alexandros\",\n  title =   \"NTUA-SLP at IEST 2018: Ensemble of Neural Transfer Methods for Implicit Emotion Classification\",\n  booktitle =   \"Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis\",\n  year =    \"2018\",\n  publisher =   \"Association for Computational Linguistics\",\n  pages =   \"57--64\",\n  location =    \"Brussels, Belgium\",\n  url =     \"http://aclweb.org/anthology/W18-6209\"\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9415923359062981
      ],
      "excerpt": "I'm \\[#:TARGETWORD#:\\] because I love you, I love you and I hate you.  (correct label: angry) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/alexandra-chron/ntua-slp-wassa-iest2018",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-06-15T15:26:00Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-06-05T02:36:56Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9595885012983165,
        0.967953619105305
      ],
      "excerpt": "This repository contains the source code of the models submitted by NTUA-SLP team in IEST of WASSA 2018 at EMNLP 2018.  \nThe model is described in the paper: http://aclweb.org/anthology/W18-6209 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.885002274290134
      ],
      "excerpt": "Task: Classify twitter messages in one of six emotion categories (happy, sad, fear, anger, surprise, disgust) without the emotion word.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9180604656374182
      ],
      "excerpt": "We use an ensemble of 3 different Transfer Learning approaches: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9284700421474613
      ],
      "excerpt": "First) Pretrain a LSTM-based language model (LM) and transfer it to a target-task classification model: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8382241330781651,
        0.9134925000018288
      ],
      "excerpt": "This follows to a great degree ULMFiT by Howard and Ruder. \nSecond) Pretrain a LSTM-based attentive classification model on a different dataset and transfer its feature extractor to the target-task classification model: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.80933407118175
      ],
      "excerpt": "Third) Use pretrained word vectors to initialize the embedding layer of a classification model: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Deep-learning Transfer Learning models of NTUA-SLP team submitted at the IEST of WASSA 2018 at EMNLP 2018.",
      "technique": "GitHub API"
    }
  ],
  "documentation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "In order to make our codebase more accessible and easier to extend, we provide an overview of the structure of our project. \n\n`datasets` : contains the datasets for the pretraining :\n- ```twitter100K/``` contains unlabeled data used for pretraining an LM\n- ```semeval2017A/``` and ```wassa_2018/``` contain the labeled datasets used for SemEval17 Task4A and WASSA IEST 2018 respectively\n\n`embeddings`: pretrained word2vec embeddings should be put here.\n\n\n`model`: scripts for running:\n- IEST classifier ```wassa.py```\n- SE17 Task4 classifier ```sentiment.py```\n- language model ```lm.py```.\n\n`modules`: the source code of the PyTorch deep-learning models and the baseline models.\n\n`submissions`: contains the script to test trained model and create submission file for WASSA.\n\n`utils`: contains helper functions.\n\n**Bibliography**\n\nA few relevant and very important papers to our work are presented below:\n\n```Universal Language Model Fine-tuning for Text Classification``` https://arxiv.org/abs/1801.06146\n\n```Regularizing and Optimizing LSTM Language Models``` https://arxiv.org/abs/1708.02182\n\n```Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm``` http://arxiv.org/abs/1708.00524\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/alexandra-chron/ntua-slp-wassa-iest2018/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 8,
      "date": "Fri, 10 Dec 2021 16:24:40 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/alexandra-chron/ntua-slp-wassa-iest2018/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "alexandra-chron/ntua-slp-wassa-iest2018",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.9952289343754815
      ],
      "excerpt": "Before you proceed, pip install -r ./requirements.txt \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8228039915603027
      ],
      "excerpt": "Download it here: pretrained + fine-tuned LM \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8411624709648624,
        0.9201103046116129
      ],
      "excerpt": "1) Pretrain a classifier: python sentiment.py \n2) Train the final classifier: python wassa.py (set pretrained_classifier = True and provide the correspondent config file.) \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/alexandra-chron/ntua-slp-wassa-iest2018/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Overview",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "ntua-slp-wassa-iest2018",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "alexandra-chron",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/alexandra-chron/ntua-slp-wassa-iest2018/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 18,
      "date": "Fri, 10 Dec 2021 16:24:40 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "transfer-learning",
      "language-models",
      "twitter",
      "python",
      "pytorch",
      "emotion-analysis",
      "deep-learning",
      "deep-neural-networks",
      "lstm",
      "sentiment-analysis"
    ],
    "technique": "GitHub API"
  }
}