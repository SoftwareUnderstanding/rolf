{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1802.08435",
      "https://arxiv.org/abs/2102.11906.\n3. Kleijn, W. B., Storus, A., Chinen, M., Denton, T., Lim, F. S., Luebs, A., ...\n   & Yeh, H. (2021). [Generative Speech Coding with Predictive Variance\n   Regularization](https://arxiv.org/pdf/2102.09660). arXiv preprint\n   https://arxiv.org/abs/2102.09660.\n<a id=\"4\">4.</a> Kalchbrenner, N., Elsen, E., Simonyan, K., Noury, S.,\n   Casagrande, N., Lockhart, E., ... & Kavukcuoglu, K. (2018, July).\n   [Efficient neural audio synthesis](https://arxiv.org/abs/1802.08435).\n   In International Conference on Machine Learning (pp. 2410-2419). PMLR.",
      "https://arxiv.org/abs/2102.09660.\n<a id=\"4\">4.</a> Kalchbrenner, N., Elsen, E., Simonyan, K., Noury, S.,\n   Casagrande, N., Lockhart, E., ... & Kavukcuoglu, K. (2018, July).\n   [Efficient neural audio synthesis](https://arxiv.org/abs/1802.08435).\n   In International Conference on Machine Learning (pp. 2410-2419). PMLR."
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9146894306581513
      ],
      "excerpt": "bazel-bin/encoder_main --model_path=wavegru --output_dir=$HOME/temp --input_path=testdata/16khz_sample_000001.wav \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9644643460693488
      ],
      "excerpt": "bazel-bin/decoder_main  --model_path=wavegru --output_dir=$HOME/temp/ --encoded_path=$HOME/temp/16khz_sample_000001.lyra \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9243176922173332
      ],
      "excerpt": "near your mouth), and then press \"Encode and decode to speaker\". You should hear \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.983188374768223
      ],
      "excerpt": "If you press 'Benchmark', you should see something like the following in logcat \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9699236969047585,
        0.9214258038320537,
        0.8224636339689146
      ],
      "excerpt": "I  I20210401 11:04:26.700352  6870 benchmark_decode_lib.cc:85] conditioning_only stats for generating 2000 frames of audio, max: 506 us, min: 368 us, mean: 391 us, stdev: 10.3923. \nI  I20210401 11:04:26.725538  6870 benchmark_decode_lib.cc:85] model_only stats for generating 2000 frames of audio, max: 12690 us, min: 9087 us, mean: 9237 us, stdev: 262.416. \nI  I20210401 11:04:26.729460  6870 benchmark_decode_lib.cc:85] combined_model_and_conditioning stats for generating 2000 frames of audio, max: 13173 us, min: 9463 us, mean: 9629 us, stdev: 270.788. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9554441738822752
      ],
      "excerpt": "Kleijn, W. B., Lim, F. S., Luebs, A., Skoglund, J., Stimberg, F., Wang, Q., & \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9999553544249983,
        0.9923824385339041
      ],
      "excerpt": "   In 2018 IEEE international conference on acoustics, speech and signal \n   processing (ICASSP) (pp. 676-680). IEEE. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9992393194923946
      ],
      "excerpt": "   arXiv preprint arXiv:2102.11906. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9895125999203164,
        0.9490753289412834
      ],
      "excerpt": "   Regularization. arXiv preprint \n   arXiv:2102.09660. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/google/lyra",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "How to Contribute\nWe'd love to accept your patches and contributions to this project. There are\njust a few small guidelines you need to follow.\nContributor License Agreement\nContributions to this project must be accompanied by a Contributor License\nAgreement (CLA). You (or your employer) retain the copyright to your\ncontribution; this simply gives us permission to use and redistribute your\ncontributions as part of the project. Head over to\nhttps://cla.developers.google.com/ to see your current agreements on file or\nto sign a new one.\nYou generally only need to submit a CLA once, so if you've already submitted one\n(even if it was for a different project), you probably don't need to do it\nagain.\nCode Reviews\nAll submissions, including submissions by project members, require review. We\nuse GitHub pull requests for this purpose. Consult\nGitHub Help for more\ninformation on using pull requests.\nCommunity Guidelines\nThis project follows\nGoogle's Open Source Community Guidelines.",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-03-19T18:38:34Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-05T16:01:20Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8528786489374756,
        0.9124771110400183,
        0.8527139558581008,
        0.9820165323071625,
        0.908925214220865,
        0.96939804781126,
        0.9217289901299186,
        0.8413989280589395,
        0.9555113387732022
      ],
      "excerpt": "is a high-quality, low-bitrate speech codec that makes voice communication \navailable even on the slowest networks. To do this it applies traditional codec \ntechniques while leveraging advances in machine learning (ML) with models \ntrained on thousands of hours of data to create a novel method for compressing \nand transmitting voice signals. \nThe basic architecture of the Lyra codec is quite simple. Features are extracted \nfrom speech every 40ms and are then compressed for transmission at a bitrate of \n3kbps. The features themselves are log mel spectrograms, a list of numbers \nrepresenting the speech energy in different frequency bands, which have \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9311099493828425,
        0.9281840556371276,
        0.9419199279417969,
        0.9381822372811761,
        0.9100712990252277
      ],
      "excerpt": "after human auditory response. On the other end, a generative model uses those \nfeatures to recreate the speech signal. \nLyra harnesses the power of new natural-sounding generative models to maintain \nthe low bitrate of parametric codecs while achieving high quality, on par with \nstate-of-the-art waveform codecs used in most streaming and communication \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8574746431519442,
        0.8445236845885941,
        0.8773536407331473
      ],
      "excerpt": "Computational complexity is reduced by using a cheaper recurrent generative \nmodel, a WaveRNN variation, that works at a lower rate, but generates in \nparallel multiple signals in different frequency ranges that it later combines \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8501443009840667,
        0.9486170137616283,
        0.9145136622597584,
        0.9863904020897697
      ],
      "excerpt": "ARM optimizations, enables Lyra to not only run on cloud servers, but also \non-device on mid-range phones, such as Pixel phones, in real time (with a \nprocessing latency of 100ms). This generative model is then trained on thousands \nof hours of speech data with speakers in over 70 languages and optimized to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9283279692306707,
        0.8252629682945533
      ],
      "excerpt": "specified by --input_path.  The --model_path flag contains the model data \nnecessary to encode, and --output_path specifies where to write the encoded \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9565681393202408
      ],
      "excerpt": "to decode the encoded data back into speech. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8242870579012054,
        0.8173508437698057
      ],
      "excerpt": "This example is an app with a minimal GUI that has buttons for two options. \nOne option is to record from the microphone and encode/decode with Lyra so you \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9529953951860328
      ],
      "excerpt": "benchmark that encodes and decodes in the background and prints the timings to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8459756327687138,
        0.8459756327687138
      ],
      "excerpt": "I  I20210401 11:04:26.725538  6870 benchmark_decode_lib.cc:85] model_only stats for generating 2000 frames of audio, max: 12690 us, min: 9087 us, mean: 9237 us, stdev: 262.416. \nI  I20210401 11:04:26.729460  6870 benchmark_decode_lib.cc:85] combined_model_and_conditioning stats for generating 2000 frames of audio, max: 13173 us, min: 9463 us, mean: 9629 us, stdev: 270.788. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8462989650705999
      ],
      "excerpt": "microseconds on average (.0096 seconds).  So decoding is performed at around \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9433453303838566
      ],
      "excerpt": "with an \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9925863332228951
      ],
      "excerpt": "There is a tutorial on building for android with Bazel in the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8197643552247942
      ],
      "excerpt": "There are also the binary targets that you can use to experiment with encoding \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9848513587828298
      ],
      "excerpt": ": Push the binary and the data it needs, including the model and .wav files: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9067290158179852,
        0.9593411343137153,
        0.908925214220865,
        0.8129323664604099
      ],
      "excerpt": "The encoder_main/decoder_main as above should also work. \nFor integrating Lyra into any project only two APIs are relevant: \nLyraEncoder and LyraDecoder. \nDISCLAIMER: At this time Lyra's API and bit-stream are not guaranteed to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8946944488960126,
        0.8380210941321287,
        0.8911539511288933,
        0.8461126187998166,
        0.9155312321633559
      ],
      "excerpt": "The static Create method instantiates a LyraEncoder with the desired sample \nrate in Hertz, number of channels and bitrate, as long as those parameters are \nsupported. Else it returns a nullptr. The Create method also needs to know if \nDTX should be enabled and where the model weights are stored. It also checks \nthat these weights exist and are compatible with the current Lyra version. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9676406063107281,
        0.9419384550488643,
        0.9921677918663967
      ],
      "excerpt": "method. The provided span of int16-formatted samples is assumed to contain 40ms \nof data at the sample rate chosen at Create time. As long as this condition is \nmet the Encode method returns the encoded packet as a vector of bytes that is \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.973187862588889
      ],
      "excerpt": "The rest of the LyraEncoder methods are just getters for the different \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8907185255262515
      ],
      "excerpt": "On the receiving end, LyraDecoder can be used to decode the encoded packet \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9355446954922647
      ],
      "excerpt": "Once again, the static Create method instantiates a LyraDecoder with the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8143980654231985,
        0.9626642271080404,
        0.9155312321633559
      ],
      "excerpt": "need to be the same as the ones in LyraEncoder. And once again, the Create \nmethod also needs to know where the model weights are stored. It also checks \nthat these weights exist and are compatible with the current Lyra version. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8047529223248597,
        0.9175759855643506
      ],
      "excerpt": "long as the total number of samples obtained this way between any two calls to \nSetEncodedPacket is less than 40ms of data at the sample rate chose at \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9968029537584643
      ],
      "excerpt": "of samples. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.973187862588889
      ],
      "excerpt": "The rest of the LyraDecoder methods are just getters for the different \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9330340763906319,
        0.8083237278977549
      ],
      "excerpt": "For an example on how to use LyraEncoder and LyraDecoder to encode and \ndecode a stream of audio, please refer to the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801550112125557,
        0.9550110443693214,
        0.9765451806768742
      ],
      "excerpt": "of sparse Matrix-Vector multiplication ops on mobile and desktop CPU platforms \n(ARM and AVX2) to allow for real-time operation on phones.  This library was \ncreated by DeepMind for their implementation of WaveRNN with sparsity [4], \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8382261197878127
      ],
      "excerpt": "A generic kernel is also provided, which enables debugging on non-optimized \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A Very Low-Bitrate Codec for Speech Compression",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/google/lyra/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 259,
      "date": "Tue, 07 Dec 2021 03:31:58 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/google/lyra/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "google/lyra",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Lyra is built using Google's build system, Bazel. Install it following these\n[instructions](https://docs.bazel.build/versions/master/install.html).\nBazel verson 4.0.0 is required, and some Linux distributions may make an older\nversion available in their application repositories, so make sure you are\nusing the required version or newer. The latest version can be downloaded via\n[Github](https://github.com/bazelbuild/bazel/releases).\n\nLyra can be built from linux using bazel for an arm android target, or a linux\ntarget.  The android target is optimized for realtime performance.  The linux\ntarget is typically used for development and debugging.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8594797941117425
      ],
      "excerpt": "You can build the cc_binaries with the default config.  encoder_main is an \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8422847597997493
      ],
      "excerpt": "Similarly, you can build decoder_main and use it on the output of encoder_main \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8728462008213858,
        0.9384926763270389
      ],
      "excerpt": "bazel build android_example:lyra_android_example --config=android_arm64 --copt=-DBENCHMARK \nadb install bazel-bin/android_example/lyra_android_example.apk \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8905771488248078
      ],
      "excerpt": "If you press 'Benchmark', you should see something like the following in logcat \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560370590210788
      ],
      "excerpt": "to create a .so that you can use in your own build system. Or you can use it \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9564594278687647
      ],
      "excerpt": "You can build the example cc_binary targets with: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8717670897477834
      ],
      "excerpt": "Given a LyraEncoder, any audio stream can be compressed using the Encode \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8258760062222703
      ],
      "excerpt": "using the following interface: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8170943117281423
      ],
      "excerpt": "example of a file encoder. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.815576407470663,
        0.8257322962958603
      ],
      "excerpt": "You can run encoder_main to encode a test .wav file with some speech in it, \nspecified by --input_path.  The --model_path flag contains the model data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8019306333022961
      ],
      "excerpt": "./encoder_main --model_path=/data/local/tmp/wavegru --output_dir=/data/local/tmp --input_path=testdata/16khz_sample_000001.wav \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8149832752949462
      ],
      "excerpt": "integration test. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/google/lyra/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "C++",
      "Starlark",
      "Java",
      "C"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Apache License 2.0",
      "url": "https://api.github.com/licenses/apache-2.0"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'\\n                                 Apache License\\n                           Version 2.0, January 2004\\n                        http://www.apache.org/licenses/\\n\\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\\n\\n   1. Definitions.\\n\\n      \"License\" shall mean the terms and conditions for use, reproduction,\\n      and distribution as defined by Sections 1 through 9 of this document.\\n\\n      \"Licensor\" shall mean the copyright owner or entity authorized by\\n      the copyright owner that is granting the License.\\n\\n      \"Legal Entity\" shall mean the union of the acting entity and all\\n      other entities that control, are controlled by, or are under common\\n      control with that entity. For the purposes of this definition,\\n      \"control\" means (i) the power, direct or indirect, to cause the\\n      direction or management of such entity, whether by contract or\\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\\n      outstanding shares, or (iii) beneficial ownership of such entity.\\n\\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\\n      exercising permissions granted by this License.\\n\\n      \"Source\" form shall mean the preferred form for making modifications,\\n      including but not limited to software source code, documentation\\n      source, and configuration files.\\n\\n      \"Object\" form shall mean any form resulting from mechanical\\n      transformation or translation of a Source form, including but\\n      not limited to compiled object code, generated documentation,\\n      and conversions to other media types.\\n\\n      \"Work\" shall mean the work of authorship, whether in Source or\\n      Object form, made available under the License, as indicated by a\\n      copyright notice that is included in or attached to the work\\n      (an example is provided in the Appendix below).\\n\\n      \"Derivative Works\" shall mean any work, whether in Source or Object\\n      form, that is based on (or derived from) the Work and for which the\\n      editorial revisions, annotations, elaborations, or other modifications\\n      represent, as a whole, an original work of authorship. For the purposes\\n      of this License, Derivative Works shall not include works that remain\\n      separable from, or merely link (or bind by name) to the interfaces of,\\n      the Work and Derivative Works thereof.\\n\\n      \"Contribution\" shall mean any work of authorship, including\\n      the original version of the Work and any modifications or additions\\n      to that Work or Derivative Works thereof, that is intentionally\\n      submitted to Licensor for inclusion in the Work by the copyright owner\\n      or by an individual or Legal Entity authorized to submit on behalf of\\n      the copyright owner. For the purposes of this definition, \"submitted\"\\n      means any form of electronic, verbal, or written communication sent\\n      to the Licensor or its representatives, including but not limited to\\n      communication on electronic mailing lists, source code control systems,\\n      and issue tracking systems that are managed by, or on behalf of, the\\n      Licensor for the purpose of discussing and improving the Work, but\\n      excluding communication that is conspicuously marked or otherwise\\n      designated in writing by the copyright owner as \"Not a Contribution.\"\\n\\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\\n      on behalf of whom a Contribution has been received by Licensor and\\n      subsequently incorporated within the Work.\\n\\n   2. Grant of Copyright License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      copyright license to reproduce, prepare Derivative Works of,\\n      publicly display, publicly perform, sublicense, and distribute the\\n      Work and such Derivative Works in Source or Object form.\\n\\n   3. Grant of Patent License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      (except as stated in this section) patent license to make, have made,\\n      use, offer to sell, sell, import, and otherwise transfer the Work,\\n      where such license applies only to those patent claims licensable\\n      by such Contributor that are necessarily infringed by their\\n      Contribution(s) alone or by combination of their Contribution(s)\\n      with the Work to which such Contribution(s) was submitted. If You\\n      institute patent litigation against any entity (including a\\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\\n      or a Contribution incorporated within the Work constitutes direct\\n      or contributory patent infringement, then any patent licenses\\n      granted to You under this License for that Work shall terminate\\n      as of the date such litigation is filed.\\n\\n   4. Redistribution. You may reproduce and distribute copies of the\\n      Work or Derivative Works thereof in any medium, with or without\\n      modifications, and in Source or Object form, provided that You\\n      meet the following conditions:\\n\\n      (a) You must give any other recipients of the Work or\\n          Derivative Works a copy of this License; and\\n\\n      (b) You must cause any modified files to carry prominent notices\\n          stating that You changed the files; and\\n\\n      (c) You must retain, in the Source form of any Derivative Works\\n          that You distribute, all copyright, patent, trademark, and\\n          attribution notices from the Source form of the Work,\\n          excluding those notices that do not pertain to any part of\\n          the Derivative Works; and\\n\\n      (d) If the Work includes a \"NOTICE\" text file as part of its\\n          distribution, then any Derivative Works that You distribute must\\n          include a readable copy of the attribution notices contained\\n          within such NOTICE file, excluding those notices that do not\\n          pertain to any part of the Derivative Works, in at least one\\n          of the following places: within a NOTICE text file distributed\\n          as part of the Derivative Works; within the Source form or\\n          documentation, if provided along with the Derivative Works; or,\\n          within a display generated by the Derivative Works, if and\\n          wherever such third-party notices normally appear. The contents\\n          of the NOTICE file are for informational purposes only and\\n          do not modify the License. You may add Your own attribution\\n          notices within Derivative Works that You distribute, alongside\\n          or as an addendum to the NOTICE text from the Work, provided\\n          that such additional attribution notices cannot be construed\\n          as modifying the License.\\n\\n      You may add Your own copyright statement to Your modifications and\\n      may provide additional or different license terms and conditions\\n      for use, reproduction, or distribution of Your modifications, or\\n      for any such Derivative Works as a whole, provided Your use,\\n      reproduction, and distribution of the Work otherwise complies with\\n      the conditions stated in this License.\\n\\n   5. Submission of Contributions. Unless You explicitly state otherwise,\\n      any Contribution intentionally submitted for inclusion in the Work\\n      by You to the Licensor shall be under the terms and conditions of\\n      this License, without any additional terms or conditions.\\n      Notwithstanding the above, nothing herein shall supersede or modify\\n      the terms of any separate license agreement you may have executed\\n      with Licensor regarding such Contributions.\\n\\n   6. Trademarks. This License does not grant permission to use the trade\\n      names, trademarks, service marks, or product names of the Licensor,\\n      except as required for reasonable and customary use in describing the\\n      origin of the Work and reproducing the content of the NOTICE file.\\n\\n   7. Disclaimer of Warranty. Unless required by applicable law or\\n      agreed to in writing, Licensor provides the Work (and each\\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\\n      implied, including, without limitation, any warranties or conditions\\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\\n      PARTICULAR PURPOSE. You are solely responsible for determining the\\n      appropriateness of using or redistributing the Work and assume any\\n      risks associated with Your exercise of permissions under this License.\\n\\n   8. Limitation of Liability. In no event and under no legal theory,\\n      whether in tort (including negligence), contract, or otherwise,\\n      unless required by applicable law (such as deliberate and grossly\\n      negligent acts) or agreed to in writing, shall any Contributor be\\n      liable to You for damages, including any direct, indirect, special,\\n      incidental, or consequential damages of any character arising as a\\n      result of this License or out of the use or inability to use the\\n      Work (including but not limited to damages for loss of goodwill,\\n      work stoppage, computer failure or malfunction, or any and all\\n      other commercial damages or losses), even if such Contributor\\n      has been advised of the possibility of such damages.\\n\\n   9. Accepting Warranty or Additional Liability. While redistributing\\n      the Work or Derivative Works thereof, You may choose to offer,\\n      and charge a fee for, acceptance of support, warranty, indemnity,\\n      or other liability obligations and/or rights consistent with this\\n      License. However, in accepting such obligations, You may act only\\n      on Your own behalf and on Your sole responsibility, not on behalf\\n      of any other Contributor, and only if You agree to indemnify,\\n      defend, and hold each Contributor harmless for any liability\\n      incurred by, or claims asserted against, such Contributor by reason\\n      of your accepting any such warranty or additional liability.\\n\\n   END OF TERMS AND CONDITIONS\\n\\n   APPENDIX: How to apply the Apache License to your work.\\n\\n      To apply the Apache License to your work, attach the following\\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\\n      replaced with your own identifying information. (Don\\'t include\\n      the brackets!)  The text should be enclosed in the appropriate\\n      comment syntax for the file format. We also recommend that a\\n      file or class name and description of purpose be included on the\\n      same \"printed page\" as the copyright notice for easier\\n      identification within third-party archives.\\n\\n   Copyright [yyyy] [name of copyright owner]\\n\\n   Licensed under the Apache License, Version 2.0 (the \"License\");\\n   you may not use this file except in compliance with the License.\\n   You may obtain a copy of the License at\\n\\n       http://www.apache.org/licenses/LICENSE-2.0\\n\\n   Unless required by applicable law or agreed to in writing, software\\n   distributed under the License is distributed on an \"AS IS\" BASIS,\\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n   See the License for the specific language governing permissions and\\n   limitations under the License.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Lyra: a generative low bitrate speech codec",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "lyra",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "google",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/google/lyra/blob/main/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "aluebs",
        "body": "Lyra version 0.0.2 is now available on [GitHub](https://github.com/google/lyra). The main improvement of this version is the open-source release of the sparse_matmul library code, which was co-developed by Google and DeepMind. That means no more pre-compiled \u201c.so\u201d dynamic library binaries and no more restrictions on which toolchain to use, which opens up the door to port Lyra onto different platforms. The full list of features and fixes include:\r\n* Release sparse_matmul library code and remove pre-compiled dynamic library binaries.\r\n* Add support for the Bazel default gcc toolchain on linux, and make this the default instead of the clang toolchain.\r\n* Fix noise bursts at the beginning of output audio files.\r\n* Abstract out UnitFloatToInt16Scalar, UnitFloatToInt16 and Int16ToUnitFloat functions.\r\n* Provide operator<< to unique_ptr<LayerWrapper> to be used with CHECK() macros.\r\n* Fix float distribution compatibility in benchmark_decode_lib. ",
        "dateCreated": "2021-06-28T16:38:41Z",
        "datePublished": "2021-06-28T16:46:12Z",
        "html_url": "https://github.com/google/lyra/releases/tag/v0.0.2",
        "name": "Lyra 0.0.2",
        "tag_name": "v0.0.2",
        "tarball_url": "https://api.github.com/repos/google/lyra/tarball/v0.0.2",
        "url": "https://api.github.com/repos/google/lyra/releases/45370519",
        "zipball_url": "https://api.github.com/repos/google/lyra/zipball/v0.0.2"
      },
      {
        "authorType": "User",
        "author_name": "mchinen",
        "body": "First release",
        "dateCreated": "2021-04-05T21:09:05Z",
        "datePublished": "2021-04-06T15:50:06Z",
        "html_url": "https://github.com/google/lyra/releases/tag/v0.0.1",
        "name": "v0.0.1",
        "tag_name": "v0.0.1",
        "tarball_url": "https://api.github.com/repos/google/lyra/tarball/v0.0.1",
        "url": "https://api.github.com/repos/google/lyra/releases/41006715",
        "zipball_url": "https://api.github.com/repos/google/lyra/zipball/v0.0.1"
      }
    ],
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "There are a few things you'll need to do to set up your computer to build Lyra.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Building on android requires downloading a specific version of the android NDK\ntoolchain. If you develop with Android Studio already, you might not need to do\nthese steps if ANDROID_HOME and ANDROID_NDK_HOME are defined and pointing at the\nright version of the NDK.\n\n1. Download the sdk manager from https://developer.android.com/studio\n2. Unzip and cd to the directory\n3. Check the available packages to install in case they don't match the following steps.\n\n``` shell\nbin/sdkmanager  --sdk_root=$HOME/android/sdk --list\n```\n\nSome systems will already have the java runtime set up.  But if you see an error\nhere like `ERROR: JAVA_HOME is not set and no 'java' command could be found\non your PATH.`, this means you need to install the java runtime with `sudo apt\ninstall default-jdk` first. You will also need to add `export\nJAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64` (type `ls /usr/lib/jvm` to see\nwhich path was installed) to your $HOME/.bashrc and reload it with `source\n$HOME/.bashrc`.\n\n4. Install the r21 ndk, android sdk 29, and build tools:\n\n``` shell\nbin/sdkmanager  --sdk_root=$HOME/android/sdk --install  \"platforms;android-29\" \"build-tools;29.0.3\" \"ndk;21.4.7075529\"\n```\n\n5. Add the following to .bashrc (or export the variables)\n\n``` shell\nexport ANDROID_NDK_HOME=$HOME/android/sdk/ndk/21.4.7075529\nexport ANDROID_HOME=$HOME/android/sdk\n```\n\n6. Reload .bashrc (with `source $HOME/.bashrc`)\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2945,
      "date": "Tue, 07 Dec 2021 03:31:58 GMT"
    },
    "technique": "GitHub API"
  }
}