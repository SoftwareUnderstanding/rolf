{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2012.12877\">paper</a> has shown that use of a distillation token for distilling knowledge from convolutional nets to vision transformer can yield small and efficient vision transformers. This repository offers the means to do distillation easily.\n\nex. distilling from Resnet50 (or any teacher",
      "https://arxiv.org/abs/2101.11986\">This paper</a> proposes that the first couple layers should downsample the image sequence by unfolding, leading to overlapping image data in each token as shown in the figure above. You can use this variant of the `ViT` as follows.\n\n```python\nimport torch\nfrom vit_pytorch.t2t import T2TViT\n\nv = T2TViT(\n    dim = 512,\n    image_size = 224,\n    depth = 5,\n    heads = 8,\n    mlp_dim = 512,\n    num_classes = 1000,\n    t2t_layers = ((7, 4",
      "https://arxiv.org/abs/2102.03902\">Nystromformer</a>\n\n```bash\n$ pip install nystrom-attention\n```\n\n```python\nimport torch\nfrom vit_pytorch.efficient import ViT\nfrom nystrom_attention import Nystromformer\n\nefficient_transformer = Nystromformer(\n    dim = 512,\n    depth = 12,\n    heads = 8,\n    num_landmarks = 256\n",
      "https://arxiv.org/abs/2002.05202\n        residual_attn = True        # ex. residual attention https://arxiv.org/abs/2012.11747\n    ",
      "https://arxiv.org/abs/2012.11747\n    "
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```bibtex\n@misc{dosovitskiy2020image,\n    title   = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},\n    author  = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},\n    year    = {2020},\n    eprint  = {2010.11929},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n```bibtex\n@misc{touvron2020training,\n    title   = {Training data-efficient image transformers & distillation through attention}, \n    author  = {Hugo Touvron and Matthieu Cord and Matthijs Douze and Francisco Massa and Alexandre Sablayrolles and Herv\u00e9 J\u00e9gou},\n    year    = {2020},\n    eprint  = {2012.12877},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n```bibtex\n@misc{yuan2021tokenstotoken,\n      title     = {Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet},\n      author    = {Li Yuan and Yunpeng Chen and Tao Wang and Weihao Yu and Yujun Shi and Francis EH Tay and Jiashi Feng and Shuicheng Yan},\n      year      = {2021},\n      eprint    = {2101.11986},\n      archivePrefix = {arXiv},\n      primaryClass = {cs.CV}\n}\n```\n\n```bibtex\n@misc{vaswani2017attention,\n    title   = {Attention Is All You Need},\n    author  = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},\n    year    = {2017},\n    eprint  = {1706.03762},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}\n```\n\n*I visualise a time when we will be to robots what dogs are to humans, and I\u2019m rooting for the machines.* \u2014 Claude Shannon\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{vaswani2017attention,\n    title   = {Attention Is All You Need},\n    author  = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},\n    year    = {2017},\n    eprint  = {1706.03762},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{yuan2021tokenstotoken,\n      title     = {Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet},\n      author    = {Li Yuan and Yunpeng Chen and Tao Wang and Weihao Yu and Yujun Shi and Francis EH Tay and Jiashi Feng and Shuicheng Yan},\n      year      = {2021},\n      eprint    = {2101.11986},\n      archivePrefix = {arXiv},\n      primaryClass = {cs.CV}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{touvron2020training,\n    title   = {Training data-efficient image transformers &amp; distillation through attention}, \n    author  = {Hugo Touvron and Matthieu Cord and Matthijs Douze and Francisco Massa and Alexandre Sablayrolles and Herv\u00e9 J\u00e9gou},\n    year    = {2020},\n    eprint  = {2012.12877},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{dosovitskiy2020image,\n    title   = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},\n    author  = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},\n    year    = {2020},\n    eprint  = {2010.11929},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "    depth = 12, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "        depth = 12, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9944484218006108,
        0.9993470052537933
      ],
      "excerpt": "        ff_glu = True,              #: ex. feed forward GLU variant https://arxiv.org/abs/2002.05202 \n        residual_attn = True        #: ex. residual attention https://arxiv.org/abs/2012.11747 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/tianhai123/vit-pytorch",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-03-02T12:15:46Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-06-06T14:01:34Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.93417752778925,
        0.8950606142086208,
        0.8691990834611633
      ],
      "excerpt": "Implementation of <a href=\"https://openreview.net/pdf?id=YicbFdNTTy\">Vision Transformer</a>, a simple way to achieve SOTA in vision classification with only a single transformer encoder, in Pytorch. Significance is further explained in <a href=\"https://www.youtube.com/watch?v=TrdevFK_am4\">Yannic Kilcher's</a> video. There's really not much to code here, but may as well lay it out for everyone so we expedite the attention revolution. \nFor a Pytorch implementation with pretrained models, please see Ross Wightman's repository <a href=\"https://github.com/rwightman/pytorch-image-models\">here</a>. \nThe official Jax repository is <a href=\"https://github.com/google-research/vision_transformer\">here</a>. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8700728345710151
      ],
      "excerpt": "Number of classes to classify. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8479167923301352
      ],
      "excerpt": "    temperature = 3,           #: temperature of distillation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8803713883949499
      ],
      "excerpt": "The DistillableViT class is identical to ViT except for how the forward pass is handled, so you should be able to load the parameters back to ViT after you have completed distillation training. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9753401858756361
      ],
      "excerpt": "    t2t_layers = ((7, 4), (3, 2), (3, 2)) #: tuples of the kernel size and stride of each consecutive layers of the initial token to token module \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "model = ViT( \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "    model, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8115681645908724
      ],
      "excerpt": "    learner.update_moving_average() #: update moving average of target encoder \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8241553636181302,
        0.8315338094020465
      ],
      "excerpt": "A pytorch-lightning script is ready for you to use at the repository link above. \nThere may be some coming from computer vision who think attention still suffers from quadratic costs. Fortunately, we have a lot of new techniques that may help. This repository offers a way for you to plugin your own sparse attention transformer. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/tianhai123/vit-pytorch/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 09 Dec 2021 22:43:49 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/tianhai123/vit-pytorch/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "tianhai123/vit-pytorch",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/tianhai123/vit-pytorch/main/examples/cats_and_dogs.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```bash\n$ pip install vit-pytorch\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8175923902236196
      ],
      "excerpt": "For a Pytorch implementation with pretrained models, please see Ross Wightman's repository <a href=\"https://github.com/rwightman/pytorch-image-models\">here</a>. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8300761503891266
      ],
      "excerpt": "You can also use the handy .to_vit method on the DistillableViT instance to get back a ViT instance. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.943201542578959
      ],
      "excerpt": "You can train this with a near SOTA self-supervised learning technique, <a href=\"https://github.com/lucidrains/byol-pytorch\">BYOL</a>, with the following code. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9995486579735738
      ],
      "excerpt": "$ pip install byol-pytorch \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9070277507087546
      ],
      "excerpt": "A pytorch-lightning script is ready for you to use at the repository link above. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.999746712887969
      ],
      "excerpt": "$ pip install nystrom-attention \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.999746712887969
      ],
      "excerpt": "$ pip install x-transformers \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8341177043401105
      ],
      "excerpt": "depth: int. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8481942365069672
      ],
      "excerpt": "<img src=\"./distill.png\" width=\"300px\"></img> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8900486270063179,
        0.8801854956928516,
        0.8869264826052469
      ],
      "excerpt": "from torchvision.models import resnet50 \nfrom vit_pytorch.distill import DistillableViT, DistillWrapper \nteacher = resnet50(pretrained = True) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639258741734444
      ],
      "excerpt": "    depth = 6, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8481942365069672
      ],
      "excerpt": "<img src=\"./t2t.png\" width=\"400px\"></img> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "from vit_pytorch.t2t import T2TViT \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639258741734444
      ],
      "excerpt": "    depth = 5, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516,
        0.8801854956928516
      ],
      "excerpt": "from vit_pytorch import ViT \nfrom byol_pytorch import BYOL \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639258741734444
      ],
      "excerpt": "    depth = 6, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "from nystrom_attention import Nystromformer \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/tianhai123/vit-pytorch/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 Phil Wang\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Vision Transformer - Pytorch",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "vit-pytorch",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "tianhai123",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/tianhai123/vit-pytorch/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Thu, 09 Dec 2021 22:43:49 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\nimport torch\nfrom vit_pytorch import ViT\n\nv = ViT(\n    image_size = 256,\n    patch_size = 32,\n    num_classes = 1000,\n    dim = 1024,\n    depth = 6,\n    heads = 16,\n    mlp_dim = 2048,\n    dropout = 0.1,\n    emb_dropout = 0.1\n)\n\nimg = torch.randn(1, 3, 256, 256)\nmask = torch.ones(1, 8, 8).bool() #: optional mask, designating which patch to attend to\n\npreds = v(img, mask = mask) #: (1, 1000)\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}