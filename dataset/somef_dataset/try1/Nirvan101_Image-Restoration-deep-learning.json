{
  "citation": [
    {
      "confidence": [
        0.8049272872996004
      ],
      "excerpt": "I constructed this model based on the paper 'Context Encoders: Feature Learning by Inpainting'  found here https://arxiv.org/pdf/1604.07379.pdf \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9252482755105951
      ],
      "excerpt": "I designed my conditional GAN based on the paper 'SEMI-SUPERVISED LEARNING WITH CONTEXT-CONDITIONAL GENERATIVE ADVERSARIAL NETWORK' found here https://arxiv.org/pdf/1611.06430v1.pdf \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Nirvan101/Image-Restoration-deep-learning",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-10-17T17:18:20Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-11T03:18:07Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9617765042098989,
        0.9060443933417174,
        0.834184425961943
      ],
      "excerpt": "Restoring images of damaged paintings using in-painting. Damaged paintings have discolored patches where the paint has faded or fallen off. These patches are often whitish. This project uses image in-painting to fill and restore these lost regions.  \nThis repository contains multiple models that I constructed to solve this task. The models include context-encoders, GANS, conditional GANS and pixel diffusion. \nThere are 68 images in the dataset provided. Out of these only 20 are of good quality. Hence, there are only 20 training images. Example of a good image: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8700467094976195,
        0.9516657298698971,
        0.9558661823692424
      ],
      "excerpt": "As you can see, the damaged painting has many discolored patches which have become white. The aim is to use image in-painting to fill these white patches. \nThe dataset is very small. The training dataset contains only 20 images. So, I have used data augmentation using the ImageDataGenerator. The images are all resized to (256,384,3) as this is the average image size in the dataset. They are then divided by 255 to normalize them.  \nTo models are trained on the good images. The image is first cropped artificially. This cropped version is input into the model and the original image is provided as the ground truth label. Hence, the model is effectively trained to convert the cropped images into their original forms. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.988422024530065
      ],
      "excerpt": "This model contains an encoder and a decoder.  During training, the image is first cropped. A lot of tiny white holes are made on this image- these resemble the white patches that exist in damaged paintings. This modified image is fed into the encoder, it is downsampled into an encoding using Conv layers. The encoding is upsampled using Conv and Upsampling layers. The output is of the same size as the input i.e (256,384,3) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9638263559693796
      ],
      "excerpt": "This was my second design for the context encoder. Here I have added a Dense layer between the encoder and decoder to generate the encoding. The intuition is that the Dense layer will connect features from different regions of the image together and this will improve the inpainting performance. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8842131605147674,
        0.9649816330467347,
        0.9740075471635313,
        0.9884058104939507,
        0.9431352569847785,
        0.8537655970757466
      ],
      "excerpt": "During training, from each image a white square is cropped out from the centre. This modified image is input into the generator. \nThe generator has an encoder-decoder network and it produces an image resembling the input image. This output image is passed to a custom keras layer which also receives the input image from the input layer. This custom layer replaces the masked central region of the input image with the corresponding central region of the generated image. This new image is the final output of the generator. The generator uses mean-square-error as it's loss function. This is callled reconstruction loss. \nThis image is passed to the discriminator. It predicts whether the image is original or generator-produced and this loss is called adversial loss. The use of adversial loss improves the training of the generator. \nThe difference between a regular GAN and a conditional GAN is the use of the custom layer after the generator. This trains the model to produce only the central masked region of the image (and not the entire image as was the case with GAN) using the surrounding regions. \nThe central portion was cropped out from the input image. The model reconstructed it to match the rest of the image. \nThis is a regular GAN where the generator has an mse loss function and the discriminator has a binary_cross_entropy loss function. The generator is designed to generate the entire input image back from the encoding. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Restoring images of damaged paintings using in-painting. Damaged paintings have discolored patches where the paint has faded or fallen off. This project uses image in-painting to fill and restore these lost regions.  The models include context-encoders, GANS, conditional GANS and pixel diffusion.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Nirvan101/Image-Restoration-deep-learning/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 9,
      "date": "Sat, 11 Dec 2021 01:48:50 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Nirvan101/Image-Restoration-deep-learning/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Nirvan101/Image-Restoration-deep-learning",
    "technique": "GitHub API"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Nirvan101/Image-Restoration-deep-learning/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Image restoration by in-painting",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Image-Restoration-deep-learning",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Nirvan101",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Nirvan101/Image-Restoration-deep-learning/blob/dev/readme.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 15,
      "date": "Sat, 11 Dec 2021 01:48:50 GMT"
    },
    "technique": "GitHub API"
  }
}