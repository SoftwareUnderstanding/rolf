{
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Code:\n\n- [openai/gpt-2](https://github.com/openai/gpt-2) has the model but not the training code, and in TensorFlow\n- [openai/image-gpt](https://github.com/openai/image-gpt) has some more modern gpt-3 like modification in its code, good reference as well\n- huggingface/transformers has a [language-modeling example](https://github.com/huggingface/transformers/tree/master/examples/language-modeling). It is full-featured but as a result also somewhat challenging to trace. E.g. some large functions have as much as 90% unused code behind various branching statements that is unused in the default setting of simple language modeling.\n\nPapers + some implementation notes:\n\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/karynaur/Attention-Free-minGPT",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-06-08T18:10:10Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-07-30T17:51:22Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9979517894653105,
        0.9238744022393898
      ],
      "excerpt": "A PyTorch re-implementation of GPT training. minGPT tries to be small, clean, interpretable and educational, as most of the currently available ones are a bit sprawling. GPT is not a complicated model and this implementation is appropriately about 300 lines of code, including boilerplate and a totally unnecessary custom causal self-attention module. Anyway, all that's going on is that a sequence of indices goes into a sequence of transformer blocks, and a probability distribution of the next index comes out. The rest of the complexity is just being clever with batching (both across examples and over sequence length) so that training is efficient. \nThe core minGPT \"library\" (hah) is two files: mingpt/model.py contains the actual Transformer model definition and mingpt/trainer.py is (GPT-independent) PyTorch boilerplate that trains the model. The attached Jupyter notebooks then show how the \"library\" (hah) can be used to train sequence models: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9857257427755592
      ],
      "excerpt": "play_char.ipynb trains a GPT to be a character-level language model on arbitrary text, similar to my older char-rnn but with a transformer instead of an RNN \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9791132511632669,
        0.9687180639016032,
        0.8492660209537171,
        0.9388642861282658
      ],
      "excerpt": "With a bpe encoder, distributed training and maybe fp16 this implementation may be able to reproduce GPT-1/GPT-2 results, though I haven't tried $$$. GPT-3 is likely out of reach as my understanding is that it does not fit into GPU memory and requires a more careful model-parallel treatment. \nOur model largely follows the original transformer work \nWe trained a 12-layer decoder-only transformer with masked self-attention heads (768 dimensional states and 12 attention heads). For the position-wise feed-forward networks, we used 3072 dimensional inner states. \nAdam max learning rate of 2.5e-4. (later GPT-3 for this model size uses 6e-4) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8972722103477111,
        0.9739691317476938,
        0.9567588029116127,
        0.9397047223015782,
        0.9325074906311407
      ],
      "excerpt": "We train for 100 epochs on minibatches of 64 randomly sampled, contiguous sequences of 512 tokens. \nSince layernorm is used extensively throughout the model, a simple weight initialization of N(0, 0.02) was sufficient \nbytepair encoding (BPE) vocabulary with 40,000 merges \nresidual, embedding, and attention dropouts with a rate of 0.1 for regularization. \nmodified version of L2 regularization proposed in (37), with w = 0.01 on all non bias or gain weights \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.947513840584867,
        0.98318092203945,
        0.9237105068539875,
        0.8957406778287104
      ],
      "excerpt": "We used learned position embeddings instead of the sinusoidal version proposed in the original work \nFor finetuning: We add dropout to the classifier with a rate of 0.1. learning rate of 6.25e-5 and a batchsize of 32. 3 epochs. We use a linear learning rate decay schedule with warmup over 0.2% of training. \u03bb was set to 0.5. \nGPT-1 model is 12 layers and d_model 768, ~117M params \nLayerNorm was moved to the input of each sub-block, similar to a pre-activation residual network \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9809822372157164,
        0.9835328454100852
      ],
      "excerpt": "modified initialization which accounts for the accumulation on the residual path with model depth is used. We scale the weights of residual layers at initialization by a factor of 1/\u221aN where N is the number of residual layers. (weird because in their released code i can only find a simple use of the old 0.02... in their release of image-gpt I found it used for c_proj, and even then only for attn, not for mlp. huh. https://github.com/openai/image-gpt/blob/master/src/model.py) \nthe vocabulary is expanded to 50,257 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9889018160060187
      ],
      "excerpt": "larger batchsize of 512 is used \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8998933554189524
      ],
      "excerpt": "GPT-3: 96 layers, 96 heads, with d_model of 12,288 (175B parameters). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8100094258067154,
        0.8879158920529847,
        0.8299762644273461
      ],
      "excerpt": "We use the same model and architecture as GPT-2, including the modified initialization, pre-normalization, and reversible tokenization described therein \nwe use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer \nwe always have the feedforward layer four times the size of the bottleneck layer, dff = 4 \u2217 dmodel \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9531743722108134,
        0.8848461256947826
      ],
      "excerpt": "All models use weight decay of 0.1 to provide a small amount of regularization. (NOTE: GPT-1 used 0.01 I believe, see above) \nclip the global norm of the gradient at 1.0 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8587769946081139,
        0.9843354887588837,
        0.8969371388004213,
        0.8605775798280201,
        0.9791898093777289,
        0.9954615831183371,
        0.8800615543717267,
        0.9192379297541117,
        0.9615721534236563
      ],
      "excerpt": "gradually increase the batch size linearly from a small value (32k tokens) to the full value over the first 4-12 billion tokens of training, depending on the model size. \nfull 2048-sized time context window is always used, with a special END OF DOCUMENT token delimiter \nWhen working with images, we pick the identity permutation \u03c0i = i for 1 \u2264 i \u2264 n, also known as raster order. \nwe create our own 9-bit color palette by clustering (R, G, B) pixel values using k-means with k = 512. \nOur largest model, iGPT-XL, contains L = 60 layers and uses an embedding size of d = 3072 for a total of 6.8B parameters. \nOur next largest model, iGPT-L, is essentially identical to GPT-2 with L = 48 layers, but contains a slightly smaller embedding size of d = 1536 (vs 1600) for a total of 1.4M parameters. \nWe use the same model code as GPT-2, except that we initialize weights in the layerdependent fashion as in Sparse Transformer (Child et al., 2019) and zero-initialize all projections producing logits. \nWe also train iGPT-M, a 455M parameter model with L = 36 and d = 1024 \niGPT-S, a 76M parameter model with L = 24 and d = 512 (okay, and how many heads? looks like the Github code claims 8) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9830908108515696,
        0.8852850687958339
      ],
      "excerpt": "Adam with \u03b21 = 0.9 and \u03b22 = 0.95 \nThe learning rate is warmed up for one epoch, and then decays to 0 \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/karynaur/Attention-Free-minGPT/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Tue, 07 Dec 2021 20:42:05 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/karynaur/Attention-Free-minGPT/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "karynaur/Attention-Free-minGPT",
    "technique": "GitHub API"
  },
  "invocation": [
    {
      "confidence": [
        0.8725573695693956
      ],
      "excerpt": "gradually increase the batch size linearly from a small value (32k tokens) to the full value over the first 4-12 billion tokens of training, depending on the model size. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/karynaur/Attention-Free-minGPT/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'The MIT License (MIT) Copyright (c) 2020 Andrej Karpathy\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "minGPT",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Attention-Free-minGPT",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "karynaur",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/karynaur/Attention-Free-minGPT/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 5,
      "date": "Tue, 07 Dec 2021 20:42:05 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This code is simple enough to just hack inline, not \"used\", but current API looks something like:\n\n```python\n\n#: you're on your own to define a class that returns individual examples as PyTorch LongTensors\nfrom torch.utils.data import Dataset\ntrain_dataset = MyDataset(...)\ntest_dataset = MyDataset(...)\n\n#: construct a GPT model\nfrom mingpt.model import GPT, GPTConfig\nmconf = GPTConfig(vocab_size, block_size, n_layer=12, n_head=12, n_embd=768) #: a GPT-1\nmodel = GPT(mconf)\n\n#: construct a trainer\nfrom mingpt.trainer import Trainer, TrainerConfig\ntconf = TrainerConfig(max_epochs=10, batch_size=256)\ntrainer = Trainer(model, train_dataset, test_dataset, tconf)\ntrainer.train()\n#: (... enjoy the show for a while... )\n\n#: sample from the model (the [None, ...] and [0] are to push/pop a needed dummy batch dimension)\nfrom mingpt.utils import sample\nx = torch.tensor([1, 2, 3], dtype=torch.long)[None, ...] #: context conditioning\ny = sample(model, x, steps=30, temperature=1.0, sample=True, top_k=5)[0]\nprint(y) #: our model filled in the integer sequence with 30 additional likely integers\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}