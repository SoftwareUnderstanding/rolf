{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1506.02640",
      "https://arxiv.org/abs/1612.08242",
      "https://arxiv.org/abs/1506.02640",
      "https://arxiv.org/abs/1612.08242"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9905302655072422
      ],
      "excerpt": "In this project, you will learn about object detection using the very powerful YOLO model. Many of the ideas in this notebook are described in the two YOLO papers: Redmon et al., 2016 (https://arxiv.org/abs/1506.02640) and Redmon and Farhadi, 2016 (https://arxiv.org/abs/1612.08242).  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9327882686852367
      ],
      "excerpt": "- Use object detection on a car detection dataset \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8550101043698384
      ],
      "excerpt": "scores[2] = 10.750582 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "           10.7506 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8550101043698399
      ],
      "excerpt": "boxes.shape = (10, 4) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "           (10,) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "           (10, 4) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "           (10,) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "def yolo_eval(yolo_outputs, image_shape = (720., 1280.), max_boxes=10, score_threshold=.6, iou_threshold=.5): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8550101043698399
      ],
      "excerpt": "boxes.shape = (10, 4) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "           (10,) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "           (10, 4) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "           (10,) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Donvink/Car_Detection_for_Autonomous_Driving",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-06-08T13:18:25Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-06-21T02:00:23Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8327406271020663
      ],
      "excerpt": "In this project, you will learn about object detection using the very powerful YOLO model. Many of the ideas in this notebook are described in the two YOLO papers: Redmon et al., 2016 (https://arxiv.org/abs/1506.02640) and Redmon and Farhadi, 2016 (https://arxiv.org/abs/1612.08242).  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9436679659450707
      ],
      "excerpt": "YOLO (\"you only look once\") is a popular algoritm because it achieves high accuracy while also being able to run in real-time. This algorithm \"only looks once\" at the image in the sense that it requires only one forward propagation pass through the network to make predictions. After non-max suppression, it then outputs recognized objects together with the bounding boxes. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9468340020087262
      ],
      "excerpt": "- The input is a batch of images of shape (m, 608, 608, 3) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8856792956662058,
        0.9370170171233058
      ],
      "excerpt": "Since we are using 5 anchor boxes, each of the 19 x19 cells thus encodes information about 5 boxes. Anchor boxes are defined only by their width and height. \nFor simplicity, we will flatten the last two last dimensions of the shape (19, 19, 5, 85) encoding. So the output of the Deep CNN is (19, 19, 425). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9173414677889161,
        0.8326347032195731
      ],
      "excerpt": "Here's one way to visualize what YOLO is predicting on an image: \n- For each of the 19x19 grid cells, find the maximum of the probability scores (taking a max across both the 5 anchor boxes and across different classes).  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8253326672094371,
        0.9719148643123814,
        0.8793025771087044
      ],
      "excerpt": "<caption><center> <u> Figure 5 </u>: Each of the 19x19 grid cells colored according to which class has the largest predicted probability in that cell.<br> </center></caption> \nNote that this visualization isn't a core part of the YOLO algorithm itself for making predictions; it's just a nice way of visualizing an intermediate result of the algorithm.  \nAnother way to visualize YOLO's output is to plot the bounding boxes that it outputs. Doing that results in a visualization like this:   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9332154252446043,
        0.9167912181299787
      ],
      "excerpt": "In the figure above, we plotted only boxes that the model had assigned a high probability to, but this is still too many boxes. You'd like to filter the algorithm's output down to a much smaller number of detected objects. To do so, you'll use non-max suppression. Specifically, you'll carry out these steps:  \n- Get rid of boxes with a low score (meaning, the box is not very confident about detecting a class) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8486188164085576
      ],
      "excerpt": "You are going to apply a first filter by thresholding. You would like to get rid of any box for which the class \"score\" is less than a chosen threshold.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9485444302863538,
        0.8286220305734527,
        0.9293806828281952
      ],
      "excerpt": "- box_confidence: tensor of shape $(19 \\times 19, 5, 1)$ containing $p_c$ (confidence probability that there's some object) for each of the 5 boxes predicted in each of the 19x19 cells. \n- boxes: tensor of shape $(19 \\times 19, 5, 4)$ containing $(b_x, b_y, b_h, b_w)$ for each of the 5 boxes per cell. \n- box_class_probs: tensor of shape $(19 \\times 19, 5, 80)$ containing the detection probabilities $(c_1, c_2, ... c_{80})$ for each of the 80 classes for each of the 5 boxes per cell. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8053963807973044,
        0.8311661673912335,
        0.8309179777945646
      ],
      "excerpt": "boxes -- tensor of shape (None, 4), containing (b_x, b_y, b_h, b_w) coordinates of selected boxes \nclasses -- tensor of shape (None,), containing the index of the class detected by the selected boxes \nNote: \"None\" is here because you don't know the exact number of selected boxes, as it depends on the threshold.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8388934377115403
      ],
      "excerpt": "#: Step 2: Find the box_classes thanks to the max box_scores, keep track of the corresponding score \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8925255001523636
      ],
      "excerpt": "#: Step 3: Create a filtering mask based on \"box_class_scores\" by using \"threshold\". The mask should have the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8361780148049197
      ],
      "excerpt": "#: Step 4: Apply the mask to scores, boxes and classes \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9319708861235405
      ],
      "excerpt": "with tf.Session() as test_a: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9218651260677998
      ],
      "excerpt": "Even after filtering by thresholding over the classes scores, you still end up a lot of overlapping boxes. A second filter for selecting the right boxes is called non-maximum suppression (NMS).  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8382488351046105
      ],
      "excerpt": "<caption><center> <u> Figure 7 </u>: In this example, the model has predicted 3 cars, but it's actually 3 predictions of the same car. Running non-max suppression (NMS) will select only the most accurate (highest probabiliy) one of the 3 boxes. <br> </center></caption> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8621455384746641
      ],
      "excerpt": "<caption><center> <u> Figure 8 </u>: Definition of \"Intersection over Union\". <br> </center></caption> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8771509334360086
      ],
      "excerpt": "- In this exercise only, we define a box using its two corners (upper left and lower right): (x1, y1, x2, y2) rather than the midpoint and height/width. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8694932219518253,
        0.9532755747856733,
        0.9532755747856733,
        0.9357274257320994,
        0.9357274257320994,
        0.986327061336651
      ],
      "excerpt": "- You'll also need to find the coordinates (xi1, yi1, xi2, yi2) of the intersection of two boxes. Remember that: \n    - xi1 = maximum of the x1 coordinates of the two boxes \n    - yi1 = maximum of the y1 coordinates of the two boxes \n    - xi2 = minimum of the x2 coordinates of the two boxes \n    - yi2 = minimum of the y2 coordinates of the two boxes \nIn this code, we use the convention that (0,0) is the top-left corner of an image, (1,0) is the upper-right corner, and (1,1) the lower-right corner.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8446164947909168
      ],
      "excerpt": "    \"\"\"Implement the intersection over union (IoU) between box1 and box2 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9693882100751168
      ],
      "excerpt": "#: Calculate the (y1, x1, y2, x2) coordinates of the intersection of box1 and box2. Calculate its Area. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8449488158128348
      ],
      "excerpt": "#: Calculate the Union area by using Formula: Union(A,B) = A + B - Inter(A,B) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8959787923592542,
        0.8801189641067418,
        0.8031011972573486,
        0.8042230283824139
      ],
      "excerpt": "2. Compute its overlap with all other boxes, and remove boxes that overlap it more than iou_threshold. \n3. Go back to step 1 and iterate until there's no more boxes with a lower score than the current selected box. \nThis will remove all boxes that have a large overlap with the selected boxes. Only the \"best\" boxes remain. \nExercise: Implement yolo_non_max_suppression() using TensorFlow. TensorFlow has two built-in functions that are used to implement non-max suppression (so you don't actually need to use your iou() implementation): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8750082570601597
      ],
      "excerpt": "    Applies Non-max suppression (NMS) to set of boxes \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8121740324293889,
        0.943090074498745,
        0.8561482335604668
      ],
      "excerpt": "scores -- tensor of shape (None,), output of yolo_filter_boxes() \nboxes -- tensor of shape (None, 4), output of yolo_filter_boxes() that have been scaled to the image size (see later) \nclasses -- tensor of shape (None,), output of yolo_filter_boxes() \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8722535671026492
      ],
      "excerpt": "function will transpose the shapes of scores, boxes, classes. This is made for convenience. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8236947129164574
      ],
      "excerpt": "#: Use tf.image.non_max_suppression() to get the list of indices corresponding to boxes you keep \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9319708861235405
      ],
      "excerpt": "with tf.Session() as test_b: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9394449182630016
      ],
      "excerpt": "Instructions for updating: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9634508859373675,
        0.9338094696613622
      ],
      "excerpt": "It's time to implement a function taking the output of the deep CNN (the 19x19x5x85 dimensional encoding) and filtering through all the boxes using the functions you've just implemented.  \nExercise: Implement yolo_eval() which takes the output of the YOLO encoding and filters the boxes using score threshold and NMS. There's just one last implementational detail you have to know. There're a few ways of representing boxes, such as via their corners or via their midpoint and height/width. YOLO converts between a few such formats at different times, using the following functions (which we have provided):  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8144515307399781
      ],
      "excerpt": "    Converts the output of YOLO encoding (a lot of boxes) to your predicted boxes along with their scores, box coordinates and classes. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.978649051831229
      ],
      "excerpt": "yolo_outputs -- output of the encoding model (for image_shape of (608, 608, 3)), contains 4 tensors: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8116493214717406
      ],
      "excerpt": "image_shape -- tensor of shape (2,) containing the input shape, in this notebook we use (608., 608.) (has to be float32 dtype) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9319708861235405
      ],
      "excerpt": "with tf.Session() as test_b: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9394449182630016
      ],
      "excerpt": "Summary for YOLO: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8053575894116508,
        0.9641024904016912
      ],
      "excerpt": "    - 425 = 5 x 85 because each cell contains predictions for 5 boxes, corresponding to 5 anchor boxes, as seen in lecture.  \n    - 85 = 5 + 80 where 5 is because $(p_c, b_x, b_y, b_h, b_w)$ has 5 numbers, and and 80 is the number of classes we'd like to detect \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9555401806354085
      ],
      "excerpt": "Recall that we are trying to detect 80 classes, and are using 5 anchor boxes. We have gathered the information about the 80 classes and 5 boxes in two files \"coco_classes.txt\" and \"yolo_anchors.txt\". Let's load these quantities into the model by running the next cell.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9911613550114479
      ],
      "excerpt": "Training a YOLO model takes a very long time and requires a fairly large dataset of labelled bounding boxes for a large range of target classes. You are going to load an existing pretrained Keras YOLO model stored in \"yolo.h5\". (These weights come from the official YOLO website, and were converted using a function written by Allan Zelener. References are at the end of this notebook. Technically, these are the parameters from the \"YOLOv2\" model, but we will more simply refer to it as \"YOLO\" in this notebook.) Run the cell below to load the model from this file. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9609950221432809
      ],
      "excerpt": "This loads the weights of a trained YOLO model. Here's a summary of the layers your model contains. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.860059181823877
      ],
      "excerpt": "Model: \"model_1\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8871103100067651,
        0.941314849369453
      ],
      "excerpt": "Reminder: this model converts a preprocessed batch of input images (shape: (m, 608, 608, 3)) into a tensor of shape (m, 19, 19, 5, 85) as explained in Figure (2). \nThe output of yolo_model is a (m, 19, 19, 5, 85) tensor that needs to pass through non-trivial processing and conversion. The following cell does that for you. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8609561449997425
      ],
      "excerpt": "You added yolo_outputs to your graph. This set of 4 tensors is ready to be used as input by your yolo_eval function. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "In this project, you will learn about object detection using the very powerful YOLO model. ",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Donvink/Car_Detection_for_Autonomous_Driving/releases",
    "technique": "GitHub API"
  },
  "faq": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You are working on a self-driving car. As a critical component of this project, you'd like to first build a car detection system. To collect data, you've mounted a camera to the hood (meaning the front) of the car, which takes pictures of the road ahead every few seconds while you drive around. \n\n<center>\n<video width=\"400\" height=\"200\" src=\"nb_images/road_video_compressed2.mp4\" type=\"video/mp4\" controls>\n</video>\n</center>\n\n<caption><center> Pictures taken from a car-mounted camera while driving around Silicon Valley. <br> We would like to especially thank [drive.ai](https://www.drive.ai/) for providing this dataset! Drive.ai is a company building the brains of self-driving vehicles.\n</center></caption>\n\n<img src=\"nb_images/driveai.png\" style=\"width:100px;height:100;\">\n\nYou've gathered all these images into a folder and have labelled them by drawing bounding boxes around every car you found. Here's an example of what your bounding boxes look like.\n\n<img src=\"nb_images/box_label.png\" style=\"width:500px;height:250;\">\n<caption><center> <u> **Figure 1** </u>: **Definition of a box**<br> </center></caption>\n\nIf you have 80 classes that you want YOLO to recognize, you can represent the class label $c$ either as an integer from 1 to 80, or as an 80-dimensional vector (with 80 numbers) one component of which is 1 and the rest of which are 0. The video lectures had used the latter representation; in this notebook, we will use both representations, depending on which is more convenient for a particular step.  \n\nIn this exercise, you will learn how YOLO works, then apply it to car detection. Because the YOLO model is very computationally expensive to train, we will load pre-trained weights for you to use. \n\n",
      "technique": "Header extraction"
    }
  ],
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 09 Dec 2021 10:10:50 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Donvink/Car_Detection_for_Autonomous_Driving/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Donvink/Car_Detection_for_Autonomous_Driving",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Donvink/Car_Detection_for_Autonomous_Driving/master/Autonomous%20driving%20application%20-%20Car%20detection.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8616112999607634
      ],
      "excerpt": "Run the following cell to load the packages and dependencies that are going to be useful for your journey! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8172233963137904
      ],
      "excerpt": "from matplotlib.pyplot import imshow \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8479533633690711
      ],
      "excerpt": "You are now ready to implement non-max suppression. The key steps are:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8048150835188328
      ],
      "excerpt": "Note: The \"None\" dimension of the output tensors has obviously to be less than max_boxes. Note also that this \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8533592862298244,
        0.858204411280099
      ],
      "excerpt": "WARNING:tensorflow:From D:\\ProgramFiles\\Anaconda3\\envs\\tf1cpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version. \nInstructions for updating: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9133368656218674,
        0.8401558704798054,
        0.9068127677393759,
        0.8456955246214731,
        0.8411106616136771,
        0.8500918837456705,
        0.9133368656218674,
        0.9457175861910134
      ],
      "excerpt": "import argparse \nimport os \nimport matplotlib.pyplot as plt \nfrom matplotlib.pyplot import imshow \nimport scipy.io \nimport scipy.misc \nimport imageio \nimport numpy as np \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9133368656218674,
        0.925671696398174
      ],
      "excerpt": "import PIL \nimport tensorflow as tf \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8608503699745261,
        0.9040368155137037,
        0.8801854956928516,
        0.8900486270063179
      ],
      "excerpt": "from keras.layers import Input, Lambda, Conv2D \nfrom keras.models import load_model, Model \nfrom yolo_utils import read_classes, read_anchors, generate_colors, preprocess_image, draw_boxes, scale_boxes \nfrom yad2k.models.keras_yolo import yolo_head, yolo_boxes_to_corners, preprocess_true_boxes, yolo_loss, yolo_body \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8610314881915188
      ],
      "excerpt": "<img src=\"nb_images/architecture.png\" style=\"width:700px;height:400;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8941910184428716
      ],
      "excerpt": "<img src=\"nb_images/flatten.png\" style=\"width:700px;height:400;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8941910184428716
      ],
      "excerpt": "<img src=\"nb_images/probability_extraction.png\" style=\"width:700px;height:400;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8812391599148273
      ],
      "excerpt": "<img src=\"nb_images/proba_map.png\" style=\"width:300px;height:300;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8899156007973423
      ],
      "excerpt": "<img src=\"nb_images/anchor_map.png\" style=\"width:200px;height:200;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8090035232002298,
        0.8343628608431144
      ],
      "excerpt": "a = np.random.randn(19*19, 5, 1) \nb = np.random.randn(19*19, 5, 80) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8562148365037651
      ],
      "excerpt": "3. Create a mask by using a threshold. As a reminder: ([0.9, 0.3, 0.4, 0.5, 0.1] &lt; 0.4) returns: [False, True, False, False, True]. The mask should be True for the boxes you want to keep.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8327911687673031
      ],
      "excerpt": "#:#:#: START CODE HERE #:#:#: (\u2248 1 line) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8327911687673031
      ],
      "excerpt": "#:#:#: START CODE HERE #:#:#: (\u2248 1 line) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8135654125968134
      ],
      "excerpt": "with tf.Session() as test_a: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8005828872011885,
        0.8242889357355769,
        0.8262115037129875,
        0.8296092981743578,
        0.8487328142258568
      ],
      "excerpt": "    print(\"boxes[2] = \" + str(boxes[2].eval())) \n    print(\"classes[2] = \" + str(classes[2].eval())) \n    print(\"scores.shape = \" + str(scores.shape)) \n    print(\"boxes.shape = \" + str(boxes.shape)) \n    print(\"classes.shape = \" + str(classes.shape)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8617238643152603
      ],
      "excerpt": "<img src=\"nb_images/non-max-suppression.png\" style=\"width:500px;height:400;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8941910184428716
      ],
      "excerpt": "<img src=\"nb_images/iou.png\" style=\"width:500px;height:400;\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8327911687673031
      ],
      "excerpt": "#:#:#: START CODE HERE #:#:#: (\u2248 1 line) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9163086562459459
      ],
      "excerpt": "print(\"iou = \" + str(iou(box1, box2))) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8327911687673031
      ],
      "excerpt": "#:#:#: START CODE HERE #:#:#: (\u2248 1 line) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8135654125968134
      ],
      "excerpt": "with tf.Session() as test_b: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8005828872011885,
        0.8242889357355769,
        0.8462285573686679,
        0.8491401053904034,
        0.86568185256088
      ],
      "excerpt": "    print(\"boxes[2] = \" + str(boxes[2].eval())) \n    print(\"classes[2] = \" + str(classes[2].eval())) \n    print(\"scores.shape = \" + str(scores.eval().shape)) \n    print(\"boxes.shape = \" + str(boxes.eval().shape)) \n    print(\"classes.shape = \" + str(classes.eval().shape)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8135654125968134
      ],
      "excerpt": "with tf.Session() as test_b: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8005828872011885,
        0.8242889357355769,
        0.8462285573686679,
        0.8491401053904034,
        0.86568185256088
      ],
      "excerpt": "    print(\"boxes[2] = \" + str(boxes[2].eval())) \n    print(\"classes[2] = \" + str(classes[2].eval())) \n    print(\"scores.shape = \" + str(scores.eval().shape)) \n    print(\"boxes.shape = \" + str(boxes.eval().shape)) \n    print(\"classes.shape = \" + str(classes.eval().shape)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8079100768284847
      ],
      "excerpt": "    - Each cell in a 19x19 grid over the input image gives 425 numbers.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8437227899947999
      ],
      "excerpt": "WARNING:tensorflow:From D:\\code\\yolo\\YAD2K\\yad2k\\models\\keras_yolo.py:32: The name tf.space_to_depth is deprecated. Please use tf.compat.v1.space_to_depth instead. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.906614234580968
      ],
      "excerpt": "Layer (type)                    Output Shape         Param \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8811609628067754
      ],
      "excerpt": "Total params: 50,983,561 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8859504590581576
      ],
      "excerpt": "yolo_outputs = yolo_head(yolo_model.output, anchors, len(class_names)) \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Donvink/Car_Detection_for_Autonomous_Driving/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other",
      "url": "https://raw.githubusercontent.com/Donvink/Car_Detection_for_Autonomous_Driving/master/LICENSE"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'COPYRIGHT\\n\\nAll contributions by deeplearning.ai (Kian Katanforoosh, Younes Bensouda Mourri, Andrew Ng):\\nCopyright (c) 2017, deeplearning.ai (Kian Katanforoosh, Younes Bensouda Mourri, Andrew Ng).\\nAll rights reserved.\\n\\nThis work incorporates contributions due to Allan Zelener released under an MIT License, reproduced below:\\n\\n----------------------------------------------------\\nAll contributions by Allan Zelener:\\nCopyright (c) 2017, Allan Zelener.\\nAll rights reserved.\\n\\nAll other contributions:\\nCopyright (c) 2017, the respective contributors.\\nAll rights reserved.\\n\\nEach contributor holds copyright over their respective contributions.\\nThe project versioning (Git) records all such contribution source information.\\n\\nLICENSE\\n\\nThe MIT License (MIT)\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n----------------------------------------------------\\n\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Autonomous driving - Car detection",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Car_Detection_for_Autonomous_Driving",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Donvink",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Donvink/Car_Detection_for_Autonomous_Driving/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Let the fun begin. You have created a (`sess`) graph that can be summarized as follows:\n\n1. <font color='purple'> yolo_model.input </font> is given to `yolo_model`. The model is used to compute the output <font color='purple'> yolo_model.output </font>\n2. <font color='purple'> yolo_model.output </font> is processed by `yolo_head`. It gives you <font color='purple'> yolo_outputs </font>\n3. <font color='purple'> yolo_outputs </font> goes through a filtering function, `yolo_eval`. It outputs your predictions: <font color='purple'> scores, boxes, classes </font>\n\n**Exercise**: Implement predict() which runs the graph to test YOLO on an image.\nYou will need to run a TensorFlow session, to have it compute `scores, boxes, classes`.\n\nThe code below also uses the following function:\n```python\nimage, image_data = preprocess_image(\"images/\" + image_file, model_image_size = (608, 608))\n```\nwhich outputs:\n- image: a python (PIL) representation of your image used for drawing boxes. You won't need to use it.\n- image_data: a numpy-array representing the image. This will be the input to the CNN.\n\n**Important note**: when a model uses BatchNorm (as is the case in YOLO), you will need to pass an additional placeholder in the feed_dict {K.learning_phase(): 0}.\n\n\n```python\ndef predict(sess, image_file):\n    \"\"\"\n    Runs the graph stored in \"sess\" to predict boxes for \"image_file\". Prints and plots the preditions.\n    \n    Arguments:\n    sess -- your tensorflow/Keras session containing the YOLO graph\n    image_file -- name of an image stored in the \"images\" folder.\n    \n    Returns:\n    out_scores -- tensor of shape (None, ), scores of the predicted boxes\n    out_boxes -- tensor of shape (None, 4), coordinates of the predicted boxes\n    out_classes -- tensor of shape (None, ), class index of the predicted boxes\n    \n    Note: \"None\" actually represents the number of predicted boxes, it varies between 0 and max_boxes. \n    \"\"\"\n\n    #: Preprocess your image\n    image, image_data = preprocess_image(\"images/\" + image_file, model_image_size = (608, 608))\n\n    #: Run the session with the correct tensors and choose the correct placeholders in the feed_dict.\n    #: You'll need to use feed_dict={yolo_model.input: ... , K.learning_phase(): 0})\n    #:#:#: START CODE HERE #:#:#: (\u2248 1 line)\n    out_scores, out_boxes, out_classes = sess.run([scores, boxes, classes],feed_dict={yolo_model.input: image_data,K.learning_phase(): 0})\n    #:#:#: END CODE HERE #:#:#:\n\n    #: Print predictions info\n    print('Found {} boxes for {}'.format(len(out_boxes), image_file))\n    #: Generate colors for drawing bounding boxes.\n    colors = generate_colors(class_names)\n    #: Draw bounding boxes on the image file\n    draw_boxes(image, out_scores, out_boxes, out_classes, class_names, colors)\n    #: Save the predicted bounding box on the image\n    image.save(os.path.join(\"out\", image_file), quality=90)\n    #: Display the results in the notebook\n    #:output_image = scipy.misc.imread(os.path.join(\"out\", image_file))\n    output_image = imageio.imread(os.path.join(\"out\", image_file))\n    imshow(output_image)\n    \n    return out_scores, out_boxes, out_classes\n```\n\nRun the following cell on the \"test.jpg\" image to verify that your function is correct.\n\n\n```python\nout_scores, out_boxes, out_classes = predict(sess, \"test.jpg\")\n```\n\n    Found 7 boxes for test.jpg\n    car 0.60 (925, 285) (1045, 374)\n    bus 0.67 (5, 267) (220, 407)\n    car 0.68 (705, 279) (786, 351)\n    car 0.70 (947, 324) (1280, 704)\n    car 0.75 (159, 303) (346, 440)\n    car 0.80 (762, 282) (942, 412)\n    car 0.89 (366, 299) (745, 648)\n    \n\n\n![png](output_47_1.png)\n\n\n**Expected Output**:\n\n<table>\n    <tr>\n        <td>\n            **Found 7 boxes for test.jpg**\n        </td>\n    </tr>\n    <tr>\n        <td>\n            **car**\n        </td>\n        <td>\n           0.60 (925, 285) (1045, 374)\n        </td>\n    </tr>\n    <tr>\n        <td>\n            **car**\n        </td>\n        <td>\n           0.66 (706, 279) (786, 350)\n        </td>\n    </tr>\n    <tr>\n        <td>\n            **bus**\n        </td>\n        <td>\n           0.67 (5, 266) (220, 407)\n        </td>\n    </tr>\n    <tr>\n        <td>\n            **car**\n        </td>\n        <td>\n           0.70 (947, 324) (1280, 705)\n        </td>\n    </tr>\n    <tr>\n        <td>\n            **car**\n        </td>\n        <td>\n           0.74 (159, 303) (346, 440)\n        </td>\n    </tr>\n    <tr>\n        <td>\n            **car**\n        </td>\n        <td>\n           0.80 (761, 282) (942, 412)\n        </td>\n    </tr>\n    <tr>\n        <td>\n            **car**\n        </td>\n        <td>\n           0.89 (367, 300) (745, 648)\n        </td>\n    </tr>\n</table>\n\nThe model you've just run is actually able to detect 80 different classes listed in \"coco_classes.txt\". To test the model on your own images:\n    1. Click on \"File\" in the upper bar of this notebook, then click \"Open\" to go on your Coursera Hub.\n    2. Add your image to this Jupyter Notebook's directory, in the \"images\" folder\n    3. Write your image's name in the cell above code\n    4. Run the code and see the output of the algorithm!\n\nIf you were to run your session in a for loop over all your images. Here's what you would get:\n\n<center>\n<video width=\"400\" height=\"200\" src=\"nb_images/pred_video_compressed2.mp4\" type=\"video/mp4\" controls>\n</video>\n</center>\n\n<caption><center> Predictions of the YOLO model on pictures taken from a camera while driving around the Silicon Valley <br> Thanks [drive.ai](https://www.drive.ai/) for providing this dataset! </center></caption>\n\n<font color='blue'>\n**What you should remember**:\n- YOLO is a state-of-the-art object detection model that is fast and accurate\n- It runs an input image through a CNN which outputs a 19x19x5x85 dimensional volume. \n- The encoding can be seen as a grid where each of the 19x19 cells contains information about 5 boxes.\n- You filter through all the boxes using non-max suppression. Specifically: \n    - Score thresholding on the probability of detecting a class to keep only accurate (high probability) boxes\n    - Intersection over Union (IoU) thresholding to eliminate overlapping boxes\n- Because training a YOLO model from randomly initialized weights is non-trivial and requires a large dataset as well as lot of computation, we used previously trained model parameters in this exercise. If you wish, you can also try fine-tuning the YOLO model with your own dataset, though this would be a fairly non-trivial exercise. \n\n**References**: The ideas presented in this notebook came primarily from the two YOLO papers. The implementation here also took significant inspiration and used many components from Allan Zelener's github repository. The pretrained weights used in this exercise came from the official YOLO website. \n- Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi - [You Only Look Once: Unified, Real-Time Object Detection](https://arxiv.org/abs/1506.02640) (2015)\n- Joseph Redmon, Ali Farhadi - [YOLO9000: Better, Faster, Stronger](https://arxiv.org/abs/1612.08242) (2016)\n- Allan Zelener - [YAD2K: Yet Another Darknet 2 Keras](https://github.com/allanzelener/YAD2K)\n- The official YOLO website (https://pjreddie.com/darknet/yolo/) \n\n**Car detection dataset**:\n<a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\" /></a><br /><span xmlns:dct=\"http://purl.org/dc/terms/\" property=\"dct:title\">The Drive.ai Sample Dataset</span> (provided by drive.ai) is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\">Creative Commons Attribution 4.0 International License</a>. We are especially grateful to Brody Huval, Chih Hu and Rahul Patel for collecting and providing this dataset. \n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 09 Dec 2021 10:10:50 GMT"
    },
    "technique": "GitHub API"
  }
}