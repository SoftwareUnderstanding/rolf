{
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/huawei-noah/Pretrained-Language-Model",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "How to Contribute\nBERT needs to maintain permanent compatibility with the pre-trained model files,\nso we do not plan to make any major changes to this library (other than what was\npromised in the README). However, we can accept small patches related to\nre-factoring and documentation. To submit contributes, there are just a few\nsmall guidelines you need to follow.\nContributor License Agreement\nContributions to this project must be accompanied by a Contributor License\nAgreement. You (or your employer) retain the copyright to your contribution;\nthis simply gives us permission to use and redistribute your contributions as\npart of the project. Head over to https://cla.developers.google.com/ to see\nyour current agreements on file or to sign a new one.\nYou generally only need to submit a CLA once, so if you've already submitted one\n(even if it was for a different project), you probably don't need to do it\nagain.\nCode reviews\nAll submissions, including submissions by project members, require review. We\nuse GitHub pull requests for this purpose. Consult\nGitHub Help for more\ninformation on using pull requests.\nCommunity Guidelines\nThis project follows\nGoogle's Open Source Community Guidelines.",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-12-02T14:26:04Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-03T06:33:16Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8883815759883056,
        0.9942892981872409,
        0.9701465476683371,
        0.9487803209564318,
        0.9580674511080016,
        0.9662932709685936,
        0.9631298128947656
      ],
      "excerpt": "This repository provides the latest pretrained language models and its related optimization techniques developed by Huawei Noah's Ark Lab. \nPanGu-\u03b1 is a Large-scale autoregressive pretrained Chinese language model with up to 200B parameter. The models are developed under the MindSpore and trained on a cluster of Ascend 910 AI processors. \nNEZHA-TensorFlow is a pretrained Chinese language model which achieves the state-of-the-art performances on several Chinese NLP tasks developed under TensorFlow. \nNEZHA-PyTorch is the PyTorch version of NEZHA. \nNEZHA-Gen-TensorFlow provides two GPT models. One is Yuefu (\u4e50\u5e9c), a Chinese Classical Poetry generation model, the other is a common Chinese GPT model. \nTinyBERT is a compressed BERT model which achieves 7.5x smaller and 9.4x faster on inference. \nTinyBERT-MindSpore is a MindSpore version of TinyBERT. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8941012850197145,
        0.9562315158071775,
        0.9775079511516038,
        0.8800643260878358,
        0.9683498993376287
      ],
      "excerpt": "PMLM is a probabilistically masked language model. Trained without the complex two-stream self-attention, PMLM can be treated as a simple approximation of XLNet. \nTernaryBERT is a weights ternarization method for BERT model developed under PyTorch. \nTernaryBERT-MindSpore is the MindSpore version of TernaryBERT. \nHyperText is an efficient text classification model based on hyperbolic geometry theories. \nBinaryBERT is a weights binarization method using ternary weight splitting for BERT model, developed under PyTorch. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Pretrained language model and its related optimization techniques developed by Huawei Noah's Ark Lab.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/huawei-noah/Pretrained-Language-Model/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 407,
      "date": "Mon, 06 Dec 2021 08:20:56 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/huawei-noah/Pretrained-Language-Model/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "huawei-noah/Pretrained-Language-Model",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/huawei-noah/Pretrained-Language-Model/master/NEZHA-TensorFlow/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/PanGu-\u03b1/docs"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/huawei-noah/Pretrained-Language-Model/master/PanGu-%CE%B1/scripts/run_distribute_predict.sh",
      "https://raw.githubusercontent.com/huawei-noah/Pretrained-Language-Model/master/PanGu-%CE%B1/scripts/run_distribute_train.sh",
      "https://raw.githubusercontent.com/huawei-noah/Pretrained-Language-Model/master/BBPE/bbpe/text2utf-8-mt-byte.sh",
      "https://raw.githubusercontent.com/huawei-noah/Pretrained-Language-Model/master/BBPE/bbpe/text2utf-8-mt-char.sh",
      "https://raw.githubusercontent.com/huawei-noah/Pretrained-Language-Model/master/NEZHA-PyTorch/run_classifier.sh",
      "https://raw.githubusercontent.com/huawei-noah/Pretrained-Language-Model/master/BinaryBERT/scripts/tws_glue.sh",
      "https://raw.githubusercontent.com/huawei-noah/Pretrained-Language-Model/master/BinaryBERT/scripts/tws_squad.sh",
      "https://raw.githubusercontent.com/huawei-noah/Pretrained-Language-Model/master/BinaryBERT/scripts/terarny_glue.sh",
      "https://raw.githubusercontent.com/huawei-noah/Pretrained-Language-Model/master/BinaryBERT/scripts/terarny_squad.sh",
      "https://raw.githubusercontent.com/huawei-noah/Pretrained-Language-Model/master/TernaryBERT-MindSpore/scripts/run_train.sh",
      "https://raw.githubusercontent.com/huawei-noah/Pretrained-Language-Model/master/TernaryBERT-MindSpore/scripts/run_eval.sh",
      "https://raw.githubusercontent.com/huawei-noah/Pretrained-Language-Model/master/NEZHA-TensorFlow/scripts/run_clf_predict.sh",
      "https://raw.githubusercontent.com/huawei-noah/Pretrained-Language-Model/master/NEZHA-TensorFlow/scripts/run_seq_labelling_predict.sh",
      "https://raw.githubusercontent.com/huawei-noah/Pretrained-Language-Model/master/NEZHA-TensorFlow/scripts/run_ner_predict.sh",
      "https://raw.githubusercontent.com/huawei-noah/Pretrained-Language-Model/master/NEZHA-TensorFlow/scripts/run_clf.sh",
      "https://raw.githubusercontent.com/huawei-noah/Pretrained-Language-Model/master/NEZHA-TensorFlow/scripts/run_pretraining.sh",
      "https://raw.githubusercontent.com/huawei-noah/Pretrained-Language-Model/master/NEZHA-TensorFlow/scripts/run_reading.sh",
      "https://raw.githubusercontent.com/huawei-noah/Pretrained-Language-Model/master/NEZHA-TensorFlow/scripts/run_seq_labelling.sh",
      "https://raw.githubusercontent.com/huawei-noah/Pretrained-Language-Model/master/TinyBERT-MindSpore/scripts/run_standalone_td.sh",
      "https://raw.githubusercontent.com/huawei-noah/Pretrained-Language-Model/master/TinyBERT-MindSpore/scripts/run_distributed_gd_ascend.sh",
      "https://raw.githubusercontent.com/huawei-noah/Pretrained-Language-Model/master/TinyBERT-MindSpore/scripts/run_standalone_gd.sh",
      "https://raw.githubusercontent.com/huawei-noah/Pretrained-Language-Model/master/TinyBERT-MindSpore/scripts/run_distributed_gd_gpu.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.93085761780587
      ],
      "excerpt": "NEZHA-PyTorch is the PyTorch version of NEZHA. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8188486291058532
      ],
      "excerpt": "DynaBERT is a dynamic BERT model with adaptive width and depth. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/huawei-noah/Pretrained-Language-Model/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell",
      "C++",
      "Dockerfile",
      "Cython"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'The MIT License (MIT)\\nCopyright (c) 2021  Huawei Technologies Co., Ltd.\\n\\nPermission is hereby granted, free of charge, to any person obtaining\\na copy of this software and associated documentation files (the\\n\"Software\"), to deal in the Software without restriction, including\\nwithout limitation the rights to use, copy, modify, merge, publish,\\ndistribute, sublicense, and/or sell copies of the Software, and to\\npermit persons to whom the Software is furnished to do so, subject to\\nthe following conditions:\\n\\nThe above copyright notice and this permission notice shall be\\nincluded in all copies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\\nLIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\\nOF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\\nWITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Pretrained Language Model",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Pretrained-Language-Model",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "huawei-noah",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/huawei-noah/Pretrained-Language-Model/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2022,
      "date": "Mon, 06 Dec 2021 08:20:56 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "pretrained-language-model",
      "knowledge-distillation",
      "model-compression",
      "quantization"
    ],
    "technique": "GitHub API"
  }
}