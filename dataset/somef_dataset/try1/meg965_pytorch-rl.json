{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1802.05957"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Playing Atari with Deep Reinforcement Learning, Mnih et al., 2013\n2. Human-level control through deep reinforcement learning, Mnih et al., 2015\n3. Deep Reinforcement Learning with Double Q-learning, van Hasselt et al., 2015\n4. Continuous control with deep reinforcement learning, Lillicrap et al., 2015\n5. CVAE-GAN: Fine-Grained Image Generation through Asymmetric Training, Bao et al., 2017\n6. beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework, Higgins et al., 2017\n7. Hindsight Experience Replay, Andrychowicz et al., 2017\n8. InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets, Chen et al., 2016\n9. World Models, Ha et al., 2018\n10. Spectral Normalization for Generative Adversarial Networks, Miyato et al., 2018\n11. Self-Attention Generative Adversarial Networks, Zhang et al., 2018\n12. Curiosity-driven Exploration by Self-supervised Prediction, Pathak et al., 2017\n13. Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor, Haarnoja et al., 2018\n14. Parameter Space Noise for Exploration, Plappert et al., 2018\n15. Noisy Network for Exploration, Fortunato et al., 2018\n16. Proximal Policy Optimization Algorithms, Schulman et al., 2017\n17. Unsupervised Real-Time Control through Variational Empowerment, Karl et al., 2017\n18. Mutual Information Neural Estimation, Belghazi et al., 2018\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8139370737453218
      ],
      "excerpt": "Montezuma's Revenge (Current Research) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9993660097487649
      ],
      "excerpt": "OpenSim Prosthetics Nips Challenge (https://www.crowdai.org/challenges/nips-2018-ai-for-prosthetics-challenge) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9258020037737749
      ],
      "excerpt": "Please refer to https://github.com/soumith/ganhacks for more information. \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/meg965/pytorch-rl",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-09-09T12:04:09Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-10-21T13:26:44Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9831387358384129,
        0.9784280462547995,
        0.9224627909670817,
        0.9303959157763716
      ],
      "excerpt": "This repository contains all standard model-free and model-based(coming) RL algorithms in Pytorch. (May also contain some research ideas I am working on currently) \npytorch-rl implements some state-of-the art deep reinforcement learning algorithms in Pytorch, especially those concerned with continuous action spaces. You can train your algorithm efficiently either on CPU or GPU. Furthermore, pytorch-rl works with OpenAI Gym out of the box. This means that evaluating and playing around with different algorithms is easy. Of course you can extend pytorch-rl according to your own needs. \nTL:DR : pytorch-rl makes it really easy to run state-of-the-art deep reinforcement learning algorithms. \nDQN (with Double Q learning) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9921290466582342
      ],
      "excerpt": "DDPG with HER (For the OpenAI Fetch Environments) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9171321382487413
      ],
      "excerpt": "For image to image translation tasks with GANs and for VAEs in general, training with Skip Connection really helps the training. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9100650895248849
      ],
      "excerpt": "Parameter Space Noise for Exploration \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/meg965/pytorch-rl/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 08 Dec 2021 01:25:20 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/meg965/pytorch-rl/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "meg965/pytorch-rl",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Install Pytorch-rl from Pypi (recommended):\n\npip install pytorch-policy\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8732101153572253
      ],
      "excerpt": "<img width=\"160px\" height=\"22px\" href=\"https://github.com/pytorch/pytorch\" src=\"https://pp.userapi.com/c847120/v847120960/82b4/xGBK9pXAkw8.jpg\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9667326416418838
      ],
      "excerpt": "PPO (https://github.com/ikostrikov/pytorch-a2c-ppo-acktr) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9849479884223464
      ],
      "excerpt": "Super Mario Bros (Follow instructions to install gym-retro https://github.com/openai/retro) \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9168531065139438,
        0.9022422665064731,
        0.891131676509874
      ],
      "excerpt": "    <td><img src=\"/assets/r_her.gif?raw=true\" width=\"200\"></td> \n    <td><img src=\"/assets/goal-3.png?raw=true\" width=\"200\"></td> \n    <td><img src=\"/assets/virtual-goal.png?raw=true\" width=\"200\"></td> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/meg965/pytorch-rl/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Deep Reinforcement Learning in Pytorch",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "pytorch-rl",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "meg965",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/meg965/pytorch-rl/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Pytorch\n2. Gym (OpenAI)\n3. mujoco-py (For the physics simulation and the robotics env in gym)\n4. Pybullet (Coming Soon)\n5. MPI (Only supported with mpi backend Pytorch installation)\n6. Tensorboardx (https://github.com/lanpa/tensorboardX)\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Wed, 08 Dec 2021 01:25:20 GMT"
    },
    "technique": "GitHub API"
  }
}