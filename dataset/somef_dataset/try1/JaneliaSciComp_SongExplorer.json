{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1802.03426",
      "https://arxiv.org/abs/1412.6980",
      "https://arxiv.org/abs/1512.03385"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "BJ Arthur, Y Ding, M Sosale, F Khalif, S Turaga, DL Stern (in prep)  \nSongExplorer: A deep learning workflow for discovery and segmentation of animal acoustic communication signals \n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9686355105364779
      ],
      "excerpt": "Please let us know where you have deposited your raw \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9507374082549614
      ],
      "excerpt": "Museo de Ciencias Naturales de Madrid's Fonoteca Zool\u00f3gica. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8179493569047226
      ],
      "excerpt": "$HOME ... on Mac. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8252017910153164
      ],
      "excerpt": "\"$HOME/songexplorer/configuration.pysh\" on Mac and Linux). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9616463396263261
      ],
      "excerpt": "top - 09:36:18 up 25 days,  1:18,  0 users,  load average: 11.40, 12.46, 12.36 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.946498592459726
      ],
      "excerpt": "%Cpu(s):  0.7 us,  0.9 sy, 87.9 ni, 10.4 id,  0.1 wa,  0.0 hi,  0.0 si,  0.0 st \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "Fri Jan 31 09:35:13 2020        \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "train_cpu_cluster_flags=\"-n 12\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8199748500934674
      ],
      "excerpt": "2020-08-09 09:30:02,377 Starting Bokeh server version 2.0.2 (running on Tornado 6.0.4) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9244029534828311
      ],
      "excerpt": "2020-08-09 09:30:02,387 Bokeh app running at: http://localhost:5006/gui \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9742177718389462,
        0.9278824608274014,
        0.8906174419333412
      ],
      "excerpt": "2020-08-09 09:30:15,054 404 GET /favicon.ico (10.60.1.47) 1.15ms \n2020-08-09 09:30:15,054 WebSocket connection opened \n2020-08-09 09:30:15,055 ServerConnection created \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9278824608274014
      ],
      "excerpt": "multi-taper harmonic analysis (Thomson, 1982; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9545822903809504
      ],
      "excerpt": "freq \u03c1, and open islands and close gaps shorter than freq smooth \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9493765143687258,
        0.955282076251007
      ],
      "excerpt": "(McInnes, Healy, and Melville (2018)), \nt-SNE (van der Maaten and Hinton \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9580284379488734
      ],
      "excerpt": "Every 2.0s: tail trained-classifier1/train_1.log      Mon Apr 22 14:37:31 2019 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.881848456479086,
        0.8628057065201727
      ],
      "excerpt": "INFO:tensorflow:Elapsed 43.414184, Step #10: accuracy 84.4%, cross entropy 0.871244 \nINFO:tensorflow:Saving to \"/home/arthurb/songexplorer/trained-classifier1/train_1k/vgg.ckpt-10\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "|10000      |12.8  |28,28  | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "| 5000      |6.4   |11,11  | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "| 6000      |10.7  |19,19  | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8407949800636363
      ],
      "excerpt": "expected durations for each entry in wanted words (e.g. 6,12,42 seconds \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8444342525991423
      ],
      "excerpt": "described above (detecting sounds, making predicions, fixing mistakes, etc) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/JaneliaSciComp/SongExplorer",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "`kernels`.  Only the second value matters when `representation` is \"waveform\".\n\n* `",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-08-28T14:02:23Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-08-25T18:14:51Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You have an audio recording, and you want to know where certain classes of\nsounds are.  SongExplorer is trained to recognize such words by manually giving\nit a few examples.  It will then automatically calculate the probability,\nover time, of when those words occur in all of your recordings.\n\nApplications suitable for SongExplorer include quantifying the rate or pattern\nof words emitted by a particular species, distinguishing a recording of one\nspecies from another, and discerning whether individuals of the same species\nproduce different song.\n\nUnderneath the hood is a deep convolutional neural network.  The input is the\nraw audio stream, and the output is a set of mutually-exclusive probability\nwaveforms corresponding to each word of interest.\n\nTraining begins by first thresholding one of your recordings in the\ntime- and frequency-domains to find sounds that exceed the ambient noise.\nThese sounds are then clustered into similar categories for you to manually\nannotate with however many word labels naturally occur.  A classifier is\nthen trained on this corpus of ground truth, and a new recording is analyzed\nby it.  The words it automatically finds are then clustered as before, but\nthis time are displayed with predicted labels.  You manually correct the\nmistakes, both re-labeling words that it got wrong, as well as labeling\nwords it missed.  These new annotations are added to the ground truth,\nand the process of retraining the classifier and analyzing and correcting\nnew recordings is repeated until the desired accuracy is reached.\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8076000359684958,
        0.9907872995391155
      ],
      "excerpt": "SongExplorer is open source and free for you to use.  However, SongExplorer is not \na static piece of software.  It\u2019s performance is improved with additional \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9887386675960254,
        0.9288388338653296
      ],
      "excerpt": "all of your primary data and annotations freely available in a recognized data \nrepository, such as figshare, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8906696039066634
      ],
      "excerpt": "already require deposition of raw data, but we strongly encourage you to also \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9576263790178838,
        0.9889474285501398
      ],
      "excerpt": "improve the performance of SongExplorer over time, helping both your own work and \nthat of everyone else. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9701910302170991,
        0.9178784397740524,
        0.9451468980676705
      ],
      "excerpt": "data and annotations by posting an issue to the SongExplorer \nrepository.  We will endeavor to \nmaintain a database of these recordings and annotations and will periodically \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8107542652825155,
        0.9601005837266672
      ],
      "excerpt": "In addition, consider donating your recordings to library or museum, like the \nCornell Lab of Ornithology's Macauley Library or the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.952027753420077,
        0.8570356281807676,
        0.8545986309353063
      ],
      "excerpt": "Throughout this document Buttons and variables in the SongExplorer graphical \nuser interface (GUI) as well as code are highlighted with backticks.  Files and \npaths are enclosed in double quotes (\"...\").  The dollar sign ($) in code \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8353941967918309
      ],
      "excerpt": "Linux and 3.3 Desktop Beta on \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9389608598299097
      ],
      "excerpt": "Data integrity checked, authentic and signed by: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8680244823752565
      ],
      "excerpt": "On Mac singularity runs within a virtual machine that is configured by default \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9465472451427016
      ],
      "excerpt": "--vm-ram flags to allocate a different amount of system resources to SongExplorer \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8713722031158514
      ],
      "excerpt": "SongExplorer is idle these resources will not be available to other programs, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8804114459223609
      ],
      "excerpt": "In System Configuration we'll make a copy of the default \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8181823906292877
      ],
      "excerpt": "Status: Image is up to date for bjarthur/songexplorer:latest \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9875296254508624
      ],
      "excerpt": "The equivalent on Mac and Linux is to put these definitions in your .bashrc (or \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8935006740448893
      ],
      "excerpt": "In System Configuration we'll make a copy of the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8431976145897639
      ],
      "excerpt": "\"$HOME/songexplorer/configuration.pysh\" on Mac and Linux). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8332751344857677,
        0.8599720413013053,
        0.807268461360199,
        0.8713722031158514
      ],
      "excerpt": "On Windows and Mac docker runs within a virtual machine that is configured by \ndefault to only use half the available CPU cores and half of the memory.  This \nconfiguration can be changed in the Preferences window.  Note that even when \nSongExplorer is idle these resources will not be available to other programs, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9674544912526079
      ],
      "excerpt": "SongExplorer is capable of training a classifier and making predictions on \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9003896348905265
      ],
      "excerpt": "Copy the exemplar configuration file out of the container and into your home \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9744373186435858,
        0.8935403577276282,
        0.8189457964986174,
        0.869714771213034
      ],
      "excerpt": "according to these _where variables.  SongExplorer is shipped with each set to \n\"local\" via the default_where variable at the top of the configuration file. \nThis value instructs SongExplorer to perform the task on the same machine as used \nfor the GUI.  You can change which computer is used to do the actual work \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8742471133228955,
        0.9139529948820061,
        0.8788447085573876
      ],
      "excerpt": "ones later in the file.  Other valid values for these variables are \"server\" \nfor a remote workstation that you can ssh into, and \"cluster\" for an \non-premise Beowulf-style cluster with a job scheduler. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9320711532102574
      ],
      "excerpt": "at once, as well as queueing a bunch of jobs for offline analysis.  By default, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9714124512705482
      ],
      "excerpt": "Depending on the size of your model, the resources you have access to and their \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8094453673139271
      ],
      "excerpt": "the number of CPU cores, number of GPU cards, and number of gigabytes of memory \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9102176864415668
      ],
      "excerpt": "Training the model in the Tutorial below, for example, only needs \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9859802511781088
      ],
      "excerpt": "train_gpu to 0 and train_cpu_{ncpu_cores,ngpu_cards,ngigabytes_memory} to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9707570609950922,
        0.858722761218456
      ],
      "excerpt": "train_gpu_{ncpu_cores,ngpu_cards,ngigabytes_memory} to 2, 1, and 1.  As it \nhappens, for the network architecture used in the tutorial, training is quicker \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9130886377916893
      ],
      "excerpt": "Note that these settings don't actually limit the job to that amount of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8801309691680292
      ],
      "excerpt": "It is important not to overburden you computer with tasks, so don't \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9935616180355716
      ],
      "excerpt": "appears, is currently using 131.2% of a CPU core (e.g. 1.3 cores), and 6.9% of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9759592732677009
      ],
      "excerpt": "python3 command as above is currently using 4946 MiB of GPU memory and 67% of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8341783601525633
      ],
      "excerpt": "Using a lab or departmental server, or perhaps a colleague's workstation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8000356077704365
      ],
      "excerpt": "in your own personal workstation's internet browser.  To do this, simply ssh \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9176970429957072
      ],
      "excerpt": "your own personal workstation and batch compute jobs to the remote server. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8942101656583737
      ],
      "excerpt": "The advantage here is that less compute intensive jobs (e.g. freeze, accuracy) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.988891434184902
      ],
      "excerpt": "\"configuration.pysh\", and all of your data. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9278188403584505
      ],
      "excerpt": "your workstation and the server to point to this same image. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9693233393948762
      ],
      "excerpt": "~/.ssh:/ssh to SONGEXPLORER_BIN. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8026919038414807,
        0.9833726842280485,
        0.8134883112117469,
        0.8385764204564942
      ],
      "excerpt": "you'll need to do all of the compute jobs remotely. \nLastly, update \"configuration.pysh\" with the IP address of the server.  As when \ndoing compute locally, SongExplorer uses a job scheduler on the server to manage \nresources.  The per-task resources used are the same as specified for the local \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9311461065090799
      ],
      "excerpt": "Submitting jobs to a cluster is similar to using a remote workstation, so read \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8609150491048788,
        0.990649882657463
      ],
      "excerpt": "the GUI code locally or on the cluster.  With the former you have the option to \nsubmit only a portion of the compute jobs to the cluster, whereas with the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8793813250513117
      ],
      "excerpt": "cluster also requires that the cluster be configured to permit hosting a web \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8974449871818497
      ],
      "excerpt": "the GUI is sitting idle. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8085066183758035
      ],
      "excerpt": "need to be on it, and the local and remote file paths must be the same or made \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8547569766979999,
        0.8527051035058014
      ],
      "excerpt": "definitely need to specify the IP address of the head node and corresponding \njob submission command and its flags.  The best person to ask for help here is \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9116160620439275,
        0.8111765448046122
      ],
      "excerpt": "The syntax used to specify the resources required is unique to the particular \nscheduler your cluster uses and how it is configured.  SongExplorer was developed \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8403747804815376
      ],
      "excerpt": "flexibility.  Instead of specifying the cores, GPUs, and RAM needed explicitly, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9162634360447458,
        0.8922792721871159
      ],
      "excerpt": "Now that we have some data, let's extract the timestamps of some sounds from \none of these as-of-yet unannotated audio recordings. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8079502840808667
      ],
      "excerpt": "2020-08-09 09:30:02,387 Starting Bokeh server with process id: 1189 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9198734280001989
      ],
      "excerpt": "printed to the terminal.  In the output above this is \"arthurb-ws2:5006\", which \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9000586270021036
      ],
      "excerpt": "three wide rectangles underneath) in which the sound recordings are displayed and \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8503049559534936,
        0.9460499995152942
      ],
      "excerpt": "classifier and make predictions with it, as well as a file browser and a large \neditable text box with \"configuration.pysh\".  On the right is this instruction \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9379038431671967,
        0.8531340127480355
      ],
      "excerpt": "Click on the Label Sounds button and then Detect.  All of the parameters \nbelow that are not used will be greyed out and disabled.  If all of the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589382161860439
      ],
      "excerpt": "numeric parameters that control the algorithm used to find sounds:  In the time \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8157250457130123
      ],
      "excerpt": "IEEE) in the frequency domain to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9749434648002194,
        0.9246858367811831,
        0.9511509473523696
      ],
      "excerpt": "milliseconds.  Sound events are considered to be periods of time which pass \neither of these two criteria.  Quiescent intervals are similarly defined as \nthose which pass neither the time nor the frequency domain criteria using the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8561801807100011
      ],
      "excerpt": "will initially be grey to indicate that it is pending, then turn black when it \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9934470991727062
      ],
      "excerpt": "(in tics) of sounds which exceeded a threshold in either the time or frequency \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8733971594085799,
        0.8733971594085799,
        0.8733971594085799
      ],
      "excerpt": "  PS_20130625111709_ch3.wav,113872,114032,detected,frequency \n  PS_20130625111709_ch3.wav,158224,158672,detected,frequency \n  PS_20130625111709_ch3.wav,182864,182960,detected,frequency \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8582207536442291
      ],
      "excerpt": "To cluster these detected sounds we're going to use the same workflow that we'll \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8291445837175211
      ],
      "excerpt": "\"untrained-classifier\") and to find the ground-truth data.  The latter should \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8654029684603137
      ],
      "excerpt": "files (i.e. \"groundtruth-data\" in this case).  Check to make sure that the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9243085385702784,
        0.9876856805714233,
        0.8107104290169256
      ],
      "excerpt": "validation % to 0, restore from to blank, wanted words to \n\"time,frequency,neither\", and label types to \"detected\".  The rest of the \nfields, most of which specify the network architecture, are filled in with \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9534255423527092
      ],
      "excerpt": "to them, along with all of the other text fields, are saved to a file named \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9422952942515648
      ],
      "excerpt": "checkpoint files prefixed with \"vgg.ckpt-\" which save the weights of the neural \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8812244842035643
      ],
      "excerpt": "Use the Activations button to save the input to the neural network as well as \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8017274762121797
      ],
      "excerpt": "model to use by selecting the last checkpoint file in the untrained \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9251518856769281,
        0.9712511316183745
      ],
      "excerpt": "The time and amount of memory this takes depends directly on the number and \ndimensionality of detected sounds.  To limit the problem to a manageable size \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9417978294608795
      ],
      "excerpt": "the relative proportion of each wanted word to equalize ratio.  In the case \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8424551384282619,
        0.8788358434175779,
        0.9242722660923476,
        0.8944900386000768
      ],
      "excerpt": "\"activations.npz\".  The two ending in \".log\" report any errors, and the \".npz\" \nfile contains the actual data in binary format. \nNow reduce the dimensionality of the hidden state activations to either two or \nthree dimensions with the Cluster button.  Choose to do so using either UMAP \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.908925214220865
      ],
      "excerpt": "t-SNE (van der Maaten and Hinton \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9199522266176395
      ],
      "excerpt": "and perplexity and exaggeration respectively), a description of which can \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8219421025793311,
        0.8343684176783959
      ],
      "excerpt": "preceded by PCA, in which case you'll need to specify the fraction of \ncoefficients to retain using PCA fraction.  You'll also need to choose which \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9551222678446301
      ],
      "excerpt": "any errors, \"cluster.npz\" contains binary data, and \"cluster-pca.pdf\" shows the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9533418678198383,
        0.9354700519140947
      ],
      "excerpt": "Finally, click on the Visualize button to render the clusters in the \nleft-most panel.  Adjust the size and transparency of the markers using the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9556237815934608,
        0.8281047267210012,
        0.8979411005071259,
        0.984445492212963
      ],
      "excerpt": "some structure to the clusters based on just the raw data alone.  This \nstructure will become much more pronounced after a model is trained with \nannotated data. \nTo browse through your recordings, click on one of the more dense areas and a \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9193909242605213,
        0.9240760354861463,
        0.970785132089211,
        0.818524578614223
      ],
      "excerpt": "the right panel are now displayed snippets of detected waveforms which are \nwithin that circle.  The size of the circle can be adjusted with the Circle \nRadius slider and the number of snippets displayed with gui_snippet_n{x,y} \nin \"configuration.pysh\".  The snippets should exhibit some similarity to one another \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.958391925495161
      ],
      "excerpt": "\"detected time\", \"detected frequency\", or \"detected neither\" to indicate which \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8939091367809249,
        0.982524641941801,
        0.8478639679487923,
        0.945999256673144,
        0.828187506578261
      ],
      "excerpt": "annotated, predicted, or missed; see below).  The color is the scale bar-- \nyellow is loud and purple is quiet.  Clicking on a snippet will show it in \ngreater temporal context in the wide panel below.  Pan and zoom with the \nbuttons labeled with arrows.  The Play button can be used to listen to the \nsound, and if the Video button is selected and a movie with the same root \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9592346081577764
      ],
      "excerpt": "text boxes to the right of the pan and zoom controls and hit return to activate \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9369288971087972
      ],
      "excerpt": "half of the wide context window nicely demarcates the temporal extent of the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9160716162523627,
        0.8759697498484704,
        0.9205606813523595,
        0.8909316763613215,
        0.8919190401885589,
        0.9792617511297822
      ],
      "excerpt": "click-and-drag in the bottom half of the wide context window to create a custom \ntime span for a new annotation.  In all cases, annotations can be deleted by \ndouble clicking any of the gray boxes. \nFor this tutorial, choose the words \"mel-pulse\", \"mel-sine\", \"ambient\", and \n\"other\".  We use the syntax \"A-B\" here, where A is the species (mel \nbeing short for D.  melanogaster) and B is the song type.  That syntax is not \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.935032804605852
      ],
      "excerpt": "groups of words that share a common prefix or suffix. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "$ tree groundtruth-data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9503385167399577
      ],
      "excerpt": "Click on the Make Predictions button to disable the irrelevant actions and \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9540222098469157
      ],
      "excerpt": "the progress, withhold 40% of the annotations to validate on, and do so every \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8061820992326775
      ],
      "excerpt": "\"mel-pulse,mel-sine,ambient,other\", andlabel typesto \"annotated\" so that it \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.993708217858801
      ],
      "excerpt": "It is common for the accuracy, as measured on the withheld data and reported as \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8172206188009832,
        0.9392244410561913
      ],
      "excerpt": "accuracy.  If so, it is an indication that the classifier does not generalize \nwell at that point.  With more training steps and more ground-truth data though \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8022925645449177
      ],
      "excerpt": "following charts and tables in the logs folder and the train_1r subdirectory \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8695761265536285
      ],
      "excerpt": "as a function of the number of training steps, wall-clock time, and epochs. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8125678426544378,
        0.8743285480603871,
        0.8294609693767894
      ],
      "excerpt": "is placed in this two-dimensional grid according to the label it was manually \nassigned and the label it was automatically predicted to be.  For a perfect \nclassifier this matrix would be diagonal-- that is, only the fuchsia numbers in \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9218858810975307,
        0.8417330614759307,
        0.9940659428764324
      ],
      "excerpt": "right triangle in each square is the number of annotations in this square \ndivided by the number of annotations in this row.  For boxes along the diagonal \nit indicates the recall, or sensitivity, for that word, which is the percentage \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8682758391562023
      ],
      "excerpt": "Similarly in the lower left is the precision, or positive predictive value-- \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8352717023490667,
        0.9050869861122878,
        0.9939147454139655
      ],
      "excerpt": "by the sum of the corresponding column.  In the title is the overall accuracy, \nwhich is calculated as the average of the upper right numbers along the diagonal. \nIn the middle panel of \"accuracy.pdf\" is the precision and recall for each word \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9241942211100549
      ],
      "excerpt": "simply the values in the corners of the boxes along the diagonal of the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9071009223786684
      ],
      "excerpt": "The right panel of \"accuracy.pdf\" shows the overall accuracy.  The fuchsia \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9743017093682693
      ],
      "excerpt": "number of annotations for the corresponding word (equivalent to the sum of the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8466071035719023
      ],
      "excerpt": "point plotted, but in Measuring Generalization and \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9547947016891254,
        0.9442713330039919,
        0.8210173850075893
      ],
      "excerpt": "\"validation-F1.pdf\" plots the F1 score (the product divided by the sum of the \nprecision and recall, times two) over time for each of the wanted words separately. \nCheck here to make sure that the accuracy of each word has converged. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8473017729184761,
        0.908925214220865,
        0.9718150987598574,
        0.900416270869324,
        0.9138096898897341
      ],
      "excerpt": "of the precision versus recall curve over number of training steps. \n\"train_1r/precision-recall.ckpt-*.pdf\" and \n\"train_1r/sensitivity-specificity.ckpt-*.pdf\" show how the ratio of false \npositives to false negatives changes as the threshold used to call an event \nchanges.  The areas underneath these curves are widely-cited metrics of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9736580479474415
      ],
      "excerpt": "histograms of the values of the classifier's output taps across all of that \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8916157754789871,
        0.8337997795164412
      ],
      "excerpt": "the P/Rs variable to specify which ratios to include.  This file is used when \ncreating ethograms (see Making Predictions). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9331348346630592
      ],
      "excerpt": "used to look for patterns in the raw data (see Examining \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8853739237085538
      ],
      "excerpt": "At this point in the tutorial we have just trained a single model, but SongExplorer \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9584311686303577
      ],
      "excerpt": "used).  In these cases, the left panel of \"accuracy.pdf\" will show the sum of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9335856146007845
      ],
      "excerpt": "showing the mean and standard deviation of the overall accuracy across all \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8809272920412858,
        0.9576974456928374
      ],
      "excerpt": "replicate, model, or fold. \nFor the next round of manual annotations, we're going to have this newly \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8954631870191389,
        0.8345280085230383
      ],
      "excerpt": "we're going to do so with a different recording so that the classifier learns \nto be insensitive to experimental conditions. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "$ $SONGEXPLORER_BIN cp /opt/songexplorer/data/20161207T102314_ch1.wav \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9593968596540993
      ],
      "excerpt": "and weight parameters into the single file that TensorFlow needs for inference. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.912684886504689
      ],
      "excerpt": "before when saving the activations (i.e. one of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8458219226440159,
        0.8643836038300096
      ],
      "excerpt": "containing the binary data.  This latter PB file can in future be chosen as the \nmodel instead of a checkpoint file. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9108442055821112
      ],
      "excerpt": "ending in \".tf\", and then converted to binary WAV files for compact storage and \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "$ ls groundtruth-data/round2/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.960469222050674,
        0.8901651415311309
      ],
      "excerpt": "Discretize these probabilities using thresholds based on a set of \nprecision-recall ratios using the Ethogram button.  Choose one of the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9002915038509116
      ],
      "excerpt": "Browser.  These are created by the Accuracy button and controlled by the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9713499023232618
      ],
      "excerpt": "detected sounds in the time and frequency domains as well as when we manually \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9365286691882057,
        0.833171072395208
      ],
      "excerpt": "In the preceding section we generated a set of predicted sounds by \napplying word-specific thresholds to the probability waveforms: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8855643034302105
      ],
      "excerpt": "A precision-recall ratio of one means these two types of errors occur at \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8694337031620168
      ],
      "excerpt": "Let's manually check whether our classifier in hand accurately calls sounds \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8916025630059118,
        0.8434260420799253,
        0.8278573331123582,
        0.8645848137608682,
        0.8445172409553738
      ],
      "excerpt": "and visualize the neural network's hidden state activations as we did before \nusing the Activations, Cluster, and Visualize buttons.  This time though \nchoose to cluster just the last hidden layer.  So that words with few samples \nare not obscured by those with many, randomly subsample the latter by setting \nequalize ratio to a small integer when saving the hidden state activations. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9388160591610601
      ],
      "excerpt": "part of the density map.  Optionally adjust the dot size, dot alpha, and \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8395681565888081
      ],
      "excerpt": "from the species and word pull-down menus and correct any mistakes, and \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9557678991227261
      ],
      "excerpt": "Keep in mind that the only words which show up in the clusters are those that \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9195030911555914,
        0.8746070252760119
      ],
      "excerpt": "them is to click on random snippets and look in the surrounding context in \nthe window below for sounds that have not been predicted.  This only works \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8952030272340994,
        0.8276797478267495
      ],
      "excerpt": "that you have to scan through the recording.  A better way is to directly \nhome in on detected sounds that don't exceed the probability threshold. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9846701206955925
      ],
      "excerpt": "Detect button as before, and create a list of the subset of these sounds which \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8002713620079881
      ],
      "excerpt": "Now visualize the hidden state activations--  Double check that the label \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8018194727870678,
        0.9859076006267984
      ],
      "excerpt": "Examine the false negatives by selecting missed in the kind pull-down menu \nand click on a dense cluster.  Were the classifier perfect, none of the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9173076095444537
      ],
      "excerpt": "earlier.  Annotate any of them that are, and add new label types for sound \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.81959808105574
      ],
      "excerpt": "false negatives, using a new recording for each iteration, until mistakes \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9774900657551182,
        0.8979411005071259
      ],
      "excerpt": "these folders to conserve disk space: \n$ rm groundtruth-data/*/oldfiles* \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8945637421487866
      ],
      "excerpt": "subsequent time is not spent correcting a prediction (or lack thereof) that \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9489325799011793
      ],
      "excerpt": "smaller percentage of them for validation and proportionately larger number of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9571613740800301
      ],
      "excerpt": "confirm that the learning curves converge, and a hundred-ish suffice to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8742483704145373
      ],
      "excerpt": "the labels) divided by the mini-batch size, and check that it actually \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9575423355799106
      ],
      "excerpt": "As the wall-clock time spent training is generally shorter with larger \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9779660337894422
      ],
      "excerpt": "of 32 are generally faster.  The caveat here is that exceedingly large \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9189678113298223
      ],
      "excerpt": "Once a qualitatively acceptable number of errors in the ethograms is \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9050905365934109
      ],
      "excerpt": "by leaving entire recordings out for validation (see Measuring \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8715424363901201
      ],
      "excerpt": "maximize the accuracy by fine tuning the hyperparameters (see Searching \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9238445822711414
      ],
      "excerpt": "with nearly all of your annotations for use in your experiments.  Optionally, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8705577499029403,
        0.8673252157588208,
        0.8927212334947513
      ],
      "excerpt": "and initial weights to measure the variance.  These replicate models \ncan also be combined into an ensemble model with even greater accuracy. \nFinally, report accuracy on an entirely separate set of densely-annotated \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8706661149438861
      ],
      "excerpt": "If a mistake is made annotating, say the wrong label is applied to a \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8013703373740471,
        0.9249485549250857
      ],
      "excerpt": "button to correct it. \nSometimes though, mistakes might slip into the ground truth and a model is \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.901484346770455,
        0.9150209391572174,
        0.8598251921728456,
        0.8582584663896994
      ],
      "excerpt": "to correcting false positives and false negatives.  Simply cluster the hidden \nstate activations using the Activations, Cluster, and Visualize buttons \nas before making sure that \"annotated\" is in label types.  Then click on \nannotated in the kind pull-down menu and select one of your labels (e.g. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9102425651791947
      ],
      "excerpt": "removed, and in the former case a new entry is created in the current \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9296098438437146
      ],
      "excerpt": "Up to this point we have validated on a small portion of each recording.  Once \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9262939902684938,
        0.9249513928020187,
        0.907760775857412,
        0.821526449098271
      ],
      "excerpt": "files to validate on.  In this way we measure the classifier's ability to \nextrapolate to different microphones, individuals, or whatever other \ncharacteristics that are unique to the withheld recordings. \nTo train one classifier with a single recording or set of recordings withheld \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.81605809224857
      ],
      "excerpt": "To train multiple classifiers, each of which withholds a single recording in a \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9904223800643563
      ],
      "excerpt": "separate files and subdirectories that are suffixed with the letter \"w\".  Of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8369077476825129,
        0.8022069877198975
      ],
      "excerpt": "instead of sequentially.  If your model is small, you might be able to fit \nmultiple on a single GPU (see the models_per_job variable in \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8996422437903528,
        0.9163418190836254,
        0.8985415876824587
      ],
      "excerpt": "access to a cluster, or patience. \nA simple jitter plot of the accuracies on withheld recordings is included in \nthe output of the Accuracy button (right panel of \"accuracy.pdf\").  It will \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8822314074494568
      ],
      "excerpt": "label more data, or try modifying the hyperparameters (Searching \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.981704942107853,
        0.8096582333917539,
        0.8527392409590925
      ],
      "excerpt": "Achieving high accuracy is not just about annotating lots of data, it also \ndepends on choosing the right model architecture.  SongExplorer by default \nuses convolutional neural networks, and there are many free parameters by \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9225285542757086
      ],
      "excerpt": "Customizing Architectures describes how to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9377117909497258
      ],
      "excerpt": "context is the temporal duration, in milliseconds, that the classifier \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9944050922877252,
        0.9199557787332853,
        0.8726177728510855,
        0.8678785903896334
      ],
      "excerpt": "shift by is the asymmetry, in milliseconds, of context with respect to the \npoint in time that is annotated or being classified.  shift by divided by \nstride (see below) should be an integer.  For positive values the duration of \nthe context preceding the annotation is longer than that succeeding it. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9679413003937085,
        0.963645183209874
      ],
      "excerpt": "a spectrogram of the waveform to input to the neural network, or to use a \nmel-frequency cepstrum (see Davis and Mermelstein 1980; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8773552508751794,
        0.8700664381883702
      ],
      "excerpt": "Waveforms do not make any assumptions about the data, and so can learn \narbitrary features that spectrograms and cepstrums might be blind to, but need \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9892916848466896
      ],
      "excerpt": "window is the length of the temporal slices, in milliseconds, that \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8747750374321248,
        0.9430914131501338
      ],
      "excerpt": "down to a power of two. \nstride is the time, in milliseconds, by which the windows in the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9151727430648375,
        0.9053485498172533,
        0.9147238636795958
      ],
      "excerpt": "mel &amp; DCT specifies how many taps to use in the mel-frequency cepstrum.  The \nfirst number is for the mel-frequency resampling and the second for the \ndiscrete cosine transform.  Modifying these is tricky as valid values depend on \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9626470925934045,
        0.9619196237802404
      ],
      "excerpt": "values for each, and are what is recommended.  See the code in \n\"tensorflow/contrib/lite/kernels/internal/mfcc.cc\" for more details. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8465480819865774
      ],
      "excerpt": "learning rate specifies the fraction of the gradient to change each weight \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9669036919871342,
        0.9531617097063615,
        0.9681026165798527,
        0.9568399439394193
      ],
      "excerpt": "The above apply to all architectures.  Specific to convolutional networks are: \nkernels is a 2-vector of the size of the convolutional kernels.  The first \nvalue is the size of the square 2D convolutions that are successively used \nfor each layer until the tensor height in the frequency axis is smaller than \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.885602345848907,
        0.9361604594876105
      ],
      "excerpt": "The accuracy statistics reported in the confusion matrices described above are \nlimited to the points in time which are annotated.  If an annotation withheld \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9038812503438913,
        0.9657885934208426,
        0.9239163860118926,
        0.8765590085632786,
        0.8475157484202286,
        0.9169139015661967,
        0.9508141488877299,
        0.8347108915864634,
        0.9518563462255624,
        0.8463715980243485,
        0.8109885546151069,
        0.8846825367771862,
        0.9294781892304942,
        0.9718864732926936
      ],
      "excerpt": "at the corresponding label, it is considered an error.  Quantifying accuracy in \nthis way is a bit misleading, as when a model is used to make ethograms, a \nword-specific threshold is applied to the probabilities instead.  Moreover, \nethograms are made over the entire recording, not just at specific times of \ninterest.  To more precisely quantify a model's accuracy then, as it would be \nused in your experiments, a dense annotation is needed-- one for which all \noccurrences of any words of interest are annotated. \nTo quantify an ethogram's accuracy, first select a set of recordings in your \nvalidation data that are collectively long enough to capture the variance in \nyour data set but short enough that you are willing to manually label every \nword in them.  Then detect and cluster the sounds in these recordings using \nthe Detect, Activations, Cluster, and Visualize buttons as described \nearlier.  Annotate every occurrence of each word of interest by jumping to the \nbeginning of each recording and panning all the way to the end.  Afterwards, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9274728291191293
      ],
      "excerpt": "best model to date and make ethograms of these densely annotated recordings \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9127548499589059
      ],
      "excerpt": "Congruence button to plot the fraction of false positives and negatives, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.978599325108974
      ],
      "excerpt": "is not acceptable, iteratively adjust the hyperparameters and/or add new \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9877399636067646,
        0.9560825680730863
      ],
      "excerpt": "and congruence plots until it is. \nOnce the accuracy is acceptable on validation data, quantify the accuracy on a \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.930278872876285,
        0.815087746479625,
        0.8782716308116222
      ],
      "excerpt": "validated on these latter data before; otherwise the resulting accuracy could \nbe erroneously better.  Label every word of interest as before, make ethograms \nwith your best model, and plot the congruence with SongExplorer's predictions. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9667915564935067
      ],
      "excerpt": "hyperparameters or add more training data, then the proper thing to do is to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8818098897888512
      ],
      "excerpt": "annotate a new set of data to test against. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8483822314621091
      ],
      "excerpt": "by all annotators (including SongExplorer), only each annotator, and not by a given \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9522142340891737,
        0.9616678139083469
      ],
      "excerpt": "Much as one can examine the mistakes of a particular model with respect to \nsparsely annotated ground truth by clustering with \"mistaken\" as one of the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8172057216311108
      ],
      "excerpt": "\"everyone|{tic,word}-{only,not}{1.0pr,annotator1,annotator2,...}\" as the label \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8582209629411326
      ],
      "excerpt": "\"disjoint-everyone.csv\" contains the intersection of intervals that SongExplorer \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.829861701947929,
        0.9602616645984672,
        0.9453235777052493
      ],
      "excerpt": "Activations, Cluster, and Visualize buttons as before. \nWhen the overall accuracy as judged by, for example, F1 is acceptable, but \nthere is an unacceptable balance between false postives and false negatives, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9390874176221768,
        0.9838893398437538
      ],
      "excerpt": "uneven distributions in the prevalence of the words.  So for a given interval \nof time, enter into prevalences a comma-separated list of a priori \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9351532198018447
      ],
      "excerpt": "for a minute of mel-pulse,mel-sine,ambient respectively); alternatively, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9700202098200944
      ],
      "excerpt": "to more common ones, and vice versa, thereby adjusting the precision-to-recall \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9784030984372291
      ],
      "excerpt": "The alternative to adusting probabilities is to adjust thresholds. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8862619143839545
      ],
      "excerpt": "way changes them equally for all words.  A word-specific re-balancing of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8498321001027922,
        0.9557488117317469,
        0.9446319056436215,
        0.8616817239045736,
        0.9463474718602215
      ],
      "excerpt": "file that was created when measuring congruence on the validation data set, \ninstead of the sparse ones created with Accuracy, when making ethograms \non the test data set.  In effect, this method measures the prevalence of \neach word, which is possible given a dense annotation, and adjusts the \nthresholds to achieve the desired precision-to-recall ratio in P/Rs. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9140822903801882
      ],
      "excerpt": "so that they are not overwritten by successive runs of Congruence. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9629612633937559,
        0.969796777279884,
        0.800004058448151
      ],
      "excerpt": "has manually annotated all of the types of words that exist in the recordings. \nOne way to check for any missed types is to look for hot spots in the clusters \nof detected sounds that have no corresponding annotations.  Annotating known \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9393907102289635,
        0.8080561331097604,
        0.8045340604935054,
        0.9077085740494368
      ],
      "excerpt": "those of the manually annotated sounds, using the Activations button by \nsetting the label types to \"annotated,detected\".  Set equalize ratio to \na small integer as well, so that words with few samples are not obscured \nby those with many.  Cluster and visualize as before.  Now rapidly and \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8353130759726746
      ],
      "excerpt": "menu to find any differences in the density distributions.  Click on any new \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8198876793005243
      ],
      "excerpt": "labeled as detected but not annotated.  Create new word types as necessary. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9114880387574513
      ],
      "excerpt": "be performed repeatedly.  To facilitate coding your analysis, SongExplorer is \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9013392785478354
      ],
      "excerpt": "documentation showing how to call it.  Here, for example, is the interface for \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9774062827000138
      ],
      "excerpt": "of recordings in different folders: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259,
        0.8979411005071259,
        0.8979411005071259
      ],
      "excerpt": "           groundtruth-data/round1/PS_20130625111709_ch3.wav \n           groundtruth-data/round2/20161207T102314_ch1.wav \n           groundtruth-data/round3/Antigua_20110313095210_ch26.wav \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8098939632554305
      ],
      "excerpt": "$ for wavfile in ${wavfiles[@]} ; do \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259,
        0.8979411005071259,
        0.8979411005071259
      ],
      "excerpt": "                 \"groundtruth-data/round1/PS_20130625111709_ch3\", \n                 \"groundtruth-data/round2/20161207T102314_ch1\", \n                 \"groundtruth-data/round3/Antigua_20110313095210_ch26\", \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9560187895509076
      ],
      "excerpt": "for wavpath_noext in wavepaths_noext: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.881623272712357
      ],
      "excerpt": "The default network architecture is a set of layered convolutions, the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.962242253529862
      ],
      "excerpt": "also comes with a recurrent network as an alternative.  To plug this in, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8295961474409966,
        0.8087850450217333
      ],
      "excerpt": "immediately above the configuration textbox in the GUI will change to reflect \nthe different hyperparameters used by this architecture.  All the workflows \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8044789483523294
      ],
      "excerpt": "can be used with recurrent networks in an identical manner. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8330400493338377
      ],
      "excerpt": "values to appear in the GUI, and (2) a function create_model which \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.961040590527039
      ],
      "excerpt": "Sometimes using control-C to quit out of SongExplorer does not work.  In this \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8061382814612357
      ],
      "excerpt": "To build an image, change to a local (i.e. not NFS mounted; e.g. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9050890103909155
      ],
      "excerpt": "To confirm that the image works: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8609806019195525
      ],
      "excerpt": "Next create an access token at cloud.sylabs.io and login using: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8033970160321426
      ],
      "excerpt": "Then push the image to the cloud: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8620647622153358
      ],
      "excerpt": "To use a copy of the SongExplorer source code outside of the container, set \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8181584437597097
      ],
      "excerpt": "To start docker on Linux and set permissions: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9044374535371251
      ],
      "excerpt": "To build a docker image and push it to docker hub: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9237717418904128
      ],
      "excerpt": "SongExplorer comes with a comprehensive set of tests to facilitate easy validation \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9366633434773048
      ],
      "excerpt": "or with docker: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "deep learning for acoustic signals",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/JaneliaSciComp/SongExplorer/releases",
    "technique": "GitHub API"
  },
  "faq": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Irrespective of where you want to perform your compute, there are additional\nvariables that need to be tailored to your specific resources.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Mistakes can possibly be corrected if more annotations are made of similar\nsounds.  To find such sounds, cluster the errors made on the ground-truth\nannotations with sounds detected in your recordings.  Then look for\nlocalized hot spots of mistakes and make annotations therein.\n\nSongExplorer provides two ways to generate lists of errors, which you'll need to\nchoose between.  The `Accuracy` button does so just for the validation data,\nwhile `Activations` uses the entire ground truth or a randomly sampled subset\nthereof.\n\nAs [mentioned earlier](#quantifying-accuracy), the `Accuracy` button creates a\n\"predictions/\" folder in the Log Folder containing CSV files itemizing whether\nthe sounds in the validation set were correctly or incorrectly classified.\nEach CSV file corresponds to a sub-folder within the ground-truth folder.  The\nfile format is similar to SongExplorer's other CSV files, with the difference being\nthat the penultimate column is the prediction and the final one the annotation.\nTo use these predictions, copy these CSV files into their corresponding\nground-truth sub-folders.\n\n    $ tail -n 10 trained-classifier1/predictions/round1-mistakes.csv \n    PS_20130625111709_ch3.wav,377778,377778,correct,mel-pulse,mel-pulse\n    PS_20130625111709_ch3.wav,157257,157257,correct,mel-pulse,mel-pulse\n    PS_20130625111709_ch3.wav,164503,165339,correct,ambient,ambient\n    PS_20130625111709_ch3.wav,379518,379518,mistaken,ambient,mel-pulse\n    PS_20130625111709_ch3.wav,377827,377827,correct,mel-pulse,mel-pulse\n    PS_20130625111709_ch3.wav,378085,378085,correct,mel-pulse,mel-pulse\n    PS_20130625111709_ch3.wav,379412,379412,mistaken,ambient,mel-pulse\n    PS_20130625111709_ch3.wav,160474,161353,correct,ambient,ambient\n    PS_20130625111709_ch3.wav,207780,208572,correct,mel-sine,mel-sine\n    PS_20130625111709_ch3.wav,157630,157630,correct,mel-pulse,mel-pulse\n\nSimilarly, the `Activations` button creates an \"activations.npz\" file\ncontaining the logits of the output layer (which is just a vector of word\nprobabilities), as well as the correct answer from the ground-truth\nannotations.  To turn these data into a CSV file, use the `Mistakes` button.\nIn the ground-truth subfolders, CSV files are created for each WAV file, with\nan extra column just like above.  No need to copy any files here.\n\nNow detect sounds in the ground-truth recordings for which you haven't done so\nalready.  Press the `Examine Errors` wizard and confirm that `label types` is\nset to \"detected,mistaken\", and save the hidden state activations, cluster, and\nvisualize as before.  Select `mistaken` in the `kind` pull-down menu to look\nfor a localized density.  View the snippets in any hot spots to examine the\nshapes of waveforms that are mis-classified-- the ones whose text label, which\nis the prediction, does not match the waveform.  Then select `detected` in the\n`kind` pull-down menu and manually annotate similar waveforms.  In principle they\nshould cluster at the same location.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "    run([\"hetero\", \"start\", str(M.local_ncpu_cores),\n         str(M.local_ngpu_cards), str(M.local_ngigabytes_memory)])\n\n    ",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "    run([\"hetero\", \"stop\"], stdout=PIPE, stderr=STDOUT)\n\nFor more details see the system tests in /opt/songexplorer/test/tutorial.{sh,py}.\nThese two files implement, as Bash and Python scripts respectively, the entire\nworkflow presented in this [Tutorial](#tutorial), from [Detecting\nSounds](#detecting-sounds) all the way to [Testing Densely](#testing-densely).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "The code is hosted on [github](https://github.com/JaneliaSciComp/SongExplorer).\nPlease file an issue there for all bug reports and feature requests.\nPull requests are also welcomed!  For major changes it is best to file an\nissue first so we can discuss implementation details.  Please work with us\nto improve SongExplorer instead instead of forking your own version.\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 4,
      "date": "Sun, 12 Dec 2021 17:51:04 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/JaneliaSciComp/SongExplorer/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "JaneliaSciComp/SongExplorer",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/JaneliaSciComp/SongExplorer/master/test/tutorial.sh",
      "https://raw.githubusercontent.com/JaneliaSciComp/SongExplorer/master/test/shiftby.sh",
      "https://raw.githubusercontent.com/JaneliaSciComp/SongExplorer/master/test/runtests.sh",
      "https://raw.githubusercontent.com/JaneliaSciComp/SongExplorer/master/src/train.sh",
      "https://raw.githubusercontent.com/JaneliaSciComp/SongExplorer/master/src/ethogram.sh",
      "https://raw.githubusercontent.com/JaneliaSciComp/SongExplorer/master/src/cluster.sh",
      "https://raw.githubusercontent.com/JaneliaSciComp/SongExplorer/master/src/classify1.sh",
      "https://raw.githubusercontent.com/JaneliaSciComp/SongExplorer/master/src/freeze.sh",
      "https://raw.githubusercontent.com/JaneliaSciComp/SongExplorer/master/src/congruence.sh",
      "https://raw.githubusercontent.com/JaneliaSciComp/SongExplorer/master/src/mistakes.sh",
      "https://raw.githubusercontent.com/JaneliaSciComp/SongExplorer/master/src/detect.sh",
      "https://raw.githubusercontent.com/JaneliaSciComp/SongExplorer/master/src/activations.sh",
      "https://raw.githubusercontent.com/JaneliaSciComp/SongExplorer/master/src/generalize.sh",
      "https://raw.githubusercontent.com/JaneliaSciComp/SongExplorer/master/src/gui.sh",
      "https://raw.githubusercontent.com/JaneliaSciComp/SongExplorer/master/src/compare.sh",
      "https://raw.githubusercontent.com/JaneliaSciComp/SongExplorer/master/src/accuracy.sh",
      "https://raw.githubusercontent.com/JaneliaSciComp/SongExplorer/master/src/misses.sh",
      "https://raw.githubusercontent.com/JaneliaSciComp/SongExplorer/master/src/xvalidate.sh",
      "https://raw.githubusercontent.com/JaneliaSciComp/SongExplorer/master/src/classify2.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "    V.time_sigma_string.value = \"4,2\"\n    V.time_smooth_ms_string.value = \"6.4\"\n    V.frequency_n_ms_string.value = \"25.6\"\n    V.frequency_nw_string.value = \"4\"\n    V.frequency_p_string.value = \"0.1,1.0\"\n    V.frequency_smooth_ms_string.value = \"25.6\"\n\n    ",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "SongExplorer can be run on all three major platforms.  The installation\nprocedure is different on each due to various support of the technologies used.\nWe recommend using Singularity on Linux, and Docker on Microsoft Windows and\nApple Macintosh.  Training your own classifier is fastest with an Nvidia\ngraphics processing unit (GPU).\n\nTensorFlow, the machine learning framework from Google that SongExplorer uses,\nsupports Ubuntu, Windows and Mac.  The catch is that Nvidia (and hence\nTensorFlow) currently doesn't support GPUs on Macs.  So while using a\npre-trained classifier would be fine on a Mac, because inference is just as\nfast on the CPU, training your own would take longer.\n\nDocker, a popular container framework which provides an easy way to deploy\nsoftware across platforms, supports Linux, Windows and Mac, but only supports\nGPUs on Linux.  Moreover, on Windows and Mac it runs within a heavy-weight\nvirtual machine, and on all platforms it requires administrator privileges to\nboth install and run.\n\nSingularity is an alternative to Docker that does not require root access.\nFor this reason it is required in certain high-performance computing\n(HPC) environments.  Currently it only natively supports Linux.  There is\na version for Macs which uses a light-weight virtual machine, but it is\nnot being actively developed anymore.  You can run Singularity on Windows\nwithin a virtual environment, like Docker does, but would have to set that\nup yourself.  As with Docker, GPUs are only accessible on Linux.\n\nTo use SongExplorer with a GPU on Windows one must install it manually, without\nthe convenience of a container.  We're looking for volunteers to write a\nConda recipe to make this easy.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8237734233098731
      ],
      "excerpt": "provide your manual annotations.  These manual annotations will serve to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8776882666588746
      ],
      "excerpt": "snippets signifies your computer terminal's command line.  Square brackets \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9275237407623528,
        0.9676613155797643
      ],
      "excerpt": "Platform-specific installation instructions can be found at \nSylabs.  SongExplorer has been tested with version 3.4 on \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9756979301587994
      ],
      "excerpt": "On Linux you'll also need to install the CUDA and CUDNN drivers from \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8615232758445557
      ],
      "excerpt": "click the Download button, or equivalently use the command line (for which you \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9275237407623528
      ],
      "excerpt": "Platform-specific installation instructions can be found at \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.909014932719211
      ],
      "excerpt": "Prompt on Windows, or the Terminal on Mac, and download the SongExplorer image \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8598128818464291
      ],
      "excerpt": "    -e SONGEXPLORER_BIN -h=`hostname` -p 5006:5006 ^ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8598128818464291
      ],
      "excerpt": "    -e SONGEXPLORER_BIN -h=`hostname` -p 5006:5006 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8360497399908176
      ],
      "excerpt": "userid.  Optionally specify the current working directory with the -w flag. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8601049357862168
      ],
      "excerpt": "definition (e.g.  \"%HOMEPATH%/songexplorer/configuration.pysh\" on Windows, or \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.954077411669322
      ],
      "excerpt": "$ $SONGEXPLORER_BIN cp /opt/songexplorer/configuration.pysh $PWD [%CD% on Windows] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8229648293305742,
        0.8462673175339311
      ],
      "excerpt": "Note that \"configuration.pysh\" must be a valid Python and Bash file.  Hence \nthe unusual \".pysh\" extension. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.859624048879049
      ],
      "excerpt": "SongExplorer provides the option to use a GPU or not with the train_gpu variable. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8811859833571627
      ],
      "excerpt": "train_{cpu,gpu}_{ncpu_cores,ngpu_cards,ngigabytes_memory} variables specify \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8842009198251607
      ],
      "excerpt": "without a GPU. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9509086941960421
      ],
      "excerpt": "underestimate the resources required, particularly memory consumption.  To make \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9404038412903858
      ],
      "excerpt": "Use the nvidia-smi command to similarly monitor the GPU card.  The same \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9632776838059387
      ],
      "excerpt": "the GPU cores.  Use the watch command to receive repeated updates (i.e. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.94819897242091
      ],
      "excerpt": "| NVIDIA-SMI 418.39       Driver Version: 418.39       CUDA Version: 10.1     | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9508262406627328
      ],
      "excerpt": "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.869707581244736,
        0.8526712850914951
      ],
      "excerpt": "| Processes:                                                       GPU Memory | \n|  GPU       PID   Type   Process name                             Usage      | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9429958235782064
      ],
      "excerpt": "Set the SONGEXPLORER environment variable plus the songexplorer alias on both \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8249743729235027
      ],
      "excerpt": "cluster also requires that the cluster be configured to permit hosting a web \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356887267026173
      ],
      "excerpt": "train_gpu_cluster_flags=\"-n 2 -gpu 'num=1' -q gpu_rtx\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8220312497325823
      ],
      "excerpt": "printed to the terminal.  In the output above this is \"arthurb-ws2:5006\", which \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8850310845344855
      ],
      "excerpt": "\"configuration.pysh\".  Now press DoIt!.  Output into the log directory are \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8961189632779787
      ],
      "excerpt": "fields.  Now train a classifier on your annotations using the Train button. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.922803799848847
      ],
      "excerpt": "thresholds that one can use to achieve a specified precision-recall ratio.  Use \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9196757100527265
      ],
      "excerpt": "        $PWD/groundtruth-data/round2 [%CD%/... on Windows] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8327338173971698
      ],
      "excerpt": "precision-recall ratios using the Ethogram button.  Choose one of the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8258063777324657
      ],
      "excerpt": "\"ckpt-*\".  You'll also need to specify which \".tf\" files to threshold using the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.859341529160677
      ],
      "excerpt": "set you specify, click on Omit One.  Select the set as described above for \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8183409162015873
      ],
      "excerpt": "Take care to choose the correct one. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8852982402038112
      ],
      "excerpt": "The following Bash code directly calls this script to make predictions on a set \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8728115396640954
      ],
      "excerpt": "The above workflow could also easily be performed in Julia, Python, Matlab, or \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9006238303602758
      ],
      "excerpt": "One can also supply your own tensorflow code that implements a \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9893272198983933,
        0.8469871890121433,
        0.8064061697085201
      ],
      "excerpt": "$ git clone https://github.com/JaneliaSciComp/SongExplorer.git \n$ rm -rf songexplorer/.git \n$ sudo singularity build -s songexplorer.img songexplorer/containers/singularity.def \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9198954622973495
      ],
      "excerpt": "$ sudo singularity build songexplorer.sif songexplorer.img \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8634549214084315
      ],
      "excerpt": "To build an image without GPU support, comment out the section titled \"install \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8669815626013222
      ],
      "excerpt": "your shell environment.  source_path in \"configuration.pysh\" must be set \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9906248903846466
      ],
      "excerpt": "$ cd songexplorer \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8863969828213569
      ],
      "excerpt": "well as the Linux Bash interfaces.  To run them, simply execute \"runtests.sh\": \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8056565635651478
      ],
      "excerpt": "INFO:    Container is signed \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.853492186136904
      ],
      "excerpt": "total 16G \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8021772751119439
      ],
      "excerpt": "docker run ^ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8021772751119439
      ],
      "excerpt": "docker run ^ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8021772751119439
      ],
      "excerpt": "alias songexplorer=\"docker run \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8052060870882319
      ],
      "excerpt": "6a26ad9d005e bjarthur/songexplorer \"detect.sh /src/p...\" 3 seconds ago Up 2 seconds ... \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8326721871928946
      ],
      "excerpt": "default settings for training a model locally: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8391842792310684
      ],
      "excerpt": "Tasks: 252 total,   1 running, 247 sleeping,   0 stopped,   4 zombie \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8596642903050132,
        0.8488096674107948
      ],
      "excerpt": "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC | \n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8128699747504312
      ],
      "excerpt": "| 22%   65C    P2   150W / 250W |   4957MiB /  6083MiB |     67%      Default | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8976285591760612
      ],
      "excerpt": "|  GPU       PID   Type   Process name                             Usage      | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8170322001177293
      ],
      "excerpt": "train_gpu_cluster_flags=\"-n 2 -gpu 'num=1' -q gpu_rtx\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8285125731742469,
        0.8240369510747106,
        0.8149439380555297
      ],
      "excerpt": "2020-08-09 09:30:15,054 404 GET /favicon.ico (10.60.1.47) 1.15ms \n2020-08-09 09:30:15,054 WebSocket connection opened \n2020-08-09 09:30:15,055 ServerConnection created \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8646285766575962
      ],
      "excerpt": "tapers, multiply the default threshold of the F-test by the first number in \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8230578985555015
      ],
      "excerpt": "The result is a file of comma-separated values with the start and stop times \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8087026091228522
      ],
      "excerpt": "$ grep -m 3 time groundtruth-data/round1/PS_20130625111709_ch3-detected.csv \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8525447642357499
      ],
      "excerpt": "$ grep -m 3 neither groundtruth-data/round1/PS_20130625111709_ch3-detected.csv \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8102519781778202
      ],
      "excerpt": "the File Browser to choose directories in which to put the log files (e.g. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.870460878743781
      ],
      "excerpt": "point to a folder one level up in the file hierarchy from the WAV and CSV \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8211609423260836
      ],
      "excerpt": "classifier's log files with the File Browser (i.e. one of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8057002474628383
      ],
      "excerpt": "network layer to cluster.  At this point, choose just the input layer.  Output \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8261446034023257
      ],
      "excerpt": "unambiguous example of a particular word.  Type the word's name into one of the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8324846755010838
      ],
      "excerpt": "As your example set grows, you might want to monitor the training progress \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8459797258641566
      ],
      "excerpt": "INFO:tensorflow:Elapsed 45.067488, Step 10: Validation accuracy = 65.4% (N=52) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.824565051487491
      ],
      "excerpt": "\"train-overlay.pdf\" shows the same validation accuracies as in \"train-loss.pdf\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8437310993496704,
        0.8005957834916414
      ],
      "excerpt": "\"trained-classifier1/train_1r/vgg.ckpt-100.{index,meta,data*}\" in this case). \nOutput into the log files directory are \"freeze.ckpt-*.log\" and \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8595958205177362
      ],
      "excerpt": "model instead of a checkpoint file. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8057618542381687
      ],
      "excerpt": "which it can parse \"ckpt-*\".  The probabilities are first stored in a text file \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8172569145964618
      ],
      "excerpt": "20161207T102314_ch1-classify.log   20161207T102314_ch1.tf \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8748136949246123
      ],
      "excerpt": "\"thresholds.ckpt-*.csv\" files in the log files folder using the File \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8189053496645293
      ],
      "excerpt": "$ cat trained-classifier/thresholds.csv  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8049702475080454
      ],
      "excerpt": "sub-folder as they will be out of date.  You might want to occasionally delete \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8655344691050912,
        0.8344691638586283
      ],
      "excerpt": "Browser to either select (1) specific WAV file(s), (2) a text file \ncontaining a list of WAV file(s) (either comma separated or one per line), or \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8140051296070774
      ],
      "excerpt": "|10000      |6.4   |15,15  | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.877881666426073
      ],
      "excerpt": "file with the name of the annotator (e.g. \"annotated-<name>.csv\").  Take your \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8293666186518244
      ],
      "excerpt": ".wav files, a text file of .wav filenames, or a folder of .wav files; see \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8327463663160168
      ],
      "excerpt": "same procedure.  Simply create \"annotated-<name>.csv\" files for each one.  The \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8401558704798054
      ],
      "excerpt": "import os \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9090805031321133,
        0.8480243275499854,
        0.9012248701992861
      ],
      "excerpt": "import model as M \nimport view as V \nimport controller as C \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8731562459058029
      ],
      "excerpt": "    V.wavtfcsvfiles_string.value = wavpath_noext+\".wav\" \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8061591066106042
      ],
      "excerpt": "\"src/speech_commands_custom/{convolutional,recurrent}.py\" as a template. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.858294571014999
      ],
      "excerpt": "The WAV,TF,CSV Files text box, being plural, can contain multiple \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.841537269952621
      ],
      "excerpt": "SINGULARITYENV_PREPEND_PATH to the full path to SongExplorer's src directory in \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8142947937546259
      ],
      "excerpt": "To run a container interactively add \"-i --tty\". \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.882232578724405
      ],
      "excerpt": "$ docker run -v %TMP%:/opt/songexplorer/test/scratch ^ \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/JaneliaSciComp/SongExplorer/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell",
      "HTML",
      "CSS"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "BSD 3-Clause \"New\" or \"Revised\" License",
      "url": "https://api.github.com/licenses/bsd-3-clause"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'Copyright 2019 The TensorFlow Authors.  All rights reserved.\\n\\n                                 Apache License\\n                           Version 2.0, January 2004\\n                        http://www.apache.org/licenses/\\n\\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\\n\\n   1. Definitions.\\n\\n      \"License\" shall mean the terms and conditions for use, reproduction,\\n      and distribution as defined by Sections 1 through 9 of this document.\\n\\n      \"Licensor\" shall mean the copyright owner or entity authorized by\\n      the copyright owner that is granting the License.\\n\\n      \"Legal Entity\" shall mean the union of the acting entity and all\\n      other entities that control, are controlled by, or are under common\\n      control with that entity. For the purposes of this definition,\\n      \"control\" means (i) the power, direct or indirect, to cause the\\n      direction or management of such entity, whether by contract or\\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\\n      outstanding shares, or (iii) beneficial ownership of such entity.\\n\\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\\n      exercising permissions granted by this License.\\n\\n      \"Source\" form shall mean the preferred form for making modifications,\\n      including but not limited to software source code, documentation\\n      source, and configuration files.\\n\\n      \"Object\" form shall mean any form resulting from mechanical\\n      transformation or translation of a Source form, including but\\n      not limited to compiled object code, generated documentation,\\n      and conversions to other media types.\\n\\n      \"Work\" shall mean the work of authorship, whether in Source or\\n      Object form, made available under the License, as indicated by a\\n      copyright notice that is included in or attached to the work\\n      (an example is provided in the Appendix below).\\n\\n      \"Derivative Works\" shall mean any work, whether in Source or Object\\n      form, that is based on (or derived from) the Work and for which the\\n      editorial revisions, annotations, elaborations, or other modifications\\n      represent, as a whole, an original work of authorship. For the purposes\\n      of this License, Derivative Works shall not include works that remain\\n      separable from, or merely link (or bind by name) to the interfaces of,\\n      the Work and Derivative Works thereof.\\n\\n      \"Contribution\" shall mean any work of authorship, including\\n      the original version of the Work and any modifications or additions\\n      to that Work or Derivative Works thereof, that is intentionally\\n      submitted to Licensor for inclusion in the Work by the copyright owner\\n      or by an individual or Legal Entity authorized to submit on behalf of\\n      the copyright owner. For the purposes of this definition, \"submitted\"\\n      means any form of electronic, verbal, or written communication sent\\n      to the Licensor or its representatives, including but not limited to\\n      communication on electronic mailing lists, source code control systems,\\n      and issue tracking systems that are managed by, or on behalf of, the\\n      Licensor for the purpose of discussing and improving the Work, but\\n      excluding communication that is conspicuously marked or otherwise\\n      designated in writing by the copyright owner as \"Not a Contribution.\"\\n\\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\\n      on behalf of whom a Contribution has been received by Licensor and\\n      subsequently incorporated within the Work.\\n\\n   2. Grant of Copyright License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      copyright license to reproduce, prepare Derivative Works of,\\n      publicly display, publicly perform, sublicense, and distribute the\\n      Work and such Derivative Works in Source or Object form.\\n\\n   3. Grant of Patent License. Subject to the terms and conditions of\\n      this License, each Contributor hereby grants to You a perpetual,\\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\\n      (except as stated in this section) patent license to make, have made,\\n      use, offer to sell, sell, import, and otherwise transfer the Work,\\n      where such license applies only to those patent claims licensable\\n      by such Contributor that are necessarily infringed by their\\n      Contribution(s) alone or by combination of their Contribution(s)\\n      with the Work to which such Contribution(s) was submitted. If You\\n      institute patent litigation against any entity (including a\\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\\n      or a Contribution incorporated within the Work constitutes direct\\n      or contributory patent infringement, then any patent licenses\\n      granted to You under this License for that Work shall terminate\\n      as of the date such litigation is filed.\\n\\n   4. Redistribution. You may reproduce and distribute copies of the\\n      Work or Derivative Works thereof in any medium, with or without\\n      modifications, and in Source or Object form, provided that You\\n      meet the following conditions:\\n\\n      (a) You must give any other recipients of the Work or\\n          Derivative Works a copy of this License; and\\n\\n      (b) You must cause any modified files to carry prominent notices\\n          stating that You changed the files; and\\n\\n      (c) You must retain, in the Source form of any Derivative Works\\n          that You distribute, all copyright, patent, trademark, and\\n          attribution notices from the Source form of the Work,\\n          excluding those notices that do not pertain to any part of\\n          the Derivative Works; and\\n\\n      (d) If the Work includes a \"NOTICE\" text file as part of its\\n          distribution, then any Derivative Works that You distribute must\\n          include a readable copy of the attribution notices contained\\n          within such NOTICE file, excluding those notices that do not\\n          pertain to any part of the Derivative Works, in at least one\\n          of the following places: within a NOTICE text file distributed\\n          as part of the Derivative Works; within the Source form or\\n          documentation, if provided along with the Derivative Works; or,\\n          within a display generated by the Derivative Works, if and\\n          wherever such third-party notices normally appear. The contents\\n          of the NOTICE file are for informational purposes only and\\n          do not modify the License. You may add Your own attribution\\n          notices within Derivative Works that You distribute, alongside\\n          or as an addendum to the NOTICE text from the Work, provided\\n          that such additional attribution notices cannot be construed\\n          as modifying the License.\\n\\n      You may add Your own copyright statement to Your modifications and\\n      may provide additional or different license terms and conditions\\n      for use, reproduction, or distribution of Your modifications, or\\n      for any such Derivative Works as a whole, provided Your use,\\n      reproduction, and distribution of the Work otherwise complies with\\n      the conditions stated in this License.\\n\\n   5. Submission of Contributions. Unless You explicitly state otherwise,\\n      any Contribution intentionally submitted for inclusion in the Work\\n      by You to the Licensor shall be under the terms and conditions of\\n      this License, without any additional terms or conditions.\\n      Notwithstanding the above, nothing herein shall supersede or modify\\n      the terms of any separate license agreement you may have executed\\n      with Licensor regarding such Contributions.\\n\\n   6. Trademarks. This License does not grant permission to use the trade\\n      names, trademarks, service marks, or product names of the Licensor,\\n      except as required for reasonable and customary use in describing the\\n      origin of the Work and reproducing the content of the NOTICE file.\\n\\n   7. Disclaimer of Warranty. Unless required by applicable law or\\n      agreed to in writing, Licensor provides the Work (and each\\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\\n      implied, including, without limitation, any warranties or conditions\\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\\n      PARTICULAR PURPOSE. You are solely responsible for determining the\\n      appropriateness of using or redistributing the Work and assume any\\n      risks associated with Your exercise of permissions under this License.\\n\\n   8. Limitation of Liability. In no event and under no legal theory,\\n      whether in tort (including negligence), contract, or otherwise,\\n      unless required by applicable law (such as deliberate and grossly\\n      negligent acts) or agreed to in writing, shall any Contributor be\\n      liable to You for damages, including any direct, indirect, special,\\n      incidental, or consequential damages of any character arising as a\\n      result of this License or out of the use or inability to use the\\n      Work (including but not limited to damages for loss of goodwill,\\n      work stoppage, computer failure or malfunction, or any and all\\n      other commercial damages or losses), even if such Contributor\\n      has been advised of the possibility of such damages.\\n\\n   9. Accepting Warranty or Additional Liability. While redistributing\\n      the Work or Derivative Works thereof, You may choose to offer,\\n      and charge a fee for, acceptance of support, warranty, indemnity,\\n      or other liability obligations and/or rights consistent with this\\n      License. However, in accepting such obligations, You may act only\\n      on Your own behalf and on Your sole responsibility, not on behalf\\n      of any other Contributor, and only if You agree to indemnify,\\n      defend, and hold each Contributor harmless for any liability\\n      incurred by, or claims asserted against, such Contributor by reason\\n      of your accepting any such warranty or additional liability.\\n\\n   END OF TERMS AND CONDITIONS\\n\\n   APPENDIX: How to apply the Apache License to your work.\\n\\n      To apply the Apache License to your work, attach the following\\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\\n      replaced with your own identifying information. (Don\\'t include\\n      the brackets!)  The text should be enclosed in the appropriate\\n      comment syntax for the file format. We also recommend that a\\n      file or class name and description of purpose be included on the\\n      same \"printed page\" as the copyright notice for easier\\n      identification within third-party archives.\\n\\n   Copyright [yyyy] [name of copyright owner]\\n\\n   Licensed under the Apache License, Version 2.0 (the \"License\");\\n   you may not use this file except in compliance with the License.\\n   You may obtain a copy of the License at\\n\\n       http://www.apache.org/licenses/LICENSE-2.0\\n\\n   Unless required by applicable law or agreed to in writing, software\\n   distributed under the License is distributed on an \"AS IS\" BASIS,\\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n   See the License for the specific language governing permissions and\\n   limitations under the License.\\n'",
    "technique": "File Exploration"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "SongExplorer",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "JaneliaSciComp",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/JaneliaSciComp/SongExplorer/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 6,
      "date": "Sun, 12 Dec 2021 17:51:04 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Let's walk through the steps needed to train a classifier completely from\nscratch.\n\nRecordings need to be monaural 16-bit little-endian PCM-encoded WAV files.\nThey should all be sampled at the same rate, which can be anything.  For this\ntutorial we supply you with *Drosophila melanogaster* data sampled at 2500 Hz.\n\nFirst, let's get some data bundled with SongExplorer into your home directory.\n\n    $ $SONGEXPLORER_BIN ls -1 /opt/songexplorer/data\n    20161207T102314_ch1-annotated-person1.csv\n    20161207T102314_ch1.wav*\n    20190122T093303a-7-annotated-person2.csv*\n    20190122T093303a-7-annotated-person3.csv*\n    20190122T093303a-7.wav*\n    20190122T132554a-14-annotated-person2.csv*\n    20190122T132554a-14-annotated-person3.csv*\n    20190122T132554a-14.wav*\n    Antigua_20110313095210_ch26.wav\n    my_frozen_graph_1k_0.pb*\n    PS_20130625111709_ch3-annotated-person1.csv\n    PS_20130625111709_ch3.wav*\n    vgg_labels.txt*\n\n    $ mkdir -p groundtruth-data/round1\n\n    $ $SONGEXPLORER_BIN cp /opt/songexplorer/data/PS_20130625111709_ch3.wav \\\n          $PWD/groundtruth-data/round1 [%CD%/... on Windows]\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "corresponding stages in `kernel_sizes`.  See [LeCun *et al* (1989; Neural\nComputation)](http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf).\n\n* `dilate after` specifies the first layer, starting from zero, at which to\nstart dilating the convolutional kernels.  See [Yu and Koltun (2016;\narXiv)](https://arxiv.org/pdf/1511.07122.pdf).\n\n* `stride after` specifies the first layer, starting from zero, at which to\nstart striding the convolutional kernels by two.\n\n* `connection` specifies whether to use identity bypasses, which can help\nmodels with many layers converge.  See [He, Zhang, Ren, and Sun (2015;\narXiv](https://arxiv.org/abs/1512.03385).\n\n* `dropout` is the fraction of hidden units on each forward pass to omit\nduring training.  See [Srivastava, Hinton, *et al* (2014; J. Machine Learning Res.)](http://jmlr.org/papers/v15/srivastava14a.html).\n\nThere is also:\n\n* `weights seed` specifies whether to randomize the initial weights or not.  A\nvalue of -1 results in different values for each fold.  They are also different\neach time you run `x-validate`.  Any other number results in a set of initial\nweights that is unique to that number across all folds and repeated runs.\n\n* `batch seed` similarly specifies whether to randomize the order in which\nsamples are drawn from the ground-truth data set during training.  A value of\n-1 results in a different order for each fold and run;  any other number\nresults in a unique order specific to that number across folds and runs.\n\nTo perform a simple grid search for the optimal value for a particular\nhyperparameter, first choose how many folds you want to partition your\nground-truth data into using `k-fold`.  Then set the hyperparameter of interest\nto the first value you want to try and choose a name for the `Logs Folder` such\nthat its prefix will be shared across all of the hyperparameter values you plan\nto validate.  Suffix any additional hyperparameters of interest using\nunderscores.  (For example, to search mini-batch and keep track of kernel size\nand feature maps, use \"mb-64_ks129_fm64\".)  If your models is small, use\n`models_per_job` in \"configuration.pysh\" to train multiple folds on a GPU.\nClick the `X-Validate` button and then `DoIt!`.  One classifier will be trained\nfor each fold, using it as the validation set and the remaining folds for\ntraining.  Separate files and subdirectories are created in the `Logs Folder`\nthat are suffixed by the fold number and the letter \"k\".  Plot overlayed\ntraining curves with the `Accuracy` button, as before.  Repeat the above\nprocedure for each of remaining hyperparameter values you want to try (e.g.\n\"mb-128_ks129_fm64\", \"mb-256_ks129_fm64\", etc.).  Then use the `Compare` button\nto create a figure of the cross-validation data over the hyperparameter values,\nspecifying the prefix that the logs folders have in common (\"mb\" in this case).\nOutput are three files:\n\n* \"[suffix]-compare-confusion-matrices.pdf\" contains the summed confusion matrix\nfor each of the values tested.\n\n* \"[suffix]-compare-overall-params-speed.pdf\" plots the accuracy, number of\ntrainable parameters, and training time for each model.\n\n* \"[suffix]-compare-precision-recall.pdf\" shows the final error rates for each\nmodel and wanted word.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "    M.init(\"configuration.pysh\")\n    V.init(None)\n    C.init(None)\n\n    ",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "    run([\"hetero\", \"start\", str(M.local_ncpu_cores),\n         str(M.local_ngpu_cards), str(M.local_ngigabytes_memory)])\n\n    ",
      "technique": "Header extraction"
    }
  ]
}