{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1707.06347",
      "https://arxiv.org/abs/1707.06347](https://arxiv.org/abs/1707.06347) (see also the associated [OpenAI blog post](https://blog.openai.com/openai-baselines-ppo/)). This implementation is written for my own personal edification, but I hope that others find it helpful or informative.\n\n\n## Installation and running\n\nThis project is based on Python 3, [Tensorflow](https://www.tensorflow.org), and the [OpenAI Gym environments](https://gym.openai.com). It's been tested on various Atari environments, although the basic algorithm can easily be applied to other scenarios.\n\nTo install the python requirements, run `pip3 install -r requirements.txt` (although you may want to create a [virtual environment](https://docs.python.org/3/tutorial/venv.html) first). The video recorder also requires [ffmepg](https://ffmpeg.org) which must be installed separately.\n\nTo run an environment, use e.g.\n\n    python3 atari_ppo.py --logdir=./logdata --pfile=../example-pong-params.json\n\nWith the example parameters, the agent should be able to win a perfect game of Pong in about 2 million frames, which closely matches the results from the OpenAI baseline implementation. Other environments can be used by modifying the parameters file. To view the training progress, use tensorboard:\n\n    tensorboard --logdir=./logdata\n\n\n## Example outputs\n\n![a game of pong](./pong.gif)\n![a game of space invaders](./space_invaders.gif)\n\n\n## Experimental modifications\n\nOne of the problems with policy gradient algorithms is that they are very sensitive to step size, and they are prone to catastrophic performance drops. Indeed, much of the original motivation for the PPO algorithm was to make the policy update robust across a larger range of step sizes. However, in my experiments with only modestly large step sizes I would frequently see performance drops from optimal to worse-than-random policies. This was easiest to reproduce in Pong, in which it's fairly straightforward to train an optimal agent. See the *[policy collapse notebook](./policy_collapse_analysis.ipynb)* for a detailed analysis of one such drop and its proximal causes.\n\nThe implementation presented here includes two experimental modifications of the standard PPO algorithm which attempt to avoid catastrophic performance drops. Both are theoretically justifiable, but neither seem to eliminate the problem. However, I've so far only scratched the surface of their effects, and more thorough experimentation may prove them useful.\n\n### Value function rescaling\n\nThe first modification rescales the gradient of the value loss function by a quantity that I'm calling the \u201cpseudo-entropy,\u201d `H\u2019 = \u2211\u03c0(a|s)(1-\u03c0(a|s))` where `\u03c0(a|s)` is the probability of taking a particular action in a particular state, and, like the standard entropy, the sum is over all possible actions. The pseudo-entropy is `1-1/N \u2248 1` when the distribution is uniform over `N` states and zero when the entropy is zero. The reason to do this rescaling is that the policy gradient contains a similar term when expanded to show the gradient with respect to the underlying logits. If the policy function is given by a softmax `\u03c0(a_i|s) = exp(x_i(s)) / \u2211_j exp(x_j(s))`, then the policy update will look like\n\n    \u2207log \u03c0(a_i|s) = \u2211_j (\u03b4_ij - \u03c0(a_j|s)) \u2207\u03c0(a_j|s)\n\nThe average magnitude of the term in parenthesis roughly corresponds to the pseudo-entropy. When the action is very certain, the update will on average be very small. This is necessary and expected: if the update weren't small, the probabilities would quickly saturate and the method would not converge to a good policy. However, a problem arises when we share weights between the value function and the policy function. In standard PPO there is no term that makes the value gradient correspondingly small, so if the policy is certain the weight updates will be driven by changes to the value function. Eventually, this may lead the agent away from a good policy. If the step size is moderately large, the agent may quickly cross over to a regime of bad policy and not recover.\n\nThis implementation of PPO includes three different types of value gradient rescaling:\n\n- *per-state rescaling*: the pseudo-entropy is applied separately to each state, such that some states are effectively weighted much more heavily than others in the determination of the value update;\n- *per-batch rescaling*: the average pseudo-entropy is calculated per mini-batch and applied uniformly across all updates in that batch;\n- *smoothed rescaling*: the average pseudo-entropy is smoothed across multiple mini-batches and applied uniformly across all updates for each batch.\n\nThe per-state rescaling performs very poorly, and tends to result in agents that never learn good policies. The problem with per-state rescaling is that it prevents the agent from learning good value functions precisely in the states in which require the most critical (low entropy) actions.\n\nThe per-batch rescaling and smoothed rescaling perform similarly to each other. Unfortunately they don't appear to have a large effect on the catastrophic policy drops, and in general have little effect on the training. The problem here is that an optimal policy may have no preferences in certain states when all actions lead to similar rewards. Therefore, an optimal policy can have a high average entropy even though the actions are very certain in critical situations. Only in situations where the entropy is habitually very low does rescaling have a large effect, and those situations tend to have poor policies already.\n\n\n### Modified Surrogate Objective\n\nProximal policy optimization builds upon standard policy gradient methods in two primary ways:\n\n1. Rather than minimizing the standard loss `L_\u03c0(a, s) = -A log \u03c0(a|s)` where `A` is the observed advantage of the state-action pair, PPO introduces surrogate objective function `L'_\u03c0(a, s)`. The gradient of the surrogate function is designed to coincide with the original gradient when policy is unchanged from the prior time step. However, when the policy change is large, either the gradient gets clipped or a penalty is added such that further changes are discouraged.\n2. The surrogate objective is minimized over several epochs over stochastic gradient descent for each batch of training data.\n\nCombined, these two features yield good training with high sample efficiency, and, for the most part, without overly noisy policy updates and catastrophic policy drops.\n\nThe surrogate objective that's used in this implementation is the *clipped surrogate objective* (as opposed to the adaptive KL penalty which is also detailed in the original paper),\n\n    L_{CLIP"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/clwainwright/proximal_policy_optimization",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-11-05T22:58:46Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-09-07T19:41:49Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.992357494631654,
        0.9939935996406094,
        0.958372223123805,
        0.9867755490377986
      ],
      "excerpt": "Proximal policy optimization is a reinforcement learning algorithm that works via a policy gradient. The original paper for the algorithm is arXiv:1707.06347 (see also the associated OpenAI blog post). This implementation is written for my own personal edification, but I hope that others find it helpful or informative. \nOne of the problems with policy gradient algorithms is that they are very sensitive to step size, and they are prone to catastrophic performance drops. Indeed, much of the original motivation for the PPO algorithm was to make the policy update robust across a larger range of step sizes. However, in my experiments with only modestly large step sizes I would frequently see performance drops from optimal to worse-than-random policies. This was easiest to reproduce in Pong, in which it's fairly straightforward to train an optimal agent. See the policy collapse notebook for a detailed analysis of one such drop and its proximal causes. \nThe implementation presented here includes two experimental modifications of the standard PPO algorithm which attempt to avoid catastrophic performance drops. Both are theoretically justifiable, but neither seem to eliminate the problem. However, I've so far only scratched the surface of their effects, and more thorough experimentation may prove them useful. \nThe first modification rescales the gradient of the value loss function by a quantity that I'm calling the \u201cpseudo-entropy,\u201d H\u2019 = \u2211\u03c0(a|s)(1-\u03c0(a|s)) where \u03c0(a|s) is the probability of taking a particular action in a particular state, and, like the standard entropy, the sum is over all possible actions. The pseudo-entropy is 1-1/N \u2248 1 when the distribution is uniform over N states and zero when the entropy is zero. The reason to do this rescaling is that the policy gradient contains a similar term when expanded to show the gradient with respect to the underlying logits. If the policy function is given by a softmax \u03c0(a_i|s) = exp(x_i(s)) / \u2211_j exp(x_j(s)), then the policy update will look like \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9630541585386182,
        0.9290162977516289,
        0.9813133563115394,
        0.8389504866458923,
        0.8660387398511435,
        0.9459405509190698,
        0.9760651491652577
      ],
      "excerpt": "The average magnitude of the term in parenthesis roughly corresponds to the pseudo-entropy. When the action is very certain, the update will on average be very small. This is necessary and expected: if the update weren't small, the probabilities would quickly saturate and the method would not converge to a good policy. However, a problem arises when we share weights between the value function and the policy function. In standard PPO there is no term that makes the value gradient correspondingly small, so if the policy is certain the weight updates will be driven by changes to the value function. Eventually, this may lead the agent away from a good policy. If the step size is moderately large, the agent may quickly cross over to a regime of bad policy and not recover. \nThis implementation of PPO includes three different types of value gradient rescaling: \nper-state rescaling: the pseudo-entropy is applied separately to each state, such that some states are effectively weighted much more heavily than others in the determination of the value update; \nper-batch rescaling: the average pseudo-entropy is calculated per mini-batch and applied uniformly across all updates in that batch; \nsmoothed rescaling: the average pseudo-entropy is smoothed across multiple mini-batches and applied uniformly across all updates for each batch. \nThe per-state rescaling performs very poorly, and tends to result in agents that never learn good policies. The problem with per-state rescaling is that it prevents the agent from learning good value functions precisely in the states in which require the most critical (low entropy) actions. \nThe per-batch rescaling and smoothed rescaling perform similarly to each other. Unfortunately they don't appear to have a large effect on the catastrophic policy drops, and in general have little effect on the training. The problem here is that an optimal policy may have no preferences in certain states when all actions lead to similar rewards. Therefore, an optimal policy can have a high average entropy even though the actions are very certain in critical situations. Only in situations where the entropy is habitually very low does rescaling have a large effect, and those situations tend to have poor policies already. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9860287446096591,
        0.9267454394039897,
        0.889855236303466,
        0.9894785668187218
      ],
      "excerpt": "Rather than minimizing the standard loss L_\u03c0(a, s) = -A log \u03c0(a|s) where A is the observed advantage of the state-action pair, PPO introduces surrogate objective function L'_\u03c0(a, s). The gradient of the surrogate function is designed to coincide with the original gradient when policy is unchanged from the prior time step. However, when the policy change is large, either the gradient gets clipped or a penalty is added such that further changes are discouraged. \nThe surrogate objective is minimized over several epochs over stochastic gradient descent for each batch of training data. \nCombined, these two features yield good training with high sample efficiency, and, for the most part, without overly noisy policy updates and catastrophic policy drops. \nThe surrogate objective that's used in this implementation is the clipped surrogate objective (as opposed to the adaptive KL penalty which is also detailed in the original paper), \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9394800736614934,
        0.9806413154651904,
        0.9838028938797373,
        0.9701703512903727
      ],
      "excerpt": "where r(\u03b8) = \u03c0(a|s,\u03b8) / \u03c0(a|s,\u03b8_old). The choice of sign just denotes that I'm doing gradient descent rather than gradient ascent. Effectively, all the clipped function does is to produce a constant gradient until the policy has improved by a factor of 1+\u03b5, at which point the gradient goes to zero and further improvement stops. \nThere are a couple of things about this function that struck me a theoretically problematic. First, it's not symmetric. If an action is favored (positive advantage) and highly likely such that \u03c0/\u03c0_old &gt; 1-\u03b5, then it won't be clipped at all and the policy can increase arbitrarily close to one. If, on the other hand, an action has high probability but negative advantage, the surrogate won't clip until \u03c0 \u2248 1-\u03b5, which may represent a many order-of-magnitude increase in the policy's entropy. Either way, the clipping allows for very large changes in the underlying weights when \u03c0_old is close to one. \nThe second problem is that once an update moves a policy into the clipped regime, there is no counteracting force to bring it back towards the trusted region. This is especially problematic given that the weights are shared both across different states in the batch and with the value function, so the policy for a single state could be dragged far away from its old trusted value due to changes elsewhere in the network. \nI have implemented two small changes to the clipped surrogate objective function which attempt to fix these problems and hopefully prevent catastrophic policy drops. The first change is to perform the clipping in logit space rather than probability space. We can rewrite the clipped loss as \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9580651753802073
      ],
      "excerpt": "where \u03c0' = \u03c0_old (1 + \u03b5 sign(A)) is the target policy. Once the new policy moves beyond the target policy the function will be clipped and the gradient will be zero. To perform the clipping in logit space, we just need to move the target policy such that it's a fixed distance from the original policy in logit space: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9780600662079496,
        0.9044234065218764,
        0.9099313524966296
      ],
      "excerpt": "When \u03c0_old = 1/2 the two formulations are equal, and when \u03c0_old \u226a 1 the logit-space formulation has approximately twice change as the original. However, when \u03c0_old \u2248 1 the new formulation provides a much tighter clipping. Note that the new formulation is symmetric: \u03c0'(\u03c0_old, A) = 1 - \u03c0'(1-\u03c0_old, -A). \nInitial experiments with this modified target did not show a large effect in the training (set the parameter delta_target_policy = \"logit\" to enable it). It didn't prevent a catastrophic policy drop, but it also didn't perform any worse than the original. I hope to do some more experiments to find out where, if anywhere, this might make a difference. \nThe second change aimed to draw the policy back towards the target policy if moves out of the trusted region. To do this, I replaced the clipped loss with \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "TensorFlow implementation of Proximal Policy Optimization for use in Atari environments",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/clwainwright/proximal_policy_optimization/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Tue, 07 Dec 2021 18:21:16 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/clwainwright/proximal_policy_optimization/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "clwainwright/proximal_policy_optimization",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/clwainwright/proximal_policy_optimization/master/policy_collapse_analysis.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This project is based on Python 3, [Tensorflow](https://www.tensorflow.org), and the [OpenAI Gym environments](https://gym.openai.com). It's been tested on various Atari environments, although the basic algorithm can easily be applied to other scenarios.\n\nTo install the python requirements, run `pip3 install -r requirements.txt` (although you may want to create a [virtual environment](https://docs.python.org/3/tutorial/venv.html) first). The video recorder also requires [ffmepg](https://ffmpeg.org) which must be installed separately.\n\nTo run an environment, use e.g.\n\n    python3 atari_ppo.py --logdir=./logdata --pfile=../example-pong-params.json\n\nWith the example parameters, the agent should be able to win a perfect game of Pong in about 2 million frames, which closely matches the results from the OpenAI baseline implementation. Other environments can be used by modifying the parameters file. To view the training progress, use tensorboard:\n\n    tensorboard --logdir=./logdata\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/clwainwright/proximal_policy_optimization/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2018 Carroll Wainwright\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Proximal Policy Optimization",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "proximal_policy_optimization",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "clwainwright",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/clwainwright/proximal_policy_optimization/blob/master/readme.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This project is based on Python 3, [Tensorflow](https://www.tensorflow.org), and the [OpenAI Gym environments](https://gym.openai.com). It's been tested on various Atari environments, although the basic algorithm can easily be applied to other scenarios.\n\nTo install the python requirements, run `pip3 install -r requirements.txt` (although you may want to create a [virtual environment](https://docs.python.org/3/tutorial/venv.html) first). The video recorder also requires [ffmepg](https://ffmpeg.org) which must be installed separately.\n\nTo run an environment, use e.g.\n\n    python3 atari_ppo.py --logdir=./logdata --pfile=../example-pong-params.json\n\nWith the example parameters, the agent should be able to win a perfect game of Pong in about 2 million frames, which closely matches the results from the OpenAI baseline implementation. Other environments can be used by modifying the parameters file. To view the training progress, use tensorboard:\n\n    tensorboard --logdir=./logdata\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Tue, 07 Dec 2021 18:21:16 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "reinforcement-learning",
      "reinforcement-learning-algorithms",
      "proximal-policy-optimization"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "![a game of pong](./pong.gif)\n![a game of space invaders](./space_invaders.gif)\n\n\n",
      "technique": "Header extraction"
    }
  ]
}