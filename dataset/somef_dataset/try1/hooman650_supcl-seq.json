{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2109.07424",
      "https://arxiv.org/abs/2109.07424",
      "https://arxiv.org/abs/2004.11362",
      "https://arxiv.org/abs/2104.08821"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[1] [Supervised Contrastive Learning](https://arxiv.org/abs/2004.11362)\n\n[2] [SimCSE: Simple Contrastive Learning of Sentence Embeddings](https://arxiv.org/abs/2104.08821)\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```bibtex\n@misc{sedghamiz2021supclseq,\n      title={SupCL-Seq: Supervised Contrastive Learning for Downstream Optimized Sequence Representations}, \n      author={Hooman Sedghamiz and Shivam Raval and Enrico Santus and Tuka Alhanai and Mohammad Ghassemi},\n      year={2021},\n      eprint={2109.07424},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{sedghamiz2021supclseq,\n      title={SupCL-Seq: Supervised Contrastive Learning for Downstream Optimized Sequence Representations}, \n      author={Hooman Sedghamiz and Shivam Raval and Enrico Santus and Tuka Alhanai and Mohammad Ghassemi},\n      year={2021},\n      eprint={2109.07424},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8942856639420041
      ],
      "excerpt": "How to Cite \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/hooman650/SupCL-Seq",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-09-08T17:14:50Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-02T09:04:23Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9885573396302046,
        0.9657071427489141
      ],
      "excerpt": "Supervised Contrastive Learning for Downstream Optimized Sequence representations (SupCS-Seq) accepted to be published in EMNLP 2021, extends the supervised contrastive learning from computer vision to the optimization of sequence representations in NLP. By altering the dropout mask probability in standard Transformer architectures (e.g. BERT_base), for every representation (anchor), we generate augmented altered views. A supervised contrastive loss is then utilized to maximize the system\u2019s capability of pulling together similar samples (e.g. anchors and their altered views) and pushing apart the samples belonging to the other classes. Despite its simplicity, SupCL-Seq leads to large gains in many sequence classification tasks on the GLUE benchmark compared to a standard BERT_base, including 6% absolute improvement on CoLA, 5.4% on MRPC, 4.7% on RTE and 2.6% on STS-B. \nThis package can be easily run on almost all of the transformer models in Huggingface:hugs: that contain an encoder including but not limited to: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9140177000822516
      ],
      "excerpt": "And many more models! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Supervised Contrastive Learning for Downstream Optimized Sequence Representations",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/hooman650/supcl-seq/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Thu, 09 Dec 2021 09:04:31 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/hooman650/SupCL-Seq/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "hooman650/SupCL-Seq",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/hooman650/supcl-seq/main/examples/glue.ipynb",
      "https://raw.githubusercontent.com/hooman650/supcl-seq/main/examples/glue-Ernie.ipynb",
      "https://raw.githubusercontent.com/hooman650/supcl-seq/main/examples/glue_bert_base.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. First you need to install one of, or both, TensorFlow 2.0 and PyTorch. Please refer to [TensorFlow installation page](https://www.tensorflow.org/install/pip), [PyTorch installation page](https://pytorch.org/) and/or Flax installation page regarding the specific install command for your platform.\n\n2. Second step:\n\n```bash\n$ pip install SupCL-Seq\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9769420979004304
      ],
      "excerpt": "Installation  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8224384430937229
      ],
      "excerpt": "Run on GLUE \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/hooman650/SupCL-Seq/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2021 Hooman Sedghamiz\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "SupCL-Seq :book:",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "SupCL-Seq",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "hooman650",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/hooman650/SupCL-Seq/blob/main/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "In order to evaluate the method on GLUE benchmark please see the [`glue.ipynb`](./examples/glue.ipynb)\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 8,
      "date": "Thu, 09 Dec 2021 09:04:31 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The package builds on the [`trainer`](https://huggingface.co/transformers/main_classes/trainer.html) from [`Huggingface`](https://huggingface.co/):hugs:. Therefore, its use is exactly similar to [`trainer`](https://huggingface.co/transformers/main_classes/trainer.html). The pipeline works as follows:\n\n1. First employ supervised contrastive learning to constratively optimize sentence embeddings using your annotated data.\n \n```python\nfrom SupCL_Seq import SupCsTrainer\n\nSupCL_trainer = SupCsTrainer.SupCsTrainer(\n            w_drop_out=[0.0,0.05,0.2],      #: Number of views and their associated mask drop-out probabilities [Optional]\n            temperature= 0.05,              #: Temeprature for the contrastive loss function [Optional]\n            def_drop_out=0.1,               #: Default drop out of the transformer, this is usually 0.1 [Optional]\n            pooling_strategy='mean',        #: Strategy used to extract embeddings can be from `mean` or `pooling` [Optional]\n            model = model,                  #: model\n            args = CL_args,                 #: Arguments from `TrainingArguments` [Optional]\n            train_dataset=train_dataset,    #: Train dataloader\n            tokenizer=tokenizer,            #: Tokenizer\n            compute_metrics=compute_metrics #: If you need a customized evaluation [Optional]\n        )\n\n```\n\n\n\n\n2. After contrastive training:\n\n    2.1 Add a linear classification layer to your model\n   \n    2.2 Freeze the base layer\n    \n    2.3 Finetune the linear layer on your annotated data\n\n\nFor detailed implementation see [`glue.ipynb`](./examples/glue.ipynb)\n\n",
      "technique": "Header extraction"
    }
  ]
}