{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2108.10831",
      "https://arxiv.org/abs/2108.10831"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you use this data for your research, please cite our paper [LLVIP: A Visible-infrared Paired Dataset for Low-light Vision](https://arxiv.org/abs/2108.10831):\n\n```\n@inproceedings{jia2021llvip,\n  title={LLVIP: A Visible-infrared Paired Dataset for Low-light Vision},\n  author={Jia, Xinyu and Zhu, Chuang and Li, Minzhen and Tang, Wenqi and Zhou, Wenli},\n  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},\n  pages={3496--3504},\n  year={2021}\n}\n```\n\n<h2> <p align=\"center\"> Image Fusion </p> </h2>  \n\nBaselines\n   - [GTF](https://github.com/jiayi-ma/GTF)\n   - [FusionGAN](https://github.com/jiayi-ma/FusionGAN)\n   - [Densefuse](https://github.com/hli1221/imagefusion_densefuse)\n   - [IFCNN](https://github.com/uzeful/IFCNN)\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{jia2021llvip,\n  title={LLVIP: A Visible-infrared Paired Dataset for Low-light Vision},\n  author={Jia, Xinyu and Zhu, Chuang and Li, Minzhen and Tang, Wenqi and Zhou, Wenli},\n  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},\n  pages={3496--3504},\n  year={2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9099845036220141
      ],
      "excerpt": "Project | Arxiv | Benchmarks| | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.80953443415843
      ],
      "excerpt": "To acquire complete LLVIP dataset, please visit https://bupt-ai-cz.github.io/LLVIP/. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9826628511165726,
        0.8955886365383559
      ],
      "excerpt": "Please visit https://github.com/uzeful/IFCNN. \n<h2> <p align=\"center\"> Pedestrian Detection </p> </h2> \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/bupt-ai-cz/LLVIP",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "email: shengjie.Liu@bupt.edu.cn, czhu@bupt.edu.cn, jiaxinyujxy@qq.com, tangwenqi@bupt.edu.cn\n",
      "technique": "Header extraction"
    }
  ],
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contributing to YOLOv5 \ud83d\ude80\nWe love your input! We want to make contributing to YOLOv5 as easy and transparent as possible, whether it's:\n\nReporting a bug\nDiscussing the current state of the code\nSubmitting a fix\nProposing a new feature\nBecoming a maintainer\n\nYOLOv5 works so well due to our combined community effort, and for every small improvement you contribute you will be\nhelping push the frontiers of what's possible in AI \ud83d\ude03!\nSubmitting a Pull Request (PR) \ud83d\udee0\ufe0f\nSubmitting a PR is easy! This example shows how to submit a PR for updating requirements.txt in 4 steps:\n1. Select File to Update\nSelect requirements.txt to update by clicking on it in GitHub.\n<p align=\"center\"><img width=\"800\" alt=\"PR_step1\" src=\"https://user-images.githubusercontent.com/26833433/122260847-08be2600-ced4-11eb-828b-8287ace4136c.png\"></p>\n\n2. Click 'Edit this file'\nButton is in top-right corner.\n<p align=\"center\"><img width=\"800\" alt=\"PR_step2\" src=\"https://user-images.githubusercontent.com/26833433/122260844-06f46280-ced4-11eb-9eec-b8a24be519ca.png\"></p>\n\n3. Make Changes\nChange matplotlib version from 3.2.2 to 3.3.\n<p align=\"center\"><img width=\"800\" alt=\"PR_step3\" src=\"https://user-images.githubusercontent.com/26833433/122260853-0a87e980-ced4-11eb-9fd2-3650fb6e0842.png\"></p>\n\n4. Preview Changes and Submit PR\nClick on the Preview changes tab to verify your updates. At the bottom of the screen select 'Create a new branch\nfor this commit', assign your branch a descriptive name such as fix/matplotlib_version and click the green Propose\nchanges button. All done, your PR is now submitted to YOLOv5 for review and approval \ud83d\ude03!\n<p align=\"center\"><img width=\"800\" alt=\"PR_step4\" src=\"https://user-images.githubusercontent.com/26833433/122260856-0b208000-ced4-11eb-8e8e-77b6151cbcc3.png\"></p>\n\nPR recommendations\nTo allow your work to be integrated as seamlessly as possible, we advise you to:\n\n\u2705 Verify your PR is up-to-date with origin/master. If your PR is behind origin/master an\n  automatic GitHub actions rebase may\n  be attempted by including the /rebase command in a comment body, or by running the following code, replacing 'feature'\n  with the name of your local branch:\n\nbash\ngit remote add upstream https://github.com/ultralytics/yolov5.git\ngit fetch upstream\ngit checkout feature  # &lt;----- replace 'feature' with local branch name\ngit merge upstream/master\ngit push -u origin -f\n\n\u2705 Verify all Continuous Integration (CI) checks are passing.\n\u2705 Reduce changes to the absolute minimum required for your bug fix or feature addition. \"It is not daily increase\n  but daily decrease, hack away the unessential. The closer to the source, the less wastage there is.\"  -Bruce Lee\n\nSubmitting a Bug Report \ud83d\udc1b\nIf you spot a problem with YOLOv5 please submit a Bug Report!\nFor us to start investigating a possibel problem we need to be able to reproduce it ourselves first. We've created a few\nshort guidelines below to help users provide what we need in order to get started.\nWhen asking a question, people will be better able to provide help if you provide code that they can easily\nunderstand and use to reproduce the problem. This is referred to by community members as creating\na minimum reproducible example. Your code that reproduces\nthe problem should be:\n\n\u2705 Minimal \u2013 Use as little code as possible that still produces the same problem\n\u2705 Complete \u2013 Provide all parts someone else needs to reproduce your problem in the question itself\n\u2705 Reproducible \u2013 Test the code you're about to provide to make sure it reproduces the problem\n\nIn addition to the above requirements, for Ultralytics to provide assistance your code\nshould be:\n\n\u2705 Current \u2013 Verify that your code is up-to-date with current\n  GitHub master, and if necessary git pull or git clone a new\n  copy to ensure your problem has not already been resolved by previous commits.\n\u2705 Unmodified \u2013 Your problem must be reproducible without any modifications to the codebase in this\n  repository. Ultralytics does not provide support for custom code \u26a0\ufe0f.\n\nIf you believe your problem meets all of the above criteria, please close this issue and raise a new one using the \ud83d\udc1b \nBug Report template and providing\na minimum reproducible example to help us better\nunderstand and diagnose your problem.\nLicense\nBy contributing, you agree that your contributions will be licensed under\nthe GPL-3.0 license",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-06-15T08:10:42Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-10T03:29:05Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8400855172165578,
        0.9375624231844276
      ],
      "excerpt": "Our trained model can be downloaded from here: Google-Drive-Yolov5-model or BaiduYun-Yolov5-model (code: qepr) \n- Click Here for the tutorial of Yolov3 \uff08Our trained Yolov3 model can be downloaded from here: Google-Drive-Yolov3-model or BaiduYun-Yolov3-model (code: ine5)\uff09. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9951279380122525
      ],
      "excerpt": "Where AP means the average of AP at IoU threshold of 0.5 to 0.95, with an interval of 0.05. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9811943048223367
      ],
      "excerpt": "The figure above shows the change of AP under different IoU thresholds. When the IoU threshold is higher than 0.7, the AP value drops rapidly. Besides, the infrared image highlights pedestrains and achieves a better effect than the visible image in the detection task, which not only proves the necessity of infrared images but also indicates that the performance of visible-image pedestrian detection algorithm is not good enough under low-light conditions. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9653246115148941
      ],
      "excerpt": "We retrained and tested pix2pixGAN  on the updated dataset(30976 images). The structure of generator is unet256, and the structure of discriminator is the basic PatchGAN as default.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "LLVIP: A Visible-infrared Paired Dataset for Low-light Vision",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/bupt-ai-cz/LLVIP/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 28,
      "date": "Fri, 10 Dec 2021 04:37:50 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/bupt-ai-cz/LLVIP/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "bupt-ai-cz/LLVIP",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/bupt-ai-cz/LLVIP/main/pix2pixGAN/docs/Dockerfile",
      "https://raw.githubusercontent.com/bupt-ai-cz/LLVIP/main/yolov5/Dockerfile",
      "https://raw.githubusercontent.com/bupt-ai-cz/LLVIP/main/yolov5/utils/google_app_engine/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/bupt-ai-cz/LLVIP/tree/main/pix2pixGAN/docs",
      "https://github.com/bupt-ai-cz/LLVIP/tree/main/yolov3/docs"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/bupt-ai-cz/LLVIP/main/pix2pixGAN/CycleGAN.ipynb",
      "https://raw.githubusercontent.com/bupt-ai-cz/LLVIP/main/pix2pixGAN/pix2pix.ipynb",
      "https://raw.githubusercontent.com/bupt-ai-cz/LLVIP/main/yolov5/tutorial.ipynb",
      "https://raw.githubusercontent.com/bupt-ai-cz/LLVIP/main/yolov3/docs/Box-Clustering.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/bupt-ai-cz/LLVIP/main/pix2pixGAN/scripts/test_cyclegan.sh",
      "https://raw.githubusercontent.com/bupt-ai-cz/LLVIP/main/pix2pixGAN/scripts/download_pix2pix_model.sh",
      "https://raw.githubusercontent.com/bupt-ai-cz/LLVIP/main/pix2pixGAN/scripts/conda_deps.sh",
      "https://raw.githubusercontent.com/bupt-ai-cz/LLVIP/main/pix2pixGAN/scripts/download_cyclegan_model.sh",
      "https://raw.githubusercontent.com/bupt-ai-cz/LLVIP/main/pix2pixGAN/scripts/install_deps.sh",
      "https://raw.githubusercontent.com/bupt-ai-cz/LLVIP/main/pix2pixGAN/scripts/test_pix2pix.sh",
      "https://raw.githubusercontent.com/bupt-ai-cz/LLVIP/main/pix2pixGAN/scripts/test_single.sh",
      "https://raw.githubusercontent.com/bupt-ai-cz/LLVIP/main/pix2pixGAN/scripts/test_colorization.sh",
      "https://raw.githubusercontent.com/bupt-ai-cz/LLVIP/main/pix2pixGAN/scripts/train_colorization.sh",
      "https://raw.githubusercontent.com/bupt-ai-cz/LLVIP/main/pix2pixGAN/scripts/train_cyclegan.sh",
      "https://raw.githubusercontent.com/bupt-ai-cz/LLVIP/main/pix2pixGAN/scripts/train_pix2pix.sh",
      "https://raw.githubusercontent.com/bupt-ai-cz/LLVIP/main/pix2pixGAN/scripts/eval_cityscapes/download_fcn8s.sh",
      "https://raw.githubusercontent.com/bupt-ai-cz/LLVIP/main/pix2pixGAN/datasets/download_cyclegan_dataset.sh",
      "https://raw.githubusercontent.com/bupt-ai-cz/LLVIP/main/pix2pixGAN/datasets/download_pix2pix_dataset.sh",
      "https://raw.githubusercontent.com/bupt-ai-cz/LLVIP/main/yolov5/data/scripts/get_coco128.sh",
      "https://raw.githubusercontent.com/bupt-ai-cz/LLVIP/main/yolov5/data/scripts/get_coco.sh",
      "https://raw.githubusercontent.com/bupt-ai-cz/LLVIP/main/yolov5/data/scripts/download_weights.sh",
      "https://raw.githubusercontent.com/bupt-ai-cz/LLVIP/main/yolov5/utils/aws/userdata.sh",
      "https://raw.githubusercontent.com/bupt-ai-cz/LLVIP/main/yolov5/utils/aws/mime.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Install requirements\n  ```bash\n  cd pix2pixGAN\n  pip install -r requirements.txt\n  ```\n- [Prepare dataset](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/datasets.md)\n- File structure\n  ```\n  pix2pixGAN\n  \u251c\u2500\u2500 ...\n  \u2514\u2500\u2500datasets\n     \u251c\u2500\u2500 ...\n     \u2514\u2500\u2500LLVIP\n        \u251c\u2500\u2500 train\n        |   \u251c\u2500\u2500 010001.jpg\n        |   \u251c\u2500\u2500 010002.jpg\n        |   \u251c\u2500\u2500 010003.jpg\n        |   \u2514\u2500\u2500 ...\n        \u2514\u2500\u2500 test\n            \u251c\u2500\u2500 190001.jpg\n            \u251c\u2500\u2500 190002.jpg\n            \u251c\u2500\u2500 190003.jpg\n            \u2514\u2500\u2500 ...\n  ```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "- Install requirements\n  ```bash\n  git clone https://github.com/bupt-ai-cz/LLVIP.git\n  cd LLVIP/imagefusion_densefuse\n  \n  #: Create your virtual environment using anaconda\n  conda create -n Densefuse python=3.7\n  conda activate Densefuse\n  \n  conda install scikit-image scipy==1.2.1 tensorflow-gpu==1.14.0\n  ```\n- File structure\n  ```\n  imagefusion_densefuse\n  \u251c\u2500\u2500 ...\n  \u251c\u2500\u2500datasets\n  |  \u251c\u2500\u2500010001_ir.jpg\n  |  \u251c\u2500\u2500010001_vi.jpg\n  |  \u2514\u2500\u2500 ...\n  \u251c\u2500\u2500test\n  |  \u251c\u2500\u2500190001_ir.jpg\n  |  \u251c\u2500\u2500190001_vi.jpg\n  |  \u2514\u2500\u2500 ...\n  \u2514\u2500\u2500LLVIP\n     \u251c\u2500\u2500 infrared\n     |   \u251c\u2500\u2500train\n     |   |  \u251c\u2500\u2500 010001.jpg\n     |   |  \u251c\u2500\u2500 010002.jpg\n     |   |  \u2514\u2500\u2500 ...\n     |   \u2514\u2500\u2500test\n     |      \u251c\u2500\u2500 190001.jpg\n     |      \u251c\u2500\u2500 190002.jpg\n     |      \u2514\u2500\u2500 ...\n     \u2514\u2500\u2500 visible\n         \u251c\u2500\u2500train\n         |   \u251c\u2500\u2500 010001.jpg\n         |   \u251c\u2500\u2500 010002.jpg\n         |   \u2514\u2500\u2500 ...\n         \u2514\u2500\u2500 test\n             \u251c\u2500\u2500 190001.jpg\n             \u251c\u2500\u2500 190002.jpg\n             \u2514\u2500\u2500 ...\n  ```\n  \n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "- Install requirements\n  ```bash\n  git clone https://github.com/bupt-ai-cz/LLVIP.git\n  cd LLVIP/FusionGAN\n  #: Create your virtual environment using anaconda\n  conda create -n FusionGAN python=3.7\n  conda activate FusionGAN\n  \n  conda install matplotlib scipy==1.2.1 tensorflow-gpu==1.14.0 \n  pip install opencv-python\n  sudo apt install libgl1-mesa-glx\n  ```\n- File structure\n  ```\n  FusionGAN\n  \u251c\u2500\u2500 ...\n  \u251c\u2500\u2500 Test_LLVIP_ir\n  |   \u251c\u2500\u2500 190001.jpg\n  |   \u251c\u2500\u2500 190002.jpg\n  |   \u2514\u2500\u2500 ...\n  \u251c\u2500\u2500 Test_LLVIP_vi\n  |   \u251c\u2500\u2500 190001.jpg\n  |   \u251c\u2500\u2500 190002.jpg\n  |   \u2514\u2500\u2500 ...\n  \u251c\u2500\u2500 Train_LLVIP_ir\n  |   \u251c\u2500\u2500 010001.jpg\n  |   \u251c\u2500\u2500 010002.jpg\n  |   \u2514\u2500\u2500 ...\n  \u2514\u2500\u2500 Train_LLVIP_vi\n      \u251c\u2500\u2500 010001.jpg\n      \u251c\u2500\u2500 010002.jpg\n      \u2514\u2500\u2500 ...\n  ```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9986422926500436,
        0.833114516308531,
        0.9755529771184988,
        0.9906248903846466,
        0.9979947896609701
      ],
      "excerpt": "Install requirements \n  bash \n  git clone https://github.com/bupt-ai-cz/LLVIP.git \n  cd LLVIP/yolov5 \n  pip install -r requirements.txt \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9224508502540122,
        0.9017032235723903
      ],
      "excerpt": "  python main.py --epoch 10 --batch_size 32 \nSee more training options in main.py. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091,
        0.913253859286409
      ],
      "excerpt": "  python test_one_image.py \nRemember to put pretrained model in your checkpoint folder and change corresponding model name in test_one_image.py. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9333384803827206,
        0.8900430029228056
      ],
      "excerpt": "  python main.py \nCheck and modify training/testing options in main.py. Before training/testing, you need to rename the images in LLVIP dataset and put them in the designated folder. We have provided a script named rename.py to rename the images and save them in the datasets or test folder. Checkpoints are saved in ./models/densefuse_gray/. To acquire complete LLVIP dataset, please visit https://bupt-ai-cz.github.io/LLVIP/. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8318570526030742
      ],
      "excerpt": "File structure \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": "     |   \u251c\u2500\u2500train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137,
        0.8639986685036579,
        0.8639986685036579
      ],
      "excerpt": "         \u251c\u2500\u2500train \n         |   \u251c\u2500\u2500 010001.jpg \n         |   \u251c\u2500\u2500 010002.jpg \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8639986685036579,
        0.8639986685036579
      ],
      "excerpt": "             \u251c\u2500\u2500 190001.jpg \n             \u251c\u2500\u2500 190002.jpg \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8369145832357987
      ],
      "excerpt": "  We provide a script named xml2txt_yolov5.py to convert xml files to txt files, remember to modify the file path before using. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9421778616494113,
        0.9231346072251411
      ],
      "excerpt": "  python train.py --img 1280 --batch 8 --epochs 200 --data LLVIP.yaml --weights yolov5l.pt --name LLVIP_export \nSee more training options in train.py. The pretrained model yolov5l.pt can be downloaded from here. The trained model will be saved in ./runs/train/LLVIP_export/weights folder. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8907450529507533,
        0.9127205053879027
      ],
      "excerpt": "  python val.py --data --img 1280 --weights last.pt --data LLVIP.yaml \n  Remember to put the trained model in the same folder as val.py. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9381405686918235
      ],
      "excerpt": "  python train.py --dataroot ./datasets/LLVIP --name LLVIP --model pix2pix --direction AtoB --batch_size 8 --preprocess scale_width_and_crop --load_size 320 --crop_size 256 --gpu_ids 0 --n_epochs 100 --n_epochs_decay 100 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9274525367239163,
        0.8273643329699455,
        0.8458005754449075
      ],
      "excerpt": "  python test.py --dataroot ./datasets/LLVIP --name LLVIP --model pix2pix --direction AtoB --gpu_ids 0 --preprocess scale_width_and_crop --load_size 320 --crop_size 256 \n  See ./pix2pixGAN/options for more train and test options. \n<img src='imgs/LLVIP.gif' align=\"right\" width=512> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/bupt-ai-cz/LLVIP/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python",
      "Shell",
      "Dockerfile",
      "MATLAB"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 YangYun\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "LLVIP: A Visible-infrared Paired Dataset for Low-light Vision ![visitors](https://visitor-badge.glitch.me/badge?page_id=bupt-ai-cz.LLVIP)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "LLVIP",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "bupt-ai-cz",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/bupt-ai-cz/LLVIP/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 131,
      "date": "Fri, 10 Dec 2021 04:37:50 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "computer-vision",
      "visible-infrared",
      "low-light-image",
      "image-fusion",
      "object-detection",
      "cnn",
      "gan",
      "deep-learning",
      "low-light-vision",
      "image-to-image-translation",
      "iccv2021"
    ],
    "technique": "GitHub API"
  }
}