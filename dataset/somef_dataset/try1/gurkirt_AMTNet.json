{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1808.00297"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- [1] Wei Liu, et al. SSD: Single Shot MultiBox Detector. [ECCV2016]((http://arxiv.org/abs/1512.02325)).\n- [2] S. Saha, G. Singh, M. Sapienza, P. H. S. Torr, and F. Cuzzolin, Deep learning for detecting multiple space-time action tubes in videos. BMVC 2016 \n- [3] X. Peng and C. Schmid. Multi-region two-stream R-CNN for action detection. ECCV 2016\n- [4] G. Singh, S Saha, M. Sapienza, P. H. S. Torr and F Cuzzolin. Online Real time Multiple Spatiotemporal Action Localisation and Prediction. ICCV, 2017.\n- [5] Kalogeiton, V., Weinzaepfel, P., Ferrari, V. and Schmid, C., 2017. Action Tubelet Detector for Spatio-Temporal Action Localization. ICCV, 2017.\n- [Original SSD Implementation (CAFFE)](https://github.com/weiliu89/caffe/tree/ssd)\n- A huge thanks to Max deGroot, Ellis Brown for Pytorch implementation of [SSD](https://github.com/amdegroot/ssd.pytorch)\n  -->\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "If this work has been helpful in your research please consider citing [1] and [4]\n\n      @inproceedings{singh2016online,\n        title={Online Real time Multiple Spatiotemporal Action Localisation and Prediction},\n        author={Singh, Gurkirt and Saha, Suman and Sapienza, Michael and Torr, Philip and Cuzzolin, Fabio},\n        jbooktitle={ICCV},\n        year={2017}\n      }\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9998065488859755
      ],
      "excerpt": "<!-- ([Online Real-time Multiple Spatiotemporal Action Localisation and Prediction](https://arxiv.org/pdf/1611.08563.pdf)) published in ICCV 2017. --> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9872118697333594
      ],
      "excerpt": "<a href='#citation'>Citation</a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9697530585149285
      ],
      "excerpt": "    <td align=\"left\">Peng et al [3] RGB+BroxFLOW </td>  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.894110301647197
      ],
      "excerpt": "    <td align=\"left\">Saha et al [2] RGB+BroxFLOW </td>  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "    <td>36.37</td>  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9553055233818778
      ],
      "excerpt": "    <td align=\"left\">Singh et al [4] RGB+FastFLOW </td>  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8550101043698384
      ],
      "excerpt": "    <td>14.10</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9553055233818778
      ],
      "excerpt": "    <td align=\"left\">Singh et al [4] RGB+BroxFLOW </td>  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "    <td>46.30</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "    <td> 91.12 </td>   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8109194328925066
      ],
      "excerpt": "    <td>64.35</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "    <td>12.23</td> \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/gurkirt/AMTNet",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-02-22T18:13:20Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-09-15T14:31:42Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9697147961045379
      ],
      "excerpt": "An implementation of AMTNet  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9331990981542456,
        0.9538638494765423
      ],
      "excerpt": "The training and evaluation code for AMTNet is completely in PyTorch. \nWe build on Pytorch implementation of our previous work (released here ROAD) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9171688405049148
      ],
      "excerpt": "Now we use linear classification and regression heads instead of convolutional heads, because we needed that chnage for other wotk TraMNet. Efficency was linear heads are the same but there is a slight increase in GPU memory consumption. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9820616742510605
      ],
      "excerpt": "Weight of pretrained SSD used in ROAD can be dowloaded from HERE. These weights are exactly the same to those produced by SSD used in ROAD. This to reduce training time. We can achived results with imagenet pretrained models as well, but with different hyper parameter, I haven't kept the track of those hyperparameters.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9104598989276682
      ],
      "excerpt": "Here, we need 2 GPUs or 16GB VRAM, or reduce the batch size to 6 or 4 and learning rate to 0.0001. Not gurrented to reproduce same results but it will be close enough. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8291745798936508
      ],
      "excerpt": "During training checkpoint is saved every 10K iteration also log it's frame-level frame-mean-ap on a subset of 15k test images. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8739462440069292
      ],
      "excerpt": "To evaluate on optical flow models \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8281982630693765
      ],
      "excerpt": "NOTE: I01onlineTubes and I02genFusedTubes not only produce video-level mAP; they also produce video-level classification accuracy on 24 classes of UCF24. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.915155772541204,
        0.9955316701809707,
        0.9531191821363978
      ],
      "excerpt": "but their ap computation from precision and recall is slightly different. \nThe table below is similar to table 1 in our paper. It contains more info than \nthat in the paper, mostly about this implementation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9709809087059526
      ],
      "excerpt": "There is an effect due to the choice of learning rate and the number of iterations the model is trained. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.901478540444451,
        0.9860976308411931
      ],
      "excerpt": "lower IoU threshold, which is done in this case. \nIn original work using caffe implementation of SSD, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9272859491121677,
        0.8734078151571438,
        0.8379210228862942,
        0.8220665962782716,
        0.916210305124579,
        0.8003220580808791
      ],
      "excerpt": "In this implementation, all the models are trained for 120K \niterations, the initial learning rate is set to 0.0005 and learning is dropped by the factor of 5 after 70K and 90K iterations. \nKalogeiton et al. [5] make use mean fusion, so I thought we could try in our pipeline which was very easy to incorporate. \nIt is evident from above table that mean fusion performs better than other fusion techniques. \nAlso, their method relies on multiple frames as input in addition to post-processing of bounding box coordinates at tubelet level. \nThis implementation is mainly focused on producing the best numbers (mAP) in the simplest manner, it can be modified to run faster. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9225930060956147
      ],
      "excerpt": " - Most of the time spent during tube generations is taken by disc operations; which can be eliminated completely. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8315343901356778,
        0.8512885411396205
      ],
      "excerpt": "I presented the timing of individual components in the paper, which still holds true. \nThanks to Zhujiagang, a matlab version of \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Action Micro Tube Network (AMTNet) - Pytorch with linear heads",
      "technique": "GitHub API"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Currently, we provide the following PyTorch models: \n    * SSD300 trained on ucf24 ; available from my [google drive](https://drive.google.com/drive/folders/1Z42S8fQt4Amp1HsqyBOoHBtgVKUzJuJ8?usp=sharing)\n      - appearence model trained on rgb-images (named `rgb-ssd300_ucf24_120000`)\n      - accurate flow model trained on brox-images (named `brox-ssd300_ucf24_120000`)\n      - real-time flow model trained on fastOF-images (named `fastOF-ssd300_ucf24_120000`)    \n- These models can be used to reproduce above table which is almost identical in our [paper](https://arxiv.org/pdf/1611.08563.pdf) \n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/gurkirt/AMTNet/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Thu, 09 Dec 2021 03:50:00 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/gurkirt/AMTNet/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "gurkirt/AMTNet",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Install [PyTorch](http://pytorch.org/)(version v1.0 as of on March 2019) by selecting your environment on the website and running the appropriate command.\n- Please install cv2 and visdom form conda-forge. \n- I recommend using anaconda 3.7. \n- You will also need Matlab. If you have distributed computing license then it would be faster otherwise it should also be fine. \nJust replace `parfor` with simple `for` in Matlab scripts. I would be happy to accept a PR for python version of this part.\n- Clone this repository. \n  * Note: We currently only support Python 3.7 with Pytorch version v1.0 on Linux system.\n- We currently support [UCF24](http://www.thumos.info/download.html) with [revised annotaions](https://github.com/gurkirt/corrected-UCF101-Annots) released with our [real-time online action detection paper](https://arxiv.org/pdf/1611.08563.pdf). Unlike [ROAD](https://github.com/gurkirt/realtime-action-detection) implementation, we support [JHMDB21](http://jhmdb.is.tue.mpg.de/) as well.\n- Similar to [ROAD](https://github.com/gurkirt/realtime-action-detection) setup, to simulate the same training and evaluation setup we provide extracted `rgb` images from videos along with optical flow images (both `brox flow` and `real-time flow`) computed for the UCF24 and JHMDB21 datasets.\nYou can download it from my [google drive link](https://drive.google.com/drive/folders/1o0JNYZl2Wv9bi66wF_SQ4N5cxdCyHTJR?usp=sharing))\n- Install opencv package for anaconda using ``conda install opencv``\n- We also support [Visdom](https://github.com/facebookresearch/visdom) for visualization of loss and frame-meanAP on validation subset during training.\n  * To use Visdom in the browser: \n  ```Shell\n  #: First install Python server and client \n  conda install -c conda-forge visdom\n  #: Start the server (probably in a screen or tmux)\n  python -m visdom.server --port=8097\n  ```\n  * Then (during training) navigate to http://localhost:8097/ (see the Training section below for more details).\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9287467277036058
      ],
      "excerpt": "<a href='#installation'>Installation</a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8443510074470136
      ],
      "excerpt": "Similar to ROAD, we requires VGG-16 weights pretrained on UCF24 using ROAD implmentation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8931493914228835
      ],
      "excerpt": "If you want you can train for these weights using ROAD \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.947140050562409
      ],
      "excerpt": "OR download the above pretrained models to above directory.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.972816952305987
      ],
      "excerpt": "You can use --fusion_type=CAT for concatnation fusion. Sum Fusion requires little less GPU memory.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9574979005657257,
        0.8773952612615806
      ],
      "excerpt": "For instructions on Visdom usage/installation, see the <a href='#installation'>Installation</a> section. By default, it is off. \nIf you don't like to use visdom then you always keep track of train using logfile which is saved under save_root directory \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8381552309669075
      ],
      "excerpt": "To compute frame-mAP you can use frameAP.m script. You will need to specify data_root, data_root. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9267542305056462
      ],
      "excerpt": " - NMS is performed once in python then again in Matlab; one has to do that on GPU in python \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8766527512859791
      ],
      "excerpt": "Also, Feynman27 pushed a python version of the incremental_linking \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8174540907975313
      ],
      "excerpt": "<a href='#training-ssd'>Training SSD</a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8641911858853529
      ],
      "excerpt": "To train AMTNet using the training script simply specify the parameters listed in train.py as a flag or manually change them in script. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9048130897244435
      ],
      "excerpt": "python train.py --seq_len=2 --num_workers=4 --batch_size=8 --ngpu=2 --fusion_type=NONE --input_type_base=rgb --input_frames_base=1 --lr=0.0005 --max_iter=70000 --stepvalues=50000 --val_step=10000 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9127007748945514
      ],
      "excerpt": "python train.py --seq_len=2 --num_workers=4 --batch_size=8 --ngpu=2 --fusion_type=NONE --input_type_base=brox --input_frames_base=5 --lr=0.0005 --max_iter=70000 --stepvalues=50000 --val_step=10000 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8140540986695102
      ],
      "excerpt": "OR download the above pretrained models to above directory.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8981628774657899
      ],
      "excerpt": "python train.py --seq_len=2 --num_workers=4 --batch_size=8 --ngpu=2 --fusion_type=SUM --input_type_base=rgb --input_type_extra=brox --input_frames_base=1 --input_frames_extra=5 --lr=0.0005 --max_iter=70000 --stepvalues=50000 --val_step=10000 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8560616667916674
      ],
      "excerpt": "To eval SSD using the test script simply specify the parameters listed in test-ucf24.py as a flag or manually change them. for e.g.: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.811854372964597
      ],
      "excerpt": "    <td>15.86</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8641699875385698
      ],
      "excerpt": "    <td>75.01</td> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/gurkirt/AMTNet/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other",
      "url": "https://raw.githubusercontent.com/gurkirt/AMTNet/master/LICENSE"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 Gurkirt Singh\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n\\nThis is adaption of Max deGroot, Ellis Brown originl code of SSD on VOC dataset and upgraded to pytorch 1'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "AMTNet: action-micro-tube-network",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "AMTNet",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "gurkirt",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/gurkirt/AMTNet/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 4,
      "date": "Thu, 09 Dec 2021 03:50:00 GMT"
    },
    "technique": "GitHub API"
  }
}