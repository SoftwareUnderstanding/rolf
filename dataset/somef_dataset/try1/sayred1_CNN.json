{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1412.6980.\n\nThe whole network was reformatted since the original was messy, and it wan't clear how to run several epochs with the model.\n\nCurrently, the NN is running, but the cost isn't decreasing rapidly enough. This could be due to several reasons that I will test.\n1"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/sayred1/CNN",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-06-25T19:10:21Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-07-18T02:48:03Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Convolutional Neural Network built w/ Numpy",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/sayred1/CNN/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 09 Dec 2021 13:17:16 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/sayred1/CNN/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "sayred1/CNN",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/sayred1/CNN/master/.ipynb_checkpoints/Gradient%20Numerical%20Approximations-checkpoint.ipynb",
      "https://raw.githubusercontent.com/sayred1/CNN/master/.ipynb_checkpoints/CNN%20with%20Numpy-checkpoint.ipynb",
      "https://raw.githubusercontent.com/sayred1/CNN/master/test/Forward%20Pass.ipynb",
      "https://raw.githubusercontent.com/sayred1/CNN/master/cnn/CNN%20with%20Numpy.ipynb"
    ],
    "technique": "File Exploration"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/sayred1/CNN/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "To run, first clone the cnn directory",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "CNN",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "sayred1",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/sayred1/CNN/blob/master/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\n#: load the data\n(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\ntrain_images = train_images.reshape((1, 28, 28, 60000)) #: (channels, rows, cols, imgs)\ntest_images = test_images.reshape((1, 28, 28, 10000)) #: (channels, rows, cols, imgs)\n```\n```python\n#: normalize the image pixel values\ntrain_images, test_images = train_images / 255.0, test_images / 255.0\n```\n\n```python\n#: initialize and store forward parameters, and first and second moments of gradient for optimization\n\"\"\"\nkn = conv. kernel\nwn = fc. weight\n\"\"\"\n\n\n#: forward parameters\nk1 = np.random.randn(32, 1, 3, 3)\nk2 = np.random.randn(64, 32, 3, 3)\nw3 = np.random.randn(64, 1600) * 0.01\nw4 = np.random.randn(10, 64) * 0.01\n\nb1 = np.zeros((k1.shape[0],1))\nb2 = np.zeros((k2.shape[0],1))\nb3 = np.zeros((w3.shape[0],1))\nb4 = np.zeros((w4.shape[0],1))\n\n#: optimization moments\nv1 = np.zeros(k1.shape)\nm1 = np.zeros(k1.shape)\nbv1 = np.zeros(b1.shape)\nbm1 = np.zeros(b1.shape)\n\n...\n\nv4 = np.zeros(w4.shape)\nm4 = np.zeros(w4.shape)\nbv4 = np.zeros(b4.shape)\nbm4 = np.zeros(b4.shape)\n\nparams = [k1, k2, w3, w4, b1, b2, b3, b4]\nmoments = [v1,m1,bv1,bm1,v2,m2,bv2,bm2,v3,m3,bv3,bm3,v4,m4,bv4,bm4]\n```\n\n```python\n#: Specify number of epochs, and train over specified batch/batches (here I train over a single batch only)\n\ncost = []       #: cost per epoch\nnumEpochs = 10  \nnumLabels = 10   \nbatchSize = 10\nY = np.zeros((batchSize,numLabels,1))\n\n#: for each image in batch, one iteration = forward, backward, and optimization\n_iter = 0 \nfor epoch in range(numEpochs):\n    cost_ = 0 #: average cost per iteration\n    for img in range(batchSize):\n        _iter += 1\n        Y[img,train_labels[img]] = 1.                                         #: one hot vector labels\n        image, label = train_images[:,:,:,img],Y[img]\n        loss, fp = myCNN().forwardPass(image, label, params)                     #: this returns the loss and forward pass\n        grads =  myCNN().backwardPass(params, loss, fp, image, label)            #: this returns the gradiets w.r.t the loss\n        cost_ += loss\n        print(\"iteration \", _iter)\n        if (img+1) % batchSize == 0:\n            print(\"now optimizing: epoch \", epoch+1)\n            params = myCNN().optimize(0.0001, 0.9, 0.999, 1E-7, moments, grads, params, _iter, batchSize)\n            cost_ = cost_/batchSize \n            print(\"average cost: \", cost_)\n            cost.append(cost_)\n```\n\nInformation on the implementation of forward, backward, and optimization was obtained at: https://github.com/Alescontrela/Numpy-CNN/tree/master/CNN, https://github.com/Kulbear/deep-learning-coursera/blob/master/Convolutional%20Neural%20Networks/Convolution%20model%20-%20Step%20by%20Step%20-%20v1.ipynb, https://arxiv.org/abs/1412.6980.\n\nThe whole network was reformatted since the original was messy, and it wan't clear how to run several epochs with the model.\n\nCurrently, the NN is running, but the cost isn't decreasing rapidly enough. This could be due to several reasons that I will test.\n1) Need more batches of data instead of one batch of 10. (going to run full dataset on cms)\n2) Need to tune the optimization hyperparameters.\n3) Optimization algorithm incorrectly updating parameters.\n4) Backprop or forward prop wrong (both checked with the above links).\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Thu, 09 Dec 2021 13:17:16 GMT"
    },
    "technique": "GitHub API"
  }
}