{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1801.04062\n\n[2] Sebastian Nowozin, Botond Cseke, Ryota Tomioka, f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization, https://arxiv.org/abs/1606.00709\n\n[3] Aaron van den Oord, Yazhe Li, Oriol Vinyals, Representation Learning with Contrastive Predictive Coding, https://arxiv.org/abs/1807.03748",
      "https://arxiv.org/abs/1606.00709\n\n[3] Aaron van den Oord, Yazhe Li, Oriol Vinyals, Representation Learning with Contrastive Predictive Coding, https://arxiv.org/abs/1807.03748",
      "https://arxiv.org/abs/1807.03748"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[1] Ishmael  Belghazi,  Sai  Rajeswar,  Aristide  Baratin,  R.  Devon  Hjelm,  and  Aaron  C.Courville.   MINE:  mutual  information  neural  estimation.CoRR,  https://arxiv.org/abs/1801.04062\n\n[2] Sebastian Nowozin, Botond Cseke, Ryota Tomioka, f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization, https://arxiv.org/abs/1606.00709\n\n[3] Aaron van den Oord, Yazhe Li, Oriol Vinyals, Representation Learning with Contrastive Predictive Coding, https://arxiv.org/abs/1807.03748\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "      dim_x = 10 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "      dim = 10 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mboudiaf/Mutual-Information-Variational-Bounds",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-10-21T18:55:11Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-09-25T04:46:33Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9378591082055604,
        0.8245725181791306
      ],
      "excerpt": "[ ] Include more architectures for the critic network \n[ ] More comments in functions \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9479287173276514
      ],
      "excerpt": "Throughout this repo, I offer a ready-to-use implementation of state-of-the-art variational methods for mutual information estimation in Tensorflow. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9774727398656498,
        0.9774727398656498
      ],
      "excerpt": "  * MINE estimator [1], based on the Donsker-Varadhan representation of the KL divergence \n  * NWJ estimator [2], based on the f-divergence variational representation of the KL divergence \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.843480925430926
      ],
      "excerpt": "For use in a Tensorflow graph (as a regulazation term for instance). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9593726201539364
      ],
      "excerpt": "In the case of correlated Gaussian random variables: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8377064149701315
      ],
      "excerpt": "We have access to the mutual information: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9151659687437302,
        0.98591844361411
      ],
      "excerpt": "The plots above present the estimated MI after 10 epochs of training on a 100k samples dataset with batch size 128, for the three estimation methods. \nThe most interesting use case of these bounds is in the context of mutual information maximization. A typical example is reduction of mode collapse in GANs. In the context of GANs, the mutual information I(Z;X) is used as a proxy for the entropy of the generator H(X), where X represents the output of the generator, and Z the noise vector. The maximization of I(X;Z) results in the maximization of H(X). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A Tensorflow implementation Mutual Information estimation methods",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mboudiaf/Mutual-Information-Variational-Bounds/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 4,
      "date": "Sun, 05 Dec 2021 20:33:42 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/mboudiaf/Mutual-Information-Variational-Bounds/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "mboudiaf/Mutual-Information-Variational-Bounds",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8837680365796365
      ],
      "excerpt": "  python \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8164589194817808
      ],
      "excerpt": "  #: Then update estimator  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8795091316982344
      ],
      "excerpt": "cd examples/correlated_gaussians/ \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8801854956928516
      ],
      "excerpt": "      from estimator import Mi_estimator \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9457175861910134
      ],
      "excerpt": "      import numpy as np \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.811854372964597,
        0.8737288687529231,
        0.8737288687529231
      ],
      "excerpt": "      dim_z = 15 \n      x_data = np.random.rand(data_size, dim_x) \n      z_data = np.random.rand(data_size, dim_z) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8044290599680651
      ],
      "excerpt": "                                 batch_size= 128, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.925671696398174
      ],
      "excerpt": "      import tensorflow as tf \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8689405296184965,
        0.8689405296184965
      ],
      "excerpt": "      x = tf.placeholder(shape=[batch_size, dim], tf.float32) \n      z = tf.placeholder(shape=[batch_size, dim], tf.float32) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8044290599680651
      ],
      "excerpt": "                                     batch_size= 128, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8101128730661935
      ],
      "excerpt": "  #: Define main training op \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8008331685760428
      ],
      "excerpt": "  sess.run([main_train_op], feed_dict={x: ..., z= ..., ...}) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8008331685760428
      ],
      "excerpt": "  sess.run(estimator_train_op, feed_dict={x: ..., z= ...}) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8946090228846771,
        0.8280991473864786
      ],
      "excerpt": "First go the example directory \ncd examples/correlated_gaussians/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991,
        0.8142563756724722
      ],
      "excerpt": "python3 run_test.py \nFinally, to run the experiments, you can check all the available options in \"demo_gaussian.py\", and loop over any parameters by modifying the header of run_exp.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991
      ],
      "excerpt": "python3 run_exp.py \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/mboudiaf/Mutual-Information-Variational-Bounds/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Yet to do",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Mutual-Information-Variational-Bounds",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "mboudiaf",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/mboudiaf/Mutual-Information-Variational-Bounds/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 27,
      "date": "Sun, 05 Dec 2021 20:33:42 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We provide code to showcase the two functionalities we just talked about\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "A naive differentiation of the previously defined loss_regularized term will probably fail to yield the expected result. A very probable reason for that is that I(X;Z) will most likely have gradients whose scale are much larger than those from the loss_gan. In order to maintain a certain balance in gradients' scale, on must use adaptive gradient clipping proposed in [1]. This simple trick consists in scaling the MI term such that its gradient norm doesn't exceed the one from the original loss. Concretely, one must instead minimize:\n\n```\nscale = min( ||G(loss)||, ||G(I(X;Z))|| ) / ||G(I(X;Z))||\nloss_regularized = loss_gan - beta * scale * I(X;Z)\n```\nwhere G(.) defines the gradient operator with respect to specified parameters.\n\nWe provide a simple example in 2D referred to as \"25 gaussians experiments\" where the target distribution is:\n\n<img src=\"https://github.com/mboudiaf/Mutual-Information-Variational-Bounds/blob/master/screens/gan_target.png\" width=\"250\">\n\nThe simple GAN will produce, with the provided generator and discriminator architecture distributions like:\n\n<img src=\"https://github.com/mboudiaf/Mutual-Information-Variational-Bounds/blob/master/screens/gan _noreg.png\" width=\"250\">\n\nWhile the above plot clearly exposes a mode collapse, one can easily reduce this mode collapse by adding a MI regularization:\n\n<img src=\"https://github.com/mboudiaf/Mutual-Information-Variational-Bounds/blob/master/screens/gan_mine.png\" width=\"250\">\n\nTo see the code for this example, first go the example directory\n```\ncd examples/gan/\n```\nThen, run some tests to make sure the code doesn't yield any bug:\n```\npython3 run_test.py\n```\nFinally, to run the experiments, you can check all the available options in \"demo_gaussian.py\", and loop over any parameters by modifying the header of run_exp.py.\nThen simply run:\n```\npython3 run_exp.py\n```\n\n\n",
      "technique": "Header extraction"
    }
  ]
}