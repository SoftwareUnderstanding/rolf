{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2011.06993"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please cite [the following paper](https://www.aclweb.org/anthology/C18-1139/) when using Flair embeddings:\n\n```\n@inproceedings{akbik2018coling,\n  title={Contextual String Embeddings for Sequence Labeling},\n  author={Akbik, Alan and Blythe, Duncan and Vollgraf, Roland},\n  booktitle = {{COLING} 2018, 27th International Conference on Computational Linguistics},\n  pages     = {1638--1649},\n  year      = {2018}\n}\n```\n\nIf you use the Flair framework for your experiments, please cite [this paper](https://www.aclweb.org/anthology/papers/N/N19/N19-4010/):\n\n```\n@inproceedings{akbik2019flair,\n  title={FLAIR: An easy-to-use framework for state-of-the-art NLP},\n  author={Akbik, Alan and Bergmann, Tanja and Blythe, Duncan and Rasul, Kashif and Schweter, Stefan and Vollgraf, Roland},\n  booktitle={{NAACL} 2019, 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)},\n  pages={54--59},\n  year={2019}\n}\n```\n\nIf you use our new \"FLERT\" models or approach, please cite [this paper](https://arxiv.org/abs/2011.06993):\n\n```\n@misc{schweter2020flert,\n    title={FLERT: Document-Level Features for Named Entity Recognition},\n    author={Stefan Schweter and Alan Akbik},\n    year={2020},\n    eprint={2011.06993},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n```\n\nIf you use our TARS approach for few-shot and zero-shot learning, please cite [this paper](https://kishaloyhalder.github.io/pdfs/tars_coling2020.pdf/):\n\n```\n@inproceedings{halder2020coling,\n  title={Task Aware Representation of Sentences for Generic Text Classification},\n  author={Halder, Kishaloy and Akbik, Alan and Krapac, Josip and Vollgraf, Roland},\n  booktitle = {{COLING} 2020, 28th International Conference on Computational Linguistics},\n  year      = {2020}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{schweter2020flert,\n    title={FLERT: Document-Level Features for Named Entity Recognition},\n    author={Stefan Schweter and Alan Akbik},\n    year={2020},\n    eprint={2011.06993},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\nIf you use our TARS approach for few-shot and zero-shot learning, please cite this paper:\n@inproceedings{halder2020coling,\n  title={Task Aware Representation of Sentences for Generic Text Classification},\n  author={Halder, Kishaloy and Akbik, Alan and Krapac, Josip and Vollgraf, Roland},\n  booktitle = {{COLING} 2020, 28th International Conference on Computational Linguistics},\n  year      = {2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{akbik2019flair,\n  title={FLAIR: An easy-to-use framework for state-of-the-art NLP},\n  author={Akbik, Alan and Bergmann, Tanja and Blythe, Duncan and Rasul, Kashif and Schweter, Stefan and Vollgraf, Roland},\n  booktitle={{NAACL} 2019, 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)},\n  pages={54--59},\n  year={2019}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{akbik2018coling,\n  title={Contextual String Embeddings for Sequence Labeling},\n  author={Akbik, Alan and Blythe, Duncan and Vollgraf, Roland},\n  booktitle = {{COLING} 2018, 27th International Conference on Computational Linguistics},\n  pages     = {1638--1649},\n  year      = {2018}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9988200434175074
      ],
      "excerpt": "If you're interested in doing NLP/ML research to pursue a PhD and love open source, consider applying to open positions for research associates and PhD candidates at Humboldt University Berlin! We currently have three open positions with application deadlines soon! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9302785107180478,
        0.9984621609422,
        0.9902272622380456,
        0.9902272622380456,
        0.9902272622380456
      ],
      "excerpt": "| English | Conll-03 (4-class)   |  94.09  | 94.3 (Yamada et al., 2020) | Flair English 4-class NER demo  | \n| English | Ontonotes (18-class)  |  90.93  | 91.3 (Yu et al., 2020) | Flair English 18-class NER demo | \n| German  | Conll-03 (4-class)   |  92.31  | 90.3 (Yu et al., 2020) | Flair German 4-class NER demo  | \n| Dutch  | Conll-03  (4-class)  |  95.25  | 93.7 (Yu et al., 2020) | Flair Dutch 4-class NER demo  | \n| Spanish  | Conll-03 (4-class)   |  90.54 | 90.3 (Yu et al., 2020) | Flair Spanish 4-class NER demo  | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeOfConduct": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://raw.githubusercontent.com/zalandoresearch/flair/master/CODE_OF_CONDUCT.md",
    "technique": "File Exploration"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/flairNLP/flair",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please email your questions or comments to [Alan Akbik](http://alanakbik.github.io/).\n\n",
      "technique": "Header extraction"
    }
  ],
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contributing to Flair\nWe are happy to accept your contributions to make flair better and more awesome! To avoid unnecessary work on either \nside, please stick to the following process:\n\nCheck if there is already an issue for your concern.\nIf there is not, open a new one to start a discussion. We hate to close finished PRs!\nIf we decide your concern needs code changes, we would be happy to accept a pull request. Please consider the \ncommit guidelines below.\n\nIn case you just want to help out and don't know where to start, \nissues with \"help wanted\" label are good for \nfirst-time contributors. \nGit Commit Guidelines\nIf there is already a ticket, use this number at the start of your commit message. \nUse meaningful commit messages that described what you did.\nExample: GH-42: Added new type of embeddings: DocumentEmbedding. \nRunning unit tests locally\nFor contributors looking to get deeper into the API we suggest cloning the repository and checking out the unit\ntests for examples of how to call methods. Nearly all classes and methods are documented, so finding your way around\nthe code should hopefully be easy.\nYou need Pipenv for this:\nbash\npipenv install --dev &amp;&amp; pipenv shell\npytest tests/\nTo run integration tests execute:\nbash\npytest --runintegration tests/\nThe integration tests will train small models.\nAfterwards, the trained model will be loaded for prediction.\nTo also run slow tests, such as loading and using the embeddings provided by flair, you should execute:\nbash\npytest --runslow tests/",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-06-11T11:04:18Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-13T16:24:46Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9817075810140062
      ],
      "excerpt": "Flair ships with state-of-the-art models for a range of NLP tasks. For instance, check out our latest NER models:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9412399365930009
      ],
      "excerpt": "Thanks for your interest in contributing! There are many ways to get involved; \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8412072194441083
      ],
      "excerpt": "check these open issues for specific tasks. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A very simple framework for state-of-the-art Natural Language Processing (NLP)",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/zalandoresearch/flair/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1783,
      "date": "Mon, 13 Dec 2021 17:26:08 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/flairNLP/flair/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "flairNLP/flair",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/zalandoresearch/flair/tree/master/resources/docs"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The project is based on PyTorch 1.5+ and Python 3.6+, because method signatures and type hints are beautiful.\nIf you do not have Python 3.6, install it first. [Here is how for Ubuntu 16.04](https://vsupalov.com/developing-with-python3-6-on-ubuntu-16-04/).\nThen, in your favorite virtual environment, simply do:\n\n```\npip install flair\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/flairNLP/flair/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other",
      "url": "https://raw.githubusercontent.com/zalandoresearch/flair/master/LICENSE"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'The MIT License (MIT)\\n\\nFlair is licensed under the following MIT License (MIT) Copyright \\xc2\\xa9 2018 Zalando SE, https://tech.zalando.com\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \\xe2\\x80\\x9cSoftware\\xe2\\x80\\x9d), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \\xe2\\x80\\x9cAS IS\\xe2\\x80\\x9d, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Join Us: Open Positions at HU-Berlin!",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "flair",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "flairNLP",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/flairNLP/flair/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "alanakbik",
        "body": "This release adds several new features such as in-built \"model cards\" for all Flair models, the first pre-trained models for Relation Extraction, better support for fine-tuning and a refactoring of the model training methods for more flexibility. It also fixes a number of critical bugs that were introduced by the refactorings in Flair 0.9.\r\n\r\n# Model Trainer Enhancements\r\n\r\n_Breaking change_: We changed the `ModelTrainer` such that you now no longer pass the optimizer during initialization. Rather, it is now passed as a parameter of the `train` or `fine_tune` method.\r\n\r\n**Old syntax**:\r\n\r\n```python\r\n# 1. initialize trainer with AdamW optimizer\r\ntrainer = ModelTrainer(classifier, corpus, optimizer=torch.optim.AdamW)\r\n\r\n# 2. run training with small learning rate and mini-batch size\r\ntrainer.train('resources/taggers/question-classification-with-transformer',\r\n              learning_rate=5.0e-5,\r\n              mini_batch_size=4,\r\n             )\r\n```\r\n\r\n**New syntax** (optimizer is parameter of train method):\r\n\r\n```python\r\n# 1. initialize trainer \r\ntrainer = ModelTrainer(classifier, corpus)\r\n\r\n# 2. run training with AdamW, small learning rate and mini-batch size\r\ntrainer.train('resources/taggers/question-classification-with-transformer',\r\n              learning_rate=5.0e-5,\r\n              mini_batch_size=4,\r\n              optimizer=torch.optim.AdamW,\r\n             )\r\n```\r\n\r\n## Convenience function for fine-tuning (#2439) \r\n\r\nAdds a `fine_tune` routine that sets default parameters used for fine-tuning (AdamW optimizer, small learning rate, few epochs, cyclic learning rate scheduling, etc.). Uses the new linear scheduler with warmup (#2415).\r\n\r\n**New syntax** with `fine_tune` method: \r\n\r\n```python\r\nfrom flair.data import Corpus\r\nfrom flair.datasets import TREC_6\r\nfrom flair.embeddings import TransformerDocumentEmbeddings\r\nfrom flair.models import TextClassifier\r\nfrom flair.trainers import ModelTrainer\r\n\r\n# 1. get the corpus\r\ncorpus: Corpus = TREC_6()\r\n\r\n# 2. what label do we want to predict?\r\nlabel_type = 'question_class'\r\n\r\n# 3. create the label dictionary\r\nlabel_dict = corpus.make_label_dictionary(label_type=label_type)\r\n\r\n# 4. initialize transformer document embeddings (many models are available)\r\ndocument_embeddings = TransformerDocumentEmbeddings('distilbert-base-uncased', fine_tune=True)\r\n\r\n# 5. create the text classifier\r\nclassifier = TextClassifier(document_embeddings, label_dictionary=label_dict, label_type=label_type)\r\n\r\n# 6. initialize trainer\r\ntrainer = ModelTrainer(classifier, corpus)\r\n\r\n# 7. run training with fine-tuning\r\ntrainer.fine_tune('resources/taggers/question-classification-with-transformer',\r\n                  learning_rate=5.0e-5,\r\n                  mini_batch_size=4,\r\n                  )\r\n```\r\n\r\n## Model Cards (#2457) \r\n\r\nWhen you train any Flair model, a \"model card\" will now automatically be saved that stores all training parameters and versions used to train this model. Later when you load a Flair model, you can print the model card and understand how the model was trained.\r\n\r\nThe following example trains a small POS-tagger and prints the model card in the end: \r\n\r\n```python\r\n# initialize corpus and make label dictionary for POS tags\r\ncorpus = UD_ENGLISH().downsample(0.01)\r\ntag_type = \"pos\"\r\ntag_dictionary = corpus.make_label_dictionary(tag_type)\r\n\r\n# simple sequence tagger\r\ntagger = SequenceTagger(hidden_size=256,\r\n                        embeddings=WordEmbeddings(\"glove\"),\r\n                        tag_dictionary=tag_dictionary,\r\n                        tag_type=tag_type)\r\n\r\n# initialize model trainer and experiment path\r\ntrainer = ModelTrainer(tagger, corpus)\r\npath = f'resources/taggers/model-card'\r\n\r\n# train for a few epochs\r\ntrainer.train(path,\r\n              max_epochs=20,\r\n              )\r\n\r\n# load best model and print \"model card\"\r\ntrained_model = SequenceTagger.load(path + '/best-model.pt')\r\ntrained_model.print_model_card()\r\n```\r\n\r\nThis should print a model card like: \r\n~~~\r\n------------------------------------\r\n--------- Flair Model Card ---------\r\n------------------------------------\r\n- this Flair model was trained with:\r\n-- Flair version 0.9\r\n-- PyTorch version 1.7.1\r\n-- Transformers version 4.8.1\r\n------------------------------------\r\n------- Training Parameters: -------\r\n------------------------------------\r\n-- base_path = resources/taggers/model-card\r\n-- learning_rate = 0.1\r\n-- mini_batch_size = 32\r\n-- mini_batch_chunk_size = None\r\n-- max_epochs = 20\r\n-- train_with_dev = False\r\n-- train_with_test = False\r\n[... shortened ...]\r\n------------------------------------\r\n~~~\r\n\r\n## Resume training any model (#2457)\r\n\r\nPreviously, we distinguished between checkpoints and model files. Now all models can function as checkpoints, meaning you can load them and continue training them. Say you want to load the model above (trained to epoch 20) and continue training it to epoch 25. Do it like this: \r\n\r\n```python\r\n# resume training best model, but this time until epoch 25\r\ntrainer.resume(trained_model,\r\n               base_path=path + '-resume',\r\n               max_epochs=25,\r\n               )\r\n``` \r\n\r\n## Pass optimizer and scheduler instance \r\n\r\nYou can also now pass an initialized optimizer and scheduler to the train and fine_tune methods. \r\n\r\n# Multi-Label Predictions and Confidence Threshold in TARS models (#2430)\r\n\r\nAdding the possibility to set confidence thresholds on multi-label prediction in TARS, and setting whether a problem is single-label or multi-label:\r\n\r\n```python\r\nfrom flair.models import TARSClassifier\r\nfrom flair.data import Sentence\r\n\r\n# 1. Load our pre-trained TARS model for English\r\ntars: TARSClassifier = TARSClassifier.load('tars-base')\r\n\r\n# switch to a multi-label task (emotion detection)\r\ntars.switch_to_task('GO_EMOTIONS')\r\n\r\n# sentence with two emotions\r\nsentence = Sentence(\"I am happy and sad\")\r\n\r\n# predict normally\r\ntars.predict(sentence)\r\nprint(sentence)\r\n\r\n# predict with lower label threshold (you can set this to 0. to get all labels)\r\ntars.predict(sentence, label_threshold=0.01)\r\nprint(sentence)\r\n\r\n# predict and enforce a single-label prediction\r\ntars.predict(sentence, label_threshold=0.01, multi_label=False)\r\nprint(sentence)\r\n```\r\n\r\n# Relation Extraction ( #2471 #2492)\r\n\r\nWe refactored the RelationExtractor for more options, hopefully better code clarity and small speed improvements. \r\n\r\nWe also added two few relation extraction models, trained over a modified version of TACRED: `relations` and `relations-fast`. To use these models, you also need an entity tagger. The tagger identifies entities, then the relation extractor possible entities. \r\n\r\nFor instance use this code: \r\n\r\n```python\r\nfrom flair.data import Sentence\r\nfrom flair.models import RelationExtractor, SequenceTagger\r\n\r\n# 1. make example sentence\r\nsentence = Sentence(\"George was born in Washington\")\r\n\r\n# 2. load entity tagger and predict entities\r\ntagger = SequenceTagger.load('ner-fast')\r\ntagger.predict(sentence)\r\n\r\n# check which entities have been found in the sentence\r\nentities = sentence.get_labels('ner')\r\nfor entity in entities:\r\n    print(entity)\r\n\r\n# 3. load relation extractor\r\nextractor: RelationExtractor = RelationExtractor.load('relations-fast')\r\n\r\n# predict relations\r\nextractor.predict(sentence)\r\n\r\n# check which relations have been found\r\nrelations = sentence.get_labels('relation')\r\nfor relation in relations:\r\n    print(relation)\r\n```\r\n\r\n\r\n# Embeddings \r\n\r\n- Refactoring of WordEmbeddings to avoid gensim version issues and enable further fine-tuning of pre-trained embeddings (#2491)\r\n- Refactoring of OneHotEmbeddings to fix errors caused by some corpora and enable \"stable embeddings\" (#2490 )\r\n\r\n\r\n# Other Enhancements and Bug Fixes \r\n\r\n- Compatibility with gensim 4 and Python 3.9 (#2496)\r\n- Fix TransformerWordEmbeddings if model_max_length not set in Tokenizer (#2502) \r\n- Fix TransformerWordEmbeddings handling of lang ids (#2417)  \r\n- Fix attention mask for special Transformer architectures (#2485)\r\n- Fix regression model (#2424) \r\n- Fix problems caused by refactoring of Dictionary (#2429 #2435 #2453) \r\n- Fix infinite loop in Span::to_original_text (#2462)\r\n- Fix result object in ModelTrainer (#2519) \r\n- Fix bug in wsd_ufsac corpus (#2521)  \r\n- Fix bugs in TARS and simple sequence tagger (#2468) \r\n- Add Amharic FLAIR EMBEDDING model (#2494) \r\n- Add MultiCoNer Dataset (#2507) \r\n- Add Korean Flair Tutorials (#2516 #2517) \r\n- Remove hyperparameter features (#2518) \r\n- Make it optional to create logfiles and loss files (#2421)  \r\n- Small simplification of TransformerWordEmbeddings (#2425)  ",
        "dateCreated": "2021-11-18T08:32:41Z",
        "datePublished": "2021-11-18T08:33:59Z",
        "html_url": "https://github.com/flairNLP/flair/releases/tag/v0.10",
        "name": "Release 0.10",
        "tag_name": "v0.10",
        "tarball_url": "https://api.github.com/repos/flairNLP/flair/tarball/v0.10",
        "url": "https://api.github.com/repos/flairNLP/flair/releases/50676263",
        "zipball_url": "https://api.github.com/repos/flairNLP/flair/zipball/v0.10"
      },
      {
        "authorType": "User",
        "author_name": "alanakbik",
        "body": "With release 0.9 we are refactoring Flair for simplicity and speed, to make Flair faster and more easily scale to new NLP tasks. The first new tasks included in this release are **Relation Extraction** (RE), support for **GLUE benchmark** tasks and **Entity Linking** - all in *beta for early adopters*! We're working towards a Flair 1.0 release that will span the whole suite of standard NLP tasks. Also included is a new approach for **Zero-Shot Sequence Labeling** based on TARS! This release also includes a wealth of new datasets for all these tasks and tons of other new features and bug fixes.\r\n\r\n## Zero-Shot Sequence Labeling with TARS (#2260) \r\n\r\nWe extend the TARS zero-shot learning approach to sequence labeling and ship a pre-trained model for English NER. Try defining some classes and see if the model can find them: \r\n\r\n```python\r\n# 1. Load zero-shot NER tagger\r\ntars = TARSTagger.load('tars-ner')\r\n\r\n# 2. Prepare some test sentences\r\nsentences = [\r\n    Sentence(\"The Humboldt University of Berlin is situated near the Spree in Berlin, Germany\"),\r\n    Sentence(\"Bayern Munich played against Real Madrid\"),\r\n    Sentence(\"I flew with an Airbus A380 to Peru to pick up my Porsche Cayenne\"),\r\n    Sentence(\"Game of Thrones is my favorite series\"),\r\n]\r\n\r\n# 3. Define some classes of named entities such as \"soccer teams\", \"TV shows\" and \"rivers\"\r\nlabels = [\"Soccer Team\", \"University\", \"Vehicle\", \"River\", \"City\", \"Country\", \"Person\", \"Movie\", \"TV Show\"]\r\ntars.add_and_switch_to_new_task('task 1', labels, label_type='ner')\r\n\r\n# 4. Predict for these classes and print results\r\nfor sentence in sentences:\r\n    tars.predict(sentence)\r\n    print(sentence.to_tagged_string(\"ner\"))\r\n```\r\n\r\nThis should print: \r\n\r\n```console\r\nThe Humboldt <B-University> University <I-University> of <I-University> Berlin <E-University> is situated near the Spree <S-River> in Berlin <S-City> , Germany <S-Country>\r\n\r\nBayern <B-Soccer Team> Munich <E-Soccer Team> played against Real <B-Soccer Team> Madrid <E-Soccer Team>\r\n\r\nI flew with an Airbus <B-Vehicle> A380 <E-Vehicle> to Peru <S-City> to pick up my Porsche <B-Vehicle> Cayenne <E-Vehicle>\r\n\r\nGame <B-TV Show> of <I-TV Show> Thrones <E-TV Show> is my favorite series\r\n```\r\n\r\nSo in these examples, we are finding entity classes such as \"TV show\" (_Game of Thrones_), \"vehicle\" (_Airbus A380_ and _Porsche Cayenne_), \"soccer team\" (_Bayern Munich_ and _Real Madrid_) and \"river\" (_Spree_), even though the model was never explicitly trained for this. Note that this is ongoing research and the examples are a bit cherry-picked. We expect the zero-shot model to improve quite a bit until the next release.\r\n\r\n## New NLP Tasks and Datasets\r\n\r\nWe prototypically now support new tasks such as GLUE benchmark, Relation Extraction and Entity Linking. With this, we ship the datasets and model classes you need to train your own models. But we are still tweaking both methods, meaning that we don't ship any pre-trained models as-of-yet. \r\n\r\n### GLUE Benchmark (#2149 #2363)  \r\n\r\nA standard benchmark to evaluate progress in language understanding, mostly consisting of single and pairwise sentence classification tasks.\r\n\r\nNew datasets in Flair: \r\n\r\n-  'GLUE_COLA' -  The Corpus of Linguistic Acceptability from GLUE benchmark \r\n-  'GLUE_MNLI' -  The Multi-Genre Natural Language Inference Corpus from the GLUE benchmark \r\n-  'GLUE_RTE' -  The RTE task from the GLUE benchmark \r\n-  'GLUE_QNLI' -  The Stanford Question Answering Dataset formated as NLI task from the GLUE benchmark \r\n-  'GLUE_WNLI' -  The Winograd Schema Challenge formated as NLI task from the GLUE benchmark \r\n-  'GLUE_MRPC' -  The MRPC task from GLUE benchmark \r\n-  'GLUE_QQP' -  The Quora Question Pairs dataset where the task is to determine whether a pair of questions are semantically equivalent \r\n\r\nInitialize datasets like so: \r\n\r\n```python\r\nfrom flair.datasets import GLUE_QNLI\r\n\r\n# load corpus\r\ncorpus = GLUE_QNLI()\r\n\r\n# print corpus\r\nprint(corpus)\r\n\r\n# print first sentence-pair of training data split\r\nprint(corpus.train[0])\r\n\r\n# print all labels in corpus\r\nprint(corpus.make_label_dictionary(\"entailment\"))\r\n```\r\n\r\n### Relation Extraction (#2333 #2352) \r\n\r\nRelation extraction classifies if and which relationship holds between two entities in a text. \r\n\r\nModel class: `RelationExtractor` \r\n\r\nDatasets in Flair: \r\n- 'RE_ENGLISH_CONLL04'  - the [CoNLL-04](https://github.com/bekou/multihead_joint_entity_relation_extraction/tree/master/data/CoNLL04) Relation Extraction dataset (#2333)\r\n- 'RE_ENGLISH_SEMEVAL2010' - the [SemEval-2010 Task 8](https://aclanthology.org/S10-1006.pdf) dataset on Multi-Way Classification of Semantic Relations Between Pairs of Nominals (#2333)\r\n- 'RE_ENGLISH_TACRED' - the TAC Relation Extraction Dataset](https://nlp.stanford.edu/projects/tacred/) with 41 relations (download required) (#2333)\r\n- 'RE_ENGLISH_DRUGPROT'  - the [DrugProt corpus from Biocreative VII Track 1](https://zenodo.org/record/5119892#.YSdSaVuxU5k/) on drug and chemical-protein interactions (#2340 #2352)\r\n\r\nInitialize datasets like so: \r\n\r\n```python\r\n# initalize CoNLL 04 corpus for Relation extraction\r\ncorpus = RE_ENGLISH_CONLL04()\r\nprint(corpus)\r\n\r\n# print first sentence of training split with annotations\r\nsentence = corpus.train[0]\r\n\r\n# print label dictionary\r\nlabel_dict = corpus.make_label_dictionary(\"relation\")\r\nprint(label_dict)\r\n```\r\n\r\n### Entity Linking (#2375)\r\n\r\nEntity Linking goes one step further than NER and uniquely links entities to knowledge bases such as Wikipedia.\r\n\r\nModel class: `EntityLinker` \r\n\r\nDatasets in Flair: \r\n- 'NEL_ENGLISH_AIDA' - the [AIDA CoNLL-YAGO Entity Linking corpus](https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/ambiverse-nlu/aida/downloads) on the CoNLL-03 dataset for English \r\n- 'NEL_ENGLISH_AQUAINT' - the Aquaint Entity Linking corpus introduced in [Milne and Witten (2008)](https://www.cms.waikato.ac.nz/~ihw/papers/08-DNM-IHW-LearningToLinkWithWikipedia.pdf) \r\n- 'NEL_ENGLISH_IITB' - the ITTB Entity Linking corpus introduced in [Sayali et al. (2009)](https://dl.acm.org/doi/10.1145/1557019.1557073) \r\n- 'NEL_ENGLISH_REDDIT' - the Reddit Entity Linking corpus introduced in [Botzer et al. (2021)](https://arxiv.org/abs/2101.01228v2) (only gold annotations)\r\n- 'NEL_ENGLISH_TWEEKI' - the ITTB Entity Linking corpus introduced in [Harandizadeh and Singh (2020)](https://aclanthology.org/2020.wnut-1.29.pdf) \r\n- 'NEL_GERMAN_HIPE' - the [HIPE](https://impresso.github.io/CLEF-HIPE-2020/) Entity Linking corpus for historical German as a [sentence-segmented version](https://github.com/stefan-it/clef-hipe) \r\n\r\n```python\r\nfrom flair.datasets import NEL_ENGLISH_REDDIT\r\n\r\n# load corpus\r\ncorpus = NEL_ENGLISH_REDDIT()\r\n\r\n# print corpus\r\nprint(corpus)\r\n\r\n# print a sentence of training data split\r\nprint(corpus.train[3])\r\n```\r\n\r\n### New NER Datasets\r\n- 'NER_ARABIC_ANER' - [Arabic Named Entity Recognition Corpus](http://curtis.ml.cmu.edu/w/courses/index.php/ANERcorp) 4-class NER  (#2188) \r\n- 'NER_ARABIC_AQMAR' - [American and Qatari Modeling of Arabic](http://www.cs.cmu.edu/~ark/AQMAR/) 4-class NER (modified)  (#2188) \r\n- 'NER_ENGLISH_PERSON' - NER for [person names](https://github.com/das-sudeshna/genid) (#2271) \r\n- 'NER_ENGLISH_WEBPAGES' - 4-class NER on web pages from [Ratinov and Roth (2009)](https://aclanthology.org/W09-1119/) (#2232 )\r\n- 'NER_GERMAN_POLITICS' - [NEMGP](https://www.thomas-zastrow.de/nlp/) corpus for German politics (#2341)\r\n- 'NER_JAPANESE' - [Japanese NER](https://github.com/Hironsan/IOB2Corpus) dataset automatically generated from Wikipedia (#2154) \r\n- 'NER_MASAKHANE' - [MasakhaNER: Named Entity Recognition for African Languages](https://github.com/masakhane-io/masakhane-ner) corpora (#2212, #2214, #2227, #2229, #2230, #2231, #2222, #2234, #2242, #2243) \r\n\r\n### Other datasets\r\n\r\n- 'YAHOO_ANSWERS' - The [10 largest main categories](https://course.fast.ai/datasets#nlp) from the Yahoo! Answers (#2198)  \r\n- Various Universal Dependencies datasets (#2211, #2216, #2219, #2221, #2244, #2245, #2246, #2247, #2223, #2248, #2235, #2236, #2239, #2226) \r\n\r\n## New Functionality\r\n\r\n### Support for Arabic NER (#2188) \r\n\r\nFlair now supports NER and POS tagging for Arabic. To tag an Arabic sentence, just load the appropriate model:\r\n\r\n```python\r\n\r\n# load model\r\ntagger = SequenceTagger.load('ar-ner')\r\n\r\n# make Arabic sentence\r\nsentence = Sentence(\"\u0627\u062d\u0628 \u0628\u0631\u0644\u064a\u0646\")\r\n\r\n# predict NER tags\r\ntagger.predict(sentence)\r\n\r\n# print sentence with predicted tags\r\nfor entity in sentence.get_labels('ner'):\r\n    print(entity)\r\n```\r\n\r\nThis should print: \r\n```console\r\nLOC [\u0628\u0631\u0644\u064a\u0646 (2)] (0.9803) \r\n```\r\n\r\n### More flexibility on main metric (#2161) \r\n\r\nWhen training models, you can now chose any standard evaluation metric for model selection (previously it was fixed to micro F1). When calling the trainer, simply pass the desired metric as `main_evaluation_metric` like so:\r\n\r\n```python\r\ntrainer.train('resources/taggers/your_model',\r\n              learning_rate=0.1,\r\n              mini_batch_size=32,\r\n              max_epochs=10,\r\n              main_evaluation_metric=(\"macro avg\", 'f1-score'),\r\n              )\r\n```\r\n\r\nIn this example, we now use macro F1 instead of the default micro F1.\r\n\r\n###  Add handling for mapping labels to 'O' #2254  \r\n\r\nIn `ColumnDataset`, labels can be remapped to other labels. But sometimes you may not wish to use all label types in a given dataset.\r\nYou can now remap them to 'O' and so exclude them.\r\n\r\nFor instance, to load CoNLL-03 without MISC, do: \r\n\r\n```python\r\ncorpus = CONLL_03(\r\n    label_name_map={'MISC': 'O'}\r\n)\r\nprint(corpus.make_label_dictionary('ner'))\r\nprint(corpus.train[0].to_tagged_string('ner'))\r\n```\r\n\r\n### Other \r\n- add per-label thresholds for prediction (#2366) \r\n- add support for Spanish clinical Flair embeddings (#2323) \r\n- added 'mean', 'max' pooling strategy for `TransformerDocumentEmbeddings` class (#2180)\r\n- new `DocumentCNNEmbeddings` class to embed text with a trainable CNN  (#2141)\r\n- allow negative examples in `ClassificationCorpus` (#2233) \r\n- added new parameter to save model each k epochs during training (#2146)\r\n- log epoch of best model instead of printing it during training (#2286)\r\n- add option to exclude specific sentences from dataset (#2262) \r\n- improved tensorboard logging (#2164) \r\n\r\n- return predictions during evaluation (#2162) \r\n\r\n\r\n## Internal Refactorings \r\n\r\n### Refactor for simplicity and extensibility (#2333 #2351 #2356 #2377 #2379 #2382 #2184)  \r\n\r\nIn order to accommodate all these new NLP task types (plus many more in the pipeline), we restructure the `flair.nn.Model` class such that most models now inherit from `DefaultClassifier`. This removes many redundancies as most models do classification and are really only different in what they classify and how they apply embeddings. Models that inherit from `DefaultClassifier` need only implement the method `forward_pass`, making each model class only a few lines of code. \r\n\r\nCheck for instance our implementation of the `RelationExtractor` class to see how easy it now is to add a new tasks!\r\n\r\n### Refactor for speed \r\n\r\n- Flair models trained with transformers (such as the FLERT models) were previously not making use of mini-batching, greatly slowing down training and application of such models. We refactored the `TransformerWordEmbeddings` class, yielding significant speed-ups depending on the mini-batch size used. We observed **speed-ups from x2 to x6**. (#2385 #2389 #2384)  \r\n\r\n- Improve training speed of Flair embeddings (#2203) \r\n\r\n\r\n## Bug fixes & improvements\r\n\r\n- fixed references to multi-X-fast Flair embedding models (#2150) \r\n- fixed serialization of DocumentRNNEmbeddings (#2155)\r\n- fixed separator in cross-attention mode (#2156)  \r\n- fixed ID for Slovene word embeddings in the doc (#2166) \r\n- close log_handler after training is complete. (#2170) \r\n- fixed bug in IMDB dataset (#2172) \r\n- fixed IMDB data splitting logic (#2175) \r\n- fixed XLNet and Transformer-XL Execution (#2191)  \r\n- remove unk token from Ner labeling (#2225)  \r\n- fxed typo in property name (#2267)  \r\n- fixed typos (#2303 #2373) \r\n- fixed parallel corpus (#2306)  \r\n- fixed SegtokSentenceSplitter Incorrect Sentence Position Attributes (#2312) \r\n- fixed loading of old serialized models (#2322) \r\n- updated url for BioSemantics corpus (#2327)  \r\n- updated requirements (#2346) \r\n- serialize multi_label_threshold for classification models (#2368)  \r\n- small refactorings in ModelTrainer (#2184)  \r\n- moving Path construction of flair.cache_root (#2241) \r\n- documentation improvement (#2304) \r\n- add model fit tests #2378  ",
        "dateCreated": "2021-08-29T22:32:37Z",
        "datePublished": "2021-08-29T23:23:01Z",
        "html_url": "https://github.com/flairNLP/flair/releases/tag/v0.9",
        "name": "Release 0.9",
        "tag_name": "v0.9",
        "tarball_url": "https://api.github.com/repos/flairNLP/flair/tarball/v0.9",
        "url": "https://api.github.com/repos/flairNLP/flair/releases/48246780",
        "zipball_url": "https://api.github.com/repos/flairNLP/flair/zipball/v0.9"
      },
      {
        "authorType": "User",
        "author_name": "alanakbik",
        "body": "Release 0.8 adds major new features to Flair, including our best named entity recognition (NER) models yet and the ability to host, share and test Flair models on the HuggingFace model hub! In addition, there is a host of improvements, new features and new datasets to check out!\r\n\r\n\r\n## FLERT (#2031 #2032 #2104) \r\n\r\nThis release adds the \"FLERT\" approach to train sequence tagging models using cross-sentence features as presented in [our recent paper](https://arxiv.org/abs/2011.06993). This yields new state-of-the-art models which we include in Flair, as well as the features to easily train your own \"FLERT\" models. \r\n\r\n### Pre-trained FLERT models (#2130)\r\n\r\nWe add 5 new NER models for English (4-class and 18-class), German, Dutch and Spanish (4-class each). Load for instance with: \r\n\r\n```python\r\nfrom flair.data import Sentence\r\nfrom flair.models import SequenceTagger\r\n\r\n# load tagger\r\ntagger = SequenceTagger.load(\"ner-large\")\r\n\r\n# make example sentence\r\nsentence = Sentence(\"George Washington went to Washington\")\r\n\r\n# predict NER tags\r\ntagger.predict(sentence)\r\n\r\n# print sentence\r\nprint(sentence)\r\n\r\n# print predicted NER spans\r\nprint('The following NER tags are found:')\r\n# iterate over entities and print\r\nfor entity in sentence.get_spans('ner'):\r\n    print(entity)\r\n```\r\n\r\nIf you want to test these models in action, for instance the new large English Ontonotes model with 18 classes, you can now use the hosted inference API on the HF model hub, like [here](https://huggingface.co/flair/ner-english-ontonotes-large).\r\n\r\n\r\n### Contextualized Sentences\r\n\r\nIn order to enable cross-sentence context, we made some changes to the Sentence object and data readers: \r\n\r\n1. `Sentence` objects now have `next_sentence()` and `previous_sentence()` methods that are set automatically if loaded through `ColumnCorpus`. This is a pointer system to navigate through sentences in a corpus: \r\n```python\r\n# load corpus\r\ncorpus = MIT_MOVIE_NER_SIMPLE(in_memory=False)\r\n\r\n# get a sentence\r\nsentence = corpus.test[123]\r\nprint(sentence)\r\n# get the previous sentence\r\nprint(sentence.previous_sentence())\r\n# get the sentence after that\r\nprint(sentence.next_sentence())\r\n# get the sentence after the next sentence\r\nprint(sentence.next_sentence().next_sentence())\r\n```\r\nThis allows dynamic computation of contexts in the embedding classes. \r\n\r\n2.  `Sentence` objects now have the `is_document_boundary` field which is set through the `ColumnCorpus`. In some datasets, there are sentences like \"-DOCSTART-\" that just indicate document boundaries. This is now recorded as a boolean in the object.\r\n\r\n\r\n### Refactored TransformerWordEmbeddings (breaking)\r\n\r\n`TransformerWordEmbeddings` refactored for dynamic context, robustness to long sentences and readability. The names of some constructor arguments have changed for clarity: `pooling_operation` is now `subtoken_pooling` (to make clear that we pool subtokens), `use_scalar_mean` is now `layer_mean` (we only do a simple layer mean) and `use_context` can now optionally take an integer to indicate the length of the context. Default arguments are also changed.\r\n\r\nFor instance, to create embeddings with a document-level context of 64 subtokens, init like this:\r\n```python\r\nembeddings = TransformerWordEmbeddings(\r\n    model='bert-base-uncased',\r\n    layers=\"-1\",\r\n    subtoken_pooling=\"first\",\r\n    fine_tune=True,\r\n    use_context=64,\r\n)\r\n```\r\n\r\n### Train your Own FLERT Models\r\n\r\nYou can train a FLERT-model like this: \r\n\r\n```python\r\nimport torch\r\n\r\nfrom flair.data import Sentence\r\nfrom flair.datasets import CONLL_03, WNUT_17\r\nfrom flair.embeddings import TransformerWordEmbeddings, DocumentPoolEmbeddings, WordEmbeddings\r\nfrom flair.models import SequenceTagger\r\nfrom flair.trainers import ModelTrainer\r\n\r\n\r\ncorpus = CONLL_03()\r\n\r\nuse_context = 64\r\nhf_model = 'xlm-roberta-large'\r\n\r\nembeddings = TransformerWordEmbeddings(\r\n    model=hf_model,\r\n    layers=\"-1\",\r\n    subtoken_pooling=\"first\",\r\n    fine_tune=True,\r\n    use_context=use_context,\r\n)\r\n\r\ntag_dictionary = corpus.make_tag_dictionary('ner')\r\n\r\n# init bare-bones tagger (no reprojection, LSTM or CRF)\r\ntagger: SequenceTagger = SequenceTagger(\r\n    hidden_size=256,\r\n    embeddings=embeddings,\r\n    tag_dictionary=tag_dictionary,\r\n    tag_type='ner',\r\n    use_crf=False,\r\n    use_rnn=False,\r\n    reproject_embeddings=False,\r\n)\r\n\r\n# train with XLM parameters (AdamW, 20 epochs, small LR)\r\ntrainer = ModelTrainer(tagger, corpus, optimizer=torch.optim.AdamW)\r\nfrom torch.optim.lr_scheduler import OneCycleLR\r\n\r\ncontext_string = '+context' if use_context else ''\r\n\r\ntrainer.train(f\"resources/flert\",\r\n              learning_rate=5.0e-6,\r\n              mini_batch_size=4,\r\n              mini_batch_chunk_size=1,\r\n              max_epochs=20,\r\n              scheduler=OneCycleLR,\r\n              embeddings_storage_mode='none',\r\n              weight_decay=0.,\r\n              )\r\n```\r\n\r\nWe recommend training FLERT this way if accuracy is by far the most important feature you need. FLERT is quite slow since it works on the document-level.\r\n\r\n\r\n## HuggingFace model hub integration (#2040 #2108  #2115) \r\n\r\nWe now host Flair sequence tagging models on the HF model hub (thanks for all the support @HuggingFace!). \r\n\r\n**Overview of all models.** There is a dedicated 'Flair' tag on the hub, so to get a list of all Flair models, check [here](https://huggingface.co/models?filter=flair). \r\n\r\nThe hub allows all users to upload and share their own models. Even better, you can enable the **Inference API** and so test all models online without downloading and running them. For instance, you can test our new very powerful English 18-class NER model [here](https://huggingface.co/flair/ner-english-ontonotes-large).\r\n\r\nTo load any sequence tagger on the model hub, use the string identifier when instantiating a model. For instance, to load our English ontonotes model with the id \"flair/ner-english-ontonotes-large\", do \r\n\r\n```python\r\nfrom flair.data import Sentence\r\nfrom flair.models import SequenceTagger\r\n\r\n# load tagger\r\ntagger = SequenceTagger.load(\"flair/ner-english-ontonotes-large\")\r\n\r\n# make example sentence\r\nsentence = Sentence(\"On September 1st George won 1 dollar while watching Game of Thrones.\")\r\n\r\n# predict NER tags\r\ntagger.predict(sentence)\r\n\r\n# print sentence\r\nprint(sentence)\r\n\r\n# print predicted NER spans\r\nprint('The following NER tags are found:')\r\n# iterate over entities and print\r\nfor entity in sentence.get_spans('ner'):\r\n    print(entity)\r\n```\r\n\r\n\r\n## Other New Features\r\n\r\n### New Task: Recognizing Textual Entailment (#2123) \r\n\r\nThanks to @marcelmmm we now support training textual entailment tasks (in fact, all pairwise sentence classification tasks) in Flair. \r\n\r\nFor instance, if you want to train an RTE task of the GLUE benchmark use this script: \r\n\r\n```python\r\nimport torch\r\n\r\nfrom flair.data import Corpus\r\nfrom flair.datasets import GLUE_RTE\r\nfrom flair.embeddings import TransformerDocumentEmbeddings\r\n\r\n# 1. get the entailment corpus\r\ncorpus: Corpus = GLUE_RTE()\r\n\r\n# 2. make the tag dictionary from the corpus\r\nlabel_dictionary = corpus.make_label_dictionary()\r\n\r\n# 3. initialize text pair tagger\r\nfrom flair.models import TextPairClassifier\r\n\r\ntagger = TextPairClassifier(\r\n    document_embeddings=TransformerDocumentEmbeddings(),\r\n    label_dictionary=label_dictionary,\r\n)\r\n\r\n# 4. train trainer with AdamW\r\nfrom flair.trainers import ModelTrainer\r\n\r\ntrainer = ModelTrainer(tagger, corpus, optimizer=torch.optim.AdamW)\r\n\r\n# 5. run training\r\ntrainer.train('resources/taggers/glue-rte-english',\r\n              learning_rate=2e-5,\r\n              mini_batch_chunk_size=2, # this can be removed if you hae a big GPU\r\n              train_with_dev=True,\r\n              max_epochs=3)\r\n```\r\n\r\n### Add possibility to specify empty label name to CSV corpora (#2068)\r\n\r\nSome CSV classification datasets contain a value that means \"no class\". We now extend the `CSVClassificationDataset` so that it is possible to specify which value should be skipped using the `no_class_label` argument. \r\n\r\nFor instance: \r\n\r\n```python\r\n# load corpus\r\ncorpus = CSVClassificationCorpus(\r\n    data_folder='resources/tasks/code/',\r\n    train_file='java_io.csv',\r\n    skip_header=True,\r\n    column_name_map={3: 'text', 4: 'label', 5: 'label', 6: 'label', 7: 'label', 8: 'label', 9: 'label'},\r\n    no_class_label='NONE',\r\n)\r\n```\r\n\r\nThis causes all entries of NONE in one of the label columns to be skipped.\r\n\r\n### More options for splits in corpora and training (#2034) \r\n\r\nFor various reasons, we might want to have a `Corpus` that does not define all three splits (train/dev/test). For instance, we might want to train a model over the entire dataset and not hold out any data for validation/evaluation. \r\n\r\nWe add several ways of doing so.\r\n\r\n1. If a dataset has predefined splits, like most NLP datasets, you can pass the arguments `train_with_test` and `train_with_dev` to the `ModelTrainer`. This causes the trainer to train over all three splits (and do no evaluation):\r\n\r\n```python\r\ntrainer.train(f\"path/to/your/folder\",\r\n    learning_rate=0.1,\r\n    mini_batch_size=16,\r\n    train_with_dev=True,\r\n    train_with_test=True,\r\n)\r\n```\r\n\r\n2. You can also now create a Corpus with fewer splits without having all three splits automatically sampled. Pass `sample_missing_splits=False` as argument to do this. For instance, to load SemCor WSD corpus only as training data, do:\r\n\r\n```python\r\nsemcor = WSD_UFSAC(train_file='semcor.xml', sample_missing_splits=False, autofind_splits=False)\r\n```\r\n\r\n### Add TFIDF Embeddings  (#2086) \r\n\r\nWe added some old-school embeddings (thanks @yosipk), namely the legendary TF-IDF document embeddings. These are often good baselines, and additionally they keep NLP veterans nostalgic, if not happy. \r\n\r\nTo initialize these embeddings, you must pass the train split of your training corpus, i.e. \r\n\r\n```python\r\nembeddings = DocumentTFIDFEmbeddings(corpus.train, max_features=10000)\r\n```\r\n\r\nThis triggers the process where the most common words are used to featurize documents.\r\n\r\n## New Datasets\r\n\r\n### Hungarian NER Corpus (#2045) \r\n\r\nAdded the Hungarian business news corpus annotated with NER information (thanks to @alibektas).\r\n\r\n```python\r\n# load Hungarian business NER corpus\r\ncorpus = BUSINESS_HUN()\r\nprint(corpus)\r\nprint(corpus.make_tag_dictionary('ner'))\r\n```\r\n\r\n### StackOverflow NER Corpus (#2052)\r\n\r\n```python\r\n# load StackOverflow business NER corpus\r\ncorpus = STACKOVERFLOW_NER()\r\nprint(corpus)\r\nprint(corpus.make_tag_dictionary('ner'))\r\n```\r\n\r\n### Added GermEval 18 Offensive Language dataset (#2102) \r\n\r\n```python\r\n# load StackOverflow business NER corpus\r\ncorpus = GERMEVAL_2018_OFFENSIVE_LANGUAGE()\r\nprint(corpus)\r\nprint(corpus.make_label_dictionary()\r\n```\r\n\r\n### Added RTE corpora of GLUE and SuperGLUE \r\n\r\n```python\r\n# load the recognizing textual entailment corpus of the GLUE benchmark\r\ncorpus = GLUE_RTE()\r\nprint(corpus)\r\nprint(corpus.make_label_dictionary()\r\n```\r\n\r\n## Improvements\r\n\r\n### Allow newlines as Tokens in a Sentence (#2070) \r\n\r\nNewlines and tabs can now become Tokens in a Sentence: \r\n\r\n```python\r\n# make sentence with newlines and tabs\r\nsentence: Sentence = Sentence([\"I\", \"\\t\", \"ich\", \"\\n\", \"you\", \"\\t\", \"du\", \"\\n\"], use_tokenizer=True)\r\n\r\n# Alternatively: sentence: Sentence = Sentence(\"I \\t ich \\n you \\t du \\n\", use_tokenizer=False)\r\n\r\n# print sentence and each token\r\nprint(sentence)\r\nfor token in sentence:\r\n    print(token)\r\n```\r\n\r\n### Improve transformer serialization (#2046) \r\n\r\nWe improved the serialization of the `TransformerWordEmbeddings` class such that you can now train a model with one version of the transformers library and load it with another version. Previously, if you trained a model with transformers 3.5.1 and loaded it with 3.1.01, or trained with 3.5.1 and loaded with 4.1.1, or other version mismatches, there would either be errors or bad predictions. \r\n\r\n**Migration guide:** If you have a model trained with an older version of Flair that uses `TransformerWordEmbeddings` you can save it in the new version-independent format by loading the model with the same transformers version you used to train it, and then saving it again. The newly saved model is then version-independent: \r\n\r\n```python\r\n# load old model, but use the *same transformer version you used when training this model*\r\ntagger = SequenceTagger.load('path/to/old-model.pt')\r\n\r\n# save the model. It is now version-independent and can for instance be loaded with transformers 4.\r\ntagger.save('path/to/new-model.pt')\r\n```\r\n\r\n### Fix regression prediction errors (#2067) \r\n\r\nFix of two problems in the regression model: \r\n- the predict() method was unable to set labels and threw errors (see #2056) \r\n- predicted labels had no label name \r\n\r\nNow, you can set a label name either in the predict method or during instantiation of the regression model you want to train. So the full code for training a regression model and using it to predict is: \r\n\r\n```python\r\n# load regression dataset\r\ncorpus = WASSA_JOY()\r\n\r\n# make simple document embeddings\r\nembeddings = DocumentPoolEmbeddings([WordEmbeddings('glove')], fine_tune_mode='linear')\r\n\r\n# init model and give name to label\r\nmodel = TextRegressor(embeddings, label_name='happiness')\r\n\r\n# target folder\r\noutput_folder = 'resources/taggers/regression_test/'\r\n\r\n# run training\r\ntrainer = ModelTrainer(model, corpus)\r\ntrainer.train(\r\n    output_folder,\r\n    mini_batch_size=16,\r\n    max_epochs=10,\r\n)\r\n\r\n# load model\r\nmodel = TextRegressor.load(output_folder + 'best-model.pt')\r\n\r\n# predict for sentence\r\nsentence = Sentence('I am so happy')\r\nmodel.predict(sentence)\r\n\r\n# print sentence and prediction\r\nprint(sentence)\r\n```\r\n\r\nIn my example run, this prints the following sentence + predicted value: \r\n~~~\r\nSentence: \"I am so happy\"   [\u2212 Tokens: 4  \u2212 Sentence-Labels: {'happiness': [0.9239126443862915 (1.0)]}]\r\n~~~\r\n\r\n### Do not shuffle first epoch during training (#2058) \r\n\r\nNormally, we shuffle sentences at each epoch during training in the ModelTrainer class. However, in some cases it makes sense to see sentences in their natural order during the first epoch, and shuffle only from the second epoch onward. \r\n\r\n \r\n## Bug Fixes and Improvements\r\n\r\n- Update to transformers 4 (#2057) \r\n- Fix the evaluate() method in the SimilarityLearner class (#2113) \r\n- Fix memory memory leak in WordEmbeddings (#2018)\r\n- Add support for Transformer-XL Embeddings (#2009) \r\n- Restrict numpy version to <1.20 for Python 3.6 (#2014) \r\n- Small formatting and variable declaration changes (#2022)\r\n- Fix document boundary offsets for Dutch CoNLL-03 (#2061)\r\n- Changed the torch version in requirements.txt: Torch>=1.5.0 (#2063)  \r\n- Fix linear input dimension if the reproject (#2073) \r\n- Various improvements for TARS (#2090 #2128)\r\n- Added a link to the interpret-flair repo (#2096) \r\n- Improve documentatin ( #2110) \r\n- Update sentencepiece and gdown version (#2131) \r\n- Add to_plain_string method to Span class (#2091) ",
        "dateCreated": "2021-03-04T19:09:52Z",
        "datePublished": "2021-03-05T11:57:03Z",
        "html_url": "https://github.com/flairNLP/flair/releases/tag/v0.8",
        "name": "Release 0.8",
        "tag_name": "v0.8",
        "tarball_url": "https://api.github.com/repos/flairNLP/flair/tarball/v0.8",
        "url": "https://api.github.com/repos/flairNLP/flair/releases/39179714",
        "zipball_url": "https://api.github.com/repos/flairNLP/flair/zipball/v0.8"
      },
      {
        "authorType": "User",
        "author_name": "alanakbik",
        "body": "Release 0.7 adds major few-shot and zero-shot learning capabilities to Flair with our new [TARS](https://kishaloyhalder.github.io/pdfs/tars_coling2020.pdf) approach, plus support for the [Universal Proposition Banks](https://github.com/System-T/UniversalPropositions), new NER datasets and lots of other new features! \r\n\r\n## Few-Shot and Zero-Shot Classification with TARS (#1917 #1926) \r\n\r\nWith TARS we add a major new feature to Flair for zero-shot and few-shot classification. Details on the approach can be found in our paper [Halder et al. (2020)](https://kishaloyhalder.github.io/pdfs/tars_coling2020.pdf). Our approach allows you to classify text in cases in which you have little or even no training data at all.\r\n\r\nThis example illustrates how you predict new classes without training data:  \r\n\r\n```python\r\n# 1. Load our pre-trained TARS model for English\r\ntars = TARSClassifier.load('tars-base')\r\n\r\n# 2. Prepare a test sentence\r\nsentence = flair.data.Sentence(\"I am so glad you liked it!\")\r\n\r\n# 3. Define some classes that you want to predict using descriptive names\r\nclasses = [\"happy\", \"sad\"]\r\n\r\n#4. Predict for these classes\r\ntars.predict_zero_shot(sentence, classes)\r\n\r\n# Print sentence with predicted labels\r\nprint(sentence)\r\n```\r\n\r\nFor a full overview of TARS features, please refer to our new [TARS tutorial](/resources/docs/TUTORIAL_10_TRAINING_ZERO_SHOT_MODEL.md). \r\n\r\n\r\n## Other New Features \r\n\r\n### Option to set Flair seed (#1979)\r\n\r\nAdds the possibility to set a seed via wrapping the Hugging Face Transformers library helper method (thanks @stefan-it). \r\n\r\nBy specifying a seed with:\r\n\r\n```python\r\nimport flair\r\n\r\nflair.set_seed(42)\r\n```\r\n\r\nyou can make experimental runs reproducible. The wrapped `set_seed` method sets seeds for `random`, `numpy` and `torch`. More details [here](https://github.com/huggingface/transformers/blob/08f534d2da47875a4b7eb1c125cfa7f0f3b79642/src/transformers/trainer_utils.py#L29-L48).\r\n\r\n### Control multi-word behavior in UD datasets (#1981) \r\n\r\nTo better handle multi-words in UD corpora, we introduce the  `split_multiwords` constructor argument to all UD corpora which by default is set to `True`. It controls the handling of multiwords that are split into different tokens. For instance the German \"am\" is split into two different tokens:  \"am\" -> \"an\" + \"dem\". Or the French \"aux\" -> \"a\" + \"les\".\r\n\r\nIf `split_multiwords` is set to `True`, they are split as in UD. If set to `False`, we keep the original multiword as a single token. Example: \r\n\r\n```python\r\n# default mode: multiwords are split\r\ncorpus = UD_GERMAN(split_multiwords=True)\r\n# print sentence 179\r\nprint(corpus.dev[179].to_plain_string())\r\n\r\n# alternative mode: multiwords are kept as original\r\ncorpus = UD_GERMAN(split_multiwords=False)\r\n# print sentence 179\r\nprint(corpus.dev[179].to_plain_string())  \r\n```\r\n\r\nThis prints \r\n\r\n~~~\r\nEin Hotel zu dem Wohlf\u00fchlen.\r\n\r\nEin Hotel zum Wohlf\u00fchlen.\r\n~~~\r\n\r\nThe latter is how it appears in text, the former is after splitting of multiwords.\r\n\r\n### Pass pretokenized sentence to Sentence object (#1965) \r\n\r\nYou can now pass pass a pretokenized sequence as list of words (thanks @ulf1):\r\n\r\n```python\r\nfrom flair.data import Sentence\r\nsentence = Sentence(['The', 'grass', 'is', 'green', '.'])\r\nprint(sentence)\r\n```\r\n\r\nThis should print:\r\n\r\n```console\r\nSentence: \"The grass is green .\"   [\u2212 Tokens: 5]\r\n```\r\n\r\n### Map label names in sequence labeling datasets (#1988)\r\n\r\nYou can now pass a label map to sequence labeling datasets to change label names (thanks @pharnisch). \r\n\r\n```python\r\n# print tag dictionary with mapped names\r\ncorpus = CONLL_03_DUTCH(label_name_map={'PER': 'person', 'ORG': 'organization', 'LOC': 'location', 'MISC': 'other'})\r\nprint(corpus.make_tag_dictionary('ner'))\r\n\r\n# print tag dictionary with original names\r\ncorpus = CONLL_03_DUTCH()\r\nprint(corpus.make_tag_dictionary('ner'))\r\n```\r\n\r\n## Data Sets  \r\n\r\n### Universal Proposition Banks (#1870 #1866 #1888) \r\n\r\nFlair 0.7 adds support 7 Universal Proposition Banks to train your own multilingual semantic role labelers (thanks to @Dabendorf).\r\n\r\nLoad for instance with: \r\n\r\n```python\r\n# load English Universal Proposition Bank\r\ncorpus = UP_ENGLISH()\r\nprint(corpus)\r\n\r\n# make dictionary of frames\r\nframe_dictionary = corpus.make_tag_dictionary('frame')\r\nprint(frame_dictionary)\r\n```\r\n\r\nNow available for Finnish, Chinese, Italian, French, German, Spanish and English\r\n\r\n### NER Corpora\r\n\r\nWe add support for 6 new NER corpora:\r\n\r\n#### Arabic NER Corpus (#1901)\r\n\r\nAdded the ANER corpus for Arabic NER (thanks to @megantosh).  \r\n\r\n```python\r\n# load Arabic NER corpus\r\ncorpus = ANER_CORP()\r\nprint(corpus)\r\n```\r\n\r\n#### Movie NER Corpora (#1912) \r\n\r\nAdded the MIT movie reviews corpora annotated with NER information, in the simple and complex variant (thanks to @pharnisch). \r\n\r\n```python\r\n# load simple movie NER corpus\r\ncorpus = MITMovieNERSimple()\r\nprint(corpus)\r\nprint(corpus.make_tag_dictionary('ner'))\r\n\r\n# load complex movie NER corpus\r\ncorpus = MITMovieNERComplex()\r\nprint(corpus)\r\nprint(corpus.make_tag_dictionary('ner'))   \r\n```\r\n\r\n#### Added SEC Fillings NER corpus (#1922) \r\n\r\nAdded corpus of SEC fillings annotated with 4-class NER tags (thanks to @samahakk).\r\n\r\n```python\r\n# load SEC fillings corpus\r\ncorpus = SEC_FILLINGS()\r\nprint(corpus)\r\nprint(corpus.make_tag_dictionary('ner'))\r\n```\r\n\r\n#### WNUT 2020 NER dataset support (#1942) \r\n\r\nAdded corpus of wet lab protocols annotated with NER information used for WNUT 2020 challenge (thanks to @aynetdia). \r\n\r\n```python\r\n# load wet lab protocol data\r\ncorpus = WNUT_2020_NER()\r\nprint(corpus)\r\nprint(corpus.make_tag_dictionary('ner'))\r\n```\r\n\r\n#### Weibo NER dataset support (#1944) \r\n\r\nAdded dataset about NER for Chinese Social Media (thanks to @87302380). \r\n\r\n```python\r\n# load Weibo NER data\r\ncorpus = WEIBO_NER()\r\nprint(corpus)\r\nprint(corpus.make_tag_dictionary('ner'))\r\n```\r\n\r\n#### Added Finnish NER corpus (#1946) \r\n\r\nAdded the TURKU corpus for Finnish NER (thanks to @melvelet). \r\n\r\n```python\r\n# load Finnish NER data\r\ncorpus = TURKU_NER()\r\nprint(corpus)\r\nprint(corpus.make_tag_dictionary('ner'))\r\n```\r\n\r\n### Universal Depdency Treebanks\r\n\r\nWe add support for 11 new UD treebanks:\r\n\r\n- Greek UD Treebank (#1933, thanks @malamasn)  \r\n- Livvi UD Treebank (#1953, thanks @hebecked) \r\n- Naija UD Treebank (#1952, thanks @teddim420) \r\n- Buryat UD Treebank (#1954, thanks @MaxDall) \r\n- North Sami UD Treebank (#1955, thanks @dobbersc) \r\n- Maltese UD Treebank (#1957, thanks @phkuep) \r\n- Marathi UD Treebank (#1958, thanks @polarlyset) \r\n- Afrikaans UD Treebank (#1959, thanks @QueStat) \r\n- Gothic UD Treebank (#1961, thanks @wjSimon) \r\n- Old French UD Treebank (#1964, thanks @Weyaaron) \r\n- Wolof UD Treebank (#1967, thanks @LukasOpp)\r\n\r\nLoad each with language name, for instance: \r\n\r\n```python\r\n# load Gothic UD treebank data\r\ncorpus = UD_GOTHIC()\r\nprint(corpus)\r\nprint(corpus.test[0])\r\n```\r\n\r\n### Added GoEmotions text classification corpus (#1914) \r\n\r\nAdded [GoEmotions dataset]( https://github.com/google-research/google-research/tree/master/goemotions ) containing 58k Reddit comments labeled with 27 emotion categories. Load with: \r\n\r\n```python\r\n# load GoEmotions corpus\r\ncorpus = GO_EMOTIONS()\r\nprint(corpus)\r\nprint(corpus.make_label_dictionary())\r\n```\r\n\r\n## Enhancements and bug fixes\r\n- Add handling for micro-average precision and recall (#1935)\r\n- Make dev and test splits in treebanks optional (#1951) \r\n- Updated communicative functions model (#1857) \r\n- Biomedical Data: Explicit encodings for Windows Support (#1893) \r\n- Fix wrong abstract method (#1923 #1940)  \r\n- Improve tutorial (#1939) \r\n- Fix requirements (#1971 )",
        "dateCreated": "2020-12-01T18:55:13Z",
        "datePublished": "2020-12-01T19:35:00Z",
        "html_url": "https://github.com/flairNLP/flair/releases/tag/v0.7",
        "name": "Release 0.7",
        "tag_name": "v0.7",
        "tarball_url": "https://api.github.com/repos/flairNLP/flair/tarball/v0.7",
        "url": "https://api.github.com/repos/flairNLP/flair/releases/34370268",
        "zipball_url": "https://api.github.com/repos/flairNLP/flair/zipball/v0.7"
      },
      {
        "authorType": "User",
        "author_name": "alanakbik",
        "body": "Release 0.6.1 is bugfix release that fixes the issues caused by moving the server that originally hosted the Flair models. Additionally, this release adds a ton of new NER datasets, including the XTREME corpus for 40 languages, and a new model for NER on German-language legal text.\r\n\r\n## New Model: Legal NER (#1872)\r\n\r\nAdd legal NER model for German. Trained using the German legal NER dataset available [here](https://github.com/elenanereiss/Legal-Entity-Recognition) that can be loaded in Flair with the `LER_GERMAN` corpus object.\r\n\r\nUses German Flair and FastText embeddings and gets **96.35** F1 score.\r\n\r\nUse like this: \r\n\r\n```python\r\n# load German LER tagger\r\ntagger = SequenceTagger.load('de-ler')\r\n\r\n# example text\r\ntext = \"vom 6. August 2020. Alle Beschwerdef\u00fchrer befinden sich derzeit gemeinsam im Urlaub auf der Insel Mallorca , die vom Robert-Koch-Institut als Risikogebiet eingestuft wird. Sie wollen am 29. August 2020 wieder nach Deutschland einreisen, ohne sich gem\u00e4\u00df \u00a7 1 Abs. 1 bis Abs. 3 der Verordnung zur Testpflicht von Einreisenden aus Risikogebieten auf das SARS-CoV-2-Virus testen zu lassen. Die Verordnung sei wegen eines Versto\u00dfes der ihr zugrunde liegenden gesetzlichen Erm\u00e4chtigungsgrundlage, des \u00a7 36 Abs. 7 IfSG , gegen Art. 80 Abs. 1 Satz 1 GG verfassungswidrig.\"\r\n\r\nsentence = Sentence(text)\r\n\r\n# predict and print entities\r\ntagger.predict(sentence)\r\n\r\nfor entity in sentence.get_spans('ner'):\r\n    print(entity)\r\n```\r\n\r\n## New Datasets\r\n\r\n### Add XTREME and WikiANN corpora for multilingual NER (#1862)\r\n\r\nThese huge corpora provide training data for NER in 176 languages. You can either load the language-specific parts of it by supplying a language code: \r\n\r\n```python\r\n# load German Xtreme\r\ngerman_corpus = XTREME('de')\r\nprint(german_corpus)\r\n\r\n# load French Xtreme\r\nfrench_corpus = XTREME('fr')\r\nprint(french_corpus)\r\n```\r\n\r\nOr you can load the default 40 languages at once into one huge MultiCorpus by not providing a language ID:\r\n\r\n```python\r\n# load Xtreme MultiCorpus for all\r\nmulti_corpus = XTREME()\r\nprint(multi_corpus)\r\n```\r\n\r\n### Add Twitter NER Dataset (#1850) \r\n\r\nDataset of [tweets](\r\nhttps://raw.githubusercontent.com/aritter/twitter_nlp/master/data/annotated/ner.txt) annotated with NER tags. Load with:\r\n\r\n```python\r\n# load twitter dataset\r\ncorpus = TWITTER_NER()\r\n\r\n# print example tweet\r\nprint(corpus.test[0])\r\n```\r\n\r\n### Add German Europarl NER Dataset (#1849) \r\n\r\nDataset of German-language speeches in the European parliament annotated with standard NER tags like person and location. Load with:\r\n\r\n```python\r\n# load corpus\r\ncorpus = EUROPARL_NER_GERMAN()\r\nprint(corpus)\r\n\r\n# print first test sentence\r\nprint(corpus.test[1])\r\n```\r\n\r\n### Add MIT Restaurant NER Dataset (#1177) \r\n\r\nDataset of English restaurant reviews annotated with entities like \"dish\", \"location\" and \"rating\". Load with: \r\n\r\n```python\r\n# load restaurant dataset\r\ncorpus = MIT_RESTAURANTS()\r\n\r\n# print example sentence\r\nprint(corpus.test[0])  \r\n```\r\n\r\n### Add Universal Propositions Banks for French and German (#1866) \r\n\r\nOur kickoff into supporting the [Universal Proposition Banks](https://github.com/System-T/UniversalPropositions) adds the first two UP datasets to Flair. Load with: \r\n\r\n```python\r\n# load German UP\r\ncorpus = UP_GERMAN()\r\nprint(corpus)\r\n\r\n# print example sentence\r\nprint(corpus.dev[1])\r\n```\r\n\r\n### Add Universal Dependencies Dataset for Chinese (#1880) \r\n\r\nAdds the Kyoto dataset for Chinese. Load with: \r\n\r\n```python\r\n# load Chinese UD dataset\r\ncorpus = UD_CHINESE_KYOTO()\r\n\r\n# print example sentence\r\nprint(corpus.test[0])  \r\n```\r\n\r\n## Bug fixes\r\n\r\n- Move models to HU server (#1834  #1839 #1842)\r\n- Fix deserialization issues in transformer tokenizers #1865\r\n- Documentation fixes (#1819 #1821 #1836 #1852)\r\n- Add link to a repo with examples of Flair on GCP (#1825)\r\n- Correct variable names (#1875)\r\n- Fix problem with custom delimiters in ColumnDataset (#1876) \r\n- Fix offensive language detection model (#1877) \r\n- Correct Dutch NER model (#1881) ",
        "dateCreated": "2020-09-23T10:24:15Z",
        "datePublished": "2020-09-23T10:40:31Z",
        "html_url": "https://github.com/flairNLP/flair/releases/tag/v0.6.1",
        "name": "Release 0.6.1",
        "tag_name": "v0.6.1",
        "tarball_url": "https://api.github.com/repos/flairNLP/flair/tarball/v0.6.1",
        "url": "https://api.github.com/repos/flairNLP/flair/releases/31687328",
        "zipball_url": "https://api.github.com/repos/flairNLP/flair/zipball/v0.6.1"
      },
      {
        "authorType": "User",
        "author_name": "alanakbik",
        "body": "Release 0.6 is a major biomedical NLP upgrade for Flair, adding state-of-the-art models for biomedical NER, support for 31 biomedical NER corpora, clinical POS tagging, speculation and negation detection in biomedical literature, and many other features such as multi-tagging and one-cycle learning. \r\n\r\n# Biomedical Models and Datasets: \r\n\r\nMost of the biomedical models and datasets were developed together with the [Knowledge Management in Bioinformatics](https://www.informatik.hu-berlin.de/de/forschung/gebiete/wbi) group at the HU Berlin, in particular @leonweber and @mariosaenger. [This page](https://github.com/flairNLP/flair/blob/master/resources/docs/HUNFLAIR.md) gives an overview of the new models and datasets, and example tutorials. Some highlights:\r\n\r\n\r\n## Biomedical NER models (#1790)\r\n\r\nFlair now has pre-trained models for biomedical NER trained over unified versions of 31 different biomedical corpora. Because they are trained on so many different datasets, the models are shown to be very robust with new datasets, outperforming all previously available off-the-shelf datasets. If you want to load a model to detect \"diseases\" in text for instance, do: \r\n\r\n```python\r\n# make a sentence\r\nsentence = Sentence(\"Behavioral abnormalities in the Fmr1 KO2 Mouse Model of Fragile X Syndrome\")\r\n\r\n# load disease tagger and predict\r\ntagger = SequenceTagger.load(\"hunflair-disease\")\r\ntagger.predict(sentence)\r\n```\r\n\r\nDone! Let's print the diseases found by the tagger:\r\n\r\n```python\r\nfor entity in sentence.get_spans():\r\n    print(entity)\r\n```\r\nThis should print:\r\n~~~\r\nSpan [1,2]: \"Behavioral abnormalities\"   [\u2212 Labels: Disease (0.6736)]\r\nSpan [10,11,12]: \"Fragile X Syndrome\"   [\u2212 Labels: Disease (0.99)]\r\n~~~\r\n\r\nYou can also get one model that finds 5 biomedical entity types (diseases, genes, species, chemicals and cell lines), like this: \r\n\r\n```python\r\n# load bio-NER tagger and predict\r\ntagger = MultiTagger.load(\"hunflair\")\r\ntagger.predict(sentence)\r\n```\r\nThis should print:\r\n~~~\r\nSpan [1,2]: \"Behavioral abnormalities\"   [\u2212 Labels: Disease (0.6736)]\r\nSpan [10,11,12]: \"Fragile X Syndrome\"   [\u2212 Labels: Disease (0.99)]\r\nSpan [5]: \"Fmr1\"   [\u2212 Labels: Gene (0.838)]\r\nSpan [7]: \"Mouse\"   [\u2212 Labels: Species (0.9979)]\r\n~~~\r\n\r\nSo it now also finds genes and species. As explained [here](https://github.com/flairNLP/flair/blob/master/resources/docs/HUNFLAIR.md) these models work best if you use them together with a biomedical tokenizer.  \r\n\r\n\r\n## Biomedical NER datasets (#1790) \r\n\r\nFlair now supports 31 biomedical NER datasets out of the box, both in their standard versions as well as the \"Huner\" splits for reproducibility of experiments. For a full list of datasets, refer to [this page](https://github.com/flairNLP/flair/blob/master/resources/docs/HUNFLAIR_CORPORA.md). \r\n\r\nYou can load a dataset like this: \r\n\r\n```python\r\n# load one of the bioinformatics corpora\r\ncorpus = JNLPBA()\r\n\r\n# print statistics and one sentence\r\nprint(corpus)\r\nprint(corpus.train[0])\r\n```\r\n\r\nWe  also include \"huner\" corpora that combine many different biomedical datasets into a single corpus. For instance, if you execute the following line: \r\n\r\n```python\r\n# load combined chemicals corpus\r\ncorpus = HUNER_CHEMICAL()\r\n```\r\n\r\nThis loads a combination of 6 different corpora that contain annotation of chemicals into a single corpus. This allows you to train stronger cross-corpus models since you now combine training data from many sources. See more info [here](https://github.com/flairNLP/flair/blob/master/resources/docs/HUNFLAIR_CORPORA.md#huner-data-sets). \r\n\r\n\r\n## POS model for Portuguese clinical text (#1789) \r\n\r\nThanks to @LucasFerroHAILab, we now include a model for part-of-speech tagging in Portuguese clinical text. Run this model like this:\r\n\r\n```python\r\n# load your tagger\r\ntagger = SequenceTagger.load('pt-pos-clinical')\r\n\r\n# example sentence\r\nsentence = Sentence('O v\u00edrus Covid causa fortes dores .')\r\ntagger.predict(sentence)\r\nprint(sentence)\r\n```\r\n\r\nYou can find more details in their paper [here](https://link.springer.com/article/10.1007/s42600-020-00067-7).\r\n\r\n\r\n## Model for negation and speculation in biomedical literature (#1758)\r\n\r\nUsing the BioScope corpus, we trained a model to recognize negation and speculation in biomedical literature. Use it like this: \r\n\r\n```python\r\nsentence = Sentence(\"The picture most likely reflects airways disease\")\r\n\r\ntagger = SequenceTagger.load(\"negation-speculation\")\r\ntagger.predict(sentence)\r\n\r\nfor entity in sentence.get_spans():\r\n    print(entity)\r\n```\r\n\r\nThis should print: \r\n\r\n~~~\r\nSpan [4,5,6,7]: \"likely reflects airways disease\"   [\u2212 Labels: SPECULATION (0.9992)]\r\n~~~\r\n\r\nThus indicating that this portion of the sentence is speculation.\r\n\r\n\r\n# Other New Features:\r\n\r\n## MultiTagger (#1791) \r\n\r\nWe added support for tagging text with multiple models at the same time. This can save memory usage and increase tagging speed. \r\n\r\nFor instance, if you want to POS tag, chunk, NER and detect frames in your text at the same time, do: \r\n\r\n```python\r\n# load tagger for POS, chunking, NER and frame detection\r\ntagger = MultiTagger.load(['pos', 'upos', 'chunk', 'ner', 'frame'])\r\n\r\n# example sentence\r\nsentence = Sentence(\"George Washington was born in Washington\")\r\n\r\n# predict\r\ntagger.predict(sentence)\r\n\r\nprint(sentence) \r\n```\r\n\r\nThis will give you a sentence annotated with 5 different layers of annotation.\r\n\r\n## Sentence splitting\r\n\r\nFlair now includes convenience methods for sentence splitting. For instance, to use segtok to split and tokenize a text into sentences, use the following code: \r\n\r\n```python\r\nfrom flair.tokenization import SegtokSentenceSplitter\r\n\r\n# example text with many sentences\r\ntext = \"This is a sentence. This is another sentence. I love Berlin.\"\r\n\r\n# initialize sentence splitter\r\nsplitter = SegtokSentenceSplitter()\r\n\r\n# use splitter to split text into list of sentences\r\nsentences = splitter.split(text)  \r\n```\r\n\r\nWe also ship other splitters, such as `SpacySentenceSplitter` (requires SpaCy to be installed).\r\n\r\n## Japanese tokenization (#1786) \r\n\r\nThanks to @himkt we now have expanded support for Japanese tokenization in Flair. For instance, use the following code to tokenize a Japanese sentence without installing extra libraries:\r\n\r\n```python\r\nfrom flair.data import Sentence\r\nfrom flair.tokenization import JapaneseTokenizer\r\n\r\n# init japanese tokenizer\r\ntokenizer = JapaneseTokenizer(\"janome\")\r\n\r\n# make sentence (and tokenize)\r\nsentence = Sentence(\"\u79c1\u306f\u30d9\u30eb\u30ea\u30f3\u304c\u597d\u304d\", use_tokenizer=tokenizer)\r\n\r\n# output tokenized sentence\r\nprint(sentence)\r\n```\r\n\r\n## One-Cycle Learning (#1776) \r\n\r\nThanks to @lucaventurini2 Flair one supports one-cycle learning, which may give quicker convergence. For instance, train a model in 20 epochs using the code below:\r\n\r\n```python\r\n# train as always\r\ntrainer = ModelTrainer(tagger, corpus)\r\n\r\n# set one cycle LR as scheduler\r\ntrainer.train('onecycle_ner',\r\n              scheduler=OneCycleLR,\r\n              max_epochs=20)\r\n```\r\n\r\n# Improvements:\r\n \r\n## Changes in convention\r\n\r\n### Turn on tokenizer by default in `Sentence` object (#1806) \r\n\r\nThe `Sentence` object now executes tokenization (`use_tokenizer=True`) by default: \r\n\r\n```python\r\n# Tokenizes by default\r\nsentence = Sentence(\"I love Berlin.\")\r\nprint(sentence)\r\n\r\n# i.e. this is equivalent to\r\nsentence = Sentence(\"I love Berlin.\", use_tokenizer=True)\r\nprint(sentence)\r\n\r\n# i.e. if you don't want to use tokenization, set it to False\r\nsentence = Sentence(\"I love Berlin.\", use_tokenizer=False)\r\nprint(sentence)\r\n```\r\n\r\n### `TransformerWordEmbeddings` now handle long documents by default \r\n\r\nPreviously, so had to set `allow_long_sentences=True` to enable handling of long sequences (greater than 512 subtokens) in `TransformerWordEmbeddings`. This is no longer necessary as this value is now set to `True` by default.\r\n\r\n\r\n## Bug fixes\r\n- Fix serialization of `BytePairEmbeddings` (#1802)\r\n- Fix issues with loading models that use `ELMoEmbeddings` (#1803)\r\n- Allow longer lengths in transformers that can handle more than 512 subtokens (#1804)\r\n- Fix encoding for WASSA datasets (#1766) \r\n- Update BPE package (#1764)  \r\n- Improve documentation (#1752 #1778)\r\n- Fix evaluation of `TextClassifier` if no `label_type` is passed (#1748)\r\n- Remove torch version checks that throw errors (#1744)\r\n- Update DaNE dataset URL (#1800) \r\n- Fix weight extraction error for empty sentences (#1805)",
        "dateCreated": "2020-08-17T12:59:20Z",
        "datePublished": "2020-08-17T13:34:00Z",
        "html_url": "https://github.com/flairNLP/flair/releases/tag/v0.6",
        "name": "Release 0.6",
        "tag_name": "v0.6",
        "tarball_url": "https://api.github.com/repos/flairNLP/flair/tarball/v0.6",
        "url": "https://api.github.com/repos/flairNLP/flair/releases/29653823",
        "zipball_url": "https://api.github.com/repos/flairNLP/flair/zipball/v0.6"
      },
      {
        "authorType": "User",
        "author_name": "alanakbik",
        "body": "Release 0.5.1 with new features, datasets and models, including support for sentence transformers, transformer embeddings for arbitrary length sentences, new Dutch NER models, new tasks and more refactorings of evaluation and training routines to better organize the code!\r\n\r\n# New Features and Enhancements: \r\n\r\n### TransformerWordEmbeddings can now process long sentences (#1680) \r\n\r\nAdds a heuristic as a workaround to the max sequence length of some transformer embeddings, making it possible to now embed sequences of arbitrary length if you set `allow_long_sentences=True`, like so: \r\n\r\n```python\r\nTransformerWordEmbeddings(\r\n        allow_long_sentences=True, # set allow_long_sentences to True to enable this features\r\n),\r\n```\r\n\r\n### Setting random seeds (#1671) \r\n\r\nIt is now possible to set seeds when loading and downsampling corpora, so that the sample is always the same:\r\n\r\n```python\r\n# set a random seed \r\nimport random\r\nrandom.seed(4)\r\n\r\n# load and downsample corpus\r\ncorpus = SENTEVAL_MR(filter_if_longer_than=50).downsample(0.1)\r\n\r\n# print first sentence of dev and test \r\nprint(corpus.dev[0])\r\nprint(corpus.test[0])\r\n```\r\n\r\n### Make reprojection layer optional (#1676)\r\n\r\nMakes the reprojection layer optional in SequenceTagger. You can control this behavior through the `reproject_embeddings` parameter. If you set it to `True`, embeddings are reprojected via linear map to identical size. If set to `False`, no reprojection happens. If you set this parameter to an integer, the linear map maps embedding vectors to vectors of this size. \r\n\r\n```python\r\n# tagger with standard reprojection\r\ntagger = SequenceTagger(\r\n    hidden_size=256,\r\n    [...]\r\n    reproject_embeddings=True,\r\n)\r\n\r\n# tagger without reprojection\r\ntagger = SequenceTagger(\r\n    hidden_size=256,\r\n    [...]\r\n    reproject_embeddings=False,\r\n)\r\n\r\n# reprojection to vectors of length 128\r\ntagger = SequenceTagger(\r\n    hidden_size=256,\r\n    [...]\r\n    reproject_embeddings=128,\r\n)\r\n```\r\n\r\n### Set label name when predicting (#1671) \r\n\r\nYou can now optionally specify the \"label name\" of the predicted label. This may be useful if you want to for instance run two different NER models on the same sentence: \r\n\r\n```python\r\nsentence = Sentence('I love Berlin')\r\n\r\n# load two NER taggers\r\ntagger_1 = SequenceTagger.load('ner')\r\ntagger_2 = SequenceTagger.load('ontonotes-ner')\r\n\r\n# specify label name of tagger_1 to be 'conll03_ner'\r\ntagger_1.predict(sentence, label_name='conll03_ner')\r\n\r\n# specify label name of tagger_2 to be 'onto_ner'\r\ntagger_1.predict(sentence, label_name='onto_ner')\r\n\r\nprint(sentence)\r\n```\r\n\r\nThis may be useful if you have multiple ner taggers and wish to tag the same sentence with them. Then you can distinguish between the tags by the taggers. It is also now no longer possible to give the predict method a string - you now must pass a sentence.\r\n\r\n### Sentence Transformers (#1696) \r\n\r\nAdds the `SentenceTransformerDocumentEmbeddings` class so you get embeddings from the [`sentence-transformer`](https://github.com/UKPLab/sentence-transformers) library. Use as follows:\r\n\r\n```python\r\nfrom flair.data import Sentence\r\nfrom flair.embeddings import SentenceTransformerDocumentEmbeddings\r\n\r\n# init embedding\r\nembedding = SentenceTransformerDocumentEmbeddings('bert-base-nli-mean-tokens')\r\n\r\n# create a sentence\r\nsentence = Sentence('The grass is green .')\r\n\r\n# embed the sentence\r\nembedding.embed(sentence)\r\n```\r\n\r\nYou can find a full list of their pretained models [here](https://docs.google.com/spreadsheets/d/14QplCdTCDwEmTqrn1LH4yrbKvdogK4oQvYO1K1aPR5M/edit#gid=0).\r\n\r\n### Other enhancements\r\n- Update to transformers 3.0.0 (#1727)\r\n- Better Memory mode presets for classification corpora (#1701) \r\n- ClassificationDataset now also accepts line with \"\\t\" seperator additionaly to blank spaces (#1654) \r\n- Change default fine-tuning in DocumentPoolEmbeddings to \"none\" (#1675) \r\n- Short-circuit the embedding loop (#1684) \r\n- Add option to pass kwargs into transformer models when initializing model (#1694) \r\n\r\n\r\n# New Datasets and Models \r\n\r\n### Two new dutch NER models (#1687) \r\n\r\nThe new default model is a BERT-based RNN model with the highest accuracy: \r\n\r\n```python\r\nfrom flair.data import Sentence\r\nfrom flair.models import SequenceTagger\r\n\r\n# load the default BERT-based model\r\ntagger = SequenceTagger.load('nl-ner')\r\n\r\n# tag sentence\r\nsentence = Sentence('Ik hou van Amsterdam')\r\ntagger.predict(sentence)\r\n```\r\n\r\nYou can also load a Flair-based RNN model (might be faster on some setups): \r\n\r\n```python\r\n# load the default BERT-based model\r\ntagger = SequenceTagger.load('nl-ner-rnn')\r\n```\r\n\r\n### Corpus of communicative functions (#1683) and pre-trained model (#1706)\r\n\r\nAdds corpus of communicate functions in scientific literature, described in this [LREC paper](https://www.researchgate.net/publication/339658767_An_Evaluation_Dataset_for_Identifying_Communicative_Functions_of_Sentences_in_English_Scholarly_Papers) and available [here](https://github.com/Alab-NII/FECFevalDataset). Load with: \r\n\r\n```python\r\ncorpus = COMMUNICATIVE_FUNCTIONS()\r\nprint(corpus)\r\n```\r\n\r\nWe also ship a pre-trained model on this corpus, which you can load with: \r\n```python\r\n# load communicative function tagger\r\ntagger = TextClassifier.load('communicative-functions')\r\n\r\n# load communicative function tagger\r\nsentence = Sentence(\"However, previous approaches are limited in scalability .\")\r\n\r\n# predict and print labels\r\ntagger.predict(sentence)\r\nprint(sentence.labels)\r\n```\r\n\r\n\r\n### Keyword Extraction Corpora (#1629) and pre-trained model (#1689) \r\n\r\nAdded 3 datasets available for [keyphrase extraction](https://github.com/midas-research/keyphrase-extraction-as-sequence-labeling-data) via sequence labeling:  [Inspec](https://github.com/midas-research/keyphrase-extraction-as-sequence-labeling-data/tree/master/Inspec),  [SemEval-2017](https://github.com/midas-research/keyphrase-extraction-as-sequence-labeling-data/tree/master/SemEval-2017)  and  [Processed SemEval-2010](https://github.com/midas-research/keyphrase-extraction-as-sequence-labeling-data/tree/master/processed_semeval-2010)  \r\n\r\nLoad like this: \r\n\r\n```python\r\ninspec_corpus = INSPEC()\r\nsemeval_2010_corpus = SEMEVAL2010()\r\nsemeval_2017 = SEMEVAL2017()\r\n```\r\n\r\nWe also ship a pre-trained model on this corpus, which you can load with: \r\n\r\n```python\r\n# load keyphrase tagger\r\ntagger = SequenceTagger.load('keyphrase')\r\n\r\n# load communicative function tagger\r\nsentence = Sentence(\"Here, we describe the engineering of a new class of ECHs through the \"\r\n                    \"functionalization of non-conductive polymers with a conductive choline-based \"\r\n                    \"bio-ionic liquid (Bio-IL).\", use_tokenizer=True)\r\n\r\n# predict and print labels\r\ntagger.predict(sentence)\r\nprint(sentence)\r\n```\r\n\r\n### Swedish NER (#1652) \r\n\r\nAdd corpus for swedish NER using dataset https://github.com/klintan/swedish-ner-corpus/. Load with: \r\n\r\n```python\r\ncorpus = NER_SWEDISH()\r\nprint(corpus)\r\n```\r\n\r\n### German Legal Named Entity Recognition (#1697)\r\n\r\nAdds corpus of legal named entities for German. Load with: \r\n```python\r\ncorpus = LER_GERMAN()\r\nprint(corpus)\r\n```\r\n\r\n# Refactoring of evaluation \r\n\r\nWe made a number of refactorings to the evaluation routines in Flair. In short: whenever possible, we now use the evaluation methods of sklearn (instead of our own implementations which kept getting issues). This applies to text classification and (most) sequence tagging. \r\n\r\nA notable exception is \"span-F1\" which is used to evaluate NER because there is no good way of counting true negatives. After this PR, our implementation should now exactly mirror the original `conlleval` script of the CoNLL-02 challenge. In addition to using our reimplementation, an output file is now automatically generated that can be directly used with the `conlleval` script. \r\n\r\nIn more detail, this PR makes the following changes: \r\n\r\n- `Span` is now a list of `Token` and can now be iterated like a sentence\r\n- `flair.DataLoader` is now used throughout \r\n- The `evaluate()` interface in the `Model` base class is changed so that it no longer requires a data loader, but ran run either over list of `Sentence` or a `Dataset`\r\n- `SequenceTagger.evaluate()` now explicitly distinguishes between F1 and Span-F1. In the latter case, no TN are counted (#1663) and a non-sklearn implementation is used. \r\n- In the `evaluate()` method of the `SequenceTagger` and `TextClassifier`, we now explicitly call the `.predict() `method. \r\n\r\n# Bug fixes:\r\n\r\n- Fix figsize issue (#1622)\r\n- Allow strings to be passed instead of Path (#1637)\r\n- Fix segtok tokenization issue (#1653)\r\n- Serialize dropout in `SequenceTagger` (#1659)\r\n- Fix serialization error in `DocumentPoolEmbeddings` (#1671) \r\n- Fix subtokenization issues in transformers (#1674) \r\n- Add new datasets to __init__.py (#1677)\r\n- Fix deprecation warnings due to invalid escape sequences. (#1678) \r\n- Fix PooledFlairEmbeddings deserialization error (#1604) \r\n- Fix transformer tokenizer deserialization (#1686)\r\n- Fix issues caused by embedding mode and lambda functions in ELMoEmbeddings (#1692) \r\n- Fix serialization error in PooledFlairEmbeddings (#1593) \r\n- Fix mean pooling in PooledFlairEmbeddings (#1698) \r\n- Fix condition to assign whitespace_after attribute in the build_spacy_tokenizer wraper (#1700) \r\n- Fix WIKINER encoding for windows (#1713)\r\n- Detect and ignore empty sentences in BERT embeddings (#1716)\r\n- Fix error in returning multiple classes (#1717)\r\n",
        "dateCreated": "2020-07-05T21:35:50Z",
        "datePublished": "2020-07-05T21:39:00Z",
        "html_url": "https://github.com/flairNLP/flair/releases/tag/v0.5.1",
        "name": "Release 0.5.1",
        "tag_name": "v0.5.1",
        "tarball_url": "https://api.github.com/repos/flairNLP/flair/tarball/v0.5.1",
        "url": "https://api.github.com/repos/flairNLP/flair/releases/28005472",
        "zipball_url": "https://api.github.com/repos/flairNLP/flair/zipball/v0.5.1"
      },
      {
        "authorType": "User",
        "author_name": "alanakbik",
        "body": "Release 0.5 with tons of new models, embeddings and datasets, support for fine-tuning transformers, greatly improved sentiment analysis models for English, tons of new features and big internal refactorings to better organize the code!\r\n\r\n# New Fine-tuneable Transformers (#1494 #1544)\r\n\r\nFlair 0.5 adds support for transformers and fine-tuning with two new embeddings classes: `TransformerWordEmbeddings` and `TransformerDocumentEmbeddings`, for word- and document-level transformer embeddings respectively. Both classes can be initialized with a model name that indicates what type of transformer (BERT, XLNet, RoBERTa, etc.) you wish to use (check the full list [Here](https://huggingface.co/transformers/pretrained_models.html))\r\n\r\n### Transformer Word Embeddings\r\n\r\nIf you want to embed the words in a sentence with transformers, do it like this: \r\n\r\n```python\r\nfrom flair.embeddings import TransformerWordEmbeddings\r\n\r\n# init embedding\r\nembedding = TransformerWordEmbeddings('bert-base-uncased')\r\n\r\n# create a sentence\r\nsentence = Sentence('The grass is green .')\r\n\r\n# embed words in sentence\r\nembedding.embed(sentence)\r\n```\r\n\r\nIf instead you want to use RoBERTa, do: \r\n\r\n```python\r\nfrom flair.embeddings import TransformerWordEmbeddings\r\n\r\n# init embedding\r\nembedding = TransformerWordEmbeddings('roberta-base')\r\n\r\n# create a sentence\r\nsentence = Sentence('The grass is green .')\r\n\r\n# embed words in sentence\r\nembedding.embed(sentence)\r\n```\r\n\r\n### Transformer Document Embeddings\r\n\r\nTo get a single embedding for the whole document with BERT, do:\r\n\r\n```python\r\nfrom flair.embeddings import TransformerDocumentEmbeddings\r\n\r\n# init embedding\r\nembedding = TransformerDocumentEmbeddings('bert-base-uncased')\r\n\r\n# create a sentence\r\nsentence = Sentence('The grass is green .')\r\n\r\n# embed the sentence\r\nembedding.embed(sentence)\r\n```\r\n\r\nIf instead you want to use RoBERTa, do: \r\n\r\n```python\r\nfrom flair.embeddings import TransformerDocumentEmbeddings\r\n\r\n# init embedding\r\nembedding = TransformerDocumentEmbeddings('roberta-base')\r\n\r\n# create a sentence\r\nsentence = Sentence('The grass is green .')\r\n\r\n# embed the sentence\r\nembedding.embed(sentence)\r\n```\r\n\r\n### Text classification by fine-tuning a transformer\r\n\r\nImportantly, you can now fine-tune transformers to get state-of-the-art accuracies in text classification tasks.\r\nUse `TransformerDocumentEmbeddings` for this and set `fine_tune=True`. Then, use the following example code: \r\n\r\n\r\n```python\r\nfrom torch.optim.adam import Adam\r\n\r\nfrom flair.data import Corpus\r\nfrom flair.datasets import TREC_6\r\nfrom flair.embeddings import TransformerDocumentEmbeddings\r\nfrom flair.models import TextClassifier\r\nfrom flair.trainers import ModelTrainer\r\n\r\n# 1. get the corpus\r\ncorpus: Corpus = TREC_6()\r\n\r\n# 2. create the label dictionary\r\nlabel_dict = corpus.make_label_dictionary()\r\n\r\n# 3. initialize transformer document embeddings (many models are available)\r\ndocument_embeddings = TransformerDocumentEmbeddings('distilbert-base-uncased', fine_tune=True)\r\n\r\n# 4. create the text classifier\r\nclassifier = TextClassifier(document_embeddings, label_dictionary=label_dict)\r\n\r\n# 5. initialize the text classifier trainer with Adam optimizer\r\ntrainer = ModelTrainer(classifier, corpus, optimizer=Adam)\r\n\r\n# 6. start the training\r\ntrainer.train('resources/taggers/trec',\r\n              learning_rate=3e-5, # use very small learning rate\r\n              mini_batch_size=16,\r\n              mini_batch_chunk_size=4, # optionally set this if transformer is too much for your machine\r\n              max_epochs=5, # terminate after 5 epochs\r\n              )\r\n```\r\n\r\n# New Taggers, Embeddings and Datasets\r\n\r\nFlair 0.5 adds a ton of new taggers, embeddings and datasets.\r\n\r\n## New Taggers\r\n\r\n### New sentiment models (#1613)\r\n\r\nWe added new sentiment models for English. The new models are trained over a combined corpus of sentiment dataset, including Amazon product reviews. So they should be applicable to more domains than the old sentiment models that were only trained with movie reviews. \r\n\r\nThere are two new models, a transformer-based model you can load like this: \r\n\r\n```python\r\n# load tagger\r\nclassifier = TextClassifier.load('sentiment')\r\n\r\n# predict for example sentence\r\nsentence = Sentence(\"enormously entertaining for moviegoers of any age .\")\r\nclassifier.predict(sentence)\r\n\r\n# check prediction\r\nprint(sentence)\r\n```\r\n\r\nAnd a faster, slightly less accurate model based on RNNs you can load like this:\r\n\r\n```python\r\nclassifier = TextClassifier.load('sentiment-fast')\r\n```\r\n\r\n### Fine-grained POS models for English (#1625)\r\n\r\nAdds fine-grained POS models for English so you now have the option between 'pos' and 'upos' models for fine-grained and universal dependencies respectively. Load like this: \r\n\r\n```python\r\n# Fine-grained POS model\r\ntagger = SequenceTagger.load('pos')\r\n\r\n# Fine-grained POS model (fast variant)\r\ntagger = SequenceTagger.load('pos-fast')\r\n\r\n# Universal POS model\r\ntagger = SequenceTagger.load('upos')\r\n\r\n# Universal POS model (fast variant)\r\ntagger = SequenceTagger.load('upos-fast')\r\n```\r\n\r\n### Added Malayalam POS and XPOS tagger model (#1522)\r\n\r\n### Added taggers for historical German speech and thought (#1532)\r\n\r\n## New Embeddings \r\n\r\n### Added language models for historical German by @redewiedergabe (#1507)\r\n\r\nLoad the language models with:\r\n\r\n```python\r\nembeddings_forward = FlairEmbeddings('de-historic-rw-forward')\r\nembeddings_backward = FlairEmbeddings('de-historic-rw-backward')\r\n```\r\n\r\n### Added Malayalam flair embeddings models (#1458)\r\n\r\n```python\r\nembeddings_forward = FlairEmbeddings('ml-forward')\r\nembeddings_backward = FlairEmbeddings('ml-backward')\r\n```\r\n\r\n### Added Flair Embeddings from CLEF HIPE Shared Task (#1554)\r\n\r\nAdds the recently trained Flair embeddings on historic newspapers for German/English/French provided by the [CLEF HIPE shared task](https://impresso.github.io/CLEF-HIPE-2020/).\r\n\r\n## New Datasets\r\n\r\n### Added NER dataset for Finnish (#1620)\r\n\r\nYou can now load a Finnish NER corpus with \r\n```python\r\nner_finnish = flair.datasets.NER_FINNISH()\r\n```\r\n\r\n### Added DaNE dataset (#1425)\r\n\r\nYou can now load a Danish NER corpus with \r\n```python\r\ndane = flair.datasets.DANE()\r\n```\r\n\r\n### Added SentEval classification datasets (#1454)\r\n\r\nAdds 6 SentEval classification datasets to Flair:\r\n\r\n```python\r\nsenteval_corpus_1 = flair.datasets.SENTEVAL_CR()\r\nsenteval_corpus_2 = flair.datasets.SENTEVAL_MR()\r\nsenteval_corpus_3 = flair.datasets.SENTEVAL_SUBJ()\r\nsenteval_corpus_4 = flair.datasets.SENTEVAL_MPQA()\r\nsenteval_corpus_5 = flair.datasets.SENTEVAL_SST_BINARY()\r\nsenteval_corpus_6 = flair.datasets.SENTEVAL_SST_GRANULAR()\r\n```\r\n\r\n### Added Sentiment Datasets (#1545) \r\n\r\nAdds two new sentiment datasets to Flair, namely AMAZON_REVIEWS, a very large corpus of Amazon reviews with sentiment labels, and SENTIMENT_140, a corpus of tweets labeled with sentiment.\r\n\r\n```python\r\namazon_reviews = flair.datasets.AMAZON_REVIEWS()\r\nsentiment_140 = flair.datasets.SENTIMENT_140()\r\n```\r\n\r\n### Added BIOfid dataset (#1589) \r\n```python\r\nbiofid = flair.datasets.BIOFID()\r\n```\r\n\r\n# Refactorings \r\n\r\n## Any DataPoint can now be labeled (#1450)\r\n\r\nRefactored the `DataPoint` class and classes that inherit from it (`Token`, `Sentence`, `Image`, `Span`, etc.) so that all have the same methods for adding and accessing labels. \r\n\r\n- `DataPoint` base class now defined labeling methods (closes #1449) \r\n- Labels can no longer be passed to `Sentence` constructor, so instead of:\r\n```python\r\nsentence_1 = Sentence(\"this is great\", labels=[Label(\"POSITIVE\")])\r\n```\r\nyou should now do: \r\n```python\r\nsentence_1 = Sentence(\"this is great\")\r\nsentence_1.add_label('sentiment', 'POSITIVE')\r\n```\r\nor: \r\n```python\r\nsentence_1 = Sentence(\"this is great\").add_label('sentiment', 'POSITIVE')\r\n```\r\n\r\nNote that Sentence labels now have a `label_type` (in the example that's 'sentiment').\r\n\r\n- The `Corpus` method `_get_class_to_count` is renamed to `_count_sentence_labels`\r\n- The `Corpus` method `_get_tag_to_count` is renamed to `_count_token_labels`\r\n- `Span` is now a `DataPoint` (so it has an `embedding` and `labels`)\r\n\r\n## Embeddings module was split into smaller submodules (#1588)\r\n\r\nSplit the previously huge `embeddings.py` into several submodules organized in an `embeddings/` folder. The submodules are:\r\n\r\n- `token.py` for all `TokenEmbeddings` classes\r\n- `document.py` for all  `DocumentEmbeddings` classes\r\n- `image.py` for all `ImageEmbeddings` classes\r\n- `legacy.py` for embeddings that are now deprecated\r\n- `base.py` for remaining basic classes\r\n\r\nAll embeddings are still exposed through the embeddings package, so the command to load them doesn't change, e.g.: \r\n\r\n```python\r\nfrom flair.embeddings import FlairEmbeddings\r\nembeddings = FlairEmbeddings('news-forward')\r\n```\r\nso specifying the submodule is not needed.\r\n\r\n## Datasets module was split into smaller submodules (#1510)\r\n\r\nSplit the previously huge `datasets.py` into several submodules organized in a `datasets/` folder. The submodules are:\r\n\r\n- `sequence_labeling.py` for all sequence labeling datasets\r\n- `document_classification.py` for all document classification datasets\r\n- `treebanks.py` for all dependency parsed corpora (UD treebanks)\r\n- `text_text.py` for all bi-text datasets (currently only parallel corpora)\r\n- `text_image.py` for all paired text-image datasets (currently only Feidegger)\r\n- `base.py` for remaining basic classes\r\n\r\nAll datasets are still exposed through the datasets package, so it is still possible to load corpora with \r\n```python\r\nfrom flair.datasets import TREC_6\r\n```\r\nwithout specifying the submodule.\r\n\r\n## Other refactorings\r\n\r\n- Refactor datasets for code legibility (#1394)\r\n\r\nSmall refactorings on `flair.datasets` for easier code legibility and fewer redundancies, removing about 100 lines of code: (1)  Moved the default sampling logic from all corpora classes to the parent `Corpus` class. You can now instantiate a `Corpus` only with a train file which will trigger the sampling. (2) Moved the default logic for identifying train, dev and test files into a dedicated method to avoid duplicates in code.\r\n\r\n- Extend string output of Sentence (#1452)\r\n\r\n# Other \r\n\r\n## New Features\r\n\r\n### Add option to specify document delimiter for language model training (#1541) \r\n\r\nYou now have the option of specifying a document_delimiter when training a LanguageModel. Say, you have a corpus of textual lists and use \"[SEP]\" to mark boundaries between two lists, like this: \r\n\r\n```\r\nColors:\r\n- blue\r\n- green\r\n- red\r\n[SEP]\r\nCities:\r\n- Berlin\r\n- Munich\r\n[SEP]\r\n...\r\n```\r\n\r\nThen you can now train a language model by setting the `document_delimiter` in the `TextCorpus` and `LanguageModel` objects. This will make sure only documents as a whole will get shuffled during training (i.e. the lists in the above example): \r\n\r\n```python\r\n# your document delimiter\r\ndelimiter = '[SEP]'\r\n\r\n# set it when you load the corpus\r\ncorpus = TextCorpus(\r\n    \"data/corpora/conala-corpus/\",\r\n    dictionary,\r\n    is_forward_lm,\r\n    character_level=True,\r\n    document_delimiter=delimiter,\r\n)\r\n\r\n# set it when you init the language model\r\nlanguage_model = LanguageModel(\r\n    dictionary,\r\n    is_forward_lm=True,\r\n    hidden_size=512,\r\n    nlayers=1,\r\n    document_delimiter=delimiter\r\n)\r\n\r\n# train your language model as always\r\ntrainer = LanguageModelTrainer(language_model, corpus)\r\n```\r\n### Allow column delimiter to be set in ColumnCorpus (#1526)\r\n\r\nAdded the possibility to set a different column delimite for `ColumnCorpus`, i.e. \r\n\r\n```python\r\ncorpus = ColumnCorpus(\r\n    Path(\"/path/to/corpus/\"),\r\n    column_format={0: 'text', 1: 'ner'},\r\n    column_delimiter='\\t', # set a different delimiter\r\n)\r\n```\r\n\r\nif you want to read a tab-separated column corpus.\r\n\r\n### Improvements in classification corpus datasets (#1545)\r\n\r\nThere are a number of improvements for the `ClassificationCorpus` and `ClassificationDataset` classes: \r\n- It is now possible to select from three memory modes ('full', 'partial' and 'disk'). Use full if the entire dataset and all objects fit into memory. Use 'partial' if it doesn't and use 'disk' if even 'partial' does not fit. \r\n- It is also now possible to provide \"name maps\" to rename labels in datasets. For instance, some sentiment analysis datasets use '0' and '1' as labels, while some others use 'POSITIVE' and 'NEGATIVE'. By providing name maps you can rename labels so they are consistent across datasets. \r\n- You can now choose which splits to downsample (for instance you might want to downsample 'train' and 'dev' but not 'test')\r\n- You can now specify the option \"filter_if_longer_than\", to filter all sentences that have more than the number of provided whitespaces. This is useful to limit corpus size as some sentiment analysis datasets are gigantic. \r\n\r\n### Added different ways to combine ELMo layers (#1547)\r\n\r\n### Improved default annealing scheme to anneal against score and loss (#1570)\r\n\r\nAdd new scheduler that uses dev score as main metric to anneal against, but additionally uses dev loss in case two epochs have the same dev score.\r\n\r\n### Added option for hidden state position in FlairEmbeddings (#1571) \r\n\r\nAdds the option to choose which hidden state to use in FlairEmbeddings: either the state at the end of each word, or the state at the whitespace after. Default is the state at the whitespace after. \r\n\r\nYou can change the default like this:\r\n```python\r\nembeddings = FlairEmbeddings('news-forward', with_whitespace=False)\r\n```\r\n\r\nThis configuration seems to be better for syntactic tasks. For POS tagging, it seems that you should set `with_whitespace=False`. For instance, on UD_ENGLISH POS-tagging, we get **96.56 +- 0.03** with whitespace and **96.72 +- 0.04** without, averaged over three runs. \r\n\r\nSee the discussion in #1362 for more details. \r\n\r\n### Other features\r\n\r\n- Added the option of passing different tokenizers when loading classification datasets (#1579)\r\n\r\n- Added option for true whitespaces in ColumnCorpus #1583\r\n\r\n- Configurable cache_root from environment variable (#507) \r\n\r\n## Performance improvements\r\n\r\n- Improve performance for loading not-in-memory corpus (#1413)\r\n\r\n- A new lmdb based alternative backend for word embeddings (#1515 #1536)\r\n\r\n- Slim down requirements (#1419)\r\n\r\n## Bug Fixes\r\n\r\n- Fix issue where flair was crashing for cpu only version of pytorch (#1393 #1418)\r\n\r\n- Fix GPU memory error in PooledFlairEmbeddings (#1417)\r\n\r\n- Various small fixes (#1402 #1533 #1511 #1560 #1616) \r\n\r\n- Improve documentation (#1446 #1447 #1520 #1525 #1556)\r\n\r\n- Fix various issues in classification datasets (#1499)\r\n",
        "dateCreated": "2020-05-24T11:55:38Z",
        "datePublished": "2020-05-24T12:00:55Z",
        "html_url": "https://github.com/flairNLP/flair/releases/tag/v0.5",
        "name": "Release 0.5",
        "tag_name": "v0.5",
        "tarball_url": "https://api.github.com/repos/flairNLP/flair/tarball/v0.5",
        "url": "https://api.github.com/repos/flairNLP/flair/releases/26818822",
        "zipball_url": "https://api.github.com/repos/flairNLP/flair/zipball/v0.5"
      },
      {
        "authorType": "User",
        "author_name": "alanakbik",
        "body": "This is an enhancement release that slims down Flair for quicker/easier installation and smaller library size. It also makes Flair compatible with torch 1.4.0 and adds enhancements that reduce model size and improve runtime speed for some embeddings. New features include the ability to steer the precision/recall tradeoff during training of models and support for CamemBERT embeddings. \r\n\r\n\r\n# Memory, Runtime and Dependency Improvements\r\n\r\n## Slim down dependency tree (#1296 #1299 #1335 #1336)\r\n\r\nWe want to keep list of dependencies of Flair generally small to avoid errors like #1245 and keep the library small and quick to setup. So we removed dependencies that were each only used for one particular feature, namely: \r\n- `ipython` and  `ipython-genutils`, only used for visualization settings in iPython notebooks\r\n- `tiny_tokenizer`, used for Japanese tokenization (replaced with instructions for how to install for all users who want to use Japanese tokenizers)\r\n- `pymongo`, used for MongoDB datasets (replaced with instructions for how to install for all users who want to use MongoDB datasets)\r\n- `torchvision`, now only loaded when needed \r\n\r\nWe also relaxed version requirements for easier installation on Google CoLab (#1335 #1336)\r\n\r\n## Dramatic speed-up of BERT embeddings (#1308) \r\n\r\n@shoarora optimized the BERTEmbeddings implementation by removing redundant calls. This was shown to lead to dramatic speed improvements.\r\n\r\n## Reduce size of models that use WordEmbeddings (#1315) \r\n\r\n@timnon added a method to replace word embeddings in trained model with sqlite database to dramatically reduce memory usage. Creates class `WordEmbeedingsStore` which can be used to replace a `WordEmbeddings`-instance in a flair model via duck-typing. By using this, @timnon  was able to reduce our ner-servers memory consumption from 6gig to 600mb (10x decrease) by adding a few lines of code. It can be tested using the following lines (also in the docstring). First create a headless version of a model without word embeddings:\r\n\r\n```python\r\nfrom flair.inference_utils import WordEmbeddingsStore\r\nfrom flair.models import SequenceTagger\r\nimport pickle\r\ntagger = SequenceTagger.load(\"multi-ner-fast\")\r\nWordEmbeddingsStore.create_stores(tagger)\r\npickle.dump(tagger, open(\"multi-ner-fast-headless.pickle\", \"wb\"))\r\n```\r\nand then to run the stored headless model without word embeddings, use:\r\n```python\r\nfrom flair.data import Sentence\r\ntagger = pickle.load(open(\"multi-ner-fast-headless.pickle\", \"rb\"))\r\nWordEmbeddingsStore.load_stores(tagger)\r\ntext = \"Schade um den Ameisenb\u00e4ren. Lukas B\u00e4rfuss ver\u00f6ffentlicht Erz\u00e4hlungen aus zwanzig Jahren.\"\r\nsentence = Sentence(text)\r\ntagger.predict(sentence)\r\n```\r\n\r\n\r\n# New Features\r\n\r\n## Prioritize precision/recall or specific classes during training (#1345)\r\n\r\n@klasocki added ways to steer the precision/recall tradeoff during training of models, as well as prioritize certain classes. This option was added to the `SequenceTagger` and the `TextClassifier`. \r\n\r\nYou can steer precision/recall tradeoff by adding the `beta` parameter, which indicates how many more times recall is important than precision. So if you set `beta=0.5`, precision becomes twice as important than recall. If you set `beta=2`, recall becomes twice as important as precision. Do it like this: \r\n\r\n```python\r\ntagger = SequenceTagger(\r\n    hidden_size=256,\r\n    embeddings=embeddings,\r\n    tag_dictionary=tag_dictionary,\r\n    tag_type=tag_type,\r\n    beta=0.5)\r\n```\r\n\r\nIf you want to prioritize classes, you can pass a `weight_loss` dictionary to the model classes. For instance, to prioritize learning the NEGATIVE class in a sentiment tagger, do: \r\n\r\n```python\r\ntagger = TextClassifier(\r\n    document_embeddings=embeddings,\r\n    label_dictionary=tag_dictionary,\r\n    loss_weights={'NEGATIVE': 10.})\r\n```\r\n\r\nwhich will increase the importance of class NEGATIVE by a factor of 10.\r\n\r\n## CamemBERT Embeddings (#1297)\r\n@stefan-it added support for the recently proposed French language model: CamemBERT.\r\n\r\nThanks to the awesome \ud83e\udd17/Transformers library, CamemBERT can be used in Flair like in this example:\r\n\r\n```python\r\nfrom flair.data import Sentence\r\nfrom flair.embeddings import CamembertEmbeddings\r\n\r\nembedding = CamembertEmbeddings()\r\n\r\nsentence = Sentence(\"J'aime le camembert !\")\r\nembedding.embed(sentence)\r\n\r\nfor token in sentence.tokens:\r\n  print(token.embedding)\r\n```\r\n\r\n# Bug fixes and enhancements\r\n\r\n- Fix new RNN format for torch 1.4.0 (#1360, #1382 )\r\n- Fix memory issue in PooledFlairEmbeddings (#1337 #1339)\r\n- Correct subtoken mapping function for GPT-2 and RoBERTa (#1242)\r\n- Update the transformers library to the latest 2.3 version (#1333)\r\n- Add staticmethod decorator to some functions (#1257)\r\n- Add a warning if validation data is too small (#1115)\r\n- Remove leftover printline from MUSE embeddings (#1224)\r\n- Correct generate_text() UTF-8 conversion (#1238)\r\n- Clarify documentation (#1295 #1332)\r\n- Replace sklearn by scikit-learn (#1321)\r\n- Fix off-by-one error in progress logging (#1334)\r\n- Fix typo and annotation (#1341)\r\n- Various improvements (#1347)\r\n- Make load_big_file work with read-only file (#1353)\r\n- Rename tiny_tokenizer to konoha (#1363)\r\n- Make test loss plotting optional (#1372)\r\n- Add pretty print function for Dictionary (#1375)\r\n",
        "dateCreated": "2020-01-24T13:46:03Z",
        "datePublished": "2020-01-24T16:08:32Z",
        "html_url": "https://github.com/flairNLP/flair/releases/tag/v0.4.5",
        "name": "Release 0.4.5",
        "tag_name": "v0.4.5",
        "tarball_url": "https://api.github.com/repos/flairNLP/flair/tarball/v0.4.5",
        "url": "https://api.github.com/repos/flairNLP/flair/releases/23108061",
        "zipball_url": "https://api.github.com/repos/flairNLP/flair/zipball/v0.4.5"
      },
      {
        "authorType": "User",
        "author_name": "alanakbik",
        "body": "Release 0.4.4 introduces dramatic improvements in inference speed for taggers (thanks to many contributions by @pommedeterresautee), Flair embeddings in 300 languages (thanks @stefan-it), modular tokenization and many new features and refactorings.\r\n\r\n \r\n# Speed optimizations\r\n\r\nMany refactorings by @pommedeterresautee to improve inference speed of sequence tagger (#1038 #1053 #1068 #1093 #1130), Flair embeddings (#1074 #1095 #1107 #1132 #1145), word embeddings (#1084), \r\nembeddings memory management (#1082 #1117), general optimizations (#1112) and classification (#1187). \r\n\r\nThe combined improvements **increase inference speed by a factor of 2-3**!\r\n\r\n# New features \r\n\r\n## Modular tokenization (#1022) \r\n\r\nYou can now pass custom tokenizers to `Sentence` objects and `Dataset` loaders to use different tokenizers than the included `segtok` library by implementing a tokenizer method. Currently, in-built support exists for whitespace tokenization, segtok tokenization and Japanese tokenization with mecab (requires mecab to be installed). In the future, we expect support for additional external tokenizers to be added.\r\n\r\nFor instance, if you wish to use Japanese tokanization performed by mecab, you can instantiate the `Sentence` object like this: \r\n\r\n```python\r\nfrom flair.data import build_japanese_tokenizer\r\nfrom flair.data import Sentence\r\n\r\n# instantiate Japanese tokenizer\r\njapanese_tokenizer = build_japanese_tokenizer()\r\n\r\n# init sentence and pass this tokenizer\r\nsentence = Sentence(\"\u79c1\u306f\u30d9\u30eb\u30ea\u30f3\u304c\u597d\u304d\u3067\u3059\u3002\", use_tokenizer=japanese_tokenizer)\r\nprint(sentence)\r\n```\r\n\r\n\r\n## Flair Embeddings for 300 languages  (#1146)\r\n\r\nThanks to @stefan-it, there is now a massivey multilingual Flair embeddings model that covers 300 languages. See #1099 for more info on these embeddings and [this repo](https://github.com/stefan-it/flair-lms#multilingual-flair-embeddings) for more details.\r\n\r\nThis replaces the old multilingual Flair embeddings that were trained for 6 languages. Load them with: \r\n\r\n```python\r\nembeddings_fw = FlairEmbeddings('multi-forward')\r\nembeddings_bw = FlairEmbeddings('multi-backward')\r\n```\r\n\r\n## Multilingual Character Dictionaries (#1157) \r\n\r\nAdds two multilingual character dictionaries computed by @stefan-it. \r\n\r\nLoad with \r\n\r\n```python\r\ndictionary = Dictionary.load('chars-large')\r\nprint(len(dictionary.idx2item))\r\n\r\ndictionary = Dictionary.load('chars-xl')\r\nprint(len(dictionary.idx2item))\r\n```\r\n\r\n## Batch-growth annealing (#1138)\r\n\r\nThe paper [Don't Decay the Learning Rate, Increase the Batch Size](https://arxiv.org/abs/1711.00489) makes the case for increasing the batch size over time instead of annealing the learning rate. \r\n\r\nThis version adds the possibility to have arbitrarily large mini-batch sizes with an accumulating gradient strategy. It introduces the parameter `mini_batch_chunk_size` that you can set to break down large mini-batches into smaller chunks for processing purposes. \r\n\r\nSo let's say you want to have a mini-batch size of 128, but your memory cannot handle more than 32 samples at a time. Then you can train like this: \r\n\r\n```python\r\ntrainer = ModelTrainer(tagger, corpus)\r\ntrainer.train(\r\n    \"path/to/experiment/folder\",\r\n    # set large mini-batch size\r\n    mini_batch_size=128,\r\n    # set chunk size to lower memory requirements\r\n    mini_batch_chunk_size=32,\r\n)\r\n```\r\n\r\nBecause we now can arbitrarly raise mini-batch size, we can now execute the annealing strategy in the above paper. Do it like this: \r\n\r\n```python\r\ntrainer = ModelTrainer(tagger, corpus)\r\ntrainer.train(\r\n    \"path/to/experiment/folder\",\r\n    # set initial mini-batch size\r\n    mini_batch_size=32,\r\n    # choose batch growth annealing \r\n    batch_growth_annealing=True,\r\n)\r\n```\r\n\r\n## Document-level sequence labeling (#1194)\r\n\r\nIntroduces the option for reading entire documents into one Sentence object for sequence labeling. This option is now supported for `CONLL_03`, `CONLL_03_GERMAN` and `CONLL_03_DUTCH` datasets which indicate document boundaries.\r\n\r\nHere's how to train a model on CoNLL-03 on the document level: \r\n\r\n```python\r\n# read CoNLL-03 with document_as_sequence=True\r\ncorpus = CONLL_03(in_memory=True, document_as_sequence=True)\r\n\r\n# what tag do we want to predict?\r\ntag_type = 'ner'\r\n\r\n# 3. make the tag dictionary from the corpus\r\ntag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\r\n\r\n# init simple tagger with GloVe embeddings\r\ntagger: SequenceTagger = SequenceTagger(\r\n    hidden_size=256,\r\n    embeddings=WordEmbeddings('glove'),\r\n    tag_dictionary=tag_dictionary,\r\n    tag_type=tag_type,\r\n)\r\n\r\n# initialize trainer\r\nfrom flair.trainers import ModelTrainer\r\n\r\ntrainer: ModelTrainer = ModelTrainer(tagger, corpus)\r\n\r\n# start training\r\ntrainer.train(\r\n    'path/to/your/experiment',\r\n    # set a much smaller mini-batch size because documents are huge\r\n    mini_batch_size=2,\r\n)\r\n```\r\n\r\n## Option to evaluate on training split (#1202)\r\n\r\nPreviously, the `ModelTrainer` only allowed monitoring of dev and test splits during training. Now, you can also monitor the train split to better check if your method is overfitting. \r\n\r\n## Support for Danish tagging (#1183) \r\n\r\nAdds support for Danish POS and NER thanks to @AmaliePauli! \r\n\r\nUse like this: \r\n\r\n```python\r\nfrom flair.data import Sentence\r\nfrom flair.models import SequenceTagger\r\n\r\n# example sentence\r\nsentence = Sentence(\"K\u00f8benhavn er en fantastisk by .\")\r\n\r\n# load Danish NER model and predict\r\nner_tagger = SequenceTagger.load('da-ner')\r\nner_tagger.predict(sentence)\r\n\r\n# print annotations (NER)\r\nprint(sentence.to_tagged_string())\r\n\r\n# load Danish POS model and predict\r\npos_tagger = SequenceTagger.load('da-pos')\r\npos_tagger.predict(sentence)\r\n\r\n# print annotations (NER + POS)\r\nprint(sentence.to_tagged_string())\r\n```\r\n\r\n## Support for DistilBERT embeddings (#1044) \r\n\r\nYou can use them like this: \r\n\r\n```python\r\nfrom flair.data import Sentence\r\nfrom flair.embeddings import BertEmbeddings\r\n\r\nembeddings = BertEmbeddings(\"distilbert-base-uncased\")\r\n\r\ns = Sentence(\"Berlin and Munich are nice cities .\")\r\nembeddings.embed(s)\r\n\r\nfor token in s.tokens:\r\n  print(token.embedding)\r\n  print(token.embedding.shape)\r\n``` \r\n\r\n## MongoDataset for reading text classification data from a Mongo database (#1192)\r\n\r\nAdds the option of reading data from MongoDB. See [this documentation](https://github.com/zalandoresearch/flair/pull/1192#issuecomment-540015019) on how to use this features.\r\n\r\n## Feidegger corpus (#1199)\r\n\r\nAdds a dataset downloader for the Feidegger corpus consisting of text-image pairs. Instantiate the corpus like this: \r\n\r\n```python\r\nfrom flair.datasets import FeideggerCorpus\r\n\r\n# instantiate Feidegger corpus\r\ncorpus = FeideggerCorpus()\r\n\r\n# print a text-image pair\r\nprint(corpus.train[0])\r\n``` \r\n\r\n# Refactorings \r\n\r\n## Refactor checkpointing mechanism (#1101)\r\n\r\nRefactored the checkpointing mechanism and slimmed down interfaces / code required to load checkpoints. \r\n\r\nIn detail: \r\n\r\n-  The methods `save_checkpoint` and `load_checkpoint` are no longer part of the `flair.nn.Model` interface. Instead, saving and restoring checkpoints is now (fully) performed by the `ModelTrainer`.\r\n- The optimizer state and scheduler state are removed from the `ModelTrainer` constructor since they are no longer required here. \r\n- Loading a checkpoint is now one line of code (previously two lines).\r\n\r\n```python\r\n# 1. initialize trainer as always with a model and a corpus\r\nfrom flair.trainers import ModelTrainer\r\ntrainer: ModelTrainer = ModelTrainer(model, corpus)\r\n\r\n# 2. train your model for 2 epochs\r\ntrainer.train(\r\n    'experiment/folder',\r\n    max_epochs=2,\r\n    # example checkpointing\r\n    checkpoint=True,\r\n)\r\n\r\n# 3. load last checkpoint with one line of code\r\ntrainer = ModelTrainer.load_checkpoint('experiment/folder/checkpoint.pt', corpus)\r\n\r\n# 4. continue training for 2 extra epochs\r\ntrainer.train('experiment/folder_2',  max_epochs=4) \r\n```\r\n\r\n## Refactor data sampling during training (#1154) \r\n\r\nAdds a `FlairSampler` interface to better enable passing custom samplers to the `ModelTrainer`. \r\n\r\nFor instance, if you want to always shuffle your dataset in chunks of 5 to 10 sentences, you provide a sampler like this: \r\n\r\n```python\r\n# your trainer\r\ntrainer: ModelTrainer = ModelTrainer(tagger, corpus)\r\n\r\n# execute training run\r\ntrainer.train('path/to/experiment/folder',\r\n              max_epochs=150,\r\n              # sample data in chunks of 5 to 10\r\n              sampler=ChunkSampler(block_size=5, plus_window=5)\r\n              )\r\n```\r\n\r\n## Other refactorings\r\n\r\n- Switch everything to batch first mode (#1077)\r\n\r\n- Refactor classification to be more consistent with SequenceTagger (#1151) \r\n\r\n- PyTorch-Transformers -> Transformers #1163\r\n\r\n- In-place transpose of tensors (#1047)\r\n\r\n \r\n# Enhancements \r\n\r\n#### Documentation fixes (#1045 #1098 #1121 #1157 #1160 #1168 )\r\n\r\n#### Add option to set ` rnn_type`  used in ` SequenceTagger`  (#1113)\r\n#### Accept string as input in NER predict (#1142)\r\n\r\nExample usage: \r\n\r\n```python\r\n# init tagger\r\ntagger= SequenceTagger.load('ner')\r\n\r\n# predict over list of strings\r\nsentences = tagger.predict(\r\n    [\r\n        'George Washington went to Berlin .', \r\n        'George Berlin lived in Washington .'\r\n    ]\r\n)\r\n\r\n# output predictions\r\nfor sentence in sentences:\r\n    print(sentence.to_tagged_string())\r\n```\r\n\r\n#### Enable One-hot Embeddings of other Tags (#1191) \r\n\r\n\r\n  \r\n# Bug fixes  \r\n\r\n- Fix the learning rate finder (#1119)\r\n- Fix OneHotEmbeddings on Cuda (#1147)\r\n- Fix encoding error in ` CSVClassificationDataset`  (#1055)\r\n- Fix encoding errors related to old windows chars (#1135)\r\n- Fix length error in ` CharacterEmbeddings`  (#1088 )\r\n- Fix tokenizer insert empty token to sentence object (#1226)\r\n- Ensure `StackedEmbeddings`  always has the same embedding order (#1114)\r\n- Use $HOME instead of ~ for ` cache_root`  (#1134)\r\n\r\n\r\n\r\n",
        "dateCreated": "2019-10-20T22:22:27Z",
        "datePublished": "2019-10-20T22:22:35Z",
        "html_url": "https://github.com/flairNLP/flair/releases/tag/v0.4.4",
        "name": "Release 0.4.4",
        "tag_name": "v0.4.4",
        "tarball_url": "https://api.github.com/repos/flairNLP/flair/tarball/v0.4.4",
        "url": "https://api.github.com/repos/flairNLP/flair/releases/20774207",
        "zipball_url": "https://api.github.com/repos/flairNLP/flair/zipball/v0.4.4"
      },
      {
        "authorType": "User",
        "author_name": "alanakbik",
        "body": "Release 0.4.3 includes a host of new features including transformer-based embeddings (roBERTa, XLNet, XLM, etc.), fine-tuneable `FlairEmbeddings`, crosslingual MUSE embeddings, new data loading/sampling methods, speed/memory optimizations, bug fixes and enhancements. It also begins a refactoring of interfaces that prepares more general applicability of Flair to other types of downstream tasks.\r\n\r\n# Embeddings\r\n\r\n## Transformer embeddings (#941 #972 #993) \r\n\r\nUpdates the old `pytorch-pretrained-BERT` library to the latest version of `pytorch-transformers` to support various new Transformer-based architectures for embeddings.\r\n\r\nA total of 7 (new/updated) transformer-based embeddings can be used in Flair now:\r\n\r\n```python\r\nfrom flair.embeddings import (\r\n    BertEmbeddings,\r\n    OpenAIGPTEmbeddings,\r\n    OpenAIGPT2Embeddings,\r\n    TransformerXLEmbeddings,\r\n    XLNetEmbeddings,\r\n    XLMEmbeddings,\r\n    RoBERTaEmbeddings,\r\n)\r\n\r\nbert_embeddings = BertEmbeddings()\r\ngpt1_embeddings = OpenAIGPTEmbeddings()\r\ngpt2_embeddings = OpenAIGPT2Embeddings()\r\ntxl_embeddings = TransformerXLEmbeddings()\r\nxlnet_embeddings = XLNetEmbeddings()\r\nxlm_embeddings = XLMEmbeddings()\r\nroberta_embeddings = RoBERTaEmbeddings()\r\n```\r\n\r\nDetailed benchmarks on the downsampled CoNLL-2003 NER dataset for English can be found in #873 . \r\n\r\n## Crosslingual MUSE Embeddings (#853)\r\n\r\nUse the new `MuseCrosslingualEmbeddings` class to embed any sentence in one of 30 languages into the same embedding space. Behind the scenes the class first does language detection of the sentence to be embedded, and then embeds it with the appropriate language embeddings. If you train a classifier or sequence labeler with (only) this class, it will automatically work across all 30 languages, though quality may widely vary. \r\n\r\nHere's how to embed: \r\n```python\r\n# initialize embeddings\r\nembeddings = MuseCrosslingualEmbeddings()\r\n\r\n# two sentences in different languages\r\nsentence_1 = Sentence(\"This red shoe is new .\")\r\nsentence_2 = Sentence(\"Dieser rote Schuh ist rot .\")\r\n\r\n# language code is auto-detected\r\nprint(sentence_1.get_language_code())\r\nprint(sentence_2.get_language_code())\r\n\r\n# embed sentences\r\nembeddings.embed([sentence_1, sentence_2])\r\n\r\n# print similarities\r\ncos = torch.nn.CosineSimilarity(dim=0, eps=1e-6)\r\nfor token_1, token_2 in zip (sentence_1, sentence_2):\r\n    print(f\"'{token_1.text}' and '{token_2.text}' similarity: {cos(token_1.embedding, token_2.embedding)}\")\r\n\r\n```\r\n\r\n## FastTextEmbeddings  (#879 )\r\n\r\nAdds `FastTextEmbeddings` capable of handling for oov words. Be warned though that these embeddings are huge. `BytePairEmbeddings` are much smaller and reportedly of similar quality so it is probably advisable to use those instead.\r\n\r\n## Fine-tuneable FlairEmbeddings (#922) \r\n\r\nYou can now fine-tune FlairEmbeddings on downstream tasks. You can **fine-tune an existing LM** by simply passing the `fine_tune` parameter in the `FlairEmbeddings` constructor, like this: \r\n\r\n```python\r\nembeddings = FlairEmbeddings('news-foward', fine_tune=True)\r\n```\r\n\r\nYou can also use this option to **task-train a wholly new language model** by passing an empty `LanguageModel` to the `FlairEmbeddings` constructor and the `fine_tune` parameter, like this: \r\n\r\n```python\r\n# make an empty language model\r\nlanguage_model = LanguageModel(\r\n    Dictionary.load('chars'),\r\n    is_forward_lm=True,\r\n    hidden_size=256,\r\n    nlayers=1)\r\n\r\n# init FlairEmbeddings to task-train this model\r\nembeddings = FlairEmbeddings(language_model, fine_tune=True)\r\n```\r\n\r\n\r\n# Optimizations\r\n\r\n## Automatic mixed precision support (#934) \r\n\r\nMixed precision training can significantly speed up training. It can now be enabled by setting `use_amp=True` in the trainer classes. For instance for training language models you can do:\r\n\r\n```python\r\n# train your language model\r\ntrainer = LanguageModelTrainer(language_model, corpus)\r\n\r\ntrainer.train('resources/taggers/language_model',\r\n              sequence_length=256,\r\n              mini_batch_size=256,\r\n              max_epochs=10,\r\n              use_amp=True)\r\n```\r\n\r\nIn our experiments, we saw 3x speedup of training large language models though results vary depending on your model size and experimental setup.\r\n\r\n## Control memory / speed tradeoff during training (#891 #809). \r\n\r\nThis release introduces the `embeddings_storage_mode` parameter to the `ModelTrainer` class and `predict()` methods. This parameter can be one of 'none', 'cpu' and 'gpu' and allows you to control the tradeoff between memory usage and speed during training:\r\n\r\n- If set to '**none**' all embeddings are deleted after usage - this has lowest memory requirements but means that embeddings need to be recomputed at each epoch of training potentially causing a slowdown. \r\n- If set to '**cpu**' all embeddings are moved to CPU memory after usage. During training, this means that they only need to be moved back to GPU for the forward pass, and not recomputed so in many cases this is faster, but requires memory. \r\n-  If set to '**gpu**' all embeddings stay on GPU memory after computation. This eliminates memory shuffling during training, causing a speedup. However this option requires enough GPU memory to be available for all embeddings of the dataset. \r\n\r\nTo use this option during training, simply set the parameter: \r\n\r\n```python\r\n        # initialize trainer\r\n        trainer: ModelTrainer = ModelTrainer(tagger, corpus)\r\n        trainer.train(\r\n            \"path/to/your/model\",\r\n            embeddings_storage_mode='gpu',\r\n        )\r\n```\r\n\r\nThis release also removes the `FlairEmbeddings`-specific disk-caching mechanism. In the future, a more general caching mechanism applicable to all embedding types may potentially be added as a fourth memory management option.\r\n\r\n## Speed-ups on in-memory datasets  (#792) \r\n\r\nA new `DataLoader` abstract base class used in Flair will speed up data loading for in-memory datasets.\r\n\r\n\r\n# Refactoring of interfaces (#891 #843)\r\n\r\nThis release also slims down interfaces of `flair.nn.Model` and adds a new `DataPoint` interface that is currently implemented by the `Token` and `Sentence` classes. The idea is to widen the applicability of Flair to other data types and other tasks. In the future, the `DataPoint` interface will for example also be implemented by an `Image` object and new downstream tasks added to Flair.  \r\n\r\nThe release also slims down the `evaluate()` method in the `flair.nn.Model` interface to take a `DataLoader` instead of a group of parameters. And refactors the logging header logic. Both refactorings prepare adding new new downstream tasks to Flair in the near future. \r\n\r\n# Other features\r\n\r\n## Training Classifiers with CSV files (#826 #952 #967) \r\n\r\nAdds the `CSVClassificationCorpus` so you can train classifiers directly from CSVs instead of first having to convert to FastText format. To load a CSV, you need to pass a `column_name_map` (like in `ColumnCorpus`), which indicates which column(s) in the CSV holds the text and which field(s) the label(s):\r\n\r\n```python\r\ncorpus = CSVClassificationCorpus(\r\n    # path to the data folder containing train / test / dev files\r\n    data_folder='path/to/data',\r\n    # indicates which columns are text and labels\r\n    column_name_map={4: \"text\", 1: \"label_topic\", 2: \"label_subtopic\"},\r\n    # if CSV has a header, you can skip it\r\n    skip_header=True)\r\n```\r\n\r\n## Data sampling (#908) \r\n\r\nWe added the first (of many) data samplers that can be passed to the `ModelTrainer` to influence training. The `ImbalancedClassificationDatasetSampler` for instance will upsample rare classes and downsample common classes in a classification dataset. It may potentially help with imbalanced datasets. Call like this: \r\n```python\r\n    # initialize trainer\r\n    trainer: ModelTrainer = ModelTrainer(tagger, corpus)\r\n    trainer.train(\r\n        'path/to/folder',\r\n        learning_rate=0.1,\r\n        mini_batch_size=32,\r\n        sampler=ImbalancedClassificationDatasetSampler,\r\n    )\r\n```\r\nThere are two experimental chunk samplers (`ChunkSampler` and `ExpandingChunkSampler`) split a dataset into chunks and shuffle them. This preserves some ordering of the original data while also randomizing the data. \r\n\r\n\r\n## Visualization \r\n\r\n- Adds HTML vizualization of sequence labeling (#933). Call like this: \r\n```python\r\nfrom flair.visual.ner_html import render_ner_html\r\n\r\ntagger = SequenceTagger.load('ner')\r\n\r\nsentence = Sentence(\r\n    \"Thibaut Pinot's challenge ended on Friday due to injury, and then Julian Alaphilippe saw \"\r\n    \"his lead fall away. The BBC's Hugh Schofield in Paris reflects on 34 years of hurt.\"\r\n)\r\n\r\ntagger.predict(sentence)\r\nhtml = render_ner_html(sentence)\r\n\r\nwith open(\"sentence.html\", \"w\") as writer:\r\n    writer.write(html)\r\n```    \r\n\r\n- Plotter now returns images for use in iPython notebooks (#943) \r\n- Initial TensorBoard support (#924) \r\n- Add pointer to Flair Visualizer (#1014)\r\n\r\n## Additional parameterization options\r\n\r\n- `CharacterEmbeddings` now let you specify number of hidden states and embedding size (#834) \r\n```python\r\nembedding = CharacterEmbedding(char_embedding_dim=64, hidden_size_char=64)\r\n```\r\n- Adds configuration option for minimal learning rate stopping criterion (#871) \r\n- `num_workers` is a parameter of `LanguageModelTrainer` (#962 )\r\n\r\n## Bug fixes / enhancements\r\n- Updates old pretrained models to remove old bugs / performance issues (#1017)\r\n- Fix error in RNN initialization in `DocumentRNNEmbeddings` (#793) \r\n- `ELMoEmbeddings` now use `flair.device` param (#825) \r\n- Fix download of TREC_6 dataset (#896) \r\n- Fix download of UD_GERMAN-HDT (#980) \r\n- Fix download of WikiNER_German (#1006)\r\n- Fix error in `ColumnCorpus` in which words that begin with hashtags were skipped as comments (#956) \r\n- Fix `max_tokens_per_do`c param in `ClassificationCorpus` (#991)\r\n- Simplify split rule in `ColumnCorpus` (#990) \r\n- Fix import error message for `ELMoEmbeddings` (#1019) \r\n- References to Persian language unified across embeddings (#773) \r\n- Updates most pre-trained models fixing quality issues / bugs (#800) \r\n- Clarifications in documentation (#803 #860 #868) \r\n- Fixes infinite loop for tokens without startpos (#1030) \r\n\r\n## Enhancements\r\n- Adds a learnable initial hidden state to `SequenceTagger` (#899) \r\n- Now keeps order of sentences in mini-batch when embedding (#866)  \r\n- `SequenceTagger` now optionally returns a distribution of tag probabilities over all classes (#782 #949 #1016)\r\n- The model trainer now outputs a 'test.tsv' file that contains prediction of final model when done training (#771 )\r\n- Releases logging handler when finishing training a model (#799) \r\n- Fixes `bad_epochs` in training logs and no longer evaluates on test data at each epoch by default (#818 )\r\n- Convenience method to remove all empty sentences from a corpus (#795)",
        "dateCreated": "2019-08-26T18:16:40Z",
        "datePublished": "2019-08-26T18:26:09Z",
        "html_url": "https://github.com/flairNLP/flair/releases/tag/v0.4.3",
        "name": "Release 0.4.3",
        "tag_name": "v0.4.3",
        "tarball_url": "https://api.github.com/repos/flairNLP/flair/tarball/v0.4.3",
        "url": "https://api.github.com/repos/flairNLP/flair/releases/19470502",
        "zipball_url": "https://api.github.com/repos/flairNLP/flair/zipball/v0.4.3"
      },
      {
        "authorType": "User",
        "author_name": "alanakbik",
        "body": "Release 0.4.2 includes new features such as streaming data loading (allowing training over very large datasets), support for OpenAI GPT Embeddings, pre-trained Flair embeddings for many new languages, better classification baselines using one-hot embeddings and fine-tuneable document pool embeddings, and text regression as a third task next to sequence labeling and text classification. \r\n\r\n# New way of loading data (#768) \r\n\r\nThe data loading part has been completely refactored to enable streaming data loading from disk using PyTorch's DataLoaders. I.e. training no longer requires the full dataset to be kept in memory, allowing us to train models over much larger datasets. This version also changes the syntax of how to load datasets. \r\n\r\nOld way (now deprecated): \r\n```python\r\nfrom flair.data_fetcher import NLPTaskDataFetcher, NLPTask\r\ncorpus = NLPTaskDataFetcher.load_corpus(NLPTask.UD_ENGLISH)\r\n```\r\n\r\nNew way: \r\n```python\r\nimport flair.datasets\r\ncorpus = flair.datasets.UD_ENGLISH()\r\n```\r\n\r\nTo use streaming loading, i.e. to not load into memory, you can pass the `in_memory` parameter:\r\n```python\r\nimport flair.datasets\r\ncorpus = flair.datasets.UD_ENGLISH(in_memory=False)\r\n```\r\n\r\n# Embeddings\r\n\r\n## Flair embeddings (#614) \r\n\r\nThis release brings Flair embeddings to 11 new languages (thanks @stefan-it!): Arabic (ar), Danish (da), Persian (fa), Finnish (fi), Hebrew (he), Hindi (hi),  Croatian (hr), Indonesian (id), Italian (it), Norwegian (no) and Swedish (sv). It also improves support for Bulgarian (bg), Czech, Basque (eu), Dutch (nl) and Slovenian (sl), and adds special language models for historical German. Load with language code, i.e. \r\n\r\n```python\r\n# load Flair embeddings for Italian \r\nembeddings = FlairEmbeddings('it-forward')\r\n```\r\n\r\n## One-hot encoded embeddings (#747)\r\n\r\nSome classification baselines work astonishingly well with simple learnable word embeddings. To support testing these baselines, we've added learnable word embeddings that start from a one-hot encoding of words. To initialize, you need to pass a corpus to initialize the vocabulary. \r\n\r\n```python\r\n# load corpus \r\nimport flair.datasets\r\ncorpus = flair.datasets.UD_ENGLISH()\r\n\r\n# init learnable word embeddings with corpus\r\nembeddings = OneHotEmbeddings(corpus)\r\n```\r\n\r\n## More options in `DocumentPoolEmbeddings`  (#747)\r\n\r\nWe now allow users to specify a fine-tuning option before the pooling operation is executed in document pool embeddings. Options are 'none' (no fine-tuning), 'linear' (linear remapping of word embeddings), 'nonlinear' (nonlinear remapping of word embeddings). Nonlinear should be used together with `WordEmbeddings`, while None should be used with `OneHotEmbeddings` (not necessary since they are already learnt on data). So, to replicate FastText classification you can either do: \r\n\r\n```python\r\n# instantiate one-hot encoded word embeddings\r\nembeddings = OneHotEmbeddings(corpus)\r\n\r\n# document pool embeddings\r\ndocument_embeddings = DocumentPoolEmbeddings([embeddings], fine_tune_mode='none')\r\n```\r\n\r\nor \r\n\r\n```python\r\n# instantiate pre-trained word embeddings\r\nembeddings = WordEmbeddings('glove')\r\n\r\n# document pool embeddings\r\ndocument_embeddings = DocumentPoolEmbeddings([embeddings], fine_tune_mode='nonlinear')\r\n```\r\n\r\n## OpenAI GPT Embeddings (#624)\r\n\r\nWe now support embeddings from the OpenAI GPT model. We use the excellent pytorch-pretrained-BERT library to download the GPT model, tokenize the input and extract embeddings from the subtokens.\r\n\r\nInitialize with: \r\n\r\n```python\r\nembeddings = OpenAIGPTEmbeddings()\r\n```\r\n## Portuguese embeddings from NILC (#576)\r\n\r\n# Extensibility to new downstream tasks (#681)\r\n\r\nPreviously, we had the `SequenceTagger` and `TextClassifier` as the two downstream tasks supported by Flair. The `ModelTrainer` had specific methods to train these two models, making it difficult for users to add new types of tasks (such as text regression) to Flair.   \r\n\r\nThis release refactors the `flair.nn.Model` and `ModelTrainer` functionality to make it uniform across tagging models and enable users to add new tasks to Flair. Now, by implementing the 5 methods in the `flair.nn.Model` interface, a custom model immediately becomes trainable with the `ModelTrainer`. Now, three types of downstream tasks implement this interface: \r\n\r\n- the `SequenceTagger`, \r\n- the `TextClassifier` \r\n- and the beta `TextRegressor`. \r\n\r\nThe code refactor removes a lot of code redundancies and slims down the interfaces of the downstream tasks classes. As the sole breaking change, it removes the `load_from_file()` methods, which are now part of the `load()` method, i.e. if previously you loaded a self-trained model like this: \r\n\r\n```python\r\ntagger = SequenceTagger.load_from_file('/path/to/model.pt')\r\n```\r\n\r\nYou now do it like this: \r\n\r\n```python\r\ntagger = SequenceTagger.load('/path/to/model.pt')\r\n```\r\n\r\n# New features\r\n\r\n- New beta support for text regression (#564)\r\n- Return confidence scores for single-label classification (#664)\r\n- Add method to find probability for each class in case of multi-class classification (#693)\r\n- Capability to change threshold during multi label classification #707\r\n- Support for customized ELMo embeddings (#661)\r\n- Detect multi-label problems automatically: Previously, users always had to specify whether their text classification problem was multi_label or not. Now, this is detected automatically if users do not specify. So now you can init like this: \r\n\r\n```python\r\n# corpus\r\ncorpus = TREC_6()\r\n\r\n# make label_dictionary\r\nlabel_dictionary = corpus.make_label_dictionary()\r\n\r\n# init text classifier\r\nclassifier = TextClassifier(document_embeddings, label_dictionary)\r\n```\r\n\r\n- We added better module descriptions to embeddings and dropout so that more parameters get printed by default for models for better logging.   (#747)\r\n- Make 'cache_root' a global variable so that different directories can be chosen for caching (#667)\r\n- Both string and Token objects can now be passed to the add_token method (#628)\r\n\r\n# New datasets\r\n- Added IMDB classification corpus to `flair.datasets` (#749)\r\n- Added TREC_6 classification corpus to `flair.datasets` (#749)\r\n- Added 20 newsgroups classification corpus to `flair.datasets` (NEWSGROUPS object)\r\n- WASSA-17 emotion intensity text regression tasks (#695)\r\n \r\n# Bug fixes \r\n\r\n- We normalized the training loss across modules so that train / test loss are consistent. (#670)\r\n- Permission error on Windows preventing model download (#557)\r\n- Handling of empty sentences (#566 #758)\r\n- Fix text generation on CUDA (#666)\r\n- others ... \r\n",
        "dateCreated": "2019-05-30T19:26:38Z",
        "datePublished": "2019-05-30T20:31:09Z",
        "html_url": "https://github.com/flairNLP/flair/releases/tag/v0.4.2",
        "name": "Release 0.4.2",
        "tag_name": "v0.4.2",
        "tarball_url": "https://api.github.com/repos/flairNLP/flair/tarball/v0.4.2",
        "url": "https://api.github.com/repos/flairNLP/flair/releases/17689582",
        "zipball_url": "https://api.github.com/repos/flairNLP/flair/zipball/v0.4.2"
      },
      {
        "authorType": "User",
        "author_name": "alanakbik",
        "body": "Release 0.4.1 with lots of new features, new embeddings (RNN, Transformer and BytePair embeddings), new languages (Japanese, Spanish, Basque), new datasets, bug fixes and speed improvements (2x training speed for language models). \r\n\r\n# New Embeddings\r\n\r\n## Biomedical Embeddings\r\n\r\nAdded first embeddings trained over PubMed data, namely \r\n* [ELMo embeddings](https://github.com/zalandoresearch/flair/pull/503)\r\n* [Flair embeddings](https://github.com/zalandoresearch/flair/issues/518)\r\n\r\nLoad these for instance with: \r\n\r\n```python\r\n# Flair embeddings PubMed\r\nflair_embedding_forward = FlairEmbeddings('pubmed-forward')\r\nflair_embedding_backward = FlairEmbeddings('pubmed-backward')\r\n\r\n# ELMo embeddings PubMed\r\nelmo_embeddings = ELMoEmbeddings('pubmed')\r\n```\r\n\r\n## Byte Pair Embeddings\r\n\r\nAdded the [byte pair embeddings](https://github.com/zalandoresearch/flair/pull/473) library by @bheinzerling. Support for 275 languages. Very useful if you want to train small models. Load these for instance with:\r\n\r\n```python\r\n# initialize embeddings\r\nembeddings =  BytePairEmbeddings(language='en')\r\n```\r\n\r\n## Transformer-XL Embeddings\r\n\r\n[Transformer-XL embeddings](https://github.com/zalandoresearch/flair/pull/523) added by @stefan-it. Load with: \r\n\r\n```python\r\n# initialize embeddings\r\nembeddings = TransformerXLEmbeddings()\r\n```\r\n\r\n## ELMo Transformer Embeddings\r\n\r\nExperimental [transformer version of ELMo embeddings](https://github.com/zalandoresearch/flair/pull/399)\r\nadded by @stefan-it. \r\n\r\n## DocumentRNNEmbeddings\r\n\r\nThe new [DocumentRNNEmbeddings](https://github.com/zalandoresearch/flair/pull/512) class replaces the now-deprecated DocumentLSTMEmbeddings. This class allows you to choose which type of RNN you want to use. By default, it uses a GRU. \r\n\r\nInitialize like this: \r\n\r\n```python\r\nfrom flair.embeddings import WordEmbeddings, DocumentRNNEmbeddings\r\n\r\nglove_embedding = WordEmbeddings('glove')\r\n\r\ndocument_lstm_embeddings = DocumentRNNEmbeddings([glove_embedding], rnn_type='LSTM')\r\n```\r\n\r\n# New languages \r\n\r\n## Japanese\r\n\r\n`FlairEmbeddings` for Japanese trained by @frtacoa and @minh-agent:\r\n\r\n```python\r\n# forward and backward embedding\r\nembeddings_fw = FlairEmbeddings('japanese-forward')\r\nembeddings_bw = FlairEmbeddings('japanese-backward')\r\n```\r\n\r\n## Spanish \r\n\r\nAdded pre-computed `FlairEmbeddings` for Spanish. Embeddings were computed over Wikipedia by @iamyihwa (see #80 )\r\n\r\nTo load Spanish `FlairEmbeddings`, simply do:\r\n\r\n```python\r\n# default forward and backward embedding\r\nembeddings_fw = FlairEmbeddings('spanish-forward')\r\nembeddings_bw = FlairEmbeddings('spanish-backward')\r\n\r\n# CPU-friendly forward and backward embedding\r\nembeddings_fw_fast = FlairEmbeddings('spanish-forward-fast')\r\nembeddings_bw_fast = FlairEmbeddings('spanish-backward-fast')\r\n```\r\n\r\n## Basque  \r\n\r\n- @stefan-it trained `FlairEmbeddings` for Basque which we now include, load with: \r\n\r\n```python\r\nforward_lm_embeddings = FlairEmbeddings('basque-forward')\r\nbackward_lm_embeddings = FlairEmbeddings('basque-backward')\r\n```\r\n- add Basque FastText embeddings, load with: \r\n```python\r\nwikipedia_embeddings = WordEmbeddings('eu-wiki')\r\ncrawl_embeddings = WordEmbeddings('eu-crawl')\r\n```\r\n\r\n# New Datasets\r\n\r\n- IMDB dataset #410 - load with \r\n```python\r\ncorpus = NLPTaskDataFetcher.load_corpus(NLPTask.IMDB)\r\n```\r\n- TREC_6 and TREC_50 #450 - load with \r\n```python\r\ncorpus = NLPTaskDataFetcher.load_corpus(NLPTask.TREC_6)\r\n```\r\n- adds download routines for Basque Universal Dependencies and Named Entities, load with \r\n```python\r\ncorpus = NLPTaskDataFetcher.load_corpus(NLPTask.UD_BASQUE)\r\ncorpus_ner = NLPTaskDataFetcher.load_corpus(NLPTask.NER_BASQUE)\r\n```\r\n\r\n# Other features \r\n\r\n## FlairEmbeddings for long text\r\n\r\n`FlairEmbeddings` can now be generated for arbitrarily long strings without causing out of memory errors. See #444  \r\n\r\n## Function for calculating perplexity of a string #531 \r\n\r\nUse like this: \r\n\r\n```python\r\nfrom flair.embeddings import FlairEmbeddings\r\n\r\n# get language model\r\nlanguage_model = FlairEmbeddings('news-forward-fast').lm\r\n\r\n# calculate perplexity for grammatical sentence\r\ngrammatical = 'The company made a profit'\r\nperplexity_gramamtical_sentence = language_model.calculate_perplexity(grammatical)\r\n\r\n# calculate perplexity for ungrammatical sentence\r\nungrammatical = 'Nook negh qapla!'\r\nperplexity_ungramamtical_sentence = language_model.calculate_perplexity(ungrammatical)\r\n\r\n# print both\r\nprint(f'\"{grammatical}\" - perplexity is {perplexity_gramamtical_sentence}')\r\nprint(f'\"{ungrammatical}\" - perplexity is {perplexity_ungramamtical_sentence}')\r\n```\r\n\r\n\r\n# Bug fixes\r\n\r\n- Overflow error in text generation #322 \r\n- Sentence embeddings are now vectors #368\r\n- macro average F-score computation #521\r\n- character embeddings on CUDA #434\r\n- accuracy calculation #553 \r\n\r\n# Speed improvements\r\n- Asynchronous loading of mini batches in language model training (roughly doubles training speed) #406\r\n- Only send mini-batches to GPU #350 \r\n- Speed up sequence tagger prediction #353 \r\n- Use new cuda semantics #402\r\n- Reduce CPU-GPU shuffling #459\r\n- LM memory tweaks #466",
        "dateCreated": "2019-02-22T12:43:30Z",
        "datePublished": "2019-02-22T13:36:48Z",
        "html_url": "https://github.com/flairNLP/flair/releases/tag/v0.4.1",
        "name": "Release 0.4.1",
        "tag_name": "v0.4.1",
        "tarball_url": "https://api.github.com/repos/flairNLP/flair/tarball/v0.4.1",
        "url": "https://api.github.com/repos/flairNLP/flair/releases/15717610",
        "zipball_url": "https://api.github.com/repos/flairNLP/flair/zipball/v0.4.1"
      },
      {
        "authorType": "User",
        "author_name": "alanakbik",
        "body": "Release 0.4 with new models, lots of new languages, experimental multilingual models, hyperparameter selection methods, BERT and ELMo embeddings, etc.\r\n\r\n# New Features\r\n\r\n## Support for new languages\r\n\r\n### Flair embeddings\r\nWe now include new language models for:\r\n* [Swedish](https://github.com/zalandoresearch/flair/issues/3)\r\n* [Polish](https://github.com/zalandoresearch/flair/issues/187)\r\n* [Bulgarian](https://github.com/zalandoresearch/flair/issues/188)\r\n* [Slovenian](https://github.com/zalandoresearch/flair/issues/202)\r\n* [Dutch](https://github.com/zalandoresearch/flair/issues/224)\r\n\r\nIn addition to English and German. You can load FlairEmbeddings for Dutch for instance with: \r\n\r\n```python\r\nflair_embeddings = FlairEmbeddings('dutch-forward')\r\n```\r\n\r\n### Word Embeddings\r\n\r\nWe now include pre-trained [FastText Embeddings for 30 languages](https://github.com/zalandoresearch/flair/issues/234):  English, German, Dutch, Italian, French, Spanish, Swedish, Danish, Norwegian, Czech, Polish, Finnish, Bulgarian, Portuguese, Slovenian, Slovakian, Romanian, Serbian, Croatian, Catalan, Russian, Hindi, Arabic, Chinese, Japanese, Korean, Hebrew, Turkish, Persian, Indonesian. \r\n\r\nEach language has embeddings trained over Wikipedia, or Web crawls. So instantiate with: \r\n\r\n```python\r\n# German embeddings computed over Wikipedia\r\ngerman_wikipedia_embeddings = WordEmbeddings('de-wiki')\r\n\r\n# German embeddings computed over web crawls\r\ngerman_crawl_embeddings = WordEmbeddings('de-crawl')\r\n```\r\n\r\n### Named Entity Recognition\r\n\r\nThanks to the Flair community, we now include NER models for: \r\n* [French](https://github.com/zalandoresearch/flair/issues/238)\r\n* [Dutch](https://github.com/zalandoresearch/flair/issues/224)\r\n\r\nNext to the previous models for English and German.\r\n\r\n### Part-of-Speech Taggigng\r\n\r\nThanks to the Flair community, we now include PoS models for: \r\n* [German tweets](https://github.com/zalandoresearch/flair/issues/51)\r\n\r\n\r\n## Multilingual models\r\n\r\nAs a major new feature, we now include models that can tag text in various languages.  \r\n\r\n### 12-language Part-of-Speech Tagging\r\n\r\nWe include a PoS model trained over 12 different languages (English, German, Dutch, Italian, French, Spanish, Portuguese, Swedish, Norwegian, Danish, Finnish, Polish, Czech).\r\n\r\n```python\r\n# load model\r\ntagger = SequenceTagger.load('pos-multi')\r\n\r\n# text with English and German sentences\r\nsentence = Sentence('George Washington went to Washington . Dort kaufte er einen Hut .')\r\n\r\n# predict PoS tags\r\ntagger.predict(sentence)\r\n\r\n# print sentence with predicted tags\r\nprint(sentence.to_tagged_string())\r\n```\r\n\r\n### 4-language Named Entity Recognition\r\n\r\nWe include a NER model trained over 4 different languages (English, German, Dutch, Spanish).\r\n\r\n```python\r\n# load model\r\ntagger = SequenceTagger.load('ner-multi')\r\n\r\n# text with English and German sentences\r\nsentence = Sentence('George Washington went to Washington . Dort traf er Thomas Jefferson .')\r\n\r\n# predict NER tags\r\ntagger.predict(sentence)\r\n\r\n# print sentence with predicted tags\r\nprint(sentence.to_tagged_string())\r\n```\r\n\r\nThis model also kind of works on other languages, such as French.\r\n\r\n## Pre-trained classification models ([issue 70](https://github.com/zalandoresearch/flair/issues/70))\r\n\r\nFlair now also includes two pre-trained classification models:\r\n* de-offensive-lanuage: detecting offensive language in German text ([GermEval 2018 Task 1](https://projects.fzai.h-da.de/iggsa/projekt/))\r\n* en-sentiment: detecting postive and negative sentiment in English text ([IMDB](http://ai.stanford.edu/~amaas/data/sentiment/))\r\n\r\nSimply load the `TextClassifier` using the preferred model, such as \r\n```python\r\nTextClassifier.load('en-sentiment')\r\n```\r\n\r\n## BERT and ELMo embeddings\r\n\r\nWe added both BERT and ELMo embeddings so you can try them out, and mix and match them with Flair embeddings or any other embedding types. We hope this will enable the research community to better compare and combine approaches.\r\n\r\n### BERT Embeddings ([issue 251](https://github.com/zalandoresearch/flair/issues/251))\r\n\r\nWe added [BERT embeddings](https://arxiv.org/pdf/1810.04805.pdf) to Flair. We are using the implementation of [huggingface](https://github.com/huggingface/pytorch-pretrained-BERT). The embeddings can be used as any other embedding type in Flair:\r\n\r\n```python\r\nfrom flair.embeddings import BertEmbeddings\r\n # init embedding\r\nembedding = BertEmbeddings()\r\n # create a sentence\r\nsentence = Sentence('The grass is green .')\r\n # embed words in sentence\r\nembedding.embed(sentence)\r\n```\r\n\r\n### ELMo Embeddings ([issue 260](https://github.com/zalandoresearch/flair/issues/260))\r\n\r\nFlair now also includes [ELMo embeddings](http://www.aclweb.org/anthology/N18-1202). We use the implementation of [AllenNLP](https://allennlp.org/elmo). As this implementation comes with a lot of sub-dependencies, you need to first install the library via `pip install allennlp` before you can use it in Flair. Using the embeddings is as simple as using any other embedding type:\r\n ```python\r\nfrom flair.embeddings import ELMoEmbeddings\r\n # init embedding\r\nembedding = ELMoEmbeddings()\r\n # create a sentence\r\nsentence = Sentence('The grass is green .')\r\n # embed words in sentence\r\nembedding.embed(sentence)\r\n```\r\n\r\n\r\n## Multi-Dataset Training ([issue 232](https://github.com/zalandoresearch/flair/issues/232))\r\n\r\nYou can now train a model on on multiple datasets with the `MultiCorpus` object. We use this to train our multilingual models. \r\n\r\nJust create multiple corpora and put them into `MultiCorpus`:\r\n\r\n```python\r\nenglish_corpus = NLPTaskDataFetcher.load_corpus(NLPTask.UD_ENGLISH)\r\ngerman_corpus = NLPTaskDataFetcher.load_corpus(NLPTask.UD_GERMAN)\r\ndutch_corpus = NLPTaskDataFetcher.load_corpus(NLPTask.UD_DUTCH)\r\n\r\nmulti_corpus = MultiCorpus([english_corpus, german_corpus, dutch_corpus])\r\n```\r\nThe `multi_corpus` can now be used for training, just as any other corpus before. Check [the tutorial](TUTORIAL_6_TRAINING_A_MODEL.md) for more details.\r\n\r\n## Parameter Selection using Hyperopt ([issue 242](https://github.com/zalandoresearch/flair/issues/242))\r\n\r\nWe built a wrapper around [hyperopt](http://hyperopt.github.io/hyperopt/) to allow you to search for the best hyperparameters for your downstream task. \r\n\r\nDefine your search space and start training using several different parameter settings. The results are written to a specific file called `param_selection.txt` in the result directory. Check [the tutorial](TUTORIAL_7_HYPER_PARAMETER.md) for more details.\r\n\r\n## NLP Dataset Downloader ([issue 243](https://github.com/zalandoresearch/flair/issues/243))\r\n\r\nTo make it as easy as possible to start training models, we have a new feature for automatically downloading publicly available NLP datasets. For instance, by running this code: \r\n\r\n```python\r\ncorpus = NLPTaskDataFetcher.load_corpus(NLPTask.UD_ENGLISH)\r\n```\r\n\r\nyou download the Universal Dependencies corpus for English and can immediately start training models. The list of available datasets can be found in [the tutorial](TUTORIAL_5_CORPUS.md).\r\n\r\n\r\n## Model training features\r\n\r\nWe added various other features to model training.\r\n\r\n### Saving training log ([issue 212](https://github.com/zalandoresearch/flair/issues/212))\r\n\r\nThe training log output will from now on be automatically saved in the result directory you provide for training.\r\nThe log will be saved in `training.log`.\r\n\r\n### Resuming training ([issue 217](https://github.com/zalandoresearch/flair/issues/217))\r\n\r\nIt is now possible to stop training at any point in time and to resume it later by training with `checkpoint` set to `True`. Check [the tutorial](TUTORIAL_6_TRAINING_A_MODEL.md) for more details.\r\n\r\n### Custom Optimizers ([issue 220](https://github.com/zalandoresearch/flair/issues/220))\r\n\r\nYou can now choose other optimizers besides SGD, i.e. any PyTorch optimizer, plus our own modified implementations of SDG and Adam, namely SGDW and AdamW.\r\n\r\n### Learning Rate Finder ([issue 228](https://github.com/zalandoresearch/flair/issues/228))\r\n\r\nA new helper method to assist you in finding a [good learning rate for model training](https://github.com/zalandoresearch/flair/blob/master/resources/docs/TUTORIAL_8_MODEL_OPTIMIZATION.md#finding-the-best-learning-rate). \r\n\r\n\r\n# Breaking Changes\r\n\r\nThis release introduces breaking changes. The most important are: \r\n\r\n## Unified Model Trainer ([issue 189](https://github.com/zalandoresearch/flair/issues/189))\r\n\r\nInstead of maintaining two separate trainer classes for sequence labeling and text classification, we now have one model training class, namely `ModelTrainer`. This replaces the earlier classes `SequenceTaggerTrainer` and `TextClassifierTrainer`. \r\n\r\nDownstream task models now implement the new `flair.nn.Model` interface. So, both the `SequenceTagger` and `TextClassifier` now inherit from `flair.nn.Model`. This allows both models to be trained with the  `ModelTrainer`, like this: \r\n\r\n```python\r\n# Training text classifier\r\ntagger = SequenceTagger(512, embeddings, tag_dictionary, 'ner')\r\ntrainer = ModelTrainer(tagger, corpus)\r\ntrainer.train('results')\r\n\r\n# Training text classifier\r\nclassifier = TextClassifier(document_embedding, label_dictionary=label_dict)\r\ntrainer = ModelTrainer(classifier, corpus)\r\ntrainer.train('results')\r\n```\r\n\r\nThe advantage is that all training parameters ans training procedures are now the same for sequence labeling and text classification, which reduces redundancy and hopefully make it easier to understand. \r\n\r\n## Metric class\r\n\r\nThe metric class is now refactored to compute micro and macro averages for F1 and accuracy. There is also a new enum `EvaluationMetric` which you can pass to the ModelTrainer to tell it what to use for evaluation.\r\n\r\n# Updates and Bug Fixes\r\n\r\n### Torch 1.0  ([issue 176](https://github.com/zalandoresearch/flair/issues/299))\r\n\r\nFlair now bulids on torch 1.0.\r\n\r\n### Use Pathlib ([issue 176](https://github.com/zalandoresearch/flair/issues/176))\r\n\r\nFlair now uses `Path` wherever possible to allow easier operations on files/directories. However, our interfaces still allows you to pass a string, which will then be transformed into a Path by Flair.\r\n\r\n## Bug Fixes\r\n\r\n* Fix: Non-whitespaced tokenized text results into an infinite loop ([issue 226](https://github.com/zalandoresearch/flair/issues/226))\r\n* Fix: Getting IndexError: list index out of range error ([issue 233](https://github.com/zalandoresearch/flair/issues/233))\r\n* Do not reset cache directory always to None ([issue 249](https://github.com/zalandoresearch/flair/issues/249))\r\n* Filter sentences with zero tokens ([issue 266](https://github.com/zalandoresearch/flair/issues/266))\r\n",
        "dateCreated": "2018-12-19T17:36:43Z",
        "datePublished": "2018-12-19T19:42:44Z",
        "html_url": "https://github.com/flairNLP/flair/releases/tag/v0.4.0",
        "name": "Release 0.4",
        "tag_name": "v0.4.0",
        "tarball_url": "https://api.github.com/repos/flairNLP/flair/tarball/v0.4.0",
        "url": "https://api.github.com/repos/flairNLP/flair/releases/14623990",
        "zipball_url": "https://api.github.com/repos/flairNLP/flair/zipball/v0.4.0"
      },
      {
        "authorType": "User",
        "author_name": "alanakbik",
        "body": "This is an update over release 0.3.1 with some critical bug fixes, a few new features and a lot more pre-packaged embeddings. \r\n\r\n# New Features \r\n\r\n## Embeddings\r\n\r\n### More word embeddings (#194 )\r\n\r\nWe added FastText embeddings for 10 languages ('en', 'de', 'fr', 'pl', 'it', 'es', 'pt', 'nl', 'ar', 'sv'), load using the two-letter language code, like this: \r\n\r\n```python\r\nfrench_embedding = WordEmbeddings('fr')\r\n```\r\n\r\n###  More character LM embeddings (#204 #187 )\r\n\r\nThanks to contribution by [@stefan-it](https://github.com/stefan-it/flair-lms), we added CharLMEmbeddings for Bulgarian and Slovenian. Load like this: \r\n\r\n```python\r\nflm_embeddings = CharLMEmbeddings('slovenian-forward')\r\nblm_embeddings = CharLMEmbeddings('slovenian-backward')\r\n```\r\n\r\n### Custom embeddings (#170 )\r\n\r\nAdd explanation on how to use your own custom word embeddings. Simply convert to gensim.KeyedVectors and point embedding class there: \r\n\r\n```python\r\ncustom_embedding = WordEmbeddings('path/to/your/custom/embeddings.gensim')\r\n```\r\n\r\n### New embeddings type: `DocumentPoolEmbeddings` (#191 )\r\n\r\nAdd a new embedding class for document-level embeddings. You can now choose between different pooling options, e.g. min, max and average. Create the new embeddings like this: \r\n\r\n```python\r\nword_embeddings = WordEmbeddings('glove')\r\npool_embeddings = DocumentPoolEmbeddings([word_embeddings], mode='min')\r\n```\r\n\r\n## Language model\r\n\r\n### New method: `generate_text()` (#167 )\r\n\r\nThe `LanguageModel` class now has an in-built `generate_text()` method to sample the LM. Run code like this: \r\n\r\n```python\r\n# load your language model\r\nmodel = LanguageModel.load_language_model('path/to/your/lm')\r\n\r\n# generate 2000 characters\r\ntext = model.generate_text(20000)\r\nprint(text)\r\n```\r\n\r\n## Metrics \r\n\r\n### Class-based metrics in `Metric` class (#164 )\r\n\r\nRefactored Metric class to provide class-based metrics, as well as micro and macro averaged F1 scores. \r\n\r\n# Bug Fixes\r\n\r\n### Fix serialization error for MacOS and Windows (#174 )\r\n\r\nOn these setups, we got errors when serializing or loading large models. We've put in place a workaround that limits model size so it works on those systems. Added bonus is that models are smaller now. \r\n\r\n### \"Frozen\" dropout (#184 )\r\n\r\nPotentially big issue in which dropout was frozen in the first epoch in embeddings produced from the character LM, meaning that throughout training the same dimensions stayed dropped. Fixed this. \r\n\r\n### Testing step in language model trainer (#178 )\r\n\r\nPreviously, the language model was never applied to test data during training. A final testing step has been added in (again). \r\n\r\n# Testing \r\n\r\n### Distinguish between unit and integration tests (#183)\r\n\r\n### Instructions on how to run tests with pipenv (#161 )\r\n\r\n# Optimizations\r\n\r\n### Disable autograd during testing and prediction (#175)\r\n\r\nSince autograd is unused here this gives us minor speedups.\r\n\r\n",
        "dateCreated": "2018-11-12T16:58:28Z",
        "datePublished": "2018-11-12T17:01:50Z",
        "html_url": "https://github.com/flairNLP/flair/releases/tag/v0.3.2",
        "name": "Release 0.3.2",
        "tag_name": "v0.3.2",
        "tarball_url": "https://api.github.com/repos/flairNLP/flair/tarball/v0.3.2",
        "url": "https://api.github.com/repos/flairNLP/flair/releases/13953074",
        "zipball_url": "https://api.github.com/repos/flairNLP/flair/zipball/v0.3.2"
      },
      {
        "authorType": "User",
        "author_name": "alanakbik",
        "body": "This is a stability-update over release 0.3.0 with small optimizations, refactorings and bug fixes. For list of new features, refer to 0.3.0.\r\n\r\n# Optimizations\r\n\r\n### Retain Token embeddings in memory by default (#146 )\r\n\r\nAllow for faster training of text classifier on large datasets by keeping token embeddings im memory.\r\n\r\n### Always clear embeddings after prediction (#149 )\r\n\r\nAfter prediction, remove embeddings from memory to avoid filling up memory.\r\n\r\n\r\n# Refactorings\r\n\r\n### Alignd TextClassificationTrainer and SquenceTaggerTrainer (#148 )\r\n\r\nAlign signatures and features of the two training classes to make it easier to understand training options.\r\n\r\n### Updated DocumentLSTMEmbeddings (#150 )\r\n\r\nRemove unused flag and code from DocumentLSTMEmbeddings \r\n\r\n### Removed unneeded AWS and Jinja2 dependencies (#158 )\r\n\r\nSome dependencies are no longer required. \r\n\r\n\r\n# Bug Fixes \r\n\r\n### Fixed error when predicting over empty sentences. (#157)\r\n\r\n### Serialization: reset cache settings when saving a model. (#153 )\r\n\r\n\r\n",
        "dateCreated": "2018-10-19T15:22:40Z",
        "datePublished": "2018-10-19T15:44:44Z",
        "html_url": "https://github.com/flairNLP/flair/releases/tag/v0.3.1",
        "name": "Release 0.3.1",
        "tag_name": "v0.3.1",
        "tarball_url": "https://api.github.com/repos/flairNLP/flair/tarball/v0.3.1",
        "url": "https://api.github.com/repos/flairNLP/flair/releases/13542725",
        "zipball_url": "https://api.github.com/repos/flairNLP/flair/zipball/v0.3.1"
      },
      {
        "authorType": "User",
        "author_name": "alanakbik",
        "body": "# Breaking Changes\r\n\r\n### New `Label` class with confidence score (https://github.com/zalandoresearch/flair/issues/38)\r\n\r\nA tag prediction is not a simple string anymore but a `Label`, which holds a value and a confidence score.\r\nTo obtain the tag name you need to call `tag.value`. To get the score call `tag.score`. This can help you build \r\napplications in which you only want to use predictions that lie above a specific confidence threshold.\r\n \r\n### `LockedDropout` moved to the new `flair.nn` module (https://github.com/zalandoresearch/flair/issues/48)\r\n\r\n\r\n# New Features\r\n\r\n### Multi-token spans (https://github.com/zalandoresearch/flair/issues/54, https://github.com/zalandoresearch/flair/issues/97)\r\nEntities are can now be wrapped into multi-token spans (type: `Span`). This is helpful for entities that span multiple words, such as \"George Washington\". A `Span` contains the position of the entity in the original text, the tag, a confidence score, and its text. You can get spans from a sentence by using the `get_spans()` method, like so: \r\n```python\r\nfrom flair.data import Sentence\r\nfrom flair.models import SequenceTagger\r\n\r\n# make a sentence\r\nsentence = Sentence('George Washington went to Washington .')\r\n\r\n# load and run NER\r\ntagger = SequenceTagger.load('ner')\r\ntagger.predict(sentence)\r\n\r\n# get span entities, together with tag and confidence score\r\nfor entity in sentence.get_spans('ner'):\r\n    print('{} {} {}'.format(entity.text, entity.tag, entity.score))\r\n```\r\n\r\n### Predictions with confidence score (https://github.com/zalandoresearch/flair/issues/38)\r\nPredicted tags are no longer simple strings, but objects of type `Label` that contain a value and a confidence score. These scores are extracted during prediction from the sequence tagger or text classifier and indicate how confident the model is of a prediction. Print confidence scores of tags like this:\r\n\r\n```python\r\nfrom flair.data import Sentence\r\nfrom flair.models import SequenceTagger\r\n\r\n# make a sentence\r\nsentence = Sentence('George Washington went to Washington .')\r\n\r\n# load the POS tagger\r\ntagger = SequenceTagger.load('pos')\r\n\r\n# run POS over sentence\r\ntagger.predict(sentence)\r\n\r\n# print token, predicted POS tag and confidence score\r\nfor token in sentence:\r\n    print('{} {} {}'.format(token.text, token.get_tag('pos').value, token.get_tag('pos').score))\r\n```\r\n\r\n### Visualization routines (https://github.com/zalandoresearch/flair/issues/61)\r\n`flair` now includes visualizations for plotting training curves and weights when training a sequence tagger or text classifier. We also added visualization routines for plotting embeddings and highlighting tags in a sentence. For instance, to visualize contextual string embeddings, do this: \r\n\r\n```python\r\nfrom flair.data_fetcher import NLPTaskDataFetcher, NLPTask\r\nfrom flair.embeddings import CharLMEmbeddings\r\nfrom flair.visual import Visualizer\r\n\r\n# get a list of Sentence objects\r\ncorpus = NLPTaskDataFetcher.fetch_data(NLPTask.CONLL_03).downsample(0.1)\r\nsentences = corpus.train + corpus.test + corpus.dev\r\n\r\n# init embeddings (can also be a StackedEmbedding)\r\nembeddings = CharLMEmbeddings('news-forward-fast')\r\n\r\n# embed corpus batch-wise\r\nbatches = [sentences[x:x + 8] for x in range(0, len(sentences), 8)]\r\nfor batch in batches:\r\n    embeddings.embed(batch)\r\n\r\n# visualize\r\nvisualizer = Visualizer()\r\nvisualizer.visualize_word_emeddings(embeddings, sentences, 'data/visual/embeddings.html')\r\n```\r\n\r\n### Implementation of different dropouts (https://github.com/zalandoresearch/flair/issues/48)\r\nDifferent dropout possibilities (Locked Dropout and Word Dropout) were added and can be used during training.\r\n\r\n### Memory management for training on large data sets (https://github.com/zalandoresearch/flair/issues/137)\r\n`flair` now stores contextual string embeddings on disk to speed up training and allow for training on larger datsets.\r\n\r\n### Pre-trained language models for Polish\r\nAdded pre-trained language models for Polish, donated by [(Borchmann et al., 2018)](https://github.com/applicaai/poleval-2018). Load the Polish embeddings like this:\r\n\r\n```python\r\nflm_embeddings = CharLMEmbeddings('polish-forward')\r\nblm_embeddings = CharLMEmbeddings('polish-backward')\r\n```\r\n\r\n# Bug Fixes\r\n\r\n### Fix evaluation of sequence tagger (https://github.com/zalandoresearch/flair/issues/79, https://github.com/zalandoresearch/flair/issues/75)\r\nThe script `eval.pl` for sequence tagger contained bugs. `flair` now uses its own evaluation methods.\r\n\r\n### Fix bugs in text classifier (https://github.com/zalandoresearch/flair/issues/108)\r\nFixed bugs in single label training and out-of-memory errors during evaluation.\r\n\r\n# Others\r\n\r\n### Standardize logging output (https://github.com/zalandoresearch/flair/issues/16)\r\nLogging output for sequence tagger and text classifier is imporved and standardized.\r\n\r\n### Update torch version (https://github.com/zalandoresearch/flair/issues/34, https://github.com/zalandoresearch/flair/issues/106)\r\nflair now uses torch version 0.4.1\r\n\r\n### Updated documentation (https://github.com/zalandoresearch/flair/issues/138, https://github.com/zalandoresearch/flair/issues/89)\r\nExpanded documentation and tutorials.\r\n",
        "dateCreated": "2018-10-16T13:36:54Z",
        "datePublished": "2018-10-16T14:41:28Z",
        "html_url": "https://github.com/flairNLP/flair/releases/tag/v0.3.0",
        "name": "Release 0.3.0",
        "tag_name": "v0.3.0",
        "tarball_url": "https://api.github.com/repos/flairNLP/flair/tarball/v0.3.0",
        "url": "https://api.github.com/repos/flairNLP/flair/releases/13452985",
        "zipball_url": "https://api.github.com/repos/flairNLP/flair/zipball/v0.3.0"
      },
      {
        "authorType": "User",
        "author_name": "alanakbik",
        "body": "# Breaking Changes\r\n\r\n## Reorganized package structure #12 \r\n\r\nThere are now two packages: `flair.models` and `flair.trainers` for the models and model trainers respectively. \r\n\r\n### Models package\r\n`flair.models` contains 3 model classes: `SequenceTagger`, `TextClassifier` and `LanguageModel`. \r\n\r\n### Trainers package\r\n`flair.trainers` contains 3 model trainer classes: `SequenceTaggerTrainer`, `TextClassifierTrainer` and `LanguageModelTrainer`.\r\n\r\n### Direct import from package\r\nYou call these classes directly from the packages, for instance the SequenceTagger is now instantiated as: \r\n\r\n```python\r\nfrom flair.models import SequenceTagger\r\ntagger = SequenceTagger.load('ner')\r\n```\r\n\r\n## Reorganized embeddings #12 \r\n\r\nClear distinction between token-level and document-level embeddings by adding two classes, namely `TokenEmbeddings` and `DocumentEmbeddings` from which respective embeddings need to inherit.\r\n\r\n\r\n# New Features\r\n\r\n### LanguageModelTrainer #24 #17 \r\n\r\nAdded `LanguageModelTrainer` class to train your own LM embeddings. \r\n\r\n### Document Classification #10\r\n\r\nAdded experimental `TextClassifier` model for document-level text classification. Also added corresponding model trainer class, i.e. `TextClassifierTrainer`.\r\n\r\n### Batch prediction #7 \r\n\r\nAdded batching into prediction method for faster sequence tagging \r\n\r\n### CPU-friendly pre-trained models #29 \r\n\r\nAdded pre-trained models with smaller LM embeddings for faster CPU-inference speed \r\n\r\nYou can load them by adding '-fast' to the model name. Only for English at present.  \r\n```python\r\nfrom flair.models import SequenceTagger\r\ntagger = SequenceTagger.load('ner-fast')\r\n```\r\n\r\n### Learning Rate Scheduling #19 \r\n\r\nAdded learning rate schedulers to all trainer classes for improved learning rate annealing functionality and control.\r\n\r\n### Auto-spawn on GPUs #19\r\n\r\nAll model classes now automatically spawn on GPUs if available. The separate `.cuda()` call is no longer necessary.\r\n\r\n# Bug Fixes\r\n\r\n### Retagging error #23 \r\n\r\nFixed error that occurred when using multiple pre-trained taggers on the same sentence.\r\n\r\n### Empty sentence error #33\r\n\r\nFixed error that caused data fetchers to sometimes create empty sentences.\r\n\r\n# Other\r\n\r\n### Unit Tests #15\r\n\r\nAdded a large set of automated unit tests for better stability. \r\n\r\n### Documentation #15\r\n\r\nExpanded documentation and tutorials. Also expanded descriptions of APIs.\r\n\r\n### Code Simplifications in sequence tagger #19\r\n\r\nA number of code simplifications all around, hopefully making the code easier to understand.\r\n",
        "dateCreated": "2018-08-03T15:14:14Z",
        "datePublished": "2018-08-03T16:25:27Z",
        "html_url": "https://github.com/flairNLP/flair/releases/tag/v0.2.0",
        "name": "Version 0.2.0",
        "tag_name": "v0.2.0",
        "tarball_url": "https://api.github.com/repos/flairNLP/flair/tarball/v0.2.0",
        "url": "https://api.github.com/repos/flairNLP/flair/releases/12246177",
        "zipball_url": "https://api.github.com/repos/flairNLP/flair/zipball/v0.2.0"
      },
      {
        "authorType": "User",
        "author_name": "alanakbik",
        "body": "First release of Flair Framework \r\n\r\nStatic word embeddings:\r\n- includes prepared word embeddings from GloVe, FastText, Numberbatch and Extvec\r\n- includes prepared word embeddings for English, German and Swedish\r\n\r\nContextual string embeddings:\r\n- includes pre-trained models for English and German\r\n\r\nText embeddings:\r\n- Two experimental methods for full-text embeddings (LSTM and Mean)\r\n\r\nSequence labeling:\r\n- pre-trained models for English (PoS-tagging, chunking and NER) \r\n- pre-trained models for German (PoS-tagging and NER) \r\n- experimental semantic frame detector for English\r\n",
        "dateCreated": "2018-07-13T13:56:05Z",
        "datePublished": "2018-07-13T13:58:33Z",
        "html_url": "https://github.com/flairNLP/flair/releases/tag/v0.1.0",
        "name": "Version 0.1.0",
        "tag_name": "v0.1.0",
        "tarball_url": "https://api.github.com/repos/flairNLP/flair/tarball/v0.1.0",
        "url": "https://api.github.com/repos/flairNLP/flair/releases/11916053",
        "zipball_url": "https://api.github.com/repos/flairNLP/flair/zipball/v0.1.0"
      }
    ],
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The project is based on PyTorch 1.5+ and Python 3.6+, because method signatures and type hints are beautiful.\nIf you do not have Python 3.6, install it first. [Here is how for Ubuntu 16.04](https://vsupalov.com/developing-with-python3-6-on-ubuntu-16-04/).\nThen, in your favorite virtual environment, simply do:\n\n```\npip install flair\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 11046,
      "date": "Mon, 13 Dec 2021 17:26:08 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "pytorch",
      "nlp",
      "named-entity-recognition",
      "sequence-labeling",
      "semantic-role-labeling",
      "word-embeddings",
      "natural-language-processing",
      "machine-learning"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Let's run named entity recognition (NER) over an example sentence. All you need to do is make a `Sentence`, load\na pre-trained model and use it to predict tags for the sentence:\n\n```python\nfrom flair.data import Sentence\nfrom flair.models import SequenceTagger\n\n#: make a sentence\nsentence = Sentence('I love Berlin .')\n\n#: load the NER tagger\ntagger = SequenceTagger.load('ner')\n\n#: run NER over sentence\ntagger.predict(sentence)\n```\n\nDone! The `Sentence` now has entity annotations. Print the sentence to see what the tagger found.\n\n```python\nprint(sentence)\nprint('The following NER tags are found:')\n\n#: iterate over entities and print\nfor entity in sentence.get_spans('ner'):\n    print(entity)\n```\n\nThis should print:\n\n```console\nSentence: \"I love Berlin .\" - 4 Tokens\n\nThe following NER tags are found:\n\nSpan [3]: \"Berlin\"   [\u2212 Labels: LOC (0.9992)]\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "We provide a set of quick tutorials to get you started with the library:\n\n* [Tutorial 1: Basics](/resources/docs/TUTORIAL_1_BASICS.md)\n* [Tutorial 2: Tagging your Text](/resources/docs/TUTORIAL_2_TAGGING.md)\n* [Tutorial 3: Embedding Words](/resources/docs/TUTORIAL_3_WORD_EMBEDDING.md)\n* [Tutorial 4: List of All Word Embeddings](/resources/docs/TUTORIAL_4_ELMO_BERT_FLAIR_EMBEDDING.md)\n* [Tutorial 5: Embedding Documents](/resources/docs/TUTORIAL_5_DOCUMENT_EMBEDDINGS.md)\n* [Tutorial 6: Loading a Dataset](/resources/docs/TUTORIAL_6_CORPUS.md)\n* [Tutorial 7: Training a Model](/resources/docs/TUTORIAL_7_TRAINING_A_MODEL.md)\n* [Tutorial 8: Training your own Flair Embeddings](/resources/docs/TUTORIAL_9_TRAINING_LM_EMBEDDINGS.md)\n* [Tutorial 9: Training a Zero Shot Text Classifier (TARS)](/resources/docs/TUTORIAL_10_TRAINING_ZERO_SHOT_MODEL.md)\n\nThe tutorials explain how the base NLP classes work, how you can load pre-trained models to tag your\ntext, how you can embed your text with different word or document embeddings, and how you can train your own\nlanguage models, sequence labeling models, and text classification models. Let us know if anything is unclear.\n\nThere is also a dedicated landing page for our **[biomedical NER and datasets](/resources/docs/HUNFLAIR.md)** with\ninstallation instructions and tutorials.\n\nThere are also good third-party articles and posts that illustrate how to use Flair:\n* [How to build a text classifier with Flair](https://towardsdatascience.com/text-classification-with-state-of-the-art-nlp-library-flair-b541d7add21f)\n* [How to build a microservice with Flair and Flask](https://shekhargulati.com/2019/01/04/building-a-sentiment-analysis-python-microservice-with-flair-and-flask/)\n* [A docker image for Flair](https://towardsdatascience.com/docker-image-for-nlp-5402c9a9069e)\n* [Great overview of Flair functionality and how to use in Colab](https://www.analyticsvidhya.com/blog/2019/02/flair-nlp-library-python/)\n* [Visualisation tool for highlighting the extracted entities](https://github.com/lunayach/visNER)\n* [Practical approach of State-of-the-Art Flair in Named Entity Recognition](https://medium.com/analytics-vidhya/practical-approach-of-state-of-the-art-flair-in-named-entity-recognition-46a837e25e6b)\n* [Benchmarking NER algorithms](https://towardsdatascience.com/benchmark-ner-algorithm-d4ab01b2d4c3)\n* [Training a Flair text classifier on Google Cloud Platform (GCP) and serving predictions on GCP](https://github.com/robinvanschaik/flair-on-gcp)\n* [Model Interpretability for transformer-based Flair models](https://github.com/robinvanschaik/interpret-flair)\n\n",
      "technique": "Header extraction"
    }
  ]
}