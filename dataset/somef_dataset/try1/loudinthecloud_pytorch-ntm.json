{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1410.5401"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/loudinthecloud/pytorch-ntm",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-09-24T05:13:53Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-07T21:54:13Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9460991490529256,
        0.9594650292281758,
        0.9789166399881951,
        0.9772937087312639,
        0.8939095077709607
      ],
      "excerpt": "PyTorch implementation of Neural Turing Machines (NTM). \nAn NTM is a memory augumented neural network (attached to external memory) where the interactions with the external memory (address, read, write) are done using differentiable transformations. Overall, the network is end-to-end differentiable and thus trainable by a gradient based optimizer. \nThe NTM is processing input in sequences, much like an LSTM, but with additional benfits: (1) The external memory allows the network to learn algorithmic tasks easier (2) Having larger capacity, without increasing the network's trainable parameters. \nThe external memory allows the NTM to learn algorithmic tasks, that are much harder for LSTM to learn, and to maintain an internal state much longer than traditional LSTMs. \nThis repository implements a vanilla NTM in a straight forward way. The following architecture is used: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.976182295674972,
        0.9682123710138442
      ],
      "excerpt": "The Copy task tests the NTM's ability to store and recall a long sequence of arbitrary information. The input to the network is a random sequence of bits, ending with a delimiter. The sequence lengths are randomised between 1 to 20. \nTraining convergence for the copy task using 4 different seeds (see the notebook for details) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.948558137676781,
        0.9682847767607622,
        0.9872268053295274,
        0.9682123710138442,
        0.9689641368918426
      ],
      "excerpt": "Here is an animated GIF that shows how the model generalize. The model was evaluated after every 500 training samples, using the target sequence shown in the upper part of the image. The bottom part shows the network output at any given training stage. \nThe following is the same, but with sequence length = 80. Note that the network was trained with sequences of lengths 1 to 20. \nThe Repeat Copy task tests whether the NTM can learn a simple nested function, and invoke it by learning to execute a for loop. The input to the network is a random sequence of bits, followed by a delimiter and a scalar value that represents the number of repetitions to output. The number of repetitions, was normalized to have zero mean and variance of one (as in the paper). Both the length of the sequence and the number of repetitions are randomised between 1 to 10. \nTraining convergence for the repeat-copy task using 4 different seeds (see the notebook for details) \nThe following image shows the input presented to the network, a sequence of bits + delimiter + num-reps scalar. Specifically the sequence length here is eight and the number of repetitions is five. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Neural Turing Machines (NTM) - PyTorch Implementation",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/loudinthecloud/pytorch-ntm/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 107,
      "date": "Mon, 13 Dec 2021 12:43:53 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/loudinthecloud/pytorch-ntm/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "loudinthecloud/pytorch-ntm",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/loudinthecloud/pytorch-ntm/master/notebooks/repeat-copy-task-plots.ipynb",
      "https://raw.githubusercontent.com/loudinthecloud/pytorch-ntm/master/notebooks/copy-task-plots.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The NTM can be used as a reusable module, currently not packaged though.\n\n1. Clone repository\n2. Install [PyTorch](http://pytorch.org/)\n3. pip install -r requirements.txt\n\n",
      "technique": "Header extraction"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/loudinthecloud/pytorch-ntm/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "BSD 3-Clause \"New\" or \"Revised\" License",
      "url": "https://api.github.com/licenses/bsd-3-clause"
    },
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "PyTorch Neural Turing Machine (NTM)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "pytorch-ntm",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "loudinthecloud",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/loudinthecloud/pytorch-ntm/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 491,
      "date": "Mon, 13 Dec 2021 12:43:53 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "pytorch",
      "ntm",
      "python",
      "notebook",
      "neural-network",
      "neural-turing-machines",
      "lstm"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Execute ./train.py\n\n```\nusage: train.py [-h] [--seed SEED] [--task {copy,repeat-copy}] [-p PARAM]\n                [--checkpoint-interval CHECKPOINT_INTERVAL]\n                [--checkpoint-path CHECKPOINT_PATH]\n                [--report-interval REPORT_INTERVAL]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --seed SEED           Seed value for RNGs\n  --task {copy,repeat-copy}\n                        Choose the task to train (default: copy)\n  -p PARAM, --param PARAM\n                        Override model params. Example: \"-pbatch_size=4\n                        -pnum_heads=2\"\n  --checkpoint-interval CHECKPOINT_INTERVAL\n                        Checkpoint interval (default: 1000). Use 0 to disable\n                        checkpointing\n  --checkpoint-path CHECKPOINT_PATH\n                        Path for saving checkpoint data (default: './')\n  --report-interval REPORT_INTERVAL\n                        Reporting interval\n```\n",
      "technique": "Header extraction"
    }
  ]
}