{
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{shen2020interfacegan,\n  title   = {InterFaceGAN: Interpreting the Disentangled Face Representation Learned by GANs},\n  author  = {Shen, Yujun and Yang, Ceyuan and Tang, Xiaoou and Zhou, Bolei},\n  journal = {TPAMI},\n  year    = {2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{shen2020interpreting,\n  title     = {Interpreting the Latent Space of GANs for Semantic Face Editing},\n  author    = {Shen, Yujun and Gu, Jinjin and Tang, Xiaoou and Zhou, Bolei},\n  booktitle = {CVPR},\n  year      = {2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9554441738822752
      ],
      "excerpt": "[Paper (CVPR)] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9548780004337553,
        0.9999263932607518,
        0.9565696357172301,
        0.9664456561658856
      ],
      "excerpt": "  title   = {InterFaceGAN: Interpreting the Disentangled Face Representation Learned by GANs}, \n  author  = {Shen, Yujun and Yang, Ceyuan and Tang, Xiaoou and Zhou, Bolei}, \n  journal = {TPAMI}, \n  year    = {2020} \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/genforce/interfacegan",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-07-26T16:49:51Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-07T04:48:59Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We provided following boundaries in folder `boundaries/`. The boundaries can be more accurate if stronger attribute predictor is used.\n\n- ProgressiveGAN model trained on CelebA-HQ dataset:\n  - Single boundary:\n    - `pggan_celebahq_pose_boundary.npy`: Pose.\n    - `pggan_celebahq_smile_boundary.npy`: Smile (expression).\n    - `pggan_celebahq_age_boundary.npy`: Age.\n    - `pggan_celebahq_gender_boundary.npy`: Gender.\n    - `pggan_celebahq_eyeglasses_boundary.npy`: Eyeglasses.\n    - `pggan_celebahq_quality_boundary.npy`: Image quality.\n  - Conditional boundary:\n    - `pggan_celebahq_age_c_gender_boundary.npy`: Age (conditioned on gender).\n    - `pggan_celebahq_age_c_eyeglasses_boundary.npy`: Age (conditioned on eyeglasses).\n    - `pggan_celebahq_age_c_gender_eyeglasses_boundary.npy`: Age (conditioned on gender and eyeglasses).\n    - `pggan_celebahq_gender_c_age_boundary.npy`: Gender (conditioned on age).\n    - `pggan_celebahq_gender_c_eyeglasses_boundary.npy`: Gender (conditioned on eyeglasses).\n    - `pggan_celebahq_gender_c_age_eyeglasses_boundary.npy`: Gender (conditioned on age and eyeglasses).\n    - `pggan_celebahq_eyeglasses_c_age_boundary.npy`: Eyeglasses (conditioned on age).\n    - `pggan_celebahq_eyeglasses_c_gender_boundary.npy`: Eyeglasses (conditioned on gender).\n    - `pggan_celebahq_eyeglasses_c_age_gender_boundary.npy`: Eyeglasses (conditioned on age and gender).\n- StyleGAN model trained on CelebA-HQ dataset:\n  - Single boundary in $\\mathcal{Z}$ space:\n    - `stylegan_celebahq_pose_boundary.npy`: Pose.\n    - `stylegan_celebahq_smile_boundary.npy`: Smile (expression).\n    - `stylegan_celebahq_age_boundary.npy`: Age.\n    - `stylegan_celebahq_gender_boundary.npy`: Gender.\n    - `stylegan_celebahq_eyeglasses_boundary.npy`: Eyeglasses.\n  - Single boundary in $\\mathcal{W}$ space:\n    - `stylegan_celebahq_pose_w_boundary.npy`: Pose.\n    - `stylegan_celebahq_smile_w_boundary.npy`: Smile (expression).\n    - `stylegan_celebahq_age_w_boundary.npy`: Age.\n    - `stylegan_celebahq_gender_w_boundary.npy`: Gender.\n    - `stylegan_celebahq_eyeglasses_w_boundary.npy`: Eyeglasses.\n\n- StyleGAN model trained on FF-HQ dataset:\n  - Single boundary in $\\mathcal{Z}$ space:\n    - `stylegan_ffhq_pose_boundary.npy`: Pose.\n    - `stylegan_ffhq_smile_boundary.npy`: Smile (expression).\n    - `stylegan_ffhq_age_boundary.npy`: Age.\n    - `stylegan_ffhq_gender_boundary.npy`: Gender.\n    - `stylegan_ffhq_eyeglasses_boundary.npy`: Eyeglasses.\n  - Conditional boundary in $\\mathcal{Z}$ space:\n    - `stylegan_ffhq_age_c_gender_boundary.npy`: Age (conditioned on gender).\n    - `stylegan_ffhq_age_c_eyeglasses_boundary.npy`: Age (conditioned on eyeglasses).\n    - `stylegan_ffhq_eyeglasses_c_age_boundary.npy`: Eyeglasses (conditioned on age).\n    - `stylegan_ffhq_eyeglasses_c_gender_boundary.npy`: Eyeglasses (conditioned on gender).\n  - Single boundary in $\\mathcal{W}$ space:\n    - `stylegan_ffhq_pose_w_boundary.npy`: Pose.\n    - `stylegan_ffhq_smile_w_boundary.npy`: Smile (expression).\n    - `stylegan_ffhq_age_w_boundary.npy`: Age.\n    - `stylegan_ffhq_gender_w_boundary.npy`: Gender.\n    - `stylegan_ffhq_eyeglasses_w_boundary.npy`: Eyeglasses.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9858744273746318
      ],
      "excerpt": "In this repository, we propose an approach, termed as InterFaceGAN, for semantic face editing. Specifically, InterFaceGAN is capable of turning an unconditionally trained face synthesis model to controllable GAN by interpreting the very first latent space and finding the hidden semantic subspaces. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9097969617726637
      ],
      "excerpt": "[Project Page] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9821433442968132
      ],
      "excerpt": "Before going into details, we would like to first introduce the two state-of-the-art GAN models used in this work, which are ProgressiveGAN (Karras el al., ICLR 2018) and StyleGAN (Karras et al., CVPR 2019). These two models achieve high-quality face synthesis by learning unconditional GANs. For more details about these two models, please refer to the original papers, as well as the official implementations. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.96482024030407
      ],
      "excerpt": "A GAN-based generative model basically maps the latent codes (commonly sampled from high-dimensional latent space, such as standart normal distribution) to photo-realistic images. Accordingly, a base class for generator, called BaseGenerator, is defined in models/base_generator.py. Basically, it should contains following member functions: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8373606867424773,
        0.9514800078688234,
        0.8725731763638491
      ],
      "excerpt": "sample(): Randomly sample latent codes. This function should specify what kind of distribution the latent code is subject to. \npreprocess(): Function to preprocess the latent codes before feeding it into the generator. \nsynthesize(): Run the model to get synthesized results (or any other intermediate outputs). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9602430825348763,
        0.9717426789551199
      ],
      "excerpt": "A clone of official tensorflow implementation: models/pggan_tf_official/. This clone is only used for converting tensorflow pre-trained weights to pytorch ones. This conversion will be done automitally when the model is used for the first time. After that, tensorflow version is not used anymore. \nPytorch implementation of official model (just for inference): models/pggan_generator_model.py. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9602430825348763,
        0.9717426789551199
      ],
      "excerpt": "A clone of official tensorflow implementation: models/stylegan_tf_official/. This clone is only used for converting tensorflow pre-trained weights to pytorch ones. This conversion will be done automitally when the model is used for the first time. After that, tensorflow version is not used anymore. \nPytorch implementation of official model (just for inference): models/stylegan_generator_model.py. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8288262952558703,
        0.8865152291791824,
        0.860059181823877
      ],
      "excerpt": "Support synthesizing images from $\\mathcal{Z}$ space, $\\mathcal{W}$ space, and extended $\\mathcal{W}$ space (18x512). \nSet truncation trick and noise randomization trick in models/model_settings.py. Among them, STYLEGAN_RANDOMIZE_NOISE is highly recommended to set as False. STYLEGAN_TRUNCATION_PSI = 0.7 and STYLEGAN_TRUNCATION_LAYERS = 8 are inherited from official implementation. Users can customize their own models. NOTE: These three settings will NOT affect the pre-trained weights. \nCustomized model: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9228910070716312
      ],
      "excerpt": "train_boundary(): This function can be used for boundary searching. It takes pre-prepared latent codes and the corresponding attributes scores as inputs, and then outputs the normal direction of the separation boundary. Basically, this goal is achieved by training a linear SVM. The returned vector can be further used for semantic face editing. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9229630841623571,
        0.8985235145637417
      ],
      "excerpt": "linear_interpolate(): This function can be used for semantic face editing. It takes a latent code and the normal direction of a particular semantic boundary as inputs, and then outputs a collection of manipulated latent codes with linear interpolation. These interpolation can be used to see how the synthesis will vary if moving the latent code along the given direction. \ngenerate_data.py: This script can be used for data preparation. It will generate a collection of syntheses (images are saved for further attribute prediction) as well as save the input latent codes. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9643066119943263
      ],
      "excerpt": "Get your own predictor for attribute $ATTRIBUTE_NAME, evaluate on all generated images, and save the inference results as data/pggan_celebahq/\"$ATTRIBUTE_NAME\"_scores.npy. NOTE: The save results should be with shape ($NUM, 1). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259,
        0.8979411005071259
      ],
      "excerpt": "    -c data/pggan_celebahq/z.npy \\ \n    -s data/pggan_celebahq/\"$ATTRIBUTE_NAME\"_scores.npy \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "[CVPR 2020] Interpreting the Latent Space of GANs for Semantic Face Editing",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/genforce/interfacegan/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 210,
      "date": "Tue, 07 Dec 2021 06:12:57 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/genforce/interfacegan/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "genforce/interfacegan",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/genforce/interfacegan/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/genforce/interfacegan/master/docs/InterFaceGAN.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```bash\nNUM=10000\npython generate_data.py -m pggan_celebahq -o data/pggan_celebahq -n \"$NUM\"\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9304372716542564
      ],
      "excerpt": "build(): Build a pytorch module. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8316083242431571
      ],
      "excerpt": "convert_tf_model() (Optional): Convert pre-trained weights from tensorflow model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8779423955433947
      ],
      "excerpt": "Before used, new model should be first registered in MODEL_POOL in file models/model_settings.py. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.838038594260824
      ],
      "excerpt": "generate_data.py: This script can be used for data preparation. It will generate a collection of syntheses (images are saved for further attribute prediction) as well as save the input latent codes. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python train_boundary.py \\ \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/genforce/interfacegan/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'Copyright (c) 2019 Yujun Shen\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy of\\nthis software and associated documentation files (the \"Software\"), to deal in\\nthe Software without restriction, including without limitation the rights to\\nuse, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\\nof the Software, and to permit persons to whom the Software is furnished to do\\nso, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\\nFOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\\nCOPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\\nIN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\\nCONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "InterFaceGAN - Interpreting the Latent Space of GANs for Semantic Face Editing",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "interfacegan",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "genforce",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/genforce/interfacegan/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1061,
      "date": "Tue, 07 Dec 2021 06:12:57 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Pick up a model, pick up a boundary, pick up a latent code, and then EDIT!\n\n```bash\n#: Before running the following code, please first download\n#: the pre-trained ProgressiveGAN model on CelebA-HQ dataset,\n#: and then place it under the folder \".models/pretrain/\".\nLATENT_CODE_NUM=10\npython edit.py \\\n    -m pggan_celebahq \\\n    -b boundaries/pggan_celebahq_smile_boundary.npy \\\n    -n \"$LATENT_CODE_NUM\" \\\n    -o results/pggan_celebahq_smile_editing\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "We take ProgressiveGAN model trained on CelebA-HQ dataset as an instance.\n\n",
      "technique": "Header extraction"
    }
  ]
}