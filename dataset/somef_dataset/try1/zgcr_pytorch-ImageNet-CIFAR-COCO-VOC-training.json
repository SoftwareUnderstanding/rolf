{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1708.02002\r\n\r\n| Network | resize | batch | gpu-num | apex | syncbn | epoch | mAP-mAP50-mAP75 |\r\n| --- | --- |  --- |  --- |  --- |  --- |  --- |  --- |\r\n| ResNet50-RetinaNet | RetinaStyleResize-400 | 32 | 2 RTX3090 | yes | no | 12 | 0.321,0.482,0.340 |\r\n| ResNet50-RetinaNet | RetinaStyleResize-800 | 8 | 2 RTX3090 | yes | no | 12 | 0.355,0.526,0.380 |\r\n\r\n## FCOS\r\n\r\nPaper:https://arxiv.org/abs/1904.01355 \r\n\r\n| Network | resize | batch | gpu-num | apex | syncbn | epoch | mAP-mAP50-mAP75 |\r\n| --- | --- |  --- |  --- |  --- |  --- |  --- |  --- |\r\n| ResNet50-FCOS | RetinaStyleResize-400 | 32 | 2 RTX3090 | yes | no | 12 | 0.346,0.527,0.366 |\r\n| ResNet50-FCOS | RetinaStyleResize-800 | 8 | 2 RTX3090 | yes | no | 12 | 0.379,0.562,0.410 |\r\n\r\n## CenterNet(Objects as Points",
      "https://arxiv.org/abs/1904.01355 \r\n\r\n| Network | resize | batch | gpu-num | apex | syncbn | epoch | mAP-mAP50-mAP75 |\r\n| --- | --- |  --- |  --- |  --- |  --- |  --- |  --- |\r\n| ResNet50-FCOS | RetinaStyleResize-400 | 32 | 2 RTX3090 | yes | no | 12 | 0.346,0.527,0.366 |\r\n| ResNet50-FCOS | RetinaStyleResize-800 | 8 | 2 RTX3090 | yes | no | 12 | 0.379,0.562,0.410 |\r\n\r\n## CenterNet(Objects as Points",
      "https://arxiv.org/abs/1904.07850\r\n\r\n| Network | resize | batch | gpu-num | apex | syncbn | epoch | mAP-mAP50-mAP75 |\r\n| --- | --- |  --- |  --- |  --- |  --- |  --- |  --- |\r\n| ResNet18DCNv2-CenterNet | YoloStyleResize-512 | 128 | 2 RTX3090 | yes | no | 140 | |\r\n\r\n## YOLO series\r\n\r\nPaper:https://arxiv.org/abs/1804.02767\r\n\r\n**How to use yolov3 anchor clustering method to generate a set of custom anchors for your own dataset?**\r\n\r\nI provide a script in simpleAICV/detection/yolov3_anchor_cluster.py,and I give two examples for generate anchors on COCO2017 and VOC2007+2012 datasets.If you want to generate anchors for your dataset,just modify the part of input code,get width and height of all annotaion boxes,then use the script to compute anchors.The anchors size will change with different datasets or different input resizes.\r\n\r\n| Network | resize | batch | gpu-num | apex | syncbn | epoch | mAP-mAP50-mAP75 |\r\n| --- | --- |  --- |  --- |  --- |  --- |  --- |  --- |\r\n| YOLOv3backbone-YOLOv4loss | YoloStyleResize-416 | 128 | 2 RTX3090 | yes | no | 500 | |\r\n\r\n\r\n# VOC2007+2012 detection training results\r\n\r\nTrained on VOC2007 trainval + VOC2012 trainval, tested on VOC2007,using 11-point interpolated AP.\r\n\r\n| Network | resize | batch | gpu-num | apex | syncbn | epoch | mAP |\r\n| --- | --- |  --- |  --- |  --- |  --- |  --- |  --- |\r\n| ResNet50-RetinaNet | RetinaStyleResize-400 | 32 | 2 RTX3090 | yes | no | 12 | 0.769 |\r\n\r\n# CIFAR100 classification training results\r\n\r\nTraining in nn.parallel mode result:\r\n\r\n| Network       | gpu-num | warm up | lr decay | total epochs | Top-1 error |\r\n| --- | --- |  --- |  --- |  --- |  --- | \r\n| ResNet-18     | 1 RTX2080Ti | no | multistep | 200 | 21.59 | \r\n| ResNet-34     | 1 RTX2080Ti | no | multistep | 200 | 21.16 | \r\n| ResNet-50     | 1 RTX2080Ti | no | multistep | 200 | 22.12 | \r\n| ResNet-101    | 1 RTX2080Ti | no | multistep | 200 | 19.84 | \r\n| ResNet-152    | 1 RTX2080Ti | no | multistep | 200 | 19.01 | \r\n\r\nYou can find more model training details in cifar100_experiments/resnet50cifar/.\r\n\r\n# ILSVRC2012(ImageNet",
      "https://arxiv.org/abs/1804.02767\r\n\r\n**How to use yolov3 anchor clustering method to generate a set of custom anchors for your own dataset?**\r\n\r\nI provide a script in simpleAICV/detection/yolov3_anchor_cluster.py,and I give two examples for generate anchors on COCO2017 and VOC2007+2012 datasets.If you want to generate anchors for your dataset,just modify the part of input code,get width and height of all annotaion boxes,then use the script to compute anchors.The anchors size will change with different datasets or different input resizes.\r\n\r\n| Network | resize | batch | gpu-num | apex | syncbn | epoch | mAP-mAP50-mAP75 |\r\n| --- | --- |  --- |  --- |  --- |  --- |  --- |  --- |\r\n| YOLOv3backbone-YOLOv4loss | YoloStyleResize-416 | 128 | 2 RTX3090 | yes | no | 500 | |\r\n\r\n\r\n# VOC2007+2012 detection training results\r\n\r\nTrained on VOC2007 trainval + VOC2012 trainval, tested on VOC2007,using 11-point interpolated AP.\r\n\r\n| Network | resize | batch | gpu-num | apex | syncbn | epoch | mAP |\r\n| --- | --- |  --- |  --- |  --- |  --- |  --- |  --- |\r\n| ResNet50-RetinaNet | RetinaStyleResize-400 | 32 | 2 RTX3090 | yes | no | 12 | 0.769 |\r\n\r\n# CIFAR100 classification training results\r\n\r\nTraining in nn.parallel mode result:\r\n\r\n| Network       | gpu-num | warm up | lr decay | total epochs | Top-1 error |\r\n| --- | --- |  --- |  --- |  --- |  --- | \r\n| ResNet-18     | 1 RTX2080Ti | no | multistep | 200 | 21.59 | \r\n| ResNet-34     | 1 RTX2080Ti | no | multistep | 200 | 21.16 | \r\n| ResNet-50     | 1 RTX2080Ti | no | multistep | 200 | 22.12 | \r\n| ResNet-101    | 1 RTX2080Ti | no | multistep | 200 | 19.84 | \r\n| ResNet-152    | 1 RTX2080Ti | no | multistep | 200 | 19.01 | \r\n\r\nYou can find more model training details in cifar100_experiments/resnet50cifar/.\r\n\r\n# ILSVRC2012(ImageNet"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\nIf you find my work useful in your research, please consider citing:\r\n```\r\n@inproceedings{zgcr,\r\n title={pytorch-ImageNet-CIFAR-COCO-VOC-training},\r\n author={zgcr},\r\n year={2020}\r\n}\r\n```",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{zgcr,\n title={pytorch-ImageNet-CIFAR-COCO-VOC-training},\n author={zgcr},\n year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8669112007448144
      ],
      "excerpt": "| ResNet50-RetinaNet | RetinaStyleResize-400 | 32 | 2 RTX3090 | yes | no | 12 | 0.321,0.482,0.340 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9977994744046882
      ],
      "excerpt": "Paper:https://arxiv.org/abs/1904.01355  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8669112007448144
      ],
      "excerpt": "| ResNet50-FCOS | RetinaStyleResize-400 | 32 | 2 RTX3090 | yes | no | 12 | 0.346,0.527,0.366 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8669112007448144
      ],
      "excerpt": "| ResNet50-RetinaNet | RetinaStyleResize-400 | 32 | 2 RTX3090 | yes | no | 12 | 0.769 | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/zgcr/simpleAICV-pytorch-ImageNet-COCO-training",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-05-31T05:37:26Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-11T15:58:48Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9898079987224274
      ],
      "excerpt": "1. Modify RetinaNet/FCOS loss calculation method.Training time is reduced by 40% and model performance is improved. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8311919047819192,
        0.8694707741450929
      ],
      "excerpt": "mAP is IoU=0.5:0.95,area=all,maxDets=100,mAP(COCOeval,stats[0]). \nmAP50 is IoU=0.5,area=all,maxDets=100,mAP(COCOeval,stats[1]). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8141812897725751,
        0.8141812897725751
      ],
      "excerpt": "| ResNet50-RetinaNet | RetinaStyleResize-400 | 32 | 2 RTX3090 | yes | no | 12 | 0.321,0.482,0.340 | \n| ResNet50-RetinaNet | RetinaStyleResize-800 | 8 | 2 RTX3090 | yes | no | 12 | 0.355,0.526,0.380 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9252087222371607,
        0.8828915741519303
      ],
      "excerpt": "How to use yolov3 anchor clustering method to generate a set of custom anchors for your own dataset? \nI provide a script in simpleAICV/detection/yolov3_anchor_cluster.py,and I give two examples for generate anchors on COCO2017 and VOC2007+2012 datasets.If you want to generate anchors for your dataset,just modify the part of input code,get width and height of all annotaion boxes,then use the script to compute anchors.The anchors size will change with different datasets or different input resizes. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8141812897725751
      ],
      "excerpt": "| ResNet50-RetinaNet | RetinaStyleResize-400 | 32 | 2 RTX3090 | yes | no | 12 | 0.769 | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Training examples and results for ImageNet(ILSVRC2012)/COCO2017/VOC2007+VOC2012 datasets.Include ResNet/DarkNet/RegNet/RetinaNet/FCOS/CenterNet/YOLO series.",
      "technique": "GitHub API"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\nYou can download all my pretrained models from here:\r\n```\r\nhttps://drive.google.com/drive/folders/1t8vmuxy_rTNczJo_Ej5zFd84fFMYap-I?usp=sharing\r\n```\r\n\r\nIf you are in China,you can download from here:\r\n```\r\n\u94fe\u63a5: https://pan.baidu.com/s/1leeoHAUZtnxc9ing38E3Nw\r\n\u63d0\u53d6\u7801: 4epf\r\n```\r\n\r\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/zgcr/pytorch-ImageNet-CIFAR-COCO-VOC-training/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 50,
      "date": "Sun, 12 Dec 2021 00:02:47 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/zgcr/simpleAICV-pytorch-ImageNet-COCO-training/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "zgcr/simpleAICV-pytorch-ImageNet-COCO-training",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/zgcr/pytorch-ImageNet-CIFAR-COCO-VOC-training/master/classification_training/cifar100/resnetforcifar_train_example/test.sh",
      "https://raw.githubusercontent.com/zgcr/pytorch-ImageNet-CIFAR-COCO-VOC-training/master/classification_training/cifar100/resnetforcifar_train_example/train.sh",
      "https://raw.githubusercontent.com/zgcr/pytorch-ImageNet-CIFAR-COCO-VOC-training/master/classification_training/imagenet/efficientnet_train_example/test.sh",
      "https://raw.githubusercontent.com/zgcr/pytorch-ImageNet-CIFAR-COCO-VOC-training/master/classification_training/imagenet/efficientnet_train_example/train.sh",
      "https://raw.githubusercontent.com/zgcr/pytorch-ImageNet-CIFAR-COCO-VOC-training/master/classification_training/imagenet/resnet_vovnet_darknet_train_example/test.sh",
      "https://raw.githubusercontent.com/zgcr/pytorch-ImageNet-CIFAR-COCO-VOC-training/master/classification_training/imagenet/resnet_vovnet_darknet_train_example/train.sh",
      "https://raw.githubusercontent.com/zgcr/pytorch-ImageNet-CIFAR-COCO-VOC-training/master/classification_training/imagenet/regnet_train_example/test.sh",
      "https://raw.githubusercontent.com/zgcr/pytorch-ImageNet-CIFAR-COCO-VOC-training/master/classification_training/imagenet/regnet_train_example/train.sh",
      "https://raw.githubusercontent.com/zgcr/pytorch-ImageNet-CIFAR-COCO-VOC-training/master/detection_training/voc/retinanet_res50_resize400_multi_ciou/test.sh",
      "https://raw.githubusercontent.com/zgcr/pytorch-ImageNet-CIFAR-COCO-VOC-training/master/detection_training/voc/retinanet_res50_resize400_multi_ciou/train.sh",
      "https://raw.githubusercontent.com/zgcr/pytorch-ImageNet-CIFAR-COCO-VOC-training/master/detection_training/coco/retinanet_res50_resize800_multi_ciou/test.sh",
      "https://raw.githubusercontent.com/zgcr/pytorch-ImageNet-CIFAR-COCO-VOC-training/master/detection_training/coco/retinanet_res50_resize800_multi_ciou/train.sh",
      "https://raw.githubusercontent.com/zgcr/pytorch-ImageNet-CIFAR-COCO-VOC-training/master/detection_training/coco/retinanet_res50_resize400_multi_ciou/test.sh",
      "https://raw.githubusercontent.com/zgcr/pytorch-ImageNet-CIFAR-COCO-VOC-training/master/detection_training/coco/retinanet_res50_resize400_multi_ciou/train.sh",
      "https://raw.githubusercontent.com/zgcr/pytorch-ImageNet-CIFAR-COCO-VOC-training/master/detection_training/coco/yolov5l_resize416_multi/test.sh",
      "https://raw.githubusercontent.com/zgcr/pytorch-ImageNet-CIFAR-COCO-VOC-training/master/detection_training/coco/yolov5l_resize416_multi/train.sh",
      "https://raw.githubusercontent.com/zgcr/pytorch-ImageNet-CIFAR-COCO-VOC-training/master/detection_training/coco/yolov4_resize416_multi/test.sh",
      "https://raw.githubusercontent.com/zgcr/pytorch-ImageNet-CIFAR-COCO-VOC-training/master/detection_training/coco/yolov4_resize416_multi/train.sh",
      "https://raw.githubusercontent.com/zgcr/pytorch-ImageNet-CIFAR-COCO-VOC-training/master/detection_training/coco/fcos_res50_resize800_multi_ciou/test.sh",
      "https://raw.githubusercontent.com/zgcr/pytorch-ImageNet-CIFAR-COCO-VOC-training/master/detection_training/coco/fcos_res50_resize800_multi_ciou/train.sh",
      "https://raw.githubusercontent.com/zgcr/pytorch-ImageNet-CIFAR-COCO-VOC-training/master/detection_training/coco/centernet_res18_yoloresize512_multi/test.sh",
      "https://raw.githubusercontent.com/zgcr/pytorch-ImageNet-CIFAR-COCO-VOC-training/master/detection_training/coco/centernet_res18_yoloresize512_multi/train.sh",
      "https://raw.githubusercontent.com/zgcr/pytorch-ImageNet-CIFAR-COCO-VOC-training/master/detection_training/coco/yolov3backbone_yolov4loss_resize416_multi/test.sh",
      "https://raw.githubusercontent.com/zgcr/pytorch-ImageNet-CIFAR-COCO-VOC-training/master/detection_training/coco/yolov3backbone_yolov4loss_resize416_multi/train.sh",
      "https://raw.githubusercontent.com/zgcr/pytorch-ImageNet-CIFAR-COCO-VOC-training/master/detection_training/coco/fcos_res50_resize400_multi_ciou/test.sh",
      "https://raw.githubusercontent.com/zgcr/pytorch-ImageNet-CIFAR-COCO-VOC-training/master/detection_training/coco/fcos_res50_resize400_multi_ciou/train.sh",
      "https://raw.githubusercontent.com/zgcr/pytorch-ImageNet-CIFAR-COCO-VOC-training/master/inference_demo/run_classify_single_image.sh",
      "https://raw.githubusercontent.com/zgcr/pytorch-ImageNet-CIFAR-COCO-VOC-training/master/inference_demo/run_detect_video.sh",
      "https://raw.githubusercontent.com/zgcr/pytorch-ImageNet-CIFAR-COCO-VOC-training/master/inference_demo/run_detect_single_image.sh",
      "https://raw.githubusercontent.com/zgcr/pytorch-ImageNet-CIFAR-COCO-VOC-training/master/segmentation_training%28developing%29/coco/solov2_res50_resize400_multi/test.sh",
      "https://raw.githubusercontent.com/zgcr/pytorch-ImageNet-CIFAR-COCO-VOC-training/master/segmentation_training%28developing%29/coco/solov2_res50_resize400_multi/train.sh",
      "https://raw.githubusercontent.com/zgcr/pytorch-ImageNet-CIFAR-COCO-VOC-training/master/segmentation_training%28developing%29/coco/condinst_res50_resize400_multi/test.sh",
      "https://raw.githubusercontent.com/zgcr/pytorch-ImageNet-CIFAR-COCO-VOC-training/master/segmentation_training%28developing%29/coco/condinst_res50_resize400_multi/train.sh",
      "https://raw.githubusercontent.com/zgcr/pytorch-ImageNet-CIFAR-COCO-VOC-training/master/distillation_training/imagenet/resnet_kd_example/train.sh",
      "https://raw.githubusercontent.com/zgcr/pytorch-ImageNet-CIFAR-COCO-VOC-training/master/distillation_training/cifar/resnetforcifar_kd_example/train.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\nIf you want to reproduce my imagenet pretrained models,you need download ILSVRC2012 dataset,and make sure the folder architecture as follows:\r\n```\r\nILSVRC2012\r\n|\r\n|-----train----1000 sub classes folders\r\n|\r\n|-----val------1000 sub classes folders\r\nPlease make sure the same class has same class folder name in train and val folders.\r\n```\r\n\r\nIf you want to reproduce my COCO pretrained models,you need download COCO2017 dataset,and make sure the folder architecture as follows:\r\n```\r\nCOCO2017\r\n|\r\n|-----annotations----all label jsons\r\n|                 \r\n|                |----train2017\r\n|----images------|----val2017\r\n                 |----test2017\r\n```\r\n\r\nIf you want to reproduce my VOC pretrained models,you need download VOC2007+VOC2012 dataset,and make sure the folder architecture as follows:\r\n```\r\nVOCdataset\r\n|                 |----Annotations\r\n|                 |----ImageSets\r\n|----VOC2007------|----JPEGImages\r\n|                 |----SegmentationClass\r\n|                 |----SegmentationObject\r\n|        \r\n|                 |----Annotations\r\n|                 |----ImageSets\r\n|----VOC2012------|----JPEGImages\r\n|                 |----SegmentationClass\r\n|                 |----SegmentationObject\r\n```\r\n\r\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8916879184195015,
        0.8080750779809578
      ],
      "excerpt": "If you want to reproduce my model,you need enter a training folder directory,then run train.sh and test.sh. \nFor example,you can enter classification_training/imagenet/resnet_vovnet_darknet_example. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8969286805388671,
        0.8410229392416168
      ],
      "excerpt": "1. All classification/detection/segmentation model have a public train.py and test.py file in tools/. \n2. For training and testing, train.info.log and test.info.log files are generated in the work directory respectively. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8857207942149545
      ],
      "excerpt": "If you want to reproduce my model,you need enter a training folder directory,then run train.sh and test.sh. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8372525635443605
      ],
      "excerpt": "If you want to train this model,run train.sh: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8396003598031524
      ],
      "excerpt": "if you want to test this model,run test.sh: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8170316732358935
      ],
      "excerpt": "classification testing example: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8048928530595778
      ],
      "excerpt": "I provide a script in simpleAICV/detection/yolov3_anchor_cluster.py,and I give two examples for generate anchors on COCO2017 and VOC2007+2012 datasets.If you want to generate anchors for your dataset,just modify the part of input code,get width and height of all annotaion boxes,then use the script to compute anchors.The anchors size will change with different datasets or different input resizes. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8041121693066079,
        0.8263938709729344
      ],
      "excerpt": "Training in nn.parallel mode result: \n| Network       | gpu-num | warm up | lr decay | total epochs | Top-1 error | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8084113468628878
      ],
      "excerpt": "| ResNet-34     | 1 RTX2080Ti | no | multistep | 200 | 21.16 |  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8263938709729344
      ],
      "excerpt": "| Network       | gpu-num | warm up | lr decay | total epochs | Top-1 error | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.818637432490057
      ],
      "excerpt": "| ResNet-34     | 4 RTX2080Ti | no | multistep | 100 | 26.264 |  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8254200472018
      ],
      "excerpt": "| ResNet-50     | 4 RTX2080Ti | no | multistep | 100 | 23.488 |  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8263938709729344
      ],
      "excerpt": "| Network       | gpu-num | sync-BN |warm up | lr decay | total epochs | Top-1 error | \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/zgcr/simpleAICV-pytorch-ImageNet-COCO-training/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\r\\n\\r\\nCopyright (c) 2020 zgcr\\r\\n\\r\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\r\\nof this software and associated documentation files (the \"Software\"), to deal\\r\\nin the Software without restriction, including without limitation the rights\\r\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\r\\ncopies of the Software, and to permit persons to whom the Software is\\r\\nfurnished to do so, subject to the following conditions:\\r\\n\\r\\nThe above copyright notice and this permission notice shall be included in all\\r\\ncopies or substantial portions of the Software.\\r\\n\\r\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\r\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\r\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\r\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\r\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\r\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\r\\nSOFTWARE.'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "my-zhihu-column)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "simpleAICV-pytorch-ImageNet-COCO-training",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "zgcr",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/zgcr/simpleAICV-pytorch-ImageNet-COCO-training/blob/master/ReadMe.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "\r\nPlatform:Ubuntu 18.04\r\n\r\n```\r\npython==3.7.7\r\ntorch==1.8.0\r\ntorchvision==0.9.0\r\ntorchaudio==0.8.0\r\npycocotools==2.0.2\r\nnumpy\r\nCython\r\nmatplotlib\r\nopencv-python\r\ntqdm\r\nthop\r\n```\r\n\r\nuse python -m pip or conda command to install those packages:\r\n\r\n```\r\npython -m pip install -r requirement.txt\r\nconda install --yes --file requirements.txt\r\n```\r\n\r\n**How to install apex?**\r\n\r\napex needs to be installed separately.Please use the following orders to install apex:\r\n```\r\ngit clone https://github.com/NVIDIA/apex\r\ncd apex\r\npip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./\r\n```\r\nIf the above command fails to install apex\uff0cyou can use the following orders to install apex:\r\n```\r\ngit clone https://github.com/NVIDIA/apex\r\ncd apex\r\npip install -v --no-cache-dir ./\r\n```\r\nUsing apex to train can reduce video memory usage by 25%-30%, but the training speed will be slower, the trained model has the same performance as not using apex.\r\n\r\n**How to use DCNv2 with apex mixed precision training opt_level='O1' (for CenterNet:Objects as Points training)?**\r\n\r\nI write DCNv2 by using torchvision.ops.deform_conv2d function in simpleAICV/detection/models/dcnv2.py. It doesn't need to install DCNv2 in https://github.com/CharlesShang/DCNv2.git, just make sure your torchvision version>=0.9.0.\r\n\r\ntorchvision.ops.deform_conv2d function can't use apex mixed precision training,so I register this function as a float function in tools/utils.py in build_training_mode function. If you use apex mixed precision training for centernet training,the torchvision.ops.deform_conv2d function actually do single precision float point computation(other layers do half precision float point computation or single precision float point computation due to apex ops rule).\r\n\r\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 213,
      "date": "Sun, 12 Dec 2021 00:02:47 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "pytorch",
      "retinanet",
      "darknet",
      "fcos",
      "centernet",
      "yolov3",
      "classification",
      "detection",
      "imagenet",
      "voc",
      "coco",
      "resnet",
      "yolov4",
      "yolov5"
    ],
    "technique": "GitHub API"
  }
}