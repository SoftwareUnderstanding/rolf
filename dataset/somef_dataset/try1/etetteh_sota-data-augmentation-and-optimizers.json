{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1905.11946",
      "https://arxiv.org/abs/1805.09501v1",
      "https://arxiv.org/abs/1907.08610"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9372151656983196
      ],
      "excerpt": "AdvProp (Adversarial Examples Improve Image Recognition) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/etetteh/sota-data-augmentation-and-optimizers",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-01-11T08:29:24Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-05T08:06:40Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9853888546053072,
        0.9127680209940369,
        0.9792745720728325,
        0.9412687546588627,
        0.8163129454805981,
        0.9592000312960244,
        0.9242547995662637,
        0.8974240188577086,
        0.8894284035869966,
        0.8403967707751868
      ],
      "excerpt": "This repository contains some of the latest data augmentation techniques and optimizers for image classification using pytorch and the CIFAR10 dataset \nThe main update was adding efficient-b4 described in Efficient and made it the main model for the bash scripts. The sample notebooks were modified to make use of the custom model. Also added the link to the pre-trained weights of efficientnet-b4, located in checkpoint \nThis repository implements the following data augmentation techniques. The links to the papers and pytorch code references are associated accordingly (some with slight modification). \nCutOut (Improved Regularization of Convolutional Neural Networks with Cutout, code) \nAutoAugment (AutoAugment: Learning Augmentation Policies from Data, code) \nRandAugment (RandAugment: Practical automated data augmentation with a reduced search space, code) \nAugMix (AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty, code) \nISDA (Implicit Semantic Data Augmentation for Deep Networks, code) \nCustom convolutional neural network model that makes use of depthwise convolution and squeeze-and-excitation and the mish activation function. \nYou may use any model of choice, but slight modification is needed in order to implement ISDA. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9567588029116127,
        0.9567588029116127
      ],
      "excerpt": "Ranger (RAdam with LookAhead)  \nAdaLook (AdaMod with LookAhead (mine)) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8983659220979224
      ],
      "excerpt": "The sample_notenook.ipynb and isda_sample_notebook.ipynb contains code to play with the various augmentation and optimizer techniques for non-isda and isda implementations. Simply comment or uncomment appropriate lines. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9399766247654442
      ],
      "excerpt": "Data Augmentation Techniques on CIFAR10 with PyTorch. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9261113223473905,
        0.9261113223473905,
        0.9261113223473905,
        0.9261113223473905
      ],
      "excerpt": "--cutout              Using CutOut data augmentation technique. \n--autoaug             Using AutoAugment data augmentation technique. \n--randaug             Using RandAugment data augmentation technique. \n--augmix              Using AugMixt data augmentation technique. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9722069705383333,
        0.8142472207259578
      ],
      "excerpt": "Results with my custom model and other models. \nCutMix (CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features, code) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "This repository contains some of the latest data augmentation techniques and optimizers for image classification using pytorch and the CIFAR10 dataset",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/etetteh/sota-data-augmentation-and-optimizers/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 6,
      "date": "Wed, 08 Dec 2021 22:59:57 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/etetteh/sota-data-augmentation-and-optimizers/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "etetteh/sota-data-augmentation-and-optimizers",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/etetteh/sota-data-augmentation-and-optimizers/master/sample_notebook.ipynb",
      "https://raw.githubusercontent.com/etetteh/sota-data-augmentation-and-optimizers/master/train.ipynb",
      "https://raw.githubusercontent.com/etetteh/sota-data-augmentation-and-optimizers/master/isda_sample_notebook.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "[Install Python 3 with anaconda](https://www.continuum.io/downloads)\n\n    $ git clone https://github.com/etetteh/sota-data-augmentation-and-optimizers\n    $ cd sota-data-augmentation-and-optimizers\n    $ pip install -r requirements.txt\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9448609967477957
      ],
      "excerpt": "You may run the following line of code in your bash terminal \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8057675898656085
      ],
      "excerpt": "           [--adalook] [--deepmemory] [--ranger] [--resume] [--path PATH] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8702932804461149
      ],
      "excerpt": "--path PATH           path to checkpoint. pass augmentation name \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9333384803827206,
        0.9530335865083206
      ],
      "excerpt": "$ python main.py -h \nusage: main.py [-h] [--cutout] [--autoaug] [--randaug] [--augmix] [--adamod] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8332655064876293
      ],
      "excerpt": "--resume, -r          resume training from checkpoint. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8491539905138519
      ],
      "excerpt": "                    input batch size for training. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.82047488157797
      ],
      "excerpt": "                    Number of iterations to print out results \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8778487586960795,
        0.8223386906086514,
        0.9143939343147436,
        0.8571102419564544,
        0.9143939343147436,
        0.8223386906086514,
        0.8821296453214145
      ],
      "excerpt": "Example:  \nTo train using adamod optimizer and augmix augmentation for 100 epochs, run: \n$ python main.py --adamod --augmix --epochs 100 \nTo resume training, run: \n$ python main.py --resume --adamod --augmix --epochs 100 \nTo train using isda, adamod optimizer and augmix augmentation for 100 epochs, run: \n$ python main_isda.py --adamod --augmix --epochs 100 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/etetteh/sota-data-augmentation-and-optimizers/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 Enoch Tetteh\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "sota-data-augmentation-and-optimizers",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "sota-data-augmentation-and-optimizers",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "etetteh",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/etetteh/sota-data-augmentation-and-optimizers/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 27,
      "date": "Wed, 08 Dec 2021 22:59:57 GMT"
    },
    "technique": "GitHub API"
  }
}