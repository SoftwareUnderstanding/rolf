{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2106.03714",
      "https://arxiv.org/abs/2104.10858",
      "https://arxiv.org/abs/2106.03714"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/zhoudaquan/Refiner_ViT",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-06-05T07:06:14Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-04T11:47:44Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "**Refined Vision Transformer** is initially described in [arxiv](https://arxiv.org/abs/2106.03714), which observes vision transformers require much more datafor model pre-training. Most of recent works thus are dedicated to designing morecomplex architectures or training methods to address the data-efficiency issue ofViTs. However, few of them explore improving the self-attention mechanism, akey factor distinguishing ViTs from CNNs.  Different from existing works, weintroduce a conceptually simple scheme, calledrefiner, to directly refine the self-attention maps of ViTs.  Specifically, refiner exploresattention expansionthatprojects the multi-head attention maps to a higher-dimensional space to promotetheir diversity.  Further, refiner applies convolutions to augment local patternsof the attention maps, which we show is equivalent to adistributed local atten-tion\u2014features are aggregated locally with learnable kernels and then globallyaggregated with self-attention.  Extensive experiments demonstrate that refinerworks surprisingly well. Significantly, it enables ViTs to achieve 86% top-1 classifi-cation accuracy on ImageNet with only 81M parameters.\n\n<p align=\"center\">\n<img src=\"https://github.com/zhoudaquan/Refiner_ViT/blob/master/figures/overall_flow.png\" | width=500>\n</p>\n\nPlease run git clone with --recursive to clone timm as submodule and install it with ` cd pytorch-image-models && pip install -e ./`\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9872208169019511
      ],
      "excerpt": "This repo is the official implementation of \"Refiner: Refining Self-attention for Vision Transformers\". The repo is build on top of timm and include the relabbeling trick included in TokenLabelling. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/zhoudaquan/Refiner_ViT/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 11,
      "date": "Tue, 07 Dec 2021 07:24:19 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/zhoudaquan/Refiner_ViT/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "zhoudaquan/Refiner_ViT",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/zhoudaquan/Refiner_ViT/master/distributed_train.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9345089952047377
      ],
      "excerpt": "bash run.sh scripts/refiner_s.yaml \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/zhoudaquan/Refiner_ViT/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "RefinerViT",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Refiner_ViT",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "zhoudaquan",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/zhoudaquan/Refiner_ViT/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "torch>=1.4.0\ntorchvision>=0.5.0\npyyaml\nnumpy\ntimm==0.4.5\n\nA summary of the results are shown below for quick reference. Details can be found in the paper.\n\n| Model                          | head  | layer|   dim |Image resolution| Param | Top 1 |\n| :--------------------------    | :-----|:---  |:------|:--------------:| -----:| -----:|\n| Refiner-ViT-S                  |  12   | 16   | 384   |    224         | 25M   | 83.6  |\n| Refiner-ViT-S                  |  12   | 16   | 384   |    384         | 25M   | 84.6  |\n| Refiner-ViT-M                  |  12   | 32   | 420   |    224         | 55M   | 84.6  |\n| Refiner-ViT-M                  |  12   | 32   | 420   |    384         | 55M   | 85.6  |\n| Refiner-ViT-L                  |  16   | 32   | 512   |    224         | 81M   | 84.9  |\n| Refiner-ViT-L                  |  16   | 32   | 512   |    384         | 81M   | 85.8  |\n| Refiner-ViT-L                  |  16   | 32   | 512   |    448         | 81M   | [86.0](https://drive.google.com/file/d/12v1WbxG6_mZQ0FYnXIhKiqbtg5bI4pGL/view?usp=sharing)  |\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 80,
      "date": "Tue, 07 Dec 2021 07:24:19 GMT"
    },
    "technique": "GitHub API"
  }
}