{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2005.00646",
      "https://arxiv.org/abs/1909.02151"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9190254459807167
      ],
      "excerpt": "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/96/Pytorch_logo.png/800px-Pytorch_logo.png\" width=\"10%\">  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9928166219158211,
        0.9996153475365146
      ],
      "excerpt": "Scalable Multi-Hop Relational Reasoning for Knowledge-Aware Question Answering \nYanlin Feng*, Xinyue Chen*, Bill Yuchen Lin, Peifeng Wang, Jun Yan and Xiang Ren. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9105368110547479
      ],
      "excerpt": "| NumberBatch     | 300            | https://github.com/commonsense/conceptnet-numberbatch   | entities | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "        \u251c\u2500\u2500 graphs/                (extracted subgraphs) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/INK-USC/MHGRN",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-04-28T08:46:41Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-05T13:44:03Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9981583325195974
      ],
      "excerpt": "This is the repo of our EMNLP'20 paper: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9786597147154409
      ],
      "excerpt": "This repository also implements other graph encoding models for question answering (including vanilla LM finetuning). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8458383779081232
      ],
      "excerpt": "Each model supports the following text encoders: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9581410409135547
      ],
      "excerpt": "We provide preprocessed ConceptNet and pretrained entity embeddings for your own usage. These resources are independent of the source code. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8790943010662566
      ],
      "excerpt": "Convert the original datasets into .jsonl files (stored in data/csqa/statement/) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8434388460651738
      ],
      "excerpt": "Identify all mentioned concepts in the questions and answers \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8979411005071259
      ],
      "excerpt": "\u2514\u2500\u2500 data/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9711356502162093
      ],
      "excerpt": "To search the parameters for RoBERTa-Large on CommonsenseQA: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.975997516047827
      ],
      "excerpt": "To search the parameters for BERT+RelationNet on CommonsenseQA: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9095355914306156
      ],
      "excerpt": "Each graph encoding model is implemented in a single script: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.897881609371616
      ],
      "excerpt": "| --ent_emb                     | {transe, numberbatch, tzw}                                 | Entity Embeddings                | default=tzw (BERT-based node features)                       | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8321606706162991
      ],
      "excerpt": "| --unfreeze_epoch              | {0, 3}                                                     | Freeze Text Encoder for N epochs | model specific                                               | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9720114067000452
      ],
      "excerpt": "To reproduce the reported results of MultiGRN on CommonsenseQA official set: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Scalable Multi-Hop Relational Reasoning for Knowledge-Aware Question Answering (EMNLP 2020)",
      "technique": "GitHub API"
    }
  ],
  "download": [
    {
      "confidence": [
        1
      ],
      "excerpt": "First, you need to download all the necessary data in order to train the model:\n\n```bash\ngit clone https://github.com/INK-USC/MHGRN.git\ncd MHGRN\nbash scripts/download.sh\n```\n\nThe script will:\n\n- Download the [CommonsenseQA](<https://www.tau-nlp.org/commonsenseqa>) dataset\n- Download [ConceptNet](<http://conceptnet.io/>)\n- Download pretrained TransE embeddings\n\n",
      "technique": "Header extraction"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/INK-USC/MHGRN/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 40,
      "date": "Mon, 06 Dec 2021 07:11:07 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/INK-USC/MHGRN/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "INK-USC/MHGRN",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/INK-USC/MHGRN/master/scripts/run_grn_csqa.sh",
      "https://raw.githubusercontent.com/INK-USC/MHGRN/master/scripts/download.sh",
      "https://raw.githubusercontent.com/INK-USC/MHGRN/master/scripts/param_search_lm.sh",
      "https://raw.githubusercontent.com/INK-USC/MHGRN/master/scripts/param_search_rn.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9793738741147477
      ],
      "excerpt": "Note that the following reousrces can be download here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8190765847387247
      ],
      "excerpt": "| NumberBatch     | 300            | https://github.com/commonsense/conceptnet-numberbatch   | entities | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8874575836382347
      ],
      "excerpt": "bash scripts/param_search_lm.sh csqa roberta-large \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8874575836382347
      ],
      "excerpt": "bash scripts/param_search_rn.sh csqa bert-large-uncased \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8360359455305265
      ],
      "excerpt": "For example, run the following command to train a RoBERTa-Large model on CommonsenseQA: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9414119019616536
      ],
      "excerpt": "bash scripts/run_grn_csqa.sh \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8265545461260216
      ],
      "excerpt": "| ConceptNet (CSV format)      | conceptnet-5.6.0-csv | English tuples extracted from the full conceptnet with merged relations | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8961600798244392
      ],
      "excerpt": "Entity embeddings are packed into a matrix of shape (#ent, dim) and stored in numpy format. Use np.load to read the file. You may need to download the vocabulary files first. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8079640800010552
      ],
      "excerpt": "| TransE          | 100            | Obtained using OpenKE with optim=sgd, lr=1e-3, epoch=1000 | entities relations | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8181435911925999
      ],
      "excerpt": "To preprocess the data, run: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python preprocess.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8884132527357781
      ],
      "excerpt": "python preprocess.py -p 20 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8031683144006825
      ],
      "excerpt": "Convert the original datasets into .jsonl files (stored in data/csqa/statement/) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9336801098518991,
        0.8210268420700089,
        0.9336801098518991,
        0.8349816249933664
      ],
      "excerpt": "| Gcon-Attn                                                    | gconattn.py |                                                              | \n| KV-Memory                                                    | kvmem.py    |                                                              | \n| MHGRN                                                        | grn.py      |                                                              | \nSome important command line arguments are listed as follows (run python {lm,rn,rgcn,...}.py -h for a complete list): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8950165314659483
      ],
      "excerpt": "| --mode                        | {train, eval, ...}                                         | Training or Evaluation           | default=train                                                | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8683880368315368
      ],
      "excerpt": "| -elr, --encoder_lr            | {1e-5, 2e-5, 3e-5, 6e-5, 1e-4}                             | Text Encoder LR                  | dataset specific and text encoder specific, default values in utils/parser_utils.py | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8123434636430626,
        0.8390460645774337
      ],
      "excerpt": "| -bs, --batch_size             | {16, 32, 64}                                               | Batch Size                       | default=32                                                   | \n| --save_dir                    | str                                                        | Checkpoint Directory             | model specific                                               | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8204085004354433
      ],
      "excerpt": "For example, run the following command to train a RoBERTa-Large model on CommonsenseQA: \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/INK-USC/MHGRN/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Multi-Hop Graph Relation Networks (EMNLP 2020)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "MHGRN",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "INK-USC",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/INK-USC/MHGRN/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- [Python](<https://www.python.org/>) >= 3.6\n- [PyTorch](<https://pytorch.org/get-started/locally/>) == 1.1.0\n- [transformers](<https://github.com/huggingface/transformers/tree/v2.0.0>) == 2.0.0\n- [tqdm](<https://github.com/tqdm/tqdm>)\n- [dgl](<https://github.com/dmlc/dgl>) == 0.3.1 (GPU version)\n- [networkx](<https://networkx.github.io/>) == 2.3\n\nRun the following commands to create a conda environment (assume CUDA10):\n\n```bash\nconda create -n krqa python=3.6 numpy matplotlib ipython\nsource activate krqa\nconda install pytorch=1.1.0 torchvision cudatoolkit=10.0 -c pytorch\npip install dgl-cu100==0.3.1\npip install transformers==2.0.0 tqdm networkx==2.3 nltk spacy==2.1.6\npython -m spacy download en\n```\n\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 173,
      "date": "Mon, 06 Dec 2021 07:11:07 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- Convert your dataset to  `{train,dev,test}.statement.jsonl`  in .jsonl format (see `data/csqa/statement/train.statement.jsonl`)\n- Create a directory in `data/{yourdataset}/` to store the .jsonl files\n- Modify `preprocess.py` and perform subgraph extraction for your data\n- Modify `utils/parser_utils.py` to support your own dataset\n- Tune `encoder_lr`,`decoder_lr` and other important hyperparameters, modify `utils/parser_utils.py` and `{model}.py` to record the tuned hyperparameters\n",
      "technique": "Header extraction"
    }
  ]
}