{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Research supported with Cloud TPUs from Google's TensorFlow Research Cloud (TFRC).\nThanks for providing access to the TFRC \u2764\ufe0f\n\nThanks to the generous support from the [Hugging Face](https://huggingface.co/) team,\nit is possible to download both cased and uncased models from their S3 storage \ud83e\udd17\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1810.04805",
      "https://arxiv.org/abs/2003.10555",
      "https://arxiv.org/abs/2008.02496",
      "https://arxiv.org/abs/1910.01108"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You can use the following BibTeX entry for citation:\n\n```bibtex\n@software{stefan_schweter_2020_4275044,\n  author       = {Stefan Schweter},\n  title        = {Europeana BERT and ELECTRA models},\n  month        = nov,\n  year         = 2020,\n  publisher    = {Zenodo},\n  version      = {1.0.0},\n  doi          = {10.5281/zenodo.4275044},\n  url          = {https://doi.org/10.5281/zenodo.4275044}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@software{stefan_schweter_2020_4275044,\n  author       = {Stefan Schweter},\n  title        = {Europeana BERT and ELECTRA models},\n  month        = nov,\n  year         = 2020,\n  publisher    = {Zenodo},\n  version      = {1.0.0},\n  doi          = {10.5281/zenodo.4275044},\n  url          = {https://doi.org/10.5281/zenodo.4275044}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8465055750691217
      ],
      "excerpt": "training corpus. The following figure shows a detailed overview (tokens per year): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9544316621874741
      ],
      "excerpt": "A Named Entity Recognition Shootout for German \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9544316621874741
      ],
      "excerpt": "Towards Robust Named Entity Recognition for Historic German \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "epochs: [1, 5, 10] \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9535979668456891,
        0.8488563248724261
      ],
      "excerpt": "Please star and watch Flair and Transformers \non GitHub! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "| XLM-R (base)             | 83.942 \u00b1 1.10        | 82.056 \u00b1 0.84 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8465055750691217
      ],
      "excerpt": "training corpus. The following figure shows a detailed overview (tokens per year): \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/stefan-it/europeana-bert",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "For questions about our Europeana BERT, ELECTRA and ConvBERT models just open a new discussion\n[here](https://github.com/stefan-it/europeana-bert/discussions) \ud83e\udd17\n\n",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-02-10T20:29:31Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-05T02:45:08Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8215940526866118
      ],
      "excerpt": "In this repository we open source BERT and ELECTRA models trained on various Europeana newspapers. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8004037812993859,
        0.9284822884411538,
        0.8677329943566432,
        0.9346785597638136
      ],
      "excerpt": "25.07.2021: Add fine-tuned evaluations results for various models \n06.02.2021: Public release of German Europeana DistilBERT and ConvBERT models \n16.11.2020: Public release of French Europeana BERT and ELECTRA models \n26.07.2020: Public release of German Europeana ELECTRA model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.936607716857678,
        0.9312887985999561
      ],
      "excerpt": "We trained different models, that are described in more detail in the following papers: \nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8376302037783142,
        0.954906564862107,
        0.9257861398135679,
        0.8594298574243672
      ],
      "excerpt": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter \nWe extracted all German texts using the language metadata attribute from the Europeana corpus. \nThe resulting corpus has a size of 51GB and consists of 8,035,986,369 tokens. \nBased on the metadata information, texts from the 18th - 20th century are mainly included in the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.831435839243333,
        0.8828321131695371
      ],
      "excerpt": "We use the awesome \ud83e\udd17 / Tokenizers library for building the BERT-compatible vocab (32,000 subwords). \nWe use the same preprocessing steps and training parameters as for our Turkish BERT and ELECTRA models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8688722947007944,
        0.8666460613235264,
        0.948140935132574,
        0.9629208005830164,
        0.8805558427280477,
        0.959517878630248,
        0.9478660598772953
      ],
      "excerpt": "and here for ELECTRA. \nThe ConvBERT model was trained with the reference implementation \non a v3-8 TPU with a maximum sequence length of 512 over the complete training corpus for 1M steps. \nFor the DistilBERT model, only 6GB of the original training data (51GB) was used. The model was trained for \ntwo epochs with the reference implementation \nfrom Transformers on 4 RTX 3090 with a batch size of 6. \nWe mainly compare our models against the NER dataset, that is used in the following \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8192505441185383
      ],
      "excerpt": "  by Riedl and Pad\u00f3 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8192505441185383,
        0.8829987463712271
      ],
      "excerpt": "  by Schweter and Baiter \nThe datasets used in these papers are: ONB (Austrian National Library) and LFT (Dr Friedrich Teman Library). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.88051749868113,
        0.9061850084448757,
        0.9531854539512927
      ],
      "excerpt": "We include various other BERT models (incl. XLM-R) in our comparison. Note: Schweter and Baiter used Flair \nEmbeddings (stacked with Word Embeddings) resulting in the current SOTA for these tasks. \nWe use the awesome Flair library for experiments with our Transformer-based models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.813422624387554,
        0.9121475736894357,
        0.9420578873490159,
        0.8186885795638805
      ],
      "excerpt": "HuggingFace model hub.  \nWe evaluate both feature-based and fine-tuned NER models. For the feature-based approach we use the mean over all layers from the \nTransformer model. We use an initial learning rate of 0.1. Then we reduce the learning rate by a factor of 0.5 with a patience of \n3. This factor determines the number of epochs with no improvement after which learning rate will be reduced. We use a batch size of 16 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9139574990562449
      ],
      "excerpt": "For the fine-tuned models we perform a hyper-parameter search over: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9009994183454789
      ],
      "excerpt": "Then we choose the best hyper-parameter configuration and train 5 models with different seeds and average F1-score over these models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8392401065252779,
        0.8629079069291566,
        0.8135452958035738
      ],
      "excerpt": "for predicting and piping the output to the CoNLL-2003 evaluation script. We use flair-ner-trainer-ft.py for fine-tuning, with all \nnecessary configuration files located in the configs folder of this repository. \nPlease star and watch Flair and Transformers \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8237894363194755,
        0.9517547263821597,
        0.9113729957376281
      ],
      "excerpt": "Notice: The ONB dataset covers texts from 1710 - 1873. The Europeana training dataset only contains little data for this specific \ntime period. We are currently working on BERT models with other training data for this period! \nMost of the pre-trained models are located on the \ud83e\udd17 / model hub: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9686887569742365,
        0.9257861398135679,
        0.8594298574243672
      ],
      "excerpt": "We also extracted all French texts using the language metadata attribute from the Europeana corpus. \nThe resulting corpus has a size of 63GB and consists of 11,052,528,456 tokens. \nBased on the metadata information, texts from the 18th - 20th century are mainly included in the \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.831435839243333,
        0.8828321131695371
      ],
      "excerpt": "We use the awesome \ud83e\udd17 / Tokenizers library for building the BERT-compatible vocab (32,000 subwords). \nWe use the same preprocessing steps and training parameters as for our Turkish BERT and ELECTRA models. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8688722947007944,
        0.9169864260610165,
        0.8053741780291633
      ],
      "excerpt": "and here for ELECTRA. \nBoth BERT and ELECTRA model weights for PyTorch and TensorFlow are available. \nFrench Europeana BERT: dbmdz/bert-base-french-europeana-cased - model hub page \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "BERT and ELECTRA models trained on Europeana Newspapers",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/stefan-it/europeana-bert/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Tue, 07 Dec 2021 17:06:39 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/stefan-it/europeana-bert/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "stefan-it/europeana-bert",
    "technique": "GitHub API"
  },
  "identifier": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "https://doi.org/10.5281/zenodo.4275044",
      "technique": "Regular expression"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/stefan-it/europeana-bert/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 Stefan Schweter\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Europeana BERT and ELECTRA models",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "europeana-bert",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "stefan-it",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/stefan-it/europeana-bert/blob/main/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "stefan-it",
        "body": "# Changelog\r\n\r\n* 16.11.2020: Public release of French Europeana BERT and ELECTRA models\r\n* 26.07.2020: Public release of German Europeana ELECTRA model\r\n* 10.02.2020: Initial version of this repo",
        "dateCreated": "2020-11-16T00:54:46Z",
        "datePublished": "2020-11-16T00:58:22Z",
        "html_url": "https://github.com/stefan-it/europeana-bert/releases/tag/1.0.0",
        "name": "First release of Europeana BERT and ELECTRA models",
        "tag_name": "1.0.0",
        "tarball_url": "https://api.github.com/repos/stefan-it/europeana-bert/tarball/1.0.0",
        "url": "https://api.github.com/repos/stefan-it/europeana-bert/releases/33982637",
        "zipball_url": "https://api.github.com/repos/stefan-it/europeana-bert/zipball/1.0.0"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 27,
      "date": "Tue, 07 Dec 2021 17:06:39 GMT"
    },
    "technique": "GitHub API"
  }
}