{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1512.04906"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/AshwinDeshpande96/Hierarchical-Softmax",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-06-01T14:37:29Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-25T11:19:29Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "In Neural Network Language Models(NNLM) with huge number of words in vocabulary, exhaustive activation functions such as Softmax are very slow.  This paper addresses shortcomings of Softmax. It consists of mainly two ideas\n1. Representing words as low-dimensional feature vectors - to learn relation between words and contexts.\n2. Clustering similar words in similar components(subtree) using the feature vectors.\n\nFollowing is the summary of the Hierarchical Log-Bilinear Model. (If this explanation doesn't summarise the content please go to Section 4 in the Paper)\n* Initially start with a random binary tree. With words as leaf.\n* Use log-bilinear model to fit training data. \n  * Input will be context: w<sub>1</sub>,w<sub>2</sub>,...,w<sub>n-1</sub>. \n    * Each word w is represented by a feature vector r<sub>w<sub>1</sub></sub>. Say shape (1,100) each.\n    * So input at each forward pass will be (n-1, 1, 100)\n  * Hidden Layers apply matrix transformations, with weights C\n<p align='center'>\n<img src='https://github.com/AshwinDeshpande96/Hierarchical-Softmax/blob/master/Screenshot%202019-06-05%20at%208.38.05%20PM.png' width=150>\n</p>\n  \n  * Output will be w<sub>n</sub>\n    * Will be a predicted feature vector r_hat\n    * So output shape at each forward pass will be (1,100)\n      * If there are 8 words in vocabulary (output classes)(Fig-1)\n        * Each of q<sub>i</sub> are multiplied with output r_hat and activated using sigmoid. Gives the probability of decision going to left subtree. \n        <p align='center'> P(d<sub>i</sub> = 1): sigmoid( r_hat * q<sub>i</sub> + b<sub>i</sub>) </p>\n        \n        * Each leaf is scored according to it's decision code. For example: \n          * leaf_5: P(d<sub>1</sub>=0, d<sub>3</sub>=1, d<sub>6</sub>=1)\n          * leaf_3: P(d<sub>1</sub>=1, d<sub>2</sub>=0, d<sub>5</sub>=1)\n* Fit the model with given data\n  * This is a teacher-forcing type of model, output at time step t is used at the next step t+1.\n  * This creates feature vectors r_hat depending on the context as we train.\n<p align='center'>\n<img src='https://github.com/AshwinDeshpande96/Hierarchical-Softmax/blob/master/tree.png'>\n</p>\n\nTask specific feature vector perform well in every NLP task, because the tailored feature vector represent the training data well: [Deep contextualized word representations](https://arxiv.org/pdf/1802.05365.pdf)\n\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8814209147919718,
        0.8867981164216421,
        0.8783641542305063,
        0.9946583575685551,
        0.9736894025622997,
        0.8919865991708649
      ],
      "excerpt": "This is a scalable hierarchical softmax layer for Neural Networks with large output classes. \nIn our previous project Next-Word Prediction: Next-Word Prediction \nthere was an issue of large vocabulary. There was a bottleneck at the Softmax Layer due to the large number of output classes. \nSince softmax is an exhaustive method of calculating probabilities distribution across the output classes, it scales poorly with growing size of the vocabulary. Softmax needs a vector that produces scores for each class. This is done only to facilitate the Softmax method. That is, the vector need not be as long as the size of vocabulary. In order to obtain this size, even a smaller feature vector(vector which represents the word's context - these are normally of size 256 or less) is scaled up to meet softmax requirements. A huge amount of parameters are created in the final fully connected layer. They are usually of the scale (feature_vector_size * vocabulary_size). For example, a vocabulary of size only 5000 will need 256 * 5000 parameters, i.e. 1280000 parameters only a single layer. This usually makes up for more than half of the total parameters \nGiven that feature vector is already obtained in the previous layer we needn't scale it up to another vector. In order to solve this issue we employed methods described in this project. \nIn the paper Strategies for Training Large Vocabulary Neural Language Models few solutions are proposed: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9378776102446339,
        0.970193986327457,
        0.9911611573885641,
        0.9420771984888655,
        0.8477456740652259
      ],
      "excerpt": "Hierarchial Softmax has great scalibility features. This is not a softmax method, i.e. it does not produce normalized exponentiated probabilities. We will see the proposed method as follows: \nThis project builds on the idea in Geoff Hinton's paper: A Scalable Hierarchical Distributed Language Model - Lecture \nWe devise a method where the control statements involved in fetching subset of nodes from a hierarchical binary tree are converted into direct matrix multiplication. This is done because in libraries such as keras the operations in forward-function needs to be differentiable. Hence, all the operations are matrix manipulation type that have gradients defined in keras which allows for back-propagation. \nThe method is as follows: \nGiven a vocabulary of words V,  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9814411169138492
      ],
      "excerpt": "Input r_hat given to this function is an array of Word Feature Vector. In our case the vector is of shape (1,100) (Fig-2) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8830874266237941
      ],
      "excerpt": "Eq-2 is executed as described in following steps: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9621034726375632
      ],
      "excerpt": "Step-1: Here q<sub>i</sub> is a vector of shape (100, 1). Hence, a matrix of shape node_vector = (|V|-1, 100, 1)(Fig-3) is created for q<sub>i</sub> where i = 1 to  |V|-1. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9838255732852941,
        0.9834837349440465
      ],
      "excerpt": "  Next Operation: q<sub>i</sub> x r_hat for all i = 1 to |V|-1.  \nnode_vector consists of q<sub>i</sub> for all i = 1 to |V|-1. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9521449084318641,
        0.8532671840899997
      ],
      "excerpt": "Step-3: Next operation consists of producing the P(d<sub>i</sub>=1) - Probability of decision from node q<sub>i</sub> to take left child route. \nd<sub>i</sub> = sigmoid(node_vector) produces a (|V|-1, 1) matrix(Fig-5), each value consisting of the probability of choosing left child. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9768831085098477
      ],
      "excerpt": "Step-4: Previous step produces probability of the decision to take left-child decision in every node in the tree. But the path from root to one particular leaf will consist of only subset of all |V|-1 nodes. Length of this path will be less than or equal to height h of the tree.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.945193573414023
      ],
      "excerpt": "Hence a matrix is created with |V| rows, and each row consists |V|-1 columns. Column values is either of three values values (1, -1, 0) signifying left-child, right-child or not-in-path. This matrix is called decision_matrix. This is a sparse matrix of shape: (|V|, |V|-1). (Fig-6) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.88740051704755,
        0.9676154605121077
      ],
      "excerpt": "Step-5: Since we have decisions for every leaf, we can now obtain the probabilities associated with those decisions.  \nFirst step is to negate P(d_i=1)(Left-decision probabilities) in order to get '-P(d_i = 1)' part of P(d_i = 0) = (1 - P(d_i = 1)) => (Right-decision probabilities). This produced by row-wise multiplication of left-child probabilities and decision matrix. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9235179386482187,
        0.9794946131206445,
        0.9027455493830051
      ],
      "excerpt": "Second step is to obtain 1 P(d_i=0) = 1 + (-P(d_i = 1)) and 0 in P(d_i=0) = 0 + P(d_i=1). \nFor this purpose base is a matrix with value 1 in the location where node in intermed_path_prob is negative(right child) and 0 for positive(left child). (Fig-7(b))  \nNote: base will consist of 1 in places where node is not present in path of that path. This does not mean that node has 100% probability, it is a minor correction to obtain the multplication of d_i(Eq-2) along say: leaf_x: [0.1, 0.2, 0.3, 0, 0, 0]. this will obtain a P(leaf_x) = 0, hence it converted to [0.1, 0.2, 0.3, 1, 1, 1] to obtain P(leaf_x) = 0.0006 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9225526107419801,
        0.9394473433038069,
        0.9114311565333368
      ],
      "excerpt": "We do this in either of two ways: \nreduce_prod function from tensorflow multiplies all the node probabilities of d<sub>i</sub> of each row(leaf or word). \nThis method gives a constant computation time of O(lg|V|). This operation reduces corrected_probs (shape: (|V|, |V|-1)) to the final probabilities (shape: (|V|, 1)).(Fig-8) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9568738760323298
      ],
      "excerpt": "Taking idea of negative log-likelihood wherein multiplication is replaced with summation, because multiplication incurs more comptutation costs than sum, this method was proposed to minimize repetitive multiply by one.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8964587263618436
      ],
      "excerpt": "Logarithm of corrected_probs: First Row(D) of that operation: <p align='center'> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9415551894799642
      ],
      "excerpt": "Both methods result in the same probability (Fig-8), but log method has a disadvantage of losing information in logarithm and exponent function. Even though the probability prediction by both method is correct (Verification: Sum all values to obtain 1.0 - 100% Probability accross all possible output classes), floating point precision is reduced to limited decimal places (rounded off) in the log and exp operation as compared to Reduce Product method which keeps all less important decimal digits. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9522027199710354
      ],
      "excerpt": "5   x1 = tf.math.reduce_sum(x1, axis=1)               #reduce_prod is replaced by reduce_sum \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9873102277081697,
        0.9531509862682904,
        0.8843426180755174
      ],
      "excerpt": "In order to test scalability we do not integrate Hierarchical-Softmax algorithm into a language model. Since probability distribution is calculated at the end of a neural network. We need only test the computational cost that is incurred in the output layer. The layers preceding the output layer of a language model, incur same delay for either Softmax or Hierarchical Softmax. Time taken to calculate probability distribution among the |V| classes remain independent of the predicted feature vector-r_hat, given the size of the feature vector remains unchanged.  \nHence we simulate a condition where a feature vector of  shape (1, |V|-1) is randomly generated every iteration. \n* Simulated Word Vector r_hat is generated once for each Vocabulary Size. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9935321768020979
      ],
      "excerpt": "  * 16000-18000 is the asymptotic limit for memory of 12GB. This algorithm is limited to a vocabulary of size 18,000 due to the usage decision matrix. Decision matrix consumes memory of the size |V| * (|V|-1), which is in quadratic order: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8150771532802734,
        0.9656190504898785,
        0.9158430885239212,
        0.9959348899101462,
        0.9401477690937277,
        0.955531668269298
      ],
      "excerpt": "    * Softmax Method: O(|V|) \nAs significant as the speed-ups are it is limited to available memory, hence optimum solution will be a trade-off. Availability of every decision is a major catalyst in estimating node probabilities. Depending on available memory we can partially use decision matrix and partially calculate the decision paths at runtime. \n* A vector of shape (1, |V|-1) is generated 5 times each iteration and used for both algorithms sequentially. \nInitial vocabulary size is 1000 and not lower as the performance of hierarchical structure is best evaluated for larger data sizes. While this algorithms performs well at lower sizes, it's scalability is best judged when the data size is increased dramatically. When the vocabulary size increases from 5k to 10k, i.e. it is double the time taken remains almost constant. This is due to the nature of the logarithmic asymptote, where the time taken may increase at lower vocabulary sizes, but plateaus eventually. \nWe see significant difference in the computational cost between the softmax and hierarchical softmax model. \nFollowing is the asymptotic relation with respected to increasing vocabulary size |V|. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.868555991296245
      ],
      "excerpt": "This is reflected very closely in run-time measurements. From Fig-10 we can see that Hierarchical-Softmax time remains almost constant while Softmax time increases linearly. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9985634740031315,
        0.9817256781813324,
        0.8044318269564356
      ],
      "excerpt": "Hierarchial Softmax has been proven to reduce computation time, although at the cost of some accuracy the speed-up is substantial. Our matrix method of implementation contributes easy to use and efficient interface to hierarchical softmax architecture. The decision_matrix and base are created when the model is built and not while training/testing, therefore is majority of computation is performed only once, irrespective of the EPOCH length. Command: tree = Tree(|V|) consists of everything from creating trees, paths, decision, decision_matrix & base. It is further easily accessible by simply using tree object: tree.decision & tree.base returns computed matrices. \nIn future, we shall try to reduce memory costs to an order of O(|V|*log|V|) and integrate this method in keras library for easy implementation. User shall be able to use this algorithm as they would with traditional Softmax algorithm. \nAn investigation of negative sampling in contrast with hierarchical softmax will be published soon. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "This is a scalable hierarchical softmax layer for Neural Networks with large output classes.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/AshwinDeshpande96/Hierarchical-Softmax/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Wed, 08 Dec 2021 09:56:35 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/AshwinDeshpande96/Hierarchical-Softmax/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "AshwinDeshpande96/Hierarchical-Softmax",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/AshwinDeshpande96/Hierarchical-Softmax/master/hsm.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.8636282509986071
      ],
      "excerpt": "<img src='https://github.com/AshwinDeshpande96/Hierarchical-Softmax/blob/master/matrix_decisions.gif' width=560> \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8546541138625192
      ],
      "excerpt": "2   x1 = tf.multiply(tree.decision_matrix, inp) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8463579979869013
      ],
      "excerpt": "4   return tf.math.reduce_prod(x1, axis=1) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8735773538610254
      ],
      "excerpt": "2   x1 = tf.multiply(tree.decision_matrix, input) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8462906430960053,
        0.8123763140827432
      ],
      "excerpt": "    e = tf.math.exp(input) \n    s = tf.math.reduce_sum(e) \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/AshwinDeshpande96/Hierarchical-Softmax/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Hierarchical-Softmax",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Hierarchical-Softmax",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "AshwinDeshpande96",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/AshwinDeshpande96/Hierarchical-Softmax/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 8,
      "date": "Wed, 08 Dec 2021 09:56:35 GMT"
    },
    "technique": "GitHub API"
  }
}