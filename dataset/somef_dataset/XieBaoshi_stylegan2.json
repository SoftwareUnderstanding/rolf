{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We thank David Luebke for helpful comments; Tero Kuosmanen and Sabu Nadarajan for their support with compute infrastructure; and Edgar Sch&ouml;nfeld for guidance on setting up unconditional BigGAN.\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2006.06676<br>\n\nAbstract: *Training generative adversarial networks (GAN",
      "https://arxiv.org/abs/1706.08500",
      "https://arxiv.org/abs/1801.01401",
      "https://arxiv.org/abs/1904.06991",
      "https://arxiv.org/abs/1606.03498",
      "https://arxiv.org/abs/1812.04948"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\n@inproceedings{Karras2020ada,\n  title     = {Training Generative Adversarial Networks with Limited Data},\n  author    = {Tero Karras and Miika Aittala and Janne Hellsten and Samuli Laine and Jaakko Lehtinen and Timo Aila},\n  booktitle = {Proc. NeurIPS},\n  year      = {2020}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{Karras2020ada,\n  title     = {Training Generative Adversarial Networks with Limited Data},\n  author    = {Tero Karras and Miika Aittala and Janne Hellsten and Samuli Laine and Jaakko Lehtinen and Timo Aila},\n  booktitle = {Proc. NeurIPS},\n  year      = {2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8334710283794773,
        0.8753150968738145
      ],
      "excerpt": "For business inquiries, please contact researchinquiries@nvidia.com<br> \nFor press and other inquiries, please contact Hector Marinez at hmarinez@nvidia.com<br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8955886365383559
      ],
      "excerpt": "| &ensp;&ensp;&boxvr;&nbsp; ada-paper.pdf | Paper PDF \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9031911419830267
      ],
      "excerpt": "| &ensp;&ensp;&ensp;&ensp;&boxvr;&nbsp; cifar10.pkl | Class-conditional CIFAR-10 at 32x32 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8097548321964125,
        0.8097548321964125
      ],
      "excerpt": "| paper256            | Reproduce results for FFHQ and LSUN Cat at 256x256 using 1, 2, 4, or 8 GPUs. \n| paper512            | Reproduce results for BreCaHAD and AFHQ at 512x512 using 1, 2, 4, or 8 GPUs. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9185642879126944
      ],
      "excerpt": "| cifar               | Reproduce results for CIFAR-10 (tuned configuration) using 1 or 2 GPUs. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9299953890831135
      ],
      "excerpt": "--resume=ffhq1024 --snap=10 performs transfer learning from FFHQ trained at 1024x1024. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9156566588472104
      ],
      "excerpt": "| 256x256    | 2    | 3h 27m    | 3d 14h     | 11.2&ndash;11.8   | 5.2 GB  | 9.0 GB \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "| 512x512    | 2    | 10h 59m   | 11d 10h    | 37.7&ndash;40.0   | 7.8 GB  | 9.8 GB \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8388977297732308
      ],
      "excerpt": "| is50k       | 13 min | 1.8 GB  | Inception score<sup>[4]</sup> for CIFAR-10 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9965033545716006,
        0.9690758647352964,
        0.9947945285377136,
        0.9876119727098102
      ],
      "excerpt": "2. Demystifying MMD GANs, Bi&nacute;kowski et al. 2018 \n3. Improved Precision and Recall Metric for Assessing Generative Models, Kynk&auml;&auml;nniemi et al. 2019 \n4. Improved Techniques for Training GANs, Salimans et al. 2016 \n5. A Style-Based Generator Architecture for Generative Adversarial Networks, Karras et al. 2018 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/XieBaoshi/stylegan2",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-04-24T02:11:53Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-08-24T00:14:44Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9801570830273105
      ],
      "excerpt": "Abstract: Training generative adversarial networks (GAN) using too little data typically leads to discriminator overfitting, causing training to diverge. We propose an adaptive discriminator augmentation mechanism that significantly stabilizes training in limited data regimes. The approach does not require changes to loss functions or network architectures, and is applicable both when training from scratch and when fine-tuning an existing GAN on another dataset. We demonstrate, on several datasets, that good results are now possible using only a few thousand training images, often matching StyleGAN2 results with an order of magnitude fewer images. We expect this to open up new application domains for GANs. We also find that the widely used CIFAR-10 is, in fact, a limited data benchmark, and improve the record FID from 5.59 to 2.42. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.997611355845477
      ],
      "excerpt": "This repository is a faithful reimplementation of StyleGAN2-ADA in PyTorch, focusing on correctness, performance, and compatibility. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8662648011562245
      ],
      "excerpt": "* Full support for all primary training configurations. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9205326420201236
      ],
      "excerpt": "* Results are expected to match in all cases, excluding the effects of pseudo-random numbers and floating-point arithmetic. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8039647491864648,
        0.9430014666314204
      ],
      "excerpt": "* Training is typically 5%&ndash;30% faster compared to the TensorFlow version on NVIDIA Tesla V100 GPUs. \n* Inference is up to 35% faster in high resolutions, but it may be slightly slower in low resolutions. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8930802407964711
      ],
      "excerpt": "* Faster startup time when training new networks (<50s), and also when using pre-trained networks (<4s). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9018749004816563,
        0.8876796744071969,
        0.8617752084142242,
        0.8710901488942233
      ],
      "excerpt": "* Compatible with old network pickles created using the TensorFlow version. \n* New ZIP/PNG based dataset format for maximal interoperability with existing 3rd party tools. \n* TFRecords datasets are no longer supported &mdash; they need to be converted to the new format. \n* New JSON-based format for logs, metrics, and training curves. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8747712870482227
      ],
      "excerpt": "| &ensp;&ensp;&ensp;&ensp;&boxvr;&nbsp; paper-fig11a-small-datasets | Models used in Fig.11a (small datasets & transfer learning) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8079246108668227
      ],
      "excerpt": "| &ensp;&ensp;&ensp;&ensp;&boxvr;&nbsp; transfer-learning-source-nets | Models used as starting point for transfer learning \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9737362881168427
      ],
      "excerpt": "The pickle contains three networks. 'G' and 'D' are instantaneous snapshots taken during training, and 'G_ema' represents a moving average of the generator weights over several training steps. The networks are regular instances of torch.nn.Module, with all of their parameters and buffers placed on the CPU at import and gradient computation disabled by default. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9278876628356257
      ],
      "excerpt": "| auto&nbsp;(default) | Automatically select reasonable defaults based on resolution and GPU count. Serves as a good starting point for new datasets but does not necessarily lead to optimal results. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9189627271442637,
        0.9189627271442637,
        0.8814547115169322,
        0.815731966784463
      ],
      "excerpt": "| paper256            | Reproduce results for FFHQ and LSUN Cat at 256x256 using 1, 2, 4, or 8 GPUs. \n| paper512            | Reproduce results for BreCaHAD and AFHQ at 512x512 using 1, 2, 4, or 8 GPUs. \n| paper1024           | Reproduce results for MetFaces at 1024x1024 using 1, 2, 4, or 8 GPUs. \n| cifar               | Reproduce results for CIFAR-10 (tuned configuration) using 1 or 2 GPUs. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.842562674286682
      ],
      "excerpt": "--mirror=1 amplifies the dataset with x-flips. Often beneficial, even with ADA. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8137279291327043
      ],
      "excerpt": "--augpipe=blit enables pixel blitting but disables all other augmentations. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9174102447425374
      ],
      "excerpt": "The total training time depends heavily on resolution, number of GPUs, dataset, desired quality, and hyperparameters. The following table lists expected wallclock times to reach different points in the training, measured in thousands of real images shown to the discriminator (\"kimg\"): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9196241412598118
      ],
      "excerpt": "| 256x256    | 2    | 3h 27m    | 3d 14h     | 11.2&ndash;11.8   | 5.2 GB  | 9.0 GB \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8638606115659256,
        0.9876033484382517,
        0.8195333936195855
      ],
      "excerpt": "The above measurements were done using NVIDIA Tesla V100 GPUs with default settings (--cfg=auto --aug=ada --metrics=fid50k_full). \"sec/kimg\" shows the expected range of variation in raw training performance, as reported in log.txt. \"GPU mem\" and \"CPU mem\" show the highest observed memory consumption, excluding the peak at the beginning caused by torch.backends.cudnn.benchmark. \nIn typical cases, 25000 kimg or more is needed to reach convergence, but the results are already quite reasonable around 5000 kimg. 1000 kimg is often enough for transfer learning, which tends to converge significantly faster. The following figure shows example convergence curves for different datasets as a function of wallclock time, using the same settings as above: \nNote: --cfg=auto serves as a reasonable first guess for the hyperparameters but it does not necessarily lead to optimal results for a given dataset. For example, --cfg=stylegan2 yields considerably better FID  for FFHQ-140k at 1024x1024 than illustrated above. We recommend trying out at least a few different values of --gamma for each new dataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9412368228618466,
        0.8526336067083691
      ],
      "excerpt": "Note that many of the metrics have a significant one-off cost when calculating them for the first time for a new dataset (up to 30min). Also note that the evaluation is done using a different random seed each time, so the results will vary if the same metric is computed multiple times. \nWe employ the following metrics in the ADA paper. Execution time and GPU memory usage is reported for one NVIDIA Tesla V100 GPU at 1024x1024 resolution: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9154570512161236
      ],
      "excerpt": "In addition, the following metrics from the StyleGAN and StyleGAN2 papers are also supported: \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/XieBaoshi/stylegan2/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sun, 12 Dec 2021 20:12:07 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/XieBaoshi/stylegan2/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "XieBaoshi/stylegan2",
    "technique": "GitHub API"
  },
  "hasBuildFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/XieBaoshi/stylegan2/main/Dockerfile"
    ],
    "technique": "File Exploration"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/XieBaoshi/stylegan2/tree/main/docs"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/XieBaoshi/stylegan2/main/docker_run.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Datasets are stored as uncompressed ZIP archives containing uncompressed PNG files and a metadata file `dataset.json` for labels.\n\nCustom datasets can be created from a folder containing images; see [`python dataset_tool.py --help`](./docs/dataset-tool-help.txt) for more information. Alternatively, the folder can also be used directly as a dataset, without running it through `dataset_tool.py` first, but doing so may lead to suboptimal performance.\n\nLegacy TFRecords datasets are not supported &mdash; see below for instructions on how to convert them.\n\n**FFHQ**:\n\nStep 1: Download the [Flickr-Faces-HQ dataset](https://github.com/NVlabs/ffhq-dataset) as TFRecords.\n\nStep 2: Extract images from TFRecords using `dataset_tool.py` from the [TensorFlow version of StyleGAN2-ADA](https://github.com/NVlabs/stylegan2-ada/):\n\n```.bash\n#: Using dataset_tool.py from TensorFlow version at\n#: https://github.com/NVlabs/stylegan2-ada/\npython ../stylegan2-ada/dataset_tool.py unpack \\\n    --tfrecord_dir=~/ffhq-dataset/tfrecords/ffhq --output_dir=/tmp/ffhq-unpacked\n```\n\nStep 3: Create ZIP archive using `dataset_tool.py` from this repository:\n\n```.bash\n#: Original 1024x1024 resolution.\npython dataset_tool.py --source=/tmp/ffhq-unpacked --dest=~/datasets/ffhq.zip\n\n#: Scaled down 256x256 resolution.\npython dataset_tool.py --source=/tmp/ffhq-unpacked --dest=~/datasets/ffhq256x256.zip \\\n    --width=256 --height=256\n```\n\n**MetFaces**: Download the [MetFaces dataset](https://github.com/NVlabs/metfaces-dataset) and create ZIP archive:\n\n```.bash\npython dataset_tool.py --source=~/downloads/metfaces/images --dest=~/datasets/metfaces.zip\n```\n\n**AFHQ**: Download the [AFHQ dataset](https://github.com/clovaai/stargan-v2/blob/master/README.md#animal-faces-hq-dataset-afhq) and create ZIP archive:\n\n```.bash\npython dataset_tool.py --source=~/downloads/afhq/train/cat --dest=~/datasets/afhqcat.zip\npython dataset_tool.py --source=~/downloads/afhq/train/dog --dest=~/datasets/afhqdog.zip\npython dataset_tool.py --source=~/downloads/afhq/train/wild --dest=~/datasets/afhqwild.zip\n```\n\n**CIFAR-10**: Download the [CIFAR-10 python version](https://www.cs.toronto.edu/~kriz/cifar.html) and convert to ZIP archive:\n\n```.bash\npython dataset_tool.py --source=~/downloads/cifar-10-python.tar.gz --dest=~/datasets/cifar10.zip\n```\n\n**LSUN**: Download the desired categories from the [LSUN project page](https://www.yf.io/p/lsun/) and convert to ZIP archive:\n\n```.bash\npython dataset_tool.py --source=~/downloads/lsun/raw/cat_lmdb --dest=~/datasets/lsuncat200k.zip \\\n    --transform=center-crop --width=256 --height=256 --max_images=200000\n\npython dataset_tool.py --source=~/downloads/lsun/raw/car_lmdb --dest=~/datasets/lsuncar200k.zip \\\n    --transform=center-crop-wide --width=512 --height=384 --max_images=200000\n```\n\n**BreCaHAD**:\n\nStep 1: Download the [BreCaHAD dataset](https://figshare.com/articles/BreCaHAD_A_Dataset_for_Breast_Cancer_Histopathological_Annotation_and_Diagnosis/7379186).\n\nStep 2: Extract 512x512 resolution crops using `dataset_tool.py` from the [TensorFlow version of StyleGAN2-ADA](https://github.com/NVlabs/stylegan2-ada/):\n\n```.bash\n#: Using dataset_tool.py from TensorFlow version at\n#: https://github.com/NVlabs/stylegan2-ada/\npython dataset_tool.py extract_brecahad_crops --cropsize=512 \\\n    --output_dir=/tmp/brecahad-crops --brecahad_dir=~/downloads/brecahad/images\n```\n\nStep 3: Create ZIP archive using `dataset_tool.py` from this repository:\n\n```.bash\npython dataset_tool.py --source=/tmp/brecahad-crops --dest=~/datasets/brecahad.zip\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9154614660996443
      ],
      "excerpt": "* GPU memory usage is comparable to the TensorFlow version. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8092009450994657
      ],
      "excerpt": "| stylegan2-ada-pytorch | Main directory hosted on Amazon S3 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.877900701141351
      ],
      "excerpt": "    --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.877900701141351
      ],
      "excerpt": "    --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8531443544452013
      ],
      "excerpt": "The above code requires torch_utils and dnnlib to be accessible via PYTHONPATH. It does not need source code for the networks themselves &mdash; their class definitions are loaded from the pickle via torch_utils.persistence. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8234586846683971
      ],
      "excerpt": "The name of the output directory reflects the training configuration. For example, 00000-mydataset-auto1 indicates that the base configuration was auto1, meaning that the hyperparameters were selected automatically for training on one GPU. The base configuration is controlled by --cfg: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.877900701141351
      ],
      "excerpt": "    --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.847143285301206
      ],
      "excerpt": "We employ the following metrics in the ADA paper. Execution time and GPU memory usage is reported for one NVIDIA Tesla V100 GPU at 1024x1024 resolution: \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8072039951643566
      ],
      "excerpt": "* New command line options for tweaking the training performance. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8646955063499943
      ],
      "excerpt": "| &ensp;&ensp;&boxvr;&nbsp; images | Curated example images produced using the pre-trained models \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8321391265386955
      ],
      "excerpt": "| &ensp;&ensp;&boxur;&nbsp; pretrained | Pre-trained models \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8658668257399048
      ],
      "excerpt": "python projector.py --outdir=out --target=~/mytargetimg.png \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8585039823775281
      ],
      "excerpt": "python generate.py --outdir=out --projected_w=out/projected_w.npz \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8184161576486038,
        0.8716228293547827
      ],
      "excerpt": "img = G.synthesis(w, noise_mode='const', force_fp32=True) \nPlease refer to generate.py, style_mixing.py, and projector.py for further examples. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9624287028644266,
        0.9545465483863659
      ],
      "excerpt": "python train.py --outdir=~/training-runs --data=~/mydataset.zip --gpus=1 --dry-run \npython train.py --outdir=~/training-runs --data=~/mydataset.zip --gpus=1 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8012268748760277
      ],
      "excerpt": "By default, train.py automatically computes FID for each network pickle exported during training. We recommend inspecting metric-fid50k_full.jsonl (or TensorBoard) at regular intervals to monitor the training progress. When desired, the automatic computation can be disabled with --metrics=none to speed up the training slightly (3%&ndash;9%). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8872848671472333
      ],
      "excerpt": ": Previous training run: look up options automatically, save result to JSONL file. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8625018385506985,
        0.8874318414634792
      ],
      "excerpt": ": Pre-trained network pickle: specify dataset explicitly, print result to stdout. \npython calc_metrics.py --metrics=fid50k_full --data=~/datasets/ffhq.zip --mirror=1 \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8454319600012077
      ],
      "excerpt": "The first example looks up the training configuration and performs the same operation as if --metrics=pr50k3_full had been specified during training. The second example downloads a pre-trained network pickle, in which case the values of --mirror and --data must be specified explicitly. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/XieBaoshi/stylegan2/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Cuda",
      "C++",
      "Shell",
      "Dockerfile"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other"
    },
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# StyleGAN2-ADA &mdash; Official PyTorch implementation",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "stylegan2",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "XieBaoshi",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/XieBaoshi/stylegan2/blob/main/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* Linux and Windows are supported, but we recommend Linux for performance and compatibility reasons.\n* 1&ndash;8 high-end NVIDIA GPUs with at least 12 GB of memory. We have done all testing and development using NVIDIA DGX-1 with 8 Tesla V100 GPUs.\n* 64-bit Python 3.7 and PyTorch 1.7.1. See [https://pytorch.org/](https://pytorch.org/) for PyTorch install instructions.\n* CUDA toolkit 11.0 or later.  Use at least version 11.1 if running on RTX 3090.  (Why is a separate CUDA toolkit installation required?  See comments in [#2](https://github.com/NVlabs/stylegan2-ada-pytorch/issues/2#issuecomment-779457121).)\n* Python libraries: `pip install click requests tqdm pyspng ninja imageio-ffmpeg==0.4.3`.  We use the Anaconda3 2020.11 distribution which installs most of these by default.\n* Docker users: use the [provided Dockerfile](./Dockerfile) to build an image with the required library dependencies.\n\nThe code relies heavily on custom PyTorch extensions that are compiled on the fly using NVCC. On Windows, the compilation requires Microsoft Visual Studio. We recommend installing [Visual Studio Community Edition](https://visualstudio.microsoft.com/vs/) and adding it into `PATH` using `\"C:\\Program Files (x86)\\Microsoft Visual Studio\\<VERSION>\\Community\\VC\\Auxiliary\\Build\\vcvars64.bat\"`.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sun, 12 Dec 2021 20:12:07 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Pre-trained networks are stored as `*.pkl` files that can be referenced using local filenames or URLs:\n\n```.bash\n#: Generate curated MetFaces images without truncation (Fig.10 left)\npython generate.py --outdir=out --trunc=1 --seeds=85,265,297,849 \\\n    --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl\n\n#: Generate uncurated MetFaces images with truncation (Fig.12 upper left)\npython generate.py --outdir=out --trunc=0.7 --seeds=600-605 \\\n    --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl\n\n#: Generate class conditional CIFAR-10 images (Fig.17 left, Car)\npython generate.py --outdir=out --seeds=0-35 --class=1 \\\n    --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/cifar10.pkl\n\n#: Style mixing example\npython style_mixing.py --outdir=out --rows=85,100,75,458,1500 --cols=55,821,1789,293 \\\n    --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl\n```\n\nOutputs from the above commands are placed under `out/*.png`, controlled by `--outdir`. Downloaded network pickles are cached under `$HOME/.cache/dnnlib`, which can be overridden by setting the `DNNLIB_CACHE_DIR` environment variable. The default PyTorch extension build directory is `$HOME/.cache/torch_extensions`, which can be overridden by setting `TORCH_EXTENSIONS_DIR`.\n\n**Docker**: You can run the above curated image example using Docker as follows:\n\n```.bash\ndocker build --tag sg2ada:latest .\n./docker_run.sh python3 generate.py --outdir=out --trunc=1 --seeds=85,265,297,849 \\\n    --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl\n```\n\nNote: The Docker image requires NVIDIA driver release `r455.23` or later.\n\n**Legacy networks**: The above commands can load most of the network pickles created using the previous TensorFlow versions of StyleGAN2 and StyleGAN2-ADA. However, for future compatibility, we recommend converting such legacy pickles into the new format used by the PyTorch version:\n\n```.bash\npython legacy.py \\\n    --source=https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-cat-config-f.pkl \\\n    --dest=stylegan2-cat-config-f.pkl\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}