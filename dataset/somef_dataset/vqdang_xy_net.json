{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2003.10778"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If any part of this code is used, please give appropriate citation to our paper. <br />\n\nBibTex entry: <br />\n```\n@article{graham2019hover,\n  title={Hover-net: Simultaneous segmentation and classification of nuclei in multi-tissue histology images},\n  author={Graham, Simon and Vu, Quoc Dang and Raza, Shan E Ahmed and Azam, Ayesha and Tsang, Yee Wah and Kwak, Jin Tae and Rajpoot, Nasir},\n  journal={Medical Image Analysis},\n  pages={101563},\n  year={2019},\n  publisher={Elsevier}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{graham2019hover,\n  title={Hover-net: Simultaneous segmentation and classification of nuclei in multi-tissue histology images},\n  author={Graham, Simon and Vu, Quoc Dang and Raza, Shan E Ahmed and Azam, Ayesha and Tsang, Yee Wah and Kwak, Jin Tae and Rajpoot, Nasir},\n  journal={Medical Image Analysis},\n  pages={101563},\n  year={2019},\n  publisher={Elsevier}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8098503068372033
      ],
      "excerpt": "Link to Medical Image Analysis paper. <br /> \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/vqdang/hover_net",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-12-06T11:29:48Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-08T05:09:45Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9766207636445898,
        0.8974917129742309,
        0.9971957671435296
      ],
      "excerpt": "A multiple branch network that performs nuclear instance segmentation and classification within a single network. The network leverages the horizontal and vertical distances of nuclear pixels to their centres of mass to separate clustered cells. A dedicated up-sampling branch is used to classify the nuclear type for each segmented instance. <br /> \nLink to Medical Image Analysis paper. <br /> \nThis is the official PyTorch implementation of HoVer-Net. For the original TensorFlow version of this code, please refer to this branch. The repository can be used for training HoVer-Net and to process image tiles or whole-slide images. As part of this repository, we supply model weights trained on the following datasets: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9058990887806508
      ],
      "excerpt": "Links to the checkpoints can be found in the inference description below. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.872970528765594
      ],
      "excerpt": "dataloader/: the data loader and augmentation pipeline \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8637227872179156
      ],
      "excerpt": "Below are the main executable scripts in the repository: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9323042099164404,
        0.9365744930136631,
        0.9623454186962558
      ],
      "excerpt": "convert_chkpt_tf2pytorch: convert tensorflow .npz model trained in original repository to pytorch supported .tar format. \nFor training, patches must be extracted using extract_patches.py. For instance segmentation, patches are stored as a 4 dimensional numpy array with channels [RGB, inst]. Here, inst is the instance segmentation ground truth. I.e pixels range from 0 to N, where 0 is background and N is the number of nuclear instances for that particular image.  \nFor simultaneous instance segmentation and classification, patches are stored as a 5 dimensional numpy array with channels [RGB, inst, type]. Here, type is the ground truth of the nuclear type. I.e every pixel ranges from 0-K, where 0 is background and K is the number of classes. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8462140786395123
      ],
      "excerpt": "- WSIs supported by OpenSlide, including svs, tif, ndpi and mrxs. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8249541471798013
      ],
      "excerpt": "    - 'type': prediction of category for each nucleus \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9114282477073753,
        0.8361765844097062,
        0.8786958660922626,
        0.8904124380743181,
        0.840868257432657
      ],
      "excerpt": "    - 'inst_map': instance map containing values from 0 to N, where N is the number of nuclei \n    - 'inst_type': list of length N containing predictions for each nucleus \n - Image tiles output a png overlay of nuclear boundaries on top of original RGB image \nModel weights obtained from training HoVer-Net as a result of the above instructions can be supplied to process input images / WSIs. Alternatively, any of the below pre-trained model weights can be used to process the data. These checkpoints were initially trained using TensorFlow and were converted using convert_chkpt_tf2pytorch.py. Provided checkpoints either are either trained for segmentation alone or for simultaneous segmentation and classification. Note, we do not provide a segmentation and classification model for CPM17 and Kumar because classification labels aren't available. \nIMPORTANT: CoNSeP, Kumar and CPM17 checkpoints use the original model mode, whereas PanNuke and MoNuSAC use the fast model mode. Refer to the inference instructions below for more information.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8237464062634241
      ],
      "excerpt": "If any of the above checkpoints are used, please ensure to cite the corresponding paper. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9689611446010602
      ],
      "excerpt": "Overlaid results of HoVer-Net trained on the CoNSeP dataset. The colour of the nuclear boundary denotes the type of nucleus. <br /> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9337397509013852,
        0.95050902800578
      ],
      "excerpt": "Ground truth files are in .mat format, refer to the README included with the datasets for further information. \nBelow we report the difference in segmentation results trained using this repository (PyTorch) and the results reported in the original manuscript (TensorFlow).  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Simultaneous Nuclear Instance Segmentation and Classification in H&E Histology Images.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/vqdang/xy_net/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 92,
      "date": "Wed, 08 Dec 2021 11:05:19 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/vqdang/hover_net/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "vqdang/hover_net",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/vqdang/xy_net/tree/master/docs"
    ],
    "technique": "File Exploration"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/vqdang/xy_net/master/examples/usage.ipynb",
      "https://raw.githubusercontent.com/vqdang/xy_net/master/examples/.ipynb_checkpoints/usage-checkpoint.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/vqdang/xy_net/master/run_wsi.sh",
      "https://raw.githubusercontent.com/vqdang/xy_net/master/run_tile.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```\nconda env create -f environment.yml\nconda activate hovernet\npip install torch==1.6.0 torchvision==0.7.0\n```\n\nAbove, we install PyTorch version 1.6 with CUDA 10.2. \n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9322609392449874
      ],
      "excerpt": "| PyTorch    | 0.8211     | 0.5904     | 0.6321    | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9322609392449874
      ],
      "excerpt": "| PyTorch    | 0.8504     | 0.5464     | 0.6009    | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9322609392449874
      ],
      "excerpt": "| PyTorch    | 0.756          | 0.636          | 0.559          | 0.557          | 0.348          | \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8315471364902568
      ],
      "excerpt": "models/: model definition, along with the main run step and hyperparameter settings   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8533161870997436
      ],
      "excerpt": "config.py: configuration file \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8628784290876165,
        0.8362088205356785,
        0.9226575682472734
      ],
      "excerpt": "extract_patches.py: extracts patches from original images \ncompute_stats.py: main metric computation script \nrun_train.py: main training script \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8871207929461882
      ],
      "excerpt": "Set path to the data directories in config.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8807602653214116
      ],
      "excerpt": "Set path to pretrained Preact-ResNet50 weights in models/hovernet/opt.py. Download the weights here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8071457352101197
      ],
      "excerpt": "- Standard images files, including png, jpg and tiff. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8289669050403863,
        0.8209895709154416
      ],
      "excerpt": "Output: <br /> \n- Both image tiles and whole-slide images output a json file with keys: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8306282302679894
      ],
      "excerpt": "- Image tiles output a mat file, with keys: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8511013135133899
      ],
      "excerpt": "Access the entire checkpoint directory, along with a README on the filename description here. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/vqdang/hover_net/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 vqdang\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "HoVer-Net: Simultaneous Segmentation and Classification of Nuclei in Multi-Tissue Histology Images",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "hover_net",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "vqdang",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/vqdang/hover_net/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 182,
      "date": "Wed, 08 Dec 2021 11:05:19 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": " \nUsage: <br />\n```\n  python run_train.py [--gpu=<id>] [--view=<dset>]\n  python run_train.py (-h | --help)\n  python run_train.py --version\n```\n\nOptions:\n```\n  -h --help       Show this string.\n  --version       Show version.\n  --gpu=<id>      Comma separated GPU list.  \n  --view=<dset>   Visualise images after augmentation. Choose 'train' or 'valid'.\n```\n\nExamples:\n\nTo visualise the training dataset as a sanity check before training use:\n```\npython run_train.py --view='train'\n```\n\nTo initialise the training script with GPUs 0 and 1, the command is:\n```\npython run_train.py --gpu='0,1' \n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Usage: <br />\n```\n  run_infer.py [options] [--help] <command> [<args>...]\n  run_infer.py --version\n  run_infer.py (-h | --help)\n```\n\nOptions:\n```\n  -h --help                   Show this string.\n  --version                   Show version.\n\n  --gpu=<id>                  GPU list. [default: 0]\n  --nr_types=<n>              Number of nuclei types to predict. [default: 0]\n  --type_info_path=<path>     Path to a json define mapping between type id, type name, \n                              and expected overlay color. [default: '']\n\n  --model_path=<path>         Path to saved checkpoint.\n  --model_mode=<mode>         Original HoVer-Net or the reduced version used in PanNuke / MoNuSAC, 'original' or 'fast'. [default: fast]\n  --nr_inference_workers=<n>  Number of workers during inference. [default: 8]\n  --nr_post_proc_workers=<n>  Number of workers during post-processing. [default: 16]\n  --batch_size=<n>            Batch size. [default: 128]\n```\n\nTile Processing Options: <br />\n```\n   --input_dir=<path>     Path to input data directory. Assumes the files are not nested within directory.\n   --output_dir=<path>    Path to output directory..\n\n   --draw_dot             To draw nuclei centroid on overlay. [default: False]\n   --save_qupath          To optionally output QuPath v0.2.3 compatible format. [default: False]\n   --save_raw_map         To save raw prediction or not. [default: False]\n```\n\nWSI Processing Options: <br />\n```\n    --input_dir=<path>      Path to input data directory. Assumes the files are not nested within directory.\n    --output_dir=<path>     Path to output directory.\n    --cache_path=<path>     Path for cache. Should be placed on SSD with at least 100GB. [default: cache]\n    --mask_dir=<path>       Path to directory containing tissue masks. \n                            Should have the same name as corresponding WSIs. [default: '']\n\n    --proc_mag=<n>          Magnification level (objective power) used for WSI processing. [default: 40]\n    --ambiguous_size=<int>  Define ambiguous region along tiling grid to perform re-post processing. [default: 128]\n    --chunk_shape=<n>       Shape of chunk for processing. [default: 10000]\n    --tile_shape=<n>        Shape of tiles for processing. [default: 2048]\n    --save_thumb            To save thumb. [default: False]\n    --save_mask             To save mask. [default: False]\n```\n\nThe above command can be used from the command line or via an executable script. We supply two example executable scripts: one for tile processing and one for WSI processing. To run the scripts, first make them executable by using `chmod +x run_tile.sh` and `chmod +x run_tile.sh`. Then run by using `./run_tile.sh` and `./run_wsi.sh`.\n\nIntermediate results are stored in cache. Therefore ensure that the specified cache location has enough space! Preferably ensure that the cache location is SSD.\n\nNote, it is important to select the correct model mode when running inference. 'original' model mode refers to the method described in the original medical image analysis paper with a 270x270 patch input and 80x80 patch output. 'fast' model mode uses a 256x256 patch input and 164x164 patch output. Model checkpoints trained on Kumar, CPM17 and CoNSeP are from our original publication and therefore the 'original' mode **must** be used. For PanNuke and MoNuSAC, the 'fast' mode **must** be selected. The model mode for each checkpoint that we provide is given in the filename. Also, if using a model trained only for segmentation, `nr_types` must be set to 0.\n\n`type_info.json` is used to specify what RGB colours are used in the overlay. Make sure to modify this for different datasets and if you would like to generally control overlay boundary colours.\n\nAs part of our tile processing implementation, we add an option to save the output in a form compatible with QuPath. \n\nTake a look on how to utilise the output in `examples/usage.ipynb`. \n\n",
      "technique": "Header extraction"
    }
  ]
}