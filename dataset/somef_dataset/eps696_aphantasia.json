{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2103.00020>"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Based on [CLIP] model by OpenAI ([paper]).  \nFFT encoding is taken from [Lucent] library, 3D depth processing made by [deKxi].\n\nThanks to [Ryan Murdock], [Jonathan Fly], [Hannu Toyryla], [@eduwatch2], [torridgristle] for ideas.\n\n<p align='center'><img src='_out/some_cute_image-FFT.jpg' /></p>\n\n[artwork]: <https://computervisionart.com/pieces2021/aphantasia>\n[Aphantasia]: <https://en.wikipedia.org/wiki/Aphantasia>\n[CLIP]: <https://openai.com/blog/clip>\n[SBERT]: <https://sbert.net>\n[Lucent]: <https://github.com/greentfrapp/lucent>\n[AdaBins]: <https://github.com/shariqfarooq123/AdaBins>\n[AdaBins model]: <https://drive.google.com/file/d/1lvyZZbC9NLcS8a__YPcUP7rDiIpbRpoF>\n[LPIPS]: <https://github.com/richzhang/PerceptualSimilarity>\n[Taming Transformers]: <https://github.com/CompVis/taming-transformers>\n[Ryan Murdock]: <https://twitter.com/advadnoun>\n[Jonathan Fly]: <https://twitter.com/jonathanfly>\n[Hannu Toyryla]: <https://twitter.com/htoyryla>\n[@eduwatch2]: <https://twitter.com/eduwatch2>\n[torridgristle]: <https://github.com/torridgristle>\n[deKxi]: <https://twitter.com/deKxi>\n[paper]: <https://arxiv.org/abs/2103.00020>\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8940583111730483
      ],
      "excerpt": "fullHD/4K resolutions and above \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8405152912215982
      ],
      "excerpt": "--invert negates the whole criteria, if you fancy checking \"totally opposite\". \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/eps696/aphantasia",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-02-28T03:12:55Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-09T22:00:36Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9767786739045392
      ],
      "excerpt": "This is a collection of text-to-image tools, evolved from the [artwork] of the same name.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8920361492105356,
        0.8228279396178251,
        0.9168894082309853
      ],
      "excerpt": "Updated: Illustrip (text-to-video with motion and depth) is added. \nUpdated: DWT (wavelets) parameterization is added. \nUpdated: Check also colabs below, with VQGAN and SIREN+FFM generators. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9554608969275651,
        0.9320403096842328,
        0.8026448274932505
      ],
      "excerpt": "[Aphantasia] is the inability to visualize mental images, the deprivation of visual dreams. \nThe image in the header is generated by the tool from this word. \nPlease be kind to mention this project, if you employ it for your masterpieces \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.908925214220865
      ],
      "excerpt": "fullHD/4K resolutions and above \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9526446688689365
      ],
      "excerpt": "depth-based 3D look (courtesy of [deKxi], based on [AdaBins]) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9461951015919234
      ],
      "excerpt": "separate text prompts for style and to subtract (avoid) topics \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8842267464273328
      ],
      "excerpt": "If --sync X argument > 0, [LPIPS] loss is added to keep the composition similar to the original image.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9165376025145731,
        0.8046028259841892
      ],
      "excerpt": "For non-English languages use either --multilang (multi-language CLIP model, trained with ViT) or --translate (Google translation, works with any visual model).  \nSet more specific query like this: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9189832641093955,
        0.8868744288817886
      ],
      "excerpt": "--dwt switches to DWT (wavelets) generator instead of FFT. There are few methods, chosen by --wave X, e.g. db2, db3, coif1, coif2, etc. \n--align XX option is about composition (or sampling distribution, to be more precise): uniform is maybe the most adequate; overscan can make semi-seamless tileable textures. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8064911699831145
      ],
      "excerpt": "--samples N sets amount of the image cuts (samples), processed at one step. With more samples you can set fewer iterations for similar result (and vice versa). 200/200 is a good guess. NB: GPU memory is mostly eaten by this count (not resolution)! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8104309400032667,
        0.8637898165097404,
        0.8476664738679287,
        0.8180646391235852
      ],
      "excerpt": "--opt_step N tells to save every Nth frame (useful with high iterations, default is 1). \n--verbose ('on' by default) enables some printouts and realtime image preview.   \nSome experimental tricks with less definite effects: \n--enforce X adds more details by boosting similarity between two parallel samples. good start is ~0.2. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8345140000053501,
        0.8794387387328334,
        0.9126882666286101
      ],
      "excerpt": "--noise X adds some noise to the parameters, possibly making composition less clogged (in a degree). \n--macro X (from 0 to 1) shifts generation to bigger forms and less disperse composition. should not be too close to 1, since the quality depends on the variety of samples. \n--prog sets progressive learning rate (from 0.1x to 2x of the one, set by lrate). it may boost macro forms creation in some cases (see more here). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.880771306486839
      ],
      "excerpt": "New method, interpolating topics as a constant flow with permanent pan/zoom motion and optional 3D look.   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8192953697125173,
        0.9243547085435488
      ],
      "excerpt": "One can also use image(s) as references with --in_img argument. Explore other arguments for more explicit control. \nThis method works best with direct RGB pixels optimization, but can also be used with FFT parameterization: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8140799567371885
      ],
      "excerpt": "Old method, generating separate images for every text line (with sequences and training videos, as in single-image mode above), then rendering final video from those (mixing images in FFT space) of the length duration in seconds.   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8197873777832149,
        0.9429569593570567
      ],
      "excerpt": "There is --keep X parameter, controlling how well the next line/image generation follows the previous. By default X = 0, and every frame is produced independently (i.e. randomly initiated).  \nSetting it higher starts each generation closer to the average of previous runs, effectively keeping the compositions more similar and the transitions smoother. Safe values are < 0.5 (higher numbers may cause the imagery getting stuck). This behaviour depends on the input, so test with your prompts and see what's better in your case. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9516759812883631
      ],
      "excerpt": "One of the best methods for colors/tones/details (especially with new Gumbel-F8 model); has quite limited resolution though (~800x600 max on Colab). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8795444933313042
      ],
      "excerpt": "Continuous mode with VQGAN (analog of Illustra) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "CLIP + FFT/DWT/RGB = text to image/video",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/eps696/aphantasia/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 48,
      "date": "Fri, 10 Dec 2021 03:27:42 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/eps696/aphantasia/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "eps696/aphantasia",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/eps696/aphantasia/master/Illustra.ipynb",
      "https://raw.githubusercontent.com/eps696/aphantasia/master/Aphantasia.ipynb",
      "https://raw.githubusercontent.com/eps696/aphantasia/master/CLIP_VQGAN.ipynb",
      "https://raw.githubusercontent.com/eps696/aphantasia/master/IllusTrip3D.ipynb"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9808010534102978
      ],
      "excerpt": "Tested on Python 3.7 with PyTorch 1.7.1 or 1.8. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9979947896609701,
        0.9984079357284334
      ],
      "excerpt": "pip install -r requirements.txt \npip install git+https://github.com/openai/CLIP.git \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8672795243743714
      ],
      "excerpt": "<p align='center'><img src='_out/Aphantasia4.jpg' /></p> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8117548936926718
      ],
      "excerpt": "text and/or image as main prompts \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9208229725017552
      ],
      "excerpt": "python clip_fft.py -t \"the text\" --size 1280-720 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9493089097131068
      ],
      "excerpt": "python clip_fft.py -i theimage.jpg --sync 0.4 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8008574570142398
      ],
      "excerpt": "python clip_fft.py -t \"topic sentence\" -t2 \"style description\" -t0 \"avoid this\" --size 1280-720 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8053368697474206,
        0.8658496985524256
      ],
      "excerpt": "Make video from two text files, processing them line by line, rendering 100 frames per line: \npython illustrip.py --in_txt mycontent.txt --in_txt2 mystyles.txt --size 1280-720 --steps 100 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8310868776098285,
        0.8876989294630051
      ],
      "excerpt": "python illustrip.py ... --gen FFT --smooth --align uniform --colors 1.8 --contrast 1.1 \nTo add 3D look, download [AdaBins model] to the main directory, and add --depth 0.01 to the command. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8977192244518177
      ],
      "excerpt": "python illustra.py -i mysong.txt --size 1280-720 --length 155 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python interpol.py -i mydir --length 155 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9202509175650415
      ],
      "excerpt": "<p><img src='_out/some_cute_image-VQGAN.jpg' /></p> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/eps696/aphantasia/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2021 Vadim Epstein\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Aphantasia",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "aphantasia",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "eps696",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/eps696/aphantasia/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "eps696",
        "body": "v1.2 + versions update",
        "dateCreated": "2021-11-13T11:58:35Z",
        "datePublished": "2021-11-13T11:59:56Z",
        "html_url": "https://github.com/eps696/aphantasia/releases/tag/v.1.2b",
        "name": "v1.2b",
        "tag_name": "v.1.2b",
        "tarball_url": "https://api.github.com/repos/eps696/aphantasia/tarball/v.1.2b",
        "url": "https://api.github.com/repos/eps696/aphantasia/releases/53290016",
        "zipball_url": "https://api.github.com/repos/eps696/aphantasia/zipball/v.1.2b"
      },
      {
        "authorType": "User",
        "author_name": "eps696",
        "body": "v1.2 + pytorch version fix",
        "dateCreated": "2021-11-12T20:00:12Z",
        "datePublished": "2021-11-13T09:18:16Z",
        "html_url": "https://github.com/eps696/aphantasia/releases/tag/v.1.2a",
        "name": "v1.2a",
        "tag_name": "v.1.2a",
        "tarball_url": "https://api.github.com/repos/eps696/aphantasia/tarball/v.1.2a",
        "url": "https://api.github.com/repos/eps696/aphantasia/releases/53287310",
        "zipball_url": "https://api.github.com/repos/eps696/aphantasia/zipball/v.1.2a"
      },
      {
        "authorType": "User",
        "author_name": "eps696",
        "body": "+ wavelets generator, vit-b/16 model, faster video encoding, starting from image, etc.",
        "dateCreated": "2021-10-14T17:09:35Z",
        "datePublished": "2021-10-18T21:04:56Z",
        "html_url": "https://github.com/eps696/aphantasia/releases/tag/v1.2",
        "name": "v1.2",
        "tag_name": "v1.2",
        "tarball_url": "https://api.github.com/repos/eps696/aphantasia/tarball/v1.2",
        "url": "https://api.github.com/repos/eps696/aphantasia/releases/51570366",
        "zipball_url": "https://api.github.com/repos/eps696/aphantasia/zipball/v1.2"
      },
      {
        "authorType": "User",
        "author_name": "eps696",
        "body": "This is considered to be quite stable version, used in few projects, therefore fixed as a release.",
        "dateCreated": "2021-06-19T17:57:18Z",
        "datePublished": "2021-06-27T15:18:54Z",
        "html_url": "https://github.com/eps696/aphantasia/releases/tag/v1.0",
        "name": "",
        "tag_name": "v1.0",
        "tarball_url": "https://api.github.com/repos/eps696/aphantasia/tarball/v1.0",
        "url": "https://api.github.com/repos/eps696/aphantasia/releases/45320470",
        "zipball_url": "https://api.github.com/repos/eps696/aphantasia/zipball/v1.0"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 431,
      "date": "Fri, 10 Dec 2021 03:27:42 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "text-to-image",
      "clip",
      "text-to-video"
    ],
    "technique": "GitHub API"
  }
}