{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1805.05622"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Pendulibrium/ai-visual-storytelling-seq2seq",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-11-07T10:18:41Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-05T14:04:08Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9925315943974822,
        0.994135319166409
      ],
      "excerpt": "Implementation of our original solution that is described in the paper Stories for Images-in-Sequence by using Visual and Narrative Components. Our project is inspired by the solution in Visual Storytelling. \nThe model generates stories, sentence by sentence with respect to the sequence of images and the previously generated sentence. The architecture of our solution consists of an image sequence encoder that models the sequential behaviour of the images, a previous-sentence encoder and a current-sentence decoder. The previous-sentence encoder encodes the sentence that was associated with the previous image and the current-sentence decoder is responsible for generating a sentence for the current image of the sequence. We also introduce a novel way of grouping the images of the sequence during the training process, in order to encapture the effect of the previous images in the sequence. Our goal with this approach was to create a model that will generate stories that contain more narrative and evaluative language and that every generated sentence in the story will be affected not only by the sequence of images but also by what has been previously generated in the story. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8112979717793496,
        0.9558071003354699
      ],
      "excerpt": "The script creates the file /dataset/models/alexnet/alexnet_image_train_features.hdf5, that contains all the image features. \nNext we need to associate every image feature vector with it's corresponding vectorized sentence. We vectorize the sentence using the functions in sis_datareader. With the function sentences_to_index we align every image feature with every sentence. If all the file paths are set properly, all of the above can be done by running the command  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8823713314111145,
        0.974315844494809,
        0.863774765332054
      ],
      "excerpt": "Other than our proposed solution, the project can be used to train an encoder-decoder and an encoder-decoder with Luong attention mechanism. \nThe architecture of the proposed model. The images highlighted with red are the ones that are encoded and together with the previous sentence, they influence the generated sentence in the current time step. \nTraining the model and adjusting the parameters is done in the training_model.py. If the attention mechanism is used, make sure that image_encoder_latent_dim = sentence_encoder_latent_dim. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Implementation of seq2seq model for Visual Storytelling Challenge (VIST) http://visionandlanguage.net/VIST/index.html",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Pendulibrium/ai-visual-storytelling-seq2seq/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 18,
      "date": "Wed, 08 Dec 2021 07:33:37 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Pendulibrium/ai-visual-storytelling-seq2seq/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Pendulibrium/ai-visual-storytelling-seq2seq",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The project is built using Python 2.7.14, Tensorflow 1.6.0 and Keras 2.1.6. Install these dependencies to get a development env running\n```\nsudo easy_install --upgrade pip\nsudo easy_install --upgrade six\nsudo pip install tensorflow\nsudo pip install keras\npip install opencv-python\npip install h5py\npip install unidecode\npython -mpip install matplotlib\n```\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8067701173726647
      ],
      "excerpt": "Download the Visual Storytelling Dataset (VIST) from http://visionandlanguage.net/VIST/dataset.html and save it in the dataset/vist_dataset directory. Also download the pre-trained weights for AlexnNet and put them in the dataset/models/alexnet directory. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9253739739301653
      ],
      "excerpt": "python dataset/models/alexnet/myalexnet_forward_newtf.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python data_reader/sis_datareader.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091,
        0.8533507135320113,
        0.9246227682586091
      ],
      "excerpt": "python training_model.py \nTo generate stories in inference_model.py set model_name to the model you want to generate from and run \npython inference_model.py \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Pendulibrium/ai-visual-storytelling-seq2seq/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Perl",
      "HTML"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "ai-visual-storytelling-seq2seq",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "ai-visual-storytelling-seq2seq",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Pendulibrium",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Pendulibrium/ai-visual-storytelling-seq2seq/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 47,
      "date": "Wed, 08 Dec 2021 07:33:37 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "seq2seq",
      "keras",
      "encoder-decoder",
      "recurrent-neural-networks",
      "visual-storytelling"
    ],
    "technique": "GitHub API"
  }
}