{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1505.04597",
      "https://arxiv.org/abs/1505.04597\n* Bozic, J., Tabernik, D. & Skocaj, D. (2021",
      "https://arxiv.org/abs/2104.06064\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* aws-samples GitHub Repository \"ML@Edge with SageMaker Edge Manager\" \nhttps://github.com/aws-samples/amazon-sagemaker-edge-manager-workshop\n* Ronneberger, O., Fischer, P., & Brox, T. (2015). U-Net: Convolutional Networks for Biomedical Image Segmentation. MICCAI. https://arxiv.org/abs/1505.04597\n* Bozic, J., Tabernik, D. & Skocaj, D. (2021). Mixed supervision for surface-defect detection: from weakly to fully supervised learning. Computers in Industry. https://arxiv.org/abs/2104.06064\n\n",
      "technique": "Header extraction"
    }
  ],
  "codeOfConduct": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://raw.githubusercontent.com/aws-samples/amazon-sagemaker-edge-defect-detection-computer-vision/main/CODE_OF_CONDUCT.md",
    "technique": "File Exploration"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/aws-samples/amazon-sagemaker-edge-defect-detection-computer-vision",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contributing Guidelines\nThank you for your interest in contributing to our project. Whether it's a bug report, new feature, correction, or additional\ndocumentation, we greatly value feedback and contributions from our community.\nPlease read through this document before submitting any issues or pull requests to ensure we have all the necessary\ninformation to effectively respond to your bug report or contribution.\nReporting Bugs/Feature Requests\nWe welcome you to use the GitHub issue tracker to report bugs or suggest features.\nWhen filing an issue, please check existing open, or recently closed, issues to make sure somebody else hasn't already\nreported the issue. Please try to include as much information as you can. Details like these are incredibly useful:\n\nA reproducible test case or series of steps\nThe version of our code being used\nAny modifications you've made relevant to the bug\nAnything unusual about your environment or deployment\n\nContributing via Pull Requests\nContributions via pull requests are much appreciated. Before sending us a pull request, please ensure that:\n\nYou are working against the latest source on the main branch.\nYou check existing open, and recently merged, pull requests to make sure someone else hasn't addressed the problem already.\nYou open an issue to discuss any significant work - we would hate for your time to be wasted.\n\nTo send us a pull request, please:\n\nFork the repository.\nModify the source; please focus on the specific change you are contributing. If you also reformat all the code, it will be hard for us to focus on your change.\nEnsure local tests pass.\nCommit to your fork using clear commit messages.\nSend us a pull request, answering any default questions in the pull request interface.\nPay attention to any automated CI failures reported in the pull request, and stay involved in the conversation.\n\nGitHub provides additional document on forking a repository and\ncreating a pull request.\nFinding contributions to work on\nLooking at the existing issues is a great way to find something to contribute on. As our projects, by default, use the default GitHub issue labels (enhancement/bug/duplicate/help wanted/invalid/question/wontfix), looking at any 'help wanted' issues is a great place to start.\nCode of Conduct\nThis project has adopted the Amazon Open Source Code of Conduct.\nFor more information see the Code of Conduct FAQ or contact\nopensource-codeofconduct@amazon.com with any additional questions or comments.\nSecurity issue notifications\nIf you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our vulnerability reporting page. Please do not create a public github issue.\nLicensing\nSee the LICENSE file for our project's licensing. We will ask you to confirm the licensing of your contribution.",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-06-30T22:26:16Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-06T03:06:02Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9777790341462468,
        0.9433119303798961
      ],
      "excerpt": "This repository is related to our blog post Detect industrial defects at low latency with computer vision at the edge with Amazon SageMaker Edge in the AWS Machine Learning blog. \nIn this workshop, we will walk you through a step by step process to build and train computer vision models with Amazon SageMaker and package and deploy them to the edge with SageMaker Edge Manager. The workshop focuses on a defect detection use case in an industrial setting with models like image classification, and semantic segmentation to detect defects across several object types. We will complete the MLOps lifecycle with continuous versioned over-the-air model updates and data capture to the cloud. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9722286810585823,
        0.8909912627363483,
        0.8772209900113406,
        0.9845393683626977,
        0.9863257333832667,
        0.8235763827534278
      ],
      "excerpt": "Model development and training on the cloud: This repository contains code for two pipelines based on SageMaker Pipelines for each of the two model types used (classification and segmentation). These pipelines will be built and executed in a SageMaker Studio notebook. \nModel deployment to the edge: Once a model building pipeline executed successfully, models will be compiled with SageMaker Neo and packaged with a SageMaker Edge packaging job. As such, they can be deployed onto the edge device via IoT Jobs. On the edge device an application is running which will receive the model deployment job payload via MQTT and download the relevant model package. \nEdge inference: The edge device is running the actual application for defect detection. In this workshop, we will use an EC2 instance to simulate an edge device - but any hardware device (RaspberryPi, Nvidia Jetson) can be used as long as SageMaker Neo compilations are supported. During setup, a configuration package is being downloaded to edge device to configure SageMaker Edge Agent. The Edge Agent on the device can then load models deployed via OTA updates and make them available for prediction via a low-latency gRPC API (see SageMaker Edge Manager documentation). \nThis workshop is designed to be used with any dataset for defect detection that includes labels and masks. To be able to use both models (see section Models), you will need a dataset of labelled images (normal and anomalous) as well as a set of respective ground truth masks which identify where the defect on a part is located. To train the models with the provided pipeline without any major code adjustments, you merely need to upload the dataset in the format together with correct path prefixes in an S3 bucket. Please refer to the Getting Started guide below on more details for model training with a dataset. \nHowever, for simplicity of this walkthrough, we will showcase the end-to-end solution using the KolektorSDD2 dataset for defect detection. This dataset consists of over 3000 images of surface defects together with respective binary masks which identify the location of those defects in the image. This makes this dataset very much suitable for our use case. \nBelow you can find examples of those images and their masks as provided in the dataset. The image was taken from the website of the creators of the KolektorSDD2 dataset (see also [Bozic et al., 2021] under References ). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.933826886794347,
        0.9279788073090837
      ],
      "excerpt": "an image classification model using the built-in SageMaker Image Classification algorithm based on the MXNet framework \na semantic segmentation model built with Tensorflow/Keras using the UNET deep learning architecture \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8791917549288409,
        0.9854943278762949,
        0.8608864329656999
      ],
      "excerpt": "\u2514\u2500\u2500 src             &lt;-- contains the actual source code for this project \n    \u251c\u2500\u2500 cloud       &lt;-- contains the code for model training in the cloud and initiation of OTA deployments to the edge \n    \u2514\u2500\u2500 edge        &lt;-- contains the code that is running on the edge device \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8769388358331166
      ],
      "excerpt": "\u251c\u2500\u2500 models_config.json          &lt;-- model configuration, also used for persisting model versions \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9080285350569219
      ],
      "excerpt": "\u251c\u2500\u2500 data_preparation.ipynb                  <-- notebook for data preprocessing of the KolektorSDD2 dataset \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.857366624620592
      ],
      "excerpt": "\u00a0\u00a0  \u251c\u2500\u2500 image_classification                <-- contains the pipeline code for image classification \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8268233370994187,
        0.9910472018815346,
        0.9177053319800383
      ],
      "excerpt": "Please follow the steps below to start building your own edge ML project. You will create a CloudFormation stack to set up all necessary resources in the cloud and prepare an edge device for usage with SageMaker Edge Manager. You will then train models in the cloud and deployment to the edge device using AWS IoT. Please note that model training in the cloud and running inference on the edge are interdependent of each other. We recommend you start by setting up the edge device first and then train the models as a second step. This way, you can then directly deploy them to the edge after you have successfully trained the models. \nThis stack configures several resources needed for this workshop. It sets up an IoT device together with certificates and roles, an Edge Manager fleet, registers the device with the fleet and creates a package for edge agent configuration which is being saved in the S3 bucket for this project. The following image illustrates the resources being created with the CloudFormation stack. \nLaunch an EC2 instance with Ubuntu Server 20 with SSH access (e.g. via Session Manager) into a public subnet and make sure it gets assigned a public IP (you will need this later to access the web application). Ensure that it has access to the S3 buckets containing your configuration package (find the bucket name in the CloudFormation output). It will also need access to the bucket containing the SageMaker Edge Agent binary. For more information, refer to the SageMaker Edge Manager documentation pages. This EC2 instance will from now be considered our \"edge device\". \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8664935103548603,
        0.9659879810338698,
        0.8288199049770775
      ],
      "excerpt": "Run the application with python3 run.py to initialize the application, verify cloud connectivity, connect to the edge agent. This application is a Flask web application running port port 8080 which is integrated with SageMaker Edge Agent and AWS IoT for OTA updates. You will see that, if you have no models deployed yet and have not downloaded any test images, nothing will happen yet in the application. It will stay idle until it can access test images in the /static folder and run inference on those with a deployed model. In the next step, we will see how we can run automated model training with SageMaker Pipelines and deploy them onto the edge device for local inference. \nGo to the EC2 dashboard and find the public IP address of your instance. Browse the public IP address on port 8080, i.e. http://&lt;PUBLIC_IP&gt;:8080. You should now see the web application in your browser window. Ensure that you allow ingress on port 8080 in the security group attached to your instance (see here for details on how to set this up). Also, make sure your local firewalls on your device allow ingress through port 8080. Refer to the Troubleshooting section for further tips. \nCreate a SageMaker Studio domain and user by following this guide in the documentation. Make sure that the IAM role used has access to the S3 bucket created during the CloudFormation deployment. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9581095090517933,
        0.9724757244076709,
        0.8069958848551461,
        0.9735931966085124,
        0.8618409092775267,
        0.9131151959043594
      ],
      "excerpt": "Once the pipeline finished successfully, your model is almost ready for use on the edge device. Verify that the latest model version in the model registry is approved to make it available for edge deployment. \nExecute the following cells of the notebook to run model compilation with SageMaker Neo and then package the model for usage with SageMaker Edge Manager.  \nFinally, you can deploy the model package onto the edge by running the IoT Job as an Over-The-Air update. If your edge application is currently running, it should receive the OTA deployment job, download the model package and load it into the Edge Agent. \nVerify that the deployment automation works by checking the log output on the edge device. You can also verify the successful deployment of a new model version by verifying the successful execution of the IoT job in the AWS IoT Core Console (under \"Manage\" --> \"Jobs\") as shown below. \nYou can set which models should be loaded initially by configuring the model_config.json file. The application will instruct the edge agent to load these models upon startup. You can update model versions by creating IoT jobs from the cloud. The OTA IoT client running alongside the application will listen to the job topics and download the model accordingly. Please also note that for each new model you deploy you might have to adjust your application code accordingly (e.g. if your input shape changes). The structure of the model_config.json file with a sample configuration is shown below. \nIn \"mappings\", you can define which model should be used for each of the two inferences in the application this name needs to align with the model name you choose during OTA deployment. In \"models\", information about the models loaded into the edge agent are persisted even after you shutdown the application. Please note that this is automatically filled out by the application and saved before you close out of the application. You do not need to manually configure this. In case you want to use a manually deployed model package with this application, you can instruct the application to load this model by manually adding a model definition into the JSON file under \"models\". \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Workshop showcasing how to run defect detection using computer vision at the edge with Amazon SageMaker",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/aws-samples/amazon-sagemaker-edge-defect-detection-computer-vision/releases",
    "technique": "GitHub API"
  },
  "faq": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* *The application running on EC2 is not accessible through via public IP address.*\nMake sure you opened up the port your application is running on in the security group attached to the instance. In case you cannot access the application through any other port than port 80, you could try to map the port 80 to 8080 by configuring a NAT redirect using the *iptables* command line tool as follows: `sudo iptables -t nat -A PREROUTING -p tcp --dport 80 -j REDIRECT --to-port 8080`\n* *The edge application fails due to errors related with SageMaker Edge Manager*\nYou can try to restart the edge agent by killing the running process and starting edge agent again with the provided shell script. Make sure that `models_config.json` is configured such that desired models get loaded automatically upon application start. You can also check out the agent logs under `agent/logs` for troubleshooting.\n\n",
      "technique": "Header extraction"
    }
  ],
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 6,
      "date": "Fri, 10 Dec 2021 03:28:07 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/aws-samples/amazon-sagemaker-edge-defect-detection-computer-vision/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "aws-samples/amazon-sagemaker-edge-defect-detection-computer-vision",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/aws-samples/amazon-sagemaker-edge-defect-detection-computer-vision/main/src/cloud/data_preparation.ipynb",
      "https://raw.githubusercontent.com/aws-samples/amazon-sagemaker-edge-defect-detection-computer-vision/main/src/cloud/semantic_segmentation_pipeline.ipynb",
      "https://raw.githubusercontent.com/aws-samples/amazon-sagemaker-edge-defect-detection-computer-vision/main/src/cloud/image_classification_pipeline.ipynb"
    ],
    "technique": "File Exploration"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/aws-samples/amazon-sagemaker-edge-defect-detection-computer-vision/main/src/edge/start_edge_agent.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Launch a new CloudFormation stack with the provided template under `setup/template.yaml`. To learn about how to deploy CloudFormation stacks, please refer to the [documentation](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-console-create-stack.html).\n2. Define a name for the stack and enter a *Project Name* parameter, that is unique in your account. It must be compliant with Amazon S3 bucket names, so please choose a lowercase string here. The project name that you define during stack creation defines the name of many of the resources that are being created with the stack. Make sure to take note of this parameter.\n3. Have a look at the CloudFormation stack outputs and take note of the provided information.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8661377326854044
      ],
      "excerpt": "This repository has the following directory structure: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9203534613205239
      ],
      "excerpt": "\u00a0\u00a0      \u251c\u2500\u2500 requirements.txt                <-- python dependencies needed for training \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9991217707190309
      ],
      "excerpt": "Install the dependencies by running sudo apt update -y &amp;&amp; sudo apt install -y build-essential procps and pip install -r requirements.txt to install the necessary python dependencies. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.880581593433202
      ],
      "excerpt": "Before running the actual application, you need to define an environment variable which determines whether you want to run the app with the Flask development server or the with a production-ready uWSGI server (using waitress). For now, lets use the production server by setting export SM_APP_ENV=prod. For debugging, you might want to later change this to dev. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8661176197453521,
        0.9717106327039013
      ],
      "excerpt": "      \"name\": \"img-classification\", \n      \"version\": \"1\", \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8048026075615984,
        0.8048026075615984
      ],
      "excerpt": "\u00a0\u00a0  \u251c\u2500\u2500 get_pipeline_definition.py          <-- CLI tool for CICD \n  \u00a0\u00a0\u251c\u2500\u2500 run_pipeline.py                     <-- CLI tool for CICD \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8561874728949072
      ],
      "excerpt": "\u00a0\u00a0  \u2502\u00a0\u00a0 \u251c\u2500\u2500 evaluation.py                   <-- script to evaluate model performance on test dataset \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8349189258612669
      ],
      "excerpt": "\u00a0\u00a0  \u2502\u00a0\u00a0 \u2514\u2500\u2500 preprocessing.py                <-- script for preprocessing (augmentation, train/test/val split) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8349189258612669
      ],
      "excerpt": "\u00a0\u00a0      \u251c\u2500\u2500 preprocessing.py                <-- script for preprocessing (augmentation, train/test/val split) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9025548016071955
      ],
      "excerpt": "\u00a0\u00a0      \u2514\u2500\u2500 train_tf.py                     <-- training script for training the unet model \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/aws-samples/amazon-sagemaker-edge-defect-detection-computer-vision/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Jupyter Notebook",
      "HTML",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT No Attribution",
      "url": "https://api.github.com/licenses/mit-0"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy of this\\nsoftware and associated documentation files (the \"Software\"), to deal in the Software\\nwithout restriction, including without limitation the rights to use, copy, modify,\\nmerge, publish, distribute, sublicense, and/or sell copies of the Software, and to\\npermit persons to whom the Software is furnished to do so.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED,\\nINCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A\\nPARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT\\nHOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\\nOF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\\nSOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Defect detection using computer vision at the edge with Amazon SageMaker",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "amazon-sagemaker-edge-defect-detection-computer-vision",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "aws-samples",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/aws-samples/amazon-sagemaker-edge-defect-detection-computer-vision/blob/main/README.md",
    "technique": "GitHub API"
  },
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To run inference on the device, you need to have fulfilled the following requirements:\n\n* The edge agent on the edge device is properly configured and can successfully authenticate against AWS IoT\n* You have downloaded test images onto the edge device in the folder `static/`\n* You have deployed at least one of the two models (image classification or semantic segmentation) via OTA updates\n* The edge agent is running and the models could be loaded successfully (for troubleshooting check command line output or edge agent logs in `agent/logs/agent.log`)\n\nIf everything is configured accordingly, you should see the edge application cycling through the provided images in the `static/` directory and run inference against both of the models. The result of the inference is then displayed in the web application. You can see a screenshot of the running web application below. The two models loaded into edge agent are displayed on the top, the incoming image from the camera stream is fed into the two models and the predictions are illustrated on the bottom of the page.\n\n![inference_ui](img/inferece_ui.png)\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 14,
      "date": "Fri, 10 Dec 2021 03:28:07 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "ml",
      "sagemaker",
      "ai",
      "aws",
      "machine-learning",
      "computer-vision",
      "edge-machine-learning"
    ],
    "technique": "GitHub API"
  }
}