{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2004.02857",
      "https://arxiv.org/abs/2110.02207",
      "https://arxiv.org/abs/2004.02857",
      "https://arxiv.org/abs/2004.02857",
      "https://arxiv.org/abs/2010.07954"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you use VLN-CE in your research, please cite the following [paper](https://arxiv.org/abs/2004.02857):\n\n```tex\n@inproceedings{krantz_vlnce_2020,\n  title={Beyond the Nav-Graph: Vision and Language Navigation in Continuous Environments},\n  author={Jacob Krantz and Erik Wijmans and Arjun Majundar and Dhruv Batra and Stefan Lee},\n  booktitle={European Conference on Computer Vision (ECCV)},\n  year={2020}\n }\n```\n\nIf you use the RxR-Habitat data, please additionally cite the following [paper](https://arxiv.org/abs/2010.07954):\n\n```tex\n@inproceedings{ku2020room,\n  title={Room-Across-Room: Multilingual Vision-and-Language Navigation with Dense Spatiotemporal Grounding},\n  author={Ku, Alexander and Anderson, Peter and Patel, Roma and Ie, Eugene and Baldridge, Jason},\n  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},\n  pages={4392--4412},\n  year={2020}\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{krantz_vlnce_2020,\n  title={Beyond the Nav-Graph: Vision and Language Navigation in Continuous Environments},\n  author={Jacob Krantz and Erik Wijmans and Arjun Majundar and Dhruv Batra and Stefan Lee},\n  booktitle={European Conference on Computer Vision (ECCV)},\n  year={2020}\n }\nIf you use the RxR-Habitat data, please additionally cite the following paper:\ntex\n@inproceedings{ku2020room,\n  title={Room-Across-Room: Multilingual Vision-and-Language Navigation with Dense Spatiotemporal Grounding},\n  author={Ku, Alexander and Anderson, Peter and Patel, Roma and Ie, Eugene and Baldridge, Jason},\n  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},\n  pages={4392--4412},\n  year={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8759645817045618
      ],
      "excerpt": "Beyond the Nav-Graph: Vision-and-Language Navigation in Continuous Environments (paper) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jacobkrantz/VLN-CE",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-04-03T17:31:00Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-04T09:26:01Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8636344724070388,
        0.8215666560474278,
        0.9917657462887167
      ],
      "excerpt": "Beyond the Nav-Graph: Vision-and-Language Navigation in Continuous Environments (paper) \nWaypoint Models for Instruction-guided Navigation in Continuous Environments (paper, README) \nVision and Language Navigation in Continuous Environments (VLN-CE) is an instruction-guided navigation task with crowdsourced instructions, realistic environments, and unconstrained agent navigation. This repo is a launching point for interacting with the VLN-CE task and provides both baseline agents and training methods. Both the Room-to-Room (R2R) and the Room-Across-Room (RxR) datasets are supported. VLN-CE is implemented using the Habitat platform. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8609889017205173
      ],
      "excerpt": "  <img width=\"775\" height=\"360\" src=\"./data/res/VLN_comparison.gif\" alt=\"VLN-CE comparison to VLN\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9981367606302665
      ],
      "excerpt": "The R2R_VLNCE dataset is a port of the Room-to-Room (R2R) dataset created by Anderson et al for use with the Matterport3DSimulator (MP3D-Sim). For details on the porting process from MP3D-Sim to the continuous reconstructions used in Habitat, please see our paper. We provide two versions of the dataset, R2R_VLNCE_v1-2 and R2R_VLNCE_v1-2_preprocessed. R2R_VLNCE_v1-2 contains the train, val_seen, val_unseen, and test splits. R2R_VLNCE_v1-2_preprocessed runs with our models out of the box. It additionally includes instruction tokens mapped to GloVe embeddings, ground truth trajectories, and a data augmentation split (envdrop) that is ported from R2R-EnvDrop. The test split does not contain episode goals or ground truth paths. For more details on the dataset contents and format, see our project page. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8743655691645721,
        0.8743655691645721
      ],
      "excerpt": "| R2R_VLNCE_v1-2.zip | data/datasets/R2R_VLNCE_v1-2 | 3 MB | \n| R2R_VLNCE_v1-2_preprocessed.zip | data/datasets/R2R_VLNCE_v1-2_preprocessed | 345 MB | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9017464044535866
      ],
      "excerpt": "The baseline models for RxR-Habitat use precomputed BERT instruction features which can be downloaded from here and saved to data/datasets/RxR_VLNCE_v0/text_features/rxr_{split}/{instruction_id}_{language}_text_features.npz. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9565983360311467
      ],
      "excerpt": "is an order of magnitude larger than existing datasets, and \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.949239081749833,
        0.9175631179139473
      ],
      "excerpt": "The challenge was hosted at the CVPR 2021 Embodied AI Workshop. While the official challenge is over, the leaderboard remains open and we encourage submissions on this difficult task! For guidelines and access, please visit: ai.google.com/research/rxr/habitat. \nSubmissions are made by running an agent locally and submitting a jsonlines file (.jsonl) containing the agent's trajectories. Starter code for generating this file is provided in the function BaseVLNCETrainer.inference(). Here is an example of generating predictions for English using the Cross-Modal Attention baseline: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8228935600967251,
        0.8557279006323186,
        0.8396629516496036
      ],
      "excerpt": "If you use different models for different languages, you can merge their predictions with scripts/merge_inference_predictions.py. Submissions are only accepted that contain all episodes from all three languages in the test-challenge split. Starter code for this challenge was originally hosted in the rxr-habitat-challenge branch but is now under continual development in master. \nThe VLN-CE Challenge is live and taking submissions for public test set evaluation. This challenge uses the R2R data ported in the original VLN-CE paper. \nTo submit to the leaderboard, you must run your agent locally and submit a JSON file containing the generated agent trajectories. Starter code for generating this JSON file is provided in the function BaseVLNCETrainer.inference(). Here is an example of generating this file using the pretrained Cross-Modal Attention baseline: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9667421967989177
      ],
      "excerpt": "The baseline model for the VLN-CE task is the cross-modal attention model trained with progress monitoring, DAgger, and augmented data (CMA_PM_DA_Aug). As evaluated on the leaderboard, this model achieves: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9921320548845546,
        0.9585861718140934
      ],
      "excerpt": "This model was originally presented with a val_unseen performance of 0.30 SPL, however the leaderboard evaluates this same model at 0.27 SPL. The model was trained and evaluated on a hardware + Habitat build that gave slightly different results, as is the case for the other paper experiments. Going forward, the leaderboard contains the performance metrics that should be used for official comparison. In our tests, the installation procedure for this repo gives nearly identical evaluation to the leaderboard, but we recognize that compute hardware along with the version and build of Habitat are factors to reproducibility. \nFor push-button replication of all VLN-CE experiments, see here. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9815989183592712
      ],
      "excerpt": "The DaggerTrainer class is the standard trainer and supports teacher forcing or dataset aggregation (DAgger). This trainer saves trajectories consisting of RGB, depth, ground-truth actions, and instructions to disk to avoid time spent in simulation. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8763035552191887
      ],
      "excerpt": "TORCH_GPU_ID: 0  #: GPU for pytorch-related code (the model) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Vision-and-Language Navigation in Continuous Environments using Habitat",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jacobkrantz/VLN-CE/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 30,
      "date": "Sat, 11 Dec 2021 11:12:03 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jacobkrantz/VLN-CE/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "jacobkrantz/VLN-CE",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/jacobkrantz/VLN-CE/master/sbatch_scripts/waypoint_train.sh",
      "https://raw.githubusercontent.com/jacobkrantz/VLN-CE/master/sbatch_scripts/cluster_example.sh",
      "https://raw.githubusercontent.com/jacobkrantz/VLN-CE/master/sbatch_scripts/waypoint_train_single_node.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This project is developed with Python 3.6. If you are using [miniconda](https://docs.conda.io/en/latest/miniconda.html) or [anaconda](https://anaconda.org/), you can create an environment:\n\n```bash\nconda create -n vlnce python3.6\nconda activate vlnce\n```\n\nVLN-CE uses [Habitat-Sim](https://github.com/facebookresearch/habitat-sim/tree/v0.1.7) 0.1.7 which can be [built from source](https://github.com/facebookresearch/habitat-sim/tree/v0.1.7#installation) or installed from conda:\n\n```bash\nconda install -c aihabitat -c conda-forge habitat-sim=0.1.7 headless\n```\n\nThen install [Habitat-Lab](https://github.com/facebookresearch/habitat-lab/tree/v0.1.7):\n\n```bash\ngit clone --branch v0.1.7 git@github.com:facebookresearch/habitat-lab.git\ncd habitat-lab\n#: installs both habitat and habitat_baselines\npython -m pip install -r requirements.txt\npython -m pip install -r habitat_baselines/rl/requirements.txt\npython -m pip install -r habitat_baselines/rl/ddppo/requirements.txt\npython setup.py develop --all\n```\n\nNow you can install VLN-CE:\n\n```bash\ngit clone git@github.com:jacobkrantz/VLN-CE.git\ncd VLN-CE\npython -m pip install -r requirements.txt\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8520738790657498
      ],
      "excerpt": "Download: RxR_VLNCE_v0.zip \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8503413135291159
      ],
      "excerpt": "Both trainers inherit from BaseVLNCETrainer. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.869065424247883
      ],
      "excerpt": "TORCH_GPU_ID: 0  #: GPU for pytorch-related code (the model) \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.902978529818504
      ],
      "excerpt": "  <img width=\"775\" height=\"360\" src=\"./data/res/VLN_comparison.gif\" alt=\"VLN-CE comparison to VLN\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.809409224375501,
        0.8969849947445017
      ],
      "excerpt": ": requires running with python 2.7 \npython download_mp.py --task habitat -o data/scene_datasets/mp3d/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8207465283366251
      ],
      "excerpt": "| Dataset | Extract path | Size | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.86053270108505,
        0.86053270108505
      ],
      "excerpt": "| R2R_VLNCE_v1-2.zip | data/datasets/R2R_VLNCE_v1-2 | 3 MB | \n| R2R_VLNCE_v1-2_preprocessed.zip | data/datasets/R2R_VLNCE_v1-2_preprocessed | 345 MB | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8507311995361109,
        0.8684759971125294
      ],
      "excerpt": "Baseline models encode depth observations using a ResNet pre-trained on PointGoal navigation. Those weights can be downloaded from here (672M). Extract the contents to data/ddppo-models/{model}.pth. \nDownload: RxR_VLNCE_v0.zip \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": "|   \u251c\u2500 train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8783991718181988
      ],
      "excerpt": "  <img width=\"573\" height=\"360\" src=\"/data/res/rxr_teaser.gif\" alt=\"RxR Challenge Teaser GIF\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9372565577971909
      ],
      "excerpt": "python run.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8312911535297215
      ],
      "excerpt": "To submit to the leaderboard, you must run your agent locally and submit a JSON file containing the generated agent trajectories. Starter code for generating this JSON file is provided in the function BaseVLNCETrainer.inference(). Here is an example of generating this file using the pretrained Cross-Modal Attention baseline: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9372565577971909
      ],
      "excerpt": "python run.py \\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8667461278717212
      ],
      "excerpt": "The run.py script controls training and evaluation for all models and datasets: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9372565577971909,
        0.8016493987723207,
        0.8559874108386343,
        0.808323768199944
      ],
      "excerpt": "python run.py \\ \n  --exp-config path/to/experiment_config.yaml \\ \n  --run-type {train | eval | inference} \nFor example, a random agent can be evaluated on 10 val-seen episodes of R2R using this command: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9273360277541451
      ],
      "excerpt": "python run.py --exp-config vlnce_baselines/config/r2r_baselines/nonlearning.yaml --run-type eval \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9159515464930487
      ],
      "excerpt": "Evaluation on validation splits can be done by running python run.py --exp-config path/to/experiment_config.yaml --run-type eval. If EVAL.EPISODE_COUNT == -1, all episodes will be evaluated. If EVAL_CKPT_PATH_DIR is a directory, each checkpoint will be evaluated one at a time. \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/jacobkrantz/VLN-CE/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 Jacob Krantz, Erik Wijmans, Arjun Majumdar, Dhruv Batra,\\nStefan Lee\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Vision-and-Language Navigation in Continuous Environments (VLN-CE)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "VLN-CE",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "jacobkrantz",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/jacobkrantz/VLN-CE/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 96,
      "date": "Sat, 11 Dec 2021 11:12:03 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "ai",
      "computer-vision",
      "robotics",
      "deep-learning",
      "research",
      "python"
    ],
    "technique": "GitHub API"
  }
}