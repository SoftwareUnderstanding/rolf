{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1412.6980v8\n\nhttp://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf\n\nhttps://github.com/commaai/research/blob/master/train_steering_model.py\n\n\nHow Referencing This Project\n---\nIf you like my code and you want to use it in your project, please refer it like this:\n\n`Amani, Sajjad. \"Train an Autonomous Vehicle by CNN Deep Learning to Drive Like Humans.\" GitHub, 3 November 2019, https://github.com/Sj-Amani/Practical_Behavioral_Cloning`"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        0.9218387569487573
      ],
      "excerpt": "conv2d_3 (Conv2D)            (None, 10, 20, 64)        51264 \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Sj-Amani/Practical_Behavioral_Cloning",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-10-15T23:39:07Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-01-20T11:59:29Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.8129621526765971
      ],
      "excerpt": "In this project, I will use CNN Deep Learnig to clone driving behavior. I will train, validate and test a model using Keras. The model will output a steering angle to an autonomous vehicle. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9748956060950598,
        0.9207724524195132
      ],
      "excerpt": "The goals / steps of this project are the following: \n* Use the simulator to collect data of good driving behavior \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8554051944274084
      ],
      "excerpt": "* Summarize the results with a written report (this README.md file) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.842358317559399,
        0.8985743788610281
      ],
      "excerpt": "My project includes the following files: \n* make_data_for_training.sh containing the script to preprocess all the data and make the data for training \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9743322772976141
      ],
      "excerpt": "One of the best ways to to add useful information to train your model is data augmentation. Here, I do the the data augmentation by flipping the images horizontally and inverting the related steering angles. This will double the data size and reduces any bias towards turning left/right. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9586872214582111,
        0.9579808817952731,
        0.9211767570089834,
        0.8935514033900369
      ],
      "excerpt": "Middel side driving data: Keep the center labled images and correct the steering angle for the right/left labled images  by -+0.15 to keep the car in the middle of the road for small deviations. Finally, we smooth the steering angle over time using a moving average function to avoid sharp changes during the auto mode. \nRight/Left side driving data: Remove the center labeled images and images whose steering angle is zero. Then correct the steering angle for the right/left labled images  by adding -+0.5 to keep the car to turn to the middle of the road in case of passing the road side lines. Finally, we smooth the steering angle over time using a moving average function to avoid sharp changes during the auto mode. \nThere has been prior work done to predict vehicle steering angles from camera images, such as NVIDIA's \"End to End Learning for Self-Driving Cars\", and comma.ai's steering angle prediction model. Here, I used the comma.ai's steering angle prediction model. \nThe CNN model that I used here has the following layers and information: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8577302739126875
      ],
      "excerpt": "dense_1 (Dense)              (None, 512)               6554112    \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8577302739126875
      ],
      "excerpt": "dense_2 (Dense)              (None, 1)                 513        \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9932303336667953,
        0.895814710611461,
        0.8154104550603786,
        0.955168876511262,
        0.9495700484864152
      ],
      "excerpt": "In order to validate the network, you need to compare model performance on the training set and a validation set. The validation set should contain image and steering data that was not used for training. A rule of thumb could be to use 80% of your data for training and 20% for validation or 70% and 30%. But here, because of small data size, I used 90% of the data for training and 10% for validation. Also, randomly shuffle the data before splitting into training and validation sets is a good practice. \nIf model predictions are poor on both the training and validation set (for example, mean squared error is high on both), then this is evidence of underfitting. Possible solutions could be to: \nincrease the number of epochs \nadd more convolutions to the network. \nWhen the model predicts well on the training set but poorly on the validation set (for example, low mean squared error for training set, high mean squared error for validation set), this is evidence of overfitting. If the model is overfitting, a few ideas could be to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9434578258017009,
        0.9252134284549425
      ],
      "excerpt": "collect more data or further augment the data set \nIdeally, the model will make good predictions on both the training and validation sets. The implication is that when the network sees an image, it can successfully predict what angle was being driven at that moment. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9587782583345836,
        0.9915824938153774
      ],
      "excerpt": "The images captured in the car simulator are much larger than the images encountered in the Traffic Sign Classifier Project, a size of 160 x 320 x 3 compared to 32 x 32 x 3. Storing 10,000 traffic sign images would take about 30 MB but storing 10,000 simulator images would take over 1.5 GB. That's a lot of memory! Not to mention that preprocessing data can change data types from an int to a float, which can increase the size of the data by a factor of 4. \nGenerators can be a great way to work with large amounts of data. Instead of storing the preprocessed data in memory all at once, using a generator you can pull pieces of the data and process them on the fly only when you need them, which is much more memory-efficient.Here, a python generator was used to generate batches of data and the images themselves were read from disk only when new batch was requested. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8669987461946895
      ],
      "excerpt": "An adam optimizer was used to minimize the mean squared error (MSE). The optimizer's learning rate was not extensively tuned, but a learning rate of 1e-4 produced stable results. The loss function was MSE because predicting steering angles is a regression problem. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9727352688289167
      ],
      "excerpt": "Once we're satisfied that the model is making good predictions on the training and validation sets, we can test the model by launching the simulator and entering autonomous mode. For testing, I just test the model on the simulator but you can define a test model for your self if you want. Please note that during the test, if your model has low mean squared error on the training and validation sets but is driving off the track, this could be because of the data collection process. It's important to feed the network examples of good driving behavior so that the car stays in the center and recovers when getting too close to the sides of the road. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9539635390125297
      ],
      "excerpt": "How Referencing This Project \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Train an autonomous vehicle by CNN deep learning to drive like humans",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Sj-Amani/Practical_Behavioral_Cloning/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Wed, 08 Dec 2021 19:27:00 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Sj-Amani/Practical_Behavioral_Cloning/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "Sj-Amani/Practical_Behavioral_Cloning",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/Sj-Amani/Practical_Behavioral_Cloning/master/make_data_for_training.sh"
    ],
    "technique": "File Exploration"
  },
  "invocation": [
    {
      "confidence": [
        0.903269439678273,
        0.8297974910023527
      ],
      "excerpt": "* Train and validate the model with a training and validation set (model.py) \n* Test that the model successfully drives around track one without leaving the road (drive.py) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.88819827571337
      ],
      "excerpt": "* model.py containing the script to create and train the model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.906614234580968
      ],
      "excerpt": "Layer (type)                 Output Shape              Param # \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8077910318745326
      ],
      "excerpt": "conv2d_3 (Conv2D)            (None, 10, 20, 64)        51264 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8934745846565956
      ],
      "excerpt": "Total params: 6,621,809 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.875013619207242
      ],
      "excerpt": "For training the model, the epochs number and batch size were 10 and 16, repectively. During the training, the training and validation loss calculations show a decreasing trend: \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/Sj-Amani/Practical_Behavioral_Cloning/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 Sajjad Amani\\nCopyright (c) 2017-2018 Udacity, Inc.\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Practical Behavioral Cloning",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Practical_Behavioral_Cloning",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "Sj-Amani",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/Sj-Amani/Practical_Behavioral_Cloning/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* python 3\n* numpy\n* matplotlib\n* Pickle\n* pillow (PIL)\n* scikit-learn\n* h5py\n* Pickle\n* TensorFlow\n* Keras\n* Udacity [simulator](https://github.com/udacity/self-driving-car-sim)\n\n`environment.yml` shows the exact environment that I used here. Please note the for using GPU you need to do some initial preparations which are not in the scope of this writeup.\nIf you want to know how to set up the GPU environment, I highly recommend to use the Docker's images! [This](https://blog.amedama.jp/entry/2017/04/03/235901) is a good starting point.\nAlso, if you need more help, please don't hesitate to contact me! I'll do my best to come back to you quickly.\n\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Run `python drive.py model.json` in a terminal and then, you need to run the [simulator](https://github.com/udacity/self-driving-car-sim) and choose the `AUTONOMOUS MODE`. \n\n![simulator-autonomous](results/Simulator-Autonomous.png)\n\n\nThen, you will see the car will start to move like the gif video provided on the top of this page.\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Run the provided Udacity [simulator](https://github.com/udacity/self-driving-car-sim) and select training mode to run the car:\n\n![simulator-training](results/Simulator-Training.png)\n\nInfront of this car, three cameras are located which are recording the images in left/center/right directions like this:\n\n\t \\|/\n\t  X\n\nWhere `X` shows the car and `\\|/` shows the camera direction in lef, center, and right, respectively.\n\n2. Save the driving images data for these cases at least for one round:\n\t- || x | - | - || : Left driving (I did one round)\n\t- || - | x | - || : Middle driving (I did two rounds)\n\t- || - | - | x || : Right driving (I did one round)\n\n`X` shows the car location in the road `|| ... ||`. \n\nFor more understanding, I've provided these images for each case:\n\n\n![left_driving](results/Left_Driving.png)\n\n![middle_driving](results/Middle_Driving.png)\n\n![right_driving](results/Right_Driving.png)\n\n\n3. Open `make_data_for_training.sh` and comple the following:\n\t- dir_workspace: the directory where this package is in it\n\t- dir_left_side_driving: the directory to the recorded left side driving data\n\t- dir_middle_side_driving: the directory to the recorded middle side driving data\n\t- dir_right_side_driving: the directory to the recorded right side driving data\n\n`make_data_for_training.sh` contains the script to preprocess all the data (e.g. flipping) and make the data ready for training.\n\n4. Run `python model.py` to creat train the CNN model.\n5. Run `python drive.py model.json` in a terminal and then, you need to run the simulator and choose the `AUTONOMOUS MODE`. Then, you will see the car will start to move like.\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Wed, 08 Dec 2021 19:27:00 GMT"
    },
    "technique": "GitHub API"
  }
}