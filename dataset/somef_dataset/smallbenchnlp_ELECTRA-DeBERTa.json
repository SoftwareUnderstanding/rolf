{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2003.10555\">this paper</a>. It speeds up training (in comparison to normal masked language modeling"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```bibtex\n@misc{kanakarajan2021smallbench,\n      title={Small-Bench NLP: Benchmark for small single GPU trained models in Natural Language Processing}, \n      author={Kamal Raj Kanakarajan and Bhuvana Kundumani and Malaikannan Sankarasubbu},\n      year={2021},\n      eprint={2109.10847},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}\n```\n\n```bibtex\n@misc{he2020deberta,\n    title={DeBERTa: Decoding-enhanced BERT with Disentangled Attention},\n    author={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},\n    year={2020},\n    eprint={2006.03654},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}\n```\n\n\n\n```bibtex\n@misc{clark2020electra,\n    title={ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators},\n    author={Kevin Clark and Minh-Thang Luong and Quoc V. Le and Christopher D. Manning},\n    year={2020},\n    eprint={2003.10555},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}\n```\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{clark2020electra,\n    title={ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators},\n    author={Kevin Clark and Minh-Thang Luong and Quoc V. Le and Christopher D. Manning},\n    year={2020},\n    eprint={2003.10555},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{he2020deberta,\n    title={DeBERTa: Decoding-enhanced BERT with Disentangled Attention},\n    author={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},\n    year={2020},\n    eprint={2006.03654},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@misc{kanakarajan2021smallbench,\n      title={Small-Bench NLP: Benchmark for small single GPU trained models in Natural Language Processing}, \n      author={Kamal Raj Kanakarajan and Bhuvana Kundumani and Malaikannan Sankarasubbu},\n      year={2021},\n      eprint={2109.10847},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}",
      "technique": "Regular expression"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/smallbenchnlp/ELECTRA-DeBERTa",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-04T13:39:56Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-29T04:25:52Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9553521689021472,
        0.8754793779325045
      ],
      "excerpt": "A simple working wrapper for fast pretraining of language models as detailed in <a href=\"https://arxiv.org/abs/2003.10555\">this paper</a>. It speeds up training (in comparison to normal masked language modeling) by a factor of 4x, and eventually reaches better performance if trained for even longer. Special thanks to <a href=\"https://github.com/enijkamp\">Erik Nijkamp</a> for taking the time to replicate the results for GLUE. \nThe generator should be roughly a quarter to at most one half of the discriminator's size for effective training. Any greater and the generator will be too good and the adversarial game collapses. This was done by reducing the hidden dimension, feed forward hidden dimension, and number of attention heads in the paper. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9503310128035527
      ],
      "excerpt": "Fine-tune on the MRPC sub-task of the GLUE benchmark. \n",
      "technique": "Supervised classification"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/smallbenchnlp/ELECTRA-DeBERTa/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Thu, 09 Dec 2021 21:34:19 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/smallbenchnlp/ELECTRA-DeBERTa/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "smallbenchnlp/ELECTRA-DeBERTa",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```bash\n$ pip install electra-pytorch\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9385097363018797
      ],
      "excerpt": "forked from electra-pytorch \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8080002376980483,
        0.999833231880651
      ],
      "excerpt": "$ cd data \n$ pip3 install gdown \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9906248903846466
      ],
      "excerpt": "$ cd .. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9224054290922052,
        0.8309391638231081
      ],
      "excerpt": "$ python setup.py test \nDownload the OpenWebText dataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8411624709648624
      ],
      "excerpt": "$ python pretraining/openwebtext/preprocess.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.808461327669838,
        0.8447634792014815
      ],
      "excerpt": "$ python pretraining/openwebtext/pretrain.py \nDownload GLUE dataset. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9633676500797465
      ],
      "excerpt": "$ python examples/glue/download.py \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/smallbenchnlp/ELECTRA-DeBERTa/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 Phil Wang\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# ELECTRA-DeBERTa - Pytorch",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "ELECTRA-DeBERTa",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "smallbenchnlp",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/smallbenchnlp/ELECTRA-DeBERTa/blob/main/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 7,
      "date": "Thu, 09 Dec 2021 21:34:19 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The following example uses `reformer-pytorch`, which is available to be pip installed.\n\n```python\nimport torch\nfrom torch import nn\nfrom reformer_pytorch import ReformerLM\n\nfrom electra_pytorch import Electra\n\n#: (1) instantiate the generator and discriminator, making sure that the generator is roughly a quarter to a half of the size of the discriminator\n\ngenerator = ReformerLM(\n    num_tokens = 20000,\n    emb_dim = 128,\n    dim = 256,              #: smaller hidden dimension\n    heads = 4,              #: less heads\n    ff_mult = 2,            #: smaller feed forward intermediate dimension\n    dim_head = 64,\n    depth = 12,\n    max_seq_len = 1024\n)\n\ndiscriminator = ReformerLM(\n    num_tokens = 20000,\n    emb_dim = 128,\n    dim = 1024,\n    dim_head = 64,\n    heads = 16,\n    depth = 12,\n    ff_mult = 4,\n    max_seq_len = 1024\n)\n\n#: (2) weight tie the token and positional embeddings of generator and discriminator\n\ngenerator.token_emb = discriminator.token_emb\ngenerator.pos_emb = discriminator.pos_emb\n#: weight tie any other embeddings if available, token type embeddings, etc.\n\n#: (3) instantiate electra\n\ntrainer = Electra(\n    generator,\n    discriminator,\n    discr_dim = 1024,           #: the embedding dimension of the discriminator\n    discr_layer = 'reformer',   #: the layer name in the discriminator, whose output would be used for predicting token is still the same or replaced\n    mask_token_id = 2,          #: the token id reserved for masking\n    pad_token_id = 0,           #: the token id for padding\n    mask_prob = 0.15,           #: masking probability for masked language modeling\n    mask_ignore_token_ids = []  #: ids of tokens to ignore for mask modeling ex. (cls, sep)\n)\n\n#: (4) train\n\ndata = torch.randint(0, 20000, (1, 1024))\n\nresults = trainer(data)\nresults.loss.backward()\n\n#: after much training, the discriminator should have improved\n\ntorch.save(discriminator, f'./pretrained-model.pt')\n```\n\nIf you would rather not have the framework auto-magically intercept the hidden output of the discriminator, you can pass in the discriminator (with the extra linear [dim x 1]) by yourself with the following.\n\n```python\nimport torch\nfrom torch import nn\nfrom reformer_pytorch import ReformerLM\n\nfrom electra_pytorch import Electra\n\n#: (1) instantiate the generator and discriminator, making sure that the generator is roughly a quarter to a half of the size of the discriminator\n\ngenerator = ReformerLM(\n    num_tokens = 20000,\n    emb_dim = 128,\n    dim = 256,              #: smaller hidden dimension\n    heads = 4,              #: less heads\n    ff_mult = 2,            #: smaller feed forward intermediate dimension\n    dim_head = 64,\n    depth = 12,\n    max_seq_len = 1024\n)\n\ndiscriminator = ReformerLM(\n    num_tokens = 20000,\n    emb_dim = 128,\n    dim = 1024,\n    dim_head = 64,\n    heads = 16,\n    depth = 12,\n    ff_mult = 4,\n    max_seq_len = 1024,\n    return_embeddings = True\n)\n\n#: (2) weight tie the token and positional embeddings of generator and discriminator\n\ngenerator.token_emb = discriminator.token_emb\ngenerator.pos_emb = discriminator.pos_emb\n#: weight tie any other embeddings if available, token type embeddings, etc.\n\n#: (3) instantiate electra\n\ndiscriminator_with_adapter = nn.Sequential(discriminator, nn.Linear(1024, 1))\n\ntrainer = Electra(\n    generator,\n    discriminator_with_adapter,\n    mask_token_id = 2,          #: the token id reserved for masking\n    pad_token_id = 0,           #: the token id for padding\n    mask_prob = 0.15,           #: masking probability for masked language modeling\n    mask_ignore_token_ids = []  #: ids of tokens to ignore for mask modeling ex. (cls, sep)\n)\n\n#: (4) train\n\ndata = torch.randint(0, 20000, (1, 1024))\n\nresults = trainer(data)\nresults.loss.backward()\n\n#: after much training, the discriminator should have improved\n\ntorch.save(discriminator, f'./pretrained-model.pt')\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}