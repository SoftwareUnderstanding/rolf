{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1909.01377",
      "https://arxiv.org/abs/2006.08656",
      "https://arxiv.org/abs/2106.14342",
      "https://arxiv.org/abs/2106.14342"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "- The transformer implementation as well as the extra modules (e.g., adaptive embeddings) were based on the [Transformer-XL](https://github.com/kimiyoung/transformer-xl) repo.\n\n- Some utilization code (e.g., model summary and yaml processing) of this repo were modified from the [HRNet](https://github.com/HRNet/HRNet-Semantic-Segmentation) repo.\n\n- We also added the RAdam optimizer as an option to the training (but didn't set it to default). The RAdam implementation is from the [RAdam](https://github.com/LiyuanLucasLiu/RAdam) repo.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{bai2021stabilizing,\n  title     = {Stabilizing Equilibrium Models by Jacobian Regularization},\n  author    = {Shaojie Bai and Vladlen Koltun and J. Zico Kolter},\n  booktitle = {International Conference on Machine Learning (ICML)},\n  year      = {2021}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{bai2020multiscale,\n  author    = {Shaojie Bai and Vladlen Koltun and J. Zico Kolter},\n  title     = {Multiscale Deep Equilibrium Models},\n  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},\n  year      = {2020},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{bai2019deep,\n  author    = {Shaojie Bai and J. Zico Kolter and Vladlen Koltun},\n  title     = {Deep Equilibrium Models},\n  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},\n  year      = {2019},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9999984155903736
      ],
      "excerpt": "If you find this repository useful for your research, please consider citing our work(s): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9933537248551054,
        0.8374301959150761,
        0.9969550989950307,
        0.9664456561658856
      ],
      "excerpt": "  author    = {Shaojie Bai and Vladlen Koltun and J. Zico Kolter}, \n  title     = {Multiscale Deep Equilibrium Models}, \n  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)}, \n  year      = {2020}, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9933537248551054,
        0.9999910773170938,
        0.9664456561658856
      ],
      "excerpt": "  author    = {Shaojie Bai and Vladlen Koltun and J. Zico Kolter}, \n  booktitle = {International Conference on Machine Learning (ICML)}, \n  year      = {2021} \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/locuslab/deq",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2019-08-06T19:07:27Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-06T15:06:54Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9585923761094836,
        0.8171468851653242,
        0.9801145833307583,
        0.9984390312597283,
        0.9420769820690094,
        0.8037802575414817,
        0.8037802575414817
      ],
      "excerpt": ":boom:2021/6: Repo updated with the multiscale DEQ (MDEQ) code, Jacobian-related analysis & regularization support, and the new, faster and simpler implicit differentiation implementation through PyTorch's backward hook! (See here.) \nFor those who would like to start with a toy version of the DEQ, the NeurIPS 2020 tutorial on \"Deep Implicit Layers\" has a detailed step-by-step introduction: tutorial video & colab notebooks here. \nA JAX version of the DEQ, including JAX implementation of Broyden's method, etc. is available here. \nThis repository contains the code for the deep equilibrium (DEQ) model, an implicit-depth architecture that directly solves for and backpropagtes through the (fixed-point) equilibrium state of an (effectively) infinitely deep network. Importantly, compared to prior implicit-depth approaches (e.g., ODE-based methods), in this work we also demonstrate the potential power and compatibility of this implicit model with modern, structured layers like Transformers, which enable the DEQ networks to achieve results on par with the SOTA deep networks (in NLP and vision) without using a \"deep\" stacking (and thus O(1) memory). Moreover, we also provide tools for regularizing the stability of these implicit models. \nSpecifically, this repo contains the code from the following papers (see bibtex at the end of this README): \n  - Deep Equilibrium Models \n  - Multiscale Deep Equilibrium Models \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9081263667755415,
        0.8855151281303947
      ],
      "excerpt": "We provide more detailed instructions for downloading/processing the datasets (WikiText-103, ImageNet, Cityscapes, etc.) in the DEQ-Sequence/ and MDEQ-Vision/ subfolders. \nStarting in 2021/6, we partition the repo into two sections, containing the sequence-model DEQ (i.e., DEQ-Sequence/) and the vision-model DEQ (i.e., MDEQ-Vision/) networks, respectively. As these two tasks require different input processing and loss objectives, they do not directly share the training framework.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9543414891558049,
        0.8402053054887708,
        0.8783774101107221
      ],
      "excerpt": "Moreover, the repo is significantly simplified from the previous version for users to extend on it. In particular,  \nTheorem 2 (Universality of \"single-layer\" DEQs, very informal): Stacking multiple DEQs  \n(with potentially different classes of transformations) does not create extra representational \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.987778655226904,
        0.8295432813707739
      ],
      "excerpt": "(See the paper for a formal statement.) By the theorem above, designing a better DEQ model boils down to designing a better stable transformation f_\\theta. Creating and playing with a DEQ is easy, and we recommend following 3 steps (which we adopt in this repo): \nTypically, this is just like any deep network layer, and should be a subclass of torch.nn.Module. Evaluating this layer requires the hidden unit z and the input injection x; e.g.: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9713989548578752,
        0.933065557377212,
        0.9787261144060408
      ],
      "excerpt": "The fixed-point formulation of DEQ models means their stability are directly characterized by the Jacobian matrix J_f at the equilibrium point. Therefore, we provide code for analyzing and regularizing the Jacobian properties (based on the ICML'21 paper Stabilizing Equilibrium Models by Jacobian Regularization). Specifically, we added the following flags to the training script: \njac_loss_weight: The strength of Jacobian regularization, where we regularize ||J_f||_F. \njac_loss_freq: The frequency p of the stochastic Jacobian regularization (i.e., we only apply this loss with probaility p during training). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9441623268453062
      ],
      "excerpt": "A full DEQ model implementation is therefore as simple as follows: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8852489272611612
      ],
      "excerpt": "    #: (Prepare for) Backward pass, see step 3 above \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8424411942909028
      ],
      "excerpt": "        #: Jacobian-related computations, see additional step above. For instance: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9822971794244353
      ],
      "excerpt": "            #: Compute the fixed point of yJ + grad, where J=J_f is the Jacobian of f at z_star \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9840382599256159
      ],
      "excerpt": "We provide PyTorch implementation of two generic solvers, broyden(...) (based on Broyden's method) and anderson(...) (based on Anderson acceleration) in lib/solvers.py. Both functions take in the transformation f whose fixed point we would like to solve for, and returns a dictionary of the following format: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8037802575414817,
        0.8037802575414817
      ],
      "excerpt": "Deep Equilibrium Models \nMultiscale Deep Equilibrium Models \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "[NeurIPS'19] Deep Equilibrium Models",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/locuslab/deq/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 53,
      "date": "Fri, 10 Dec 2021 14:01:42 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/locuslab/deq/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "locuslab/deq",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/locuslab/deq/master/DEQ-Sequence/wt103_deq_transformer_reg.sh",
      "https://raw.githubusercontent.com/locuslab/deq/master/DEQ-Sequence/wt103_deq_transformer.sh",
      "https://raw.githubusercontent.com/locuslab/deq/master/DEQ-Sequence/get_data.sh",
      "https://raw.githubusercontent.com/locuslab/deq/master/DEQ-Sequence/wt103_deq_transformer_preln_reg.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "As a DEQ model can use any *black-box* root solver. We provide PyTorch fixed-point solver implementations `anderson(...)` and `broyden(...)` in `lib/solvers.py` that output a dictionary containing the basic information of the optimization process. By default, we use the *relative residual difference* (i.e., |f(z)-z|/|z|) as the criterion for stopping the iterative process.\n\nThe forward pass can then be reduced to 2 lines:\n```python\nwith torch.no_grad():\n    #: x is the input injection; z0 is the initial estimate of the fixed point.\n    z_star = self.solver(lambda z: f(z, x, *args), z0, threshold=f_thres)['result']\n```\nwhere we note that the forward pass does not need to store **any** intermediate state, so we put it in a `torch.no_grad()` block.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8888627220377687
      ],
      "excerpt": "(Version 2.0 released now! :grinning:) \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8440897215149695
      ],
      "excerpt": " \"result\": ... (The closest estimate to the fixed point), \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/locuslab/deq/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 CMU Locus Lab\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Deep Equilibrium Models",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "deq",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "locuslab",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/locuslab/deq/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Python >= 3.6 and PyTorch >= 1.10. 4 GPUs strongly recommended for computational efficiency.\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 489,
      "date": "Fri, 10 Dec 2021 14:01:42 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "As a DEQ model can use any *black-box* root solver. We provide PyTorch fixed-point solver implementations `anderson(...)` and `broyden(...)` in `lib/solvers.py` that output a dictionary containing the basic information of the optimization process. By default, we use the *relative residual difference* (i.e., |f(z)-z|/|z|) as the criterion for stopping the iterative process.\n\nThe forward pass can then be reduced to 2 lines:\n```python\nwith torch.no_grad():\n    #: x is the input injection; z0 is the initial estimate of the fixed point.\n    z_star = self.solver(lambda z: f(z, x, *args), z0, threshold=f_thres)['result']\n```\nwhere we note that the forward pass does not need to store **any** intermediate state, so we put it in a `torch.no_grad()` block.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "Finally, we need to ensure there is a way to compute the backward pass of a DEQ, which relies on implicit function theorem. To do this, we can use the `register_hook` function in PyTorch that registers a backward hook function to be executed in the backward pass. As we noted in the paper, the backward pass is simply solving for the fixed point of a *linear system* involving the Jacobian at the equilibrium:\n```python\nnew_z_star = self.f(z_star.requires_grad_(), x, *args)\n\ndef backward_hook(grad):\n    if self.hook is not None:\n        self.hook.remove()\n        torch.cuda.synchronize()   #: To avoid infinite recursion\n    #: Compute the fixed point of yJ + grad, where J=J_f is the Jacobian of f at z_star\n    new_grad = self.solver(lambda y: autograd.grad(new_z_star, z_star, y, retain_graph=True)[0] + grad, \\\n                           torch.zeros_like(grad), threshold=b_thres)['result']\n    return new_grad\n\nself.hook = new_z_star.register_hook(backward_hook)\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}