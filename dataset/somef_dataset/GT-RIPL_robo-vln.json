{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* This code is built upon the implementation from [VLN-CE](https://github.com/jacobkrantz/VLN-CE)\n",
      "technique": "Header extraction"
    }
  ],
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2104.10674",
      "https://arxiv.org/abs/1911.00357",
      "https://arxiv.org/abs/2104.10674}\n}\n```\n\n## Acknowledgments\n* This code is built upon the implementation from [VLN-CE](https://github.com/jacobkrantz/VLN-CE"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "If you find this repository useful, please cite our paper:\n\n```\n@inproceedings{irshad2021hierarchical,\ntitle={Hierarchical Cross-Modal Agent for Robotics Vision-and-Language Navigation},\nauthor={Muhammad Zubair Irshad and Chih-Yao Ma and Zsolt Kira},\nbooktitle={Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},\nyear={2021},\nurl={https://arxiv.org/abs/2104.10674}\n}\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{irshad2021hierarchical,\ntitle={Hierarchical Cross-Modal Agent for Robotics Vision-and-Language Navigation},\nauthor={Muhammad Zubair Irshad and Chih-Yao Ma and Zsolt Kira},\nbooktitle={Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},\nyear={2021},\nurl={https://arxiv.org/abs/2104.10674}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8106378215134423
      ],
      "excerpt": "Hierarchical Cross-Modal Agent for Robotics Vision-and-Language Navigation<br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9970666188509012,
        0.9418407671070347
      ],
      "excerpt": "International Conference on Robotics and Automation (ICRA), 2021<br> \n[Project Page] [arXiv] [GitHub]  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "            'start_position': [10.257800102233887, 0.09358400106430054, -2.379739999771118], \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266
      ],
      "excerpt": "                [10.257800102233887, 0.09358400106430054, -2.379739999771118],  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9030859728368266,
        0.9030859728368266
      ],
      "excerpt": "            [10.257800102233887, 0.09358400106430054, -2.379739999771118], \n            [10.257800102233887, 0.09358400106430054, -2.379739999771118], \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "            [-12.644463539123535, 0.1518409252166748, 4.2241311073303220] \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/GT-RIPL/robo-vln",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-04-21T12:33:55Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-04T09:19:31Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9984391875101895
      ],
      "excerpt": "This repository is the pytorch implementation of our paper: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9441200927091015
      ],
      "excerpt": "Similar to Habitat-API, we expect a data folder (or symlink) with a particular structure in the top-level directory of this project. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8882716167186476
      ],
      "excerpt": "The Robo-VLN dataset is a continuous control formualtion of the VLN-CE dataset by Krantz et al ported over from Room-to-Room (R2R) dataset created by Anderson et al. The details regarding converting discrete VLN dataset into continuous control formulation can be found in our paper.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8743655691645721
      ],
      "excerpt": "| robo_vln_v1.zip     | data/datasets/robo_vln_v1           | 76.9 MB   | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9792738467357276
      ],
      "excerpt": "Similar to VLN-CE, our learning-based models utilizes a depth encoder pretained on a large-scale point-goal navigation task i.e. DDPPO. We utilize depth pretraining by using the DDPPO features from the ResNet50 from the original paper. The pretrained network can be downloaded here. Extract the contents of ddppo-models.zip to data/ddppo-models/{model}.pth. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8830435987553074,
        0.8198461653198219
      ],
      "excerpt": "EVAL.SPLIT  #: which dataset split to evaluate on (typically val_seen or val_unseen) \nEVAL.EPISODE_COUNT  #: how many episodes to evaluate \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9575210231472119
      ],
      "excerpt": "All our models require an off-line data buffer for training. To collect the continuous control dataset for both train and val_seen splits, run the following commands before training (Please note that it would take some time on a single GPU to store data. Please also make sure to dedicate around ~1.5 TB of hard-disk space for data collection): \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9386232831770897
      ],
      "excerpt": "Collect data buffer for val_seen split: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9567464196020847
      ],
      "excerpt": "| Seq2Seq | Sequence-to-Sequence. Please see our paper on modification made to the model to match the continuous action spaces in robo-vln                                                                                                                                                          | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.96816278389783
      ],
      "excerpt": "| CMA     | Cross-Modal Attention model. Please see our paper on modification made to the model to match the continuous action spaces in robo-vln                                                                                                                                                          | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Pytorch code for ICRA'21 paper: \"Hierarchical Cross-Modal Agent for Robotics Vision-and-Language Navigation\"",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/GT-RIPL/robo-vln/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 4,
      "date": "Tue, 07 Dec 2021 06:15:16 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/GT-RIPL/robo-vln/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "GT-RIPL/robo-vln",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Clone the current repository and required submodules:\n\n```bash\ngit clone https://github.com/GT-RIPL/robo-vln\ncd robo-vln\n  \nexport robovln_rootdir=$PWD\n    \ngit submodule init \ngit submodule update\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9292279388519822
      ],
      "excerpt": "<img src=\"demo/Pytorch_logo.png\" width=\"10%\">   \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9469897878213216
      ],
      "excerpt": "<img src=\"demo/ACMI_final.jpg\" width=\"100%\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.809409224375501,
        0.8969849947445017
      ],
      "excerpt": ": requires running with python 2.7 \npython download_mp.py --task habitat -o data/scene_datasets/mp3d/ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9210390879232014
      ],
      "excerpt": "<img src=\"demo/GIF.gif\" width=\"100%\"> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8146491994495434
      ],
      "excerpt": "| Dataset   | Path to extract               | Size      | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.86053270108505
      ],
      "excerpt": "| robo_vln_v1.zip     | data/datasets/robo_vln_v1           | 76.9 MB   | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8589534893990137
      ],
      "excerpt": "train: 7739 episodes \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9004131694594727
      ],
      "excerpt": "We use run.py script to train and evaluate all of our baseline models. Use run.py along with a configuration file and a run type (either train or eval) to train or evaluate: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9433728723808227
      ],
      "excerpt": "python run.py --exp-config path/to/config.yaml --run-type {train | eval} \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8894392000066461
      ],
      "excerpt": "All models can be evaluated using python run.py --exp-config path/to/config.yaml --run-type eval. The relevant config entries for evaluation are: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8270712954880892
      ],
      "excerpt": "EVAL.USE_CKPT_CONFIG  #: if True, use the config saved in the checkpoint file \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8030549249870454
      ],
      "excerpt": "Collect data buffer for train split: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9401401674791242
      ],
      "excerpt": "python run.py --exp-config robo_vln_baselines/config/paper_configs/robovln_data_train.yaml --run-type train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9401401674791242
      ],
      "excerpt": "python run.py --exp-config robo_vln_baselines/config/paper_configs/robovln_data_val.yaml --run-type train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.935679721687017
      ],
      "excerpt": "CUDA_VISIBLE_DEVICES=0,1 python run.py --exp-config robo_vln_baselines/config/paper_configs/hierarchical_cma.yaml --run-type train \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8363956025335797
      ],
      "excerpt": "| Seq2Seq            | 0.34         | 0.30           | seq2seq_robo.yaml                                                                                                                        | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8083872570519504
      ],
      "excerpt": "| Pre-trained Model     | Size      | \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/GT-RIPL/robo-vln/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2020 Jacob Krantz, Erik Wijmans, Arjun Majumdar, Dhruv Batra,\\nStefan Lee\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Hierarchical Cross-Modal Agent for Robotics Vision-and-Language Navigation",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "robo-vln",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "GT-RIPL",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "Organization",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/GT-RIPL/robo-vln/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Install `robo-vln` dependencies as follows:\n```bash\nconda create -n habitat python=3.6 cmake=3.14.0\ncd $robovln_rootdir\npython -m pip install -r requirements.txt\n```\n\nWe use modified versions of [Habitat-Sim](https://github.com/facebookresearch/habitat-sim) and [Habitat-API](https://github.com/facebookresearch/habitat-lab) to support continuous control/action-spaces in Habitat Simulator. The details regarding continuous action spaces and converting discrete VLN dataset into continuous control formulation can be found in our [paper](https://arxiv.org/pdf/2104.10674.pdf). The specific commits of our modified [Habitat-Sim](https://github.com/facebookresearch/habitat-sim) and [Habitat-API](https://github.com/facebookresearch/habitat-lab) versions are mentioned below.\n\n```bash\t\n#: installs both habitat-api and habitat_baselines\ncd $robovln_rootdir/environments/habitat-lab\npython -m pip install -r requirements.txt\npython -m pip install -r habitat_baselines/rl/requirements.txt\npython -m pip install -r habitat_baselines/rl/ddppo/requirements.txt\npython setup.py develop --all\n\t\n#: Install habitat-sim\ncd $robovln_rootdir/environments/habitat-sim\npython setup.py install --headless --with-cuda\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 29,
      "date": "Tue, 07 Dec 2021 06:15:16 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "deep-learning",
      "deep-neural-networks",
      "supervised-learning",
      "artificial-intelligence",
      "robotics",
      "computer-vision",
      "language",
      "transformers",
      "bert",
      "pytorch",
      "python",
      "habitat-api",
      "habitat-sim",
      "vision-and-language",
      "navigation",
      "vision-and-language-navigation"
    ],
    "technique": "GitHub API"
  }
}