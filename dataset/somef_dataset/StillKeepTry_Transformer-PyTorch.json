{
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{46201,\n  title = {Attention is All You Need},\n  author  = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},\n  year  = {2017},\n  booktitle = {Proc. of NIPS},\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@inproceedings{gehring2017convs2s,\n  author    = {Gehring, Jonas, and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N},\n  title     = \"{Convolutional Sequence to Sequence Learning}\",\n  booktitle = {Proc. of ICML},\n  year      = 2017,\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.8906174419333412
      ],
      "excerpt": "|25K jointly-sub-word|32.12| \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/StillKeepTry/Transformer-PyTorch",
    "technique": "GitHub API"
  },
  "contributingGuidelines": {
    "confidence": [
      1.0
    ],
    "excerpt": "Contributing to FAIR Sequence-to-Sequence Toolkit (PyTorch)\nWe want to make contributing to this project as easy and transparent as\npossible.\nPull Requests\nWe actively welcome your pull requests.\n\nFork the repo and create your branch from master.\nIf you've added code that should be tested, add tests.\nIf you've changed APIs, update the documentation.\nEnsure the test suite passes.\nMake sure your code lints.\nIf you haven't already, complete the Contributor License Agreement (\"CLA\").\n\nContributor License Agreement (\"CLA\")\nIn order to accept your pull request, we need you to submit a CLA. You only need\nto do this once to work on any of Facebook's open source projects.\nComplete your CLA here: https://code.facebook.com/cla\nIssues\nWe use GitHub issues to track public bugs. Please ensure your description is\nclear and has sufficient instructions to be able to reproduce the issue.\nCoding Style\nWe try to follow the PEP style guidelines and encourage you to as well.\nLicense\nBy contributing to FAIR Sequence-to-Sequence Toolkit, you agree that your contributions will be licensed\nunder the LICENSE file in the root directory of this source tree.",
    "technique": "File Exploration"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-03-01T11:43:30Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-10-04T09:23:07Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This project provides a PyTorch implementation about [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf) based on [fairseq-py](https://github.com/facebookresearch/fairseq-py) (An official toolkit of facebook research). You can also use official code about *Attention is all you need* from [tensor2tensor](https://github.com/tensorflow/tensor2tensor).\n\nIf you use this code about cnn, please cite:\n```\n@inproceedings{gehring2017convs2s,\n  author    = {Gehring, Jonas, and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N},\n  title     = \"{Convolutional Sequence to Sequence Learning}\",\n  booktitle = {Proc. of ICML},\n  year      = 2017,\n}\n```\nAnd if you use this code about transformer, please cite:\n```\n@inproceedings{46201,\n  title = {Attention is All You Need},\n  author  = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},\n  year  = {2017},\n  booktitle = {Proc. of NIPS},\n}\n```\nFeel grateful for the contribution of the facebook research and google research. **Besides, if you get benefits from this repository, please give me a star.**\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.8490045212471349
      ],
      "excerpt": "This dataset contains 160K training sentences. We recommend you to use transformer_small setting. The beam size is set as 5. The results are as follow: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.863543787424266,
        0.9219225405797511
      ],
      "excerpt": "Please try more checkpoint, not only the last checkpoint. \nThis dataset contains 1.25M training sentences. We learn a 25K subword dictionary for source and target languages respectively. We adopt a transformer_base model setting. The results are as follow: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9556971658306854
      ],
      "excerpt": "This dataset include 36M sentence pairs. We learned a 40K BPE for english and french. Beam size = 5. And 8 GPUs are used in this task. For base model setting, the batch size is 4000 for each gpu. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9035583361044265
      ],
      "excerpt": "And For big model, the batch size is 3072 for each gpu. The result is as: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9693663478441276,
        0.8415519207136886
      ],
      "excerpt": "This project is only maintained by myself. Therefore, there still exists some minor errors in code style. \nInstead of adam, i try NAG as the default optimizer, i find this optimized method can also produce better performance. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9516746110443903
      ],
      "excerpt": "Our many works are built upon this project, include: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "A PyTorch implementation of Attention is all you need",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/StillKeepTry/Transformer-PyTorch/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Sun, 05 Dec 2021 19:23:49 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/StillKeepTry/Transformer-PyTorch/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "StillKeepTry/Transformer-PyTorch",
    "technique": "GitHub API"
  },
  "hasScriptFile": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/StillKeepTry/Transformer-PyTorch/master/run_iwslt14_transformer.sh",
      "https://raw.githubusercontent.com/StillKeepTry/Transformer-PyTorch/master/run_wmt14_enfr_transformer.sh",
      "https://raw.githubusercontent.com/StillKeepTry/Transformer-PyTorch/master/run_wmt14_enfr_transformer_big.sh",
      "https://raw.githubusercontent.com/StillKeepTry/Transformer-PyTorch/master/data/generate_wmt14en2fr.sh",
      "https://raw.githubusercontent.com/StillKeepTry/Transformer-PyTorch/master/data/prepare-wmt14en2fr.sh",
      "https://raw.githubusercontent.com/StillKeepTry/Transformer-PyTorch/master/data/prepare-iwslt14.sh",
      "https://raw.githubusercontent.com/StillKeepTry/Transformer-PyTorch/master/data/generate_wmt14en2de.sh",
      "https://raw.githubusercontent.com/StillKeepTry/Transformer-PyTorch/master/data/prepare-wmt14en2de.sh"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "You first need to install PyTorch >= 0.4.0 and Python = 3.6. And then\n```\npip install -r requirements.txt\npython setup.py build\npython setup.py develop\n```\n\nGenerating binary data, please follow the script under [data/](data/), i have provide a [run script](run_iwslt14_transformer.sh) for iwslt14.\n\n",
      "technique": "Header extraction"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/StillKeepTry/Transformer-PyTorch/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python",
      "Shell",
      "Lua",
      "C++"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "Other",
      "url": "https://raw.githubusercontent.com/StillKeepTry/Transformer-PyTorch/master/LICENSE"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'BSD License\\n\\nFor fairseq software\\n\\nCopyright (c) 2017-present, Facebook, Inc. All rights reserved.\\n\\nRedistribution and use in source and binary forms, with or without modification,\\nare permitted provided that the following conditions are met:\\n\\n * Redistributions of source code must retain the above copyright notice, this\\n    list of conditions and the following disclaimer.\\n\\n * Redistributions in binary form must reproduce the above copyright notice,\\n    this list of conditions and the following disclaimer in the documentation\\n       and/or other materials provided with the distribution.\\n\\n * Neither the name Facebook nor the names of its contributors may be used to\\n    endorse or promote products derived from this software without specific\\n       prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR\\nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\\nANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Introduction",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "Transformer-PyTorch",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "StillKeepTry",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/StillKeepTry/Transformer-PyTorch/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 41,
      "date": "Sun, 05 Dec 2021 19:23:49 GMT"
    },
    "technique": "GitHub API"
  }
}