{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1312.6114",
      "https://arxiv.org/abs/1610.00291",
      "https://arxiv.org/abs/1711.00848",
      "https://arxiv.org/abs/1706.02262",
      "https://arxiv.org/abs/1802.04942",
      "https://arxiv.org/abs/2002.02886",
      "https://arxiv.org/abs/2002.02886",
      "https://arxiv.org/abs/1802.04403",
      "https://arxiv.org/abs/1802.05983",
      "https://arxiv.org/abs/1802.04942",
      "https://arxiv.org/abs/1711.00848",
      "https://arxiv.org/abs/1910.05587",
      "https://arxiv.org/abs/1802.05312",
      "https://arxiv.org/abs/1903.10145",
      "https://arxiv.org/abs/1903.10145\nmodule.register_schedule(\n  'beta', CyclicSchedule(\n    period=1024,  # repeat every: trainer.global_step % period\n  ",
      "https://arxiv.org/abs/2002.02886"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Please use the following citation if you use Disent in your own research:\n\n```bibtex\n@Misc{Michlo2021Disent,\n  author =       {Nathan Juraj Michlo},\n  title =        {Disent - A modular disentangled representation learning framework for pytorch},\n  howpublished = {Github},\n  year =         {2021},\n  url =          {https://github.com/nmichlo/disent}\n}\n```\n\n----------------------\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@Misc{Michlo2021Disent,\n  author =       {Nathan Juraj Michlo},\n  title =        {Disent - A modular disentangled representation learning framework for pytorch},\n  howpublished = {Github},\n  year =         {2021},\n  url =          {https://github.com/nmichlo/disent}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9944484218006108,
        0.9966489686656222
      ],
      "excerpt": "+ [DCIMIG](https://arxiv.org/abs/1910.05587) \n+ [Modularity and Explicitness](https://arxiv.org/abs/1802.05312) \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/nmichlo/disent",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-04-16T16:34:17Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-06T06:48:00Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9367360419139207,
        0.8218718659133779
      ],
      "excerpt": "Disent is a modular disentangled representation learning framework for auto-encoders, \nbuilt upon PyTorch-Lightning. This framework consists of various composable components \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.828212701700259
      ],
      "excerpt": "The name of the framework is derived from both disentanglement and scientific dissent. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8591283519046155
      ],
      "excerpt": "1. Provide high quality, readable, consistent and easily comparable implementations of frameworks \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8362272469384888,
        0.8977073986497588
      ],
      "excerpt": "disent.dataset.transform: common data transforms and augmentations \ndisent.dataset.wrapper: wrapped datasets are no longer ground-truth datasets, these may have some elements masked out. We can still unwrap these classes to obtain the original datasets for benchmarking. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.882071674974554,
        0.839619163717452
      ],
      "excerpt": "disent.metrics: metrics for evaluating disentanglement using ground truth datasets \ndisent.model: common encoder and decoder models used for VAE research \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8328073927122458
      ],
      "excerpt": "disent.util: helper classes, functions, callbacks, anything unrelated to a pytorch system/model/framework. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8771919652103536,
        0.8492839131224635
      ],
      "excerpt": "Disent is still under active development. Features and APIs are mostly stable but may change! A limited \nset of tests currently exist which will be expanded upon in time. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9286587077695976
      ],
      "excerpt": "Disent includes implementations of modules, metrics and \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9427828245924801
      ],
      "excerpt": "  with a \"\ud83e\uddf5\" are introduced in and are unique to disent! \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9035353471779038
      ],
      "excerpt": "verification and automatic chunk-size optimization of underlying hdf5 formats for \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9474491512340959
      ],
      "excerpt": "\ud83e\uddf5 XYObject: A simplistic version of dSprites with a single square. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9374461142827121
      ],
      "excerpt": "\ud83e\uddf5 DSpritesImagenet: Version of DSprite with foreground or background deterministically masked out with tiny-imagenet data \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.824119622254291
      ],
      "excerpt": "- Input based transforms are supported. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8641745100165491,
        0.8167016748029416
      ],
      "excerpt": "Hyper-parameter annealing is supported through the use of schedules. \nThe currently implemented schedules include: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.854450164611435
      ],
      "excerpt": "Cosine Wave Schedule \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8824356632095415
      ],
      "excerpt": "- Created as part of my Computer Science MSc scheduled for completion in 2021. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "\ud83e\uddf6 Modular VAE disentanglement framework for python built with PyTorch Lightning. Easily configured and run with Hydra config. Including metrics and datasets, with strong, weakly supervised and unsupervised methods. Early library design based off disentanglement_lib.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/nmichlo/disent/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Sun, 12 Dec 2021 19:01:26 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/nmichlo/disent/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "nmichlo/disent",
    "technique": "GitHub API"
  },
  "hasDocumentation": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://github.com/nmichlo/disent/tree/main/docs"
    ],
    "technique": "File Exploration"
  },
  "installation": [
    {
      "confidence": [
        0.9841887178782714
      ],
      "excerpt": "Get started with disent by installing it with $pip install disent or cloning this repository. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9816914741436259
      ],
      "excerpt": "are not available from pip install. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9350027058690061
      ],
      "excerpt": "- A pytorch version of disentanglement_lib. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8860016370382856,
        0.8427677765548
      ],
      "excerpt": "Python Example \nHydra Config Example \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8304382061238806
      ],
      "excerpt": "disent.dataset.data: raw datasets \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9204210353881815
      ],
      "excerpt": "experiment/run.py: entrypoint for running basic experiments with hydra config \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8840880317771688
      ],
      "excerpt": "    <img width=\"384\" src=\"docs/img/xy-object-traversal.png\" alt=\"XYObject Dataset Factor Traversals\"> \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/nmichlo/disent/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "# Table Of Contents",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "disent",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "nmichlo",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/nmichlo/disent/blob/main/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "nmichlo",
        "body": "## Fixes\r\n- `disent.util.math` was not a module, added empty `__init__.py` file",
        "dateCreated": "2021-11-28T23:12:22Z",
        "datePublished": "2021-11-28T23:14:02Z",
        "html_url": "https://github.com/nmichlo/disent/releases/tag/v0.3.3",
        "name": "v0.3.3",
        "tag_name": "v0.3.3",
        "tarball_url": "https://api.github.com/repos/nmichlo/disent/tarball/v0.3.3",
        "url": "https://api.github.com/repos/nmichlo/disent/releases/54203503",
        "zipball_url": "https://api.github.com/repos/nmichlo/disent/zipball/v0.3.3"
      },
      {
        "authorType": "User",
        "author_name": "nmichlo",
        "body": "## Fixes\r\n\r\n- Fix `FftKernel`, accidentally forgot to freeze tensor weights.\r\n- Fix callbacks logging l1 instead of l2 distance\r\n- Fix callbacks failure if metrics are NaN\r\n- dsprites_imagenet macos prepare fix\r\n\r\n## Added\r\n- `run_action=skip` experiment action to just test if hydra is working.\r\n- VAEs now log the ratios between different loss terms.\r\n\r\n## Breaking\r\n- `experiment.run.hydra_check_cuda` renamed to `hydra_get_gpus`. Now returns an integer for the number of GPUs to use. Intended to be passed to a PyTorch Lightning Trainer.\r\n- Removed `XYObjectData` warning that things are now different\r\n\r\n\r\n",
        "dateCreated": "2021-11-22T12:12:15Z",
        "datePublished": "2021-11-22T12:21:02Z",
        "html_url": "https://github.com/nmichlo/disent/releases/tag/v0.3.2",
        "name": "v0.3.2",
        "tag_name": "v0.3.2",
        "tarball_url": "https://api.github.com/repos/nmichlo/disent/tarball/v0.3.2",
        "url": "https://api.github.com/repos/nmichlo/disent/releases/53835152",
        "zipball_url": "https://api.github.com/repos/nmichlo/disent/zipball/v0.3.2"
      },
      {
        "authorType": "User",
        "author_name": "nmichlo",
        "body": "## Experiment Fixes\r\n- `run_action=prepare_data` has been fixed\r\n\r\n## Experiment Additions\r\n-  new tests to ensure this continues to work properly\r\n\r\n## Experiment Changes\r\n- correct action is now chosen via the `experiment.run.run_action(cfg)` method\r\n  + `experiment.run.train` renamed to `action_train`\r\n  + `experiment.run.prepare_data` renamed to `action_prepare_data`\r\n- input config is no longer mutated\r\n",
        "dateCreated": "2021-11-11T10:10:14Z",
        "datePublished": "2021-11-11T10:14:13Z",
        "html_url": "https://github.com/nmichlo/disent/releases/tag/v0.3.1",
        "name": "v0.3.1",
        "tag_name": "v0.3.1",
        "tarball_url": "https://api.github.com/repos/nmichlo/disent/tarball/v0.3.1",
        "url": "https://api.github.com/repos/nmichlo/disent/releases/53154851",
        "zipball_url": "https://api.github.com/repos/nmichlo/disent/zipball/v0.3.1"
      },
      {
        "authorType": "User",
        "author_name": "nmichlo",
        "body": "This release touches most of the codebase.\r\n\r\n## Major Additions\r\n- added `XYObjectShadedData` dataset, which is exactly the same as `XYObjectData` but the ground truth factors differ. This might be useful for testing how metrics are affected by the ground truth representation of factors. Note that XYObjectData differs from previous versions due to this.\r\n- added `DSpritesImagenetData` dataset that is the same as `DSpritesData` but masks that background or foreground depending on the mode and replaces the content with deterministic data from tiny-imagenet\r\n- added `disent.framework.vae.AdaGVaeMinimal` which is a minimal implementation of `AdaVae` configured to run in `gvae`\r\n- added `disent.util.lightning.callbacks.VaeGtDistsLoggingCallback` which logs various distances matrices computed from averaged ground truth factor traversals.\r\n- Updated experiment files to use hydra 1.1\r\n  + can now switch between `train` and `prepare_data` modes with the defaults group `run_action=train`\r\n\r\n## Other Additions\r\n- added `shallow_copy` to `disent.dataset.DisentDataset` enabling a shallow copy of the dataset but overriding specific properties such as the transform\r\n- added new `disent.dataset.transform` including `ToImgTensorF32` (was `ToStandardisedTensor `) and `ToImgTensorU8`\r\n- additions to `H5Builder`\r\n    + `add_dataset_from_array` that constructs and fills a dataset in the hdf5 file from an array\r\n    + converted into context manager instead of manually opening the hdf5 file\r\n- additions to `StateSpace` (and ground truth dataset child classes)\r\n   + `normalise_factor_idx` convert names of ground truth factors into the numerical value\r\n   + `normalise_factor_idxs` convert a name, an idx, lists of names, or lists of idxs to the numerical values of the ground truth factors.\r\n- `disent.dataset.util.stats` added `compute_data_mean_std(data)` to compute the mean and std of datasets\r\n- added `disent.schedule.SingleSchedule`\r\n- improved `disent.util.deprecate.deprecated`, now prints the stack trace for the call location of the deprecated function by default. This can be disabled.\r\n- added `restart` method  to `disent.util.profiling.Timer` for easy use within a loop\r\n- added `disent.util.vizualize.plot` which contains various matplotlib helper code used throughout the library and PyTorch lightning callbacks.\r\n\r\n## Breaking Changes\r\n- removed confusing `observation_shape` and `obs_shape` properties from `GroundTruthData` and any child classes. Any methods that require these properties across disent had their names update too. For example the `ArrayGroundTruthData` class now takes `x_shape`.\r\n    + `observation_shape` `(H, W, C)` should be replaced with `img_shape`, you will need to update your overrides in child classes\r\n    + `obs_shape` `(C, H, W)` should be replaced with `x_shape`\r\n- `XYObjectData` default parameters updated for `XYObjectShadedData `, dataset and colour palettes differs slightly from previous versions.\r\n- moved module `disent.nn.transform` to `disent.dataset.transform`\r\n    + renamed `ToStandardisedTensor` to `ToImgTensorF32`\r\n- `H5Builder` converted into context manager, similar API to `open` or `h5py.File`\r\n- `ReconLossHandlerMse` changed to not scale or centre the output, this is because we now normalise the data instead which is more correct\r\n- `AdaVae` and inheriting classes have various functions renamed for clarity\r\n- `disent.metrics` functions have `ground_truth_dataset` parameter renamed to `dataset`\r\n-  `disent.model.ae` renamed `DecoderTest` and `EncoderTest` to `DecoderLinear` and `EncoderLinear`\r\n- `disent.registry` updated registry to use new more simple class structure and format. Some variables have been renamed, and registry names have been changed to plurals, eg. `OPTIMIZER` is now `OPTIMIZERS`\r\n- `disent.schedule` cleaned up\r\n  + renamed various variables and parameters `min_step` -> `start_step`, `max_step` -> `end_step`\r\n  + removed `disent.schedule.lerp.scale()` function, as it is the same as `lerp` just not clipped\r\n- `disent.util.lightning.callbacks.VaeDisentanglementLoggingCallback` renamed to `VaeMetricLoggingCallback`\r\n- `docs.examples` updated to use new `XYObjectData` version and `ToImgTensorF32` transform\r\n\r\n## Deprecations\r\n- deprecated `ground_truth_data` property on `DisentDataset `, this should be replaced with the shorter `gt_data` property. References to `ground_truth_data ` have been replaced in disent.\r\n\r\n## Fixes\r\n- Fixed `Mpi3dData` datasets, and added file hashes\r\n- Updated requirements\r\n- Many minor fixes, usability and error message improvements\r\n\r\n## Hydra Experiment Changes\r\n\r\nHydra Config has finally been updated from version 1.0 to 1.1, adding support for recursive defaults and recursive instantiation. This allows is to remove all of our custom & hacky hydra helper code that previously enabled these features.\r\n- hydra now supports recursive instantiation\r\n- value based specialisation can now be done with recursive defaults using dummy groups\r\n\r\nUpdating hydra was a good opportunity to re-structure the configuration format.\r\n- All settings defined in the root config that are referenced elsewhere are now in the `settings` key.\r\n- Default settings defined in various subgroups that are referenced elsewhere are often placed in the `dsettings` key.\r\n- Keys for various objects were renamed for clarity, eg. `augment.transform` was renamed to `augment.augment_cls`\r\n- All datasets now require the `meta.vis_mean` and `meta.vis_std` keys that are used both to normalise the dataset and used to re-scale it between [0, 1] for visualisation during training.\r\n\r\nEvery config file has been touched, the best approach is probably to look at the new system. The general structure remains the same, but the recursive defaults from Hydra 1.1 allows us to implement various things in a more clean way.\r\n- new defaults group `run_launcher` to easily swap between `slurm` and `local`\r\n- defaults group `run_location` only specifies machine resources and paths\r\n- new defaults group `sampling` specifies details and the sampling strategy to be used by the frameworks\r\n- new defaults group `run_action` to switch between `training` and downloading and installing datasets `prepare_data`\r\n",
        "dateCreated": "2021-11-11T08:13:19Z",
        "datePublished": "2021-11-11T09:12:01Z",
        "html_url": "https://github.com/nmichlo/disent/releases/tag/v0.3.0",
        "name": "v0.3.0",
        "tag_name": "v0.3.0",
        "tarball_url": "https://api.github.com/repos/nmichlo/disent/tarball/v0.3.0",
        "url": "https://api.github.com/repos/nmichlo/disent/releases/53150878",
        "zipball_url": "https://api.github.com/repos/nmichlo/disent/zipball/v0.3.0"
      },
      {
        "authorType": "User",
        "author_name": "nmichlo",
        "body": "Under the hood, quite a lot of code has been added or changed for this release, however the API remains very much the same.\r\n\r\n**Additions**\r\n- Wrapped datasets, instances of `disent.dataset.wrapper.WrappedDataset` are datasets that have some sort of mask applied to them that hides the true state space and resizes the dataset.\r\n    + `disent.dataset.wrapper.DitheredDataset` applies an n-dimensional dithering operation to ground truth factors\r\n    + `disent.dataset.wrapper.MaskedDataset` applies some provided boolean mask over the dataset\r\n- `disent.dataset.DisentDataset` now supports wrapped datasets (instances of `disent.dataset.wrapper.WrappedDataset`). New methods and properties have been added to compliment this feature:\r\n    + `is_wrapped_data` check if there is wrapped data\r\n    + `is_wrapped_gt_data` check if there is wrapped data and the wrapped data is ground truth data\r\n    + `wrapped_data` obtain the wrapped data, otherwise throw an error\r\n    + `wrapped_gt_data` obtain the wrapped ground truth data, otherwise throw an error\r\n    + `unwrapped_disent_dataset` creates a copy of the disent dataset with everything the same, except the data is unwrapped.\r\n- `disent.util.lightning.callbacks` additions\r\n    + Support for wrapped datasets. They automatically try to unwrap them to obtain the ground truth data which can be used to compute metrics and perform visualisations.\r\n    + Support model output scaling to a certain range of values, fixing visualisations when using `VaeLatentCycleLoggingCallback`\r\n- new utilities\r\n    +  `disent.util.math.dither`\r\n    +  `disent.util.math.random`\r\n- Self contained HDF5 ground-truth datasets. These store all the information needed to construct the dataset and state space in one file, including the factor names.\r\n    + Added `disent.dataset.data.SelfContainedHdf5GroundTruthData` to read these files\r\n    + Added `disent.dataset.util.H5Builder` for creating these files. (API is not yet finalised)\r\n- `disent.dataset.util.StateSpace` added helper function `iter_traversal_indices`\r\n- `disent.nn.transform` added `ToUint8Tensor` which acts like `ToStandardisedTensor`, but instead of loading images as `float32`, it loads them as `uint8`. This is useful when you need to use datasets outside of a ML Model context, eg. performing analysis. This takes up less memory.\r\n    + corresponding functional version exists `to_uint_tensor` complimenting `to_standardised_tensor`\r\n- Begun work on a component & function registry, although do not use this as the API will change significantly.\r\n- \r\n\r\n**API Breakages**\r\n- Under the hood, implementing wrapped data and `DisentDataset` copying requires the ability to copy samplers, so each sampler implementation should have the `uninit_copy` method implemented too.\r\n- `ArrayGroundTruthData` is more strict about the `observation_shape` must be `(H, W, C)` or `(C, H, W)` depending on `array_chn_is_last`\r\n- Removed reconstruction losses:\r\n    + `ReconLossHandlerMse4` aka. `\"mse4\"`\r\n    + `ReconLossHandlerMae2` aka. `\"mae2\"`\r\n- Renamed `disent.util.visualize.get_factor_traversal` to `get_idx_traversal`\r\n\r\n**Deprecations**\r\n- `GroundTruthData` property aliases:\r\n    + `img_shape` new property for the deprecated `observation_shape`\r\n    + `obs_shape` new property for the deprecated `x_shape`\r\n    + `img_channels` new property for the number of channels in the image\r\n\r\n**Fixes**\r\n- `disent.util.inout.files.AtomicSaveFile` minor fix to overwriting files\r\n- `disent.util.lightning.callbacks.LoggerProgressCallback` fix to datatypes and potential crashes due to floats\r\n- More stable experiment runs when performing sweeps. Better error handling, error messages and error catching.\r\n- fixes to the various `requirement*.txt` files\r\n- many other minor fixes",
        "dateCreated": "2021-10-04T23:21:36Z",
        "datePublished": "2021-10-04T23:53:26Z",
        "html_url": "https://github.com/nmichlo/disent/releases/tag/v0.2.1",
        "name": "v0.2.1",
        "tag_name": "v0.2.1",
        "tarball_url": "https://api.github.com/repos/nmichlo/disent/tarball/v0.2.1",
        "url": "https://api.github.com/repos/nmichlo/disent/releases/50779192",
        "zipball_url": "https://api.github.com/repos/nmichlo/disent/zipball/v0.2.1"
      },
      {
        "authorType": "User",
        "author_name": "nmichlo",
        "body": "**API Breakages**\r\n- `DisentFramework` no longer takes in `make_optimizer_fn` callback, but instead includes this as part of the `cfg` by specifying `optimizer` and `optimizer_kwargs`.\r\n- `Ae` derived subclasses now take in an instantiated `AutoEncoder` instance to the `model` param instead of the `make_model_fn` callback.\r\n\r\n**Additions**\r\n- `DisentDataset` can now return observation indices in the `\"idx\"` field if `return_indices=True`\r\n- `sample_random_obs_traversal` added to `GroundTruthData`\r\n- new basic experiment test\r\n\r\n**Chages**\r\n- python 3.8 and 3.9 support (3.7 is unsupported due to missing standard library typing features)\r\n- `TempNumpySeed` now inherits from `contextlib.ContextDecorator`\r\n- updated `hydra-core` to `1.0.7`\r\n\r\n**Fixes**\r\n- `SmallNorbData` by default now returns observations of size `(96, 96, 1)` instead of `(96, 96)`\r\n- Removed `Deprecated` dependency which also couldn't be pickled, fixing hydra submittit issues\r\n- `LoggerProgressCallback` displays more reliable information and now supports PyTorch Lightning 1.4\r\n- `HydraDataModule` now supports PyTorch Lightning 1.4\r\n- `merge_specializations` fixed to depend on OmegaConf not Hydra\r\n\r\n\r\n",
        "dateCreated": "2021-10-04T14:21:28Z",
        "datePublished": "2021-10-04T14:34:46Z",
        "html_url": "https://github.com/nmichlo/disent/releases/tag/v0.2.0",
        "name": "v0.2.0",
        "tag_name": "v0.2.0",
        "tarball_url": "https://api.github.com/repos/nmichlo/disent/tarball/v0.2.0",
        "url": "https://api.github.com/repos/nmichlo/disent/releases/50747830",
        "zipball_url": "https://api.github.com/repos/nmichlo/disent/zipball/v0.2.0"
      },
      {
        "authorType": "User",
        "author_name": "nmichlo",
        "body": "# Initial Release\r\n\r\n## Overview\r\n\r\nThe initial release of **Disent**\r\n- please see the docs and readme for new usage examples, changes should be easy to make to existing code, notably the `DisentDataset` and `DisentSampler` changes.\r\n\r\n### Changes\r\n\r\n+ Replaced sampling datasets with one common class `disent.dataset.DisentDataset`\r\n    + Wraps other datasets (`torch.utils.data.Dataset` or `disent.dataset.data.GroundTruthData`)\r\n    + Accepts an implemented subclass of `disent.dataset.sampling.BaseDisentSampler` which controls how many observations are sampled and returned (eg. for triplet networks).\r\n    + eg. `disent.dataset.groundtruth.GroundTruthDatasetPairs` is now `disent.dataset.sampling.GroundTruthPairSampler`\r\n\r\n- Removed all experimental code & features unique to Disent. Hydra configs and runners for non-experimental features remain. These features will be cleaned up and re-added once I submit my dissertation.\r\n    - \u274c  experimental frameworks\r\n    - \u274c  experimental datasets\r\n    - \u274c  experimental metrics\r\n    - \u274c  experimental models\r\n    - \u274c  experimental augmentations\r\n    - \u274c  experiment files\r\n\r\n+ Verified models\r\n   - some models had potentially diverged from their original implementations and papers.\r\n   - Added a new test model: EncoderTest & DecoderTest\r\n\r\n+ `disent.nn` Changes:\r\n   - Added `disent.nn.activations.Swish`\r\n   - Removed loss reduction mode `\"sum\"` in `disent.nn.loss.reduction`\r\n   - Split out triplet mining logic from frameworks into `torch.nn.loss.triplet_mining`\r\n   - Replaced `from torch.nn.modules import BatchView, Unsqueeze3D, Flatten3D` with pytorch 1.9 equivalents\r\n   - Backwards compatible opt-in `disent.nn.transform.ToStandardisedTensor` enhancements\r\n\r\n+ `disent.util` Refactor, grouping logic into submodules:\r\n   - `disent.util.inout`: utilities for working with paths, files and saving files.\r\n   - `disent.util.lightning`: various helper functions and callbacks for pytorch lightning, some incorperated from past experiment files.\r\n   - `disent.util.strings`: utilities for working with strings and ansi escape codes\r\n   - `disent.util.visualise`: moved `disent.visualise` into this module, separating framework logic and helper logic in disent.\r\n\r\n+ Cleaned up `requirements.txt`\r\n   - optional requirements moved into: `requirements-test.txt` and `requirements-exp.txt`\r\n\r\n+ New tests\r\n   - samplers\r\n   - models\r\n\r\n+ And a many bug-fixes   ",
        "dateCreated": "2021-07-28T12:32:15Z",
        "datePublished": "2021-07-28T12:43:29Z",
        "html_url": "https://github.com/nmichlo/disent/releases/tag/v0.1.0",
        "name": "v0.1.0 - Initial Release",
        "tag_name": "v0.1.0",
        "tarball_url": "https://api.github.com/repos/nmichlo/disent/tarball/v0.1.0",
        "url": "https://api.github.com/repos/nmichlo/disent/releases/46901943",
        "zipball_url": "https://api.github.com/repos/nmichlo/disent/zipball/v0.1.0"
      },
      {
        "authorType": "User",
        "author_name": "nmichlo",
        "body": "## Overview\r\n\r\nThis release is mostly a large set of refactors, and reproducibility improvements with regards to seeds and datasets.\r\n\r\n### Notable Changes\r\n- Data now relies on `disent.data.datafile.DataFile`s, which are deterministic, hash and cache based, file generators that can fetch or pre-process data.\r\n- Added `XYSquaresMinimalData`, which is a minimal faster version of `XYSquaresData` without any configuration options. With default parameters, data from `XYSquaresData` should equal `XYSquaresMinimalData` \r\n- Added `PickleH5pyFile` that can pickle an hdf5 file and dataset. This is intended to be used with `torch` `DataLoader`s or multiprocessing.\r\n\r\n### _Definitely_ Breaking Changes\r\n- renamed classes:\r\n    + renamed `AugmentableDataset` to `DisentDataset`\r\n    + renamed `BaseFramework` to `DisentFramework`\r\n    + renamed `BaseEncoderModule` to `DisentEncoder`\r\n    + renamed `BaseDecoderModule` to `DisentDecoder`\r\n\r\n- consolidated maths and helper functions into new submodule `disent.nn`\r\n    + `disent.nn.weights` initialisation functions from originally `disent.model.init`\r\n    + `disent.nn.modules` basic modules from various locations including `DisentModule`, `DisentLightningModule`, `BatchView`, `Unsqueeze3D`, `Flatten3D`\r\n    + `disent.nn.transform` transform and augment functions and classes from `disent.transform`, still needs to be cleaned up in future releases.\r\n    + `disent.nn.loss` various loss functions from other places including `triplet`, `kl`, `softsort` and `reduction` modules\r\n    + `torch.nn.functional` various differentiable torch helper functions mostly from `disent.util.math`, including functions for computing the Covariance, Correlation, Generalised Mean, PCA, DCT, Channel-Wise convolutions and more! Some functions such as kernel generation need to be moved out of here.\r\n\r\n- split up and consolidated utilities:\r\n    + `disent.util.cache` caching utilities including the `stalefile` decorator that only runs the wrapped function if the specified file is stale (hash does not match, or file does not exist)\r\n    + `disent.util.colors` ANSI escape codes\r\n    + `disent.util.function` wrapper, decorator and inspect utilities\r\n    + `disent.util.hashing` compute the `full` hash of a file or a `fast` hash based on the README for the [imohash](https://github.com/kalafut/py-imohash) algorithm.\r\n    + `disent.util.in_out` originally from `disent.data.util` for handling file retrieval/downloading/copying and saving\r\n    + `disent.util.iters` general iterators or map functions, including `iter_chunks` and `iter_rechunk`\r\n    + `disent.util.paths` path handling and file or directory management\r\n    + `disent.util.profiling` timers & memory usage\r\n    + `disent.util.seeds` seed management contexts and functions\r\n    + `disent.util.strings` string formatting helper functions\r\n\r\n- removed and cleaned up functions from:\r\n    + `disent.data.hdf5`\r\n    +  `disent.dataset.__init__`\r\n    +  `disent.util.__init__`\r\n    + `disent.schedule.lerp` renamed `activate` to `scale_ratio` and removed other functions.\r\n\r\n### Other Changes\r\n- Replaced `GroundTruthData` specialisations with general loading from `DataFile`s.\r\n- `StateSpace` now stores `factor_names` instead of `GroundTruthData` - preparing for rewrite of datasets to use dependency injections and samplers.\r\n\r\n### Experiment Config & Runner Changes\r\n- Many config fixes for refactors\r\n- Experiment can now be seeded\r\n\r\n### New Tests\r\n- test `PickleH5pyFile` multiprocessing support\r\n- test `XYSquaresData` and `XYSquaresMinimalData` similarity",
        "dateCreated": "2021-06-04T22:14:25Z",
        "datePublished": "2021-06-04T23:08:30Z",
        "html_url": "https://github.com/nmichlo/disent/releases/tag/v0.0.1.dev14",
        "name": "v0.0.1.dev14",
        "tag_name": "v0.0.1.dev14",
        "tarball_url": "https://api.github.com/repos/nmichlo/disent/tarball/v0.0.1.dev14",
        "url": "https://api.github.com/repos/nmichlo/disent/releases/44135837",
        "zipball_url": "https://api.github.com/repos/nmichlo/disent/zipball/v0.0.1.dev14"
      },
      {
        "authorType": "User",
        "author_name": "nmichlo",
        "body": "**Notable Changes:**\r\n- new Auto-Encoders:\r\n    + `Ae`\r\n    + `TripletAe` (`Ae` version of `TripletVae`)\r\n    + `AdaAe` (`Ae` version of `AdaVae`)\r\n    + `AdaNegTripletAe` (`Ae` version of `AdaNegTripletVae`)\r\n- custom dataset MNIST example in the docs\r\n\r\n**Breaking Changes**\r\n- flattened `disent.frameworks.vae` and `disent.frameworks.ae` modules, `unsupervised`, `weaklysupervised`, and `supervised` submodules no longer exist.\r\n- remove latent parameter classes from VAEs, VAEs now directly encode distributions with the `encode_dists()` function, this simplified a lot of other code.\r\n- Datasets now only return `'x'` in the observation dictionary if an `augment` is specified, ~5% performance boost\r\n- some dependencies are optional, more work is still required to minimise dependencies\r\n- Removed `sample_random_traversal_factors`, `sample_random_cycle_factors` from `StateSpace` and replaced with generic function `sample_random_factor_traversal`\r\n- renamed all autoencoders `AE` to `Ae`\r\n\r\n**Other Changes:**\r\n- hdf5 dataset performance fix, now up to 5x faster when not loaded into memory\r\n- all Auto-Encoders have new config options to disable the augment loss, recon loss, or detach the decoder so that no loss flows back through the encoder. VAEs can additionally have the regularisation loss disabled.\r\n- new `laplace` latent distribution, can be specified in VAE configs.\r\n- triplet loss helper functions\r\n- flatness components metric helper functions for use elsewhere: `compute_linear_score`, `compute_axis_score`\r\n- `FftKernel` augment module inheriting from `torch.nn.Module`, applies a channel-wise convolution to the input.\r\n- `to_standardised_tensor` fix for non-`PIL.Image.Image` inputs\r\n- more math helper functions:\r\n    + `torch_normalize` normalise values along an axis between 0 and 1\r\n    + `torch_mean_generalized` now supports the `keepdim` argument\r\n- `disent.visualise.visualise_module` removed old redundant code adapted from disentanglement_lib\r\n- `disent.visualise.visualise_util` additions\r\n    + `make_image_grid` and `make_animated_image_grid` auto-detect border colour from input dtype\r\n    + replaced `cycle_factor` with `get_factor_traversal` that accepts different modes: `interval` and `cycle`\r\n- cleaned up experiments\r\n\r\n**++ many more additions and minor fixes ++**",
        "dateCreated": "2021-05-26T14:00:52Z",
        "datePublished": "2021-05-26T14:59:34Z",
        "html_url": "https://github.com/nmichlo/disent/releases/tag/v0.0.1.dev13",
        "name": "v0.0.1.dev13",
        "tag_name": "v0.0.1.dev13",
        "tarball_url": "https://api.github.com/repos/nmichlo/disent/tarball/v0.0.1.dev13",
        "url": "https://api.github.com/repos/nmichlo/disent/releases/43604582",
        "zipball_url": "https://api.github.com/repos/nmichlo/disent/zipball/v0.0.1.dev13"
      },
      {
        "authorType": "User",
        "author_name": "nmichlo",
        "body": "Large Release\r\n\r\n+ utility additions\r\n    - dct\r\n    - kernels: gaussian + box\r\n    - conv2d channel wise\r\n    - differentiable sorting, spearman rank loss\r\n+ ground truth dataset with factors\r\n+ more reconstruction losses\r\n    - kernel reconstruction losses\r\n    - recon loss fixes\r\n    - parameterised recon losses\r\n+ scaled hard averaging for adatvae and adanegtvae\r\n+ DataOverlapRankVAE - uses differentiable sorting to optimise spearman rank correlation coefficient instead of triplet loss\r\n+ DataOverlapTripletVAE - fixes, simplifications, moved out triplet mining\r\n+ removed unnecessary metric values\r\n+ Conv64Alt encoder and decoder that support normalisation layers for faster convergence\r\n+ FFT gaussian and box blur augments\r\n+ more experiment schedules\r\n+ more experiments\r\n\r\nAnd much more...",
        "dateCreated": "2021-05-09T22:47:13Z",
        "datePublished": "2021-05-09T22:57:20Z",
        "html_url": "https://github.com/nmichlo/disent/releases/tag/v0.0.1.dev12",
        "name": "v0.0.1.dev12",
        "tag_name": "v0.0.1.dev12",
        "tarball_url": "https://api.github.com/repos/nmichlo/disent/tarball/v0.0.1.dev12",
        "url": "https://api.github.com/repos/nmichlo/disent/releases/42673749",
        "zipball_url": "https://api.github.com/repos/nmichlo/disent/zipball/v0.0.1.dev12"
      },
      {
        "authorType": "User",
        "author_name": "nmichlo",
        "body": "+ fixed init files",
        "dateCreated": "2021-04-07T19:21:16Z",
        "datePublished": "2021-04-07T19:22:17Z",
        "html_url": "https://github.com/nmichlo/disent/releases/tag/v0.0.1.dev11",
        "name": "v0.0.1.dev11",
        "tag_name": "v0.0.1.dev11",
        "tarball_url": "https://api.github.com/repos/nmichlo/disent/tarball/v0.0.1.dev11",
        "url": "https://api.github.com/repos/nmichlo/disent/releases/41080486",
        "zipball_url": "https://api.github.com/repos/nmichlo/disent/zipball/v0.0.1.dev11"
      },
      {
        "authorType": "User",
        "author_name": "nmichlo",
        "body": "**frameworks**\r\n+ simplified ada frameworks\r\n+ moved schedules out of ada frameworks into configs\r\n+ extra kl divergence modes\r\n\r\n**metrics**\r\n+ combined flatness components\r\n    - axis alignment ratio\r\n    - linearity ratio\r\n    - incorrect swap ratio\r\n\r\n**experiments**\r\n+ existing configs should be frozen -- changes should be added to experiment scripts below\r\n+ helper script\r\n+ experiment scripts\r\n\r\n**more**\r\n+ and much more\r\n    ",
        "dateCreated": "2021-04-07T16:57:38Z",
        "datePublished": "2021-04-07T17:02:49Z",
        "html_url": "https://github.com/nmichlo/disent/releases/tag/v0.0.1.dev10",
        "name": "v0.0.1.dev10",
        "tag_name": "v0.0.1.dev10",
        "tarball_url": "https://api.github.com/repos/nmichlo/disent/tarball/v0.0.1.dev10",
        "url": "https://api.github.com/repos/nmichlo/disent/releases/41073355",
        "zipball_url": "https://api.github.com/repos/nmichlo/disent/zipball/v0.0.1.dev10"
      },
      {
        "authorType": "User",
        "author_name": "nmichlo",
        "body": "**metrics**\r\n+ flatness components\r\n    - reworked linearity component, now uses PCA to measure linearity along single arbitrary basis, and variance of embeddings to measure linearity along axis.\r\n\r\n**dataset wrappers**\r\n+ new random dist dataset\r\n   - only triplets have some sort of order, otherwise everything is sampled randomly\r\n+ RandomEpisodeDataset now has RandomDataset as parent\r\n\r\n**torch util - math**\r\n+ PCA functions\r\n\r\n",
        "dateCreated": "2021-03-22T20:53:35Z",
        "datePublished": "2021-03-22T20:57:18Z",
        "html_url": "https://github.com/nmichlo/disent/releases/tag/v0.0.1.dev9",
        "name": "v0.0.1.dev9",
        "tag_name": "v0.0.1.dev9",
        "tarball_url": "https://api.github.com/repos/nmichlo/disent/tarball/v0.0.1.dev9",
        "url": "https://api.github.com/repos/nmichlo/disent/releases/40187433",
        "zipball_url": "https://api.github.com/repos/nmichlo/disent/zipball/v0.0.1.dev9"
      },
      {
        "authorType": "User",
        "author_name": "nmichlo",
        "body": "+ renamed `flatness components` metric (originally dual flatness)",
        "dateCreated": "2021-03-19T11:14:12Z",
        "datePublished": "2021-03-19T11:16:12Z",
        "html_url": "https://github.com/nmichlo/disent/releases/tag/v0.0.1.dev8",
        "name": "v0.0.1.dev8",
        "tag_name": "v0.0.1.dev8",
        "tarball_url": "https://api.github.com/repos/nmichlo/disent/tarball/v0.0.1.dev8",
        "url": "https://api.github.com/repos/nmichlo/disent/releases/40057731",
        "zipball_url": "https://api.github.com/repos/nmichlo/disent/zipball/v0.0.1.dev8"
      },
      {
        "authorType": "User",
        "author_name": "nmichlo",
        "body": "**metrics**\r\n+ dual flatness metric\r\n    - measures **linearity** and **ordering** of latent traversals\r\n    - ordering over randomly sampled embeddings compared to  ground truth factors\r\n\r\n**utils**\r\n+ torch math helper functions\r\n    - Covariance matrix\r\n    - Pearson's correlation matrix\r\n    - Spearman's rank correlation matrix\r\n    - Generalised mean (p from `-inf` to `inf`, special cases for harmonic, geometric, arithmetic, min, max, quadratic means)\r\n\r\n**state spaces**\r\n+ fixes for `pos_to_idx` and `idx_to_pos` with array sizes with more than 1 dimension (excluding last factor or idx dim)\r\n\r\n",
        "dateCreated": "2021-03-19T10:38:10Z",
        "datePublished": "2021-03-19T10:44:55Z",
        "html_url": "https://github.com/nmichlo/disent/releases/tag/v0.0.1.dev7",
        "name": "v0.0.1.dev7",
        "tag_name": "v0.0.1.dev7",
        "tarball_url": "https://api.github.com/repos/nmichlo/disent/tarball/v0.0.1.dev7",
        "url": "https://api.github.com/repos/nmichlo/disent/releases/40056334",
        "zipball_url": "https://api.github.com/repos/nmichlo/disent/zipball/v0.0.1.dev7"
      },
      {
        "authorType": "User",
        "author_name": "nmichlo",
        "body": "**new frameworks**\r\n+ Beta-TCVAE (probably wrong, needs verification & correct loss scaling)\r\n\r\n**frameworks**\r\n+ DFC-VAE input fixes & support for 1 channel",
        "dateCreated": "2021-03-14T19:18:00Z",
        "datePublished": "2021-03-14T19:34:39Z",
        "html_url": "https://github.com/nmichlo/disent/releases/tag/v0.0.1.dev6",
        "name": "v0.0.1.dev6",
        "tag_name": "v0.0.1.dev6",
        "tarball_url": "https://api.github.com/repos/nmichlo/disent/tarball/v0.0.1.dev6",
        "url": "https://api.github.com/repos/nmichlo/disent/releases/39787660",
        "zipball_url": "https://api.github.com/repos/nmichlo/disent/zipball/v0.0.1.dev6"
      },
      {
        "authorType": "User",
        "author_name": "nmichlo",
        "body": "**new frameworks**\r\n+ DIP-VAE\r\n+ Info-VAE\r\n+ EXPERIMENTAL: Data Overlap TVAE\r\n\r\n**frameworks**\r\n+ rewrite of frameworks extending from single VAE class, hooks are now made available for easy overrides. Removed lots of duplicate code.\r\n\r\n**datasets**\r\n+ Fully random paired datasets\r\n\r\n",
        "dateCreated": "2021-03-14T17:49:44Z",
        "datePublished": "2021-03-14T17:52:14Z",
        "html_url": "https://github.com/nmichlo/disent/releases/tag/v0.0.1.dev5",
        "name": "v0.0.1.dev5",
        "tag_name": "v0.0.1.dev5",
        "tarball_url": "https://api.github.com/repos/nmichlo/disent/tarball/v0.0.1.dev5",
        "url": "https://api.github.com/repos/nmichlo/disent/releases/39786003",
        "zipball_url": "https://api.github.com/repos/nmichlo/disent/zipball/v0.0.1.dev5"
      },
      {
        "authorType": "User",
        "author_name": "nmichlo",
        "body": "**cfg**\r\n+ renamed `loss_rediction` mode `batch_mean` to `mean_sum`\r\n+ changed default `loss_reduction` mode to `mean`\r\n+ changed default `beta` to match new `loss_reduction` mode\r\n\r\n**flatness metric**\r\n+ added average angle along traversals\r\n+ simplified greatly\r\n\r\n**reconstruction loss**\r\n+ more reconstruction losses based off of distributions\r\n    - `bernoulli`\r\n    - `continuous_bernoulli`\r\n    - `normal`\r\n\r\n**shedules**\r\n+ new schedules:\r\n    - NoopSchedule: does absolutely nothing!\r\n    - CosineWaveSchedule: smooth cosine wave\r\n+ Adjusted arguments of most schedules\r\n\r\n**bugs**\r\n+ fixed various runtime bugs\r\n    - logging crash for W&B\r\n    - disabled checkpointing in trainer\r\n    - flatness metric did not support axis size < 2\r\n",
        "dateCreated": "2021-02-27T16:11:40Z",
        "datePublished": "2021-02-27T17:27:09Z",
        "html_url": "https://github.com/nmichlo/disent/releases/tag/v0.0.1.dev4",
        "name": "v0.0.1.dev4",
        "tag_name": "v0.0.1.dev4",
        "tarball_url": "https://api.github.com/repos/nmichlo/disent/tarball/v0.0.1.dev4",
        "url": "https://api.github.com/repos/nmichlo/disent/releases/38995985",
        "zipball_url": "https://api.github.com/repos/nmichlo/disent/zipball/v0.0.1.dev4"
      },
      {
        "authorType": "User",
        "author_name": "nmichlo",
        "body": "+ Custom annealing schedules\r\n+ BaseFramework & experiment schedules/annealing support\r\n- Removed BetaVaeH (this is just an annealed version of the BetaVAE)\r\n",
        "dateCreated": "2021-02-25T12:06:36Z",
        "datePublished": "2021-02-27T17:19:10Z",
        "html_url": "https://github.com/nmichlo/disent/releases/tag/v0.0.1.dev3",
        "name": "v0.0.1.dev3",
        "tag_name": "v0.0.1.dev3",
        "tarball_url": "https://api.github.com/repos/nmichlo/disent/tarball/v0.0.1.dev3",
        "url": "https://api.github.com/repos/nmichlo/disent/releases/38990289",
        "zipball_url": "https://api.github.com/repos/nmichlo/disent/zipball/v0.0.1.dev3"
      },
      {
        "authorType": "User",
        "author_name": "nmichlo",
        "body": "+ weight initialisation\r\n+ torch.distributions instead of manual sampling and loss calculation\r\n+ selectable loss reduction modes 'sum', 'mean', 'batch_mean'\r\n+ better experiment logging and crash reporting\r\n+ metric summary logging\r\n+ flatness metric\r\n+ MIT License\r\n+ synthetic datasets rgb=False fix\r\n+ config cleanup\r\n+ loss nan/out of bounds error\r\n",
        "dateCreated": "2021-02-24T19:06:52Z",
        "datePublished": "2021-02-27T17:14:35Z",
        "html_url": "https://github.com/nmichlo/disent/releases/tag/v0.0.1.dev2",
        "name": "v0.0.1.dev2",
        "tag_name": "v0.0.1.dev2",
        "tarball_url": "https://api.github.com/repos/nmichlo/disent/tarball/v0.0.1.dev2",
        "url": "https://api.github.com/repos/nmichlo/disent/releases/38987118",
        "zipball_url": "https://api.github.com/repos/nmichlo/disent/zipball/v0.0.1.dev2"
      },
      {
        "authorType": "User",
        "author_name": "nmichlo",
        "body": "",
        "dateCreated": "2021-02-05T02:42:04Z",
        "datePublished": "2021-02-05T02:43:02Z",
        "html_url": "https://github.com/nmichlo/disent/releases/tag/v0.0.1.dev1",
        "name": "v0.0.1.dev1",
        "tag_name": "v0.0.1.dev1",
        "tarball_url": "https://api.github.com/repos/nmichlo/disent/tarball/v0.0.1.dev1",
        "url": "https://api.github.com/repos/nmichlo/disent/releases/37387532",
        "zipball_url": "https://api.github.com/repos/nmichlo/disent/zipball/v0.0.1.dev1"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 30,
      "date": "Sun, 12 Dec 2021 19:01:26 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "pytorch",
      "pytorch-lightning",
      "ada-gvae",
      "beta-vae",
      "vae",
      "disentanglement",
      "disentangled-representations",
      "dfc-vae",
      "disentanglement-lib",
      "disentanglement-library",
      "disentanglement-frameworks",
      "hydra",
      "disentanglement-learning",
      "metrics",
      "configurable",
      "datasets",
      "python",
      "python3",
      "variational-autoencoders",
      "autoencoders"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "The following is a basic working example of disent that trains a BetaVAE with a cyclic\nbeta schedule and evaluates the trained model with various metrics.\n\n<details><summary><b>\ud83d\udcbe Basic Example</b></summary>\n<p>\n\n```python3\nimport os\nimport pytorch_lightning as pl\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom disent.dataset import DisentDataset\nfrom disent.dataset.data import XYObjectData\nfrom disent.dataset.sampling import SingleSampler\nfrom disent.dataset.transform import ToImgTensorF32\nfrom disent.frameworks.vae import BetaVae\nfrom disent.metrics import metric_dci\nfrom disent.metrics import metric_mig\nfrom disent.model import AutoEncoder\nfrom disent.model.ae import DecoderConv64\nfrom disent.model.ae import EncoderConv64\nfrom disent.schedule import CyclicSchedule\n\n#: create the dataset & dataloaders\n#: - ToImgTensorF32 transforms images from numpy arrays to tensors and performs checks\ndata = XYObjectData()\ndataset = DisentDataset(dataset=data, sampler=SingleSampler(), transform=ToImgTensorF32())\ndataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=True, num_workers=os.cpu_count())\n\n#: create the BetaVAE model\n#: - adjusting the beta, learning rate, and representation size.\nmodule = BetaVae(\n  model=AutoEncoder(\n    #: z_multiplier is needed to output mu & logvar when parameterising normal distribution\n    encoder=EncoderConv64(x_shape=data.x_shape, z_size=10, z_multiplier=2),\n    decoder=DecoderConv64(x_shape=data.x_shape, z_size=10),\n  ),\n  cfg=BetaVae.cfg(\n    optimizer='adam',\n    optimizer_kwargs=dict(lr=1e-3),\n    loss_reduction='mean_sum',\n    beta=4,\n  )\n)\n\n#: cyclic schedule for target 'beta' in the config/cfg. The initial value from the\n#: config is saved and multiplied by the ratio from the schedule on each step.\n#: - based on: https://arxiv.org/abs/1903.10145\nmodule.register_schedule(\n  'beta', CyclicSchedule(\n    period=1024,  #: repeat every: trainer.global_step % period\n  )\n)\n\n#: train model\n#: - for 2048 batches/steps\ntrainer = pl.Trainer(\n  max_steps=2048, gpus=1 if torch.cuda.is_available() else None, logger=False, checkpoint_callback=False\n)\ntrainer.fit(module, dataloader)\n\n#: compute disentanglement metrics\n#: - we cannot guarantee which device the representation is on\n#: - this will take a while to run\nget_repr = lambda x: module.encode(x.to(module.device))\n\nmetrics = {\n  **metric_dci(dataset, get_repr, num_train=1000, num_test=500, show_progress=True),\n  **metric_mig(dataset, get_repr, num_train=2000),\n}\n\n#: evaluate\nprint('metrics:', metrics)\n```\n\n</p>\n</details>\n\nVisit the [docs](https://disent.dontpanic.sh) for more examples!\n\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "The entrypoint for basic experiments is `experiment/run.py`.\n\nSome configuration will be required, but basic experiments can\nbe adjusted by modifying the [Hydra Config 1.0](https://github.com/facebookresearch/hydra)\nfiles in `experiment/config` (Please note that hydra 1.1 is not yet supported).\n\nModifying the main `experiment/config/config.yaml` is all you\nneed for most basic experiments. The main config file contains\na defaults list with entries corresponding to yaml configuration\nfiles (config options) in the subfolders (config groups) in\n`experiment/config/<config_group>/<option>.yaml`.\n\n<details><summary><b>\ud83d\udcbe Config Defaults Example</b></summary>\n<p>\n\n```yaml\ndefaults:\n  #: data\n  - sampling: default__bb\n  - dataset: xyobject\n  - augment: none\n  #: system\n  - framework: adavae_os\n  - model: vae_conv64\n  #: training\n  - optimizer: adam\n  - schedule: beta_cyclic\n  - metrics: fast\n  - run_length: short\n  #: logs\n  - run_callbacks: vis\n  - run_logging: wandb\n  #: runtime\n  - run_location: local\n  - run_launcher: local\n  - run_action: train\n\n#: <rest of config.yaml left out>\n...\n```\n\n</p>\n</details>\n\nEasily modify  any of these values to adjust how the basic experiment\nwill be run. For example, change `framework: adavae` to `framework: betavae`, or\nchange the dataset from `xyobject` to `shapes3d`. Add new options by adding new\nyaml files in the config group folders.\n\n[Weights and Biases](https://docs.wandb.ai/quickstart) is supported by changing `run_logging: none` to\n`run_logging: wandb`. However, you will need to login from the command line. W&B logging supports\nvisualisations of latent traversals.\n\n\n----------------------\n\n",
      "technique": "Header extraction"
    }
  ]
}