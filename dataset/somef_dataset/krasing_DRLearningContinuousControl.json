{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1509.02971\n\n* Implementations of the **Deep Deterministic Policy Gradients** in OpenAI Gym environments:\n    - [Pendulum](https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-pendulum",
      "https://arxiv.org/abs/1509.02971v5, https://arxiv.org/abs/1509.02971\n\n* Implementations of the **Deep Deterministic Policy Gradients** in OpenAI Gym environments:\n    - [Pendulum](https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-pendulum): Use OpenAI Gym's Pendulum environment.\n    - [BipedalWalker](https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-bipedal): Use OpenAI Gym's BipedalWalker environment.\n\n* [Cheatsheet](https://github.com/udacity/deep-reinforcement-learning/blob/master/cheatsheet) for reinforcement learning"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1
      ],
      "excerpt": "* The Udacity's [Deep Reinforcement Learning Nanodegree program](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) repository, https://github.com/udacity/deep-reinforcement-learning\n\n* Lillicrap T.P., Hunt J.J., Pritzel A., et.al., Continuous control with deep reinforcement learning, arXiv:1509.02971v5, https://arxiv.org/abs/1509.02971\n\n* Implementations of the **Deep Deterministic Policy Gradients** in OpenAI Gym environments:\n    - [Pendulum](https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-pendulum): Use OpenAI Gym's Pendulum environment.\n    - [BipedalWalker](https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-bipedal): Use OpenAI Gym's BipedalWalker environment.\n\n* [Cheatsheet](https://github.com/udacity/deep-reinforcement-learning/blob/master/cheatsheet) for reinforcement learning.",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "[image1]: https://user-images.githubusercontent.com/10624937/42135602-b0335606-7d12-11e8-8689-dd1cf9fa11a9.gif \"Trained Agents\"\n[image2]: images/kernel.png \"Kernel\"\n\n",
      "technique": "Header extraction"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/krasing/DRLearningContinuousControl",
    "technique": "GitHub API"
  },
  "contact": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This repository contains material related to the **Continuous Control** project of the Udacity's [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.  \n\n![Trained Agents](./images/scene.png)\n\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2018-07-26T22:09:09Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-05-20T09:36:42Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Project with Udacity",
      "technique": "GitHub API"
    }
  ],
  "documentation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "http://ipython.readthedocs.io/",
      "technique": "Regular expression"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/krasing/DRLearningContinuousControl/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 1,
      "date": "Tue, 07 Dec 2021 19:33:15 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/krasing/DRLearningContinuousControl/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "krasing/DRLearningContinuousControl",
    "technique": "GitHub API"
  },
  "hasExecutableNotebook": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://raw.githubusercontent.com/krasing/DRLearningContinuousControl/master/Continuous_Control.ipynb"
    ],
    "technique": "File Exploration"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/krasing/DRLearningContinuousControl/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Jupyter Notebook",
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "(Image References)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "DRLearningContinuousControl",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "krasing",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/krasing/DRLearningContinuousControl/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "To set up your python environment to run the code in this repository, follow the instructions below.\n\n1. Create (and activate) a new environment with Python 3.6. (__Linux__ or __Mac__)\n\t```bash\n\tconda create --name drlnd python=3.6\n\tsource activate drlnd\n\t```\n\n2. Clone the repository (if you haven't already!), and navigate to the `python/` folder.  Then, install several dependencies.\n```bash\ngit clone https://github.com/udacity/deep-reinforcement-learning.git\ncd deep-reinforcement-learning/python\npip install .\n```\n\n3. Create an [IPython kernel](http://ipython.readthedocs.io/en/stable/install/kernel_install.html) for the `drlnd` environment.  \n```bash\npython -m ipykernel install --user --name drlnd --display-name \"drlnd\"\n```\n\n4. Download the Unity Environment:\n\n    Download [this file](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher_Linux.zip) for Linux operating system. Decompress in the same folder where the project files are. This project uses rich simulation environments from [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents) but you will not need to install Unity - the environment is already built into the downloaded file.\n\n5. If you plan to play with the OpenAI gym examples, follow the instructions in [this repository](https://github.com/openai/gym):\n    - perform a minimal install of OpenAI gym:\n```bash\ngit clone https://github.com/openai/gym.git\ncd gym\npip install -e .\n```    \n\t- install the **classic control** environment group:\n```bash\npip install -e '.[classic_control]'\n```\n\t- install the **box2d** environment group:\n```bash\npip install -e '.[box2d]'\n```\n\n",
      "technique": "Header extraction"
    }
  ],
  "run": [
    {
      "confidence": [
        1
      ],
      "excerpt": "1. Execute in terminal `jupyter notebook Continuous_Control.ipynb` to load the notebook. Make sure that *ddpg_agent.py* and *model.py* are in the same folder.\n\n2. Before running code in the notebook, change the kernel to match the `drlnd` environment by using the drop-down `Kernel` menu. \n\n  ![Kernel][image2]\n\n3. To train the Agent execute:\n  - Sections 1. Start the Environment\n  - Section 2. Examine the State and Action Spaces\n  - Section 3. Train the Agent with DDPG \n  \n4. To watch a Smart Agent execute:\n  - Sections 1. Start the Environment\n  - Section 4. Watch a Smart Agent!\n\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 2,
      "date": "Tue, 07 Dec 2021 19:33:15 GMT"
    },
    "technique": "GitHub API"
  }
}