{
  "acknowledgement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "This implementation has been tested on Tensorflow r0.12.\n",
      "technique": "Header extraction"
    }
  ],
  "citation": [
    {
      "confidence": [
        0.8654671031158477
      ],
      "excerpt": "Weight Initializer : he > trauncated normal = xaiver > normal \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/hwalsuklee/tensorflow-mnist-MLP-batch_normalization-weight_initializers",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2017-01-02T13:18:47Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-23T08:47:32Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9558211976035755,
        0.946794516244432,
        0.919512176476662
      ],
      "excerpt": "An implementation of weight/bias initializers and batch normalization in Tensorflow. \nMNIST database is used to show performance-comparison \nIn order to examine the effect of initializers and batch normalization, a simple network architecture called multilayer perceptrons (MLP) is employed. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9285091981692867
      ],
      "excerpt": "The following initializers for weights/biases of network are considered. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9156188684512657,
        0.8641164889553343
      ],
      "excerpt": "xaiver : Understanding the difficulty of training deep feedforward neural networks \nhe : Delving Deep into Rectifiers \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9349226028279232
      ],
      "excerpt": "Batch normalization improves performance of network in terms of final accuracy and convergence rate.</br> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "MNIST classification using Multi-Layer Perceptron (MLP) with 2 hidden layers. Some weight-initializers and batch-normalization are implemented.",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/hwalsuklee/tensorflow-mnist-MLP-batch_normalization-weight_initializers/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 16,
      "date": "Sat, 11 Dec 2021 20:31:30 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/hwalsuklee/tensorflow-mnist-MLP-batch_normalization-weight_initializers/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "hwalsuklee/tensorflow-mnist-MLP-batch_normalization-weight_initializers",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8256731612875947
      ],
      "excerpt": "MLP has following architecture. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8046535237759266
      ],
      "excerpt": "input layer : 784 nodes (MNIST images size) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8212584075129916
      ],
      "excerpt": "output layer : 10 nodes (number of class for MNIST) \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/hwalsuklee/tensorflow-mnist-MLP-batch_normalization-weight_initializers/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Various initializers and batch normalization",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "tensorflow-mnist-MLP-batch_normalization-weight_initializers",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "hwalsuklee",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/hwalsuklee/tensorflow-mnist-MLP-batch_normalization-weight_initializers/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 48,
      "date": "Sat, 11 Dec 2021 20:31:30 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "tensorflow",
      "mnist",
      "mnist-classification",
      "batch-normalization",
      "weight-initializers",
      "xavier",
      "he-initializer",
      "xavier-initializer",
      "he"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "`python run_main.py --weight-init <weight initializer> --bias-init <bias initializer> --batch-norm <True or False>`\n\n`<weight initializer>` must be selected in [normal, truncated_normal, xavier, he].</br>\n`<bias initializer>` must be selected in [normal, zero].\n\nYou may command like `python run_main.py --weight-init xavier --bias-init zero --batch-norm True`.\n\n",
      "technique": "Header extraction"
    }
  ]
}