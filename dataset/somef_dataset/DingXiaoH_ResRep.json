{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/2007.03260",
      "https://arxiv.org/abs/2105.01883",
      "https://arxiv.org/abs/2101.03697",
      "https://arxiv.org/abs/2007.03260",
      "https://arxiv.org/abs/2103.13425",
      "https://arxiv.org/abs/2007.03260"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@article{ding2020lossless,\ntitle={Lossless CNN Channel Pruning via Decoupling Remembering and Forgetting},\nauthor={Ding, Xiaohan and Hao, Tianxiang and Liu, Ji and Han, Jungong and Guo, Yuchen and Ding, Guiguang},\njournal={arXiv preprint arXiv:2007.03260},\nyear={2020}\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9208940044786343,
        0.9999639527783479,
        0.9999268952549604
      ],
      "excerpt": "title={Lossless CNN Channel Pruning via Decoupling Remembering and Forgetting}, \nauthor={Ding, Xiaohan and Hao, Tianxiang and Liu, Ji and Han, Jungong and Guo, Yuchen and Ding, Guiguang}, \njournal={arXiv preprint arXiv:2007.03260}, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.874446915586759
      ],
      "excerpt": "RESNET50_ORIGIN_DEPS_FLATTENED = [64,256,64,64,256,64,64,256,64,64,256,512,128,128,512,128,128,512,128,128,512,128,128,512, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9267867627836347
      ],
      "excerpt": "My open-sourced papers and repos:  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8656442788361102
      ],
      "excerpt": "RepMLP: Re-parameterizing Convolutions into Fully-connected Layers for Image Recognition\\ \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/DingXiaoH/ResRep",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-07-07T07:29:42Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-12-07T10:44:37Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        1
      ],
      "excerpt": "We propose ResRep, a novel method for lossless channel pruning (a.k.a. filter pruning), which aims to slim down a convolutional neural network (CNN) by reducing the width (number of output channels) of convolutional layers. Inspired by the neurobiology research about the independence of remembering and forgetting, we propose to re-parameterize a CNN into the remembering parts and forgetting parts, where the former learn to maintain the performance and the latter learn for efficiency. By training the re-parameterized model using regular SGD on the former but a novel update rule with penalty gradients on the latter, we realize structured sparsity, enabling us to equivalently convert the re-parameterized model into the original architecture with narrower layers. Such a methodology distinguishes ResRep from the traditional learning-based pruning paradigm that applies a penalty on parameters to produce structured sparsity, which may suppress the parameters essential for the remembering. Our method slims down a standard ResNet-50 with 76.15% accuracy on ImageNet to a narrower one with only 45% FLOPs and no accuracy drop, which is the first to achieve lossless pruning with such a high compression ratio, to the best of our knowledge.\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        0.9527327336930088
      ],
      "excerpt": "State-of-the-art channel pruning (a.k.a. filter pruning)! This repo contains the code for Lossless CNN Channel Pruning via Decoupling Remembering and Forgetting. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9264877373820627
      ],
      "excerpt": "Update: released the log of the 54.5%-pruned ResNet-50 as kindly requested by several readers. (The experiments were done 15 months ago and the results are still SOTA.) \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9353105764738667,
        0.8135988015307392
      ],
      "excerpt": "1. Reproduce 54.5% pruning ratio of ResNet-50 on ImageNet with 8 GPUs without accuracy drop. \n2. Reproduce 52.9% pruning ratio of ResNet-56 on CIFAR-10 with 1 GPU: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9813688349176403
      ],
      "excerpt": "2. Our method does not rely on any new or deprecated features of any libraries, so there is no need to make an identical environment. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8782602137350566
      ],
      "excerpt": "Show the name and shape of weights in the pruned model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8782602137350566
      ],
      "excerpt": "Show the name and shape of weights in the pruned model. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9158184795922654,
        0.8809634885958042,
        0.9012644176437191
      ],
      "excerpt": "Pruning simple models or easy-to-prune layers in a complicated model \nFirst, let's clarify the meanings of some architecutre-specific constants and functions defined for the pruning process. \ndeps, the width of every conv layer is defined by an array named \"deps\". For example, \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8623770454968542,
        0.9381885181243756,
        0.9104926717756381,
        0.8938813365376812
      ],
      "excerpt": "Note that we build the projection (1x1 conv shortcut) layer before the parallel residual block (L61 in stagewise_resnet.py), so that its width (256) preceds the widths of the three layers of the residual block (64, 64, 256). \ncalculate_SOME_MODEL_flops, the function to calculate the FLOPs of a specific architecuture given the \"deps\". It is architecture-specific. You may follow calculate_resnet_bottleneck_flops in resrep_scripts.py to define it for your own model. \nsucceeding_strategy defines how the layers follow others. If layer B follows layer A (i.e., pruning the output channels of layer A triggers the removal of the corresponding input channels of layer B), we should have succeeding_strategy\\[A\\]=B. This is the common case of simple models. For example, the succeeding_strategy of VGG-16 should be \\{0:1, 1:2, 2:3, ...\\}.  \nHowever, some layers in some complicated models are a bit tricky to prune. In the experiments reported in the paper, we only pruned the internal layers of ResNets (i.e., the first layer of every res block of Res56 and the first two layers of every res block of Res50) but did not prune the tricky layers. You may skip the following content if you do not intend to prune those layers. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9363913936213417
      ],
      "excerpt": "Complicated succeeding_strategy. For example, when you prune the last layers of stage1 and stage2 in ResNet-56 (i.e., the last layer of the last res block), which are indexed 18 and 37, you need to prune the input channels of the first two layers of the next stage accordingly, so that the succeeding_strategy is {1: 2, 3: 4, ..., 18: [19, 20], ..., 37: [38, 39], ...}.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9366747277339642
      ],
      "excerpt": "The above-mentioned constants are inputs to compactor_convert, which is a generic method for pruning and converting a model with compactors. However, given a specific architecture, compared to figuring out how such constants should be defined, writing a standalone pruning function for your architecture may be easier. ``compactor_convert_mi1``` is an example for pruning MobileNet-V1. You need to \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8039771522124469
      ],
      "excerpt": "(preprint, 2021) A powerful MLP-style CNN building block\\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9823882208147127,
        0.8803719052026773
      ],
      "excerpt": "ACB (ICCV 2019) is a CNN component without any inference-time costs. The first work of our Structural Re-parameterization Universe.\\ \nACNet: Strengthening the Kernel Skeletons for Powerful CNN via Asymmetric Convolution Blocks.\\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9797640903181857
      ],
      "excerpt": "DBB (CVPR 2021) is a CNN component with higher performance than ACB and still no inference-time costs. Sometimes I call it ACNet v2 because \"DBB\" is 2 bits larger than \"ACB\" in ASCII (lol).\\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9450573325026418
      ],
      "excerpt": "Model compression and acceleration: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8293788909236824
      ],
      "excerpt": "(ICML 2019) Channel pruning: Approximated Oracle Filter Pruning for Destructive CNN Width Optimization\\ \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Lossless CNN Channel Pruning via Decoupling Remembering and Forgetting",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/DingXiaoH/ResRep/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 28,
      "date": "Tue, 07 Dec 2021 15:57:07 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/DingXiaoH/ResRep/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "DingXiaoH/ResRep",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.8405489669108047
      ],
      "excerpt": "1. We used torch==1.3.0, torchvision==0.4.1, CUDA==10.2, NVIDIA driver version==440.82, tensorboard==1.11.0 on a machine with eight 2080Ti GPUs.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9459246068628353
      ],
      "excerpt": "3. If you get any errors regarding tensorboard or tensorflow, you may simply delete the code related to tensorboard or SummaryWriter. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.807445338771463
      ],
      "excerpt": "2. If some layers must be pruned following others, do that correctly. \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python transform_torchvision.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8235779579190644,
        0.8099194818651142
      ],
      "excerpt": "Show the name and shape of weights in the pruned model. \npython display_hdf5.py resrep_models/sres50_train/finish_converted.hdf5 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python train_base_model.py -a src56 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091,
        0.8235779579190644,
        0.8099194818651142
      ],
      "excerpt": "python rr/exp_resrep.py -a src56 \nShow the name and shape of weights in the pruned model. \npython display_hdf5.py resrep_models/src56_train/finish_converted.hdf5 \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/DingXiaoH/ResRep/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "MIT License",
      "url": "https://api.github.com/licenses/mit"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'MIT License\\n\\nCopyright (c) 2019 DingXiaoH\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "ResRep (ICCV 2021)",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "ResRep",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "DingXiaoH",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/DingXiaoH/ResRep/blob/master/README.md",
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 163,
      "date": "Tue, 07 Dec 2021 15:57:07 GMT"
    },
    "technique": "GitHub API"
  }
}