{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1312.5602v1"
    ],
    "technique": "Regular expression"
  },
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/RLeike/connect-four",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-05-28T16:08:05Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-06-20T07:44:23Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9527677070231061,
        0.9372435031206305,
        0.8653354737932805,
        0.8515430240097962
      ],
      "excerpt": "This repository provides code for training a neural network to play Connect Four. \nConnect Four is a small strategic board game where two players take turns laying a stone of their color into one of seven columns. If a player manages to have four connecting stones in a row, column, or diagonal, it wins. \nDeep Q-Learning was first introduced by [1], who trained a relatively small neural network to play Atari games. The idea is to train a neural network to return the Q-function for each action given the state of the game. \nThe Q-function is defined as the expected discounted reward when following a policy. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8242203420007953
      ],
      "excerpt": "Exploring different actions once in a while can help with exploration. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.907396752868629,
        0.8955087936101137
      ],
      "excerpt": "main.py holds the training loop, in which many rounds of self-play are carried out. The generated trajectories are stored in a Memory object, of which the implementation is inside memory.py. This object then provides preprocessed batches for training the agent. \nThe agent itself is defined in agent.py. It consists of a simple feed forward convolutional neural network.  \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9065720875051354,
        0.9573469582446928,
        0.9208879046703318,
        0.8478401404235651
      ],
      "excerpt": "Hereby the power is enhanced by recursively looking ahead using a min-max algorithm. \nThe Q-learner picks up some aspects and tactics, such as seeing the benefit of starting in the middle.  \nThis happens already when training for only a short amount of time. \nHowever, the bot can still be defeated with a bit of effort, even when its power is enhanced by recursively looking ahead. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8745610836035609,
        0.9471373190256355,
        0.9165234808045625
      ],
      "excerpt": "Potentially the power could be increased by training a larger network using more computation time. \nAnother potential pitfall might be the outdated approach that is taken here. Normally, Q-learner learn by training on trajectories generated by one previous version of themselves. Here, however, the Q-learner is trained on trajectories generated by itself, and then immediately updated with a sample from the memory buffer. This might lead to undesired feedback loops and makes the algorithm less stable overall. \nThe algorithm could also be improved by using a more modern actor critic learning algorithm. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "Implementing a game environment and Q-learner for Connect Four",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/RLeike/connect-four/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sun, 12 Dec 2021 06:00:32 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/RLeike/connect-four/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "RLeike/connect-four",
    "technique": "GitHub API"
  },
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/RLeike/connect-four/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "technique": "GitHub API"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "Deep Q-Learning for Connect Four",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "connect-four",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "RLeike",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/RLeike/connect-four/blob/master/README.md",
    "technique": "GitHub API"
  },
  "requirement": [
    {
      "confidence": [
        1
      ],
      "excerpt": "python3 3.8\n\njax\n\nscipy\n\n",
      "technique": "Header extraction"
    }
  ],
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 0,
      "date": "Sun, 12 Dec 2021 06:00:32 GMT"
    },
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "Run the code with\n\n`python3 main.py`.\n\n",
      "technique": "Header extraction"
    }
  ]
}