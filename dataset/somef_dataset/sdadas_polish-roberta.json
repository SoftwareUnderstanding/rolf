{
  "arxivLinks": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "https://arxiv.org/abs/1907.11692",
      "https://arxiv.org/abs/2006.04229"
    ],
    "technique": "Regular expression"
  },
  "citation": [
    {
      "confidence": [
        1.0
      ],
      "excerpt": "@InProceedings{dadas2020pretraining,\n  title=\"Pre-training Polish Transformer-Based Language Models at Scale\",\n  author=\"Dadas, S{\\l}awomir and Pere{\\l}kiewicz, Micha{\\l} and Po{\\'{s}}wiata, Rafa{\\l}\",\n  booktitle=\"Artificial Intelligence and Soft Computing\",\n  year=\"2020\",\n  publisher=\"Springer International Publishing\",\n  pages=\"301--314\",\n  isbn=\"978-3-030-61534-5\"\n}",
      "technique": "Regular expression"
    },
    {
      "confidence": [
        0.9507374082549614
      ],
      "excerpt": "  <td>12&nbsp;/&nbsp;768&nbsp;/&nbsp;12</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9507374082549614
      ],
      "excerpt": "  <td>12&nbsp;/&nbsp;768&nbsp;/&nbsp;12</td> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8356013927728488
      ],
      "excerpt": "| 1       |   94.80  |  94.20 |  94.30 | 69.62 |     90.58    |     78.74     | 71.23 | 98.62 | 87.99 |  86.68  | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9728768603164633
      ],
      "excerpt": "| 3       |   93.73  |  94.30 |  94.64 | 70.67 |     91.41    |     78.14     | 74.44 | 98.92 | 87.64 |  87.10  | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8670498468297771
      ],
      "excerpt": "| 3       |   95.24  |  93.30 |  94.61 | 71.59 |     91.41    |     82.19     | 75.35 | 98.64 | 89.31 |  87.96  | \n",
      "technique": "Supervised classification"
    }
  ],
  "codeRepository": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/sdadas/polish-roberta",
    "technique": "GitHub API"
  },
  "dateCreated": {
    "confidence": [
      1.0
    ],
    "excerpt": "2020-03-15T16:07:55Z",
    "technique": "GitHub API"
  },
  "dateModified": {
    "confidence": [
      1.0
    ],
    "excerpt": "2021-11-28T01:41:37Z",
    "technique": "GitHub API"
  },
  "description": [
    {
      "confidence": [
        0.9810294965916394,
        0.9769470863332282
      ],
      "excerpt": "This repository contains pre-trained RoBERTa models for Polish as well as evaluation code for several Polish linguistic tasks. The released models were trained using Fairseq toolkit in the National Information Processing Institute, Warsaw, Poland. We provide two models based on BERT base and BERT large architectures. Two versions of each model are available: one for Fairseq and one for Huggingface Transformers. \n21.03.2021 - We release a new version of the base model. The updated model has been trained on the same corpus as the original model but we used different hyperparameters. We made the following changes: 1) Sentencepiece Unigram model was used instead of BPE, 2) The model was trained with whole-word masking objective instead of classic token masking, 3) We utilized the full context of 512 tokens so training examples could include more than one sentence (the original model was trained on single sencentes only), 4) Longer pretraining (400k steps). \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8204518195341585,
        0.9536515418858866
      ],
      "excerpt": "** Average KLEJ score over 5 runs, see evaluation section for detailed results<br/> \nMore details are available in the paper Pre-training Polish Transformer-based Language Models at Scale. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8758229505459186
      ],
      "excerpt": "Next, run run_tasks.py script to prepare the data, fine-tune and evaluate the model. We used the following parameters for each task: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9649901934226883
      ],
      "excerpt": "Below we show the evaluation results of our models on the tasks included in KLEJ Benchmark. We fine-tuned both models 5 times for each task. \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9373884219510621
      ],
      "excerpt": "Table 1. KLEJ results for RoBERTa base model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9373884219510621
      ],
      "excerpt": "Table 2. KLEJ results for RoBERTa-v2 base model \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9349971120384497,
        0.8237177563673344
      ],
      "excerpt": "Table 3. KLEJ results for RoBERTa large model \n| Task                 | Task type                   | Metric |Base model                  | Large model                  | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        1.0
      ],
      "excerpt": "RoBERTa models for Polish",
      "technique": "GitHub API"
    }
  ],
  "downloadUrl": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/sdadas/polish-roberta/releases",
    "technique": "GitHub API"
  },
  "forks_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 11,
      "date": "Mon, 06 Dec 2021 07:10:31 GMT"
    },
    "technique": "GitHub API"
  },
  "forks_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/sdadas/polish-roberta/forks",
    "technique": "GitHub API"
  },
  "fullName": {
    "confidence": [
      1.0
    ],
    "excerpt": "sdadas/polish-roberta",
    "technique": "GitHub API"
  },
  "installation": [
    {
      "confidence": [
        0.9243519120760402
      ],
      "excerpt": "  <a href=\"https://github.com/sdadas/polish-roberta/releases/download/models/roberta_base_fairseq.zip\">v0.9.0</a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.895560074543537
      ],
      "excerpt": "  <a href=\"https://github.com/sdadas/polish-roberta/releases/download/models-transformers-v3.4.0/roberta_base_transformers.zip\">v3.4</a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8686807905723154
      ],
      "excerpt": "  <a href=\"https://github.com/sdadas/polish-roberta/releases/download/models-v2/roberta_base_fairseq.zip\">v0.10.1</a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.895560074543537
      ],
      "excerpt": "  <a href=\"https://github.com/sdadas/polish-roberta/releases/download/models-v2/roberta_base_transformers.zip\">v4.4</a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9243519120760402
      ],
      "excerpt": "  <a href=\"https://github.com/sdadas/polish-roberta/releases/download/models/roberta_large_fairseq.zip\">v0.9.0</a> \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.895560074543537
      ],
      "excerpt": "  <a href=\"https://github.com/sdadas/polish-roberta/releases/download/models-transformers-v3.4.0/roberta_large_transformers.zip\">v3.4</a> \n",
      "technique": "Supervised classification"
    }
  ],
  "invocation": [
    {
      "confidence": [
        0.8490463442364617
      ],
      "excerpt": "To replicate our experiments, first download the required datasets using download_data.py script: \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9246227682586091
      ],
      "excerpt": "python download_data.py \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.9221853417390514,
        0.9221853417390514,
        0.9221853417390514,
        0.9121972317453948,
        0.917688996106416,
        0.9228956196405198,
        0.9221853417390514,
        0.9221853417390514,
        0.9221853417390514,
        0.9221853417390514,
        0.9221853417390514,
        0.9221853417390514
      ],
      "excerpt": "python run_tasks.py --arch roberta_base --model_dir roberta_base_fairseq --train-epochs 10 --tasks KLEJ-NKJP --fp16 True --max-sentences 8 --update-freq 2 \npython run_tasks.py --arch roberta_base --model_dir roberta_base_fairseq --train-epochs 10 --tasks KLEJ-CDS-E --fp16 True --max-sentences 8 --update-freq 2 \npython run_tasks.py --arch roberta_base --model_dir roberta_base_fairseq --train-epochs 10 --tasks KLEJ-CDS-R --fp16 True --max-sentences 8 --update-freq 2 \npython run_tasks.py --arch roberta_base --model_dir roberta_base_fairseq --train-epochs 1 --tasks KLEJ-CBD --fp16 True --max-sentences 8 --update-freq 4 --resample 0:0.75,1:3 \npython run_tasks.py --arch roberta_base --model_dir roberta_base_fairseq --train-epochs 10 --tasks KLEJ-POLEMO-IN --fp16 True --max-sentences 8 --update-freq 2 \npython run_tasks.py --arch roberta_base --model_dir roberta_base_fairseq --train-epochs 10 --tasks KLEJ-POLEMO-OUT --fp16 True --max-sentences 8 --update-freq 2 \npython run_tasks.py --arch roberta_base --model_dir roberta_base_fairseq --train-epochs 10 --tasks KLEJ-DYK --fp16 True --max-sentences 8 --update-freq 4 --resample 0:1,1:3 \npython run_tasks.py --arch roberta_base --model_dir roberta_base_fairseq --train-epochs 10 --tasks KLEJ-PSC --fp16 True --max-sentences 8 --update-freq 4 --resample 0:1,1:3 \npython run_tasks.py --arch roberta_base --model_dir roberta_base_fairseq --train-epochs 10 --tasks KLEJ-ECR --fp16 True --max-sentences 8 --update-freq 2 \npython run_tasks.py --arch roberta_base --model_dir roberta_base_fairseq --train-epochs 10 --tasks 8TAGS --fp16 True --max-sentences 8 --update-freq 2 \npython run_tasks.py --arch roberta_base --model_dir roberta_base_fairseq --train-epochs 10 --tasks SICK-E --fp16 True --max-sentences 8 --update-freq 2 \npython run_tasks.py --arch roberta_base --model_dir roberta_base_fairseq --train-epochs 10 --tasks SICK-R --fp16 True --max-sentences 8 --update-freq 2 \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8388517492496647
      ],
      "excerpt": "| 1       |   93.15  |  93.30 |  94.26 | 66.67 |     91.97    |     78.74     | 66.86 | 98.63 | 87.75 |  85.70  | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8549221682441439
      ],
      "excerpt": "| 1       |   94.80  |  94.20 |  94.30 | 69.62 |     90.58    |     78.74     | 71.23 | 98.62 | 87.99 |  86.68  | \n",
      "technique": "Supervised classification"
    },
    {
      "confidence": [
        0.8251781430569786
      ],
      "excerpt": "| 5       |   94.31  |  94.20 |  94.71 | 70.46 |     91.00    |     77.94     | 71.67 | 98.48 | 88.15 |  86.77  | \n",
      "technique": "Supervised classification"
    }
  ],
  "issueTracker": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://api.github.com/repos/sdadas/polish-roberta/issues{/number}",
    "technique": "GitHub API"
  },
  "languages": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "Python"
    ],
    "technique": "GitHub API"
  },
  "license": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "name": "GNU Lesser General Public License v3.0",
      "url": "https://api.github.com/licenses/lgpl-3.0"
    },
    "technique": "GitHub API"
  },
  "licenseText": {
    "confidence": [
      1.0
    ],
    "excerpt": "b'                   GNU LESSER GENERAL PUBLIC LICENSE\\n                       Version 3, 29 June 2007\\n\\n Copyright (C) 2007 Free Software Foundation, Inc. https://fsf.org/\\n Everyone is permitted to copy and distribute verbatim copies\\n of this license document, but changing it is not allowed.\\n\\n\\n  This version of the GNU Lesser General Public License incorporates\\nthe terms and conditions of version 3 of the GNU General Public\\nLicense, supplemented by the additional permissions listed below.\\n\\n  0. Additional Definitions.\\n\\n  As used herein, \"this License\" refers to version 3 of the GNU Lesser\\nGeneral Public License, and the \"GNU GPL\" refers to version 3 of the GNU\\nGeneral Public License.\\n\\n  \"The Library\" refers to a covered work governed by this License,\\nother than an Application or a Combined Work as defined below.\\n\\n  An \"Application\" is any work that makes use of an interface provided\\nby the Library, but which is not otherwise based on the Library.\\nDefining a subclass of a class defined by the Library is deemed a mode\\nof using an interface provided by the Library.\\n\\n  A \"Combined Work\" is a work produced by combining or linking an\\nApplication with the Library.  The particular version of the Library\\nwith which the Combined Work was made is also called the \"Linked\\nVersion\".\\n\\n  The \"Minimal Corresponding Source\" for a Combined Work means the\\nCorresponding Source for the Combined Work, excluding any source code\\nfor portions of the Combined Work that, considered in isolation, are\\nbased on the Application, and not on the Linked Version.\\n\\n  The \"Corresponding Application Code\" for a Combined Work means the\\nobject code and/or source code for the Application, including any data\\nand utility programs needed for reproducing the Combined Work from the\\nApplication, but excluding the System Libraries of the Combined Work.\\n\\n  1. Exception to Section 3 of the GNU GPL.\\n\\n  You may convey a covered work under sections 3 and 4 of this License\\nwithout being bound by section 3 of the GNU GPL.\\n\\n  2. Conveying Modified Versions.\\n\\n  If you modify a copy of the Library, and, in your modifications, a\\nfacility refers to a function or data to be supplied by an Application\\nthat uses the facility (other than as an argument passed when the\\nfacility is invoked), then you may convey a copy of the modified\\nversion:\\n\\n   a) under this License, provided that you make a good faith effort to\\n   ensure that, in the event an Application does not supply the\\n   function or data, the facility still operates, and performs\\n   whatever part of its purpose remains meaningful, or\\n\\n   b) under the GNU GPL, with none of the additional permissions of\\n   this License applicable to that copy.\\n\\n  3. Object Code Incorporating Material from Library Header Files.\\n\\n  The object code form of an Application may incorporate material from\\na header file that is part of the Library.  You may convey such object\\ncode under terms of your choice, provided that, if the incorporated\\nmaterial is not limited to numerical parameters, data structure\\nlayouts and accessors, or small macros, inline functions and templates\\n(ten or fewer lines in length), you do both of the following:\\n\\n   a) Give prominent notice with each copy of the object code that the\\n   Library is used in it and that the Library and its use are\\n   covered by this License.\\n\\n   b) Accompany the object code with a copy of the GNU GPL and this license\\n   document.\\n\\n  4. Combined Works.\\n\\n  You may convey a Combined Work under terms of your choice that,\\ntaken together, effectively do not restrict modification of the\\nportions of the Library contained in the Combined Work and reverse\\nengineering for debugging such modifications, if you also do each of\\nthe following:\\n\\n   a) Give prominent notice with each copy of the Combined Work that\\n   the Library is used in it and that the Library and its use are\\n   covered by this License.\\n\\n   b) Accompany the Combined Work with a copy of the GNU GPL and this license\\n   document.\\n\\n   c) For a Combined Work that displays copyright notices during\\n   execution, include the copyright notice for the Library among\\n   these notices, as well as a reference directing the user to the\\n   copies of the GNU GPL and this license document.\\n\\n   d) Do one of the following:\\n\\n       0) Convey the Minimal Corresponding Source under the terms of this\\n       License, and the Corresponding Application Code in a form\\n       suitable for, and under terms that permit, the user to\\n       recombine or relink the Application with a modified version of\\n       the Linked Version to produce a modified Combined Work, in the\\n       manner specified by section 6 of the GNU GPL for conveying\\n       Corresponding Source.\\n\\n       1) Use a suitable shared library mechanism for linking with the\\n       Library.  A suitable mechanism is one that (a) uses at run time\\n       a copy of the Library already present on the user\\'s computer\\n       system, and (b) will operate properly with a modified version\\n       of the Library that is interface-compatible with the Linked\\n       Version.\\n\\n   e) Provide Installation Information, but only if you would otherwise\\n   be required to provide such information under section 6 of the\\n   GNU GPL, and only to the extent that such information is\\n   necessary to install and execute a modified version of the\\n   Combined Work produced by recombining or relinking the\\n   Application with a modified version of the Linked Version. (If\\n   you use option 4d0, the Installation Information must accompany\\n   the Minimal Corresponding Source and Corresponding Application\\n   Code. If you use option 4d1, you must provide the Installation\\n   Information in the manner specified by section 6 of the GNU GPL\\n   for conveying Corresponding Source.)\\n\\n  5. Combined Libraries.\\n\\n  You may place library facilities that are a work based on the\\nLibrary side by side in a single library together with other library\\nfacilities that are not Applications and are not covered by this\\nLicense, and convey such a combined library under terms of your\\nchoice, if you do both of the following:\\n\\n   a) Accompany the combined library with a copy of the same work based\\n   on the Library, uncombined with any other library facilities,\\n   conveyed under the terms of this License.\\n\\n   b) Give prominent notice with the combined library that part of it\\n   is a work based on the Library, and explaining where to find the\\n   accompanying uncombined form of the same work.\\n\\n  6. Revised Versions of the GNU Lesser General Public License.\\n\\n  The Free Software Foundation may publish revised and/or new versions\\nof the GNU Lesser General Public License from time to time. Such new\\nversions will be similar in spirit to the present version, but may\\ndiffer in detail to address new problems or concerns.\\n\\n  Each version is given a distinguishing version number. If the\\nLibrary as you received it specifies that a certain numbered version\\nof the GNU Lesser General Public License \"or any later version\"\\napplies to it, you have the option of following the terms and\\nconditions either of that published version or of any later version\\npublished by the Free Software Foundation. If the Library as you\\nreceived it does not specify a version number of the GNU Lesser\\nGeneral Public License, you may choose any version of the GNU Lesser\\nGeneral Public License ever published by the Free Software Foundation.\\n\\n  If the Library as you received it specifies that a proxy can decide\\nwhether future versions of the GNU Lesser General Public License shall\\napply, that proxy\\'s public statement of acceptance of any version is\\npermanent authorization for you to choose that version for the\\nLibrary.\\n'",
    "technique": "File Exploration"
  },
  "long_title": {
    "confidence": [
      1.0
    ],
    "excerpt": "## Polish RoBERTa",
    "technique": "Regular expression"
  },
  "name": {
    "confidence": [
      1.0
    ],
    "excerpt": "polish-roberta",
    "technique": "GitHub API"
  },
  "owner": {
    "confidence": [
      1.0
    ],
    "excerpt": "sdadas",
    "technique": "GitHub API"
  },
  "ownerType": {
    "confidence": [
      1.0
    ],
    "excerpt": "User",
    "technique": "GitHub API"
  },
  "readme_url": {
    "confidence": [
      1.0
    ],
    "excerpt": "https://github.com/sdadas/polish-roberta/blob/master/README.md",
    "technique": "GitHub API"
  },
  "releases": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      {
        "authorType": "User",
        "author_name": "sdadas",
        "body": "",
        "dateCreated": "2020-11-21T14:33:17Z",
        "datePublished": "2021-03-21T15:00:28Z",
        "html_url": "https://github.com/sdadas/polish-roberta/releases/tag/models-v2",
        "name": "RoBERTa-v2 models",
        "tag_name": "models-v2",
        "tarball_url": "https://api.github.com/repos/sdadas/polish-roberta/tarball/models-v2",
        "url": "https://api.github.com/repos/sdadas/polish-roberta/releases/40122167",
        "zipball_url": "https://api.github.com/repos/sdadas/polish-roberta/zipball/models-v2"
      },
      {
        "authorType": "User",
        "author_name": "sdadas",
        "body": "Models which utilise new tokenization API introduced in Transformers 3.",
        "dateCreated": "2020-10-04T14:55:15Z",
        "datePublished": "2020-10-23T17:04:31Z",
        "html_url": "https://github.com/sdadas/polish-roberta/releases/tag/models-transformers-v3.4.0",
        "name": "Models compatible with Huggingface Transformers 3.4.0",
        "tag_name": "models-transformers-v3.4.0",
        "tarball_url": "https://api.github.com/repos/sdadas/polish-roberta/tarball/models-transformers-v3.4.0",
        "url": "https://api.github.com/repos/sdadas/polish-roberta/releases/32989230",
        "zipball_url": "https://api.github.com/repos/sdadas/polish-roberta/zipball/models-transformers-v3.4.0"
      },
      {
        "authorType": "User",
        "author_name": "sdadas",
        "body": "",
        "dateCreated": "2020-07-16T18:11:58Z",
        "datePublished": "2020-10-04T08:05:13Z",
        "html_url": "https://github.com/sdadas/polish-roberta/releases/tag/english-datasets",
        "name": "English datasets",
        "tag_name": "english-datasets",
        "tarball_url": "https://api.github.com/repos/sdadas/polish-roberta/tarball/english-datasets",
        "url": "https://api.github.com/repos/sdadas/polish-roberta/releases/32134724",
        "zipball_url": "https://api.github.com/repos/sdadas/polish-roberta/zipball/english-datasets"
      },
      {
        "authorType": "User",
        "author_name": "sdadas",
        "body": "Models compatible with Huggingface Transformers 2.9.0",
        "dateCreated": "2020-05-13T14:37:56Z",
        "datePublished": "2020-05-13T15:06:16Z",
        "html_url": "https://github.com/sdadas/polish-roberta/releases/tag/models-transformers-v2.9.0",
        "name": "Models compatible with Huggingface Transformers 2.9.0",
        "tag_name": "models-transformers-v2.9.0",
        "tarball_url": "https://api.github.com/repos/sdadas/polish-roberta/tarball/models-transformers-v2.9.0",
        "url": "https://api.github.com/repos/sdadas/polish-roberta/releases/26471843",
        "zipball_url": "https://api.github.com/repos/sdadas/polish-roberta/zipball/models-transformers-v2.9.0"
      },
      {
        "authorType": "User",
        "author_name": "sdadas",
        "body": "",
        "dateCreated": "2020-04-30T14:20:56Z",
        "datePublished": "2020-05-03T09:48:00Z",
        "html_url": "https://github.com/sdadas/polish-roberta/releases/tag/models",
        "name": "RoBERTa models",
        "tag_name": "models",
        "tarball_url": "https://api.github.com/repos/sdadas/polish-roberta/tarball/models",
        "url": "https://api.github.com/repos/sdadas/polish-roberta/releases/26116042",
        "zipball_url": "https://api.github.com/repos/sdadas/polish-roberta/zipball/models"
      }
    ],
    "technique": "GitHub API"
  },
  "stargazers_count": {
    "confidence": [
      1.0
    ],
    "excerpt": {
      "count": 57,
      "date": "Mon, 06 Dec 2021 07:10:31 GMT"
    },
    "technique": "GitHub API"
  },
  "topics": {
    "confidence": [
      1.0
    ],
    "excerpt": [
      "bert",
      "roberta",
      "polish-language"
    ],
    "technique": "GitHub API"
  },
  "usage": [
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\nimport os\nfrom fairseq.models.roberta import RobertaModel, RobertaHubInterface\nfrom fairseq import hub_utils\n\nmodel_path = \"roberta_large_fairseq\"\nloaded = hub_utils.from_pretrained(\n    model_name_or_path=model_path,\n    data_name_or_path=model_path,\n    bpe=\"sentencepiece\",\n    sentencepiece_vocab=os.path.join(model_path, \"sentencepiece.bpe.model\"),\n    load_checkpoint_heads=True,\n    archive_map=RobertaModel.hub_models(),\n    cpu=True\n)\nroberta = RobertaHubInterface(loaded['args'], loaded['task'], loaded['models'][0])\nroberta.eval()\ninput = roberta.encode(\"Za\u017c\u00f3\u0142ci\u0107 g\u0119\u015bl\u0105 ja\u017a\u0144.\")\noutput = roberta.extract_features(input)\nprint(output[0][1])\n```\n\n",
      "technique": "Header extraction"
    },
    {
      "confidence": [
        1
      ],
      "excerpt": "```python\nimport torch, os\nfrom transformers import RobertaModel, AutoModel, PreTrainedTokenizerFast\n\nmodel_dir = \"roberta_base_transformers\"\ntokenizer = PreTrainedTokenizerFast(tokenizer_file=os.path.join(model_dir, \"tokenizer.json\"))\nmodel: RobertaModel = AutoModel.from_pretrained(model_dir)\ninput = tokenizer.encode(\"Za\u017c\u00f3\u0142ci\u0107 g\u0119\u015bl\u0105 ja\u017a\u0144.\")\noutput = model(torch.tensor([input]))[0]\nprint(output[0][1])\n```\n\n",
      "technique": "Header extraction"
    }
  ]
}